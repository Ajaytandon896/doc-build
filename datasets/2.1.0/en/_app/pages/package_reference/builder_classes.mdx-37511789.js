import{S as Nl,i as jl,s as Rl,e as s,k as d,w as p,t as r,M as Cl,c as n,d as t,m as l,a as o,x as f,h as i,b as c,F as e,g as v,y as u,L as kl,q as g,o as h,B as m,v as Ol}from"../../chunks/vendor-8138ceec.js";import{D as b}from"../../chunks/Docstring-fd69b455.js";import{C as qt}from"../../chunks/CodeBlock-fc89709f.js";import{I as Al}from"../../chunks/IconCopyLink-2dd3a6ac.js";function Ll(td){let q,fs,H,ne,Ht,Ee,en,Gt,tn,us,P,an,ft,sn,nn,ut,on,rn,gs,$,De,dn,Wt,ln,cn,gt,Xt,pn,fn,un,G,ht,zt,gn,hn,mn,mt,_t,_n,vn,bn,oe,vt,$n,wn,Jt,En,Dn,xn,N,Kt,yn,Tn,Yt,In,Bn,Qt,Sn,Nn,Zt,jn,Rn,Cn,re,xe,kn,ea,On,An,ie,ye,Ln,ta,Pn,Vn,de,Te,Fn,aa,Mn,Un,le,Ie,qn,sa,Hn,Gn,ce,Be,Wn,na,Xn,hs,k,Se,zn,oa,Jn,Kn,V,ra,Yn,Qn,ia,Zn,eo,da,to,ao,ms,W,Ne,so,la,no,_s,X,je,oo,ca,ro,vs,S,Re,io,Ce,lo,bt,co,po,fo,ke,uo,$t,go,ho,mo,F,Oe,_o,pa,vo,bo,z,fa,$o,wo,ua,Eo,Do,ga,xo,bs,E,Ae,yo,pe,Le,To,ha,Io,Bo,j,Pe,So,ma,No,jo,_a,Ro,Co,Ve,ko,fe,Fe,Oo,Me,Ao,va,Lo,Po,Vo,ue,Ue,Fo,ba,Mo,Uo,ge,qe,qo,$a,Ho,Go,he,He,Wo,wa,Xo,zo,me,Ge,Jo,Ea,Ko,$s,B,We,Yo,wt,Da,Qo,Zo,er,Xe,tr,xa,ar,sr,nr,ya,or,rr,ze,Ta,J,ws,ir,Ia,dr,lr,Ba,cr,pr,K,Y,Et,Sa,fr,ur,gr,Na,hr,mr,ja,_r,vr,Q,Ra,Ca,br,$r,ka,wr,Er,Oa,Dr,xr,Z,Aa,La,yr,Tr,Pa,Ir,Br,Va,Sr,Es,O,Je,Nr,Fa,jr,Rr,ee,Cr,Ma,kr,Or,Ua,Ar,Lr,Ds,y,Ke,Pr,Dt,qa,Vr,Fr,Mr,Ha,Ur,qr,A,xt,Ga,Hr,Gr,Wr,yt,Wa,Xr,zr,Jr,Tt,Xa,Kr,Yr,Qr,It,za,Zr,ei,ti,Bt,ai,Ja,si,ni,Ye,oi,Ka,ri,ii,xs,w,Qe,di,Ya,li,ci,Qa,pi,fi,Ze,ui,Za,gi,hi,et,mi,es,_i,vi,tt,bi,ts,$i,wi,at,ys,te,st,Ei,as,Di,Ts,T,nt,xi,ss,yi,Ti,ns,Ii,Bi,ot,Si,_e,rt,Ni,os,ji,Ri,M,it,Ci,rs,ki,Oi,is,Ai,Is,ae,dt,Li,ds,Pi,Bs,L,lt,Vi,ls,Fi,Mi,ve,ct,Ui,cs,qi,Ss;return Ee=new Al({}),De=new b({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L177"}}),xe=new b({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L744",returnDescription:`
<p>datasets.Dataset</p>
`}}),ye=new b({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.save_infos",description:"<strong>save_infos</strong> (<code>bool</code>) &#x2014; Save the dataset information (checksums/size/splits/&#x2026;)",name:"save_infos"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L487"}}),Te=new b({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L320"}}),Ie=new b({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L328"}}),Be=new b({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L482"}}),Se=new b({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L1012"}}),Ne=new b({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L1172"}}),je=new b({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L1110"}}),Re=new b({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L81"}}),Oe=new b({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/builder.py#L120"}}),Ae=new b({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:": bool = True"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/download_manager.py#L141"}}),Le=new b({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/download_manager.py#L259",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Pe=new b({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/download_manager.py#L367",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ve=new qt({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),Fe=new b({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/download_manager.py#L218",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ue=new b({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/download_manager.py#L337",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),qe=new b({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/download_manager.py#L310"}}),He=new b({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/download_manager.py#L326"}}),Ge=new b({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/download_manager.py#L183"}}),We=new b({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/download_manager.py#L44"}}),Je=new b({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/splits.py#L549"}}),Ke=new b({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/splits.py#L387"}}),Qe=new b({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/splits.py#L303"}}),Ze=new qt({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),et=new qt({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),tt=new qt({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),at=new qt({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),st=new b({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/splits.py#L372"}}),nt=new b({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/arrow_reader.py#L456"}}),ot=new qt({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),rt=new b({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/arrow_reader.py#L536",returnDescription:`
<p>ReadInstruction instance.</p>
`}}),it=new b({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/arrow_reader.py#L604",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),dt=new b({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[pathlib.Path, str, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/file_utils.py#L153"}}),lt=new b({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/version.py#L30"}}),ct=new b({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/2.1.0/src/datasets/utils/version.py#L92"}}),{c(){q=s("meta"),fs=d(),H=s("h1"),ne=s("a"),Ht=s("span"),p(Ee.$$.fragment),en=d(),Gt=s("span"),tn=r("Builder classes"),us=d(),P=s("p"),an=r("\u{1F917} Datasets relies on two main classes during the dataset building process: "),ft=s("a"),sn=r("DatasetBuilder"),nn=r(" and "),ut=s("a"),on=r("BuilderConfig"),rn=r("."),gs=d(),$=s("div"),p(De.$$.fragment),dn=d(),Wt=s("p"),ln=r("Abstract base class for all datasets."),cn=d(),gt=s("p"),Xt=s("em"),pn=r("DatasetBuilder"),fn=r(" has 3 key methods:"),un=d(),G=s("ul"),ht=s("li"),zt=s("code"),gn=r("datasets.DatasetBuilder.info"),hn=r(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),mn=d(),mt=s("li"),_t=s("a"),_n=r("datasets.DatasetBuilder.download_and_prepare()"),vn=r(`: Downloads the source data
and writes it to disk.`),bn=d(),oe=s("li"),vt=s("a"),$n=r("datasets.DatasetBuilder.as_dataset()"),wn=r(": Generates a "),Jt=s("em"),En=r("Dataset"),Dn=r("."),xn=d(),N=s("p"),Kt=s("strong"),yn=r("Configuration"),Tn=r(": Some "),Yt=s("em"),In=r("DatasetBuilder"),Bn=r(`s expose multiple variants of the
dataset by defining a `),Qt=s("em"),Sn=r("datasets.BuilderConfig"),Nn=r(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),Zt=s("code"),jn=r("datasets.DatasetBuilder.builder_configs()"),Rn=r("."),Cn=d(),re=s("div"),p(xe.$$.fragment),kn=d(),ea=s("p"),On=r("Return a Dataset for the specified split."),An=d(),ie=s("div"),p(ye.$$.fragment),Ln=d(),ta=s("p"),Pn=r("Downloads and prepares dataset for reading."),Vn=d(),de=s("div"),p(Te.$$.fragment),Fn=d(),aa=s("p"),Mn=r("Empty dict if doesn\u2019t exist"),Un=d(),le=s("div"),p(Ie.$$.fragment),qn=d(),sa=s("p"),Hn=r("Empty DatasetInfo if doesn\u2019t exist"),Gn=d(),ce=s("div"),p(Be.$$.fragment),Wn=d(),na=s("p"),Xn=r("Return the path of the module of this class or subclass."),hs=d(),k=s("div"),p(Se.$$.fragment),zn=d(),oa=s("p"),Jn=r("Base class for datasets with data generation based on dict generators."),Kn=d(),V=s("p"),ra=s("code"),Yn=r("GeneratorBasedBuilder"),Qn=r(` is a convenience class that abstracts away much
of the data writing and reading of `),ia=s("code"),Zn=r("DatasetBuilder"),eo=r(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),da=s("code"),to=r("_split_generators"),ao=r("). See the method docstrings for details."),ms=d(),W=s("div"),p(Ne.$$.fragment),so=d(),la=s("p"),no=r("Beam based Builder."),_s=d(),X=s("div"),p(je.$$.fragment),oo=d(),ca=s("p"),ro=r("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),vs=d(),S=s("div"),p(Re.$$.fragment),io=d(),Ce=s("p"),lo=r("Base class for "),bt=s("a"),co=r("DatasetBuilder"),po=r(" data configuration."),fo=d(),ke=s("p"),uo=r(`DatasetBuilder subclasses with data configuration options should subclass
`),$t=s("a"),go=r("BuilderConfig"),ho=r(" and add their own properties."),mo=d(),F=s("div"),p(Oe.$$.fragment),_o=d(),pa=s("p"),vo=r(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),bo=d(),z=s("ul"),fa=s("li"),$o=r("the config kwargs that can be used to overwrite attributes"),wo=d(),ua=s("li"),Eo=r("the custom features used to write the dataset"),Do=d(),ga=s("li"),xo=r(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),bs=d(),E=s("div"),p(Ae.$$.fragment),yo=d(),pe=s("div"),p(Le.$$.fragment),To=d(),ha=s("p"),Io=r("Download given url(s)."),Bo=d(),j=s("div"),p(Pe.$$.fragment),So=d(),ma=s("p"),No=r("Download and extract given url_or_urls."),jo=d(),_a=s("p"),Ro=r("Is roughly equivalent to:"),Co=d(),p(Ve.$$.fragment),ko=d(),fe=s("div"),p(Fe.$$.fragment),Oo=d(),Me=s("p"),Ao=r("Download given urls(s) by calling "),va=s("code"),Lo=r("custom_download"),Po=r("."),Vo=d(),ue=s("div"),p(Ue.$$.fragment),Fo=d(),ba=s("p"),Mo=r("Extract given path(s)."),Uo=d(),ge=s("div"),p(qe.$$.fragment),qo=d(),$a=s("p"),Ho=r("Iterate over files within an archive."),Go=d(),he=s("div"),p(He.$$.fragment),Wo=d(),wa=s("p"),Xo=r("Iterate over file paths."),zo=d(),me=s("div"),p(Ge.$$.fragment),Jo=d(),Ea=s("p"),Ko=r("Ship the files using Beam FileSystems to the pipeline temp dir."),$s=d(),B=s("div"),p(We.$$.fragment),Yo=d(),wt=s("p"),Da=s("code"),Qo=r("Enum"),Zo=r(" for how to treat pre-existing downloads and data."),er=d(),Xe=s("p"),tr=r("The default mode is "),xa=s("code"),ar=r("REUSE_DATASET_IF_EXISTS"),sr=r(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),nr=d(),ya=s("p"),or=r("The generations modes:"),rr=d(),ze=s("table"),Ta=s("thead"),J=s("tr"),ws=s("th"),ir=d(),Ia=s("th"),dr=r("Downloads"),lr=d(),Ba=s("th"),cr=r("Dataset"),pr=d(),K=s("tbody"),Y=s("tr"),Et=s("td"),Sa=s("code"),fr=r("REUSE_DATASET_IF_EXISTS"),ur=r(" (default)"),gr=d(),Na=s("td"),hr=r("Reuse"),mr=d(),ja=s("td"),_r=r("Reuse"),vr=d(),Q=s("tr"),Ra=s("td"),Ca=s("code"),br=r("REUSE_CACHE_IF_EXISTS"),$r=d(),ka=s("td"),wr=r("Reuse"),Er=d(),Oa=s("td"),Dr=r("Fresh"),xr=d(),Z=s("tr"),Aa=s("td"),La=s("code"),yr=r("FORCE_REDOWNLOAD"),Tr=d(),Pa=s("td"),Ir=r("Fresh"),Br=d(),Va=s("td"),Sr=r("Fresh"),Es=d(),O=s("div"),p(Je.$$.fragment),Nr=d(),Fa=s("p"),jr=r("Defines the split information for the generator."),Rr=d(),ee=s("p"),Cr=r(`This should be used as returned value of
`),Ma=s("code"),kr=r("GeneratorBasedBuilder._split_generators()"),Or=r(`.
See `),Ua=s("code"),Ar=r("GeneratorBasedBuilder._split_generators()"),Lr=r(` for more info and example
of usage.`),Ds=d(),y=s("div"),p(Ke.$$.fragment),Pr=d(),Dt=s("p"),qa=s("code"),Vr=r("Enum"),Fr=r(" for dataset splits."),Mr=d(),Ha=s("p"),Ur=r(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),qr=d(),A=s("ul"),xt=s("li"),Ga=s("code"),Hr=r("TRAIN"),Gr=r(": the training data."),Wr=d(),yt=s("li"),Wa=s("code"),Xr=r("VALIDATION"),zr=r(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Jr=d(),Tt=s("li"),Xa=s("code"),Kr=r("TEST"),Yr=r(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Qr=d(),It=s("li"),za=s("code"),Zr=r("ALL"),ei=r(": the union of all defined dataset splits."),ti=d(),Bt=s("p"),ai=r("Note: All splits, including compositions inherit from "),Ja=s("code"),si=r("datasets.SplitBase"),ni=d(),Ye=s("p"),oi=r("See the :doc:"),Ka=s("code"),ri=r("guide on splits </loading>"),ii=r(" for more information."),xs=d(),w=s("div"),p(Qe.$$.fragment),di=d(),Ya=s("p"),li=r("Descriptor corresponding to a named split (train, test, \u2026)."),ci=d(),Qa=s("p"),pi=r("Example:"),fi=d(),p(Ze.$$.fragment),ui=d(),Za=s("p"),gi=r(`Warning:
A split cannot be added twice, so the following will fail:`),hi=d(),p(et.$$.fragment),mi=d(),es=s("p"),_i=r(`Warning:
The slices can be applied only one time. So the following are valid:`),vi=d(),p(tt.$$.fragment),bi=d(),ts=s("p"),$i=r("But not:"),wi=d(),p(at.$$.fragment),ys=d(),te=s("div"),p(st.$$.fragment),Ei=d(),as=s("p"),Di=r("Split corresponding to the union of all defined dataset splits."),Ts=d(),T=s("div"),p(nt.$$.fragment),xi=d(),ss=s("p"),yi=r("Reading instruction for a dataset."),Ti=d(),ns=s("p"),Ii=r("Examples:"),Bi=d(),p(ot.$$.fragment),Si=d(),_e=s("div"),p(rt.$$.fragment),Ni=d(),os=s("p"),ji=r("Creates a ReadInstruction instance out of a string spec."),Ri=d(),M=s("div"),p(it.$$.fragment),Ci=d(),rs=s("p"),ki=r("Translate instruction into a list of absolute instructions."),Oi=d(),is=s("p"),Ai=r("Those absolute instructions are then to be added together."),Is=d(),ae=s("div"),p(dt.$$.fragment),Li=d(),ds=s("p"),Pi=r("Configuration for our cached path manager."),Bs=d(),L=s("div"),p(lt.$$.fragment),Vi=d(),ls=s("p"),Fi=r("Dataset version MAJOR.MINOR.PATCH."),Mi=d(),ve=s("div"),p(ct.$$.fragment),Ui=d(),cs=s("p"),qi=r("Returns True if other_version matches."),this.h()},l(a){const _=Cl('[data-svelte="svelte-1phssyn"]',document.head);q=n(_,"META",{name:!0,content:!0}),_.forEach(t),fs=l(a),H=n(a,"H1",{class:!0});var Ns=o(H);ne=n(Ns,"A",{id:!0,class:!0,href:!0});var ad=o(ne);Ht=n(ad,"SPAN",{});var sd=o(Ht);f(Ee.$$.fragment,sd),sd.forEach(t),ad.forEach(t),en=l(Ns),Gt=n(Ns,"SPAN",{});var nd=o(Gt);tn=i(nd,"Builder classes"),nd.forEach(t),Ns.forEach(t),us=l(a),P=n(a,"P",{});var St=o(P);an=i(St,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),ft=n(St,"A",{href:!0});var od=o(ft);sn=i(od,"DatasetBuilder"),od.forEach(t),nn=i(St," and "),ut=n(St,"A",{href:!0});var rd=o(ut);on=i(rd,"BuilderConfig"),rd.forEach(t),rn=i(St,"."),St.forEach(t),gs=l(a),$=n(a,"DIV",{class:!0});var D=o($);f(De.$$.fragment,D),dn=l(D),Wt=n(D,"P",{});var id=o(Wt);ln=i(id,"Abstract base class for all datasets."),id.forEach(t),cn=l(D),gt=n(D,"P",{});var Hi=o(gt);Xt=n(Hi,"EM",{});var dd=o(Xt);pn=i(dd,"DatasetBuilder"),dd.forEach(t),fn=i(Hi," has 3 key methods:"),Hi.forEach(t),un=l(D),G=n(D,"UL",{});var Nt=o(G);ht=n(Nt,"LI",{});var Gi=o(ht);zt=n(Gi,"CODE",{});var ld=o(zt);gn=i(ld,"datasets.DatasetBuilder.info"),ld.forEach(t),hn=i(Gi,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),Gi.forEach(t),mn=l(Nt),mt=n(Nt,"LI",{});var Wi=o(mt);_t=n(Wi,"A",{href:!0});var cd=o(_t);_n=i(cd,"datasets.DatasetBuilder.download_and_prepare()"),cd.forEach(t),vn=i(Wi,`: Downloads the source data
and writes it to disk.`),Wi.forEach(t),bn=l(Nt),oe=n(Nt,"LI",{});var ps=o(oe);vt=n(ps,"A",{href:!0});var pd=o(vt);$n=i(pd,"datasets.DatasetBuilder.as_dataset()"),pd.forEach(t),wn=i(ps,": Generates a "),Jt=n(ps,"EM",{});var fd=o(Jt);En=i(fd,"Dataset"),fd.forEach(t),Dn=i(ps,"."),ps.forEach(t),Nt.forEach(t),xn=l(D),N=n(D,"P",{});var se=o(N);Kt=n(se,"STRONG",{});var ud=o(Kt);yn=i(ud,"Configuration"),ud.forEach(t),Tn=i(se,": Some "),Yt=n(se,"EM",{});var gd=o(Yt);In=i(gd,"DatasetBuilder"),gd.forEach(t),Bn=i(se,`s expose multiple variants of the
dataset by defining a `),Qt=n(se,"EM",{});var hd=o(Qt);Sn=i(hd,"datasets.BuilderConfig"),hd.forEach(t),Nn=i(se,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),Zt=n(se,"CODE",{});var md=o(Zt);jn=i(md,"datasets.DatasetBuilder.builder_configs()"),md.forEach(t),Rn=i(se,"."),se.forEach(t),Cn=l(D),re=n(D,"DIV",{class:!0});var js=o(re);f(xe.$$.fragment,js),kn=l(js),ea=n(js,"P",{});var _d=o(ea);On=i(_d,"Return a Dataset for the specified split."),_d.forEach(t),js.forEach(t),An=l(D),ie=n(D,"DIV",{class:!0});var Rs=o(ie);f(ye.$$.fragment,Rs),Ln=l(Rs),ta=n(Rs,"P",{});var vd=o(ta);Pn=i(vd,"Downloads and prepares dataset for reading."),vd.forEach(t),Rs.forEach(t),Vn=l(D),de=n(D,"DIV",{class:!0});var Cs=o(de);f(Te.$$.fragment,Cs),Fn=l(Cs),aa=n(Cs,"P",{});var bd=o(aa);Mn=i(bd,"Empty dict if doesn\u2019t exist"),bd.forEach(t),Cs.forEach(t),Un=l(D),le=n(D,"DIV",{class:!0});var ks=o(le);f(Ie.$$.fragment,ks),qn=l(ks),sa=n(ks,"P",{});var $d=o(sa);Hn=i($d,"Empty DatasetInfo if doesn\u2019t exist"),$d.forEach(t),ks.forEach(t),Gn=l(D),ce=n(D,"DIV",{class:!0});var Os=o(ce);f(Be.$$.fragment,Os),Wn=l(Os),na=n(Os,"P",{});var wd=o(na);Xn=i(wd,"Return the path of the module of this class or subclass."),wd.forEach(t),Os.forEach(t),D.forEach(t),hs=l(a),k=n(a,"DIV",{class:!0});var jt=o(k);f(Se.$$.fragment,jt),zn=l(jt),oa=n(jt,"P",{});var Ed=o(oa);Jn=i(Ed,"Base class for datasets with data generation based on dict generators."),Ed.forEach(t),Kn=l(jt),V=n(jt,"P",{});var pt=o(V);ra=n(pt,"CODE",{});var Dd=o(ra);Yn=i(Dd,"GeneratorBasedBuilder"),Dd.forEach(t),Qn=i(pt,` is a convenience class that abstracts away much
of the data writing and reading of `),ia=n(pt,"CODE",{});var xd=o(ia);Zn=i(xd,"DatasetBuilder"),xd.forEach(t),eo=i(pt,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),da=n(pt,"CODE",{});var yd=o(da);to=i(yd,"_split_generators"),yd.forEach(t),ao=i(pt,"). See the method docstrings for details."),pt.forEach(t),jt.forEach(t),ms=l(a),W=n(a,"DIV",{class:!0});var As=o(W);f(Ne.$$.fragment,As),so=l(As),la=n(As,"P",{});var Td=o(la);no=i(Td,"Beam based Builder."),Td.forEach(t),As.forEach(t),_s=l(a),X=n(a,"DIV",{class:!0});var Ls=o(X);f(je.$$.fragment,Ls),oo=l(Ls),ca=n(Ls,"P",{});var Id=o(ca);ro=i(Id,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Id.forEach(t),Ls.forEach(t),vs=l(a),S=n(a,"DIV",{class:!0});var be=o(S);f(Re.$$.fragment,be),io=l(be),Ce=n(be,"P",{});var Ps=o(Ce);lo=i(Ps,"Base class for "),bt=n(Ps,"A",{href:!0});var Bd=o(bt);co=i(Bd,"DatasetBuilder"),Bd.forEach(t),po=i(Ps," data configuration."),Ps.forEach(t),fo=l(be),ke=n(be,"P",{});var Vs=o(ke);uo=i(Vs,`DatasetBuilder subclasses with data configuration options should subclass
`),$t=n(Vs,"A",{href:!0});var Sd=o($t);go=i(Sd,"BuilderConfig"),Sd.forEach(t),ho=i(Vs," and add their own properties."),Vs.forEach(t),mo=l(be),F=n(be,"DIV",{class:!0});var Rt=o(F);f(Oe.$$.fragment,Rt),_o=l(Rt),pa=n(Rt,"P",{});var Nd=o(pa);vo=i(Nd,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Nd.forEach(t),bo=l(Rt),z=n(Rt,"UL",{});var Ct=o(z);fa=n(Ct,"LI",{});var jd=o(fa);$o=i(jd,"the config kwargs that can be used to overwrite attributes"),jd.forEach(t),wo=l(Ct),ua=n(Ct,"LI",{});var Rd=o(ua);Eo=i(Rd,"the custom features used to write the dataset"),Rd.forEach(t),Do=l(Ct),ga=n(Ct,"LI",{});var Cd=o(ga);xo=i(Cd,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Cd.forEach(t),Ct.forEach(t),Rt.forEach(t),be.forEach(t),bs=l(a),E=n(a,"DIV",{class:!0});var I=o(E);f(Ae.$$.fragment,I),yo=l(I),pe=n(I,"DIV",{class:!0});var Fs=o(pe);f(Le.$$.fragment,Fs),To=l(Fs),ha=n(Fs,"P",{});var kd=o(ha);Io=i(kd,"Download given url(s)."),kd.forEach(t),Fs.forEach(t),Bo=l(I),j=n(I,"DIV",{class:!0});var $e=o(j);f(Pe.$$.fragment,$e),So=l($e),ma=n($e,"P",{});var Od=o(ma);No=i(Od,"Download and extract given url_or_urls."),Od.forEach(t),jo=l($e),_a=n($e,"P",{});var Ad=o(_a);Ro=i(Ad,"Is roughly equivalent to:"),Ad.forEach(t),Co=l($e),f(Ve.$$.fragment,$e),$e.forEach(t),ko=l(I),fe=n(I,"DIV",{class:!0});var Ms=o(fe);f(Fe.$$.fragment,Ms),Oo=l(Ms),Me=n(Ms,"P",{});var Us=o(Me);Ao=i(Us,"Download given urls(s) by calling "),va=n(Us,"CODE",{});var Ld=o(va);Lo=i(Ld,"custom_download"),Ld.forEach(t),Po=i(Us,"."),Us.forEach(t),Ms.forEach(t),Vo=l(I),ue=n(I,"DIV",{class:!0});var qs=o(ue);f(Ue.$$.fragment,qs),Fo=l(qs),ba=n(qs,"P",{});var Pd=o(ba);Mo=i(Pd,"Extract given path(s)."),Pd.forEach(t),qs.forEach(t),Uo=l(I),ge=n(I,"DIV",{class:!0});var Hs=o(ge);f(qe.$$.fragment,Hs),qo=l(Hs),$a=n(Hs,"P",{});var Vd=o($a);Ho=i(Vd,"Iterate over files within an archive."),Vd.forEach(t),Hs.forEach(t),Go=l(I),he=n(I,"DIV",{class:!0});var Gs=o(he);f(He.$$.fragment,Gs),Wo=l(Gs),wa=n(Gs,"P",{});var Fd=o(wa);Xo=i(Fd,"Iterate over file paths."),Fd.forEach(t),Gs.forEach(t),zo=l(I),me=n(I,"DIV",{class:!0});var Ws=o(me);f(Ge.$$.fragment,Ws),Jo=l(Ws),Ea=n(Ws,"P",{});var Md=o(Ea);Ko=i(Md,"Ship the files using Beam FileSystems to the pipeline temp dir."),Md.forEach(t),Ws.forEach(t),I.forEach(t),$s=l(a),B=n(a,"DIV",{class:!0});var U=o(B);f(We.$$.fragment,U),Yo=l(U),wt=n(U,"P",{});var Xi=o(wt);Da=n(Xi,"CODE",{});var Ud=o(Da);Qo=i(Ud,"Enum"),Ud.forEach(t),Zo=i(Xi," for how to treat pre-existing downloads and data."),Xi.forEach(t),er=l(U),Xe=n(U,"P",{});var Xs=o(Xe);tr=i(Xs,"The default mode is "),xa=n(Xs,"CODE",{});var qd=o(xa);ar=i(qd,"REUSE_DATASET_IF_EXISTS"),qd.forEach(t),sr=i(Xs,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Xs.forEach(t),nr=l(U),ya=n(U,"P",{});var Hd=o(ya);or=i(Hd,"The generations modes:"),Hd.forEach(t),rr=l(U),ze=n(U,"TABLE",{});var zs=o(ze);Ta=n(zs,"THEAD",{});var Gd=o(Ta);J=n(Gd,"TR",{});var kt=o(J);ws=n(kt,"TH",{}),o(ws).forEach(t),ir=l(kt),Ia=n(kt,"TH",{});var Wd=o(Ia);dr=i(Wd,"Downloads"),Wd.forEach(t),lr=l(kt),Ba=n(kt,"TH",{});var Xd=o(Ba);cr=i(Xd,"Dataset"),Xd.forEach(t),kt.forEach(t),Gd.forEach(t),pr=l(zs),K=n(zs,"TBODY",{});var Ot=o(K);Y=n(Ot,"TR",{});var At=o(Y);Et=n(At,"TD",{});var zi=o(Et);Sa=n(zi,"CODE",{});var zd=o(Sa);fr=i(zd,"REUSE_DATASET_IF_EXISTS"),zd.forEach(t),ur=i(zi," (default)"),zi.forEach(t),gr=l(At),Na=n(At,"TD",{});var Jd=o(Na);hr=i(Jd,"Reuse"),Jd.forEach(t),mr=l(At),ja=n(At,"TD",{});var Kd=o(ja);_r=i(Kd,"Reuse"),Kd.forEach(t),At.forEach(t),vr=l(Ot),Q=n(Ot,"TR",{});var Lt=o(Q);Ra=n(Lt,"TD",{});var Yd=o(Ra);Ca=n(Yd,"CODE",{});var Qd=o(Ca);br=i(Qd,"REUSE_CACHE_IF_EXISTS"),Qd.forEach(t),Yd.forEach(t),$r=l(Lt),ka=n(Lt,"TD",{});var Zd=o(ka);wr=i(Zd,"Reuse"),Zd.forEach(t),Er=l(Lt),Oa=n(Lt,"TD",{});var el=o(Oa);Dr=i(el,"Fresh"),el.forEach(t),Lt.forEach(t),xr=l(Ot),Z=n(Ot,"TR",{});var Pt=o(Z);Aa=n(Pt,"TD",{});var tl=o(Aa);La=n(tl,"CODE",{});var al=o(La);yr=i(al,"FORCE_REDOWNLOAD"),al.forEach(t),tl.forEach(t),Tr=l(Pt),Pa=n(Pt,"TD",{});var sl=o(Pa);Ir=i(sl,"Fresh"),sl.forEach(t),Br=l(Pt),Va=n(Pt,"TD",{});var nl=o(Va);Sr=i(nl,"Fresh"),nl.forEach(t),Pt.forEach(t),Ot.forEach(t),zs.forEach(t),U.forEach(t),Es=l(a),O=n(a,"DIV",{class:!0});var Vt=o(O);f(Je.$$.fragment,Vt),Nr=l(Vt),Fa=n(Vt,"P",{});var ol=o(Fa);jr=i(ol,"Defines the split information for the generator."),ol.forEach(t),Rr=l(Vt),ee=n(Vt,"P",{});var Ft=o(ee);Cr=i(Ft,`This should be used as returned value of
`),Ma=n(Ft,"CODE",{});var rl=o(Ma);kr=i(rl,"GeneratorBasedBuilder._split_generators()"),rl.forEach(t),Or=i(Ft,`.
See `),Ua=n(Ft,"CODE",{});var il=o(Ua);Ar=i(il,"GeneratorBasedBuilder._split_generators()"),il.forEach(t),Lr=i(Ft,` for more info and example
of usage.`),Ft.forEach(t),Vt.forEach(t),Ds=l(a),y=n(a,"DIV",{class:!0});var R=o(y);f(Ke.$$.fragment,R),Pr=l(R),Dt=n(R,"P",{});var Ji=o(Dt);qa=n(Ji,"CODE",{});var dl=o(qa);Vr=i(dl,"Enum"),dl.forEach(t),Fr=i(Ji," for dataset splits."),Ji.forEach(t),Mr=l(R),Ha=n(R,"P",{});var ll=o(Ha);Ur=i(ll,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),ll.forEach(t),qr=l(R),A=n(R,"UL",{});var we=o(A);xt=n(we,"LI",{});var Ki=o(xt);Ga=n(Ki,"CODE",{});var cl=o(Ga);Hr=i(cl,"TRAIN"),cl.forEach(t),Gr=i(Ki,": the training data."),Ki.forEach(t),Wr=l(we),yt=n(we,"LI",{});var Yi=o(yt);Wa=n(Yi,"CODE",{});var pl=o(Wa);Xr=i(pl,"VALIDATION"),pl.forEach(t),zr=i(Yi,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Yi.forEach(t),Jr=l(we),Tt=n(we,"LI",{});var Qi=o(Tt);Xa=n(Qi,"CODE",{});var fl=o(Xa);Kr=i(fl,"TEST"),fl.forEach(t),Yr=i(Qi,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Qi.forEach(t),Qr=l(we),It=n(we,"LI",{});var Zi=o(It);za=n(Zi,"CODE",{});var ul=o(za);Zr=i(ul,"ALL"),ul.forEach(t),ei=i(Zi,": the union of all defined dataset splits."),Zi.forEach(t),we.forEach(t),ti=l(R),Bt=n(R,"P",{});var ed=o(Bt);ai=i(ed,"Note: All splits, including compositions inherit from "),Ja=n(ed,"CODE",{});var gl=o(Ja);si=i(gl,"datasets.SplitBase"),gl.forEach(t),ed.forEach(t),ni=l(R),Ye=n(R,"P",{});var Js=o(Ye);oi=i(Js,"See the :doc:"),Ka=n(Js,"CODE",{});var hl=o(Ka);ri=i(hl,"guide on splits </loading>"),hl.forEach(t),ii=i(Js," for more information."),Js.forEach(t),R.forEach(t),xs=l(a),w=n(a,"DIV",{class:!0});var x=o(w);f(Qe.$$.fragment,x),di=l(x),Ya=n(x,"P",{});var ml=o(Ya);li=i(ml,"Descriptor corresponding to a named split (train, test, \u2026)."),ml.forEach(t),ci=l(x),Qa=n(x,"P",{});var _l=o(Qa);pi=i(_l,"Example:"),_l.forEach(t),fi=l(x),f(Ze.$$.fragment,x),ui=l(x),Za=n(x,"P",{});var vl=o(Za);gi=i(vl,`Warning:
A split cannot be added twice, so the following will fail:`),vl.forEach(t),hi=l(x),f(et.$$.fragment,x),mi=l(x),es=n(x,"P",{});var bl=o(es);_i=i(bl,`Warning:
The slices can be applied only one time. So the following are valid:`),bl.forEach(t),vi=l(x),f(tt.$$.fragment,x),bi=l(x),ts=n(x,"P",{});var $l=o(ts);$i=i($l,"But not:"),$l.forEach(t),wi=l(x),f(at.$$.fragment,x),x.forEach(t),ys=l(a),te=n(a,"DIV",{class:!0});var Ks=o(te);f(st.$$.fragment,Ks),Ei=l(Ks),as=n(Ks,"P",{});var wl=o(as);Di=i(wl,"Split corresponding to the union of all defined dataset splits."),wl.forEach(t),Ks.forEach(t),Ts=l(a),T=n(a,"DIV",{class:!0});var C=o(T);f(nt.$$.fragment,C),xi=l(C),ss=n(C,"P",{});var El=o(ss);yi=i(El,"Reading instruction for a dataset."),El.forEach(t),Ti=l(C),ns=n(C,"P",{});var Dl=o(ns);Ii=i(Dl,"Examples:"),Dl.forEach(t),Bi=l(C),f(ot.$$.fragment,C),Si=l(C),_e=n(C,"DIV",{class:!0});var Ys=o(_e);f(rt.$$.fragment,Ys),Ni=l(Ys),os=n(Ys,"P",{});var xl=o(os);ji=i(xl,"Creates a ReadInstruction instance out of a string spec."),xl.forEach(t),Ys.forEach(t),Ri=l(C),M=n(C,"DIV",{class:!0});var Mt=o(M);f(it.$$.fragment,Mt),Ci=l(Mt),rs=n(Mt,"P",{});var yl=o(rs);ki=i(yl,"Translate instruction into a list of absolute instructions."),yl.forEach(t),Oi=l(Mt),is=n(Mt,"P",{});var Tl=o(is);Ai=i(Tl,"Those absolute instructions are then to be added together."),Tl.forEach(t),Mt.forEach(t),C.forEach(t),Is=l(a),ae=n(a,"DIV",{class:!0});var Qs=o(ae);f(dt.$$.fragment,Qs),Li=l(Qs),ds=n(Qs,"P",{});var Il=o(ds);Pi=i(Il,"Configuration for our cached path manager."),Il.forEach(t),Qs.forEach(t),Bs=l(a),L=n(a,"DIV",{class:!0});var Ut=o(L);f(lt.$$.fragment,Ut),Vi=l(Ut),ls=n(Ut,"P",{});var Bl=o(ls);Fi=i(Bl,"Dataset version MAJOR.MINOR.PATCH."),Bl.forEach(t),Mi=l(Ut),ve=n(Ut,"DIV",{class:!0});var Zs=o(ve);f(ct.$$.fragment,Zs),Ui=l(Zs),cs=n(Zs,"P",{});var Sl=o(cs);qi=i(Sl,"Returns True if other_version matches."),Sl.forEach(t),Zs.forEach(t),Ut.forEach(t),this.h()},h(){c(q,"name","hf:doc:metadata"),c(q,"content",JSON.stringify(Pl)),c(ne,"id","datasets.DatasetBuilder"),c(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ne,"href","#datasets.DatasetBuilder"),c(H,"class","relative group"),c(ft,"href","/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.DatasetBuilder"),c(ut,"href","/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.BuilderConfig"),c(_t,"href","/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),c(vt,"href","/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),c(re,"class","docstring"),c(ie,"class","docstring"),c(de,"class","docstring"),c(le,"class","docstring"),c(ce,"class","docstring"),c($,"class","docstring"),c(k,"class","docstring"),c(W,"class","docstring"),c(X,"class","docstring"),c(bt,"href","/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.DatasetBuilder"),c($t,"href","/docs/datasets/2.1.0/en/package_reference/builder_classes#datasets.BuilderConfig"),c(F,"class","docstring"),c(S,"class","docstring"),c(pe,"class","docstring"),c(j,"class","docstring"),c(fe,"class","docstring"),c(ue,"class","docstring"),c(ge,"class","docstring"),c(he,"class","docstring"),c(me,"class","docstring"),c(E,"class","docstring"),c(B,"class","docstring"),c(O,"class","docstring"),c(y,"class","docstring"),c(w,"class","docstring"),c(te,"class","docstring"),c(_e,"class","docstring"),c(M,"class","docstring"),c(T,"class","docstring"),c(ae,"class","docstring"),c(ve,"class","docstring"),c(L,"class","docstring")},m(a,_){e(document.head,q),v(a,fs,_),v(a,H,_),e(H,ne),e(ne,Ht),u(Ee,Ht,null),e(H,en),e(H,Gt),e(Gt,tn),v(a,us,_),v(a,P,_),e(P,an),e(P,ft),e(ft,sn),e(P,nn),e(P,ut),e(ut,on),e(P,rn),v(a,gs,_),v(a,$,_),u(De,$,null),e($,dn),e($,Wt),e(Wt,ln),e($,cn),e($,gt),e(gt,Xt),e(Xt,pn),e(gt,fn),e($,un),e($,G),e(G,ht),e(ht,zt),e(zt,gn),e(ht,hn),e(G,mn),e(G,mt),e(mt,_t),e(_t,_n),e(mt,vn),e(G,bn),e(G,oe),e(oe,vt),e(vt,$n),e(oe,wn),e(oe,Jt),e(Jt,En),e(oe,Dn),e($,xn),e($,N),e(N,Kt),e(Kt,yn),e(N,Tn),e(N,Yt),e(Yt,In),e(N,Bn),e(N,Qt),e(Qt,Sn),e(N,Nn),e(N,Zt),e(Zt,jn),e(N,Rn),e($,Cn),e($,re),u(xe,re,null),e(re,kn),e(re,ea),e(ea,On),e($,An),e($,ie),u(ye,ie,null),e(ie,Ln),e(ie,ta),e(ta,Pn),e($,Vn),e($,de),u(Te,de,null),e(de,Fn),e(de,aa),e(aa,Mn),e($,Un),e($,le),u(Ie,le,null),e(le,qn),e(le,sa),e(sa,Hn),e($,Gn),e($,ce),u(Be,ce,null),e(ce,Wn),e(ce,na),e(na,Xn),v(a,hs,_),v(a,k,_),u(Se,k,null),e(k,zn),e(k,oa),e(oa,Jn),e(k,Kn),e(k,V),e(V,ra),e(ra,Yn),e(V,Qn),e(V,ia),e(ia,Zn),e(V,eo),e(V,da),e(da,to),e(V,ao),v(a,ms,_),v(a,W,_),u(Ne,W,null),e(W,so),e(W,la),e(la,no),v(a,_s,_),v(a,X,_),u(je,X,null),e(X,oo),e(X,ca),e(ca,ro),v(a,vs,_),v(a,S,_),u(Re,S,null),e(S,io),e(S,Ce),e(Ce,lo),e(Ce,bt),e(bt,co),e(Ce,po),e(S,fo),e(S,ke),e(ke,uo),e(ke,$t),e($t,go),e(ke,ho),e(S,mo),e(S,F),u(Oe,F,null),e(F,_o),e(F,pa),e(pa,vo),e(F,bo),e(F,z),e(z,fa),e(fa,$o),e(z,wo),e(z,ua),e(ua,Eo),e(z,Do),e(z,ga),e(ga,xo),v(a,bs,_),v(a,E,_),u(Ae,E,null),e(E,yo),e(E,pe),u(Le,pe,null),e(pe,To),e(pe,ha),e(ha,Io),e(E,Bo),e(E,j),u(Pe,j,null),e(j,So),e(j,ma),e(ma,No),e(j,jo),e(j,_a),e(_a,Ro),e(j,Co),u(Ve,j,null),e(E,ko),e(E,fe),u(Fe,fe,null),e(fe,Oo),e(fe,Me),e(Me,Ao),e(Me,va),e(va,Lo),e(Me,Po),e(E,Vo),e(E,ue),u(Ue,ue,null),e(ue,Fo),e(ue,ba),e(ba,Mo),e(E,Uo),e(E,ge),u(qe,ge,null),e(ge,qo),e(ge,$a),e($a,Ho),e(E,Go),e(E,he),u(He,he,null),e(he,Wo),e(he,wa),e(wa,Xo),e(E,zo),e(E,me),u(Ge,me,null),e(me,Jo),e(me,Ea),e(Ea,Ko),v(a,$s,_),v(a,B,_),u(We,B,null),e(B,Yo),e(B,wt),e(wt,Da),e(Da,Qo),e(wt,Zo),e(B,er),e(B,Xe),e(Xe,tr),e(Xe,xa),e(xa,ar),e(Xe,sr),e(B,nr),e(B,ya),e(ya,or),e(B,rr),e(B,ze),e(ze,Ta),e(Ta,J),e(J,ws),e(J,ir),e(J,Ia),e(Ia,dr),e(J,lr),e(J,Ba),e(Ba,cr),e(ze,pr),e(ze,K),e(K,Y),e(Y,Et),e(Et,Sa),e(Sa,fr),e(Et,ur),e(Y,gr),e(Y,Na),e(Na,hr),e(Y,mr),e(Y,ja),e(ja,_r),e(K,vr),e(K,Q),e(Q,Ra),e(Ra,Ca),e(Ca,br),e(Q,$r),e(Q,ka),e(ka,wr),e(Q,Er),e(Q,Oa),e(Oa,Dr),e(K,xr),e(K,Z),e(Z,Aa),e(Aa,La),e(La,yr),e(Z,Tr),e(Z,Pa),e(Pa,Ir),e(Z,Br),e(Z,Va),e(Va,Sr),v(a,Es,_),v(a,O,_),u(Je,O,null),e(O,Nr),e(O,Fa),e(Fa,jr),e(O,Rr),e(O,ee),e(ee,Cr),e(ee,Ma),e(Ma,kr),e(ee,Or),e(ee,Ua),e(Ua,Ar),e(ee,Lr),v(a,Ds,_),v(a,y,_),u(Ke,y,null),e(y,Pr),e(y,Dt),e(Dt,qa),e(qa,Vr),e(Dt,Fr),e(y,Mr),e(y,Ha),e(Ha,Ur),e(y,qr),e(y,A),e(A,xt),e(xt,Ga),e(Ga,Hr),e(xt,Gr),e(A,Wr),e(A,yt),e(yt,Wa),e(Wa,Xr),e(yt,zr),e(A,Jr),e(A,Tt),e(Tt,Xa),e(Xa,Kr),e(Tt,Yr),e(A,Qr),e(A,It),e(It,za),e(za,Zr),e(It,ei),e(y,ti),e(y,Bt),e(Bt,ai),e(Bt,Ja),e(Ja,si),e(y,ni),e(y,Ye),e(Ye,oi),e(Ye,Ka),e(Ka,ri),e(Ye,ii),v(a,xs,_),v(a,w,_),u(Qe,w,null),e(w,di),e(w,Ya),e(Ya,li),e(w,ci),e(w,Qa),e(Qa,pi),e(w,fi),u(Ze,w,null),e(w,ui),e(w,Za),e(Za,gi),e(w,hi),u(et,w,null),e(w,mi),e(w,es),e(es,_i),e(w,vi),u(tt,w,null),e(w,bi),e(w,ts),e(ts,$i),e(w,wi),u(at,w,null),v(a,ys,_),v(a,te,_),u(st,te,null),e(te,Ei),e(te,as),e(as,Di),v(a,Ts,_),v(a,T,_),u(nt,T,null),e(T,xi),e(T,ss),e(ss,yi),e(T,Ti),e(T,ns),e(ns,Ii),e(T,Bi),u(ot,T,null),e(T,Si),e(T,_e),u(rt,_e,null),e(_e,Ni),e(_e,os),e(os,ji),e(T,Ri),e(T,M),u(it,M,null),e(M,Ci),e(M,rs),e(rs,ki),e(M,Oi),e(M,is),e(is,Ai),v(a,Is,_),v(a,ae,_),u(dt,ae,null),e(ae,Li),e(ae,ds),e(ds,Pi),v(a,Bs,_),v(a,L,_),u(lt,L,null),e(L,Vi),e(L,ls),e(ls,Fi),e(L,Mi),e(L,ve),u(ct,ve,null),e(ve,Ui),e(ve,cs),e(cs,qi),Ss=!0},p:kl,i(a){Ss||(g(Ee.$$.fragment,a),g(De.$$.fragment,a),g(xe.$$.fragment,a),g(ye.$$.fragment,a),g(Te.$$.fragment,a),g(Ie.$$.fragment,a),g(Be.$$.fragment,a),g(Se.$$.fragment,a),g(Ne.$$.fragment,a),g(je.$$.fragment,a),g(Re.$$.fragment,a),g(Oe.$$.fragment,a),g(Ae.$$.fragment,a),g(Le.$$.fragment,a),g(Pe.$$.fragment,a),g(Ve.$$.fragment,a),g(Fe.$$.fragment,a),g(Ue.$$.fragment,a),g(qe.$$.fragment,a),g(He.$$.fragment,a),g(Ge.$$.fragment,a),g(We.$$.fragment,a),g(Je.$$.fragment,a),g(Ke.$$.fragment,a),g(Qe.$$.fragment,a),g(Ze.$$.fragment,a),g(et.$$.fragment,a),g(tt.$$.fragment,a),g(at.$$.fragment,a),g(st.$$.fragment,a),g(nt.$$.fragment,a),g(ot.$$.fragment,a),g(rt.$$.fragment,a),g(it.$$.fragment,a),g(dt.$$.fragment,a),g(lt.$$.fragment,a),g(ct.$$.fragment,a),Ss=!0)},o(a){h(Ee.$$.fragment,a),h(De.$$.fragment,a),h(xe.$$.fragment,a),h(ye.$$.fragment,a),h(Te.$$.fragment,a),h(Ie.$$.fragment,a),h(Be.$$.fragment,a),h(Se.$$.fragment,a),h(Ne.$$.fragment,a),h(je.$$.fragment,a),h(Re.$$.fragment,a),h(Oe.$$.fragment,a),h(Ae.$$.fragment,a),h(Le.$$.fragment,a),h(Pe.$$.fragment,a),h(Ve.$$.fragment,a),h(Fe.$$.fragment,a),h(Ue.$$.fragment,a),h(qe.$$.fragment,a),h(He.$$.fragment,a),h(Ge.$$.fragment,a),h(We.$$.fragment,a),h(Je.$$.fragment,a),h(Ke.$$.fragment,a),h(Qe.$$.fragment,a),h(Ze.$$.fragment,a),h(et.$$.fragment,a),h(tt.$$.fragment,a),h(at.$$.fragment,a),h(st.$$.fragment,a),h(nt.$$.fragment,a),h(ot.$$.fragment,a),h(rt.$$.fragment,a),h(it.$$.fragment,a),h(dt.$$.fragment,a),h(lt.$$.fragment,a),h(ct.$$.fragment,a),Ss=!1},d(a){t(q),a&&t(fs),a&&t(H),m(Ee),a&&t(us),a&&t(P),a&&t(gs),a&&t($),m(De),m(xe),m(ye),m(Te),m(Ie),m(Be),a&&t(hs),a&&t(k),m(Se),a&&t(ms),a&&t(W),m(Ne),a&&t(_s),a&&t(X),m(je),a&&t(vs),a&&t(S),m(Re),m(Oe),a&&t(bs),a&&t(E),m(Ae),m(Le),m(Pe),m(Ve),m(Fe),m(Ue),m(qe),m(He),m(Ge),a&&t($s),a&&t(B),m(We),a&&t(Es),a&&t(O),m(Je),a&&t(Ds),a&&t(y),m(Ke),a&&t(xs),a&&t(w),m(Qe),m(Ze),m(et),m(tt),m(at),a&&t(ys),a&&t(te),m(st),a&&t(Ts),a&&t(T),m(nt),m(ot),m(rt),m(it),a&&t(Is),a&&t(ae),m(dt),a&&t(Bs),a&&t(L),m(lt),m(ct)}}}const Pl={local:"datasets.DatasetBuilder",title:"Builder classes"};function Vl(td){return Ol(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Hl extends Nl{constructor(q){super();jl(this,q,Vl,Ll,Rl,{})}}export{Hl as default,Pl as metadata};
