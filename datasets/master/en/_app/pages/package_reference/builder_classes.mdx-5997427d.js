import{S as Sl,i as Nl,s as jl,e as s,k as d,w as p,t as o,M as Rl,c as n,d as t,m as l,a as r,x as f,h as i,b as c,F as e,g as v,y as m,L as Cl,q as u,o as g,B as h,v as kl}from"../../chunks/vendor-aa873a46.js";import{D as b}from"../../chunks/Docstring-337b9264.js";import{C as qt}from"../../chunks/CodeBlock-1f14baf3.js";import{I as Ol}from"../../chunks/IconCopyLink-d0ca3106.js";function Al(ed){let q,fs,H,se,Ht,Ee,en,Gt,tn,ms,P,an,ft,sn,nn,mt,rn,on,us,$,De,dn,Wt,ln,cn,ut,Xt,pn,fn,mn,G,gt,zt,un,gn,hn,ht,_t,_n,vn,bn,ne,vt,$n,wn,Jt,En,Dn,xn,C,Kt,yn,Tn,Yt,In,Bn,Qt,Sn,Nn,Zt,jn,Rn,re,xe,Cn,ea,kn,On,oe,ye,An,ta,Ln,Pn,ie,Te,Vn,aa,Fn,Mn,de,Ie,Un,sa,qn,Hn,le,Be,Gn,na,Wn,gs,k,Se,Xn,ra,zn,Jn,V,oa,Kn,Yn,ia,Qn,Zn,da,er,tr,hs,W,Ne,ar,la,sr,_s,X,je,nr,ca,rr,vs,S,Re,or,Ce,ir,bt,dr,lr,cr,ke,pr,$t,fr,mr,ur,F,Oe,gr,pa,hr,_r,z,fa,vr,br,ma,$r,wr,ua,Er,bs,E,Ae,Dr,ce,Le,xr,ga,yr,Tr,N,Pe,Ir,ha,Br,Sr,_a,Nr,jr,Ve,Rr,pe,Fe,Cr,Me,kr,va,Or,Ar,Lr,fe,Ue,Pr,ba,Vr,Fr,me,qe,Mr,$a,Ur,qr,ue,He,Hr,wa,Gr,Wr,ge,Ge,Xr,Ea,zr,$s,B,We,Jr,wt,Da,Kr,Yr,Qr,Xe,Zr,xa,eo,to,ao,ya,so,no,ze,Ta,J,ws,ro,Ia,oo,io,Ba,lo,co,K,Y,Et,Sa,po,fo,mo,Na,uo,go,ja,ho,_o,Q,Ra,Ca,vo,bo,ka,$o,wo,Oa,Eo,Do,Z,Aa,La,xo,yo,Pa,To,Io,Va,Bo,Es,O,Je,So,Fa,No,jo,ee,Ro,Ma,Co,ko,Ua,Oo,Ao,Ds,y,Ke,Lo,Dt,qa,Po,Vo,Fo,Ha,Mo,Uo,A,xt,Ga,qo,Ho,Go,yt,Wa,Wo,Xo,zo,Tt,Xa,Jo,Ko,Yo,It,za,Qo,Zo,ei,Bt,ti,Ja,ai,si,Ye,ni,Ka,ri,oi,xs,w,Qe,ii,Ya,di,li,Qa,ci,pi,Ze,fi,Za,mi,ui,et,gi,es,hi,_i,tt,vi,ts,bi,$i,at,ys,te,st,wi,as,Ei,Ts,T,nt,Di,ss,xi,yi,ns,Ti,Ii,rt,Bi,he,ot,Si,rs,Ni,ji,M,it,Ri,os,Ci,ki,is,Oi,Is,ae,dt,Ai,ds,Li,Bs,L,lt,Pi,ls,Vi,Fi,_e,ct,Mi,cs,Ui,Ss;return Ee=new Ol({}),De=new b({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L177"}}),xe=new b({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L744",parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],returnDescription:`
<p>datasets.Dataset</p>
`}}),ye=new b({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L487",parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.save_infos",description:"<strong>save_infos</strong> (<code>bool</code>) &#x2014; Save the dataset information (checksums/size/splits/&#x2026;)",name:"save_infos"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}]}}),Te=new b({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L320"}}),Ie=new b({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L328"}}),Be=new b({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L482"}}),Se=new b({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1012"}}),Ne=new b({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1172"}}),je=new b({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1110"}}),Re=new b({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L81",parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/master/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}]}}),Oe=new b({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L120"}}),Ae=new b({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:": bool = True"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L141"}}),Le=new b({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L259",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Pe=new b({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L367",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ve=new qt({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),Fe=new b({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L218",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Ue=new b({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L337",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),qe=new b({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L310",parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}]}}),He=new b({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L326",parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}]}}),Ge=new b({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L183"}}),We=new b({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L44"}}),Je=new b({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L549",parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}]}}),Ke=new b({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L387"}}),Qe=new b({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L303"}}),Ze=new qt({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),et=new qt({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),tt=new qt({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),at=new qt({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),st=new b({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L372"}}),nt=new b({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L456"}}),rt=new qt({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),ot=new b({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L536",parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],returnDescription:`
<p>ReadInstruction instance.</p>
`}}),it=new b({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L604",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),dt=new b({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[pathlib.Path, str, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/file_utils.py#L153",parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}]}}),lt=new b({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/version.py#L30",parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}]}}),ct=new b({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/master/src/datasets/utils/version.py#L92"}}),{c(){q=s("meta"),fs=d(),H=s("h1"),se=s("a"),Ht=s("span"),p(Ee.$$.fragment),en=d(),Gt=s("span"),tn=o("Builder classes"),ms=d(),P=s("p"),an=o("\u{1F917} Datasets relies on two main classes during the dataset building process: "),ft=s("a"),sn=o("DatasetBuilder"),nn=o(" and "),mt=s("a"),rn=o("BuilderConfig"),on=o("."),us=d(),$=s("div"),p(De.$$.fragment),dn=d(),Wt=s("p"),ln=o("Abstract base class for all datasets."),cn=d(),ut=s("p"),Xt=s("em"),pn=o("DatasetBuilder"),fn=o(" has 3 key methods:"),mn=d(),G=s("ul"),gt=s("li"),zt=s("code"),un=o("datasets.DatasetBuilder.info"),gn=o(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),hn=d(),ht=s("li"),_t=s("a"),_n=o("datasets.DatasetBuilder.download_and_prepare()"),vn=o(`: Downloads the source data
and writes it to disk.`),bn=d(),ne=s("li"),vt=s("a"),$n=o("datasets.DatasetBuilder.as_dataset()"),wn=o(": Generates a "),Jt=s("em"),En=o("Dataset"),Dn=o("."),xn=d(),C=s("p"),Kt=s("strong"),yn=o("Configuration"),Tn=o(": Some "),Yt=s("em"),In=o("DatasetBuilder"),Bn=o(`s expose multiple variants of the
dataset by defining a `),Qt=s("em"),Sn=o("datasets.BuilderConfig"),Nn=o(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),Zt=s("code"),jn=o("datasets.DatasetBuilder.builder_configs()"),Rn=d(),re=s("div"),p(xe.$$.fragment),Cn=d(),ea=s("p"),kn=o("Return a Dataset for the specified split."),On=d(),oe=s("div"),p(ye.$$.fragment),An=d(),ta=s("p"),Ln=o("Downloads and prepares dataset for reading."),Pn=d(),ie=s("div"),p(Te.$$.fragment),Vn=d(),aa=s("p"),Fn=o("Empty dict if doesn\u2019t exist"),Mn=d(),de=s("div"),p(Ie.$$.fragment),Un=d(),sa=s("p"),qn=o("Empty DatasetInfo if doesn\u2019t exist"),Hn=d(),le=s("div"),p(Be.$$.fragment),Gn=d(),na=s("p"),Wn=o("Return the path of the module of this class or subclass."),gs=d(),k=s("div"),p(Se.$$.fragment),Xn=d(),ra=s("p"),zn=o("Base class for datasets with data generation based on dict generators."),Jn=d(),V=s("p"),oa=s("code"),Kn=o("GeneratorBasedBuilder"),Yn=o(` is a convenience class that abstracts away much
of the data writing and reading of `),ia=s("code"),Qn=o("DatasetBuilder"),Zn=o(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),da=s("code"),er=o("_split_generators"),tr=o("). See the method docstrings for details."),hs=d(),W=s("div"),p(Ne.$$.fragment),ar=d(),la=s("p"),sr=o("Beam based Builder."),_s=d(),X=s("div"),p(je.$$.fragment),nr=d(),ca=s("p"),rr=o("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),vs=d(),S=s("div"),p(Re.$$.fragment),or=d(),Ce=s("p"),ir=o("Base class for "),bt=s("a"),dr=o("DatasetBuilder"),lr=o(" data configuration."),cr=d(),ke=s("p"),pr=o(`DatasetBuilder subclasses with data configuration options should subclass
`),$t=s("a"),fr=o("BuilderConfig"),mr=o(" and add their own properties."),ur=d(),F=s("div"),p(Oe.$$.fragment),gr=d(),pa=s("p"),hr=o(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),_r=d(),z=s("ul"),fa=s("li"),vr=o("the config kwargs that can be used to overwrite attributes"),br=d(),ma=s("li"),$r=o("the custom features used to write the dataset"),wr=d(),ua=s("li"),Er=o(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),bs=d(),E=s("div"),p(Ae.$$.fragment),Dr=d(),ce=s("div"),p(Le.$$.fragment),xr=d(),ga=s("p"),yr=o("Download given url(s)."),Tr=d(),N=s("div"),p(Pe.$$.fragment),Ir=d(),ha=s("p"),Br=o("Download and extract given url_or_urls."),Sr=d(),_a=s("p"),Nr=o("Is roughly equivalent to:"),jr=d(),p(Ve.$$.fragment),Rr=d(),pe=s("div"),p(Fe.$$.fragment),Cr=d(),Me=s("p"),kr=o("Download given urls(s) by calling "),va=s("code"),Or=o("custom_download"),Ar=o("."),Lr=d(),fe=s("div"),p(Ue.$$.fragment),Pr=d(),ba=s("p"),Vr=o("Extract given path(s)."),Fr=d(),me=s("div"),p(qe.$$.fragment),Mr=d(),$a=s("p"),Ur=o("Iterate over files within an archive."),qr=d(),ue=s("div"),p(He.$$.fragment),Hr=d(),wa=s("p"),Gr=o("Iterate over file paths."),Wr=d(),ge=s("div"),p(Ge.$$.fragment),Xr=d(),Ea=s("p"),zr=o("Ship the files using Beam FileSystems to the pipeline temp dir."),$s=d(),B=s("div"),p(We.$$.fragment),Jr=d(),wt=s("p"),Da=s("code"),Kr=o("Enum"),Yr=o(" for how to treat pre-existing downloads and data."),Qr=d(),Xe=s("p"),Zr=o("The default mode is "),xa=s("code"),eo=o("REUSE_DATASET_IF_EXISTS"),to=o(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),ao=d(),ya=s("p"),so=o("The generations modes:"),no=d(),ze=s("table"),Ta=s("thead"),J=s("tr"),ws=s("th"),ro=d(),Ia=s("th"),oo=o("Downloads"),io=d(),Ba=s("th"),lo=o("Dataset"),co=d(),K=s("tbody"),Y=s("tr"),Et=s("td"),Sa=s("code"),po=o("REUSE_DATASET_IF_EXISTS"),fo=o(" (default)"),mo=d(),Na=s("td"),uo=o("Reuse"),go=d(),ja=s("td"),ho=o("Reuse"),_o=d(),Q=s("tr"),Ra=s("td"),Ca=s("code"),vo=o("REUSE_CACHE_IF_EXISTS"),bo=d(),ka=s("td"),$o=o("Reuse"),wo=d(),Oa=s("td"),Eo=o("Fresh"),Do=d(),Z=s("tr"),Aa=s("td"),La=s("code"),xo=o("FORCE_REDOWNLOAD"),yo=d(),Pa=s("td"),To=o("Fresh"),Io=d(),Va=s("td"),Bo=o("Fresh"),Es=d(),O=s("div"),p(Je.$$.fragment),So=d(),Fa=s("p"),No=o("Defines the split information for the generator."),jo=d(),ee=s("p"),Ro=o(`This should be used as returned value of
`),Ma=s("code"),Co=o("GeneratorBasedBuilder._split_generators()"),ko=o(`
See `),Ua=s("code"),Oo=o("GeneratorBasedBuilder._split_generators()"),Ao=o(`for more info and example
of usage.`),Ds=d(),y=s("div"),p(Ke.$$.fragment),Lo=d(),Dt=s("p"),qa=s("code"),Po=o("Enum"),Vo=o(" for dataset splits."),Fo=d(),Ha=s("p"),Mo=o(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Uo=d(),A=s("ul"),xt=s("li"),Ga=s("code"),qo=o("TRAIN"),Ho=o(": the training data."),Go=d(),yt=s("li"),Wa=s("code"),Wo=o("VALIDATION"),Xo=o(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),zo=d(),Tt=s("li"),Xa=s("code"),Jo=o("TEST"),Ko=o(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Yo=d(),It=s("li"),za=s("code"),Qo=o("ALL"),Zo=o(": the union of all defined dataset splits."),ei=d(),Bt=s("p"),ti=o("Note: All splits, including compositions inherit from "),Ja=s("code"),ai=o("datasets.SplitBase"),si=d(),Ye=s("p"),ni=o("See the :doc:"),Ka=s("code"),ri=o("guide on splits </loading>"),oi=o(" for more information."),xs=d(),w=s("div"),p(Qe.$$.fragment),ii=d(),Ya=s("p"),di=o("Descriptor corresponding to a named split (train, test, \u2026)."),li=d(),Qa=s("p"),ci=o("Example:"),pi=d(),p(Ze.$$.fragment),fi=d(),Za=s("p"),mi=o(`Warning:
A split cannot be added twice, so the following will fail:`),ui=d(),p(et.$$.fragment),gi=d(),es=s("p"),hi=o(`Warning:
The slices can be applied only one time. So the following are valid:`),_i=d(),p(tt.$$.fragment),vi=d(),ts=s("p"),bi=o("But not:"),$i=d(),p(at.$$.fragment),ys=d(),te=s("div"),p(st.$$.fragment),wi=d(),as=s("p"),Ei=o("Split corresponding to the union of all defined dataset splits."),Ts=d(),T=s("div"),p(nt.$$.fragment),Di=d(),ss=s("p"),xi=o("Reading instruction for a dataset."),yi=d(),ns=s("p"),Ti=o("Examples:"),Ii=d(),p(rt.$$.fragment),Bi=d(),he=s("div"),p(ot.$$.fragment),Si=d(),rs=s("p"),Ni=o("Creates a ReadInstruction instance out of a string spec."),ji=d(),M=s("div"),p(it.$$.fragment),Ri=d(),os=s("p"),Ci=o("Translate instruction into a list of absolute instructions."),ki=d(),is=s("p"),Oi=o("Those absolute instructions are then to be added together."),Is=d(),ae=s("div"),p(dt.$$.fragment),Ai=d(),ds=s("p"),Li=o("Configuration for our cached path manager."),Bs=d(),L=s("div"),p(lt.$$.fragment),Pi=d(),ls=s("p"),Vi=o("Dataset version MAJOR.MINOR.PATCH."),Fi=d(),_e=s("div"),p(ct.$$.fragment),Mi=d(),cs=s("p"),Ui=o("Returns True if other_version matches."),this.h()},l(a){const _=Rl('[data-svelte="svelte-1phssyn"]',document.head);q=n(_,"META",{name:!0,content:!0}),_.forEach(t),fs=l(a),H=n(a,"H1",{class:!0});var Ns=r(H);se=n(Ns,"A",{id:!0,class:!0,href:!0});var td=r(se);Ht=n(td,"SPAN",{});var ad=r(Ht);f(Ee.$$.fragment,ad),ad.forEach(t),td.forEach(t),en=l(Ns),Gt=n(Ns,"SPAN",{});var sd=r(Gt);tn=i(sd,"Builder classes"),sd.forEach(t),Ns.forEach(t),ms=l(a),P=n(a,"P",{});var St=r(P);an=i(St,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),ft=n(St,"A",{href:!0});var nd=r(ft);sn=i(nd,"DatasetBuilder"),nd.forEach(t),nn=i(St," and "),mt=n(St,"A",{href:!0});var rd=r(mt);rn=i(rd,"BuilderConfig"),rd.forEach(t),on=i(St,"."),St.forEach(t),us=l(a),$=n(a,"DIV",{class:!0});var D=r($);f(De.$$.fragment,D),dn=l(D),Wt=n(D,"P",{});var od=r(Wt);ln=i(od,"Abstract base class for all datasets."),od.forEach(t),cn=l(D),ut=n(D,"P",{});var qi=r(ut);Xt=n(qi,"EM",{});var id=r(Xt);pn=i(id,"DatasetBuilder"),id.forEach(t),fn=i(qi," has 3 key methods:"),qi.forEach(t),mn=l(D),G=n(D,"UL",{});var Nt=r(G);gt=n(Nt,"LI",{});var Hi=r(gt);zt=n(Hi,"CODE",{});var dd=r(zt);un=i(dd,"datasets.DatasetBuilder.info"),dd.forEach(t),gn=i(Hi,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),Hi.forEach(t),hn=l(Nt),ht=n(Nt,"LI",{});var Gi=r(ht);_t=n(Gi,"A",{href:!0});var ld=r(_t);_n=i(ld,"datasets.DatasetBuilder.download_and_prepare()"),ld.forEach(t),vn=i(Gi,`: Downloads the source data
and writes it to disk.`),Gi.forEach(t),bn=l(Nt),ne=n(Nt,"LI",{});var ps=r(ne);vt=n(ps,"A",{href:!0});var cd=r(vt);$n=i(cd,"datasets.DatasetBuilder.as_dataset()"),cd.forEach(t),wn=i(ps,": Generates a "),Jt=n(ps,"EM",{});var pd=r(Jt);En=i(pd,"Dataset"),pd.forEach(t),Dn=i(ps,"."),ps.forEach(t),Nt.forEach(t),xn=l(D),C=n(D,"P",{});var ve=r(C);Kt=n(ve,"STRONG",{});var fd=r(Kt);yn=i(fd,"Configuration"),fd.forEach(t),Tn=i(ve,": Some "),Yt=n(ve,"EM",{});var md=r(Yt);In=i(md,"DatasetBuilder"),md.forEach(t),Bn=i(ve,`s expose multiple variants of the
dataset by defining a `),Qt=n(ve,"EM",{});var ud=r(Qt);Sn=i(ud,"datasets.BuilderConfig"),ud.forEach(t),Nn=i(ve,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),Zt=n(ve,"CODE",{});var gd=r(Zt);jn=i(gd,"datasets.DatasetBuilder.builder_configs()"),gd.forEach(t),ve.forEach(t),Rn=l(D),re=n(D,"DIV",{class:!0});var js=r(re);f(xe.$$.fragment,js),Cn=l(js),ea=n(js,"P",{});var hd=r(ea);kn=i(hd,"Return a Dataset for the specified split."),hd.forEach(t),js.forEach(t),On=l(D),oe=n(D,"DIV",{class:!0});var Rs=r(oe);f(ye.$$.fragment,Rs),An=l(Rs),ta=n(Rs,"P",{});var _d=r(ta);Ln=i(_d,"Downloads and prepares dataset for reading."),_d.forEach(t),Rs.forEach(t),Pn=l(D),ie=n(D,"DIV",{class:!0});var Cs=r(ie);f(Te.$$.fragment,Cs),Vn=l(Cs),aa=n(Cs,"P",{});var vd=r(aa);Fn=i(vd,"Empty dict if doesn\u2019t exist"),vd.forEach(t),Cs.forEach(t),Mn=l(D),de=n(D,"DIV",{class:!0});var ks=r(de);f(Ie.$$.fragment,ks),Un=l(ks),sa=n(ks,"P",{});var bd=r(sa);qn=i(bd,"Empty DatasetInfo if doesn\u2019t exist"),bd.forEach(t),ks.forEach(t),Hn=l(D),le=n(D,"DIV",{class:!0});var Os=r(le);f(Be.$$.fragment,Os),Gn=l(Os),na=n(Os,"P",{});var $d=r(na);Wn=i($d,"Return the path of the module of this class or subclass."),$d.forEach(t),Os.forEach(t),D.forEach(t),gs=l(a),k=n(a,"DIV",{class:!0});var jt=r(k);f(Se.$$.fragment,jt),Xn=l(jt),ra=n(jt,"P",{});var wd=r(ra);zn=i(wd,"Base class for datasets with data generation based on dict generators."),wd.forEach(t),Jn=l(jt),V=n(jt,"P",{});var pt=r(V);oa=n(pt,"CODE",{});var Ed=r(oa);Kn=i(Ed,"GeneratorBasedBuilder"),Ed.forEach(t),Yn=i(pt,` is a convenience class that abstracts away much
of the data writing and reading of `),ia=n(pt,"CODE",{});var Dd=r(ia);Qn=i(Dd,"DatasetBuilder"),Dd.forEach(t),Zn=i(pt,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),da=n(pt,"CODE",{});var xd=r(da);er=i(xd,"_split_generators"),xd.forEach(t),tr=i(pt,"). See the method docstrings for details."),pt.forEach(t),jt.forEach(t),hs=l(a),W=n(a,"DIV",{class:!0});var As=r(W);f(Ne.$$.fragment,As),ar=l(As),la=n(As,"P",{});var yd=r(la);sr=i(yd,"Beam based Builder."),yd.forEach(t),As.forEach(t),_s=l(a),X=n(a,"DIV",{class:!0});var Ls=r(X);f(je.$$.fragment,Ls),nr=l(Ls),ca=n(Ls,"P",{});var Td=r(ca);rr=i(Td,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Td.forEach(t),Ls.forEach(t),vs=l(a),S=n(a,"DIV",{class:!0});var be=r(S);f(Re.$$.fragment,be),or=l(be),Ce=n(be,"P",{});var Ps=r(Ce);ir=i(Ps,"Base class for "),bt=n(Ps,"A",{href:!0});var Id=r(bt);dr=i(Id,"DatasetBuilder"),Id.forEach(t),lr=i(Ps," data configuration."),Ps.forEach(t),cr=l(be),ke=n(be,"P",{});var Vs=r(ke);pr=i(Vs,`DatasetBuilder subclasses with data configuration options should subclass
`),$t=n(Vs,"A",{href:!0});var Bd=r($t);fr=i(Bd,"BuilderConfig"),Bd.forEach(t),mr=i(Vs," and add their own properties."),Vs.forEach(t),ur=l(be),F=n(be,"DIV",{class:!0});var Rt=r(F);f(Oe.$$.fragment,Rt),gr=l(Rt),pa=n(Rt,"P",{});var Sd=r(pa);hr=i(Sd,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Sd.forEach(t),_r=l(Rt),z=n(Rt,"UL",{});var Ct=r(z);fa=n(Ct,"LI",{});var Nd=r(fa);vr=i(Nd,"the config kwargs that can be used to overwrite attributes"),Nd.forEach(t),br=l(Ct),ma=n(Ct,"LI",{});var jd=r(ma);$r=i(jd,"the custom features used to write the dataset"),jd.forEach(t),wr=l(Ct),ua=n(Ct,"LI",{});var Rd=r(ua);Er=i(Rd,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Rd.forEach(t),Ct.forEach(t),Rt.forEach(t),be.forEach(t),bs=l(a),E=n(a,"DIV",{class:!0});var I=r(E);f(Ae.$$.fragment,I),Dr=l(I),ce=n(I,"DIV",{class:!0});var Fs=r(ce);f(Le.$$.fragment,Fs),xr=l(Fs),ga=n(Fs,"P",{});var Cd=r(ga);yr=i(Cd,"Download given url(s)."),Cd.forEach(t),Fs.forEach(t),Tr=l(I),N=n(I,"DIV",{class:!0});var $e=r(N);f(Pe.$$.fragment,$e),Ir=l($e),ha=n($e,"P",{});var kd=r(ha);Br=i(kd,"Download and extract given url_or_urls."),kd.forEach(t),Sr=l($e),_a=n($e,"P",{});var Od=r(_a);Nr=i(Od,"Is roughly equivalent to:"),Od.forEach(t),jr=l($e),f(Ve.$$.fragment,$e),$e.forEach(t),Rr=l(I),pe=n(I,"DIV",{class:!0});var Ms=r(pe);f(Fe.$$.fragment,Ms),Cr=l(Ms),Me=n(Ms,"P",{});var Us=r(Me);kr=i(Us,"Download given urls(s) by calling "),va=n(Us,"CODE",{});var Ad=r(va);Or=i(Ad,"custom_download"),Ad.forEach(t),Ar=i(Us,"."),Us.forEach(t),Ms.forEach(t),Lr=l(I),fe=n(I,"DIV",{class:!0});var qs=r(fe);f(Ue.$$.fragment,qs),Pr=l(qs),ba=n(qs,"P",{});var Ld=r(ba);Vr=i(Ld,"Extract given path(s)."),Ld.forEach(t),qs.forEach(t),Fr=l(I),me=n(I,"DIV",{class:!0});var Hs=r(me);f(qe.$$.fragment,Hs),Mr=l(Hs),$a=n(Hs,"P",{});var Pd=r($a);Ur=i(Pd,"Iterate over files within an archive."),Pd.forEach(t),Hs.forEach(t),qr=l(I),ue=n(I,"DIV",{class:!0});var Gs=r(ue);f(He.$$.fragment,Gs),Hr=l(Gs),wa=n(Gs,"P",{});var Vd=r(wa);Gr=i(Vd,"Iterate over file paths."),Vd.forEach(t),Gs.forEach(t),Wr=l(I),ge=n(I,"DIV",{class:!0});var Ws=r(ge);f(Ge.$$.fragment,Ws),Xr=l(Ws),Ea=n(Ws,"P",{});var Fd=r(Ea);zr=i(Fd,"Ship the files using Beam FileSystems to the pipeline temp dir."),Fd.forEach(t),Ws.forEach(t),I.forEach(t),$s=l(a),B=n(a,"DIV",{class:!0});var U=r(B);f(We.$$.fragment,U),Jr=l(U),wt=n(U,"P",{});var Wi=r(wt);Da=n(Wi,"CODE",{});var Md=r(Da);Kr=i(Md,"Enum"),Md.forEach(t),Yr=i(Wi," for how to treat pre-existing downloads and data."),Wi.forEach(t),Qr=l(U),Xe=n(U,"P",{});var Xs=r(Xe);Zr=i(Xs,"The default mode is "),xa=n(Xs,"CODE",{});var Ud=r(xa);eo=i(Ud,"REUSE_DATASET_IF_EXISTS"),Ud.forEach(t),to=i(Xs,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Xs.forEach(t),ao=l(U),ya=n(U,"P",{});var qd=r(ya);so=i(qd,"The generations modes:"),qd.forEach(t),no=l(U),ze=n(U,"TABLE",{});var zs=r(ze);Ta=n(zs,"THEAD",{});var Hd=r(Ta);J=n(Hd,"TR",{});var kt=r(J);ws=n(kt,"TH",{}),r(ws).forEach(t),ro=l(kt),Ia=n(kt,"TH",{});var Gd=r(Ia);oo=i(Gd,"Downloads"),Gd.forEach(t),io=l(kt),Ba=n(kt,"TH",{});var Wd=r(Ba);lo=i(Wd,"Dataset"),Wd.forEach(t),kt.forEach(t),Hd.forEach(t),co=l(zs),K=n(zs,"TBODY",{});var Ot=r(K);Y=n(Ot,"TR",{});var At=r(Y);Et=n(At,"TD",{});var Xi=r(Et);Sa=n(Xi,"CODE",{});var Xd=r(Sa);po=i(Xd,"REUSE_DATASET_IF_EXISTS"),Xd.forEach(t),fo=i(Xi," (default)"),Xi.forEach(t),mo=l(At),Na=n(At,"TD",{});var zd=r(Na);uo=i(zd,"Reuse"),zd.forEach(t),go=l(At),ja=n(At,"TD",{});var Jd=r(ja);ho=i(Jd,"Reuse"),Jd.forEach(t),At.forEach(t),_o=l(Ot),Q=n(Ot,"TR",{});var Lt=r(Q);Ra=n(Lt,"TD",{});var Kd=r(Ra);Ca=n(Kd,"CODE",{});var Yd=r(Ca);vo=i(Yd,"REUSE_CACHE_IF_EXISTS"),Yd.forEach(t),Kd.forEach(t),bo=l(Lt),ka=n(Lt,"TD",{});var Qd=r(ka);$o=i(Qd,"Reuse"),Qd.forEach(t),wo=l(Lt),Oa=n(Lt,"TD",{});var Zd=r(Oa);Eo=i(Zd,"Fresh"),Zd.forEach(t),Lt.forEach(t),Do=l(Ot),Z=n(Ot,"TR",{});var Pt=r(Z);Aa=n(Pt,"TD",{});var el=r(Aa);La=n(el,"CODE",{});var tl=r(La);xo=i(tl,"FORCE_REDOWNLOAD"),tl.forEach(t),el.forEach(t),yo=l(Pt),Pa=n(Pt,"TD",{});var al=r(Pa);To=i(al,"Fresh"),al.forEach(t),Io=l(Pt),Va=n(Pt,"TD",{});var sl=r(Va);Bo=i(sl,"Fresh"),sl.forEach(t),Pt.forEach(t),Ot.forEach(t),zs.forEach(t),U.forEach(t),Es=l(a),O=n(a,"DIV",{class:!0});var Vt=r(O);f(Je.$$.fragment,Vt),So=l(Vt),Fa=n(Vt,"P",{});var nl=r(Fa);No=i(nl,"Defines the split information for the generator."),nl.forEach(t),jo=l(Vt),ee=n(Vt,"P",{});var Ft=r(ee);Ro=i(Ft,`This should be used as returned value of
`),Ma=n(Ft,"CODE",{});var rl=r(Ma);Co=i(rl,"GeneratorBasedBuilder._split_generators()"),rl.forEach(t),ko=i(Ft,`
See `),Ua=n(Ft,"CODE",{});var ol=r(Ua);Oo=i(ol,"GeneratorBasedBuilder._split_generators()"),ol.forEach(t),Ao=i(Ft,`for more info and example
of usage.`),Ft.forEach(t),Vt.forEach(t),Ds=l(a),y=n(a,"DIV",{class:!0});var j=r(y);f(Ke.$$.fragment,j),Lo=l(j),Dt=n(j,"P",{});var zi=r(Dt);qa=n(zi,"CODE",{});var il=r(qa);Po=i(il,"Enum"),il.forEach(t),Vo=i(zi," for dataset splits."),zi.forEach(t),Fo=l(j),Ha=n(j,"P",{});var dl=r(Ha);Mo=i(dl,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),dl.forEach(t),Uo=l(j),A=n(j,"UL",{});var we=r(A);xt=n(we,"LI",{});var Ji=r(xt);Ga=n(Ji,"CODE",{});var ll=r(Ga);qo=i(ll,"TRAIN"),ll.forEach(t),Ho=i(Ji,": the training data."),Ji.forEach(t),Go=l(we),yt=n(we,"LI",{});var Ki=r(yt);Wa=n(Ki,"CODE",{});var cl=r(Wa);Wo=i(cl,"VALIDATION"),cl.forEach(t),Xo=i(Ki,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Ki.forEach(t),zo=l(we),Tt=n(we,"LI",{});var Yi=r(Tt);Xa=n(Yi,"CODE",{});var pl=r(Xa);Jo=i(pl,"TEST"),pl.forEach(t),Ko=i(Yi,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Yi.forEach(t),Yo=l(we),It=n(we,"LI",{});var Qi=r(It);za=n(Qi,"CODE",{});var fl=r(za);Qo=i(fl,"ALL"),fl.forEach(t),Zo=i(Qi,": the union of all defined dataset splits."),Qi.forEach(t),we.forEach(t),ei=l(j),Bt=n(j,"P",{});var Zi=r(Bt);ti=i(Zi,"Note: All splits, including compositions inherit from "),Ja=n(Zi,"CODE",{});var ml=r(Ja);ai=i(ml,"datasets.SplitBase"),ml.forEach(t),Zi.forEach(t),si=l(j),Ye=n(j,"P",{});var Js=r(Ye);ni=i(Js,"See the :doc:"),Ka=n(Js,"CODE",{});var ul=r(Ka);ri=i(ul,"guide on splits </loading>"),ul.forEach(t),oi=i(Js," for more information."),Js.forEach(t),j.forEach(t),xs=l(a),w=n(a,"DIV",{class:!0});var x=r(w);f(Qe.$$.fragment,x),ii=l(x),Ya=n(x,"P",{});var gl=r(Ya);di=i(gl,"Descriptor corresponding to a named split (train, test, \u2026)."),gl.forEach(t),li=l(x),Qa=n(x,"P",{});var hl=r(Qa);ci=i(hl,"Example:"),hl.forEach(t),pi=l(x),f(Ze.$$.fragment,x),fi=l(x),Za=n(x,"P",{});var _l=r(Za);mi=i(_l,`Warning:
A split cannot be added twice, so the following will fail:`),_l.forEach(t),ui=l(x),f(et.$$.fragment,x),gi=l(x),es=n(x,"P",{});var vl=r(es);hi=i(vl,`Warning:
The slices can be applied only one time. So the following are valid:`),vl.forEach(t),_i=l(x),f(tt.$$.fragment,x),vi=l(x),ts=n(x,"P",{});var bl=r(ts);bi=i(bl,"But not:"),bl.forEach(t),$i=l(x),f(at.$$.fragment,x),x.forEach(t),ys=l(a),te=n(a,"DIV",{class:!0});var Ks=r(te);f(st.$$.fragment,Ks),wi=l(Ks),as=n(Ks,"P",{});var $l=r(as);Ei=i($l,"Split corresponding to the union of all defined dataset splits."),$l.forEach(t),Ks.forEach(t),Ts=l(a),T=n(a,"DIV",{class:!0});var R=r(T);f(nt.$$.fragment,R),Di=l(R),ss=n(R,"P",{});var wl=r(ss);xi=i(wl,"Reading instruction for a dataset."),wl.forEach(t),yi=l(R),ns=n(R,"P",{});var El=r(ns);Ti=i(El,"Examples:"),El.forEach(t),Ii=l(R),f(rt.$$.fragment,R),Bi=l(R),he=n(R,"DIV",{class:!0});var Ys=r(he);f(ot.$$.fragment,Ys),Si=l(Ys),rs=n(Ys,"P",{});var Dl=r(rs);Ni=i(Dl,"Creates a ReadInstruction instance out of a string spec."),Dl.forEach(t),Ys.forEach(t),ji=l(R),M=n(R,"DIV",{class:!0});var Mt=r(M);f(it.$$.fragment,Mt),Ri=l(Mt),os=n(Mt,"P",{});var xl=r(os);Ci=i(xl,"Translate instruction into a list of absolute instructions."),xl.forEach(t),ki=l(Mt),is=n(Mt,"P",{});var yl=r(is);Oi=i(yl,"Those absolute instructions are then to be added together."),yl.forEach(t),Mt.forEach(t),R.forEach(t),Is=l(a),ae=n(a,"DIV",{class:!0});var Qs=r(ae);f(dt.$$.fragment,Qs),Ai=l(Qs),ds=n(Qs,"P",{});var Tl=r(ds);Li=i(Tl,"Configuration for our cached path manager."),Tl.forEach(t),Qs.forEach(t),Bs=l(a),L=n(a,"DIV",{class:!0});var Ut=r(L);f(lt.$$.fragment,Ut),Pi=l(Ut),ls=n(Ut,"P",{});var Il=r(ls);Vi=i(Il,"Dataset version MAJOR.MINOR.PATCH."),Il.forEach(t),Fi=l(Ut),_e=n(Ut,"DIV",{class:!0});var Zs=r(_e);f(ct.$$.fragment,Zs),Mi=l(Zs),cs=n(Zs,"P",{});var Bl=r(cs);Ui=i(Bl,"Returns True if other_version matches."),Bl.forEach(t),Zs.forEach(t),Ut.forEach(t),this.h()},h(){c(q,"name","hf:doc:metadata"),c(q,"content",JSON.stringify(Ll)),c(se,"id","datasets.DatasetBuilder"),c(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(se,"href","#datasets.DatasetBuilder"),c(H,"class","relative group"),c(ft,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder"),c(mt,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.BuilderConfig"),c(_t,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),c(vt,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),c(re,"class","docstring"),c(oe,"class","docstring"),c(ie,"class","docstring"),c(de,"class","docstring"),c(le,"class","docstring"),c($,"class","docstring"),c(k,"class","docstring"),c(W,"class","docstring"),c(X,"class","docstring"),c(bt,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder"),c($t,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.BuilderConfig"),c(F,"class","docstring"),c(S,"class","docstring"),c(ce,"class","docstring"),c(N,"class","docstring"),c(pe,"class","docstring"),c(fe,"class","docstring"),c(me,"class","docstring"),c(ue,"class","docstring"),c(ge,"class","docstring"),c(E,"class","docstring"),c(B,"class","docstring"),c(O,"class","docstring"),c(y,"class","docstring"),c(w,"class","docstring"),c(te,"class","docstring"),c(he,"class","docstring"),c(M,"class","docstring"),c(T,"class","docstring"),c(ae,"class","docstring"),c(_e,"class","docstring"),c(L,"class","docstring")},m(a,_){e(document.head,q),v(a,fs,_),v(a,H,_),e(H,se),e(se,Ht),m(Ee,Ht,null),e(H,en),e(H,Gt),e(Gt,tn),v(a,ms,_),v(a,P,_),e(P,an),e(P,ft),e(ft,sn),e(P,nn),e(P,mt),e(mt,rn),e(P,on),v(a,us,_),v(a,$,_),m(De,$,null),e($,dn),e($,Wt),e(Wt,ln),e($,cn),e($,ut),e(ut,Xt),e(Xt,pn),e(ut,fn),e($,mn),e($,G),e(G,gt),e(gt,zt),e(zt,un),e(gt,gn),e(G,hn),e(G,ht),e(ht,_t),e(_t,_n),e(ht,vn),e(G,bn),e(G,ne),e(ne,vt),e(vt,$n),e(ne,wn),e(ne,Jt),e(Jt,En),e(ne,Dn),e($,xn),e($,C),e(C,Kt),e(Kt,yn),e(C,Tn),e(C,Yt),e(Yt,In),e(C,Bn),e(C,Qt),e(Qt,Sn),e(C,Nn),e(C,Zt),e(Zt,jn),e($,Rn),e($,re),m(xe,re,null),e(re,Cn),e(re,ea),e(ea,kn),e($,On),e($,oe),m(ye,oe,null),e(oe,An),e(oe,ta),e(ta,Ln),e($,Pn),e($,ie),m(Te,ie,null),e(ie,Vn),e(ie,aa),e(aa,Fn),e($,Mn),e($,de),m(Ie,de,null),e(de,Un),e(de,sa),e(sa,qn),e($,Hn),e($,le),m(Be,le,null),e(le,Gn),e(le,na),e(na,Wn),v(a,gs,_),v(a,k,_),m(Se,k,null),e(k,Xn),e(k,ra),e(ra,zn),e(k,Jn),e(k,V),e(V,oa),e(oa,Kn),e(V,Yn),e(V,ia),e(ia,Qn),e(V,Zn),e(V,da),e(da,er),e(V,tr),v(a,hs,_),v(a,W,_),m(Ne,W,null),e(W,ar),e(W,la),e(la,sr),v(a,_s,_),v(a,X,_),m(je,X,null),e(X,nr),e(X,ca),e(ca,rr),v(a,vs,_),v(a,S,_),m(Re,S,null),e(S,or),e(S,Ce),e(Ce,ir),e(Ce,bt),e(bt,dr),e(Ce,lr),e(S,cr),e(S,ke),e(ke,pr),e(ke,$t),e($t,fr),e(ke,mr),e(S,ur),e(S,F),m(Oe,F,null),e(F,gr),e(F,pa),e(pa,hr),e(F,_r),e(F,z),e(z,fa),e(fa,vr),e(z,br),e(z,ma),e(ma,$r),e(z,wr),e(z,ua),e(ua,Er),v(a,bs,_),v(a,E,_),m(Ae,E,null),e(E,Dr),e(E,ce),m(Le,ce,null),e(ce,xr),e(ce,ga),e(ga,yr),e(E,Tr),e(E,N),m(Pe,N,null),e(N,Ir),e(N,ha),e(ha,Br),e(N,Sr),e(N,_a),e(_a,Nr),e(N,jr),m(Ve,N,null),e(E,Rr),e(E,pe),m(Fe,pe,null),e(pe,Cr),e(pe,Me),e(Me,kr),e(Me,va),e(va,Or),e(Me,Ar),e(E,Lr),e(E,fe),m(Ue,fe,null),e(fe,Pr),e(fe,ba),e(ba,Vr),e(E,Fr),e(E,me),m(qe,me,null),e(me,Mr),e(me,$a),e($a,Ur),e(E,qr),e(E,ue),m(He,ue,null),e(ue,Hr),e(ue,wa),e(wa,Gr),e(E,Wr),e(E,ge),m(Ge,ge,null),e(ge,Xr),e(ge,Ea),e(Ea,zr),v(a,$s,_),v(a,B,_),m(We,B,null),e(B,Jr),e(B,wt),e(wt,Da),e(Da,Kr),e(wt,Yr),e(B,Qr),e(B,Xe),e(Xe,Zr),e(Xe,xa),e(xa,eo),e(Xe,to),e(B,ao),e(B,ya),e(ya,so),e(B,no),e(B,ze),e(ze,Ta),e(Ta,J),e(J,ws),e(J,ro),e(J,Ia),e(Ia,oo),e(J,io),e(J,Ba),e(Ba,lo),e(ze,co),e(ze,K),e(K,Y),e(Y,Et),e(Et,Sa),e(Sa,po),e(Et,fo),e(Y,mo),e(Y,Na),e(Na,uo),e(Y,go),e(Y,ja),e(ja,ho),e(K,_o),e(K,Q),e(Q,Ra),e(Ra,Ca),e(Ca,vo),e(Q,bo),e(Q,ka),e(ka,$o),e(Q,wo),e(Q,Oa),e(Oa,Eo),e(K,Do),e(K,Z),e(Z,Aa),e(Aa,La),e(La,xo),e(Z,yo),e(Z,Pa),e(Pa,To),e(Z,Io),e(Z,Va),e(Va,Bo),v(a,Es,_),v(a,O,_),m(Je,O,null),e(O,So),e(O,Fa),e(Fa,No),e(O,jo),e(O,ee),e(ee,Ro),e(ee,Ma),e(Ma,Co),e(ee,ko),e(ee,Ua),e(Ua,Oo),e(ee,Ao),v(a,Ds,_),v(a,y,_),m(Ke,y,null),e(y,Lo),e(y,Dt),e(Dt,qa),e(qa,Po),e(Dt,Vo),e(y,Fo),e(y,Ha),e(Ha,Mo),e(y,Uo),e(y,A),e(A,xt),e(xt,Ga),e(Ga,qo),e(xt,Ho),e(A,Go),e(A,yt),e(yt,Wa),e(Wa,Wo),e(yt,Xo),e(A,zo),e(A,Tt),e(Tt,Xa),e(Xa,Jo),e(Tt,Ko),e(A,Yo),e(A,It),e(It,za),e(za,Qo),e(It,Zo),e(y,ei),e(y,Bt),e(Bt,ti),e(Bt,Ja),e(Ja,ai),e(y,si),e(y,Ye),e(Ye,ni),e(Ye,Ka),e(Ka,ri),e(Ye,oi),v(a,xs,_),v(a,w,_),m(Qe,w,null),e(w,ii),e(w,Ya),e(Ya,di),e(w,li),e(w,Qa),e(Qa,ci),e(w,pi),m(Ze,w,null),e(w,fi),e(w,Za),e(Za,mi),e(w,ui),m(et,w,null),e(w,gi),e(w,es),e(es,hi),e(w,_i),m(tt,w,null),e(w,vi),e(w,ts),e(ts,bi),e(w,$i),m(at,w,null),v(a,ys,_),v(a,te,_),m(st,te,null),e(te,wi),e(te,as),e(as,Ei),v(a,Ts,_),v(a,T,_),m(nt,T,null),e(T,Di),e(T,ss),e(ss,xi),e(T,yi),e(T,ns),e(ns,Ti),e(T,Ii),m(rt,T,null),e(T,Bi),e(T,he),m(ot,he,null),e(he,Si),e(he,rs),e(rs,Ni),e(T,ji),e(T,M),m(it,M,null),e(M,Ri),e(M,os),e(os,Ci),e(M,ki),e(M,is),e(is,Oi),v(a,Is,_),v(a,ae,_),m(dt,ae,null),e(ae,Ai),e(ae,ds),e(ds,Li),v(a,Bs,_),v(a,L,_),m(lt,L,null),e(L,Pi),e(L,ls),e(ls,Vi),e(L,Fi),e(L,_e),m(ct,_e,null),e(_e,Mi),e(_e,cs),e(cs,Ui),Ss=!0},p:Cl,i(a){Ss||(u(Ee.$$.fragment,a),u(De.$$.fragment,a),u(xe.$$.fragment,a),u(ye.$$.fragment,a),u(Te.$$.fragment,a),u(Ie.$$.fragment,a),u(Be.$$.fragment,a),u(Se.$$.fragment,a),u(Ne.$$.fragment,a),u(je.$$.fragment,a),u(Re.$$.fragment,a),u(Oe.$$.fragment,a),u(Ae.$$.fragment,a),u(Le.$$.fragment,a),u(Pe.$$.fragment,a),u(Ve.$$.fragment,a),u(Fe.$$.fragment,a),u(Ue.$$.fragment,a),u(qe.$$.fragment,a),u(He.$$.fragment,a),u(Ge.$$.fragment,a),u(We.$$.fragment,a),u(Je.$$.fragment,a),u(Ke.$$.fragment,a),u(Qe.$$.fragment,a),u(Ze.$$.fragment,a),u(et.$$.fragment,a),u(tt.$$.fragment,a),u(at.$$.fragment,a),u(st.$$.fragment,a),u(nt.$$.fragment,a),u(rt.$$.fragment,a),u(ot.$$.fragment,a),u(it.$$.fragment,a),u(dt.$$.fragment,a),u(lt.$$.fragment,a),u(ct.$$.fragment,a),Ss=!0)},o(a){g(Ee.$$.fragment,a),g(De.$$.fragment,a),g(xe.$$.fragment,a),g(ye.$$.fragment,a),g(Te.$$.fragment,a),g(Ie.$$.fragment,a),g(Be.$$.fragment,a),g(Se.$$.fragment,a),g(Ne.$$.fragment,a),g(je.$$.fragment,a),g(Re.$$.fragment,a),g(Oe.$$.fragment,a),g(Ae.$$.fragment,a),g(Le.$$.fragment,a),g(Pe.$$.fragment,a),g(Ve.$$.fragment,a),g(Fe.$$.fragment,a),g(Ue.$$.fragment,a),g(qe.$$.fragment,a),g(He.$$.fragment,a),g(Ge.$$.fragment,a),g(We.$$.fragment,a),g(Je.$$.fragment,a),g(Ke.$$.fragment,a),g(Qe.$$.fragment,a),g(Ze.$$.fragment,a),g(et.$$.fragment,a),g(tt.$$.fragment,a),g(at.$$.fragment,a),g(st.$$.fragment,a),g(nt.$$.fragment,a),g(rt.$$.fragment,a),g(ot.$$.fragment,a),g(it.$$.fragment,a),g(dt.$$.fragment,a),g(lt.$$.fragment,a),g(ct.$$.fragment,a),Ss=!1},d(a){t(q),a&&t(fs),a&&t(H),h(Ee),a&&t(ms),a&&t(P),a&&t(us),a&&t($),h(De),h(xe),h(ye),h(Te),h(Ie),h(Be),a&&t(gs),a&&t(k),h(Se),a&&t(hs),a&&t(W),h(Ne),a&&t(_s),a&&t(X),h(je),a&&t(vs),a&&t(S),h(Re),h(Oe),a&&t(bs),a&&t(E),h(Ae),h(Le),h(Pe),h(Ve),h(Fe),h(Ue),h(qe),h(He),h(Ge),a&&t($s),a&&t(B),h(We),a&&t(Es),a&&t(O),h(Je),a&&t(Ds),a&&t(y),h(Ke),a&&t(xs),a&&t(w),h(Qe),h(Ze),h(et),h(tt),h(at),a&&t(ys),a&&t(te),h(st),a&&t(Ts),a&&t(T),h(nt),h(rt),h(ot),h(it),a&&t(Is),a&&t(ae),h(dt),a&&t(Bs),a&&t(L),h(lt),h(ct)}}}const Ll={local:"datasets.DatasetBuilder",title:"Builder classes"};function Pl(ed){return kl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ql extends Sl{constructor(q){super();Nl(this,q,Pl,Al,jl,{})}}export{ql as default,Ll as metadata};
