import{S as ph,i as dh,s as fh,e as l,k as d,w as u,t as i,M as ch,c as o,d as t,m as f,a as n,x as m,h as p,b as c,F as a,g as r,y as g,q as _,o as v,B as $}from"../chunks/vendor-e67aec41.js";import{T as ce}from"../chunks/Tip-76459d1c.js";import{I as P}from"../chunks/IconCopyLink-ffd7f84e.js";import{C as k}from"../chunks/CodeBlock-ccf09204.js";import{C as Dl}from"../chunks/CodeBlockFw-a711fc3f.js";function hh(C){let h,x,y,b,E;return{c(){h=l("p"),x=i("Refer to the "),y=l("a"),b=i("upload_dataset_repo"),E=i(" guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),this.h()},l(w){h=o(w,"P",{});var j=n(h);x=p(j,"Refer to the "),y=o(j,"A",{href:!0});var q=n(y);b=p(q,"upload_dataset_repo"),q.forEach(t),E=p(j," guide for more instructions on how to create a dataset repository on the Hub, and how to upload your data files."),j.forEach(t),this.h()},h(){c(y,"href","#upload_dataset_repo")},m(w,j){r(w,h,j),a(h,x),a(h,y),a(y,b),a(h,E)},d(w){w&&t(h)}}}function uh(C){let h,x,y,b,E;return{c(){h=l("p"),x=i("If you don\u2019t specify which data files to use, "),y=l("code"),b=i("load_dataset"),E=i(" will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data.")},l(w){h=o(w,"P",{});var j=n(h);x=p(j,"If you don\u2019t specify which data files to use, "),y=o(j,"CODE",{});var q=n(y);b=p(q,"load_dataset"),q.forEach(t),E=p(j," will return all the data files. This can take a long time if you are loading a large dataset like C4, which is approximately 13TB of data."),j.forEach(t)},m(w,j){r(w,h,j),a(h,x),a(h,y),a(y,b),a(h,E)},d(w){w&&t(h)}}}function mh(C){let h,x,y,b,E,w,j,q,X,vs,F,K,$s,L,R,ys,A;return{c(){h=l("p"),x=i("An object data type in "),y=l("a"),b=i("pandas.Series"),E=i(" doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=l("a"),j=i("datasets.Features"),q=i(" using the "),X=l("code"),vs=i("from_dict"),F=i(" or "),K=l("code"),$s=i("from_pandas"),L=i(" methods. See the "),R=l("a"),ys=i("troubleshoot"),A=i(" for more details on how to explicitly specify your own features."),this.h()},l(H){h=o(H,"P",{});var S=n(h);x=p(S,"An object data type in "),y=o(S,"A",{href:!0,rel:!0});var ca=n(y);b=p(ca,"pandas.Series"),ca.forEach(t),E=p(S," doesn\u2019t always carry enough information for Arrow to automatically infer a data type. For example, if a DataFrame is of length 0 or the Series only contains None/nan objects, the type is set to null. Avoid potential errors by constructing an explicit schema with "),w=o(S,"A",{href:!0});var ws=n(w);j=p(ws,"datasets.Features"),ws.forEach(t),q=p(S," using the "),X=o(S,"CODE",{});var ha=n(X);vs=p(ha,"from_dict"),ha.forEach(t),F=p(S," or "),K=o(S,"CODE",{});var ua=n(K);$s=p(ua,"from_pandas"),ua.forEach(t),L=p(S," methods. See the "),R=o(S,"A",{href:!0});var js=n(R);ys=p(js,"troubleshoot"),js.forEach(t),A=p(S," for more details on how to explicitly specify your own features."),S.forEach(t),this.h()},h(){c(y,"href","https://pandas.pydata.org/docs/reference/api/pandas.Series.html"),c(y,"rel","nofollow"),c(w,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Features"),c(R,"href","#troubleshoot")},m(H,S){r(H,h,S),a(h,x),a(h,y),a(y,b),a(h,E),a(h,w),a(w,j),a(h,q),a(h,X),a(X,vs),a(h,F),a(h,K),a(K,$s),a(h,L),a(h,R),a(R,ys),a(h,A)},d(H){H&&t(h)}}}function gh(C){let h,x,y,b,E;return{c(){h=l("p"),x=i("Using "),y=l("code"),b=i("pct1_dropremainder"),E=i(" rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100.")},l(w){h=o(w,"P",{});var j=n(h);x=p(j,"Using "),y=o(j,"CODE",{});var q=n(y);b=p(q,"pct1_dropremainder"),q.forEach(t),E=p(j," rounding may truncate the last examples in a dataset if the number of examples in your dataset don\u2019t divide evenly by 100."),j.forEach(t)},m(w,j){r(w,h,j),a(h,x),a(h,y),a(y,b),a(h,E)},d(w){w&&t(h)}}}function _h(C){let h,x,y,b,E;return{c(){h=l("p"),x=i("See the "),y=l("a"),b=i("metric_script"),E=i(" guide for more details on how to write your own metric loading script."),this.h()},l(w){h=o(w,"P",{});var j=n(h);x=p(j,"See the "),y=o(j,"A",{href:!0});var q=n(y);b=p(q,"metric_script"),q.forEach(t),E=p(j," guide for more details on how to write your own metric loading script."),j.forEach(t),this.h()},h(){c(y,"href","#metric_script")},m(w,j){r(w,h,j),a(h,x),a(h,y),a(y,b),a(h,E)},d(w){w&&t(h)}}}function vh(C){let h,x,y,b,E;return{c(){h=l("p"),x=i("Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=l("a"),b=i("datasets.Metric.compute()"),E=i(" gathers all the predictions and references from the nodes, and computes the final metric."),this.h()},l(w){h=o(w,"P",{});var j=n(h);x=p(j,"Once you\u2019ve loaded a metric for distributed usage, you can compute the metric as usual. Behind the scenes, "),y=o(j,"A",{href:!0});var q=n(y);b=p(q,"datasets.Metric.compute()"),q.forEach(t),E=p(j," gathers all the predictions and references from the nodes, and computes the final metric."),j.forEach(t),this.h()},h(){c(y,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Metric.compute")},m(w,j){r(w,h,j),a(h,x),a(h,y),a(y,b),a(h,E)},d(w){w&&t(h)}}}function $h(C){let h,x,y,b,E,w,j,q,X,vs,F,K,$s,L,R,ys,A,H,S,ca,ws,ha,ua,js,gr,_r,he,vr,$r,ue,yr,Cl,ma,wr,Nl,ga,Il,Z,bs,me,nt,jr,ge,br,Ol,xs,xr,_e,kr,Er,Hl,M,qr,_a,Pr,Ar,rt,Sr,Tr,Fl,it,Ll,va,Dr,Rl,ks,Cr,ve,Nr,Ir,Ml,pt,Vl,Es,zl,T,Or,$e,Hr,Fr,ye,Lr,Rr,we,Mr,Vr,je,zr,Jr,be,Ur,Br,Jl,dt,Ul,qs,Bl,V,Yr,xe,Wr,Gr,ft,Qr,Xr,Yl,ct,Wl,Ps,Kr,ke,Zr,si,Gl,ht,Ql,ss,As,Ee,ut,ti,qe,ai,Xl,D,ei,Pe,li,oi,Ae,ni,ri,Se,ii,pi,Te,di,fi,$a,ci,hi,Kl,ts,Ss,De,mt,ui,Ce,mi,Zl,ya,gi,so,gt,to,wa,_i,ao,_t,eo,ja,vi,lo,vt,oo,ba,$i,no,$t,ro,xa,yi,io,yt,po,as,Ts,Ne,wt,wi,Ie,ji,fo,Ds,bi,ka,xi,ki,co,jt,ho,Ea,Ei,uo,bt,mo,Cs,qi,Oe,Pi,Ai,go,xt,_o,qa,Si,vo,kt,$o,Pa,Ti,yo,es,Ns,He,Et,Di,Fe,Ci,wo,Aa,Ni,jo,qt,bo,Sa,Ii,xo,Pt,ko,ls,Is,Le,At,Oi,Re,Hi,Eo,Ta,Fi,qo,St,Po,Da,Li,Ao,Tt,So,os,Os,Me,Dt,Ri,Ve,Mi,To,Hs,Vi,Ca,zi,Ji,Do,ns,Fs,ze,Ct,Ui,Je,Bi,Co,Ls,Yi,Na,Wi,Gi,No,Nt,Io,rs,Rs,Ue,It,Qi,Be,Xi,Oo,Ms,Ki,Ia,Zi,sp,Ho,Ot,Fo,Vs,Lo,is,zs,Ye,Ht,tp,We,ap,Ro,Oa,ep,Mo,z,lp,Ge,op,np,Qe,rp,ip,Vo,ps,Js,Xe,Ft,pp,Ke,dp,zo,J,fp,Ha,cp,hp,Fa,up,mp,Jo,U,gp,Ze,_p,vp,sl,$p,yp,Uo,Lt,Bo,Us,wp,tl,jp,bp,Yo,Rt,Wo,La,xp,Go,Mt,Qo,Ra,kp,Xo,Vt,Ko,Ma,Ep,Zo,zt,sn,ds,Bs,al,Jt,qp,el,Pp,tn,Va,Ap,an,Ut,en,Ys,Sp,ll,Tp,Dp,ln,Bt,on,Ws,nn,za,rn,fs,Gs,ol,Yt,Cp,nl,Np,pn,Ja,Ip,dn,cs,Qs,rl,Wt,Op,il,Hp,fn,N,Fp,Ua,Lp,Rp,pl,Mp,Vp,dl,zp,Jp,cn,Xs,Up,Gt,Bp,Yp,hn,Qt,un,hs,Ks,fl,Xt,Wp,cl,Gp,mn,B,Qp,Ba,Xp,Kp,Kt,Zp,sd,gn,Y,td,Ya,ad,ed,Wa,ld,od,_n,Zt,vn,W,nd,hl,rd,id,Ga,pd,dd,$n,sa,yn,Qa,fd,wn,ta,jn,us,Zs,ul,aa,cd,ml,hd,bn,Xa,ud,xn,ea,kn,st,En,ms,tt,gl,la,md,_l,gd,qn,Ka,_d,Pn,oa,An,gs,at,vl,na,vd,$l,$d,Sn,Za,yd,Tn,se,wd,Dn,G,yl,ra,jd,wl,bd,xd,kd,jl,_s,Ed,bl,qd,Pd,xl,Ad,Sd,Td,kl,ia,Dd,te,Cd,Nd,Cn,pa,Nn,et,In,lt,Id,El,Od,Hd,On,da,Hn;return w=new P({}),nt=new P({}),it=new k({props:{codee:`from datasets import load_dataset
dataset = load_dataset('lhoestq/demo1'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;lhoestq/demo1&#x27;</span>)`}}),pt=new k({props:{codee:`dataset = load_dataset(
  "lhoestq/custom_squad",
  revision="main"  # tag name, or branch name, or commit hash
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(
<span class="hljs-meta">&gt;&gt;&gt; </span>  <span class="hljs-string">&quot;lhoestq/custom_squad&quot;</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>  revision=<span class="hljs-string">&quot;main&quot;</span>  <span class="hljs-comment"># tag name, or branch name, or commit hash</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>)`}}),Es=new ce({props:{$$slots:{default:[hh]},$$scope:{ctx:C}}}),dt=new k({props:{codee:`data_files = {"train": "train.csv", "test": "test.csv"}
dataset = load_dataset("namespace/your_dataset_name", data_files=data_files),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: <span class="hljs-string">&quot;train.csv&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>: <span class="hljs-string">&quot;test.csv&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;namespace/your_dataset_name&quot;</span>, data_files=data_files)`}}),qs=new ce({props:{warning:"&lcub;true}",$$slots:{default:[uh]},$$scope:{ctx:C}}}),ct=new k({props:{codee:`from datasets import load_dataset
c4_subset = load_dataset('allenai/c4', data_files='en/c4-train.0000*-of-01024.json.gz'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_subset = load_dataset(<span class="hljs-string">&#x27;allenai/c4&#x27;</span>, data_files=<span class="hljs-string">&#x27;en/c4-train.0000*-of-01024.json.gz&#x27;</span>)`}}),ht=new k({props:{codee:`data_files = {"validation": "en/c4-validation.*.json.gz"}
c4_validation = load_dataset("allenai/c4", data_files=data_files, split="validation"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;validation&quot;</span>: <span class="hljs-string">&quot;en/c4-validation.*.json.gz&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>c4_validation = load_dataset(<span class="hljs-string">&quot;allenai/c4&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;validation&quot;</span>)`}}),ut=new P({}),mt=new P({}),gt=new k({props:{codee:`from datasets import load_dataset
dataset = load_dataset('csv', data_files='my_file.csv'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.csv&#x27;</span>)`}}),_t=new k({props:{codee:"dataset = load_dataset('csv', data_files=['my_file_1.csv', 'my_file_2.csv', 'my_file_3.csv']),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=[<span class="hljs-string">&#x27;my_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_2.csv&#x27;</span>, <span class="hljs-string">&#x27;my_file_3.csv&#x27;</span>])'}}),vt=new k({props:{codee:"dataset = load_dataset('csv', data_files={'train': ['my_train_file_1.csv', 'my_train_file_2.csv'] 'test': 'my_test_file.csv'}),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_train_file_1.csv&#x27;</span>, <span class="hljs-string">&#x27;my_train_file_2.csv&#x27;</span>] <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.csv&#x27;</span>})'}}),$t=new k({props:{codee:`base_url = "https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/"
dataset = load_dataset('csv', data_files={'train': base_url + 'train.csv', 'test': base_url + 'test.csv'}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://huggingface.co/datasets/lhoestq/demo1/resolve/main/data/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: base_url + <span class="hljs-string">&#x27;test.csv&#x27;</span>})`}}),yt=new k({props:{codee:`url = "https://domain.org/train_data.zip"
data_files = {"train": url}
dataset = load_dataset("csv", data_files=data_files),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://domain.org/train_data.zip&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: url}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)`}}),wt=new P({}),jt=new k({props:{codee:`from datasets import load_dataset
dataset = load_dataset('json', data_files='my_file.json'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>)`}}),bt=new k({props:{codee:`{"a": 1, "b": 2.0, "c": "foo", "d": false}
{"a": 4, "b": -5.5, "c": null, "d": true},`,highlighted:`{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false}
{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}`}}),xt=new k({props:{codee:`{"version": "0.1.0",
    "data": [{"a": 1, "b": 2.0, "c": "foo", "d": false},
            {"a": 4, "b": -5.5, "c": null, "d": true}]
}

>>> from datasets import load_dataset
>>> dataset = load_dataset('json', data_files='my_file.json', field='data'),`,highlighted:`{<span class="hljs-string">&quot;version&quot;</span>: <span class="hljs-string">&quot;0.1.0&quot;</span>,
    <span class="hljs-string">&quot;data&quot;</span>: [{<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&quot;b&quot;</span>: <span class="hljs-number">2.0</span>, <span class="hljs-string">&quot;c&quot;</span>: <span class="hljs-string">&quot;foo&quot;</span>, <span class="hljs-string">&quot;d&quot;</span>: false},
            {<span class="hljs-string">&quot;a&quot;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&quot;b&quot;</span>: -<span class="hljs-number">5.5</span>, <span class="hljs-string">&quot;c&quot;</span>: null, <span class="hljs-string">&quot;d&quot;</span>: true}]
}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;my_file.json&#x27;</span>, field=<span class="hljs-string">&#x27;data&#x27;</span>)`}}),kt=new k({props:{codee:`base_url = "https://rajpurkar.github.io/SQuAD-explorer/dataset/"
dataset = load_dataset('json', data_files={'train': base_url + 'train-v1.1.json', 'validation': base_url + 'dev-v1.1.json'}, field="data"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: base_url + <span class="hljs-string">&#x27;train-v1.1.json&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>: base_url + <span class="hljs-string">&#x27;dev-v1.1.json&#x27;</span>}, field=<span class="hljs-string">&quot;data&quot;</span>)`}}),Et=new P({}),qt=new k({props:{codee:`from datasets import load_dataset
dataset = load_dataset('text', data_files={'train': ['my_text_1.txt', 'my_text_2.txt'], 'test': 'my_test_file.txt'}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: [<span class="hljs-string">&#x27;my_text_1.txt&#x27;</span>, <span class="hljs-string">&#x27;my_text_2.txt&#x27;</span>], <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;my_test_file.txt&#x27;</span>})`}}),Pt=new k({props:{codee:"dataset = load_dataset('text', data_files='https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt'),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;text&#x27;</span>, data_files=<span class="hljs-string">&#x27;https://huggingface.co/datasets/lhoestq/test/resolve/main/some_text.txt&#x27;</span>)'}}),At=new P({}),St=new k({props:{codee:`from datasets import load_dataset
dataset = load_dataset("parquet", data_files={'train': 'train.parquet', 'test': 'test.parquet'}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files={<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.parquet&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.parquet&#x27;</span>})`}}),Tt=new k({props:{codee:`base_url = "https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/"
data_files = {"train": base_url + "wikipedia-train.parquet"}
wiki = load_dataset("parquet", data_files=data_files, split="train"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>base_url = <span class="hljs-string">&quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: base_url + <span class="hljs-string">&quot;wikipedia-train.parquet&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>wiki = load_dataset(<span class="hljs-string">&quot;parquet&quot;</span>, data_files=data_files, split=<span class="hljs-string">&quot;train&quot;</span>)`}}),Dt=new P({}),Ct=new P({}),Nt=new k({props:{codee:`from datasets import Dataset
my_dict = {"a": [1, 2, 3]}
dataset = Dataset.from_dict(my_dict),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>my_dict = {<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_dict(my_dict)`}}),It=new P({}),Ot=new k({props:{codee:`from datasets import Dataset
import pandas as pd
df = pd.DataFrame({"a": [1, 2, 3]})
dataset = Dataset.from_pandas(df),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-meta">&gt;&gt;&gt; </span>df = pd.DataFrame({<span class="hljs-string">&quot;a&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]})
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = Dataset.from_pandas(df)`}}),Vs=new ce({props:{warning:"&lcub;true}",$$slots:{default:[mh]},$$scope:{ctx:C}}}),Ht=new P({}),Ft=new P({}),Lt=new Dl({props:{group1:{id:"stringapi",code:"train_test_ds = datasets.load_dataset('bookcorpus', split='train+test')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train+test&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = datasets.ReadInstruction('train') + datasets.ReadInstruction('test')
train_test_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_test_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),Rt=new Dl({props:{group1:{id:"stringapi",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split='train[10:20]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[10:20]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', from_=10, to=20, unit='abs'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">10</span>, to=<span class="hljs-number">20</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>))'}}}),Mt=new Dl({props:{group1:{id:"stringapi",code:"train_10pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]&#x27;</span>)'},group2:{id:"readinstruction",code:"train_10_20_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train', to=10, unit='%'))",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_20_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))'}}}),Vt=new Dl({props:{group1:{id:"stringapi",code:"train_10_80pct_ds = datasets.load_dataset('bookcorpus', split='train[:10%]+train[-80%:]')",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[:10%]+train[-80%:]&#x27;</span>)'},group2:{id:"readinstruction",code:`ri = (datasets.ReadInstruction('train', to=10, unit='%') + datasets.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = datasets.load_dataset('bookcorpus', split=ri)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ri = (datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=-<span class="hljs-number">80</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>train_10_80pct_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=ri)`}}}),zt=new Dl({props:{group1:{id:"stringapi",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', split=[f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', split=[f'train[:{k}%]+train[{k+10}%:]' for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[<span class="hljs-subst">{k}</span>%:<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=[<span class="hljs-string">f&#x27;train[:<span class="hljs-subst">{k}</span>%]+train[<span class="hljs-subst">{k+<span class="hljs-number">10</span>}</span>%:]&#x27;</span> <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`},group2:{id:"readinstruction",code:`# 10-fold cross-validation (see also next section on rounding behavior):
# The validation datasets are each going to be 10%:
# [0%:10%], [10%:20%], ..., [90%:100%].
# And the training datasets are each going to be the complementary 90%:
# [10%:100%] (for a corresponding validation set of [0%:10%]),
# [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,
# [0%:90%] (for a validation set of [90%:100%]).
vals_ds = datasets.load_dataset('bookcorpus', [datasets.ReadInstruction('train', from_=k, to=k+10, unit='%') for k in range(0, 100, 10)])
trains_ds = datasets.load_dataset('bookcorpus', [(datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')) for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 10-fold cross-validation (see also next section on rounding behavior):</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The validation datasets are each going to be 10%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%], [10%:20%], ..., [90%:100%].</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># And the training datasets are each going to be the complementary 90%:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [10%:100%] (for a corresponding validation set of [0%:10%]),</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:10%] + [20%:100%] (for a validation set of [10%:20%]), ...,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [0%:90%] (for a validation set of [90%:100%]).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vals_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
<span class="hljs-meta">&gt;&gt;&gt; </span>trains_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, [(datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}}),Jt=new P({}),Ut=new k({props:{codee:`# Assuming *train* split contains 999 records.
# 19 records, from 500 (included) to 519 (excluded).
>>> train_50_52_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%]')
# 20 records, from 519 (included) to 539 (excluded).
>>> train_52_54_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%]'),`,highlighted:`<span class="hljs-comment"># Assuming *train* split contains 999 records.</span>
<span class="hljs-comment"># 19 records, from 500 (included) to 519 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%]&#x27;</span>)
<span class="hljs-comment"># 20 records, from 519 (included) to 539 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%]&#x27;</span>)`}}),Bt=new k({props:{codee:`# 18 records, from 450 (included) to 468 (excluded).
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction( 'train', from_=50, to=52, unit='%', rounding='pct1_dropremainder'))
# 18 records, from 468 (included) to 486 (excluded).
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split=datasets.ReadInstruction('train',from_=52, to=54, unit='%', rounding='pct1_dropremainder'))
# Or equivalently:
>>> train_50_52pct1_ds = datasets.load_dataset('bookcorpus', split='train[50%:52%](pct1_dropremainder)')
>>> train_52_54pct1_ds = datasets.load_dataset('bookcorpus', split='train[52%:54%](pct1_dropremainder)'),`,highlighted:`<span class="hljs-comment"># 18 records, from 450 (included) to 468 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction( <span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">50</span>, to=<span class="hljs-number">52</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># 18 records, from 468 (included) to 486 (excluded).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>,from_=<span class="hljs-number">52</span>, to=<span class="hljs-number">54</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&#x27;pct1_dropremainder&#x27;</span>))
<span class="hljs-comment"># Or equivalently:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>train_50_52pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[50%:52%](pct1_dropremainder)&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_52_54pct1_ds = datasets.load_dataset(<span class="hljs-string">&#x27;bookcorpus&#x27;</span>, split=<span class="hljs-string">&#x27;train[52%:54%](pct1_dropremainder)&#x27;</span>)`}}),Ws=new ce({props:{warning:"&lcub;true}",$$slots:{default:[gh]},$$scope:{ctx:C}}}),Yt=new P({}),Wt=new P({}),Qt=new k({props:{codee:'dataset = load_dataset("matinf", "summarization"),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;matinf&quot;</span>, <span class="hljs-string">&quot;summarization&quot;</span>)
Downloading <span class="hljs-keyword">and</span> preparing dataset matinf/summarization (download: Unknown size, generated: <span class="hljs-number">246.89</span> MiB, post-processed: Unknown size, total: <span class="hljs-number">246.89</span> MiB) to /root/.cache/huggingface/datasets/matinf/summarization/<span class="hljs-number">1.0</span><span class="hljs-number">.0</span>/82eee5e71c3ceaf20d909bca36ff237452b4e4ab195d3be7ee1c78b53e6f540e...
AssertionError: The dataset matinf <span class="hljs-keyword">with</span> config summarization requires manual data. 
Please follow the manual download instructions: To use MATINF you have to download it manually. Please fill this google form (https://forms.gle/nkH4LVE4iNQeDzsc9). You will receive a download link <span class="hljs-keyword">and</span> a password once you complete the form. Please extract <span class="hljs-built_in">all</span> files <span class="hljs-keyword">in</span> one folder <span class="hljs-keyword">and</span> load the dataset <span class="hljs-keyword">with</span>: *datasets.load_dataset(<span class="hljs-string">&#x27;matinf&#x27;</span>, data_dir=<span class="hljs-string">&#x27;path/to/folder/folder_name&#x27;</span>)*. 
Manual data can be loaded <span class="hljs-keyword">with</span> \`datasets.load_dataset(matinf, data_dir=<span class="hljs-string">&#x27;&lt;path/to/manual/data&gt;&#x27;</span>) `}}),Xt=new P({}),Zt=new k({props:{codee:`class_names = ["sadness", "joy", "love", "anger", "fear", "surprise"]
emotion_features = Features({'text': Value('string'), 'label': ClassLabel(names=class_names)}),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_names = [<span class="hljs-string">&quot;sadness&quot;</span>, <span class="hljs-string">&quot;joy&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;anger&quot;</span>, <span class="hljs-string">&quot;fear&quot;</span>, <span class="hljs-string">&quot;surprise&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>emotion_features = Features({<span class="hljs-string">&#x27;text&#x27;</span>: Value(<span class="hljs-string">&#x27;string&#x27;</span>), <span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(names=class_names)})`}}),sa=new k({props:{codee:"dataset = load_dataset('csv', data_files=file_dict, delimiter=';', column_names=['text', 'label'], features=emotion_features),",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=file_dict, delimiter=<span class="hljs-string">&#x27;;&#x27;</span>, column_names=[<span class="hljs-string">&#x27;text&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>], features=emotion_features)'}}),ta=new k({props:{codee:"dataset['train'].features,",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&#x27;train&#x27;</span>].features
{<span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">6</span>, names=[<span class="hljs-string">&#x27;sadness&#x27;</span>, <span class="hljs-string">&#x27;joy&#x27;</span>, <span class="hljs-string">&#x27;love&#x27;</span>, <span class="hljs-string">&#x27;anger&#x27;</span>, <span class="hljs-string">&#x27;fear&#x27;</span>, <span class="hljs-string">&#x27;surprise&#x27;</span>], names_file=<span class="hljs-literal">None</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),aa=new P({}),ea=new k({props:{codee:`from datasets import load_metric
metric = load_metric('PATH/TO/MY/METRIC/SCRIPT')

# Example of typical usage
for batch in dataset:
    inputs, references = batch
    predictions = model(inputs)
    metric.add_batch(predictions=predictions, references=references)
score = metric.compute(),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;PATH/TO/MY/METRIC/SCRIPT&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Example of typical usage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> dataset:
<span class="hljs-meta">... </span>    inputs, references = batch
<span class="hljs-meta">... </span>    predictions = model(inputs)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=references)
<span class="hljs-meta">&gt;&gt;&gt; </span>score = metric.compute()`}}),st=new ce({props:{$$slots:{default:[_h]},$$scope:{ctx:C}}}),la=new P({}),oa=new k({props:{codee:`from datasets import load_metric
metric = load_metric('bleurt', name='bleurt-base-128')
metric = load_metric('bleurt', name='bleurt-base-512'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-128&#x27;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">metric = load_metric(<span class="hljs-string">&#x27;bleurt&#x27;</span>, name=<span class="hljs-string">&#x27;bleurt-base-512&#x27;</span>)</span>`}}),na=new P({}),pa=new k({props:{codee:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=rank),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=rank)`}}),et=new ce({props:{$$slots:{default:[vh]},$$scope:{ctx:C}}}),da=new k({props:{codee:`from datasets import load_metric
metric = load_metric('glue', 'mrpc', num_process=num_process, process_id=process_id, experiment_id="My_experiment_10"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>metric = load_metric(<span class="hljs-string">&#x27;glue&#x27;</span>, <span class="hljs-string">&#x27;mrpc&#x27;</span>, num_process=num_process, process_id=process_id, experiment_id=<span class="hljs-string">&quot;My_experiment_10&quot;</span>)`}}),{c(){h=l("meta"),x=d(),y=l("h1"),b=l("a"),E=l("span"),u(w.$$.fragment),j=d(),q=l("span"),X=i("Load"),vs=d(),F=l("p"),K=i("You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),$s=d(),L=l("p"),R=i("This guide will show you how to load a dataset from:"),ys=d(),A=l("ul"),H=l("li"),S=i("The Hub without a dataset loading script"),ca=d(),ws=l("li"),ha=i("Local files"),ua=d(),js=l("li"),gr=i("In-memory data"),_r=d(),he=l("li"),vr=i("Offline"),$r=d(),ue=l("li"),yr=i("A specific slice of a split"),Cl=d(),ma=l("p"),wr=i("You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),Nl=d(),ga=l("a"),Il=d(),Z=l("h2"),bs=l("a"),me=l("span"),u(nt.$$.fragment),jr=d(),ge=l("span"),br=i("Hugging Face Hub"),Ol=d(),xs=l("p"),xr=i("In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),_e=l("strong"),kr=i("without"),Er=i(" a loading script!"),Hl=d(),M=l("p"),qr=i("First, create a dataset repository and upload your data files. Then you can use "),_a=l("a"),Pr=i("datasets.load_dataset()"),Ar=i(" like you learned in the tutorial. For example, load the files from this "),rt=l("a"),Sr=i("demo repository"),Tr=i(" by providing the repository namespace and dataset name:"),Fl=d(),u(it.$$.fragment),Ll=d(),va=l("p"),Dr=i("This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Rl=d(),ks=l("p"),Cr=i("Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),ve=l("code"),Nr=i("revision"),Ir=i(" flag to specify which dataset version you want to load:"),Ml=d(),u(pt.$$.fragment),Vl=d(),u(Es.$$.fragment),zl=d(),T=l("p"),Or=i("If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),$e=l("code"),Hr=i("train"),Fr=i(" split. Use the "),ye=l("code"),Lr=i("data_files"),Rr=i(" parameter to map data files to splits like "),we=l("code"),Mr=i("train"),Vr=i(", "),je=l("code"),zr=i("validation"),Jr=i(" and "),be=l("code"),Ur=i("test"),Br=i(":"),Jl=d(),u(dt.$$.fragment),Ul=d(),u(qs.$$.fragment),Bl=d(),V=l("p"),Yr=i("You can also load a specific subset of the files with the "),xe=l("code"),Wr=i("data_files"),Gr=i(" parameter. The example below loads files from the "),ft=l("a"),Qr=i("C4 dataset"),Xr=i(":"),Yl=d(),u(ct.$$.fragment),Wl=d(),Ps=l("p"),Kr=i("Specify a custom split with the "),ke=l("code"),Zr=i("split"),si=i(" parameter:"),Gl=d(),u(ht.$$.fragment),Ql=d(),ss=l("h2"),As=l("a"),Ee=l("span"),u(ut.$$.fragment),ti=d(),qe=l("span"),ai=i("Local and remote files"),Xl=d(),D=l("p"),ei=i("\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Pe=l("code"),li=i("csv"),oi=i(", "),Ae=l("code"),ni=i("json"),ri=i(", "),Se=l("code"),ii=i("txt"),pi=i(" or "),Te=l("code"),di=i("parquet"),fi=i(" file. The "),$a=l("a"),ci=i("datasets.load_dataset()"),hi=i(" method is able to load each of these file types."),Kl=d(),ts=l("h3"),Ss=l("a"),De=l("span"),u(mt.$$.fragment),ui=d(),Ce=l("span"),mi=i("CSV"),Zl=d(),ya=l("p"),gi=i("\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),so=d(),u(gt.$$.fragment),to=d(),wa=l("p"),_i=i("If you have more than one CSV file:"),ao=d(),u(_t.$$.fragment),eo=d(),ja=l("p"),vi=i("You can also map the training and test splits to specific CSV files:"),lo=d(),u(vt.$$.fragment),oo=d(),ba=l("p"),$i=i("To load remote CSV files via HTTP, you can pass the URLs:"),no=d(),u($t.$$.fragment),ro=d(),xa=l("p"),yi=i("To load zipped CSV files:"),io=d(),u(yt.$$.fragment),po=d(),as=l("h3"),Ts=l("a"),Ne=l("span"),u(wt.$$.fragment),wi=d(),Ie=l("span"),ji=i("JSON"),fo=d(),Ds=l("p"),bi=i("JSON files are loaded directly with "),ka=l("a"),xi=i("datasets.load_dataset()"),ki=i(" as shown below:"),co=d(),u(jt.$$.fragment),ho=d(),Ea=l("p"),Ei=i("JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),uo=d(),u(bt.$$.fragment),mo=d(),Cs=l("p"),qi=i("Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Oe=l("code"),Pi=i("field"),Ai=i(" argument as shown in the following:"),go=d(),u(xt.$$.fragment),_o=d(),qa=l("p"),Si=i("To load remote JSON files via HTTP, you can pass the URLs:"),vo=d(),u(kt.$$.fragment),$o=d(),Pa=l("p"),Ti=i("While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),yo=d(),es=l("h3"),Ns=l("a"),He=l("span"),u(Et.$$.fragment),Di=d(),Fe=l("span"),Ci=i("Text files"),wo=d(),Aa=l("p"),Ni=i("Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),jo=d(),u(qt.$$.fragment),bo=d(),Sa=l("p"),Ii=i("To load remote TXT files via HTTP, you can pass the URLs:"),xo=d(),u(Pt.$$.fragment),ko=d(),ls=l("h3"),Is=l("a"),Le=l("span"),u(At.$$.fragment),Oi=d(),Re=l("span"),Hi=i("Parquet"),Eo=d(),Ta=l("p"),Fi=i("Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),qo=d(),u(St.$$.fragment),Po=d(),Da=l("p"),Li=i("To load remote parquet files via HTTP, you can pass the URLs:"),Ao=d(),u(Tt.$$.fragment),So=d(),os=l("h2"),Os=l("a"),Me=l("span"),u(Dt.$$.fragment),Ri=d(),Ve=l("span"),Mi=i("In-memory data"),To=d(),Hs=l("p"),Vi=i("\u{1F917} Datasets will also allow you to create a "),Ca=l("a"),zi=i("datasets.Dataset"),Ji=i(" directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Do=d(),ns=l("h3"),Fs=l("a"),ze=l("span"),u(Ct.$$.fragment),Ui=d(),Je=l("span"),Bi=i("Python dictionary"),Co=d(),Ls=l("p"),Yi=i("Load Python dictionaries with "),Na=l("a"),Wi=i("datasets.Dataset.from_dict()"),Gi=i(":"),No=d(),u(Nt.$$.fragment),Io=d(),rs=l("h3"),Rs=l("a"),Ue=l("span"),u(It.$$.fragment),Qi=d(),Be=l("span"),Xi=i("Pandas DataFrame"),Oo=d(),Ms=l("p"),Ki=i("Load Pandas DataFrames with "),Ia=l("a"),Zi=i("datasets.Dataset.from_pandas()"),sp=i(":"),Ho=d(),u(Ot.$$.fragment),Fo=d(),u(Vs.$$.fragment),Lo=d(),is=l("h2"),zs=l("a"),Ye=l("span"),u(Ht.$$.fragment),tp=d(),We=l("span"),ap=i("Offline"),Ro=d(),Oa=l("p"),ep=i("Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),Mo=d(),z=l("p"),lp=i("If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),Ge=l("code"),op=i("HF_DATASETS_OFFLINE"),np=i(" to "),Qe=l("code"),rp=i("1"),ip=i(" to enable full offline mode."),Vo=d(),ps=l("h2"),Js=l("a"),Xe=l("span"),u(Ft.$$.fragment),pp=d(),Ke=l("span"),dp=i("Slice splits"),zo=d(),J=l("p"),fp=i("For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ha=l("a"),cp=i("datasets.ReadInstruction"),hp=i(". Strings are more compact and readable for simple cases, while "),Fa=l("a"),up=i("datasets.ReadInstruction"),mp=i(" is easier to use with variable slicing parameters."),Jo=d(),U=l("p"),gp=i("Concatenate the "),Ze=l("code"),_p=i("train"),vp=i(" and "),sl=l("code"),$p=i("test"),yp=i(" split by:"),Uo=d(),u(Lt.$$.fragment),Bo=d(),Us=l("p"),wp=i("Select specific rows of the "),tl=l("code"),jp=i("train"),bp=i(" split:"),Yo=d(),u(Rt.$$.fragment),Wo=d(),La=l("p"),xp=i("Or select a percentage of the split with:"),Go=d(),u(Mt.$$.fragment),Qo=d(),Ra=l("p"),kp=i("You can even select a combination of percentages from each split:"),Xo=d(),u(Vt.$$.fragment),Ko=d(),Ma=l("p"),Ep=i("Finally, create cross-validated dataset splits by:"),Zo=d(),u(zt.$$.fragment),sn=d(),ds=l("h3"),Bs=l("a"),al=l("span"),u(Jt.$$.fragment),qp=d(),el=l("span"),Pp=i("Percent slicing and rounding"),tn=d(),Va=l("p"),Ap=i("For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),an=d(),u(Ut.$$.fragment),en=d(),Ys=l("p"),Sp=i("If you want equal sized splits, use "),ll=l("code"),Tp=i("pct1_dropremainder"),Dp=i(" rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),ln=d(),u(Bt.$$.fragment),on=d(),u(Ws.$$.fragment),nn=d(),za=l("a"),rn=d(),fs=l("h2"),Gs=l("a"),ol=l("span"),u(Yt.$$.fragment),Cp=d(),nl=l("span"),Np=i("Troubleshooting"),pn=d(),Ja=l("p"),Ip=i("Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),dn=d(),cs=l("h3"),Qs=l("a"),rl=l("span"),u(Wt.$$.fragment),Op=d(),il=l("span"),Hp=i("Manual download"),fn=d(),N=l("p"),Fp=i("Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Ua=l("a"),Lp=i("datasets.load_dataset()"),Rp=i(" to throw an "),pl=l("code"),Mp=i("AssertionError"),Vp=i(". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),dl=l("code"),zp=i("data_dir"),Jp=i(" argument to specify the path to the files you just downloaded."),cn=d(),Xs=l("p"),Up=i("For example, if you try to download a configuration from the "),Gt=l("a"),Bp=i("MATINF"),Yp=i(" dataset:"),hn=d(),u(Qt.$$.fragment),un=d(),hs=l("h3"),Ks=l("a"),fl=l("span"),u(Xt.$$.fragment),Wp=d(),cl=l("span"),Gp=i("Specify features"),mn=d(),B=l("p"),Qp=i("When you create a dataset from local files, the "),Ba=l("a"),Xp=i("datasets.Features"),Kp=i(" are automatically inferred by "),Kt=l("a"),Zp=i("Apache Arrow"),sd=i(". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),gn=d(),Y=l("p"),td=i("The following example shows how you can add custom labels with "),Ya=l("a"),ad=i("datasets.ClassLabel"),ed=i(". First, define your own labels using the "),Wa=l("a"),ld=i("datasets.Features"),od=i(" class:"),_n=d(),u(Zt.$$.fragment),vn=d(),W=l("p"),nd=i("Next, specify the "),hl=l("code"),rd=i("features"),id=i(" argument in "),Ga=l("a"),pd=i("datasets.load_dataset()"),dd=i(" with the features you just created:"),$n=d(),u(sa.$$.fragment),yn=d(),Qa=l("p"),fd=i("Now when you look at your dataset features, you can see it uses the custom labels you defined:"),wn=d(),u(ta.$$.fragment),jn=d(),us=l("h2"),Zs=l("a"),ul=l("span"),u(aa.$$.fragment),cd=d(),ml=l("span"),hd=i("Metrics"),bn=d(),Xa=l("p"),ud=i("When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),xn=d(),u(ea.$$.fragment),kn=d(),u(st.$$.fragment),En=d(),ms=l("h3"),tt=l("a"),gl=l("span"),u(la.$$.fragment),md=d(),_l=l("span"),gd=i("Load configurations"),qn=d(),Ka=l("p"),_d=i("It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),Pn=d(),u(oa.$$.fragment),An=d(),gs=l("h3"),at=l("a"),vl=l("span"),u(na.$$.fragment),vd=d(),$l=l("span"),$d=i("Distributed setup"),Sn=d(),Za=l("p"),yd=i("When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Tn=d(),se=l("p"),wd=i("For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),Dn=d(),G=l("ol"),yl=l("li"),ra=l("p"),jd=i("Define the total number of processes with the "),wl=l("code"),bd=i("num_process"),xd=i(" argument."),kd=d(),jl=l("li"),_s=l("p"),Ed=i("Set the process "),bl=l("code"),qd=i("rank"),Pd=i(" as an integer between zero and "),xl=l("code"),Ad=i("num_process - 1"),Sd=i("."),Td=d(),kl=l("li"),ia=l("p"),Dd=i("Load your metric with "),te=l("a"),Cd=i("datasets.load_metric()"),Nd=i(" with these arguments:"),Cn=d(),u(pa.$$.fragment),Nn=d(),u(et.$$.fragment),In=d(),lt=l("p"),Id=i("In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),El=l("code"),Od=i("experiment_id"),Hd=i(" to distinguish the separate evaluations:"),On=d(),u(da.$$.fragment),this.h()},l(s){const e=ch('[data-svelte="svelte-1phssyn"]',document.head);h=o(e,"META",{name:!0,content:!0}),e.forEach(t),x=f(s),y=o(s,"H1",{class:!0});var fa=n(y);b=o(fa,"A",{id:!0,class:!0,href:!0});var ql=n(b);E=o(ql,"SPAN",{});var Pl=n(E);m(w.$$.fragment,Pl),Pl.forEach(t),ql.forEach(t),j=f(fa),q=o(fa,"SPAN",{});var Al=n(q);X=p(Al,"Load"),Al.forEach(t),fa.forEach(t),vs=f(s),F=o(s,"P",{});var Sl=n(F);K=p(Sl,"You have already seen how to load a dataset from the Hugging Face Hub. But datasets are stored in a variety of places, and sometimes you won\u2019t find the one you want on the Hub. A dataset can be on disk on your local machine, in a Github repository, and in in-memory data structures like Python dictionaries and Pandas DataFrames. Wherever your dataset may be stored, \u{1F917} Datasets provides a way for you to load and use it for training."),Sl.forEach(t),$s=f(s),L=o(s,"P",{});var Tl=n(L);R=p(Tl,"This guide will show you how to load a dataset from:"),Tl.forEach(t),ys=f(s),A=o(s,"UL",{});var Q=n(A);H=o(Q,"LI",{});var Fd=n(H);S=p(Fd,"The Hub without a dataset loading script"),Fd.forEach(t),ca=f(Q),ws=o(Q,"LI",{});var Ld=n(ws);ha=p(Ld,"Local files"),Ld.forEach(t),ua=f(Q),js=o(Q,"LI",{});var Rd=n(js);gr=p(Rd,"In-memory data"),Rd.forEach(t),_r=f(Q),he=o(Q,"LI",{});var Md=n(he);vr=p(Md,"Offline"),Md.forEach(t),$r=f(Q),ue=o(Q,"LI",{});var Vd=n(ue);yr=p(Vd,"A specific slice of a split"),Vd.forEach(t),Q.forEach(t),Cl=f(s),ma=o(s,"P",{});var zd=n(ma);wr=p(zd,"You will also learn how to troubleshoot common errors, and how to load specific configurations of a metric."),zd.forEach(t),Nl=f(s),ga=o(s,"A",{id:!0}),n(ga).forEach(t),Il=f(s),Z=o(s,"H2",{class:!0});var Fn=n(Z);bs=o(Fn,"A",{id:!0,class:!0,href:!0});var Jd=n(bs);me=o(Jd,"SPAN",{});var Ud=n(me);m(nt.$$.fragment,Ud),Ud.forEach(t),Jd.forEach(t),jr=f(Fn),ge=o(Fn,"SPAN",{});var Bd=n(ge);br=p(Bd,"Hugging Face Hub"),Bd.forEach(t),Fn.forEach(t),Ol=f(s),xs=o(s,"P",{});var Ln=n(xs);xr=p(Ln,"In the tutorial, you learned how to load a dataset from the Hub. This method relies on a dataset loading script that downloads and builds the dataset. However, you can also load a dataset from any dataset repository on the Hub "),_e=o(Ln,"STRONG",{});var Yd=n(_e);kr=p(Yd,"without"),Yd.forEach(t),Er=p(Ln," a loading script!"),Ln.forEach(t),Hl=f(s),M=o(s,"P",{});var ae=n(M);qr=p(ae,"First, create a dataset repository and upload your data files. Then you can use "),_a=o(ae,"A",{href:!0});var Wd=n(_a);Pr=p(Wd,"datasets.load_dataset()"),Wd.forEach(t),Ar=p(ae," like you learned in the tutorial. For example, load the files from this "),rt=o(ae,"A",{href:!0,rel:!0});var Gd=n(rt);Sr=p(Gd,"demo repository"),Gd.forEach(t),Tr=p(ae," by providing the repository namespace and dataset name:"),ae.forEach(t),Fl=f(s),m(it.$$.fragment,s),Ll=f(s),va=o(s,"P",{});var Qd=n(va);Dr=p(Qd,"This dataset repository contains CSV files, and this code loads all the data from the CSV files."),Qd.forEach(t),Rl=f(s),ks=o(s,"P",{});var Rn=n(ks);Cr=p(Rn,"Some datasets may have more than one version, based on Git tags, branches or commits. Use the "),ve=o(Rn,"CODE",{});var Xd=n(ve);Nr=p(Xd,"revision"),Xd.forEach(t),Ir=p(Rn," flag to specify which dataset version you want to load:"),Rn.forEach(t),Ml=f(s),m(pt.$$.fragment,s),Vl=f(s),m(Es.$$.fragment,s),zl=f(s),T=o(s,"P",{});var I=n(T);Or=p(I,"If the dataset doesn\u2019t have a dataset loading script, then by default, all the data will be loaded in the "),$e=o(I,"CODE",{});var Kd=n($e);Hr=p(Kd,"train"),Kd.forEach(t),Fr=p(I," split. Use the "),ye=o(I,"CODE",{});var Zd=n(ye);Lr=p(Zd,"data_files"),Zd.forEach(t),Rr=p(I," parameter to map data files to splits like "),we=o(I,"CODE",{});var sf=n(we);Mr=p(sf,"train"),sf.forEach(t),Vr=p(I,", "),je=o(I,"CODE",{});var tf=n(je);zr=p(tf,"validation"),tf.forEach(t),Jr=p(I," and "),be=o(I,"CODE",{});var af=n(be);Ur=p(af,"test"),af.forEach(t),Br=p(I,":"),I.forEach(t),Jl=f(s),m(dt.$$.fragment,s),Ul=f(s),m(qs.$$.fragment,s),Bl=f(s),V=o(s,"P",{});var ee=n(V);Yr=p(ee,"You can also load a specific subset of the files with the "),xe=o(ee,"CODE",{});var ef=n(xe);Wr=p(ef,"data_files"),ef.forEach(t),Gr=p(ee," parameter. The example below loads files from the "),ft=o(ee,"A",{href:!0,rel:!0});var lf=n(ft);Qr=p(lf,"C4 dataset"),lf.forEach(t),Xr=p(ee,":"),ee.forEach(t),Yl=f(s),m(ct.$$.fragment,s),Wl=f(s),Ps=o(s,"P",{});var Mn=n(Ps);Kr=p(Mn,"Specify a custom split with the "),ke=o(Mn,"CODE",{});var of=n(ke);Zr=p(of,"split"),of.forEach(t),si=p(Mn," parameter:"),Mn.forEach(t),Gl=f(s),m(ht.$$.fragment,s),Ql=f(s),ss=o(s,"H2",{class:!0});var Vn=n(ss);As=o(Vn,"A",{id:!0,class:!0,href:!0});var nf=n(As);Ee=o(nf,"SPAN",{});var rf=n(Ee);m(ut.$$.fragment,rf),rf.forEach(t),nf.forEach(t),ti=f(Vn),qe=o(Vn,"SPAN",{});var pf=n(qe);ai=p(pf,"Local and remote files"),pf.forEach(t),Vn.forEach(t),Xl=f(s),D=o(s,"P",{});var O=n(D);ei=p(O,"\u{1F917} Datasets can be loaded from local files stored on your computer, and also from remote files. The datasets are most likely stored as a "),Pe=o(O,"CODE",{});var df=n(Pe);li=p(df,"csv"),df.forEach(t),oi=p(O,", "),Ae=o(O,"CODE",{});var ff=n(Ae);ni=p(ff,"json"),ff.forEach(t),ri=p(O,", "),Se=o(O,"CODE",{});var cf=n(Se);ii=p(cf,"txt"),cf.forEach(t),pi=p(O," or "),Te=o(O,"CODE",{});var hf=n(Te);di=p(hf,"parquet"),hf.forEach(t),fi=p(O," file. The "),$a=o(O,"A",{href:!0});var uf=n($a);ci=p(uf,"datasets.load_dataset()"),uf.forEach(t),hi=p(O," method is able to load each of these file types."),O.forEach(t),Kl=f(s),ts=o(s,"H3",{class:!0});var zn=n(ts);Ss=o(zn,"A",{id:!0,class:!0,href:!0});var mf=n(Ss);De=o(mf,"SPAN",{});var gf=n(De);m(mt.$$.fragment,gf),gf.forEach(t),mf.forEach(t),ui=f(zn),Ce=o(zn,"SPAN",{});var _f=n(Ce);mi=p(_f,"CSV"),_f.forEach(t),zn.forEach(t),Zl=f(s),ya=o(s,"P",{});var vf=n(ya);gi=p(vf,"\u{1F917} Datasets can read a dataset made up of one or several CSV files:"),vf.forEach(t),so=f(s),m(gt.$$.fragment,s),to=f(s),wa=o(s,"P",{});var $f=n(wa);_i=p($f,"If you have more than one CSV file:"),$f.forEach(t),ao=f(s),m(_t.$$.fragment,s),eo=f(s),ja=o(s,"P",{});var yf=n(ja);vi=p(yf,"You can also map the training and test splits to specific CSV files:"),yf.forEach(t),lo=f(s),m(vt.$$.fragment,s),oo=f(s),ba=o(s,"P",{});var wf=n(ba);$i=p(wf,"To load remote CSV files via HTTP, you can pass the URLs:"),wf.forEach(t),no=f(s),m($t.$$.fragment,s),ro=f(s),xa=o(s,"P",{});var jf=n(xa);yi=p(jf,"To load zipped CSV files:"),jf.forEach(t),io=f(s),m(yt.$$.fragment,s),po=f(s),as=o(s,"H3",{class:!0});var Jn=n(as);Ts=o(Jn,"A",{id:!0,class:!0,href:!0});var bf=n(Ts);Ne=o(bf,"SPAN",{});var xf=n(Ne);m(wt.$$.fragment,xf),xf.forEach(t),bf.forEach(t),wi=f(Jn),Ie=o(Jn,"SPAN",{});var kf=n(Ie);ji=p(kf,"JSON"),kf.forEach(t),Jn.forEach(t),fo=f(s),Ds=o(s,"P",{});var Un=n(Ds);bi=p(Un,"JSON files are loaded directly with "),ka=o(Un,"A",{href:!0});var Ef=n(ka);xi=p(Ef,"datasets.load_dataset()"),Ef.forEach(t),ki=p(Un," as shown below:"),Un.forEach(t),co=f(s),m(jt.$$.fragment,s),ho=f(s),Ea=o(s,"P",{});var qf=n(Ea);Ei=p(qf,"JSON files can have diverse formats, but we think the most efficient format is to have multiple JSON objects; each line represents an individual row of data. For example:"),qf.forEach(t),uo=f(s),m(bt.$$.fragment,s),mo=f(s),Cs=o(s,"P",{});var Bn=n(Cs);qi=p(Bn,"Another JSON format you may encounter is a nested field, in which case you will need to specify the "),Oe=o(Bn,"CODE",{});var Pf=n(Oe);Pi=p(Pf,"field"),Pf.forEach(t),Ai=p(Bn," argument as shown in the following:"),Bn.forEach(t),go=f(s),m(xt.$$.fragment,s),_o=f(s),qa=o(s,"P",{});var Af=n(qa);Si=p(Af,"To load remote JSON files via HTTP, you can pass the URLs:"),Af.forEach(t),vo=f(s),m(kt.$$.fragment,s),$o=f(s),Pa=o(s,"P",{});var Sf=n(Pa);Ti=p(Sf,"While these are the most common JSON formats, you will see other datasets that are formatted differently. \u{1F917} Datasets recognizes these other formats, and will fallback accordingly on the Python JSON loading methods to handle them."),Sf.forEach(t),yo=f(s),es=o(s,"H3",{class:!0});var Yn=n(es);Ns=o(Yn,"A",{id:!0,class:!0,href:!0});var Tf=n(Ns);He=o(Tf,"SPAN",{});var Df=n(He);m(Et.$$.fragment,Df),Df.forEach(t),Tf.forEach(t),Di=f(Yn),Fe=o(Yn,"SPAN",{});var Cf=n(Fe);Ci=p(Cf,"Text files"),Cf.forEach(t),Yn.forEach(t),wo=f(s),Aa=o(s,"P",{});var Nf=n(Aa);Ni=p(Nf,"Text files are one of the most common file types for storing a dataset. \u{1F917} Datasets will read the text file line by line to build the dataset."),Nf.forEach(t),jo=f(s),m(qt.$$.fragment,s),bo=f(s),Sa=o(s,"P",{});var If=n(Sa);Ii=p(If,"To load remote TXT files via HTTP, you can pass the URLs:"),If.forEach(t),xo=f(s),m(Pt.$$.fragment,s),ko=f(s),ls=o(s,"H3",{class:!0});var Wn=n(ls);Is=o(Wn,"A",{id:!0,class:!0,href:!0});var Of=n(Is);Le=o(Of,"SPAN",{});var Hf=n(Le);m(At.$$.fragment,Hf),Hf.forEach(t),Of.forEach(t),Oi=f(Wn),Re=o(Wn,"SPAN",{});var Ff=n(Re);Hi=p(Ff,"Parquet"),Ff.forEach(t),Wn.forEach(t),Eo=f(s),Ta=o(s,"P",{});var Lf=n(Ta);Fi=p(Lf,"Parquet files are stored in a columnar format unlike row-based files like CSV. Large datasets may be stored in a Parquet file because it is more efficient, and faster at returning your query. Load a Parquet file as shown in the following example:"),Lf.forEach(t),qo=f(s),m(St.$$.fragment,s),Po=f(s),Da=o(s,"P",{});var Rf=n(Da);Li=p(Rf,"To load remote parquet files via HTTP, you can pass the URLs:"),Rf.forEach(t),Ao=f(s),m(Tt.$$.fragment,s),So=f(s),os=o(s,"H2",{class:!0});var Gn=n(os);Os=o(Gn,"A",{id:!0,class:!0,href:!0});var Mf=n(Os);Me=o(Mf,"SPAN",{});var Vf=n(Me);m(Dt.$$.fragment,Vf),Vf.forEach(t),Mf.forEach(t),Ri=f(Gn),Ve=o(Gn,"SPAN",{});var zf=n(Ve);Mi=p(zf,"In-memory data"),zf.forEach(t),Gn.forEach(t),To=f(s),Hs=o(s,"P",{});var Qn=n(Hs);Vi=p(Qn,"\u{1F917} Datasets will also allow you to create a "),Ca=o(Qn,"A",{href:!0});var Jf=n(Ca);zi=p(Jf,"datasets.Dataset"),Jf.forEach(t),Ji=p(Qn," directly from in-memory data structures like Python dictionaries and Pandas DataFrames."),Qn.forEach(t),Do=f(s),ns=o(s,"H3",{class:!0});var Xn=n(ns);Fs=o(Xn,"A",{id:!0,class:!0,href:!0});var Uf=n(Fs);ze=o(Uf,"SPAN",{});var Bf=n(ze);m(Ct.$$.fragment,Bf),Bf.forEach(t),Uf.forEach(t),Ui=f(Xn),Je=o(Xn,"SPAN",{});var Yf=n(Je);Bi=p(Yf,"Python dictionary"),Yf.forEach(t),Xn.forEach(t),Co=f(s),Ls=o(s,"P",{});var Kn=n(Ls);Yi=p(Kn,"Load Python dictionaries with "),Na=o(Kn,"A",{href:!0});var Wf=n(Na);Wi=p(Wf,"datasets.Dataset.from_dict()"),Wf.forEach(t),Gi=p(Kn,":"),Kn.forEach(t),No=f(s),m(Nt.$$.fragment,s),Io=f(s),rs=o(s,"H3",{class:!0});var Zn=n(rs);Rs=o(Zn,"A",{id:!0,class:!0,href:!0});var Gf=n(Rs);Ue=o(Gf,"SPAN",{});var Qf=n(Ue);m(It.$$.fragment,Qf),Qf.forEach(t),Gf.forEach(t),Qi=f(Zn),Be=o(Zn,"SPAN",{});var Xf=n(Be);Xi=p(Xf,"Pandas DataFrame"),Xf.forEach(t),Zn.forEach(t),Oo=f(s),Ms=o(s,"P",{});var sr=n(Ms);Ki=p(sr,"Load Pandas DataFrames with "),Ia=o(sr,"A",{href:!0});var Kf=n(Ia);Zi=p(Kf,"datasets.Dataset.from_pandas()"),Kf.forEach(t),sp=p(sr,":"),sr.forEach(t),Ho=f(s),m(Ot.$$.fragment,s),Fo=f(s),m(Vs.$$.fragment,s),Lo=f(s),is=o(s,"H2",{class:!0});var tr=n(is);zs=o(tr,"A",{id:!0,class:!0,href:!0});var Zf=n(zs);Ye=o(Zf,"SPAN",{});var sc=n(Ye);m(Ht.$$.fragment,sc),sc.forEach(t),Zf.forEach(t),tp=f(tr),We=o(tr,"SPAN",{});var tc=n(We);ap=p(tc,"Offline"),tc.forEach(t),tr.forEach(t),Ro=f(s),Oa=o(s,"P",{});var ac=n(Oa);ep=p(ac,"Even if you don\u2019t have an internet connection, it is still possible to load a dataset. As long as you\u2019ve downloaded a dataset from the Hub or \u{1F917} Datasets GitHub repository before, it should be cached. This means you can reload the dataset from the cache and use it offline."),ac.forEach(t),Mo=f(s),z=o(s,"P",{});var le=n(z);lp=p(le,"If you know you won\u2019t have internet access, you can run \u{1F917} Datasets in full offline mode. This saves time because instead of waiting for the Dataset builder download to time out, \u{1F917} Datasets will look directly in the cache. Set the environment variable "),Ge=o(le,"CODE",{});var ec=n(Ge);op=p(ec,"HF_DATASETS_OFFLINE"),ec.forEach(t),np=p(le," to "),Qe=o(le,"CODE",{});var lc=n(Qe);rp=p(lc,"1"),lc.forEach(t),ip=p(le," to enable full offline mode."),le.forEach(t),Vo=f(s),ps=o(s,"H2",{class:!0});var ar=n(ps);Js=o(ar,"A",{id:!0,class:!0,href:!0});var oc=n(Js);Xe=o(oc,"SPAN",{});var nc=n(Xe);m(Ft.$$.fragment,nc),nc.forEach(t),oc.forEach(t),pp=f(ar),Ke=o(ar,"SPAN",{});var rc=n(Ke);dp=p(rc,"Slice splits"),rc.forEach(t),ar.forEach(t),zo=f(s),J=o(s,"P",{});var oe=n(J);fp=p(oe,"For even greater control over how to load a split, you can choose to only load specific slices of a split. There are two options for slicing a split: using strings or "),Ha=o(oe,"A",{href:!0});var ic=n(Ha);cp=p(ic,"datasets.ReadInstruction"),ic.forEach(t),hp=p(oe,". Strings are more compact and readable for simple cases, while "),Fa=o(oe,"A",{href:!0});var pc=n(Fa);up=p(pc,"datasets.ReadInstruction"),pc.forEach(t),mp=p(oe," is easier to use with variable slicing parameters."),oe.forEach(t),Jo=f(s),U=o(s,"P",{});var ne=n(U);gp=p(ne,"Concatenate the "),Ze=o(ne,"CODE",{});var dc=n(Ze);_p=p(dc,"train"),dc.forEach(t),vp=p(ne," and "),sl=o(ne,"CODE",{});var fc=n(sl);$p=p(fc,"test"),fc.forEach(t),yp=p(ne," split by:"),ne.forEach(t),Uo=f(s),m(Lt.$$.fragment,s),Bo=f(s),Us=o(s,"P",{});var er=n(Us);wp=p(er,"Select specific rows of the "),tl=o(er,"CODE",{});var cc=n(tl);jp=p(cc,"train"),cc.forEach(t),bp=p(er," split:"),er.forEach(t),Yo=f(s),m(Rt.$$.fragment,s),Wo=f(s),La=o(s,"P",{});var hc=n(La);xp=p(hc,"Or select a percentage of the split with:"),hc.forEach(t),Go=f(s),m(Mt.$$.fragment,s),Qo=f(s),Ra=o(s,"P",{});var uc=n(Ra);kp=p(uc,"You can even select a combination of percentages from each split:"),uc.forEach(t),Xo=f(s),m(Vt.$$.fragment,s),Ko=f(s),Ma=o(s,"P",{});var mc=n(Ma);Ep=p(mc,"Finally, create cross-validated dataset splits by:"),mc.forEach(t),Zo=f(s),m(zt.$$.fragment,s),sn=f(s),ds=o(s,"H3",{class:!0});var lr=n(ds);Bs=o(lr,"A",{id:!0,class:!0,href:!0});var gc=n(Bs);al=o(gc,"SPAN",{});var _c=n(al);m(Jt.$$.fragment,_c),_c.forEach(t),gc.forEach(t),qp=f(lr),el=o(lr,"SPAN",{});var vc=n(el);Pp=p(vc,"Percent slicing and rounding"),vc.forEach(t),lr.forEach(t),tn=f(s),Va=o(s,"P",{});var $c=n(Va);Ap=p($c,"For datasets where the requested slice boundaries do not divide evenly by 100, the default behavior is to round the boundaries to the nearest integer. As a result, some slices may contain more examples than others as shown in the following example:"),$c.forEach(t),an=f(s),m(Ut.$$.fragment,s),en=f(s),Ys=o(s,"P",{});var or=n(Ys);Sp=p(or,"If you want equal sized splits, use "),ll=o(or,"CODE",{});var yc=n(ll);Tp=p(yc,"pct1_dropremainder"),yc.forEach(t),Dp=p(or," rounding instead. This will treat the specified percentage boundaries as multiples of 1%."),or.forEach(t),ln=f(s),m(Bt.$$.fragment,s),on=f(s),m(Ws.$$.fragment,s),nn=f(s),za=o(s,"A",{id:!0}),n(za).forEach(t),rn=f(s),fs=o(s,"H2",{class:!0});var nr=n(fs);Gs=o(nr,"A",{id:!0,class:!0,href:!0});var wc=n(Gs);ol=o(wc,"SPAN",{});var jc=n(ol);m(Yt.$$.fragment,jc),jc.forEach(t),wc.forEach(t),Cp=f(nr),nl=o(nr,"SPAN",{});var bc=n(nl);Np=p(bc,"Troubleshooting"),bc.forEach(t),nr.forEach(t),pn=f(s),Ja=o(s,"P",{});var xc=n(Ja);Ip=p(xc,"Sometimes, you may get unexpected results when you load a dataset. In this section, you will learn how to solve two common issues you may encounter when you load a dataset: manually download a dataset, and specify features of a dataset."),xc.forEach(t),dn=f(s),cs=o(s,"H3",{class:!0});var rr=n(cs);Qs=o(rr,"A",{id:!0,class:!0,href:!0});var kc=n(Qs);rl=o(kc,"SPAN",{});var Ec=n(rl);m(Wt.$$.fragment,Ec),Ec.forEach(t),kc.forEach(t),Op=f(rr),il=o(rr,"SPAN",{});var qc=n(il);Hp=p(qc,"Manual download"),qc.forEach(t),rr.forEach(t),fn=f(s),N=o(s,"P",{});var ot=n(N);Fp=p(ot,"Certain datasets require you to manually download the dataset files due to licensing incompatibility, or if the files are hidden behind a login page. This will cause "),Ua=o(ot,"A",{href:!0});var Pc=n(Ua);Lp=p(Pc,"datasets.load_dataset()"),Pc.forEach(t),Rp=p(ot," to throw an "),pl=o(ot,"CODE",{});var Ac=n(pl);Mp=p(Ac,"AssertionError"),Ac.forEach(t),Vp=p(ot,". But \u{1F917} Datasets provides detailed instructions for downloading the missing files. After you have downloaded the files, use the "),dl=o(ot,"CODE",{});var Sc=n(dl);zp=p(Sc,"data_dir"),Sc.forEach(t),Jp=p(ot," argument to specify the path to the files you just downloaded."),ot.forEach(t),cn=f(s),Xs=o(s,"P",{});var ir=n(Xs);Up=p(ir,"For example, if you try to download a configuration from the "),Gt=o(ir,"A",{href:!0,rel:!0});var Tc=n(Gt);Bp=p(Tc,"MATINF"),Tc.forEach(t),Yp=p(ir," dataset:"),ir.forEach(t),hn=f(s),m(Qt.$$.fragment,s),un=f(s),hs=o(s,"H3",{class:!0});var pr=n(hs);Ks=o(pr,"A",{id:!0,class:!0,href:!0});var Dc=n(Ks);fl=o(Dc,"SPAN",{});var Cc=n(fl);m(Xt.$$.fragment,Cc),Cc.forEach(t),Dc.forEach(t),Wp=f(pr),cl=o(pr,"SPAN",{});var Nc=n(cl);Gp=p(Nc,"Specify features"),Nc.forEach(t),pr.forEach(t),mn=f(s),B=o(s,"P",{});var re=n(B);Qp=p(re,"When you create a dataset from local files, the "),Ba=o(re,"A",{href:!0});var Ic=n(Ba);Xp=p(Ic,"datasets.Features"),Ic.forEach(t),Kp=p(re," are automatically inferred by "),Kt=o(re,"A",{href:!0,rel:!0});var Oc=n(Kt);Zp=p(Oc,"Apache Arrow"),Oc.forEach(t),sd=p(re,". However, the features of the dataset may not always align with your expectations or you may want to define the features yourself."),re.forEach(t),gn=f(s),Y=o(s,"P",{});var ie=n(Y);td=p(ie,"The following example shows how you can add custom labels with "),Ya=o(ie,"A",{href:!0});var Hc=n(Ya);ad=p(Hc,"datasets.ClassLabel"),Hc.forEach(t),ed=p(ie,". First, define your own labels using the "),Wa=o(ie,"A",{href:!0});var Fc=n(Wa);ld=p(Fc,"datasets.Features"),Fc.forEach(t),od=p(ie," class:"),ie.forEach(t),_n=f(s),m(Zt.$$.fragment,s),vn=f(s),W=o(s,"P",{});var pe=n(W);nd=p(pe,"Next, specify the "),hl=o(pe,"CODE",{});var Lc=n(hl);rd=p(Lc,"features"),Lc.forEach(t),id=p(pe," argument in "),Ga=o(pe,"A",{href:!0});var Rc=n(Ga);pd=p(Rc,"datasets.load_dataset()"),Rc.forEach(t),dd=p(pe," with the features you just created:"),pe.forEach(t),$n=f(s),m(sa.$$.fragment,s),yn=f(s),Qa=o(s,"P",{});var Mc=n(Qa);fd=p(Mc,"Now when you look at your dataset features, you can see it uses the custom labels you defined:"),Mc.forEach(t),wn=f(s),m(ta.$$.fragment,s),jn=f(s),us=o(s,"H2",{class:!0});var dr=n(us);Zs=o(dr,"A",{id:!0,class:!0,href:!0});var Vc=n(Zs);ul=o(Vc,"SPAN",{});var zc=n(ul);m(aa.$$.fragment,zc),zc.forEach(t),Vc.forEach(t),cd=f(dr),ml=o(dr,"SPAN",{});var Jc=n(ml);hd=p(Jc,"Metrics"),Jc.forEach(t),dr.forEach(t),bn=f(s),Xa=o(s,"P",{});var Uc=n(Xa);ud=p(Uc,"When the metric you want to use is not supported by \u{1F917} Datasets, you can write and use your own metric script. Load your metric by providing the path to your local metric loading script:"),Uc.forEach(t),xn=f(s),m(ea.$$.fragment,s),kn=f(s),m(st.$$.fragment,s),En=f(s),ms=o(s,"H3",{class:!0});var fr=n(ms);tt=o(fr,"A",{id:!0,class:!0,href:!0});var Bc=n(tt);gl=o(Bc,"SPAN",{});var Yc=n(gl);m(la.$$.fragment,Yc),Yc.forEach(t),Bc.forEach(t),md=f(fr),_l=o(fr,"SPAN",{});var Wc=n(_l);gd=p(Wc,"Load configurations"),Wc.forEach(t),fr.forEach(t),qn=f(s),Ka=o(s,"P",{});var Gc=n(Ka);_d=p(Gc,"It is possible for a metric to have different configurations. The configurations are stored in the [\u2018datasets.Metric.config_name\u2019] attribute. When you load a metric, provide the configuration name as shown in the following:"),Gc.forEach(t),Pn=f(s),m(oa.$$.fragment,s),An=f(s),gs=o(s,"H3",{class:!0});var cr=n(gs);at=o(cr,"A",{id:!0,class:!0,href:!0});var Qc=n(at);vl=o(Qc,"SPAN",{});var Xc=n(vl);m(na.$$.fragment,Xc),Xc.forEach(t),Qc.forEach(t),vd=f(cr),$l=o(cr,"SPAN",{});var Kc=n($l);$d=p(Kc,"Distributed setup"),Kc.forEach(t),cr.forEach(t),Sn=f(s),Za=o(s,"P",{});var Zc=n(Za);yd=p(Zc,"When you work in a distributed or parallel processing environment, loading and computing a metric can be tricky because these processes are executed in parallel on separate subsets of the data. \u{1F917} Datasets supports distributed usage with a few additional arguments when you load a metric."),Zc.forEach(t),Tn=f(s),se=o(s,"P",{});var sh=n(se);wd=p(sh,"For example, imagine you are training and evaluating on eight parallel processes. Here\u2019s how you would load a metric in this distributed setting:"),sh.forEach(t),Dn=f(s),G=o(s,"OL",{});var de=n(G);yl=o(de,"LI",{});var th=n(yl);ra=o(th,"P",{});var hr=n(ra);jd=p(hr,"Define the total number of processes with the "),wl=o(hr,"CODE",{});var ah=n(wl);bd=p(ah,"num_process"),ah.forEach(t),xd=p(hr," argument."),hr.forEach(t),th.forEach(t),kd=f(de),jl=o(de,"LI",{});var eh=n(jl);_s=o(eh,"P",{});var fe=n(_s);Ed=p(fe,"Set the process "),bl=o(fe,"CODE",{});var lh=n(bl);qd=p(lh,"rank"),lh.forEach(t),Pd=p(fe," as an integer between zero and "),xl=o(fe,"CODE",{});var oh=n(xl);Ad=p(oh,"num_process - 1"),oh.forEach(t),Sd=p(fe,"."),fe.forEach(t),eh.forEach(t),Td=f(de),kl=o(de,"LI",{});var nh=n(kl);ia=o(nh,"P",{});var ur=n(ia);Dd=p(ur,"Load your metric with "),te=o(ur,"A",{href:!0});var rh=n(te);Cd=p(rh,"datasets.load_metric()"),rh.forEach(t),Nd=p(ur," with these arguments:"),ur.forEach(t),nh.forEach(t),de.forEach(t),Cn=f(s),m(pa.$$.fragment,s),Nn=f(s),m(et.$$.fragment,s),In=f(s),lt=o(s,"P",{});var mr=n(lt);Id=p(mr,"In some instances, you may be simultaneously running multiple independent distributed evaluations on the same server and files. To avoid any conflicts, it is important to provide an "),El=o(mr,"CODE",{});var ih=n(El);Od=p(ih,"experiment_id"),ih.forEach(t),Hd=p(mr," to distinguish the separate evaluations:"),mr.forEach(t),On=f(s),m(da.$$.fragment,s),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(yh)),c(b,"id","load"),c(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b,"href","#load"),c(y,"class","relative group"),c(ga,"id","load-from-the-hub"),c(bs,"id","hugging-face-hub"),c(bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bs,"href","#hugging-face-hub"),c(Z,"class","relative group"),c(_a,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),c(rt,"href","https://huggingface.co/datasets/lhoestq/demo1"),c(rt,"rel","nofollow"),c(ft,"href","https://huggingface.co/datasets/allenai/c4"),c(ft,"rel","nofollow"),c(As,"id","local-and-remote-files"),c(As,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(As,"href","#local-and-remote-files"),c(ss,"class","relative group"),c($a,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),c(Ss,"id","csv"),c(Ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ss,"href","#csv"),c(ts,"class","relative group"),c(Ts,"id","json"),c(Ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ts,"href","#json"),c(as,"class","relative group"),c(ka,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),c(Ns,"id","text-files"),c(Ns,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ns,"href","#text-files"),c(es,"class","relative group"),c(Is,"id","parquet"),c(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Is,"href","#parquet"),c(ls,"class","relative group"),c(Os,"id","inmemory-data"),c(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Os,"href","#inmemory-data"),c(os,"class","relative group"),c(Ca,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset"),c(Fs,"id","python-dictionary"),c(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fs,"href","#python-dictionary"),c(ns,"class","relative group"),c(Na,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.from_dict"),c(Rs,"id","pandas-dataframe"),c(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rs,"href","#pandas-dataframe"),c(rs,"class","relative group"),c(Ia,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.from_pandas"),c(zs,"id","offline"),c(zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zs,"href","#offline"),c(is,"class","relative group"),c(Js,"id","slice-splits"),c(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Js,"href","#slice-splits"),c(ps,"class","relative group"),c(Ha,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Fa,"href","/docs/datasets/master/en/package_reference/builder_classes#datasets.ReadInstruction"),c(Bs,"id","percent-slicing-and-rounding"),c(Bs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bs,"href","#percent-slicing-and-rounding"),c(ds,"class","relative group"),c(za,"id","troubleshoot"),c(Gs,"id","troubleshooting"),c(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gs,"href","#troubleshooting"),c(fs,"class","relative group"),c(Qs,"id","manual-download"),c(Qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qs,"href","#manual-download"),c(cs,"class","relative group"),c(Ua,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),c(Gt,"href","https://huggingface.co/datasets/matinf"),c(Gt,"rel","nofollow"),c(Ks,"id","specify-features"),c(Ks,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ks,"href","#specify-features"),c(hs,"class","relative group"),c(Ba,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Features"),c(Kt,"href","https://arrow.apache.org/docs/"),c(Kt,"rel","nofollow"),c(Ya,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.ClassLabel"),c(Wa,"href","/docs/datasets/master/en/package_reference/main_classes#datasets.Features"),c(Ga,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_dataset"),c(Zs,"id","metrics"),c(Zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zs,"href","#metrics"),c(us,"class","relative group"),c(tt,"id","load-configurations"),c(tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(tt,"href","#load-configurations"),c(ms,"class","relative group"),c(at,"id","distributed-setup"),c(at,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(at,"href","#distributed-setup"),c(gs,"class","relative group"),c(te,"href","/docs/datasets/master/en/package_reference/loading_methods#datasets.load_metric")},m(s,e){a(document.head,h),r(s,x,e),r(s,y,e),a(y,b),a(b,E),g(w,E,null),a(y,j),a(y,q),a(q,X),r(s,vs,e),r(s,F,e),a(F,K),r(s,$s,e),r(s,L,e),a(L,R),r(s,ys,e),r(s,A,e),a(A,H),a(H,S),a(A,ca),a(A,ws),a(ws,ha),a(A,ua),a(A,js),a(js,gr),a(A,_r),a(A,he),a(he,vr),a(A,$r),a(A,ue),a(ue,yr),r(s,Cl,e),r(s,ma,e),a(ma,wr),r(s,Nl,e),r(s,ga,e),r(s,Il,e),r(s,Z,e),a(Z,bs),a(bs,me),g(nt,me,null),a(Z,jr),a(Z,ge),a(ge,br),r(s,Ol,e),r(s,xs,e),a(xs,xr),a(xs,_e),a(_e,kr),a(xs,Er),r(s,Hl,e),r(s,M,e),a(M,qr),a(M,_a),a(_a,Pr),a(M,Ar),a(M,rt),a(rt,Sr),a(M,Tr),r(s,Fl,e),g(it,s,e),r(s,Ll,e),r(s,va,e),a(va,Dr),r(s,Rl,e),r(s,ks,e),a(ks,Cr),a(ks,ve),a(ve,Nr),a(ks,Ir),r(s,Ml,e),g(pt,s,e),r(s,Vl,e),g(Es,s,e),r(s,zl,e),r(s,T,e),a(T,Or),a(T,$e),a($e,Hr),a(T,Fr),a(T,ye),a(ye,Lr),a(T,Rr),a(T,we),a(we,Mr),a(T,Vr),a(T,je),a(je,zr),a(T,Jr),a(T,be),a(be,Ur),a(T,Br),r(s,Jl,e),g(dt,s,e),r(s,Ul,e),g(qs,s,e),r(s,Bl,e),r(s,V,e),a(V,Yr),a(V,xe),a(xe,Wr),a(V,Gr),a(V,ft),a(ft,Qr),a(V,Xr),r(s,Yl,e),g(ct,s,e),r(s,Wl,e),r(s,Ps,e),a(Ps,Kr),a(Ps,ke),a(ke,Zr),a(Ps,si),r(s,Gl,e),g(ht,s,e),r(s,Ql,e),r(s,ss,e),a(ss,As),a(As,Ee),g(ut,Ee,null),a(ss,ti),a(ss,qe),a(qe,ai),r(s,Xl,e),r(s,D,e),a(D,ei),a(D,Pe),a(Pe,li),a(D,oi),a(D,Ae),a(Ae,ni),a(D,ri),a(D,Se),a(Se,ii),a(D,pi),a(D,Te),a(Te,di),a(D,fi),a(D,$a),a($a,ci),a(D,hi),r(s,Kl,e),r(s,ts,e),a(ts,Ss),a(Ss,De),g(mt,De,null),a(ts,ui),a(ts,Ce),a(Ce,mi),r(s,Zl,e),r(s,ya,e),a(ya,gi),r(s,so,e),g(gt,s,e),r(s,to,e),r(s,wa,e),a(wa,_i),r(s,ao,e),g(_t,s,e),r(s,eo,e),r(s,ja,e),a(ja,vi),r(s,lo,e),g(vt,s,e),r(s,oo,e),r(s,ba,e),a(ba,$i),r(s,no,e),g($t,s,e),r(s,ro,e),r(s,xa,e),a(xa,yi),r(s,io,e),g(yt,s,e),r(s,po,e),r(s,as,e),a(as,Ts),a(Ts,Ne),g(wt,Ne,null),a(as,wi),a(as,Ie),a(Ie,ji),r(s,fo,e),r(s,Ds,e),a(Ds,bi),a(Ds,ka),a(ka,xi),a(Ds,ki),r(s,co,e),g(jt,s,e),r(s,ho,e),r(s,Ea,e),a(Ea,Ei),r(s,uo,e),g(bt,s,e),r(s,mo,e),r(s,Cs,e),a(Cs,qi),a(Cs,Oe),a(Oe,Pi),a(Cs,Ai),r(s,go,e),g(xt,s,e),r(s,_o,e),r(s,qa,e),a(qa,Si),r(s,vo,e),g(kt,s,e),r(s,$o,e),r(s,Pa,e),a(Pa,Ti),r(s,yo,e),r(s,es,e),a(es,Ns),a(Ns,He),g(Et,He,null),a(es,Di),a(es,Fe),a(Fe,Ci),r(s,wo,e),r(s,Aa,e),a(Aa,Ni),r(s,jo,e),g(qt,s,e),r(s,bo,e),r(s,Sa,e),a(Sa,Ii),r(s,xo,e),g(Pt,s,e),r(s,ko,e),r(s,ls,e),a(ls,Is),a(Is,Le),g(At,Le,null),a(ls,Oi),a(ls,Re),a(Re,Hi),r(s,Eo,e),r(s,Ta,e),a(Ta,Fi),r(s,qo,e),g(St,s,e),r(s,Po,e),r(s,Da,e),a(Da,Li),r(s,Ao,e),g(Tt,s,e),r(s,So,e),r(s,os,e),a(os,Os),a(Os,Me),g(Dt,Me,null),a(os,Ri),a(os,Ve),a(Ve,Mi),r(s,To,e),r(s,Hs,e),a(Hs,Vi),a(Hs,Ca),a(Ca,zi),a(Hs,Ji),r(s,Do,e),r(s,ns,e),a(ns,Fs),a(Fs,ze),g(Ct,ze,null),a(ns,Ui),a(ns,Je),a(Je,Bi),r(s,Co,e),r(s,Ls,e),a(Ls,Yi),a(Ls,Na),a(Na,Wi),a(Ls,Gi),r(s,No,e),g(Nt,s,e),r(s,Io,e),r(s,rs,e),a(rs,Rs),a(Rs,Ue),g(It,Ue,null),a(rs,Qi),a(rs,Be),a(Be,Xi),r(s,Oo,e),r(s,Ms,e),a(Ms,Ki),a(Ms,Ia),a(Ia,Zi),a(Ms,sp),r(s,Ho,e),g(Ot,s,e),r(s,Fo,e),g(Vs,s,e),r(s,Lo,e),r(s,is,e),a(is,zs),a(zs,Ye),g(Ht,Ye,null),a(is,tp),a(is,We),a(We,ap),r(s,Ro,e),r(s,Oa,e),a(Oa,ep),r(s,Mo,e),r(s,z,e),a(z,lp),a(z,Ge),a(Ge,op),a(z,np),a(z,Qe),a(Qe,rp),a(z,ip),r(s,Vo,e),r(s,ps,e),a(ps,Js),a(Js,Xe),g(Ft,Xe,null),a(ps,pp),a(ps,Ke),a(Ke,dp),r(s,zo,e),r(s,J,e),a(J,fp),a(J,Ha),a(Ha,cp),a(J,hp),a(J,Fa),a(Fa,up),a(J,mp),r(s,Jo,e),r(s,U,e),a(U,gp),a(U,Ze),a(Ze,_p),a(U,vp),a(U,sl),a(sl,$p),a(U,yp),r(s,Uo,e),g(Lt,s,e),r(s,Bo,e),r(s,Us,e),a(Us,wp),a(Us,tl),a(tl,jp),a(Us,bp),r(s,Yo,e),g(Rt,s,e),r(s,Wo,e),r(s,La,e),a(La,xp),r(s,Go,e),g(Mt,s,e),r(s,Qo,e),r(s,Ra,e),a(Ra,kp),r(s,Xo,e),g(Vt,s,e),r(s,Ko,e),r(s,Ma,e),a(Ma,Ep),r(s,Zo,e),g(zt,s,e),r(s,sn,e),r(s,ds,e),a(ds,Bs),a(Bs,al),g(Jt,al,null),a(ds,qp),a(ds,el),a(el,Pp),r(s,tn,e),r(s,Va,e),a(Va,Ap),r(s,an,e),g(Ut,s,e),r(s,en,e),r(s,Ys,e),a(Ys,Sp),a(Ys,ll),a(ll,Tp),a(Ys,Dp),r(s,ln,e),g(Bt,s,e),r(s,on,e),g(Ws,s,e),r(s,nn,e),r(s,za,e),r(s,rn,e),r(s,fs,e),a(fs,Gs),a(Gs,ol),g(Yt,ol,null),a(fs,Cp),a(fs,nl),a(nl,Np),r(s,pn,e),r(s,Ja,e),a(Ja,Ip),r(s,dn,e),r(s,cs,e),a(cs,Qs),a(Qs,rl),g(Wt,rl,null),a(cs,Op),a(cs,il),a(il,Hp),r(s,fn,e),r(s,N,e),a(N,Fp),a(N,Ua),a(Ua,Lp),a(N,Rp),a(N,pl),a(pl,Mp),a(N,Vp),a(N,dl),a(dl,zp),a(N,Jp),r(s,cn,e),r(s,Xs,e),a(Xs,Up),a(Xs,Gt),a(Gt,Bp),a(Xs,Yp),r(s,hn,e),g(Qt,s,e),r(s,un,e),r(s,hs,e),a(hs,Ks),a(Ks,fl),g(Xt,fl,null),a(hs,Wp),a(hs,cl),a(cl,Gp),r(s,mn,e),r(s,B,e),a(B,Qp),a(B,Ba),a(Ba,Xp),a(B,Kp),a(B,Kt),a(Kt,Zp),a(B,sd),r(s,gn,e),r(s,Y,e),a(Y,td),a(Y,Ya),a(Ya,ad),a(Y,ed),a(Y,Wa),a(Wa,ld),a(Y,od),r(s,_n,e),g(Zt,s,e),r(s,vn,e),r(s,W,e),a(W,nd),a(W,hl),a(hl,rd),a(W,id),a(W,Ga),a(Ga,pd),a(W,dd),r(s,$n,e),g(sa,s,e),r(s,yn,e),r(s,Qa,e),a(Qa,fd),r(s,wn,e),g(ta,s,e),r(s,jn,e),r(s,us,e),a(us,Zs),a(Zs,ul),g(aa,ul,null),a(us,cd),a(us,ml),a(ml,hd),r(s,bn,e),r(s,Xa,e),a(Xa,ud),r(s,xn,e),g(ea,s,e),r(s,kn,e),g(st,s,e),r(s,En,e),r(s,ms,e),a(ms,tt),a(tt,gl),g(la,gl,null),a(ms,md),a(ms,_l),a(_l,gd),r(s,qn,e),r(s,Ka,e),a(Ka,_d),r(s,Pn,e),g(oa,s,e),r(s,An,e),r(s,gs,e),a(gs,at),a(at,vl),g(na,vl,null),a(gs,vd),a(gs,$l),a($l,$d),r(s,Sn,e),r(s,Za,e),a(Za,yd),r(s,Tn,e),r(s,se,e),a(se,wd),r(s,Dn,e),r(s,G,e),a(G,yl),a(yl,ra),a(ra,jd),a(ra,wl),a(wl,bd),a(ra,xd),a(G,kd),a(G,jl),a(jl,_s),a(_s,Ed),a(_s,bl),a(bl,qd),a(_s,Pd),a(_s,xl),a(xl,Ad),a(_s,Sd),a(G,Td),a(G,kl),a(kl,ia),a(ia,Dd),a(ia,te),a(te,Cd),a(ia,Nd),r(s,Cn,e),g(pa,s,e),r(s,Nn,e),g(et,s,e),r(s,In,e),r(s,lt,e),a(lt,Id),a(lt,El),a(El,Od),a(lt,Hd),r(s,On,e),g(da,s,e),Hn=!0},p(s,[e]){const fa={};e&2&&(fa.$$scope={dirty:e,ctx:s}),Es.$set(fa);const ql={};e&2&&(ql.$$scope={dirty:e,ctx:s}),qs.$set(ql);const Pl={};e&2&&(Pl.$$scope={dirty:e,ctx:s}),Vs.$set(Pl);const Al={};e&2&&(Al.$$scope={dirty:e,ctx:s}),Ws.$set(Al);const Sl={};e&2&&(Sl.$$scope={dirty:e,ctx:s}),st.$set(Sl);const Tl={};e&2&&(Tl.$$scope={dirty:e,ctx:s}),et.$set(Tl)},i(s){Hn||(_(w.$$.fragment,s),_(nt.$$.fragment,s),_(it.$$.fragment,s),_(pt.$$.fragment,s),_(Es.$$.fragment,s),_(dt.$$.fragment,s),_(qs.$$.fragment,s),_(ct.$$.fragment,s),_(ht.$$.fragment,s),_(ut.$$.fragment,s),_(mt.$$.fragment,s),_(gt.$$.fragment,s),_(_t.$$.fragment,s),_(vt.$$.fragment,s),_($t.$$.fragment,s),_(yt.$$.fragment,s),_(wt.$$.fragment,s),_(jt.$$.fragment,s),_(bt.$$.fragment,s),_(xt.$$.fragment,s),_(kt.$$.fragment,s),_(Et.$$.fragment,s),_(qt.$$.fragment,s),_(Pt.$$.fragment,s),_(At.$$.fragment,s),_(St.$$.fragment,s),_(Tt.$$.fragment,s),_(Dt.$$.fragment,s),_(Ct.$$.fragment,s),_(Nt.$$.fragment,s),_(It.$$.fragment,s),_(Ot.$$.fragment,s),_(Vs.$$.fragment,s),_(Ht.$$.fragment,s),_(Ft.$$.fragment,s),_(Lt.$$.fragment,s),_(Rt.$$.fragment,s),_(Mt.$$.fragment,s),_(Vt.$$.fragment,s),_(zt.$$.fragment,s),_(Jt.$$.fragment,s),_(Ut.$$.fragment,s),_(Bt.$$.fragment,s),_(Ws.$$.fragment,s),_(Yt.$$.fragment,s),_(Wt.$$.fragment,s),_(Qt.$$.fragment,s),_(Xt.$$.fragment,s),_(Zt.$$.fragment,s),_(sa.$$.fragment,s),_(ta.$$.fragment,s),_(aa.$$.fragment,s),_(ea.$$.fragment,s),_(st.$$.fragment,s),_(la.$$.fragment,s),_(oa.$$.fragment,s),_(na.$$.fragment,s),_(pa.$$.fragment,s),_(et.$$.fragment,s),_(da.$$.fragment,s),Hn=!0)},o(s){v(w.$$.fragment,s),v(nt.$$.fragment,s),v(it.$$.fragment,s),v(pt.$$.fragment,s),v(Es.$$.fragment,s),v(dt.$$.fragment,s),v(qs.$$.fragment,s),v(ct.$$.fragment,s),v(ht.$$.fragment,s),v(ut.$$.fragment,s),v(mt.$$.fragment,s),v(gt.$$.fragment,s),v(_t.$$.fragment,s),v(vt.$$.fragment,s),v($t.$$.fragment,s),v(yt.$$.fragment,s),v(wt.$$.fragment,s),v(jt.$$.fragment,s),v(bt.$$.fragment,s),v(xt.$$.fragment,s),v(kt.$$.fragment,s),v(Et.$$.fragment,s),v(qt.$$.fragment,s),v(Pt.$$.fragment,s),v(At.$$.fragment,s),v(St.$$.fragment,s),v(Tt.$$.fragment,s),v(Dt.$$.fragment,s),v(Ct.$$.fragment,s),v(Nt.$$.fragment,s),v(It.$$.fragment,s),v(Ot.$$.fragment,s),v(Vs.$$.fragment,s),v(Ht.$$.fragment,s),v(Ft.$$.fragment,s),v(Lt.$$.fragment,s),v(Rt.$$.fragment,s),v(Mt.$$.fragment,s),v(Vt.$$.fragment,s),v(zt.$$.fragment,s),v(Jt.$$.fragment,s),v(Ut.$$.fragment,s),v(Bt.$$.fragment,s),v(Ws.$$.fragment,s),v(Yt.$$.fragment,s),v(Wt.$$.fragment,s),v(Qt.$$.fragment,s),v(Xt.$$.fragment,s),v(Zt.$$.fragment,s),v(sa.$$.fragment,s),v(ta.$$.fragment,s),v(aa.$$.fragment,s),v(ea.$$.fragment,s),v(st.$$.fragment,s),v(la.$$.fragment,s),v(oa.$$.fragment,s),v(na.$$.fragment,s),v(pa.$$.fragment,s),v(et.$$.fragment,s),v(da.$$.fragment,s),Hn=!1},d(s){t(h),s&&t(x),s&&t(y),$(w),s&&t(vs),s&&t(F),s&&t($s),s&&t(L),s&&t(ys),s&&t(A),s&&t(Cl),s&&t(ma),s&&t(Nl),s&&t(ga),s&&t(Il),s&&t(Z),$(nt),s&&t(Ol),s&&t(xs),s&&t(Hl),s&&t(M),s&&t(Fl),$(it,s),s&&t(Ll),s&&t(va),s&&t(Rl),s&&t(ks),s&&t(Ml),$(pt,s),s&&t(Vl),$(Es,s),s&&t(zl),s&&t(T),s&&t(Jl),$(dt,s),s&&t(Ul),$(qs,s),s&&t(Bl),s&&t(V),s&&t(Yl),$(ct,s),s&&t(Wl),s&&t(Ps),s&&t(Gl),$(ht,s),s&&t(Ql),s&&t(ss),$(ut),s&&t(Xl),s&&t(D),s&&t(Kl),s&&t(ts),$(mt),s&&t(Zl),s&&t(ya),s&&t(so),$(gt,s),s&&t(to),s&&t(wa),s&&t(ao),$(_t,s),s&&t(eo),s&&t(ja),s&&t(lo),$(vt,s),s&&t(oo),s&&t(ba),s&&t(no),$($t,s),s&&t(ro),s&&t(xa),s&&t(io),$(yt,s),s&&t(po),s&&t(as),$(wt),s&&t(fo),s&&t(Ds),s&&t(co),$(jt,s),s&&t(ho),s&&t(Ea),s&&t(uo),$(bt,s),s&&t(mo),s&&t(Cs),s&&t(go),$(xt,s),s&&t(_o),s&&t(qa),s&&t(vo),$(kt,s),s&&t($o),s&&t(Pa),s&&t(yo),s&&t(es),$(Et),s&&t(wo),s&&t(Aa),s&&t(jo),$(qt,s),s&&t(bo),s&&t(Sa),s&&t(xo),$(Pt,s),s&&t(ko),s&&t(ls),$(At),s&&t(Eo),s&&t(Ta),s&&t(qo),$(St,s),s&&t(Po),s&&t(Da),s&&t(Ao),$(Tt,s),s&&t(So),s&&t(os),$(Dt),s&&t(To),s&&t(Hs),s&&t(Do),s&&t(ns),$(Ct),s&&t(Co),s&&t(Ls),s&&t(No),$(Nt,s),s&&t(Io),s&&t(rs),$(It),s&&t(Oo),s&&t(Ms),s&&t(Ho),$(Ot,s),s&&t(Fo),$(Vs,s),s&&t(Lo),s&&t(is),$(Ht),s&&t(Ro),s&&t(Oa),s&&t(Mo),s&&t(z),s&&t(Vo),s&&t(ps),$(Ft),s&&t(zo),s&&t(J),s&&t(Jo),s&&t(U),s&&t(Uo),$(Lt,s),s&&t(Bo),s&&t(Us),s&&t(Yo),$(Rt,s),s&&t(Wo),s&&t(La),s&&t(Go),$(Mt,s),s&&t(Qo),s&&t(Ra),s&&t(Xo),$(Vt,s),s&&t(Ko),s&&t(Ma),s&&t(Zo),$(zt,s),s&&t(sn),s&&t(ds),$(Jt),s&&t(tn),s&&t(Va),s&&t(an),$(Ut,s),s&&t(en),s&&t(Ys),s&&t(ln),$(Bt,s),s&&t(on),$(Ws,s),s&&t(nn),s&&t(za),s&&t(rn),s&&t(fs),$(Yt),s&&t(pn),s&&t(Ja),s&&t(dn),s&&t(cs),$(Wt),s&&t(fn),s&&t(N),s&&t(cn),s&&t(Xs),s&&t(hn),$(Qt,s),s&&t(un),s&&t(hs),$(Xt),s&&t(mn),s&&t(B),s&&t(gn),s&&t(Y),s&&t(_n),$(Zt,s),s&&t(vn),s&&t(W),s&&t($n),$(sa,s),s&&t(yn),s&&t(Qa),s&&t(wn),$(ta,s),s&&t(jn),s&&t(us),$(aa),s&&t(bn),s&&t(Xa),s&&t(xn),$(ea,s),s&&t(kn),$(st,s),s&&t(En),s&&t(ms),$(la),s&&t(qn),s&&t(Ka),s&&t(Pn),$(oa,s),s&&t(An),s&&t(gs),$(na),s&&t(Sn),s&&t(Za),s&&t(Tn),s&&t(se),s&&t(Dn),s&&t(G),s&&t(Cn),$(pa,s),s&&t(Nn),$(et,s),s&&t(In),s&&t(lt),s&&t(On),$(da,s)}}}const yh={local:"load",sections:[{local:"hugging-face-hub",title:"Hugging Face Hub"},{local:"local-and-remote-files",sections:[{local:"csv",title:"CSV"},{local:"json",title:"JSON"},{local:"text-files",title:"Text files"},{local:"parquet",title:"Parquet"}],title:"Local and remote files"},{local:"inmemory-data",sections:[{local:"python-dictionary",title:"Python dictionary"},{local:"pandas-dataframe",title:"Pandas DataFrame"}],title:"In-memory data"},{local:"offline",title:"Offline"},{local:"slice-splits",sections:[{local:"percent-slicing-and-rounding",title:"Percent slicing and rounding"}],title:"Slice splits"},{local:"troubleshooting",sections:[{local:"manual-download",title:"Manual download"},{local:"specify-features",title:"Specify features"}],title:"Troubleshooting"},{local:"metrics",sections:[{local:"load-configurations",title:"Load configurations"},{local:"distributed-setup",title:"Distributed setup"}],title:"Metrics"}],title:"Load"};function wh(C,h,x){let{fw:y}=h;return C.$$set=b=>{"fw"in b&&x(0,y=b.fw)},[y]}class qh extends ph{constructor(h){super();dh(this,h,wh,$h,fh,{fw:0})}}export{qh as default,yh as metadata};
