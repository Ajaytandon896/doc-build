---
local: datasets.DatasetBuilder
title: Builder classes
---
<script>
import Tip from "./Tip.svelte";
import Youtube from "./Youtube.svelte";
import Docstring from "./Docstring.svelte";
import CodeBlock from "./CodeBlock.svelte";
import CodeBlockFw from "./CodeBlockFw.svelte";
import DocNotebookDropdown from "./DocNotebookDropdown.svelte";
import IconCopyLink from "./IconCopyLink.svelte";
export let fw: "pt" | "tf"
</script>
<h1 id="datasets.DatasetBuilder">Builder classes</h1>

ðŸ¤— Datasets relies on two main classes during the dataset building process: [datasets.DatasetBuilder](/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder) and [datasets.BuilderConfig](/docs/datasets/master/en/package_reference/builder_classes#datasets.BuilderConfig).

<div class="docstring">

<docstring><name>class datasets.DatasetBuilder</name><anchor>datasets.DatasetBuilder</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L170</source><parameters>[{"name": "cache_dir", "val": ": typing.Optional[str] = None"}, {"name": "name", "val": ": typing.Optional[str] = None"}, {"name": "hash", "val": ": typing.Optional[str] = None"}, {"name": "base_path", "val": ": typing.Optional[str] = None"}, {"name": "info", "val": ": typing.Optional[datasets.info.DatasetInfo] = None"}, {"name": "features", "val": ": typing.Optional[datasets.features.features.Features] = None"}, {"name": "use_auth_token", "val": ": typing.Union[str, bool, NoneType] = None"}, {"name": "namespace", "val": ": typing.Optional[str] = None"}, {"name": "data_files", "val": ": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"}, {"name": "data_dir", "val": ": typing.Optional[str] = None"}, {"name": "**config_kwargs", "val": ""}]</parameters></docstring>
Abstract base class for all datasets.

*DatasetBuilder* has 3 key methods:

- `datasets.DatasetBuilder.info`: Documents the dataset, including feature
  names, types, and shapes, version, splits, citation, etc.
- [datasets.DatasetBuilder.download_and_prepare()](/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare): Downloads the source data
  and writes it to disk.
- [datasets.DatasetBuilder.as_dataset()](/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset): Generates a *Dataset*.

**Configuration**: Some *DatasetBuilder*s expose multiple variants of the
dataset by defining a *datasets.BuilderConfig* subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `datasets.DatasetBuilder.builder_configs()`



<div class="docstring">
<docstring><name>as\_dataset</name><anchor>datasets.DatasetBuilder.as_dataset</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L733</source><parameters>[{"name": "split", "val": ": typing.Optional[datasets.splits.Split] = None"}, {"name": "run_post_process", "val": " = True"}, {"name": "ignore_verifications", "val": " = False"}, {"name": "in_memory", "val": " = False"}]</parameters><paramsdesc>- **split** (`datasets.Split`) -- Which subset of the data to return.
- **run_post_process** (bool, default=True) -- Whether to run post-processing dataset transforms and/or add
  indexes.
- **ignore_verifications** (bool, default=False) -- Whether to ignore the verifications of the
  downloaded/processed dataset information (checksums/size/splits/...).
- **in_memory** (bool, default=False) -- Whether to copy the data in-memory.</paramsdesc><paramgroups>0</paramgroups><retdesc>datasets.Dataset</retdesc></docstring>
Return a Dataset for the specified split.






</div>
<div class="docstring">
<docstring><name>download\_and\_prepare</name><anchor>datasets.DatasetBuilder.download_and_prepare</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L477</source><parameters>[{"name": "download_config", "val": ": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"}, {"name": "download_mode", "val": ": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"}, {"name": "ignore_verifications", "val": ": bool = False"}, {"name": "try_from_hf_gcs", "val": ": bool = True"}, {"name": "dl_manager", "val": ": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"}, {"name": "base_path", "val": ": typing.Optional[str] = None"}, {"name": "use_auth_token", "val": ": typing.Union[str, bool, NoneType] = None"}, {"name": "**download_and_prepare_kwargs", "val": ""}]</parameters><paramsdesc>- **download_config** ([DownloadConfig](/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadConfig), optional) -- specific download configuration parameters.
- **download_mode** ([DownloadMode](/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadMode), optional) -- select the download/generate mode - Default to `REUSE_DATASET_IF_EXISTS`
- **ignore_verifications** (`bool`) -- Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...)
- **save_infos** (`bool`) -- Save the dataset information (checksums/size/splits/...)
- **try_from_hf_gcs** (`bool`) -- If True, it will try to download the already prepared dataset from the Hf google cloud storage
- **dl_manager** ([DownloadManager](/docs/datasets/master/en/package_reference/builder_classes#datasets.DownloadManager), optional) -- specific Download Manger to use
- **base_path** (`str`, optional) -- base path for relative paths that are used to download files. This can be a remote url.
  If not specified, the value of the *base_path* attribute (*self.base_path*) will be used instead.
- **use_auth_token** (`Union[str, bool]`, optional) -- Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
  If True, will get token from ~/.huggingface.</paramsdesc><paramgroups>0</paramgroups></docstring>
Downloads and prepares dataset for reading.




</div>
<div class="docstring">
<docstring><name>get\_all\_exported\_dataset\_infos</name><anchor>datasets.DatasetBuilder.get_all_exported_dataset_infos</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L310</source><parameters>[]</parameters></docstring>
Empty dict if doesn't exist

</div>
<div class="docstring">
<docstring><name>get\_exported\_dataset\_info</name><anchor>datasets.DatasetBuilder.get_exported_dataset_info</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L318</source><parameters>[]</parameters></docstring>
Empty DatasetInfo if doesn't exist

</div>
<div class="docstring">
<docstring><name>get\_imported\_module\_dir</name><anchor>datasets.DatasetBuilder.get_imported_module_dir</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L472</source><parameters>[]</parameters></docstring>
Return the path of the module of this class or subclass.

</div></div>

<div class="docstring">

<docstring><name>class datasets.GeneratorBasedBuilder</name><anchor>datasets.GeneratorBasedBuilder</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1001</source><parameters>[{"name": "*args", "val": ""}, {"name": "writer_batch_size", "val": " = None"}, {"name": "**kwargs", "val": ""}]</parameters></docstring>
Base class for datasets with data generation based on dict generators.

`GeneratorBasedBuilder` is a convenience class that abstracts away much
of the data writing and reading of `DatasetBuilder`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`_split_generators`). See the method docstrings for details.


</div>

<div class="docstring">

<docstring><name>class datasets.BeamBasedBuilder</name><anchor>datasets.BeamBasedBuilder</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1158</source><parameters>[{"name": "*args", "val": ""}, {"name": "**kwargs", "val": ""}]</parameters></docstring>
Beam based Builder.

</div>

<div class="docstring">

<docstring><name>class datasets.ArrowBasedBuilder</name><anchor>datasets.ArrowBasedBuilder</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L1096</source><parameters>[{"name": "cache_dir", "val": ": typing.Optional[str] = None"}, {"name": "name", "val": ": typing.Optional[str] = None"}, {"name": "hash", "val": ": typing.Optional[str] = None"}, {"name": "base_path", "val": ": typing.Optional[str] = None"}, {"name": "info", "val": ": typing.Optional[datasets.info.DatasetInfo] = None"}, {"name": "features", "val": ": typing.Optional[datasets.features.features.Features] = None"}, {"name": "use_auth_token", "val": ": typing.Union[str, bool, NoneType] = None"}, {"name": "namespace", "val": ": typing.Optional[str] = None"}, {"name": "data_files", "val": ": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"}, {"name": "data_dir", "val": ": typing.Optional[str] = None"}, {"name": "**config_kwargs", "val": ""}]</parameters></docstring>
Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet).

</div>

<div class="docstring">

<docstring><name>class datasets.BuilderConfig</name><anchor>datasets.BuilderConfig</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L74</source><parameters>[{"name": "name", "val": ": str = 'default'"}, {"name": "version", "val": ": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"}, {"name": "data_dir", "val": ": typing.Optional[str] = None"}, {"name": "data_files", "val": ": typing.Optional[datasets.data_files.DataFilesDict] = None"}, {"name": "description", "val": ": typing.Optional[str] = None"}]</parameters><paramsdesc>- **name** (`str`, default `"default"`) --
- **version** ([Version](/docs/datasets/master/en/package_reference/builder_classes#datasets.Version) or `str`, optional) --
- **data_dir** (`str`, optional) --
- **data_files** (`str` or `Sequence` or `Mapping`, optional) -- Path(s) to source data file(s).
- **description** (`str`, optional) --</paramsdesc><paramgroups>0</paramgroups></docstring>
Base class for [DatasetBuilder](/docs/datasets/master/en/package_reference/builder_classes#datasets.DatasetBuilder) data configuration.

DatasetBuilder subclasses with data configuration options should subclass
[BuilderConfig](/docs/datasets/master/en/package_reference/builder_classes#datasets.BuilderConfig) and add their own properties.





<div class="docstring">
<docstring><name>create\_config\_id</name><anchor>datasets.BuilderConfig.create_config_id</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/builder.py#L113</source><parameters>[{"name": "config_kwargs", "val": ": dict"}, {"name": "custom_features", "val": ": typing.Optional[datasets.features.features.Features] = None"}]</parameters></docstring>

The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn't take into account:
- the config kwargs that can be used to overwrite attributes
- the custom features used to write the dataset
- the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.


</div></div>

<div class="docstring">

<docstring><name>class datasets.DownloadManager</name><anchor>datasets.DownloadManager</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L75</source><parameters>[{"name": "dataset_name", "val": ": typing.Optional[str] = None"}, {"name": "data_dir", "val": ": typing.Optional[str] = None"}, {"name": "download_config", "val": ": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"}, {"name": "base_path", "val": ": typing.Optional[str] = None"}]</parameters></docstring>



<div class="docstring">
<docstring><name>download</name><anchor>datasets.DownloadManager.download</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L188</source><parameters>[{"name": "url_or_urls", "val": ""}]</parameters><paramsdesc>url_or_urls -- url or `list`/`dict` of urls to download and extract. Each
url is a `str`.</paramsdesc><paramgroups>0</paramgroups><rettype>downloaded_path(s)</rettype><retdesc>`str`, The downloaded paths matching the given input
url_or_urls.</retdesc></docstring>
Download given url(s).








</div>
<div class="docstring">
<docstring><name>download\_and\_extract</name><anchor>datasets.DownloadManager.download_and_extract</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L319</source><parameters>[{"name": "url_or_urls", "val": ""}]</parameters><paramsdesc>url_or_urls -- url or `list`/`dict` of urls to download and extract. Each
url is a `str`.</paramsdesc><paramgroups>0</paramgroups><rettype>extracted_path(s)</rettype><retdesc>`str`, extracted paths of given URL(s).</retdesc></docstring>
Download and extract given url_or_urls.

Is roughly equivalent to:

```
extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))
```








</div>
<div class="docstring">
<docstring><name>download\_custom</name><anchor>datasets.DownloadManager.download_custom</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L147</source><parameters>[{"name": "url_or_urls", "val": ""}, {"name": "custom_download", "val": ""}]</parameters><paramsdesc>url_or_urls -- url or `list`/`dict` of urls to download and extract. Each
url is a `str`.
custom_download -- Callable with signature (src_url: str, dst_path: str) -> Any
as for example `tf.io.gfile.copy`, that lets you download from google storage</paramsdesc><paramgroups>0</paramgroups><rettype>downloaded_path(s)</rettype><retdesc>`str`, The downloaded paths matching the given input
url_or_urls.</retdesc></docstring>

Download given urls(s) by calling `custom_download`.








</div>
<div class="docstring">
<docstring><name>extract</name><anchor>datasets.DownloadManager.extract</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L289</source><parameters>[{"name": "path_or_paths", "val": ""}, {"name": "num_proc", "val": " = None"}]</parameters><paramsdesc>path_or_paths -- path or `list`/`dict` of path of file to extract. Each
path is a `str`.
num_proc -- Use multi-processing if `num_proc` > 1 and the length of
`path_or_paths` is larger than `num_proc`</paramsdesc><paramgroups>0</paramgroups><rettype>extracted_path(s)</rettype><retdesc>`str`, The extracted paths matching the given input
path_or_paths.</retdesc></docstring>
Extract given path(s).








</div>
<div class="docstring">
<docstring><name>iter\_archive</name><anchor>datasets.DownloadManager.iter_archive</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L239</source><parameters>[{"name": "path_or_buf", "val": ": typing.Union[str, _io.BufferedReader]"}]</parameters><paramsdesc>- **path_or_buf** (`str` or `io.BufferedReader`) -- Archive path or archive binary file object.</paramsdesc><paramgroups>0</paramgroups><yieldtype>`tuple`[``str`, `io.BufferedReader``]</yieldtype><yielddesc>2-tuple (path_within_archive, file_object).
File object is opened in binary mode.</yielddesc></docstring>
Iterate over files within an archive.








</div>
<div class="docstring">
<docstring><name>iter\_files</name><anchor>datasets.DownloadManager.iter_files</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L272</source><parameters>[{"name": "paths", "val": ""}]</parameters><paramsdesc>- **paths** (list) -- Root paths.</paramsdesc><paramgroups>0</paramgroups><yieldtype>str</yieldtype><yielddesc>File path.</yielddesc></docstring>
Iterate over file paths.








</div>
<div class="docstring">
<docstring><name>ship\_files\_with\_pipeline</name><anchor>datasets.DownloadManager.ship_files_with_pipeline</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L114</source><parameters>[{"name": "downloaded_path_or_paths", "val": ""}, {"name": "pipeline", "val": ""}]</parameters></docstring>

Ship the files using Beam FileSystems to the pipeline temp dir.


</div></div>

<div class="docstring">

<docstring><name>class datasets.DownloadMode</name><anchor>datasets.DownloadMode</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/download_manager.py#L44</source><parameters>[{"name": "value", "val": ""}, {"name": "names", "val": " = None"}, {"name": "module", "val": " = None"}, {"name": "qualname", "val": " = None"}, {"name": "type", "val": " = None"}, {"name": "start", "val": " = 1"}]</parameters></docstring>
`Enum` for how to treat pre-existing downloads and data.

The default mode is `REUSE_DATASET_IF_EXISTS`, which will reuse both
raw downloads and the prepared dataset if they exist.

The generations modes:

|                                     | Downloads | Dataset |
|-------------------------------------|-----------|---------|
| `REUSE_DATASET_IF_EXISTS` (default) | Reuse     | Reuse   |
| `REUSE_CACHE_IF_EXISTS`             | Reuse     | Fresh   |
| `FORCE_REDOWNLOAD`                  | Fresh     | Fresh   |



</div>

<div class="docstring">

<docstring><name>class datasets.SplitGenerator</name><anchor>datasets.SplitGenerator</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L549</source><parameters>[{"name": "name", "val": ": str"}, {"name": "gen_kwargs", "val": ": typing.Dict = <factory>"}]</parameters><paramsdesc>- **name** (str) -- Name of the Split for which the generator will
  create the examples.
  **gen_kwargs -- Keyword arguments to forward to the `DatasetBuilder._generate_examples` method
  of the builder.</paramsdesc><paramgroups>0</paramgroups></docstring>
Defines the split information for the generator.

This should be used as returned value of
`GeneratorBasedBuilder._split_generators()`
See `GeneratorBasedBuilder._split_generators()`for more info and example
of usage.




</div>

<div class="docstring">

<docstring><name>class datasets.Split</name><anchor>datasets.Split</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L387</source><parameters>[{"name": "name", "val": ""}]</parameters></docstring>
`Enum` for dataset splits.

Datasets are typically split into different subsets to be used at various
stages of training and evaluation.

- `TRAIN`: the training data.
- `VALIDATION`: the validation data. If present, this is typically used as
  evaluation data while iterating on a model (e.g. changing hyperparameters,
  model architecture, etc.).
- `TEST`: the testing data. This is the data to report metrics on. Typically
  you do not want to use this during model iteration as you may overfit to it.
- `ALL`: the union of all defined dataset splits.

Note: All splits, including compositions inherit from `datasets.SplitBase`

See the :doc:`guide on splits &amp;lt;/loading>` for more information.


</div>

<div class="docstring">

<docstring><name>class datasets.NamedSplit</name><anchor>datasets.NamedSplit</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L303</source><parameters>[{"name": "name", "val": ""}]</parameters></docstring>
Descriptor corresponding to a named split (train, test, ...).

Example:

```python
Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.
```

    Warning:
A split cannot be added twice, so the following will fail:

```python
split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error
```

Warning:
The slices can be applied only one time. So the following are valid:

```python
split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])
```

But not:

```python
train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])
```


</div>

<div class="docstring">

<docstring><name>class datasets.NamedSplitAll</name><anchor>datasets.NamedSplitAll</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/splits.py#L372</source><parameters>[]</parameters></docstring>
Split corresponding to the union of all defined dataset splits.

</div>

<div class="docstring">

<docstring><name>class datasets.ReadInstruction</name><anchor>datasets.ReadInstruction</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L457</source><parameters>[{"name": "split_name", "val": ""}, {"name": "rounding", "val": " = None"}, {"name": "from_", "val": " = None"}, {"name": "to", "val": " = None"}, {"name": "unit", "val": " = None"}]</parameters></docstring>
Reading instruction for a dataset.

Examples:

```python
# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])
```



<div class="docstring">
<docstring><name>from\_spec</name><anchor>datasets.ReadInstruction.from_spec</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L537</source><parameters>[{"name": "spec", "val": ""}]</parameters><paramsdesc>- **spec** (str) -- split(s) + optional slice(s) to read + optional rounding
  if percents are used as the slicing unit. A slice can be specified,
  using absolute numbers (int) or percentages (int). E.g.
  `test`: test split.
  `test + validation`: test split + validation split.
  `test[10:]`: test split, minus its first 10 records.
  `test[:10%]`: first 10% records of test split.
  `test[:20%](pct1_dropremainder)`: first 10% records, rounded with
  the `pct1_dropremainder` rounding.
  `test[:-5%]+train[40%:60%]`: first 95% of test + middle 20% of
  train.</paramsdesc><paramgroups>0</paramgroups><retdesc>ReadInstruction instance.</retdesc></docstring>
Creates a ReadInstruction instance out of a string spec.






</div>
<div class="docstring">
<docstring><name>to\_absolute</name><anchor>datasets.ReadInstruction.to_absolute</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/arrow_reader.py#L605</source><parameters>[{"name": "name2len", "val": ""}]</parameters><paramsdesc>name2len -- dict associating split names to number of examples.</paramsdesc><paramgroups>0</paramgroups><retdesc>list of _AbsoluteInstruction instances (corresponds to the + in spec).</retdesc></docstring>
Translate instruction into a list of absolute instructions.

Those absolute instructions are then to be added together.






</div></div>

<div class="docstring">

<docstring><name>class datasets.DownloadConfig</name><anchor>datasets.DownloadConfig</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/file_utils.py#L219</source><parameters>[{"name": "cache_dir", "val": ": typing.Union[pathlib.Path, str, NoneType] = None"}, {"name": "force_download", "val": ": bool = False"}, {"name": "resume_download", "val": ": bool = False"}, {"name": "local_files_only", "val": ": bool = False"}, {"name": "proxies", "val": ": typing.Optional[typing.Dict] = None"}, {"name": "user_agent", "val": ": typing.Optional[str] = None"}, {"name": "extract_compressed_file", "val": ": bool = False"}, {"name": "force_extract", "val": ": bool = False"}, {"name": "delete_extracted", "val": ": bool = False"}, {"name": "use_etag", "val": ": bool = True"}, {"name": "num_proc", "val": ": typing.Optional[int] = None"}, {"name": "max_retries", "val": ": int = 1"}, {"name": "use_auth_token", "val": ": typing.Union[bool, str, NoneType] = None"}, {"name": "ignore_url_params", "val": ": bool = False"}, {"name": "download_desc", "val": ": typing.Optional[str] = None"}]</parameters><paramsdesc>- **cache_dir** (`str` or `Path`, optional) -- Specify a cache directory to save the file to (overwrite the
  default cache dir).
- **force_download** (`bool`, default `False`) -- If True, re-dowload the file even if it's already cached in
  the cache dir.
- **resume_download** (`bool`, default `False`) -- If True, resume the download if incompletly recieved file is
  found.
- **proxies** (`dict`, optional) --
- **user_agent** (`str`, optional) -- Optional string or dict that will be appended to the user-agent on remote
  requests.
- **extract_compressed_file** (`bool`, default `False`) -- If True and the path point to a zip or tar file,
  extract the compressed file in a folder along the archive.
- **force_extract** (`bool`, default `False`) -- If True when extract_compressed_file is True and the archive
  was already extracted, re-extract the archive and override the folder where it was extracted.
- **delete_extracted** (`bool`, default `False`) -- Whether to delete (or keep) the extracted files.
- **use_etag** (`bool`, default `True`) -- Whether to use the ETag HTTP response header to validate the cached files.
- **num_proc** (`int`, optional) -- The number of processes to launch to download the files in parallel.
- **max_retries** (`int`, default `1`) -- The number of times to retry an HTTP request if it fails.
- **use_auth_token** (`str` or `bool`, optional) -- Optional string or boolean to use as Bearer token
  for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.
- **ignore_url_params** (`bool`, default `False`) -- Whether to strip all query parameters and #fragments from
  the download URL before using it for caching the file.
- **download_desc** (`str`, optional) -- A description to be displayed alongside with the progress bar while downloading the files.</paramsdesc><paramgroups>0</paramgroups></docstring>
Configuration for our cached path manager.




</div>

<div class="docstring">

<docstring><name>class datasets.Version</name><anchor>datasets.Version</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/version.py#L30</source><parameters>[{"name": "version_str", "val": ": str"}, {"name": "description", "val": ": typing.Optional[str] = None"}, {"name": "major", "val": ": typing.Union[str, int, NoneType] = None"}, {"name": "minor", "val": ": typing.Union[str, int, NoneType] = None"}, {"name": "patch", "val": ": typing.Union[str, int, NoneType] = None"}]</parameters><paramsdesc>- **version_str** (`str`) -- Eg: "1.2.3".
- **description** (`str`) -- A description of what is new in this version.
- **version_str** (`str`) -- Eg: "1.2.3".
- **description** (`str`) -- A description of what is new in this version.
- **major** (`str`) --
- **minor** (`str`) --
- **patch** (`str`) --</paramsdesc><paramgroups>0</paramgroups></docstring>
Dataset version MAJOR.MINOR.PATCH.









<div class="docstring">
<docstring><name>match</name><anchor>datasets.Version.match</anchor><source>https://github.com/huggingface/datasets/blob/master/src/datasets/utils/version.py#L92</source><parameters>[{"name": "other_version", "val": ""}]</parameters><paramsdesc>other_version -- string, of the form "x[.y[.x]]" where &amp;lcub;x,y,z} can be a
number or a wildcard.</paramsdesc><paramgroups>0</paramgroups></docstring>
Returns True if other_version matches.




</div></div>
