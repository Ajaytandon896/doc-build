---
local: cloud-storage
sections:
- local: listing-datasets
  sections:
  - local: google-cloud-storage
    title: Google Cloud Storage
  title: Listing datasets
- local: saving-datasets
  title: Saving datasets
- local: loading-datasets
  title: Loading datasets
title: Cloud storage
---
<script>
import Tip from "./Tip.svelte";
import Youtube from "./Youtube.svelte";
import Docstring from "./Docstring.svelte";
import CodeBlock from "./CodeBlock.svelte";
import CodeBlockFwV2 from "./CodeBlockFwV2.svelte";
import DocNotebookDropdown from "./DocNotebookDropdown.svelte";
import IconCopyLink from "./IconCopyLink.svelte";
export let fw: "pt" | "tf"
</script>
<h1 id="cloud-storage">Cloud storage</h1>

ðŸ¤— Datasets supports access to cloud storage providers through a S3 filesystem implementation: `datasets.filesystems.S3FileSystem`. You can save and load datasets from your Amazon S3 bucket in a Pythonic way. Take a look at the following table for other supported cloud storage providers:

| Storage provider     | Filesystem implementation                                     |
|----------------------|---------------------------------------------------------------|
| Amazon S3            | [s3fs](https://s3fs.readthedocs.io/en/latest/)                |
| Google Cloud Storage | [gcsfs](https://gcsfs.readthedocs.io/en/latest/)              |
| Azure DataLake       | [adl](https://github.com/dask/adlfs)                          |
| Azure Blob           | [abfs](https://github.com/dask/adlfs)                         |
| Dropbox              | [dropboxdrivefs](https://github.com/MarineChap/dropboxdrivefs)|
| Google Drive         | [gdrivefs](https://github.com/intake/gdrivefs)                |

This guide will show you how to save and load datasets with **s3fs** to a S3 bucket, but other filesystem implementations can be used similarly.

<h2 id="listing-datasets">Listing datasets</h2>

1. Install the S3 dependency with ðŸ¤— Datasets:

```
>>> pip install datasets[s3]
```

2. List files from a public S3 bucket with `s3.ls`:

```py
>>> import datasets
>>> s3 = datasets.filesystems.S3FileSystem(anon=True)  
>>> s3.ls('public-datasets/imdb/train')
['dataset_info.json.json','dataset.arrow','state.json']
```

Access a private S3 bucket by entering your `aws_access_key_id` and `aws_secret_access_key`:

```py
>>> import datasets
>>> s3 = datasets.filesystems.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)  
>>> s3.ls('my-private-datasets/imdb/train')  
['dataset_info.json.json','dataset.arrow','state.json']
```

<h3 id="google-cloud-storage">Google Cloud Storage</h3>

Other filesystem implementations, like Google Cloud Storage, are used similarly:

1. Install the Google Cloud Storage implementation:

```
>>> conda install -c conda-forge gcsfs
# or install with pip
>>> pip install gcsfs
```

2. Load your dataset:

```py
>>> import gcsfs
>>> gcs = gcsfs.GCSFileSystem(project='my-google-project') 

>>> # saves encoded_dataset to your s3 bucket
>>> encoded_dataset.save_to_disk('gcs://my-private-datasets/imdb/train', fs=gcs)
```

<h2 id="saving-datasets">Saving datasets</h2>

After you have processed your dataset, you can save it to S3 with [datasets.Dataset.save_to_disk()](/docs/datasets/master/en/package_reference/main_classes#datasets.Dataset.save_to_disk):

```py
>>> from datasets.filesystems import S3FileSystem

>>> # create S3FileSystem instance
>>> s3 = S3FileSystem(anon=True)  

>>> # saves encoded_dataset to your s3 bucket
>>> encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train', fs=s3)
```

<Tip>

Remember to include your `aws_access_key_id` and `aws_secret_access_key` whenever you are interacting with a private S3 bucket.

</Tip>

Save your dataset with `botocore.session.Session` and a custom AWS profile:

```py
>>> import botocore
>>> from datasets.filesystems import S3FileSystem

>>> # creates a botocore session with the provided AWS profile
>>> s3_session = botocore.session.Session(profile='my_profile_name')

>>> # create S3FileSystem instance with s3_session
>>> s3 = S3FileSystem(session=s3_session)  

>>> # saves encoded_dataset to your s3 bucket
>>> encoded_dataset.save_to_disk('s3://my-private-datasets/imdb/train',fs=s3)
```

<h2 id="loading-datasets">Loading datasets</h2>

When you are ready to use your dataset again, reload it with `datasets.load_from_disk`:

```py
>>> from datasets import load_from_disk
>>> from datasets.filesystems import S3FileSystem

>>> # create S3FileSystem without credentials
>>> s3 = S3FileSystem(anon=True)  

>>> # load encoded_dataset to from s3 bucket
>>> dataset = load_from_disk('s3://a-public-datasets/imdb/train',fs=s3)  

>>> print(len(dataset))
>>> # 25000
```

Load with `botocore.session.Session` and custom AWS profile:

```py
>>> import botocore
>>> from datasets.filesystems import S3FileSystem

>>> # create S3FileSystem instance with aws_access_key_id and aws_secret_access_key
>>> s3_session = botocore.session.Session(profile='my_profile_name')

>>> # create S3FileSystem instance with s3_session
>>> s3 = S3FileSystem(session=s3_session)

>>> # load encoded_dataset to from s3 bucket
>>> dataset = load_from_disk('s3://my-private-datasets/imdb/train',fs=s3)  

>>> print(len(dataset))
>>> # 25000
```
