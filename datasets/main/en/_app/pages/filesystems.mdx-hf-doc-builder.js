import{S as Nn,i as On,s as In,e as a,k as d,w as c,t as i,M as Fn,c as o,d as e,m as f,a as r,x as h,h as p,b as u,G as s,g as n,y as m,q as g,o as _,B as v,v as xn}from"../chunks/vendor-hf-doc-builder.js";import{T as Hn}from"../chunks/Tip-hf-doc-builder.js";import{I as D}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as w}from"../chunks/CodeBlock-hf-doc-builder.js";function Bn(ks){let y,U,$,j,C,b,gt,L;return{c(){y=a("p"),U=i("Remember to define your credentials in your "),$=a("a"),j=i("FileSystem instance"),C=d(),b=a("code"),gt=i("fs"),L=i(" whenever you are interacting with a private cloud storage."),this.h()},l(z){y=o(z,"P",{});var E=r(y);U=p(E,"Remember to define your credentials in your "),$=o(E,"A",{href:!0});var A=r($);j=p(A,"FileSystem instance"),A.forEach(e),C=f(E),b=o(E,"CODE",{});var ie=r(b);gt=p(ie,"fs"),ie.forEach(e),L=p(E," whenever you are interacting with a private cloud storage."),E.forEach(e),this.h()},h(){u($,"href","#set-up-your-cloud-storage-filesystem")},m(z,E){n(z,y,E),s(y,U),s(y,$),s($,j),s(y,C),s(y,b),s(b,gt),s(y,L)},d(z){z&&e(y)}}}function Gn(ks){let y,U,$,j,C,b,gt,L,z,E,A,ie,be,Va,Xa,js,M,Ee,_t,ke,Za,to,je,eo,so,k,vt,qe,ao,oo,Se,yt,ro,lo,$t,Ae,no,io,De,wt,po,fo,bt,Te,uo,co,Pe,Et,ho,mo,kt,Ce,go,_o,Le,jt,vo,yo,qt,ze,$o,wo,Ne,St,bo,qs,pe,Eo,Ss,N,K,Oe,At,ko,Ie,jo,As,O,W,Fe,Dt,qo,xe,So,Ds,de,He,Ao,Ts,Tt,Ps,Pt,Be,Do,Cs,q,To,Ge,Po,Co,Re,Lo,zo,Ye,No,Oo,Ls,Ct,zs,Lt,Ue,Io,Ns,zt,Os,I,J,Me,Nt,Fo,Ke,xo,Is,fe,We,Ho,Fs,Ot,xs,It,Je,Bo,Hs,Ft,Bs,xt,Qe,Go,Gs,Ht,Rs,F,Q,Ve,Bt,Ro,Xe,Yo,Ys,ue,Ze,Uo,Us,Gt,Ms,Rt,ts,Mo,Ks,Yt,Ws,Ut,es,Ko,Js,Mt,Qs,x,V,ss,Kt,Wo,as,Jo,Vs,H,X,os,Wt,Qo,rs,Vo,Xs,S,Xo,ls,Zo,tr,ns,er,sr,is,ar,or,Zs,Z,rr,ps,lr,nr,ta,tt,et,ir,ds,pr,dr,ce,fr,ur,fs,cr,ea,st,hr,he,mr,gr,sa,Jt,aa,at,_r,me,vr,yr,oa,Qt,ra,ot,$r,ge,wr,br,la,Vt,na,rt,Er,us,kr,jr,ia,B,lt,cs,Xt,qr,hs,Sr,pa,_e,Ar,da,ve,Dr,fa,Zt,ua,nt,Tr,te,Pr,Cr,ca,G,it,ms,ee,Lr,gs,zr,ha,pt,Nr,ye,Or,Ir,ma,se,ga,dt,_a,R,ft,_s,ae,Fr,vs,xr,va,T,Hr,ys,Br,Gr,$s,Rr,Yr,ya,oe,$a,Y,ut,ws,re,Ur,bs,Mr,wa,ct,Kr,$e,Wr,Jr,ba,le,Ea;return b=new D({}),At=new D({}),Dt=new D({}),Tt=new w({props:{code:"pip install datasets[s3]",highlighted:'&gt;&gt;&gt; pip <span class="hljs-keyword">install </span>datasets[<span class="hljs-built_in">s3</span>]'}}),Ct=new w({props:{code:`storage_options = {"anon": True}  # for anonymous connection
storage_options = {"key": aws_access_key_id, "secret": aws_secret_access_key}  # for private buckets
import botocore
s3_session = botocore.session.Session(profile="my_profile_name")
storage_options = {"session": s3_session}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;anon&quot;</span>: <span class="hljs-literal">True</span>}  <span class="hljs-comment"># for anonymous connection</span>
<span class="hljs-comment"># or use your credentials</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;key&quot;</span>: aws_access_key_id, <span class="hljs-string">&quot;secret&quot;</span>: aws_secret_access_key}  <span class="hljs-comment"># for private buckets</span>
<span class="hljs-comment"># or use a botocore session</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> botocore
<span class="hljs-meta">&gt;&gt;&gt; </span>s3_session = botocore.session.Session(profile=<span class="hljs-string">&quot;my_profile_name&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;session&quot;</span>: s3_session}`}}),zt=new w({props:{code:`import s3fs
fs = s3fs.S3FileSystem(**storage_options)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> s3fs
<span class="hljs-meta">&gt;&gt;&gt; </span>fs = s3fs.S3FileSystem(**storage_options)`}}),Nt=new D({}),Ot=new w({props:{code:`conda install -c conda-forge gcsfs
pip install gcsfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge gcsfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install gcsfs</span>`}}),Ft=new w({props:{code:`storage_options={"token": "anon"}  # for anonymous connection
storage_options={"project": "my-google-project"}
storage_options={"project": "my-google-project", "token": TOKEN}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;token&quot;</span>: <span class="hljs-string">&quot;anon&quot;</span>}  <span class="hljs-comment"># for anonymous connection</span>
<span class="hljs-comment"># or use your credentials of your default gcloud credentials or from the google metadata service</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;project&quot;</span>: <span class="hljs-string">&quot;my-google-project&quot;</span>}
<span class="hljs-comment"># or use your credentials from elsewhere, see the documentation at https://gcsfs.readthedocs.io/</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;project&quot;</span>: <span class="hljs-string">&quot;my-google-project&quot;</span>, <span class="hljs-string">&quot;token&quot;</span>: TOKEN}`}}),Ht=new w({props:{code:`import gcsfs
fs = gcsfs.GCSFileSystem(**storage_options)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> gcsfs
<span class="hljs-meta">&gt;&gt;&gt; </span>fs = gcsfs.GCSFileSystem(**storage_options)`}}),Bt=new D({}),Gt=new w({props:{code:`conda install -c conda-forge adlfs
pip install adlfs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">conda install -c conda-forge adlfs</span>
# or install with pip
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">pip install adlfs</span>`}}),Yt=new w({props:{code:`storage_options = {"anon": True}  # for anonymous connection
storage_options = {"account_name": ACCOUNT_NAME, "account_key": ACCOUNT_KEY)  # gen 2 filesystem
storage_options={"tenant_id": TENANT_ID, "client_id": CLIENT_ID, "client_secret": CLIENT_SECRET}`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;anon&quot;</span>: <span class="hljs-literal">True</span>}  <span class="hljs-comment"># for anonymous connection</span>
<span class="hljs-comment"># or use your credentials</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options = {<span class="hljs-string">&quot;account_name&quot;</span>: ACCOUNT_NAME, <span class="hljs-string">&quot;account_key&quot;</span>: ACCOUNT_KEY)  <span class="hljs-comment"># gen 2 filesystem</span>
<span class="hljs-comment"># or use your credentials with the gen 1 filesystem</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>storage_options={<span class="hljs-string">&quot;tenant_id&quot;</span>: TENANT_ID, <span class="hljs-string">&quot;client_id&quot;</span>: CLIENT_ID, <span class="hljs-string">&quot;client_secret&quot;</span>: CLIENT_SECRET}`}}),Mt=new w({props:{code:`import adlfs
fs = adlfs.AzureBlobFileSystem(**storage_options)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> adlfs
<span class="hljs-meta">&gt;&gt;&gt; </span>fs = adlfs.AzureBlobFileSystem(**storage_options)`}}),Kt=new D({}),Wt=new D({}),Jt=new w({props:{code:`output_dir = "s3://my-bucket/imdb"
builder = load_dataset_builder("imdb")
builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>output_dir = <span class="hljs-string">&quot;s3://my-bucket/imdb&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;imdb&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),Qt=new w({props:{code:`output_dir = "s3://my-bucket/imdb"
builder = load_dataset_builder("path/to/local/loading_script/loading_script.py")
builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>output_dir = <span class="hljs-string">&quot;s3://my-bucket/imdb&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;path/to/local/loading_script/loading_script.py&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),Vt=new w({props:{code:`data_files = {"train": ["path/to/train.csv"]}
output_dir = "s3://my-bucket/imdb"
builder = load_dataset_builder("csv", data_files=data_files)
builder.download_and_prepare(output_dir, storage_options=storage_options, file_format="parquet")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&quot;train&quot;</span>: [<span class="hljs-string">&quot;path/to/train.csv&quot;</span>]}
<span class="hljs-meta">&gt;&gt;&gt; </span>output_dir = <span class="hljs-string">&quot;s3://my-bucket/imdb&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>builder = load_dataset_builder(<span class="hljs-string">&quot;csv&quot;</span>, data_files=data_files)
<span class="hljs-meta">&gt;&gt;&gt; </span>builder.download_and_prepare(output_dir, storage_options=storage_options, file_format=<span class="hljs-string">&quot;parquet&quot;</span>)`}}),Xt=new D({}),Zt=new w({props:{code:`import dask.dataframe as dd

df = dd.read_parquet(output_dir, storage_options=storage_options)

# or if your dataset is split into train/valid/test
df_train = dd.read_parquet(output_dir + f"/{builder.name}-train-*.parquet", storage_options=storage_options)
df_valid = dd.read_parquet(output_dir + f"/{builder.name}-validation-*.parquet", storage_options=storage_options)
df_test = dd.read_parquet(output_dir + f"/{builder.name}-test-*.parquet", storage_options=storage_options)`,highlighted:`<span class="hljs-keyword">import</span> dask.dataframe <span class="hljs-keyword">as</span> dd

df = dd.read_parquet(output_dir, storage_options=storage_options)

<span class="hljs-comment"># or if your dataset is split into train/valid/test</span>
df_train = dd.read_parquet(output_dir + <span class="hljs-string">f&quot;/<span class="hljs-subst">{builder.name}</span>-train-*.parquet&quot;</span>, storage_options=storage_options)
df_valid = dd.read_parquet(output_dir + <span class="hljs-string">f&quot;/<span class="hljs-subst">{builder.name}</span>-validation-*.parquet&quot;</span>, storage_options=storage_options)
df_test = dd.read_parquet(output_dir + <span class="hljs-string">f&quot;/<span class="hljs-subst">{builder.name}</span>-test-*.parquet&quot;</span>, storage_options=storage_options)`}}),ee=new D({}),se=new w({props:{code:`encoded_dataset.save_to_disk("s3://my-private-datasets/imdb/train", fs=fs)
encoded_dataset.save_to_disk("gcs://my-private-datasets/imdb/train", fs=fs)
encoded_dataset.save_to_disk("adl://my-private-datasets/imdb/train", fs=fs)`,highlighted:`<span class="hljs-comment"># saves encoded_dataset to amazon s3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;s3://my-private-datasets/imdb/train&quot;</span>, fs=fs)
<span class="hljs-comment"># saves encoded_dataset to google cloud storage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;gcs://my-private-datasets/imdb/train&quot;</span>, fs=fs)
<span class="hljs-comment"># saves encoded_dataset to microsoft azure blob/datalake</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dataset.save_to_disk(<span class="hljs-string">&quot;adl://my-private-datasets/imdb/train&quot;</span>, fs=fs)`}}),dt=new Hn({props:{$$slots:{default:[Bn]},$$scope:{ctx:ks}}}),ae=new D({}),oe=new w({props:{code:'fs.ls("my-private-datasets/imdb/train")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>fs.ls(<span class="hljs-string">&quot;my-private-datasets/imdb/train&quot;</span>)
[<span class="hljs-string">&quot;dataset_info.json.json&quot;</span>,<span class="hljs-string">&quot;dataset.arrow&quot;</span>,<span class="hljs-string">&quot;state.json&quot;</span>]`}}),re=new D({}),le=new w({props:{code:`from datasets import load_from_disk
dataset = load_from_disk("s3://a-public-datasets/imdb/train", fs=fs)  
print(len(dataset))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk
<span class="hljs-comment"># load encoded_dataset from cloud storage</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_from_disk(<span class="hljs-string">&quot;s3://a-public-datasets/imdb/train&quot;</span>, fs=fs)  
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(dataset))
<span class="hljs-number">25000</span>`}}),{c(){y=a("meta"),U=d(),$=a("h1"),j=a("a"),C=a("span"),c(b.$$.fragment),gt=d(),L=a("span"),z=i("Cloud storage"),E=d(),A=a("p"),ie=i("\u{1F917} Datasets supports access to cloud storage providers through a "),be=a("code"),Va=i("fsspec"),Xa=i(` FileSystem implementations.
You can save and load datasets from any cloud storage in a Pythonic way.
Take a look at the following table for some example of supported cloud storage providers:`),js=d(),M=a("table"),Ee=a("thead"),_t=a("tr"),ke=a("th"),Za=i("Storage provider"),to=d(),je=a("th"),eo=i("Filesystem implementation"),so=d(),k=a("tbody"),vt=a("tr"),qe=a("td"),ao=i("Amazon S3"),oo=d(),Se=a("td"),yt=a("a"),ro=i("s3fs"),lo=d(),$t=a("tr"),Ae=a("td"),no=i("Google Cloud Storage"),io=d(),De=a("td"),wt=a("a"),po=i("gcsfs"),fo=d(),bt=a("tr"),Te=a("td"),uo=i("Azure Blob/DataLake"),co=d(),Pe=a("td"),Et=a("a"),ho=i("adlfs"),mo=d(),kt=a("tr"),Ce=a("td"),go=i("Dropbox"),_o=d(),Le=a("td"),jt=a("a"),vo=i("dropboxdrivefs"),yo=d(),qt=a("tr"),ze=a("td"),$o=i("Google Drive"),wo=d(),Ne=a("td"),St=a("a"),bo=i("gdrivefs"),qs=d(),pe=a("p"),Eo=i(`This guide will show you how to save and load datasets with any cloud storage.
Here are examples for S3, Google Cloud Storage and Azure Blob Storage.`),Ss=d(),N=a("h2"),K=a("a"),Oe=a("span"),c(At.$$.fragment),ko=d(),Ie=a("span"),jo=i("Set up your cloud storage FileSystem"),As=d(),O=a("h3"),W=a("a"),Fe=a("span"),c(Dt.$$.fragment),qo=d(),xe=a("span"),So=i("Amazon S3"),Ds=d(),de=a("ol"),He=a("li"),Ao=i("Install the S3 dependency with \u{1F917} Datasets:"),Ts=d(),c(Tt.$$.fragment),Ps=d(),Pt=a("ol"),Be=a("li"),Do=i("Define your credentials"),Cs=d(),q=a("p"),To=i("To use an anonymous connection, use "),Ge=a("code"),Po=i("anon=True"),Co=i(`.
Otherwise, include your `),Re=a("code"),Lo=i("aws_access_key_id"),zo=i(" and "),Ye=a("code"),No=i("aws_secret_access_key"),Oo=i(" whenever you are interacting with a private S3 bucket."),Ls=d(),c(Ct.$$.fragment),zs=d(),Lt=a("ol"),Ue=a("li"),Io=i("Create your FileSystem instance"),Ns=d(),c(zt.$$.fragment),Os=d(),I=a("h3"),J=a("a"),Me=a("span"),c(Nt.$$.fragment),Fo=d(),Ke=a("span"),xo=i("Google Cloud Storage"),Is=d(),fe=a("ol"),We=a("li"),Ho=i("Install the Google Cloud Storage implementation:"),Fs=d(),c(Ot.$$.fragment),xs=d(),It=a("ol"),Je=a("li"),Bo=i("Define your credentials"),Hs=d(),c(Ft.$$.fragment),Bs=d(),xt=a("ol"),Qe=a("li"),Go=i("Create your FileSystem instance"),Gs=d(),c(Ht.$$.fragment),Rs=d(),F=a("h3"),Q=a("a"),Ve=a("span"),c(Bt.$$.fragment),Ro=d(),Xe=a("span"),Yo=i("Azure Blob Storage"),Ys=d(),ue=a("ol"),Ze=a("li"),Uo=i("Install the Azure Blob Storage implementation:"),Us=d(),c(Gt.$$.fragment),Ms=d(),Rt=a("ol"),ts=a("li"),Mo=i("Define your credentials"),Ks=d(),c(Yt.$$.fragment),Ws=d(),Ut=a("ol"),es=a("li"),Ko=i("Create your FileSystem instance"),Js=d(),c(Mt.$$.fragment),Qs=d(),x=a("h2"),V=a("a"),ss=a("span"),c(Kt.$$.fragment),Wo=d(),as=a("span"),Jo=i("Load and Save your datasets using your cloud storage FileSystem"),Vs=d(),H=a("h3"),X=a("a"),os=a("span"),c(Wt.$$.fragment),Qo=d(),rs=a("span"),Vo=i("Download and prepare a dataset into a cloud storage"),Xs=d(),S=a("p"),Xo=i("You can download and prepare a dataset into your cloud storage by specifying a remote "),ls=a("code"),Zo=i("output_dir"),tr=i(" in "),ns=a("code"),er=i("download_and_prepare"),sr=i(`.
Don\u2019t forget to use the previously defined `),is=a("code"),ar=i("storage_options"),or=i(" containing your credentials to write into a private cloud storage."),Zs=d(),Z=a("p"),rr=i("The "),ps=a("code"),lr=i("download_and_prepare"),nr=i(" method works in two steps:"),ta=d(),tt=a("ol"),et=a("li"),ir=i("it first downloads the raw data files (if any) in your local cache. You can set your cache directory by passing "),ds=a("code"),pr=i("cache_dir"),dr=i(" to "),ce=a("a"),fr=i("load_dataset_builder()"),ur=d(),fs=a("li"),cr=i("then it generates the dataset in Arrow or Parquet format in your cloud storage by iterating over the raw data files."),ea=d(),st=a("p"),hr=i("Load a dataset builder from the Hugging Face Hub (see "),he=a("a"),mr=i("how to load from the Hugging Face Hub"),gr=i("):"),sa=d(),c(Jt.$$.fragment),aa=d(),at=a("p"),_r=i("Load a dataset builder using a loading script (see "),me=a("a"),vr=i("how to load a local loading script"),yr=i("):"),oa=d(),c(Qt.$$.fragment),ra=d(),ot=a("p"),$r=i("Use your own data files (see "),ge=a("a"),wr=i("how to load local and remote files"),br=i("):"),la=d(),c(Vt.$$.fragment),na=d(),rt=a("p"),Er=i("It is highly recommended to save the files as compressed Parquet files to optimize I/O by specifying "),us=a("code"),kr=i('file_format="parquet"'),jr=i(`.
Otherwise the dataset is saved as an uncompressed Arrow file.`),ia=d(),B=a("h4"),lt=a("a"),cs=a("span"),c(Xt.$$.fragment),qr=d(),hs=a("span"),Sr=i("Dask"),pa=d(),_e=a("p"),Ar=i(`Dask is a parallel computing library and it has a pandas-like API for working with larger than memory Parquet datasets in parallel.
Dask can use multiple threads or processes on a single machine, or a cluster of machines to process data in parallel.
Dask supports local data but also data from a cloud storage.`),da=d(),ve=a("p"),Dr=i("Therefore you can load a dataset saved as sharded Parquet files in Dask with"),fa=d(),c(Zt.$$.fragment),ua=d(),nt=a("p"),Tr=i("You can find more about dask dataframes in their "),te=a("a"),Pr=i("documentation"),Cr=i("."),ca=d(),G=a("h2"),it=a("a"),ms=a("span"),c(ee.$$.fragment),Lr=d(),gs=a("span"),zr=i("Saving serialized datasets"),ha=d(),pt=a("p"),Nr=i("After you have processed your dataset, you can save it to your cloud storage with "),ye=a("a"),Or=i("Dataset.save_to_disk()"),Ir=i(":"),ma=d(),c(se.$$.fragment),ga=d(),c(dt.$$.fragment),_a=d(),R=a("h2"),ft=a("a"),_s=a("span"),c(ae.$$.fragment),Fr=d(),vs=a("span"),xr=i("Listing serialized datasets"),va=d(),T=a("p"),Hr=i("List files from a cloud storage with your FileSystem instance "),ys=a("code"),Br=i("fs"),Gr=i(", using "),$s=a("code"),Rr=i("fs.ls"),Yr=i(":"),ya=d(),c(oe.$$.fragment),$a=d(),Y=a("h3"),ut=a("a"),ws=a("span"),c(re.$$.fragment),Ur=d(),bs=a("span"),Mr=i("Load serialized datasets"),wa=d(),ct=a("p"),Kr=i("When you are ready to use your dataset again, reload it with "),$e=a("a"),Wr=i("Dataset.load_from_disk()"),Jr=i(":"),ba=d(),c(le.$$.fragment),this.h()},l(t){const l=Fn('[data-svelte="svelte-1phssyn"]',document.head);y=o(l,"META",{name:!0,content:!0}),l.forEach(e),U=f(t),$=o(t,"H1",{class:!0});var ne=r($);j=o(ne,"A",{id:!0,class:!0,href:!0});var Qr=r(j);C=o(Qr,"SPAN",{});var Vr=r(C);h(b.$$.fragment,Vr),Vr.forEach(e),Qr.forEach(e),gt=f(ne),L=o(ne,"SPAN",{});var Xr=r(L);z=p(Xr,"Cloud storage"),Xr.forEach(e),ne.forEach(e),E=f(t),A=o(t,"P",{});var ka=r(A);ie=p(ka,"\u{1F917} Datasets supports access to cloud storage providers through a "),be=o(ka,"CODE",{});var Zr=r(be);Va=p(Zr,"fsspec"),Zr.forEach(e),Xa=p(ka,` FileSystem implementations.
You can save and load datasets from any cloud storage in a Pythonic way.
Take a look at the following table for some example of supported cloud storage providers:`),ka.forEach(e),js=f(t),M=o(t,"TABLE",{});var ja=r(M);Ee=o(ja,"THEAD",{});var tl=r(Ee);_t=o(tl,"TR",{});var qa=r(_t);ke=o(qa,"TH",{});var el=r(ke);Za=p(el,"Storage provider"),el.forEach(e),to=f(qa),je=o(qa,"TH",{});var sl=r(je);eo=p(sl,"Filesystem implementation"),sl.forEach(e),qa.forEach(e),tl.forEach(e),so=f(ja),k=o(ja,"TBODY",{});var P=r(k);vt=o(P,"TR",{});var Sa=r(vt);qe=o(Sa,"TD",{});var al=r(qe);ao=p(al,"Amazon S3"),al.forEach(e),oo=f(Sa),Se=o(Sa,"TD",{});var ol=r(Se);yt=o(ol,"A",{href:!0,rel:!0});var rl=r(yt);ro=p(rl,"s3fs"),rl.forEach(e),ol.forEach(e),Sa.forEach(e),lo=f(P),$t=o(P,"TR",{});var Aa=r($t);Ae=o(Aa,"TD",{});var ll=r(Ae);no=p(ll,"Google Cloud Storage"),ll.forEach(e),io=f(Aa),De=o(Aa,"TD",{});var nl=r(De);wt=o(nl,"A",{href:!0,rel:!0});var il=r(wt);po=p(il,"gcsfs"),il.forEach(e),nl.forEach(e),Aa.forEach(e),fo=f(P),bt=o(P,"TR",{});var Da=r(bt);Te=o(Da,"TD",{});var pl=r(Te);uo=p(pl,"Azure Blob/DataLake"),pl.forEach(e),co=f(Da),Pe=o(Da,"TD",{});var dl=r(Pe);Et=o(dl,"A",{href:!0,rel:!0});var fl=r(Et);ho=p(fl,"adlfs"),fl.forEach(e),dl.forEach(e),Da.forEach(e),mo=f(P),kt=o(P,"TR",{});var Ta=r(kt);Ce=o(Ta,"TD",{});var ul=r(Ce);go=p(ul,"Dropbox"),ul.forEach(e),_o=f(Ta),Le=o(Ta,"TD",{});var cl=r(Le);jt=o(cl,"A",{href:!0,rel:!0});var hl=r(jt);vo=p(hl,"dropboxdrivefs"),hl.forEach(e),cl.forEach(e),Ta.forEach(e),yo=f(P),qt=o(P,"TR",{});var Pa=r(qt);ze=o(Pa,"TD",{});var ml=r(ze);$o=p(ml,"Google Drive"),ml.forEach(e),wo=f(Pa),Ne=o(Pa,"TD",{});var gl=r(Ne);St=o(gl,"A",{href:!0,rel:!0});var _l=r(St);bo=p(_l,"gdrivefs"),_l.forEach(e),gl.forEach(e),Pa.forEach(e),P.forEach(e),ja.forEach(e),qs=f(t),pe=o(t,"P",{});var vl=r(pe);Eo=p(vl,`This guide will show you how to save and load datasets with any cloud storage.
Here are examples for S3, Google Cloud Storage and Azure Blob Storage.`),vl.forEach(e),Ss=f(t),N=o(t,"H2",{class:!0});var Ca=r(N);K=o(Ca,"A",{id:!0,class:!0,href:!0});var yl=r(K);Oe=o(yl,"SPAN",{});var $l=r(Oe);h(At.$$.fragment,$l),$l.forEach(e),yl.forEach(e),ko=f(Ca),Ie=o(Ca,"SPAN",{});var wl=r(Ie);jo=p(wl,"Set up your cloud storage FileSystem"),wl.forEach(e),Ca.forEach(e),As=f(t),O=o(t,"H3",{class:!0});var La=r(O);W=o(La,"A",{id:!0,class:!0,href:!0});var bl=r(W);Fe=o(bl,"SPAN",{});var El=r(Fe);h(Dt.$$.fragment,El),El.forEach(e),bl.forEach(e),qo=f(La),xe=o(La,"SPAN",{});var kl=r(xe);So=p(kl,"Amazon S3"),kl.forEach(e),La.forEach(e),Ds=f(t),de=o(t,"OL",{});var jl=r(de);He=o(jl,"LI",{});var ql=r(He);Ao=p(ql,"Install the S3 dependency with \u{1F917} Datasets:"),ql.forEach(e),jl.forEach(e),Ts=f(t),h(Tt.$$.fragment,t),Ps=f(t),Pt=o(t,"OL",{start:!0});var Sl=r(Pt);Be=o(Sl,"LI",{});var Al=r(Be);Do=p(Al,"Define your credentials"),Al.forEach(e),Sl.forEach(e),Cs=f(t),q=o(t,"P",{});var ht=r(q);To=p(ht,"To use an anonymous connection, use "),Ge=o(ht,"CODE",{});var Dl=r(Ge);Po=p(Dl,"anon=True"),Dl.forEach(e),Co=p(ht,`.
Otherwise, include your `),Re=o(ht,"CODE",{});var Tl=r(Re);Lo=p(Tl,"aws_access_key_id"),Tl.forEach(e),zo=p(ht," and "),Ye=o(ht,"CODE",{});var Pl=r(Ye);No=p(Pl,"aws_secret_access_key"),Pl.forEach(e),Oo=p(ht," whenever you are interacting with a private S3 bucket."),ht.forEach(e),Ls=f(t),h(Ct.$$.fragment,t),zs=f(t),Lt=o(t,"OL",{start:!0});var Cl=r(Lt);Ue=o(Cl,"LI",{});var Ll=r(Ue);Io=p(Ll,"Create your FileSystem instance"),Ll.forEach(e),Cl.forEach(e),Ns=f(t),h(zt.$$.fragment,t),Os=f(t),I=o(t,"H3",{class:!0});var za=r(I);J=o(za,"A",{id:!0,class:!0,href:!0});var zl=r(J);Me=o(zl,"SPAN",{});var Nl=r(Me);h(Nt.$$.fragment,Nl),Nl.forEach(e),zl.forEach(e),Fo=f(za),Ke=o(za,"SPAN",{});var Ol=r(Ke);xo=p(Ol,"Google Cloud Storage"),Ol.forEach(e),za.forEach(e),Is=f(t),fe=o(t,"OL",{});var Il=r(fe);We=o(Il,"LI",{});var Fl=r(We);Ho=p(Fl,"Install the Google Cloud Storage implementation:"),Fl.forEach(e),Il.forEach(e),Fs=f(t),h(Ot.$$.fragment,t),xs=f(t),It=o(t,"OL",{start:!0});var xl=r(It);Je=o(xl,"LI",{});var Hl=r(Je);Bo=p(Hl,"Define your credentials"),Hl.forEach(e),xl.forEach(e),Hs=f(t),h(Ft.$$.fragment,t),Bs=f(t),xt=o(t,"OL",{start:!0});var Bl=r(xt);Qe=o(Bl,"LI",{});var Gl=r(Qe);Go=p(Gl,"Create your FileSystem instance"),Gl.forEach(e),Bl.forEach(e),Gs=f(t),h(Ht.$$.fragment,t),Rs=f(t),F=o(t,"H3",{class:!0});var Na=r(F);Q=o(Na,"A",{id:!0,class:!0,href:!0});var Rl=r(Q);Ve=o(Rl,"SPAN",{});var Yl=r(Ve);h(Bt.$$.fragment,Yl),Yl.forEach(e),Rl.forEach(e),Ro=f(Na),Xe=o(Na,"SPAN",{});var Ul=r(Xe);Yo=p(Ul,"Azure Blob Storage"),Ul.forEach(e),Na.forEach(e),Ys=f(t),ue=o(t,"OL",{});var Ml=r(ue);Ze=o(Ml,"LI",{});var Kl=r(Ze);Uo=p(Kl,"Install the Azure Blob Storage implementation:"),Kl.forEach(e),Ml.forEach(e),Us=f(t),h(Gt.$$.fragment,t),Ms=f(t),Rt=o(t,"OL",{start:!0});var Wl=r(Rt);ts=o(Wl,"LI",{});var Jl=r(ts);Mo=p(Jl,"Define your credentials"),Jl.forEach(e),Wl.forEach(e),Ks=f(t),h(Yt.$$.fragment,t),Ws=f(t),Ut=o(t,"OL",{start:!0});var Ql=r(Ut);es=o(Ql,"LI",{});var Vl=r(es);Ko=p(Vl,"Create your FileSystem instance"),Vl.forEach(e),Ql.forEach(e),Js=f(t),h(Mt.$$.fragment,t),Qs=f(t),x=o(t,"H2",{class:!0});var Oa=r(x);V=o(Oa,"A",{id:!0,class:!0,href:!0});var Xl=r(V);ss=o(Xl,"SPAN",{});var Zl=r(ss);h(Kt.$$.fragment,Zl),Zl.forEach(e),Xl.forEach(e),Wo=f(Oa),as=o(Oa,"SPAN",{});var tn=r(as);Jo=p(tn,"Load and Save your datasets using your cloud storage FileSystem"),tn.forEach(e),Oa.forEach(e),Vs=f(t),H=o(t,"H3",{class:!0});var Ia=r(H);X=o(Ia,"A",{id:!0,class:!0,href:!0});var en=r(X);os=o(en,"SPAN",{});var sn=r(os);h(Wt.$$.fragment,sn),sn.forEach(e),en.forEach(e),Qo=f(Ia),rs=o(Ia,"SPAN",{});var an=r(rs);Vo=p(an,"Download and prepare a dataset into a cloud storage"),an.forEach(e),Ia.forEach(e),Xs=f(t),S=o(t,"P",{});var mt=r(S);Xo=p(mt,"You can download and prepare a dataset into your cloud storage by specifying a remote "),ls=o(mt,"CODE",{});var on=r(ls);Zo=p(on,"output_dir"),on.forEach(e),tr=p(mt," in "),ns=o(mt,"CODE",{});var rn=r(ns);er=p(rn,"download_and_prepare"),rn.forEach(e),sr=p(mt,`.
Don\u2019t forget to use the previously defined `),is=o(mt,"CODE",{});var ln=r(is);ar=p(ln,"storage_options"),ln.forEach(e),or=p(mt," containing your credentials to write into a private cloud storage."),mt.forEach(e),Zs=f(t),Z=o(t,"P",{});var Fa=r(Z);rr=p(Fa,"The "),ps=o(Fa,"CODE",{});var nn=r(ps);lr=p(nn,"download_and_prepare"),nn.forEach(e),nr=p(Fa," method works in two steps:"),Fa.forEach(e),ta=f(t),tt=o(t,"OL",{});var xa=r(tt);et=o(xa,"LI",{});var Es=r(et);ir=p(Es,"it first downloads the raw data files (if any) in your local cache. You can set your cache directory by passing "),ds=o(Es,"CODE",{});var pn=r(ds);pr=p(pn,"cache_dir"),pn.forEach(e),dr=p(Es," to "),ce=o(Es,"A",{href:!0});var dn=r(ce);fr=p(dn,"load_dataset_builder()"),dn.forEach(e),Es.forEach(e),ur=f(xa),fs=o(xa,"LI",{});var fn=r(fs);cr=p(fn,"then it generates the dataset in Arrow or Parquet format in your cloud storage by iterating over the raw data files."),fn.forEach(e),xa.forEach(e),ea=f(t),st=o(t,"P",{});var Ha=r(st);hr=p(Ha,"Load a dataset builder from the Hugging Face Hub (see "),he=o(Ha,"A",{href:!0});var un=r(he);mr=p(un,"how to load from the Hugging Face Hub"),un.forEach(e),gr=p(Ha,"):"),Ha.forEach(e),sa=f(t),h(Jt.$$.fragment,t),aa=f(t),at=o(t,"P",{});var Ba=r(at);_r=p(Ba,"Load a dataset builder using a loading script (see "),me=o(Ba,"A",{href:!0});var cn=r(me);vr=p(cn,"how to load a local loading script"),cn.forEach(e),yr=p(Ba,"):"),Ba.forEach(e),oa=f(t),h(Qt.$$.fragment,t),ra=f(t),ot=o(t,"P",{});var Ga=r(ot);$r=p(Ga,"Use your own data files (see "),ge=o(Ga,"A",{href:!0});var hn=r(ge);wr=p(hn,"how to load local and remote files"),hn.forEach(e),br=p(Ga,"):"),Ga.forEach(e),la=f(t),h(Vt.$$.fragment,t),na=f(t),rt=o(t,"P",{});var Ra=r(rt);Er=p(Ra,"It is highly recommended to save the files as compressed Parquet files to optimize I/O by specifying "),us=o(Ra,"CODE",{});var mn=r(us);kr=p(mn,'file_format="parquet"'),mn.forEach(e),jr=p(Ra,`.
Otherwise the dataset is saved as an uncompressed Arrow file.`),Ra.forEach(e),ia=f(t),B=o(t,"H4",{class:!0});var Ya=r(B);lt=o(Ya,"A",{id:!0,class:!0,href:!0});var gn=r(lt);cs=o(gn,"SPAN",{});var _n=r(cs);h(Xt.$$.fragment,_n),_n.forEach(e),gn.forEach(e),qr=f(Ya),hs=o(Ya,"SPAN",{});var vn=r(hs);Sr=p(vn,"Dask"),vn.forEach(e),Ya.forEach(e),pa=f(t),_e=o(t,"P",{});var yn=r(_e);Ar=p(yn,`Dask is a parallel computing library and it has a pandas-like API for working with larger than memory Parquet datasets in parallel.
Dask can use multiple threads or processes on a single machine, or a cluster of machines to process data in parallel.
Dask supports local data but also data from a cloud storage.`),yn.forEach(e),da=f(t),ve=o(t,"P",{});var $n=r(ve);Dr=p($n,"Therefore you can load a dataset saved as sharded Parquet files in Dask with"),$n.forEach(e),fa=f(t),h(Zt.$$.fragment,t),ua=f(t),nt=o(t,"P",{});var Ua=r(nt);Tr=p(Ua,"You can find more about dask dataframes in their "),te=o(Ua,"A",{href:!0,rel:!0});var wn=r(te);Pr=p(wn,"documentation"),wn.forEach(e),Cr=p(Ua,"."),Ua.forEach(e),ca=f(t),G=o(t,"H2",{class:!0});var Ma=r(G);it=o(Ma,"A",{id:!0,class:!0,href:!0});var bn=r(it);ms=o(bn,"SPAN",{});var En=r(ms);h(ee.$$.fragment,En),En.forEach(e),bn.forEach(e),Lr=f(Ma),gs=o(Ma,"SPAN",{});var kn=r(gs);zr=p(kn,"Saving serialized datasets"),kn.forEach(e),Ma.forEach(e),ha=f(t),pt=o(t,"P",{});var Ka=r(pt);Nr=p(Ka,"After you have processed your dataset, you can save it to your cloud storage with "),ye=o(Ka,"A",{href:!0});var jn=r(ye);Or=p(jn,"Dataset.save_to_disk()"),jn.forEach(e),Ir=p(Ka,":"),Ka.forEach(e),ma=f(t),h(se.$$.fragment,t),ga=f(t),h(dt.$$.fragment,t),_a=f(t),R=o(t,"H2",{class:!0});var Wa=r(R);ft=o(Wa,"A",{id:!0,class:!0,href:!0});var qn=r(ft);_s=o(qn,"SPAN",{});var Sn=r(_s);h(ae.$$.fragment,Sn),Sn.forEach(e),qn.forEach(e),Fr=f(Wa),vs=o(Wa,"SPAN",{});var An=r(vs);xr=p(An,"Listing serialized datasets"),An.forEach(e),Wa.forEach(e),va=f(t),T=o(t,"P",{});var we=r(T);Hr=p(we,"List files from a cloud storage with your FileSystem instance "),ys=o(we,"CODE",{});var Dn=r(ys);Br=p(Dn,"fs"),Dn.forEach(e),Gr=p(we,", using "),$s=o(we,"CODE",{});var Tn=r($s);Rr=p(Tn,"fs.ls"),Tn.forEach(e),Yr=p(we,":"),we.forEach(e),ya=f(t),h(oe.$$.fragment,t),$a=f(t),Y=o(t,"H3",{class:!0});var Ja=r(Y);ut=o(Ja,"A",{id:!0,class:!0,href:!0});var Pn=r(ut);ws=o(Pn,"SPAN",{});var Cn=r(ws);h(re.$$.fragment,Cn),Cn.forEach(e),Pn.forEach(e),Ur=f(Ja),bs=o(Ja,"SPAN",{});var Ln=r(bs);Mr=p(Ln,"Load serialized datasets"),Ln.forEach(e),Ja.forEach(e),wa=f(t),ct=o(t,"P",{});var Qa=r(ct);Kr=p(Qa,"When you are ready to use your dataset again, reload it with "),$e=o(Qa,"A",{href:!0});var zn=r($e);Wr=p(zn,"Dataset.load_from_disk()"),zn.forEach(e),Jr=p(Qa,":"),Qa.forEach(e),ba=f(t),h(le.$$.fragment,t),this.h()},h(){u(y,"name","hf:doc:metadata"),u(y,"content",JSON.stringify(Rn)),u(j,"id","cloud-storage"),u(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(j,"href","#cloud-storage"),u($,"class","relative group"),u(yt,"href","https://s3fs.readthedocs.io/en/latest/"),u(yt,"rel","nofollow"),u(wt,"href","https://gcsfs.readthedocs.io/en/latest/"),u(wt,"rel","nofollow"),u(Et,"href","https://github.com/fsspec/adlfs"),u(Et,"rel","nofollow"),u(jt,"href","https://github.com/MarineChap/dropboxdrivefs"),u(jt,"rel","nofollow"),u(St,"href","https://github.com/intake/gdrivefs"),u(St,"rel","nofollow"),u(K,"id","set-up-your-cloud-storage-filesystem"),u(K,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(K,"href","#set-up-your-cloud-storage-filesystem"),u(N,"class","relative group"),u(W,"id","amazon-s3"),u(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(W,"href","#amazon-s3"),u(O,"class","relative group"),u(Pt,"start","2"),u(Lt,"start","3"),u(J,"id","google-cloud-storage"),u(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(J,"href","#google-cloud-storage"),u(I,"class","relative group"),u(It,"start","2"),u(xt,"start","3"),u(Q,"id","azure-blob-storage"),u(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(Q,"href","#azure-blob-storage"),u(F,"class","relative group"),u(Rt,"start","2"),u(Ut,"start","3"),u(V,"id","load-and-save-your-datasets-using-your-cloud-storage-filesystem"),u(V,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(V,"href","#load-and-save-your-datasets-using-your-cloud-storage-filesystem"),u(x,"class","relative group"),u(X,"id","download-and-prepare-a-dataset-into-a-cloud-storage"),u(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(X,"href","#download-and-prepare-a-dataset-into-a-cloud-storage"),u(H,"class","relative group"),u(ce,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset_builder"),u(he,"href","./loading#hugging-face-hub"),u(me,"href","./loading#local-loading-script"),u(ge,"href","./loading#local-and-remote-files"),u(lt,"id","dask"),u(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(lt,"href","#dask"),u(B,"class","relative group"),u(te,"href","https://docs.dask.org/en/stable/dataframe.html"),u(te,"rel","nofollow"),u(it,"id","saving-serialized-datasets"),u(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(it,"href","#saving-serialized-datasets"),u(G,"class","relative group"),u(ye,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.save_to_disk"),u(ft,"id","listing-serialized-datasets"),u(ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ft,"href","#listing-serialized-datasets"),u(R,"class","relative group"),u(ut,"id","load-serialized-datasets"),u(ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),u(ut,"href","#load-serialized-datasets"),u(Y,"class","relative group"),u($e,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.load_from_disk")},m(t,l){s(document.head,y),n(t,U,l),n(t,$,l),s($,j),s(j,C),m(b,C,null),s($,gt),s($,L),s(L,z),n(t,E,l),n(t,A,l),s(A,ie),s(A,be),s(be,Va),s(A,Xa),n(t,js,l),n(t,M,l),s(M,Ee),s(Ee,_t),s(_t,ke),s(ke,Za),s(_t,to),s(_t,je),s(je,eo),s(M,so),s(M,k),s(k,vt),s(vt,qe),s(qe,ao),s(vt,oo),s(vt,Se),s(Se,yt),s(yt,ro),s(k,lo),s(k,$t),s($t,Ae),s(Ae,no),s($t,io),s($t,De),s(De,wt),s(wt,po),s(k,fo),s(k,bt),s(bt,Te),s(Te,uo),s(bt,co),s(bt,Pe),s(Pe,Et),s(Et,ho),s(k,mo),s(k,kt),s(kt,Ce),s(Ce,go),s(kt,_o),s(kt,Le),s(Le,jt),s(jt,vo),s(k,yo),s(k,qt),s(qt,ze),s(ze,$o),s(qt,wo),s(qt,Ne),s(Ne,St),s(St,bo),n(t,qs,l),n(t,pe,l),s(pe,Eo),n(t,Ss,l),n(t,N,l),s(N,K),s(K,Oe),m(At,Oe,null),s(N,ko),s(N,Ie),s(Ie,jo),n(t,As,l),n(t,O,l),s(O,W),s(W,Fe),m(Dt,Fe,null),s(O,qo),s(O,xe),s(xe,So),n(t,Ds,l),n(t,de,l),s(de,He),s(He,Ao),n(t,Ts,l),m(Tt,t,l),n(t,Ps,l),n(t,Pt,l),s(Pt,Be),s(Be,Do),n(t,Cs,l),n(t,q,l),s(q,To),s(q,Ge),s(Ge,Po),s(q,Co),s(q,Re),s(Re,Lo),s(q,zo),s(q,Ye),s(Ye,No),s(q,Oo),n(t,Ls,l),m(Ct,t,l),n(t,zs,l),n(t,Lt,l),s(Lt,Ue),s(Ue,Io),n(t,Ns,l),m(zt,t,l),n(t,Os,l),n(t,I,l),s(I,J),s(J,Me),m(Nt,Me,null),s(I,Fo),s(I,Ke),s(Ke,xo),n(t,Is,l),n(t,fe,l),s(fe,We),s(We,Ho),n(t,Fs,l),m(Ot,t,l),n(t,xs,l),n(t,It,l),s(It,Je),s(Je,Bo),n(t,Hs,l),m(Ft,t,l),n(t,Bs,l),n(t,xt,l),s(xt,Qe),s(Qe,Go),n(t,Gs,l),m(Ht,t,l),n(t,Rs,l),n(t,F,l),s(F,Q),s(Q,Ve),m(Bt,Ve,null),s(F,Ro),s(F,Xe),s(Xe,Yo),n(t,Ys,l),n(t,ue,l),s(ue,Ze),s(Ze,Uo),n(t,Us,l),m(Gt,t,l),n(t,Ms,l),n(t,Rt,l),s(Rt,ts),s(ts,Mo),n(t,Ks,l),m(Yt,t,l),n(t,Ws,l),n(t,Ut,l),s(Ut,es),s(es,Ko),n(t,Js,l),m(Mt,t,l),n(t,Qs,l),n(t,x,l),s(x,V),s(V,ss),m(Kt,ss,null),s(x,Wo),s(x,as),s(as,Jo),n(t,Vs,l),n(t,H,l),s(H,X),s(X,os),m(Wt,os,null),s(H,Qo),s(H,rs),s(rs,Vo),n(t,Xs,l),n(t,S,l),s(S,Xo),s(S,ls),s(ls,Zo),s(S,tr),s(S,ns),s(ns,er),s(S,sr),s(S,is),s(is,ar),s(S,or),n(t,Zs,l),n(t,Z,l),s(Z,rr),s(Z,ps),s(ps,lr),s(Z,nr),n(t,ta,l),n(t,tt,l),s(tt,et),s(et,ir),s(et,ds),s(ds,pr),s(et,dr),s(et,ce),s(ce,fr),s(tt,ur),s(tt,fs),s(fs,cr),n(t,ea,l),n(t,st,l),s(st,hr),s(st,he),s(he,mr),s(st,gr),n(t,sa,l),m(Jt,t,l),n(t,aa,l),n(t,at,l),s(at,_r),s(at,me),s(me,vr),s(at,yr),n(t,oa,l),m(Qt,t,l),n(t,ra,l),n(t,ot,l),s(ot,$r),s(ot,ge),s(ge,wr),s(ot,br),n(t,la,l),m(Vt,t,l),n(t,na,l),n(t,rt,l),s(rt,Er),s(rt,us),s(us,kr),s(rt,jr),n(t,ia,l),n(t,B,l),s(B,lt),s(lt,cs),m(Xt,cs,null),s(B,qr),s(B,hs),s(hs,Sr),n(t,pa,l),n(t,_e,l),s(_e,Ar),n(t,da,l),n(t,ve,l),s(ve,Dr),n(t,fa,l),m(Zt,t,l),n(t,ua,l),n(t,nt,l),s(nt,Tr),s(nt,te),s(te,Pr),s(nt,Cr),n(t,ca,l),n(t,G,l),s(G,it),s(it,ms),m(ee,ms,null),s(G,Lr),s(G,gs),s(gs,zr),n(t,ha,l),n(t,pt,l),s(pt,Nr),s(pt,ye),s(ye,Or),s(pt,Ir),n(t,ma,l),m(se,t,l),n(t,ga,l),m(dt,t,l),n(t,_a,l),n(t,R,l),s(R,ft),s(ft,_s),m(ae,_s,null),s(R,Fr),s(R,vs),s(vs,xr),n(t,va,l),n(t,T,l),s(T,Hr),s(T,ys),s(ys,Br),s(T,Gr),s(T,$s),s($s,Rr),s(T,Yr),n(t,ya,l),m(oe,t,l),n(t,$a,l),n(t,Y,l),s(Y,ut),s(ut,ws),m(re,ws,null),s(Y,Ur),s(Y,bs),s(bs,Mr),n(t,wa,l),n(t,ct,l),s(ct,Kr),s(ct,$e),s($e,Wr),s(ct,Jr),n(t,ba,l),m(le,t,l),Ea=!0},p(t,[l]){const ne={};l&2&&(ne.$$scope={dirty:l,ctx:t}),dt.$set(ne)},i(t){Ea||(g(b.$$.fragment,t),g(At.$$.fragment,t),g(Dt.$$.fragment,t),g(Tt.$$.fragment,t),g(Ct.$$.fragment,t),g(zt.$$.fragment,t),g(Nt.$$.fragment,t),g(Ot.$$.fragment,t),g(Ft.$$.fragment,t),g(Ht.$$.fragment,t),g(Bt.$$.fragment,t),g(Gt.$$.fragment,t),g(Yt.$$.fragment,t),g(Mt.$$.fragment,t),g(Kt.$$.fragment,t),g(Wt.$$.fragment,t),g(Jt.$$.fragment,t),g(Qt.$$.fragment,t),g(Vt.$$.fragment,t),g(Xt.$$.fragment,t),g(Zt.$$.fragment,t),g(ee.$$.fragment,t),g(se.$$.fragment,t),g(dt.$$.fragment,t),g(ae.$$.fragment,t),g(oe.$$.fragment,t),g(re.$$.fragment,t),g(le.$$.fragment,t),Ea=!0)},o(t){_(b.$$.fragment,t),_(At.$$.fragment,t),_(Dt.$$.fragment,t),_(Tt.$$.fragment,t),_(Ct.$$.fragment,t),_(zt.$$.fragment,t),_(Nt.$$.fragment,t),_(Ot.$$.fragment,t),_(Ft.$$.fragment,t),_(Ht.$$.fragment,t),_(Bt.$$.fragment,t),_(Gt.$$.fragment,t),_(Yt.$$.fragment,t),_(Mt.$$.fragment,t),_(Kt.$$.fragment,t),_(Wt.$$.fragment,t),_(Jt.$$.fragment,t),_(Qt.$$.fragment,t),_(Vt.$$.fragment,t),_(Xt.$$.fragment,t),_(Zt.$$.fragment,t),_(ee.$$.fragment,t),_(se.$$.fragment,t),_(dt.$$.fragment,t),_(ae.$$.fragment,t),_(oe.$$.fragment,t),_(re.$$.fragment,t),_(le.$$.fragment,t),Ea=!1},d(t){e(y),t&&e(U),t&&e($),v(b),t&&e(E),t&&e(A),t&&e(js),t&&e(M),t&&e(qs),t&&e(pe),t&&e(Ss),t&&e(N),v(At),t&&e(As),t&&e(O),v(Dt),t&&e(Ds),t&&e(de),t&&e(Ts),v(Tt,t),t&&e(Ps),t&&e(Pt),t&&e(Cs),t&&e(q),t&&e(Ls),v(Ct,t),t&&e(zs),t&&e(Lt),t&&e(Ns),v(zt,t),t&&e(Os),t&&e(I),v(Nt),t&&e(Is),t&&e(fe),t&&e(Fs),v(Ot,t),t&&e(xs),t&&e(It),t&&e(Hs),v(Ft,t),t&&e(Bs),t&&e(xt),t&&e(Gs),v(Ht,t),t&&e(Rs),t&&e(F),v(Bt),t&&e(Ys),t&&e(ue),t&&e(Us),v(Gt,t),t&&e(Ms),t&&e(Rt),t&&e(Ks),v(Yt,t),t&&e(Ws),t&&e(Ut),t&&e(Js),v(Mt,t),t&&e(Qs),t&&e(x),v(Kt),t&&e(Vs),t&&e(H),v(Wt),t&&e(Xs),t&&e(S),t&&e(Zs),t&&e(Z),t&&e(ta),t&&e(tt),t&&e(ea),t&&e(st),t&&e(sa),v(Jt,t),t&&e(aa),t&&e(at),t&&e(oa),v(Qt,t),t&&e(ra),t&&e(ot),t&&e(la),v(Vt,t),t&&e(na),t&&e(rt),t&&e(ia),t&&e(B),v(Xt),t&&e(pa),t&&e(_e),t&&e(da),t&&e(ve),t&&e(fa),v(Zt,t),t&&e(ua),t&&e(nt),t&&e(ca),t&&e(G),v(ee),t&&e(ha),t&&e(pt),t&&e(ma),v(se,t),t&&e(ga),v(dt,t),t&&e(_a),t&&e(R),v(ae),t&&e(va),t&&e(T),t&&e(ya),v(oe,t),t&&e($a),t&&e(Y),v(re),t&&e(wa),t&&e(ct),t&&e(ba),v(le,t)}}}const Rn={local:"cloud-storage",sections:[{local:"set-up-your-cloud-storage-filesystem",sections:[{local:"amazon-s3",title:"Amazon S3"},{local:"google-cloud-storage",title:"Google Cloud Storage"},{local:"azure-blob-storage",title:"Azure Blob Storage"}],title:"Set up your cloud storage FileSystem"},{local:"load-and-save-your-datasets-using-your-cloud-storage-filesystem",sections:[{local:"download-and-prepare-a-dataset-into-a-cloud-storage",sections:[{local:"dask",title:"Dask"}],title:"Download and prepare a dataset into a cloud storage"}],title:"Load and Save your datasets using your cloud storage FileSystem"},{local:"saving-serialized-datasets",title:"Saving serialized datasets"},{local:"listing-serialized-datasets",sections:[{local:"load-serialized-datasets",title:"Load serialized datasets"}],title:"Listing serialized datasets"}],title:"Cloud storage"};function Yn(ks){return xn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Jn extends Nn{constructor(y){super();On(this,y,Yn,Gn,In,{})}}export{Jn as default,Rn as metadata};
