import{S as Kl,i as Zl,s as ed,e as o,k as m,w as y,t as l,M as td,c as n,d as a,m as f,a as r,x as b,h as d,b as v,G as t,g as u,y as w,q as x,o as k,B as E,v as ad,L as V}from"../../chunks/vendor-hf-doc-builder.js";import{T as Ci}from"../../chunks/Tip-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as A}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Q}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as U}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";import{D as Si}from"../../chunks/Deprecated-hf-doc-builder.js";function sd(j){let i,$,h,p,_;return p=new A({props:{code:`from datasets import list_datasets
list_datasets()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> list_datasets
<span class="hljs-meta">&gt;&gt;&gt; </span>list_datasets()
[<span class="hljs-string">&#x27;acronym_identification&#x27;</span>,
 <span class="hljs-string">&#x27;ade_corpus_v2&#x27;</span>,
 <span class="hljs-string">&#x27;adversarial_qa&#x27;</span>,
 <span class="hljs-string">&#x27;aeslc&#x27;</span>,
 <span class="hljs-string">&#x27;afrikaans_ner_corpus&#x27;</span>,
 <span class="hljs-string">&#x27;ag_news&#x27;</span>,
 ...
]`}}),{c(){i=o("p"),$=l("Example:"),h=m(),y(p.$$.fragment)},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Example:"),c.forEach(a),h=f(s),b(p.$$.fragment,s)},m(s,c){u(s,i,c),t(i,$),u(s,h,c),w(p,s,c),_=!0},p:V,i(s){_||(x(p.$$.fragment,s),_=!0)},o(s){k(p.$$.fragment,s),_=!1},d(s){s&&a(i),s&&a(h),E(p,s)}}}function od(j){let i,$,h,p,_;return{c(){i=o("p"),$=l("Passing "),h=o("em"),p=l("use_auth_token=True"),_=l(" is required when you want to access a private dataset.")},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Passing "),h=n(c,"EM",{});var N=r(h);p=d(N,"use_auth_token=True"),N.forEach(a),_=d(c," is required when you want to access a private dataset."),c.forEach(a)},m(s,c){u(s,i,c),t(i,$),t(i,h),t(h,p),t(i,_)},d(s){s&&a(i)}}}function nd(j){let i,$,h,p,_;return p=new A({props:{code:`from datasets import load_dataset
ds = load_dataset('rotten_tomatoes', split='train')

data_files = {'train': 'train.csv', 'test': 'test.csv'}
ds = load_dataset('namespace/your_dataset_name', data_files=data_files)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)

<span class="hljs-comment"># Map data files to splits</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>data_files = {<span class="hljs-string">&#x27;train&#x27;</span>: <span class="hljs-string">&#x27;train.csv&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>: <span class="hljs-string">&#x27;test.csv&#x27;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&#x27;namespace/your_dataset_name&#x27;</span>, data_files=data_files)`}}),{c(){i=o("p"),$=l("Load a dataset from the Hugging Face Hub:"),h=m(),y(p.$$.fragment)},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Load a dataset from the Hugging Face Hub:"),c.forEach(a),h=f(s),b(p.$$.fragment,s)},m(s,c){u(s,i,c),t(i,$),u(s,h,c),w(p,s,c),_=!0},p:V,i(s){_||(x(p.$$.fragment,s),_=!0)},o(s){k(p.$$.fragment,s),_=!1},d(s){s&&a(i),s&&a(h),E(p,s)}}}function rd(j){let i,$,h,p,_;return p=new A({props:{code:`from datasets import load_dataset
ds = load_dataset('csv', data_files='path/to/local/my_dataset.csv')

from datasets import load_dataset
ds = load_dataset('json', data_files='path/to/local/my_dataset.json')

from datasets import load_dataset
ds = load_dataset('path/to/local/loading_script/loading_script.py', split='train')`,highlighted:`<span class="hljs-comment"># Load a CSV file</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&#x27;csv&#x27;</span>, data_files=<span class="hljs-string">&#x27;path/to/local/my_dataset.csv&#x27;</span>)

<span class="hljs-comment"># Load a JSON file</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&#x27;json&#x27;</span>, data_files=<span class="hljs-string">&#x27;path/to/local/my_dataset.json&#x27;</span>)

<span class="hljs-comment"># Load from a local loading script</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&#x27;path/to/local/loading_script/loading_script.py&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)`}}),{c(){i=o("p"),$=l("Load a local dataset:"),h=m(),y(p.$$.fragment)},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Load a local dataset:"),c.forEach(a),h=f(s),b(p.$$.fragment,s)},m(s,c){u(s,i,c),t(i,$),u(s,h,c),w(p,s,c),_=!0},p:V,i(s){_||(x(p.$$.fragment,s),_=!0)},o(s){k(p.$$.fragment,s),_=!1},d(s){s&&a(i),s&&a(h),E(p,s)}}}function id(j){let i,$,h,p,_,s,c,N;return c=new A({props:{code:`from datasets import load_dataset
ds = load_dataset('rotten_tomatoes', split='train', streaming=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>, streaming=<span class="hljs-literal">True</span>)`}}),{c(){i=o("p"),$=l("Load an ["),h=o("em"),p=l("~datasets.IterableDataset"),_=l("]:"),s=m(),y(c.$$.fragment)},l(D){i=n(D,"P",{});var q=r(i);$=d(q,"Load an ["),h=n(q,"EM",{});var O=r(h);p=d(O,"~datasets.IterableDataset"),O.forEach(a),_=d(q,"]:"),q.forEach(a),s=f(D),b(c.$$.fragment,D)},m(D,q){u(D,i,q),t(i,$),t(i,h),t(h,p),t(i,_),u(D,s,q),w(c,D,q),N=!0},p:V,i(D){N||(x(c.$$.fragment,D),N=!0)},o(D){k(c.$$.fragment,D),N=!1},d(D){D&&a(i),D&&a(s),E(c,D)}}}function ld(j){let i,$,h,p,_,s,c,N;return c=new A({props:{code:`from datasets import load_dataset
ds = load_dataset('imagefolder', data_dir='/path/to/images', split='train')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&#x27;imagefolder&#x27;</span>, data_dir=<span class="hljs-string">&#x27;/path/to/images&#x27;</span>, split=<span class="hljs-string">&#x27;train&#x27;</span>)`}}),{c(){i=o("p"),$=l("Load an image dataset with the "),h=o("em"),p=l("ImageFolder"),_=l(" dataset builder:"),s=m(),y(c.$$.fragment)},l(D){i=n(D,"P",{});var q=r(i);$=d(q,"Load an image dataset with the "),h=n(q,"EM",{});var O=r(h);p=d(O,"ImageFolder"),O.forEach(a),_=d(q," dataset builder:"),q.forEach(a),s=f(D),b(c.$$.fragment,D)},m(D,q){u(D,i,q),t(i,$),t(i,h),t(h,p),t(i,_),u(D,s,q),w(c,D,q),N=!0},p:V,i(D){N||(x(c.$$.fragment,D),N=!0)},o(D){k(c.$$.fragment,D),N=!1},d(D){D&&a(i),D&&a(s),E(c,D)}}}function dd(j){let i,$,h,p,_;return p=new A({props:{code:`from datasets import load_from_disk
ds = load_from_disk('path/to/dataset/directory')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_from_disk
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_from_disk(<span class="hljs-string">&#x27;path/to/dataset/directory&#x27;</span>)`}}),{c(){i=o("p"),$=l("Example:"),h=m(),y(p.$$.fragment)},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Example:"),c.forEach(a),h=f(s),b(p.$$.fragment,s)},m(s,c){u(s,i,c),t(i,$),u(s,h,c),w(p,s,c),_=!0},p:V,i(s){_||(x(p.$$.fragment,s),_=!0)},o(s){k(p.$$.fragment,s),_=!1},d(s){s&&a(i),s&&a(h),E(p,s)}}}function cd(j){let i,$,h,p,_;return{c(){i=o("p"),$=l("Passing "),h=o("em"),p=l("use_auth_token=True"),_=l(" is required when you want to access a private dataset.")},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Passing "),h=n(c,"EM",{});var N=r(h);p=d(N,"use_auth_token=True"),N.forEach(a),_=d(c," is required when you want to access a private dataset."),c.forEach(a)},m(s,c){u(s,i,c),t(i,$),t(i,h),t(h,p),t(i,_)},d(s){s&&a(i)}}}function pd(j){let i,$,h,p,_;return p=new A({props:{code:`from datasets import load_dataset_builder
ds_builder = load_dataset_builder('rotten_tomatoes')
ds_builder.info.features`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset_builder
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder = load_dataset_builder(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds_builder.info.features
{<span class="hljs-string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="hljs-number">2</span>, names=[<span class="hljs-string">&#x27;neg&#x27;</span>, <span class="hljs-string">&#x27;pos&#x27;</span>], <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>),
 <span class="hljs-string">&#x27;text&#x27;</span>: Value(dtype=<span class="hljs-string">&#x27;string&#x27;</span>, <span class="hljs-built_in">id</span>=<span class="hljs-literal">None</span>)}`}}),{c(){i=o("p"),$=l("Example:"),h=m(),y(p.$$.fragment)},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Example:"),c.forEach(a),h=f(s),b(p.$$.fragment,s)},m(s,c){u(s,i,c),t(i,$),u(s,h,c),w(p,s,c),_=!0},p:V,i(s){_||(x(p.$$.fragment,s),_=!0)},o(s){k(p.$$.fragment,s),_=!1},d(s){s&&a(i),s&&a(h),E(p,s)}}}function md(j){let i,$,h,p,_;return p=new A({props:{code:`from datasets import get_dataset_config_names
get_dataset_config_names("glue")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> get_dataset_config_names
<span class="hljs-meta">&gt;&gt;&gt; </span>get_dataset_config_names(<span class="hljs-string">&quot;glue&quot;</span>)
[<span class="hljs-string">&#x27;cola&#x27;</span>,
 <span class="hljs-string">&#x27;sst2&#x27;</span>,
 <span class="hljs-string">&#x27;mrpc&#x27;</span>,
 <span class="hljs-string">&#x27;qqp&#x27;</span>,
 <span class="hljs-string">&#x27;stsb&#x27;</span>,
 <span class="hljs-string">&#x27;mnli&#x27;</span>,
 <span class="hljs-string">&#x27;mnli_mismatched&#x27;</span>,
 <span class="hljs-string">&#x27;mnli_matched&#x27;</span>,
 <span class="hljs-string">&#x27;qnli&#x27;</span>,
 <span class="hljs-string">&#x27;rte&#x27;</span>,
 <span class="hljs-string">&#x27;wnli&#x27;</span>,
 <span class="hljs-string">&#x27;ax&#x27;</span>]`}}),{c(){i=o("p"),$=l("Example:"),h=m(),y(p.$$.fragment)},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Example:"),c.forEach(a),h=f(s),b(p.$$.fragment,s)},m(s,c){u(s,i,c),t(i,$),u(s,h,c),w(p,s,c),_=!0},p:V,i(s){_||(x(p.$$.fragment,s),_=!0)},o(s){k(p.$$.fragment,s),_=!1},d(s){s&&a(i),s&&a(h),E(p,s)}}}function fd(j){let i,$,h,p,_;return p=new A({props:{code:`from datasets import get_dataset_infos
get_dataset_infos('rotten_tomatoes')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> get_dataset_infos
<span class="hljs-meta">&gt;&gt;&gt; </span>get_dataset_infos(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
{<span class="hljs-string">&#x27;default&#x27;</span>: DatasetInfo(description=<span class="hljs-string">&quot;Movie Review Dataset.
 is a dataset of containing 5,331 positive and 5,331 negative processed
ences from Rotten Tomatoes movie reviews...), ...}</span>`}}),{c(){i=o("p"),$=l("Example:"),h=m(),y(p.$$.fragment)},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Example:"),c.forEach(a),h=f(s),b(p.$$.fragment,s)},m(s,c){u(s,i,c),t(i,$),u(s,h,c),w(p,s,c),_=!0},p:V,i(s){_||(x(p.$$.fragment,s),_=!0)},o(s){k(p.$$.fragment,s),_=!1},d(s){s&&a(i),s&&a(h),E(p,s)}}}function gd(j){let i,$,h,p,_;return p=new A({props:{code:`from datasets import get_dataset_split_names
get_dataset_split_names('rotten_tomatoes')`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> get_dataset_split_names
<span class="hljs-meta">&gt;&gt;&gt; </span>get_dataset_split_names(<span class="hljs-string">&#x27;rotten_tomatoes&#x27;</span>)
[<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;validation&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>]`}}),{c(){i=o("p"),$=l("Example:"),h=m(),y(p.$$.fragment)},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Example:"),c.forEach(a),h=f(s),b(p.$$.fragment,s)},m(s,c){u(s,i,c),t(i,$),u(s,h,c),w(p,s,c),_=!0},p:V,i(s){_||(x(p.$$.fragment,s),_=!0)},o(s){k(p.$$.fragment,s),_=!1},d(s){s&&a(i),s&&a(h),E(p,s)}}}function hd(j){let i,$,h,p,_;return{c(){i=o("p"),$=l("Metrics is deprecated in \u{1F917} Datasets. To learn more about how to use metrics, take a look at the library \u{1F917} "),h=o("a"),p=l("Evaluate"),_=l("! In addition to metrics, you can find more tools for evaluating models and datasets."),this.h()},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Metrics is deprecated in \u{1F917} Datasets. To learn more about how to use metrics, take a look at the library \u{1F917} "),h=n(c,"A",{href:!0,rel:!0});var N=r(h);p=d(N,"Evaluate"),N.forEach(a),_=d(c,"! In addition to metrics, you can find more tools for evaluating models and datasets."),c.forEach(a),this.h()},h(){v(h,"href","https://huggingface.co/docs/evaluate/index"),v(h,"rel","nofollow")},m(s,c){u(s,i,c),t(i,$),t(i,h),t(h,p),t(i,_)},d(s){s&&a(i)}}}function ud(j){let i,$,h,p,_,s,c;return{c(){i=o("p"),$=l("Use "),h=o("em"),p=l("evaluate.list_evaluation_modules"),_=l(" instead, from the new library \u{1F917} Evaluate: "),s=o("a"),c=l("https://huggingface.co/docs/evaluate"),this.h()},l(N){i=n(N,"P",{});var D=r(i);$=d(D,"Use "),h=n(D,"EM",{});var q=r(h);p=d(q,"evaluate.list_evaluation_modules"),q.forEach(a),_=d(D," instead, from the new library \u{1F917} Evaluate: "),s=n(D,"A",{href:!0,rel:!0});var O=r(s);c=d(O,"https://huggingface.co/docs/evaluate"),O.forEach(a),D.forEach(a),this.h()},h(){v(s,"href","https://huggingface.co/docs/evaluate"),v(s,"rel","nofollow")},m(N,D){u(N,i,D),t(i,$),t(i,h),t(h,p),t(i,_),t(i,s),t(s,c)},d(N){N&&a(i)}}}function _d(j){let i,$,h,p,_;return p=new A({props:{code:`from datasets import list_metrics
list_metrics()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> list_metrics
<span class="hljs-meta">&gt;&gt;&gt; </span>list_metrics()
[<span class="hljs-string">&#x27;accuracy&#x27;</span>,
 <span class="hljs-string">&#x27;bertscore&#x27;</span>,
 <span class="hljs-string">&#x27;bleu&#x27;</span>,
 <span class="hljs-string">&#x27;bleurt&#x27;</span>,
 <span class="hljs-string">&#x27;cer&#x27;</span>,
 <span class="hljs-string">&#x27;chrf&#x27;</span>,
 ...
]`}}),{c(){i=o("p"),$=l("Example:"),h=m(),y(p.$$.fragment)},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Example:"),c.forEach(a),h=f(s),b(p.$$.fragment,s)},m(s,c){u(s,i,c),t(i,$),u(s,h,c),w(p,s,c),_=!0},p:V,i(s){_||(x(p.$$.fragment,s),_=!0)},o(s){k(p.$$.fragment,s),_=!1},d(s){s&&a(i),s&&a(h),E(p,s)}}}function vd(j){let i,$,h,p,_,s,c;return{c(){i=o("p"),$=l("Use "),h=o("em"),p=l("evaluate.load"),_=l(" instead, from the new library \u{1F917} Evaluate: "),s=o("a"),c=l("https://huggingface.co/docs/evaluate"),this.h()},l(N){i=n(N,"P",{});var D=r(i);$=d(D,"Use "),h=n(D,"EM",{});var q=r(h);p=d(q,"evaluate.load"),q.forEach(a),_=d(D," instead, from the new library \u{1F917} Evaluate: "),s=n(D,"A",{href:!0,rel:!0});var O=r(s);c=d(O,"https://huggingface.co/docs/evaluate"),O.forEach(a),D.forEach(a),this.h()},h(){v(s,"href","https://huggingface.co/docs/evaluate"),v(s,"rel","nofollow")},m(N,D){u(N,i,D),t(i,$),t(i,h),t(h,p),t(i,_),t(i,s),t(s,c)},d(N){N&&a(i)}}}function $d(j){let i,$,h,p,_;return p=new A({props:{code:`from datasets import load_metric
accuracy = load_metric('accuracy')
accuracy.compute(references=[1, 0], predictions=[1, 1])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_metric
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = load_metric(<span class="hljs-string">&#x27;accuracy&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy.compute(references=[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>], predictions=[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>])
{<span class="hljs-string">&#x27;accuracy&#x27;</span>: <span class="hljs-number">0.5</span>}`}}),{c(){i=o("p"),$=l("Example:"),h=m(),y(p.$$.fragment)},l(s){i=n(s,"P",{});var c=r(i);$=d(c,"Example:"),c.forEach(a),h=f(s),b(p.$$.fragment,s)},m(s,c){u(s,i,c),t(i,$),u(s,h,c),w(p,s,c),_=!0},p:V,i(s){_||(x(p.$$.fragment,s),_=!0)},o(s){k(p.$$.fragment,s),_=!1},d(s){s&&a(i),s&&a(h),E(p,s)}}}function yd(j){let i,$,h,p,_,s,c;return{c(){i=o("p"),$=l("Use "),h=o("em"),p=l("evaluate.inspect_evaluation_module"),_=l(" instead, from the new library \u{1F917} Evaluate instead: "),s=o("a"),c=l("https://huggingface.co/docs/evaluate"),this.h()},l(N){i=n(N,"P",{});var D=r(i);$=d(D,"Use "),h=n(D,"EM",{});var q=r(h);p=d(q,"evaluate.inspect_evaluation_module"),q.forEach(a),_=d(D," instead, from the new library \u{1F917} Evaluate instead: "),s=n(D,"A",{href:!0,rel:!0});var O=r(s);c=d(O,"https://huggingface.co/docs/evaluate"),O.forEach(a),D.forEach(a),this.h()},h(){v(s,"href","https://huggingface.co/docs/evaluate"),v(s,"rel","nofollow")},m(N,D){u(N,i,D),t(i,$),t(i,h),t(h,p),t(i,_),t(i,s),t(s,c)},d(N){N&&a(i)}}}function bd(j){let i,$,h,p,_,s,c,N,D,q,O,Po,js,W,ye,oa,tt,To,na,Io,Cs,M,at,Lo,ra,Ao,Ho,be,Ss,C,st,Uo,ia,Vo,Mo,K,Bo,ot,Ro,Jo,la,Yo,zo,Go,da,Xo,Qo,nt,ca,Wo,Ko,pa,Zo,en,ma,tn,an,fa,sn,on,Z,B,rt,nn,ga,rn,ln,dn,ha,cn,pn,ua,mn,fn,we,gn,it,hn,un,lt,_n,vn,dt,_a,$n,yn,ct,va,$a,bn,wn,pt,ya,xn,kn,ba,En,Dn,wa,mt,Nn,xa,jn,Cn,Sn,ka,qn,On,xe,Fn,Ea,Pn,Tn,ke,In,Ee,Ln,De,An,Ne,qs,R,ft,Hn,J,Un,Jt,Vn,Mn,Yt,Bn,Rn,Da,Jn,Yn,zn,je,Os,F,gt,Gn,Na,Xn,Qn,ee,Wn,ht,Kn,Zn,ja,er,tr,ar,Ca,sr,or,ut,Sa,nr,rr,qa,ir,lr,Oa,dr,cr,Ce,pr,Se,Fs,Y,_t,mr,Fa,fr,gr,qe,Ps,z,vt,hr,Pa,ur,_r,Oe,Ts,G,$t,vr,Ta,$r,yr,Fe,Is,te,yt,br,Ia,wr,Ls,ae,Pe,La,bt,xr,Aa,kr,As,Te,Hs,I,wt,Er,Ha,Dr,Nr,Ie,jr,Le,Us,L,xt,Cr,kt,Sr,Ua,qr,Or,Fr,Ae,Pr,He,Vs,X,Et,Tr,Va,Ir,Lr,Ue,Ms,se,Ve,Ma,Dt,Ar,Ba,Hr,Bs,zt,Ur,Rs,Me,Gt,Vr,Ra,Mr,Br,Xt,Rr,Ja,Jr,Js,H,Yr,Ya,zr,Gr,za,Xr,Qr,Qt,Wr,Kr,Ys,Nt,zs,oe,Be,Ga,jt,Zr,Xa,ei,Gs,ne,Ct,ti,Qa,ai,Xs,re,Re,Wa,St,si,Ka,oi,Qs,ie,qt,ni,Za,ri,Ws,le,Je,es,Ot,ii,ts,li,Ks,de,Ft,di,as,ci,Zs,ce,Ye,ss,Pt,pi,os,mi,eo,pe,Tt,fi,ns,gi,to,me,ze,rs,It,hi,is,ui,ao,fe,Lt,_i,ls,vi,so,ge,Ge,ds,At,$i,cs,yi,oo,he,Ht,bi,ps,wi,no,ue,Xe,ms,Ut,xi,fs,ki,ro,_e,Vt,Ei,gs,Di,io;return s=new Q({}),tt=new Q({}),at=new P({props:{name:"datasets.list_datasets",anchor:"datasets.list_datasets",parameters:[{name:"with_community_datasets",val:" = True"},{name:"with_details",val:" = False"}],parametersDescription:[{anchor:"datasets.list_datasets.with_community_datasets",description:"<strong>with_community_datasets</strong> (<code>bool</code>, optional, default <code>True</code>) &#x2014; Include the community provided datasets.",name:"with_community_datasets"},{anchor:"datasets.list_datasets.with_details",description:"<strong>with_details</strong> (<code>bool</code>, optional, default <code>False</code>) &#x2014; Return the full details on the datasets instead of only the short name.",name:"with_details"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/inspect.py#L44"}}),be=new U({props:{anchor:"datasets.list_datasets.example",$$slots:{default:[sd]},$$scope:{ctx:j}}}),st=new P({props:{name:"datasets.load_dataset",anchor:"datasets.load_dataset",parameters:[{name:"path",val:": str"},{name:"name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, typing.Sequence[str], typing.Mapping[str, typing.Union[str, typing.Sequence[str]]], NoneType] = None"},{name:"split",val:": typing.Union[str, datasets.splits.Split, NoneType] = None"},{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.download.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"keep_in_memory",val:": typing.Optional[bool] = None"},{name:"save_infos",val:": bool = False"},{name:"revision",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"task",val:": typing.Union[str, datasets.tasks.base.TaskTemplate, NoneType] = None"},{name:"streaming",val:": bool = False"},{name:"**config_kwargs",val:""}],parametersDescription:[{anchor:"datasets.load_dataset.path",description:`<strong>path</strong> (<code>str</code>) &#x2014; Path or name of the dataset.
Depending on <code>path</code>, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.</p>
<p>For local datasets:</p>
<ul>
<li>if <code>path</code> is a local directory (containing data files only)
-&gt; load a generic dataset builder (csv, json, text etc.) based on the content of the directory
e.g. <code>&apos;./path/to/directory/with/my/csv/data&apos;</code>.</li>
<li>if <code>path</code> is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory):
-&gt; load the dataset builder from the dataset script
e.g. <code>&apos;./dataset/squad&apos;</code> or <code>&apos;./dataset/squad/squad.py&apos;</code>.</li>
</ul>
<p>For datasets on the Hugging Face Hub (list all available datasets and ids with <code>datasets.list_datasets()</code>)</p>
<ul>
<li>if <code>path</code> is a dataset repository on the HF hub (containing data files only)
-&gt; load a generic dataset builder (csv, text etc.) based on the content of the repository
e.g. <code>&apos;username/dataset_name&apos;</code>, a dataset repository on the HF hub containing your data files.</li>
<li>if <code>path</code> is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)
-&gt; load the dataset builder from the dataset script in the dataset repository
e.g. <code>glue</code>, <code>squad</code>, <code>&apos;username/dataset_name&apos;</code>, a dataset repository on the HF hub containing a dataset script <em>&#x2018;dataset_name.py&#x2019;</em>.</li>
</ul>`,name:"path"},{anchor:"datasets.load_dataset.name",description:"<strong>name</strong> (<code>str</code>, optional) &#x2014; Defining the name of the dataset configuration.",name:"name"},{anchor:"datasets.load_dataset.data_dir",description:`<strong>data_dir</strong> (<code>str</code>, optional) &#x2014; Defining the data_dir of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and <em>data_files</em> is None,
the behavior is equal to passing <em>os.path.join(data_dir, **)</em> as <em>data_files</em> to reference all the files in a directory.`,name:"data_dir"},{anchor:"datasets.load_dataset.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.load_dataset.split",description:`<strong>split</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.Split">Split</a> or <code>str</code>) &#x2014; Which split of the data to load.
If None, will return a <em>dict</em> with all splits (typically <em>datasets.Split.TRAIN</em> and <em>datasets.Split.TEST</em>).
If given, will return a single Dataset.
Splits can be combined and specified like in tensorflow-datasets.`,name:"split"},{anchor:"datasets.load_dataset.cache_dir",description:"<strong>cache_dir</strong> (<code>str</code>, optional) &#x2014; Directory to read/write data. Defaults to &#x201D;~/.cache/huggingface/datasets&#x201D;.",name:"cache_dir"},{anchor:"datasets.load_dataset.features",description:'<strong>features</strong> (<a href="/docs/datasets/main/en/package_reference/main_classes#datasets.Features">Features</a>, optional) &#x2014; Set the features type to use for this dataset.',name:"features"},{anchor:"datasets.load_dataset.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; Specific download configuration parameters.',name:"download_config"},{anchor:"datasets.load_dataset.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, default <code>REUSE_DATASET_IF_EXISTS</code>) &#x2014; Download/generate mode.',name:"download_mode"},{anchor:"datasets.load_dataset.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;).",name:"ignore_verifications"},{anchor:"datasets.load_dataset.keep_in_memory",description:`<strong>keep_in_memory</strong> (<code>bool</code>, default <code>None</code>) &#x2014; Whether to copy the dataset in-memory. If <em>None</em>, the dataset
will not be copied in-memory unless explicitly enabled by setting <em>datasets.config.IN_MEMORY_MAX_SIZE</em> to
nonzero. See more details in the <a href="/docs/datasets/main/en/package_reference/loading_methods#load_dataset_enhancing_performance">load_dataset_enhancing_performance</a> section.`,name:"keep_in_memory"},{anchor:"datasets.load_dataset.save_infos",description:"<strong>save_infos</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Save the dataset information (checksums/size/splits/&#x2026;).",name:"save_infos"},{anchor:"datasets.load_dataset.revision",description:`<strong>revision</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014; Version of the dataset script to load:</p>
<ul>
<li>For datasets in the <em>huggingface/datasets</em> library on GitHub like &#x201C;squad&#x201D;, the default version of the module is the local version of the lib.
You can specify a different version from your local version of the lib (e.g. &#x201C;main&#x201D; or &#x201C;1.2.0&#x201D;) but it might cause compatibility issues.</li>
<li>For community datasets like &#x201C;lhoestq/squad&#x201D; that have their own git repository on the Datasets Hub, the default version &#x201C;main&#x201D; corresponds to the &#x201C;main&#x201D; branch.
You can specify a different version that the default &#x201C;main&#x201D; by using a commit sha or a git tag of the dataset repository.</li>
</ul>`,name:"revision"},{anchor:"datasets.load_dataset.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from <em>&#x201D;~/.huggingface&#x201D;</em>.`,name:"use_auth_token"},{anchor:"datasets.load_dataset.task",description:'<strong>task</strong> (<code>str</code>) &#x2014; The task to prepare the dataset for during training and evaluation. Casts the dataset&#x2019;s <a href="/docs/datasets/main/en/package_reference/main_classes#datasets.Features">Features</a> to standardized column names and types as detailed in :py:mod:<em>datasets.tasks</em>.',name:"task"},{anchor:"datasets.load_dataset.streaming",description:`<strong>streaming</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If set to True, don&#x2019;t download the data files. Instead, it streams the data progressively while
iterating on the dataset. An IterableDataset or IterableDatasetDict is returned instead in this case.</p>
<p>Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.
Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats
like rar and xz are not yet supported. The tgz format doesn&#x2019;t allow streaming.`,name:"streaming"},{anchor:"datasets.load_dataset.*config_kwargs",description:`*<strong>*config_kwargs</strong> (additional keyword arguments) &#x2014; Keyword arguments to be passed to the <a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a>
and used in the <a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DatasetBuilder">DatasetBuilder</a>.`,name:"*config_kwargs"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/load.py#L1539",returnDescription:`
<ul>
<li>if <em>split</em> is not None: the dataset requested,</li>
<li>if <em>split</em> is None, a <code>datasets.DatasetDict</code> with each split.</li>
</ul>
<p>or <a
  href="/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDataset"
>IterableDataset</a> or <a
  href="/docs/datasets/main/en/package_reference/main_classes#datasets.IterableDatasetDict"
>IterableDatasetDict</a>: if streaming=True</p>
<ul>
<li>if <em>split</em> is not None: the dataset requested,</li>
<li>if <em>split</em> is None, a <code>datasets.streaming.IterableDatasetDict</code> with each split.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset"
>Dataset</a> or <a
  href="/docs/datasets/main/en/package_reference/main_classes#datasets.DatasetDict"
>DatasetDict</a></p>
`}}),xe=new Ci({props:{$$slots:{default:[od]},$$scope:{ctx:j}}}),ke=new U({props:{anchor:"datasets.load_dataset.example",$$slots:{default:[nd]},$$scope:{ctx:j}}}),Ee=new U({props:{anchor:"datasets.load_dataset.example-2",$$slots:{default:[rd]},$$scope:{ctx:j}}}),De=new U({props:{anchor:"datasets.load_dataset.example-3",$$slots:{default:[id]},$$scope:{ctx:j}}}),Ne=new U({props:{anchor:"datasets.load_dataset.example-4",$$slots:{default:[ld]},$$scope:{ctx:j}}}),ft=new P({props:{name:"datasets.load_from_disk",anchor:"datasets.load_from_disk",parameters:[{name:"dataset_path",val:": str"},{name:"fs",val:" = None"},{name:"keep_in_memory",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"datasets.load_from_disk.dataset_path",description:`<strong>dataset_path</strong> (<code>str</code>) &#x2014; Path (e.g. <em>&#x201C;dataset/train&#x201D;</em>) or remote URI (e.g.
<em>&#x201C;s3://my-bucket/dataset/train&#x201D;</em>) of the Dataset or DatasetDict directory where the dataset will be
loaded from.`,name:"dataset_path"},{anchor:"datasets.load_from_disk.fs",description:`<strong>fs</strong> (<a href="/docs/datasets/main/en/package_reference/main_classes#datasets.filesystems.S3FileSystem">S3FileSystem</a> or <code>fsspec.spec.AbstractFileSystem</code>, optional, default <code>None</code>) &#x2014;
Instance of the remote filesystem used to download the files from.`,name:"fs"},{anchor:"datasets.load_from_disk.keep_in_memory",description:`<strong>keep_in_memory</strong> (<code>bool</code>, default <code>None</code>) &#x2014; Whether to copy the dataset in-memory. If <em>None</em>, the dataset
will not be copied in-memory unless explicitly enabled by setting <em>datasets.config.IN_MEMORY_MAX_SIZE</em> to
nonzero. See more details in the <a href="/docs/datasets/main/en/package_reference/loading_methods#load_dataset_enhancing_performance">load_dataset_enhancing_performance</a> section.`,name:"keep_in_memory"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/load.py#L1766",returnDescription:`
<ul>
<li>If <em>dataset_path</em> is a path of a dataset directory: the dataset requested.</li>
<li>If <em>dataset_path</em> is a path of a dataset dict directory: a <code>datasets.DatasetDict</code> with each split.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset"
>Dataset</a> or <a
  href="/docs/datasets/main/en/package_reference/main_classes#datasets.DatasetDict"
>DatasetDict</a></p>
`}}),je=new U({props:{anchor:"datasets.load_from_disk.example",$$slots:{default:[dd]},$$scope:{ctx:j}}}),gt=new P({props:{name:"datasets.load_dataset_builder",anchor:"datasets.load_dataset_builder",parameters:[{name:"path",val:": str"},{name:"name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, typing.Sequence[str], typing.Mapping[str, typing.Union[str, typing.Sequence[str]]], NoneType] = None"},{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.download.download_manager.DownloadMode] = None"},{name:"revision",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**config_kwargs",val:""}],parametersDescription:[{anchor:"datasets.load_dataset_builder.path",description:`<strong>path</strong> (<code>str</code>) &#x2014; Path or name of the dataset.
Depending on <code>path</code>, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.</p>
<p>For local datasets:</p>
<ul>
<li>if <code>path</code> is a local directory (containing data files only)
-&gt; load a generic dataset builder (csv, json, text etc.) based on the content of the directory
e.g. <code>&apos;./path/to/directory/with/my/csv/data&apos;</code>.</li>
<li>if <code>path</code> is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory):
-&gt; load the dataset builder from the dataset script
e.g. <code>&apos;./dataset/squad&apos;</code> or <code>&apos;./dataset/squad/squad.py&apos;</code>.</li>
</ul>
<p>For datasets on the Hugging Face Hub (list all available datasets and ids with <code>datasets.list_datasets()</code>)</p>
<ul>
<li>if <code>path</code> is a dataset repository on the HF hub (containing data files only)
-&gt; load a generic dataset builder (csv, text etc.) based on the content of the repository
e.g. <code>&apos;username/dataset_name&apos;</code>, a dataset repository on the HF hub containing your data files.</li>
<li>if <code>path</code> is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)
-&gt; load the dataset builder from the dataset script in the dataset repository
e.g. <code>glue</code>, <code>squad</code>, <code>&apos;username/dataset_name&apos;</code>, a dataset repository on the HF hub containing a dataset script <em>&#x2018;dataset_name.py&#x2019;</em>.</li>
</ul>`,name:"path"},{anchor:"datasets.load_dataset_builder.name",description:"<strong>name</strong> (<code>str</code>, optional) &#x2014; Defining the name of the dataset configuration.",name:"name"},{anchor:"datasets.load_dataset_builder.data_dir",description:`<strong>data_dir</strong> (<code>str</code>, optional) &#x2014; Defining the data_dir of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and <em>data_files</em> is None,
the behavior is equal to passing <em>os.path.join(data_dir, **)</em> as <em>data_files</em> to reference all the files in a directory.`,name:"data_dir"},{anchor:"datasets.load_dataset_builder.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.load_dataset_builder.cache_dir",description:"<strong>cache_dir</strong> (<code>str</code>, optional) &#x2014; Directory to read/write data. Defaults to &#x201D;~/.cache/huggingface/datasets&#x201D;.",name:"cache_dir"},{anchor:"datasets.load_dataset_builder.features",description:'<strong>features</strong> (<a href="/docs/datasets/main/en/package_reference/main_classes#datasets.Features">Features</a>, optional) &#x2014; Set the features type to use for this dataset.',name:"features"},{anchor:"datasets.load_dataset_builder.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; Specific download configuration parameters.',name:"download_config"},{anchor:"datasets.load_dataset_builder.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, default <code>REUSE_DATASET_IF_EXISTS</code>) &#x2014; Download/generate mode.',name:"download_mode"},{anchor:"datasets.load_dataset_builder.revision",description:`<strong>revision</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014; Version of the dataset script to load:</p>
<ul>
<li>For datasets in the <em>huggingface/datasets</em> library on GitHub like &#x201C;squad&#x201D;, the default version of the module is the local version of the lib.
You can specify a different version from your local version of the lib (e.g. &#x201C;main&#x201D; or &#x201C;1.2.0&#x201D;) but it might cause compatibility issues.</li>
<li>For community datasets like &#x201C;lhoestq/squad&#x201D; that have their own git repository on the Datasets Hub, the default version &#x201C;main&#x201D; corresponds to the &#x201C;main&#x201D; branch.
You can specify a different version that the default &#x201C;main&#x201D; by using a commit sha or a git tag of the dataset repository.</li>
</ul>`,name:"revision"},{anchor:"datasets.load_dataset_builder.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from <em>&#x201D;~/.huggingface&#x201D;</em>.`,name:"use_auth_token"},{anchor:"datasets.load_dataset_builder.*config_kwargs",description:`*<strong>*config_kwargs</strong> (additional keyword arguments) &#x2014; Keyword arguments to be passed to the <a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.BuilderConfig">BuilderConfig</a>
and used in the <a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DatasetBuilder">DatasetBuilder</a>.`,name:"*config_kwargs"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/load.py#L1407",returnType:`
<p><a
  href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DatasetBuilder"
>DatasetBuilder</a></p>
`}}),Ce=new Ci({props:{$$slots:{default:[cd]},$$scope:{ctx:j}}}),Se=new U({props:{anchor:"datasets.load_dataset_builder.example",$$slots:{default:[pd]},$$scope:{ctx:j}}}),_t=new P({props:{name:"datasets.get_dataset_config_names",anchor:"datasets.get_dataset_config_names",parameters:[{name:"path",val:": str"},{name:"revision",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.download.download_manager.DownloadMode] = None"},{name:"dynamic_modules_path",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, typing.List, typing.Dict, NoneType] = None"},{name:"**download_kwargs",val:""}],parametersDescription:[{anchor:"datasets.get_dataset_config_names.path",description:`<strong>path</strong> (<code>str</code>) &#x2014; path to the dataset processing script with the dataset builder. Can be either:</p>
<ul>
<li>a local path to processing script or the directory containing the script (if the script has the same name as the directory),
e.g. <code>&apos;./dataset/squad&apos;</code> or <code>&apos;./dataset/squad/squad.py&apos;</code></li>
<li>a dataset identifier on the Hugging Face Hub (list all available datasets and ids with <code>datasets.list_datasets()</code>)
e.g. <code>&apos;squad&apos;</code>, <code>&apos;glue&apos;</code> or <code>&apos;openai/webtext&apos;</code></li>
</ul>`,name:"path"},{anchor:"datasets.get_dataset_config_names.revision",description:`<strong>revision</strong> (Optional <code>Union[str, datasets.Version]</code>) &#x2014;
If specified, the dataset module will be loaded from the datasets repository at this version.
By default:</p>
<ul>
<li>it is set to the local version of the lib.</li>
<li>it will also try to load it from the main branch if it&#x2019;s not available at the local version of the lib.
Specifying a version that is different from your local version of the lib might cause compatibility issues.</li>
</ul>`,name:"revision"},{anchor:"datasets.get_dataset_config_names.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; Specific download configuration parameters.',name:"download_config"},{anchor:"datasets.get_dataset_config_names.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, default <code>REUSE_DATASET_IF_EXISTS</code>) &#x2014; Download/generate mode.',name:"download_mode"},{anchor:"datasets.get_dataset_config_names.dynamic_modules_path",description:`<strong>dynamic_modules_path</strong> (Optional str, defaults to HF_MODULES_CACHE / &#x201C;datasets_modules&#x201D;, i.e. ~/.cache/huggingface/modules/datasets_modules) &#x2014;
Optional path to the directory in which the dynamic modules are saved. It must have been initialized with <code>init_dynamic_modules</code>.
By default the datasets and metrics are stored inside the <em>datasets_modules</em> module.`,name:"dynamic_modules_path"},{anchor:"datasets.get_dataset_config_names.data_files",description:"<strong>data_files</strong> (<code>Union[Dict, List, str]</code>, optional) &#x2014; Defining the data_files of the dataset configuration.",name:"data_files"},{anchor:"datasets.get_dataset_config_names.*download_kwargs",description:`*<strong>*download_kwargs</strong> (additional keyword arguments) &#x2014; optional attributes for DownloadConfig() which will override the attributes in download_config if supplied,
for example <code>use_auth_token</code>`,name:"*download_kwargs"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/inspect.py#L256"}}),qe=new U({props:{anchor:"datasets.get_dataset_config_names.example",$$slots:{default:[md]},$$scope:{ctx:j}}}),vt=new P({props:{name:"datasets.get_dataset_infos",anchor:"datasets.get_dataset_infos",parameters:[{name:"path",val:": str"},{name:"data_files",val:": typing.Union[str, typing.List, typing.Dict, NoneType] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.download.download_manager.DownloadMode] = None"},{name:"revision",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**config_kwargs",val:""}],parametersDescription:[{anchor:"datasets.get_dataset_infos.path",description:`<strong>path</strong> (<code>str</code>) &#x2014; path to the dataset processing script with the dataset builder. Can be either:</p>
<ul>
<li>a local path to processing script or the directory containing the script (if the script has the same name as the directory),
e.g. <code>&apos;./dataset/squad&apos;</code> or <code>&apos;./dataset/squad/squad.py&apos;</code></li>
<li>a dataset identifier on the Hugging Face Hub (list all available datasets and ids with <code>datasets.list_datasets()</code>)
e.g. <code>&apos;squad&apos;</code>, <code>&apos;glue&apos;</code> or <code>&apos;openai/webtext&apos;</code></li>
</ul>`,name:"path"},{anchor:"datasets.get_dataset_infos.revision",description:`<strong>revision</strong> (Optional <code>Union[str, datasets.Version]</code>) &#x2014;
If specified, the dataset module will be loaded from the datasets repository at this version.
By default:</p>
<ul>
<li>it is set to the local version of the lib.</li>
<li>it will also try to load it from the main branch if it&#x2019;s not available at the local version of the lib.
Specifying a version that is different from your local version of the lib might cause compatibility issues.</li>
</ul>`,name:"revision"},{anchor:"datasets.get_dataset_infos.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; Specific download configuration parameters.',name:"download_config"},{anchor:"datasets.get_dataset_infos.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, default <code>REUSE_DATASET_IF_EXISTS</code>) &#x2014; Download/generate mode.',name:"download_mode"},{anchor:"datasets.get_dataset_infos.data_files",description:"<strong>data_files</strong> (<code>Union[Dict, List, str]</code>, optional) &#x2014; Defining the data_files of the dataset configuration.",name:"data_files"},{anchor:"datasets.get_dataset_infos.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from <em>&#x201D;~/.huggingface&#x201D;</em>.`,name:"use_auth_token"},{anchor:"datasets.get_dataset_infos.*config_kwargs",description:"*<strong>*config_kwargs</strong> (additional keyword arguments) &#x2014; optional attributes for builder class which will override the attributes if supplied.",name:"*config_kwargs"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/inspect.py#L195"}}),Oe=new U({props:{anchor:"datasets.get_dataset_infos.example",$$slots:{default:[fd]},$$scope:{ctx:j}}}),$t=new P({props:{name:"datasets.get_dataset_split_names",anchor:"datasets.get_dataset_split_names",parameters:[{name:"path",val:": str"},{name:"config_name",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, typing.Sequence[str], typing.Mapping[str, typing.Union[str, typing.Sequence[str]]], NoneType] = None"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.download.download_manager.DownloadMode] = None"},{name:"revision",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**config_kwargs",val:""}],parametersDescription:[{anchor:"datasets.get_dataset_split_names.path",description:`<strong>path</strong> (<code>str</code>) &#x2014; path to the dataset processing script with the dataset builder. Can be either:</p>
<ul>
<li>a local path to processing script or the directory containing the script (if the script has the same name as the directory),
e.g. <code>&apos;./dataset/squad&apos;</code> or <code>&apos;./dataset/squad/squad.py&apos;</code></li>
<li>a dataset identifier on the Hugging Face Hub (list all available datasets and ids with <code>datasets.list_datasets()</code>)
e.g. <code>&apos;squad&apos;</code>, <code>&apos;glue&apos;</code> or <code>&apos;openai/webtext&apos;</code></li>
</ul>`,name:"path"},{anchor:"datasets.get_dataset_split_names.config_name",description:"<strong>config_name</strong> (<code>str</code>, optional) &#x2014; Defining the name of the dataset configuration.",name:"config_name"},{anchor:"datasets.get_dataset_split_names.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.get_dataset_split_names.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; Specific download configuration parameters.',name:"download_config"},{anchor:"datasets.get_dataset_split_names.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, default <code>REUSE_DATASET_IF_EXISTS</code>) &#x2014; Download/generate mode.',name:"download_mode"},{anchor:"datasets.get_dataset_split_names.revision",description:`<strong>revision</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014; Version of the dataset script to load:</p>
<ul>
<li>For datasets in the <em>huggingface/datasets</em> library on GitHub like &#x201C;squad&#x201D;, the default version of the module is the local version of the lib.
You can specify a different version from your local version of the lib (e.g. &#x201C;main&#x201D; or &#x201C;1.2.0&#x201D;) but it might cause compatibility issues.</li>
<li>For community datasets like &#x201C;lhoestq/squad&#x201D; that have their own git repository on the Datasets Hub, the default version &#x201C;main&#x201D; corresponds to the &#x201C;main&#x201D; branch.
You can specify a different version that the default &#x201C;main&#x201D; by using a commit sha or a git tag of the dataset repository.</li>
</ul>`,name:"revision"},{anchor:"datasets.get_dataset_split_names.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from <em>&#x201D;~/.huggingface&#x201D;</em>.`,name:"use_auth_token"},{anchor:"datasets.get_dataset_split_names.*config_kwargs",description:"*<strong>*config_kwargs</strong> (additional keyword arguments) &#x2014; optional attributes for builder class which will override the attributes if supplied.",name:"*config_kwargs"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/inspect.py#L385"}}),Fe=new U({props:{anchor:"datasets.get_dataset_split_names.example",$$slots:{default:[gd]},$$scope:{ctx:j}}}),yt=new P({props:{name:"datasets.inspect_dataset",anchor:"datasets.inspect_dataset",parameters:[{name:"path",val:": str"},{name:"local_path",val:": str"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"**download_kwargs",val:""}],parametersDescription:[{anchor:"datasets.inspect_dataset.path",description:`<strong>path</strong> (<em>str</em>) &#x2014; Path to the dataset processing script with the dataset builder. Can be either:</p>
<ul>
<li>a local path to processing script or the directory containing the script (if the script has the same name
as the directory),
e.g. <code>&apos;./dataset/squad&apos;</code> or <code>&apos;./dataset/squad/squad.py&apos;</code>.</li>
<li>a dataset identifier on the Hugging Face Hub (list all available datasets and ids with [<em>list_datasets</em>])
e.g. <code>&apos;squad&apos;</code>, <code>&apos;glue&apos;</code> or <code>&apos;openai/webtext&apos;</code>.</li>
</ul>`,name:"path"},{anchor:"datasets.inspect_dataset.local_path",description:"<strong>local_path</strong> (<em>str</em>) &#x2014; Path to the local folder to copy the dataset script to.",name:"local_path"},{anchor:"datasets.inspect_dataset.download_config",description:"<strong>download_config</strong> ([<em>DownloadConfig</em>], <em>optional</em>) &#x2014; Specific download configuration parameters.",name:"download_config"},{anchor:"datasets.inspect_dataset.*download_kwargs",description:`*<strong>*download_kwargs</strong> (additional keyword arguments) &#x2014; Optional arguments for [<em>DownloadConfig</em>] which will override
the attributes of <em>download_config</em> if supplied.`,name:"*download_kwargs"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/inspect.py#L113"}}),bt=new Q({}),Te=new Ci({props:{warning:!0,$$slots:{default:[hd]},$$scope:{ctx:j}}}),wt=new P({props:{name:"datasets.list_metrics",anchor:"datasets.list_metrics",parameters:[{name:"with_community_metrics",val:" = True"},{name:"with_details",val:" = False"}],parametersDescription:[{anchor:"datasets.list_metrics.with_community_metrics",description:"<strong>with_community_metrics</strong> (<code>bool</code>, optional, default <code>True</code>) &#x2014; Include the community provided metrics.",name:"with_community_metrics"},{anchor:"datasets.list_metrics.with_details",description:"<strong>with_details</strong> (<code>bool</code>, optional, default <code>False</code>) &#x2014; Return the full details on the metrics instead of only the short name.",name:"with_details"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/inspect.py#L74"}}),Ie=new Si({props:{version:"2.5.0",$$slots:{default:[ud]},$$scope:{ctx:j}}}),Le=new U({props:{anchor:"datasets.list_metrics.example",$$slots:{default:[_d]},$$scope:{ctx:j}}}),xt=new P({props:{name:"datasets.load_metric",anchor:"datasets.load_metric",parameters:[{name:"path",val:": str"},{name:"config_name",val:": typing.Optional[str] = None"},{name:"process_id",val:": int = 0"},{name:"num_process",val:": int = 1"},{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"experiment_id",val:": typing.Optional[str] = None"},{name:"keep_in_memory",val:": bool = False"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.download.download_manager.DownloadMode] = None"},{name:"revision",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = None"},{name:"**metric_init_kwargs",val:""}],parametersDescription:[{anchor:"datasets.load_metric.path",description:`<strong>path</strong> (<code>str</code>) &#x2014;
path to the metric processing script with the metric builder. Can be either:<ul>
<li>a local path to processing script or the directory containing the script (if the script has the same name as the directory),
e.g. <code>&apos;./metrics/rouge&apos;</code> or <code>&apos;./metrics/rogue/rouge.py&apos;</code></li>
<li>a metric identifier on the HuggingFace datasets repo (list all available metrics with <code>datasets.list_metrics()</code>)
e.g. <code>&apos;rouge&apos;</code> or <code>&apos;bleu&apos;</code></li>
</ul>`,name:"path"},{anchor:"datasets.load_metric.config_name",description:"<strong>config_name</strong> (<code>str</code>, optional) &#x2014; selecting a configuration for the metric (e.g. the GLUE metric has a configuration for each subset)",name:"config_name"},{anchor:"datasets.load_metric.process_id",description:"<strong>process_id</strong> (<code>int</code>, optional) &#x2014; for distributed evaluation: id of the process",name:"process_id"},{anchor:"datasets.load_metric.num_process",description:"<strong>num_process</strong> (<code>int</code>, optional) &#x2014; for distributed evaluation: total number of processes",name:"num_process"},{anchor:"datasets.load_metric.cache_dir",description:"<strong>cache_dir</strong> (Optional str) &#x2014; path to store the temporary predictions and references (default to <em>~/.cache/huggingface/metrics/</em>)",name:"cache_dir"},{anchor:"datasets.load_metric.experiment_id",description:`<strong>experiment_id</strong> (<code>str</code>) &#x2014; A specific experiment id. This is used if several distributed evaluations share the same file system.
This is useful to compute metrics in distributed setups (in particular non-additive metrics like F1).`,name:"experiment_id"},{anchor:"datasets.load_metric.keep_in_memory",description:"<strong>keep_in_memory</strong> (bool) &#x2014; Whether to store the temporary results in memory (defaults to False)",name:"keep_in_memory"},{anchor:"datasets.load_metric.download_config",description:"<strong>download_config</strong> (Optional <code>datasets.DownloadConfig</code> &#x2014; specific download configuration parameters.",name:"download_config"},{anchor:"datasets.load_metric.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, default <code>REUSE_DATASET_IF_EXISTS</code>) &#x2014; Download/generate mode.',name:"download_mode"},{anchor:"datasets.load_metric.revision",description:`<strong>revision</strong> (Optional <code>Union[str, datasets.Version]</code>) &#x2014; if specified, the module will be loaded from the datasets repository
at this version. By default, it is set to the local version of the lib. Specifying a version that is different from
your local version of the lib might cause compatibility issues.`,name:"revision"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/load.py#L1327",returnDescription:`
<p><em>datasets.Metric</em></p>
`}}),Ae=new Si({props:{version:"2.5.0",$$slots:{default:[vd]},$$scope:{ctx:j}}}),He=new U({props:{anchor:"datasets.load_metric.example",$$slots:{default:[$d]},$$scope:{ctx:j}}}),Et=new P({props:{name:"datasets.inspect_metric",anchor:"datasets.inspect_metric",parameters:[{name:"path",val:": str"},{name:"local_path",val:": str"},{name:"download_config",val:": typing.Optional[datasets.download.download_config.DownloadConfig] = None"},{name:"**download_kwargs",val:""}],parametersDescription:[{anchor:"datasets.inspect_metric.path",description:`<strong>path</strong> (<code>str</code>) &#x2014; path to the dataset processing script with the dataset builder. Can be either:</p>
<ul>
<li>a local path to processing script or the directory containing the script (if the script has the same name as the directory),
e.g. <code>&apos;./dataset/squad&apos;</code> or <code>&apos;./dataset/squad/squad.py&apos;</code></li>
<li>a dataset identifier on the Hugging Face Hub (list all available datasets and ids with <code>datasets.list_datasets()</code>)
e.g. <code>&apos;squad&apos;</code>, <code>&apos;glue&apos;</code> or <code>&apos;openai/webtext&apos;</code></li>
</ul>`,name:"path"},{anchor:"datasets.inspect_metric.local_path",description:"<strong>local_path</strong> (<code>str</code>) &#x2014; path to the local folder to copy the datset script to.",name:"local_path"},{anchor:"datasets.inspect_metric.download_config",description:"<strong>download_config</strong> (Optional <code>datasets.DownloadConfig</code>) &#x2014; specific download configuration parameters.",name:"download_config"},{anchor:"datasets.inspect_metric.*download_kwargs",description:"*<strong>*download_kwargs</strong> (additional keyword arguments) &#x2014; optional attributes for DownloadConfig() which will override the attributes in download_config if supplied.",name:"*download_kwargs"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/inspect.py#L151"}}),Ue=new Si({props:{version:"2.5.0",$$slots:{default:[yd]},$$scope:{ctx:j}}}),Dt=new Q({}),Nt=new A({props:{code:'load_dataset("csv", data_dir="path/to/data/dir", sep="\\t")',highlighted:'load_dataset(<span class="hljs-string">&quot;csv&quot;</span>, data_dir=<span class="hljs-string">&quot;path/to/data/dir&quot;</span>, sep=<span class="hljs-string">&quot;\\t&quot;</span>)'}}),jt=new Q({}),Ct=new P({props:{name:"class datasets.packaged_modules.text.TextConfig",anchor:"datasets.packaged_modules.text.TextConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = 0.0.0"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"encoding",val:": str = 'utf-8'"},{name:"chunksize",val:": int = 10485760"},{name:"keep_linebreaks",val:": bool = False"},{name:"sample_by",val:": str = 'line'"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/packaged_modules/text/text.py#L17"}}),St=new Q({}),qt=new P({props:{name:"class datasets.packaged_modules.csv.CsvConfig",anchor:"datasets.packaged_modules.csv.CsvConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = 0.0.0"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"},{name:"sep",val:": str = ','"},{name:"delimiter",val:": typing.Optional[str] = None"},{name:"header",val:": typing.Union[int, typing.List[int], str, NoneType] = 'infer'"},{name:"names",val:": typing.Optional[typing.List[str]] = None"},{name:"column_names",val:": typing.Optional[typing.List[str]] = None"},{name:"index_col",val:": typing.Union[int, str, typing.List[int], typing.List[str], NoneType] = None"},{name:"usecols",val:": typing.Union[typing.List[int], typing.List[str], NoneType] = None"},{name:"prefix",val:": typing.Optional[str] = None"},{name:"mangle_dupe_cols",val:": bool = True"},{name:"engine",val:": typing.Optional[str] = None"},{name:"true_values",val:": typing.Optional[list] = None"},{name:"false_values",val:": typing.Optional[list] = None"},{name:"skipinitialspace",val:": bool = False"},{name:"skiprows",val:": typing.Union[int, typing.List[int], NoneType] = None"},{name:"nrows",val:": typing.Optional[int] = None"},{name:"na_values",val:": typing.Union[str, typing.List[str], NoneType] = None"},{name:"keep_default_na",val:": bool = True"},{name:"na_filter",val:": bool = True"},{name:"verbose",val:": bool = False"},{name:"skip_blank_lines",val:": bool = True"},{name:"thousands",val:": typing.Optional[str] = None"},{name:"decimal",val:": str = '.'"},{name:"lineterminator",val:": typing.Optional[str] = None"},{name:"quotechar",val:`: str = '"'`},{name:"quoting",val:": int = 0"},{name:"escapechar",val:": typing.Optional[str] = None"},{name:"comment",val:": typing.Optional[str] = None"},{name:"encoding",val:": typing.Optional[str] = None"},{name:"dialect",val:": typing.Optional[str] = None"},{name:"error_bad_lines",val:": bool = True"},{name:"warn_bad_lines",val:": bool = True"},{name:"skipfooter",val:": int = 0"},{name:"doublequote",val:": bool = True"},{name:"memory_map",val:": bool = False"},{name:"float_precision",val:": typing.Optional[str] = None"},{name:"chunksize",val:": int = 10000"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"encoding_errors",val:": typing.Optional[str] = 'strict'"},{name:"on_bad_lines",val:": typing.Literal['error', 'warn', 'skip'] = 'error'"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/packaged_modules/csv/csv.py#L23"}}),Ot=new Q({}),Ft=new P({props:{name:"class datasets.packaged_modules.json.JsonConfig",anchor:"datasets.packaged_modules.json.JsonConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = 0.0.0"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"field",val:": typing.Optional[str] = None"},{name:"use_threads",val:": bool = True"},{name:"block_size",val:": typing.Optional[int] = None"},{name:"chunksize",val:": int = 10485760"},{name:"newlines_in_values",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/packaged_modules/json/json.py#L38"}}),Pt=new Q({}),Tt=new P({props:{name:"class datasets.packaged_modules.parquet.ParquetConfig",anchor:"datasets.packaged_modules.parquet.ParquetConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = 0.0.0"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"},{name:"batch_size",val:": int = 10000"},{name:"columns",val:": typing.Optional[typing.List[str]] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/packaged_modules/parquet/parquet.py#L16"}}),It=new Q({}),Lt=new P({props:{name:"class datasets.packaged_modules.sql.SqlConfig",anchor:"datasets.packaged_modules.sql.SqlConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = 0.0.0"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"},{name:"sql",val:": typing.Union[str, ForwardRef('sqlalchemy.sql.Selectable')] = None"},{name:"con",val:": str = None"},{name:"index_col",val:": typing.Union[str, typing.List[str], NoneType] = None"},{name:"coerce_float",val:": bool = True"},{name:"params",val:": typing.Union[typing.List, typing.Tuple, typing.Dict, NoneType] = None"},{name:"parse_dates",val:": typing.Union[typing.List, typing.Dict, NoneType] = None"},{name:"columns",val:": typing.Optional[typing.List[str]] = None"},{name:"chunksize",val:": typing.Optional[int] = 10000"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/packaged_modules/sql/sql.py#L19"}}),At=new Q({}),Ht=new P({props:{name:"class datasets.packaged_modules.imagefolder.ImageFolderConfig",anchor:"datasets.packaged_modules.imagefolder.ImageFolderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = 0.0.0"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"drop_labels",val:": bool = None"},{name:"drop_metadata",val:": bool = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/packaged_modules/imagefolder/imagefolder.py#L12"}}),Ut=new Q({}),Vt=new P({props:{name:"class datasets.packaged_modules.audiofolder.AudioFolderConfig",anchor:"datasets.packaged_modules.audiofolder.AudioFolderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = 0.0.0"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"drop_labels",val:": bool = None"},{name:"drop_metadata",val:": bool = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/packaged_modules/audiofolder/audiofolder.py#L12"}}),{c(){i=o("meta"),$=m(),h=o("h1"),p=o("a"),_=o("span"),y(s.$$.fragment),c=m(),N=o("span"),D=l("Loading methods"),q=m(),O=o("p"),Po=l("Methods for listing and loading datasets and metrics:"),js=m(),W=o("h2"),ye=o("a"),oa=o("span"),y(tt.$$.fragment),To=m(),na=o("span"),Io=l("Datasets"),Cs=m(),M=o("div"),y(at.$$.fragment),Lo=m(),ra=o("p"),Ao=l("List all the datasets scripts available on the Hugging Face Hub."),Ho=m(),y(be.$$.fragment),Ss=m(),C=o("div"),y(st.$$.fragment),Uo=m(),ia=o("p"),Vo=l("Load a dataset from the Hugging Face Hub, or a local dataset."),Mo=m(),K=o("p"),Bo=l("You can find the list of datasets on the Hub at "),ot=o("a"),Ro=l("https://huggingface.co/datasets"),Jo=l(" or with "),la=o("code"),Yo=l("datasets.list_datasets()"),zo=l("."),Go=m(),da=o("p"),Xo=l("A dataset is a directory that contains:"),Qo=m(),nt=o("ul"),ca=o("li"),Wo=l("some data files in generic formats (JSON, CSV, Parquet, text, etc.)"),Ko=m(),pa=o("li"),Zo=l("and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures."),en=m(),ma=o("p"),tn=l("Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online."),an=m(),fa=o("p"),sn=l("This function does the following under the hood:"),on=m(),Z=o("ol"),B=o("li"),rt=o("p"),nn=l("Download and import in the library the dataset script from "),ga=o("code"),rn=l("path"),ln=l(" if it\u2019s not already cached inside the library."),dn=m(),ha=o("p"),cn=l("If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)"),pn=m(),ua=o("p"),mn=l(`Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,
contain the path or URL to the original data files and the code to load examples from the original data files.`),fn=m(),we=o("p"),gn=l("You can find some scripts here: "),it=o("a"),hn=l("https://github.com/huggingface/datasets/tree/main/datasets"),un=l(`
You can find the complete list of datasets in the Datasets Hub at `),lt=o("a"),_n=l("https://huggingface.co/datasets"),vn=m(),dt=o("li"),_a=o("p"),$n=l("Run the dataset script which will:"),yn=m(),ct=o("ul"),va=o("li"),$a=o("p"),bn=l("Download the dataset file from the original URL (see the script) if it\u2019s not already available locally or cached."),wn=m(),pt=o("li"),ya=o("p"),xn=l("Process and cache the dataset in typed Arrow tables for caching."),kn=m(),ba=o("p"),En=l(`Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.
They can be directly accessed from disk, loaded in RAM or even streamed over the web.`),Dn=m(),wa=o("li"),mt=o("p"),Nn=l("Return a dataset built from the requested splits in "),xa=o("code"),jn=l("split"),Cn=l(" (default: all)."),Sn=m(),ka=o("p"),qn=l(`It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.
In this case, it automatically loads all the data files from the directory or the dataset repository.`),On=m(),y(xe.$$.fragment),Fn=m(),Ea=o("p"),Pn=l("Example:"),Tn=m(),y(ke.$$.fragment),In=m(),y(Ee.$$.fragment),Ln=m(),y(De.$$.fragment),An=m(),y(Ne.$$.fragment),qs=m(),R=o("div"),y(ft.$$.fragment),Hn=m(),J=o("p"),Un=l("Loads a dataset that was previously saved using "),Jt=o("a"),Vn=l("Dataset.save_to_disk()"),Mn=l(` from a dataset directory, or
from a filesystem using either `),Yt=o("a"),Bn=l("datasets.filesystems.S3FileSystem"),Rn=l(` or any implementation of
`),Da=o("code"),Jn=l("fsspec.spec.AbstractFileSystem"),Yn=l("."),zn=m(),y(je.$$.fragment),Os=m(),F=o("div"),y(gt.$$.fragment),Gn=m(),Na=o("p"),Xn=l(`Load a dataset builder from the Hugging Face Hub, or a local dataset. A dataset builder can be used to inspect general information that is required to build a dataset (cache directory, config, dataset info, etc.)
without downloading the dataset itself.`),Qn=m(),ee=o("p"),Wn=l("You can find the list of datasets on the Hub at "),ht=o("a"),Kn=l("https://huggingface.co/datasets"),Zn=l(" or with "),ja=o("code"),er=l("datasets.list_datasets()"),tr=l("."),ar=m(),Ca=o("p"),sr=l("A dataset is a directory that contains:"),or=m(),ut=o("ul"),Sa=o("li"),nr=l("some data files in generic formats (JSON, CSV, Parquet, text, etc.)"),rr=m(),qa=o("li"),ir=l("and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures."),lr=m(),Oa=o("p"),dr=l("Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online."),cr=m(),y(Ce.$$.fragment),pr=m(),y(Se.$$.fragment),Fs=m(),Y=o("div"),y(_t.$$.fragment),mr=m(),Fa=o("p"),fr=l("Get the list of available config names for a particular dataset."),gr=m(),y(qe.$$.fragment),Ps=m(),z=o("div"),y(vt.$$.fragment),hr=m(),Pa=o("p"),ur=l("Get the meta information about a dataset, returned as a dict mapping config name to DatasetInfoDict."),_r=m(),y(Oe.$$.fragment),Ts=m(),G=o("div"),y($t.$$.fragment),vr=m(),Ta=o("p"),$r=l("Get the list of available splits for a particular config and dataset."),yr=m(),y(Fe.$$.fragment),Is=m(),te=o("div"),y(yt.$$.fragment),br=m(),Ia=o("p"),wr=l("Allow inspection/modification of a dataset script by copying on local drive at local_path."),Ls=m(),ae=o("h2"),Pe=o("a"),La=o("span"),y(bt.$$.fragment),xr=m(),Aa=o("span"),kr=l("Metrics"),As=m(),y(Te.$$.fragment),Hs=m(),I=o("div"),y(wt.$$.fragment),Er=m(),Ha=o("p"),Dr=l("List all the metrics script available on the Hugging Face Hub."),Nr=m(),y(Ie.$$.fragment),jr=m(),y(Le.$$.fragment),Us=m(),L=o("div"),y(xt.$$.fragment),Cr=m(),kt=o("p"),Sr=l("Load a "),Ua=o("em"),qr=l("datasets.Metric"),Or=l("."),Fr=m(),y(Ae.$$.fragment),Pr=m(),y(He.$$.fragment),Vs=m(),X=o("div"),y(Et.$$.fragment),Tr=m(),Va=o("p"),Ir=l("Allow inspection/modification of a metric script by copying it on local drive at local_path."),Lr=m(),y(Ue.$$.fragment),Ms=m(),se=o("h2"),Ve=o("a"),Ma=o("span"),y(Dt.$$.fragment),Ar=m(),Ba=o("span"),Hr=l("From files"),Bs=m(),zt=o("p"),Ur=l(`Configurations used to load data files.
They are used when loading local files or a dataset repository:`),Rs=m(),Me=o("ul"),Gt=o("li"),Vr=l("local files: "),Ra=o("code"),Mr=l('load_dataset("parquet", data_dir="path/to/data/dir")'),Br=m(),Xt=o("li"),Rr=l("dataset repository: "),Ja=o("code"),Jr=l('load_dataset("allenai/c4")'),Js=m(),H=o("p"),Yr=l("You can pass arguments to "),Ya=o("code"),zr=l("load_dataset"),Gr=l(` to configure data loading.
For example you can specify the `),za=o("code"),Xr=l("sep"),Qr=l(" parameter to define the "),Qt=o("a"),Wr=l("CsvConfig"),Kr=l(" that is used to load the data:"),Ys=m(),y(Nt.$$.fragment),zs=m(),oe=o("h3"),Be=o("a"),Ga=o("span"),y(jt.$$.fragment),Zr=m(),Xa=o("span"),ei=l("Text"),Gs=m(),ne=o("div"),y(Ct.$$.fragment),ti=m(),Qa=o("p"),ai=l("BuilderConfig for text files."),Xs=m(),re=o("h3"),Re=o("a"),Wa=o("span"),y(St.$$.fragment),si=m(),Ka=o("span"),oi=l("CSV"),Qs=m(),ie=o("div"),y(qt.$$.fragment),ni=m(),Za=o("p"),ri=l("BuilderConfig for CSV."),Ws=m(),le=o("h3"),Je=o("a"),es=o("span"),y(Ot.$$.fragment),ii=m(),ts=o("span"),li=l("JSON"),Ks=m(),de=o("div"),y(Ft.$$.fragment),di=m(),as=o("p"),ci=l("BuilderConfig for JSON."),Zs=m(),ce=o("h3"),Ye=o("a"),ss=o("span"),y(Pt.$$.fragment),pi=m(),os=o("span"),mi=l("Parquet"),eo=m(),pe=o("div"),y(Tt.$$.fragment),fi=m(),ns=o("p"),gi=l("BuilderConfig for Parquet."),to=m(),me=o("h3"),ze=o("a"),rs=o("span"),y(It.$$.fragment),hi=m(),is=o("span"),ui=l("SQL"),ao=m(),fe=o("div"),y(Lt.$$.fragment),_i=m(),ls=o("p"),vi=l("BuilderConfig for SQL."),so=m(),ge=o("h3"),Ge=o("a"),ds=o("span"),y(At.$$.fragment),$i=m(),cs=o("span"),yi=l("Images"),oo=m(),he=o("div"),y(Ht.$$.fragment),bi=m(),ps=o("p"),wi=l("BuilderConfig for ImageFolder."),no=m(),ue=o("h3"),Xe=o("a"),ms=o("span"),y(Ut.$$.fragment),xi=m(),fs=o("span"),ki=l("Audio"),ro=m(),_e=o("div"),y(Vt.$$.fragment),Ei=m(),gs=o("p"),Di=l("Builder Config for AudioFolder."),this.h()},l(e){const g=td('[data-svelte="svelte-1phssyn"]',document.head);i=n(g,"META",{name:!0,content:!0}),g.forEach(a),$=f(e),h=n(e,"H1",{class:!0});var Mt=r(h);p=n(Mt,"A",{id:!0,class:!0,href:!0});var hs=r(p);_=n(hs,"SPAN",{});var us=r(_);b(s.$$.fragment,us),us.forEach(a),hs.forEach(a),c=f(Mt),N=n(Mt,"SPAN",{});var _s=r(N);D=d(_s,"Loading methods"),_s.forEach(a),Mt.forEach(a),q=f(e),O=n(e,"P",{});var vs=r(O);Po=d(vs,"Methods for listing and loading datasets and metrics:"),vs.forEach(a),js=f(e),W=n(e,"H2",{class:!0});var Bt=r(W);ye=n(Bt,"A",{id:!0,class:!0,href:!0});var $s=r(ye);oa=n($s,"SPAN",{});var ys=r(oa);b(tt.$$.fragment,ys),ys.forEach(a),$s.forEach(a),To=f(Bt),na=n(Bt,"SPAN",{});var bs=r(na);Io=d(bs,"Datasets"),bs.forEach(a),Bt.forEach(a),Cs=f(e),M=n(e,"DIV",{class:!0});var ve=r(M);b(at.$$.fragment,ve),Lo=f(ve),ra=n(ve,"P",{});var ws=r(ra);Ao=d(ws,"List all the datasets scripts available on the Hugging Face Hub."),ws.forEach(a),Ho=f(ve),b(be.$$.fragment,ve),ve.forEach(a),Ss=f(e),C=n(e,"DIV",{class:!0});var S=r(C);b(st.$$.fragment,S),Uo=f(S),ia=n(S,"P",{});var xs=r(ia);Vo=d(xs,"Load a dataset from the Hugging Face Hub, or a local dataset."),xs.forEach(a),Mo=f(S),K=n(S,"P",{});var $e=r(K);Bo=d($e,"You can find the list of datasets on the Hub at "),ot=n($e,"A",{href:!0,rel:!0});var ks=r(ot);Ro=d(ks,"https://huggingface.co/datasets"),ks.forEach(a),Jo=d($e," or with "),la=n($e,"CODE",{});var Es=r(la);Yo=d(Es,"datasets.list_datasets()"),Es.forEach(a),zo=d($e,"."),$e.forEach(a),Go=f(S),da=n(S,"P",{});var Ds=r(da);Xo=d(Ds,"A dataset is a directory that contains:"),Ds.forEach(a),Qo=f(S),nt=n(S,"UL",{});var Rt=r(nt);ca=n(Rt,"LI",{});var qi=r(ca);Wo=d(qi,"some data files in generic formats (JSON, CSV, Parquet, text, etc.)"),qi.forEach(a),Ko=f(Rt),pa=n(Rt,"LI",{});var Oi=r(pa);Zo=d(Oi,"and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures."),Oi.forEach(a),Rt.forEach(a),en=f(S),ma=n(S,"P",{});var Fi=r(ma);tn=d(Fi,"Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online."),Fi.forEach(a),an=f(S),fa=n(S,"P",{});var Pi=r(fa);sn=d(Pi,"This function does the following under the hood:"),Pi.forEach(a),on=f(S),Z=n(S,"OL",{});var Wt=r(Z);B=n(Wt,"LI",{});var Qe=r(B);rt=n(Qe,"P",{});var lo=r(rt);nn=d(lo,"Download and import in the library the dataset script from "),ga=n(lo,"CODE",{});var Ti=r(ga);rn=d(Ti,"path"),Ti.forEach(a),ln=d(lo," if it\u2019s not already cached inside the library."),lo.forEach(a),dn=f(Qe),ha=n(Qe,"P",{});var Ii=r(ha);cn=d(Ii,"If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)"),Ii.forEach(a),pn=f(Qe),ua=n(Qe,"P",{});var Li=r(ua);mn=d(Li,`Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,
contain the path or URL to the original data files and the code to load examples from the original data files.`),Li.forEach(a),fn=f(Qe),we=n(Qe,"P",{});var Ns=r(we);gn=d(Ns,"You can find some scripts here: "),it=n(Ns,"A",{href:!0,rel:!0});var Ai=r(it);hn=d(Ai,"https://github.com/huggingface/datasets/tree/main/datasets"),Ai.forEach(a),un=d(Ns,`
You can find the complete list of datasets in the Datasets Hub at `),lt=n(Ns,"A",{href:!0,rel:!0});var Hi=r(lt);_n=d(Hi,"https://huggingface.co/datasets"),Hi.forEach(a),Ns.forEach(a),Qe.forEach(a),vn=f(Wt),dt=n(Wt,"LI",{});var co=r(dt);_a=n(co,"P",{});var Ui=r(_a);$n=d(Ui,"Run the dataset script which will:"),Ui.forEach(a),yn=f(co),ct=n(co,"UL",{});var po=r(ct);va=n(po,"LI",{});var Vi=r(va);$a=n(Vi,"P",{});var Mi=r($a);bn=d(Mi,"Download the dataset file from the original URL (see the script) if it\u2019s not already available locally or cached."),Mi.forEach(a),Vi.forEach(a),wn=f(po),pt=n(po,"LI",{});var mo=r(pt);ya=n(mo,"P",{});var Bi=r(ya);xn=d(Bi,"Process and cache the dataset in typed Arrow tables for caching."),Bi.forEach(a),kn=f(mo),ba=n(mo,"P",{});var Ri=r(ba);En=d(Ri,`Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.
They can be directly accessed from disk, loaded in RAM or even streamed over the web.`),Ri.forEach(a),mo.forEach(a),po.forEach(a),co.forEach(a),Dn=f(Wt),wa=n(Wt,"LI",{});var Ji=r(wa);mt=n(Ji,"P",{});var fo=r(mt);Nn=d(fo,"Return a dataset built from the requested splits in "),xa=n(fo,"CODE",{});var Yi=r(xa);jn=d(Yi,"split"),Yi.forEach(a),Cn=d(fo," (default: all)."),fo.forEach(a),Ji.forEach(a),Wt.forEach(a),Sn=f(S),ka=n(S,"P",{});var zi=r(ka);qn=d(zi,`It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.
In this case, it automatically loads all the data files from the directory or the dataset repository.`),zi.forEach(a),On=f(S),b(xe.$$.fragment,S),Fn=f(S),Ea=n(S,"P",{});var Gi=r(Ea);Pn=d(Gi,"Example:"),Gi.forEach(a),Tn=f(S),b(ke.$$.fragment,S),In=f(S),b(Ee.$$.fragment,S),Ln=f(S),b(De.$$.fragment,S),An=f(S),b(Ne.$$.fragment,S),S.forEach(a),qs=f(e),R=n(e,"DIV",{class:!0});var Kt=r(R);b(ft.$$.fragment,Kt),Hn=f(Kt),J=n(Kt,"P",{});var We=r(J);Un=d(We,"Loads a dataset that was previously saved using "),Jt=n(We,"A",{href:!0});var Xi=r(Jt);Vn=d(Xi,"Dataset.save_to_disk()"),Xi.forEach(a),Mn=d(We,` from a dataset directory, or
from a filesystem using either `),Yt=n(We,"A",{href:!0});var Qi=r(Yt);Bn=d(Qi,"datasets.filesystems.S3FileSystem"),Qi.forEach(a),Rn=d(We,` or any implementation of
`),Da=n(We,"CODE",{});var Wi=r(Da);Jn=d(Wi,"fsspec.spec.AbstractFileSystem"),Wi.forEach(a),Yn=d(We,"."),We.forEach(a),zn=f(Kt),b(je.$$.fragment,Kt),Kt.forEach(a),Os=f(e),F=n(e,"DIV",{class:!0});var T=r(F);b(gt.$$.fragment,T),Gn=f(T),Na=n(T,"P",{});var Ki=r(Na);Xn=d(Ki,`Load a dataset builder from the Hugging Face Hub, or a local dataset. A dataset builder can be used to inspect general information that is required to build a dataset (cache directory, config, dataset info, etc.)
without downloading the dataset itself.`),Ki.forEach(a),Qn=f(T),ee=n(T,"P",{});var Zt=r(ee);Wn=d(Zt,"You can find the list of datasets on the Hub at "),ht=n(Zt,"A",{href:!0,rel:!0});var Zi=r(ht);Kn=d(Zi,"https://huggingface.co/datasets"),Zi.forEach(a),Zn=d(Zt," or with "),ja=n(Zt,"CODE",{});var el=r(ja);er=d(el,"datasets.list_datasets()"),el.forEach(a),tr=d(Zt,"."),Zt.forEach(a),ar=f(T),Ca=n(T,"P",{});var tl=r(Ca);sr=d(tl,"A dataset is a directory that contains:"),tl.forEach(a),or=f(T),ut=n(T,"UL",{});var go=r(ut);Sa=n(go,"LI",{});var al=r(Sa);nr=d(al,"some data files in generic formats (JSON, CSV, Parquet, text, etc.)"),al.forEach(a),rr=f(go),qa=n(go,"LI",{});var sl=r(qa);ir=d(sl,"and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures."),sl.forEach(a),go.forEach(a),lr=f(T),Oa=n(T,"P",{});var ol=r(Oa);dr=d(ol,"Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online."),ol.forEach(a),cr=f(T),b(Ce.$$.fragment,T),pr=f(T),b(Se.$$.fragment,T),T.forEach(a),Fs=f(e),Y=n(e,"DIV",{class:!0});var ea=r(Y);b(_t.$$.fragment,ea),mr=f(ea),Fa=n(ea,"P",{});var nl=r(Fa);fr=d(nl,"Get the list of available config names for a particular dataset."),nl.forEach(a),gr=f(ea),b(qe.$$.fragment,ea),ea.forEach(a),Ps=f(e),z=n(e,"DIV",{class:!0});var ta=r(z);b(vt.$$.fragment,ta),hr=f(ta),Pa=n(ta,"P",{});var rl=r(Pa);ur=d(rl,"Get the meta information about a dataset, returned as a dict mapping config name to DatasetInfoDict."),rl.forEach(a),_r=f(ta),b(Oe.$$.fragment,ta),ta.forEach(a),Ts=f(e),G=n(e,"DIV",{class:!0});var aa=r(G);b($t.$$.fragment,aa),vr=f(aa),Ta=n(aa,"P",{});var il=r(Ta);$r=d(il,"Get the list of available splits for a particular config and dataset."),il.forEach(a),yr=f(aa),b(Fe.$$.fragment,aa),aa.forEach(a),Is=f(e),te=n(e,"DIV",{class:!0});var ho=r(te);b(yt.$$.fragment,ho),br=f(ho),Ia=n(ho,"P",{});var ll=r(Ia);wr=d(ll,"Allow inspection/modification of a dataset script by copying on local drive at local_path."),ll.forEach(a),ho.forEach(a),Ls=f(e),ae=n(e,"H2",{class:!0});var uo=r(ae);Pe=n(uo,"A",{id:!0,class:!0,href:!0});var dl=r(Pe);La=n(dl,"SPAN",{});var cl=r(La);b(bt.$$.fragment,cl),cl.forEach(a),dl.forEach(a),xr=f(uo),Aa=n(uo,"SPAN",{});var pl=r(Aa);kr=d(pl,"Metrics"),pl.forEach(a),uo.forEach(a),As=f(e),b(Te.$$.fragment,e),Hs=f(e),I=n(e,"DIV",{class:!0});var Ke=r(I);b(wt.$$.fragment,Ke),Er=f(Ke),Ha=n(Ke,"P",{});var ml=r(Ha);Dr=d(ml,"List all the metrics script available on the Hugging Face Hub."),ml.forEach(a),Nr=f(Ke),b(Ie.$$.fragment,Ke),jr=f(Ke),b(Le.$$.fragment,Ke),Ke.forEach(a),Us=f(e),L=n(e,"DIV",{class:!0});var Ze=r(L);b(xt.$$.fragment,Ze),Cr=f(Ze),kt=n(Ze,"P",{});var _o=r(kt);Sr=d(_o,"Load a "),Ua=n(_o,"EM",{});var fl=r(Ua);qr=d(fl,"datasets.Metric"),fl.forEach(a),Or=d(_o,"."),_o.forEach(a),Fr=f(Ze),b(Ae.$$.fragment,Ze),Pr=f(Ze),b(He.$$.fragment,Ze),Ze.forEach(a),Vs=f(e),X=n(e,"DIV",{class:!0});var sa=r(X);b(Et.$$.fragment,sa),Tr=f(sa),Va=n(sa,"P",{});var gl=r(Va);Ir=d(gl,"Allow inspection/modification of a metric script by copying it on local drive at local_path."),gl.forEach(a),Lr=f(sa),b(Ue.$$.fragment,sa),sa.forEach(a),Ms=f(e),se=n(e,"H2",{class:!0});var vo=r(se);Ve=n(vo,"A",{id:!0,class:!0,href:!0});var hl=r(Ve);Ma=n(hl,"SPAN",{});var ul=r(Ma);b(Dt.$$.fragment,ul),ul.forEach(a),hl.forEach(a),Ar=f(vo),Ba=n(vo,"SPAN",{});var _l=r(Ba);Hr=d(_l,"From files"),_l.forEach(a),vo.forEach(a),Bs=f(e),zt=n(e,"P",{});var vl=r(zt);Ur=d(vl,`Configurations used to load data files.
They are used when loading local files or a dataset repository:`),vl.forEach(a),Rs=f(e),Me=n(e,"UL",{});var $o=r(Me);Gt=n($o,"LI",{});var Ni=r(Gt);Vr=d(Ni,"local files: "),Ra=n(Ni,"CODE",{});var $l=r(Ra);Mr=d($l,'load_dataset("parquet", data_dir="path/to/data/dir")'),$l.forEach(a),Ni.forEach(a),Br=f($o),Xt=n($o,"LI",{});var ji=r(Xt);Rr=d(ji,"dataset repository: "),Ja=n(ji,"CODE",{});var yl=r(Ja);Jr=d(yl,'load_dataset("allenai/c4")'),yl.forEach(a),ji.forEach(a),$o.forEach(a),Js=f(e),H=n(e,"P",{});var et=r(H);Yr=d(et,"You can pass arguments to "),Ya=n(et,"CODE",{});var bl=r(Ya);zr=d(bl,"load_dataset"),bl.forEach(a),Gr=d(et,` to configure data loading.
For example you can specify the `),za=n(et,"CODE",{});var wl=r(za);Xr=d(wl,"sep"),wl.forEach(a),Qr=d(et," parameter to define the "),Qt=n(et,"A",{href:!0});var xl=r(Qt);Wr=d(xl,"CsvConfig"),xl.forEach(a),Kr=d(et," that is used to load the data:"),et.forEach(a),Ys=f(e),b(Nt.$$.fragment,e),zs=f(e),oe=n(e,"H3",{class:!0});var yo=r(oe);Be=n(yo,"A",{id:!0,class:!0,href:!0});var kl=r(Be);Ga=n(kl,"SPAN",{});var El=r(Ga);b(jt.$$.fragment,El),El.forEach(a),kl.forEach(a),Zr=f(yo),Xa=n(yo,"SPAN",{});var Dl=r(Xa);ei=d(Dl,"Text"),Dl.forEach(a),yo.forEach(a),Gs=f(e),ne=n(e,"DIV",{class:!0});var bo=r(ne);b(Ct.$$.fragment,bo),ti=f(bo),Qa=n(bo,"P",{});var Nl=r(Qa);ai=d(Nl,"BuilderConfig for text files."),Nl.forEach(a),bo.forEach(a),Xs=f(e),re=n(e,"H3",{class:!0});var wo=r(re);Re=n(wo,"A",{id:!0,class:!0,href:!0});var jl=r(Re);Wa=n(jl,"SPAN",{});var Cl=r(Wa);b(St.$$.fragment,Cl),Cl.forEach(a),jl.forEach(a),si=f(wo),Ka=n(wo,"SPAN",{});var Sl=r(Ka);oi=d(Sl,"CSV"),Sl.forEach(a),wo.forEach(a),Qs=f(e),ie=n(e,"DIV",{class:!0});var xo=r(ie);b(qt.$$.fragment,xo),ni=f(xo),Za=n(xo,"P",{});var ql=r(Za);ri=d(ql,"BuilderConfig for CSV."),ql.forEach(a),xo.forEach(a),Ws=f(e),le=n(e,"H3",{class:!0});var ko=r(le);Je=n(ko,"A",{id:!0,class:!0,href:!0});var Ol=r(Je);es=n(Ol,"SPAN",{});var Fl=r(es);b(Ot.$$.fragment,Fl),Fl.forEach(a),Ol.forEach(a),ii=f(ko),ts=n(ko,"SPAN",{});var Pl=r(ts);li=d(Pl,"JSON"),Pl.forEach(a),ko.forEach(a),Ks=f(e),de=n(e,"DIV",{class:!0});var Eo=r(de);b(Ft.$$.fragment,Eo),di=f(Eo),as=n(Eo,"P",{});var Tl=r(as);ci=d(Tl,"BuilderConfig for JSON."),Tl.forEach(a),Eo.forEach(a),Zs=f(e),ce=n(e,"H3",{class:!0});var Do=r(ce);Ye=n(Do,"A",{id:!0,class:!0,href:!0});var Il=r(Ye);ss=n(Il,"SPAN",{});var Ll=r(ss);b(Pt.$$.fragment,Ll),Ll.forEach(a),Il.forEach(a),pi=f(Do),os=n(Do,"SPAN",{});var Al=r(os);mi=d(Al,"Parquet"),Al.forEach(a),Do.forEach(a),eo=f(e),pe=n(e,"DIV",{class:!0});var No=r(pe);b(Tt.$$.fragment,No),fi=f(No),ns=n(No,"P",{});var Hl=r(ns);gi=d(Hl,"BuilderConfig for Parquet."),Hl.forEach(a),No.forEach(a),to=f(e),me=n(e,"H3",{class:!0});var jo=r(me);ze=n(jo,"A",{id:!0,class:!0,href:!0});var Ul=r(ze);rs=n(Ul,"SPAN",{});var Vl=r(rs);b(It.$$.fragment,Vl),Vl.forEach(a),Ul.forEach(a),hi=f(jo),is=n(jo,"SPAN",{});var Ml=r(is);ui=d(Ml,"SQL"),Ml.forEach(a),jo.forEach(a),ao=f(e),fe=n(e,"DIV",{class:!0});var Co=r(fe);b(Lt.$$.fragment,Co),_i=f(Co),ls=n(Co,"P",{});var Bl=r(ls);vi=d(Bl,"BuilderConfig for SQL."),Bl.forEach(a),Co.forEach(a),so=f(e),ge=n(e,"H3",{class:!0});var So=r(ge);Ge=n(So,"A",{id:!0,class:!0,href:!0});var Rl=r(Ge);ds=n(Rl,"SPAN",{});var Jl=r(ds);b(At.$$.fragment,Jl),Jl.forEach(a),Rl.forEach(a),$i=f(So),cs=n(So,"SPAN",{});var Yl=r(cs);yi=d(Yl,"Images"),Yl.forEach(a),So.forEach(a),oo=f(e),he=n(e,"DIV",{class:!0});var qo=r(he);b(Ht.$$.fragment,qo),bi=f(qo),ps=n(qo,"P",{});var zl=r(ps);wi=d(zl,"BuilderConfig for ImageFolder."),zl.forEach(a),qo.forEach(a),no=f(e),ue=n(e,"H3",{class:!0});var Oo=r(ue);Xe=n(Oo,"A",{id:!0,class:!0,href:!0});var Gl=r(Xe);ms=n(Gl,"SPAN",{});var Xl=r(ms);b(Ut.$$.fragment,Xl),Xl.forEach(a),Gl.forEach(a),xi=f(Oo),fs=n(Oo,"SPAN",{});var Ql=r(fs);ki=d(Ql,"Audio"),Ql.forEach(a),Oo.forEach(a),ro=f(e),_e=n(e,"DIV",{class:!0});var Fo=r(_e);b(Vt.$$.fragment,Fo),Ei=f(Fo),gs=n(Fo,"P",{});var Wl=r(gs);Di=d(Wl,"Builder Config for AudioFolder."),Wl.forEach(a),Fo.forEach(a),this.h()},h(){v(i,"name","hf:doc:metadata"),v(i,"content",JSON.stringify(wd)),v(p,"id","loading-methods"),v(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(p,"href","#loading-methods"),v(h,"class","relative group"),v(ye,"id","datasets.list_datasets"),v(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(ye,"href","#datasets.list_datasets"),v(W,"class","relative group"),v(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(ot,"href","https://huggingface.co/datasets"),v(ot,"rel","nofollow"),v(it,"href","https://github.com/huggingface/datasets/tree/main/datasets"),v(it,"rel","nofollow"),v(lt,"href","https://huggingface.co/datasets"),v(lt,"rel","nofollow"),v(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(Jt,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.save_to_disk"),v(Yt,"href","/docs/datasets/main/en/package_reference/main_classes#datasets.filesystems.S3FileSystem"),v(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(ht,"href","https://huggingface.co/datasets"),v(ht,"rel","nofollow"),v(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(Pe,"id","datasets.list_metrics"),v(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(Pe,"href","#datasets.list_metrics"),v(ae,"class","relative group"),v(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(Ve,"id","from-files"),v(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(Ve,"href","#from-files"),v(se,"class","relative group"),v(Qt,"href","/docs/datasets/main/en/package_reference/loading_methods#datasets.packaged_modules.csv.CsvConfig"),v(Be,"id","datasets.packaged_modules.text.TextConfig"),v(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(Be,"href","#datasets.packaged_modules.text.TextConfig"),v(oe,"class","relative group"),v(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(Re,"id","datasets.packaged_modules.csv.CsvConfig"),v(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(Re,"href","#datasets.packaged_modules.csv.CsvConfig"),v(re,"class","relative group"),v(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(Je,"id","datasets.packaged_modules.json.JsonConfig"),v(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(Je,"href","#datasets.packaged_modules.json.JsonConfig"),v(le,"class","relative group"),v(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(Ye,"id","datasets.packaged_modules.parquet.ParquetConfig"),v(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(Ye,"href","#datasets.packaged_modules.parquet.ParquetConfig"),v(ce,"class","relative group"),v(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(ze,"id","datasets.packaged_modules.sql.SqlConfig"),v(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(ze,"href","#datasets.packaged_modules.sql.SqlConfig"),v(me,"class","relative group"),v(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(Ge,"id","datasets.packaged_modules.imagefolder.ImageFolderConfig"),v(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(Ge,"href","#datasets.packaged_modules.imagefolder.ImageFolderConfig"),v(ge,"class","relative group"),v(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(Xe,"id","datasets.packaged_modules.audiofolder.AudioFolderConfig"),v(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),v(Xe,"href","#datasets.packaged_modules.audiofolder.AudioFolderConfig"),v(ue,"class","relative group"),v(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,g){t(document.head,i),u(e,$,g),u(e,h,g),t(h,p),t(p,_),w(s,_,null),t(h,c),t(h,N),t(N,D),u(e,q,g),u(e,O,g),t(O,Po),u(e,js,g),u(e,W,g),t(W,ye),t(ye,oa),w(tt,oa,null),t(W,To),t(W,na),t(na,Io),u(e,Cs,g),u(e,M,g),w(at,M,null),t(M,Lo),t(M,ra),t(ra,Ao),t(M,Ho),w(be,M,null),u(e,Ss,g),u(e,C,g),w(st,C,null),t(C,Uo),t(C,ia),t(ia,Vo),t(C,Mo),t(C,K),t(K,Bo),t(K,ot),t(ot,Ro),t(K,Jo),t(K,la),t(la,Yo),t(K,zo),t(C,Go),t(C,da),t(da,Xo),t(C,Qo),t(C,nt),t(nt,ca),t(ca,Wo),t(nt,Ko),t(nt,pa),t(pa,Zo),t(C,en),t(C,ma),t(ma,tn),t(C,an),t(C,fa),t(fa,sn),t(C,on),t(C,Z),t(Z,B),t(B,rt),t(rt,nn),t(rt,ga),t(ga,rn),t(rt,ln),t(B,dn),t(B,ha),t(ha,cn),t(B,pn),t(B,ua),t(ua,mn),t(B,fn),t(B,we),t(we,gn),t(we,it),t(it,hn),t(we,un),t(we,lt),t(lt,_n),t(Z,vn),t(Z,dt),t(dt,_a),t(_a,$n),t(dt,yn),t(dt,ct),t(ct,va),t(va,$a),t($a,bn),t(ct,wn),t(ct,pt),t(pt,ya),t(ya,xn),t(pt,kn),t(pt,ba),t(ba,En),t(Z,Dn),t(Z,wa),t(wa,mt),t(mt,Nn),t(mt,xa),t(xa,jn),t(mt,Cn),t(C,Sn),t(C,ka),t(ka,qn),t(C,On),w(xe,C,null),t(C,Fn),t(C,Ea),t(Ea,Pn),t(C,Tn),w(ke,C,null),t(C,In),w(Ee,C,null),t(C,Ln),w(De,C,null),t(C,An),w(Ne,C,null),u(e,qs,g),u(e,R,g),w(ft,R,null),t(R,Hn),t(R,J),t(J,Un),t(J,Jt),t(Jt,Vn),t(J,Mn),t(J,Yt),t(Yt,Bn),t(J,Rn),t(J,Da),t(Da,Jn),t(J,Yn),t(R,zn),w(je,R,null),u(e,Os,g),u(e,F,g),w(gt,F,null),t(F,Gn),t(F,Na),t(Na,Xn),t(F,Qn),t(F,ee),t(ee,Wn),t(ee,ht),t(ht,Kn),t(ee,Zn),t(ee,ja),t(ja,er),t(ee,tr),t(F,ar),t(F,Ca),t(Ca,sr),t(F,or),t(F,ut),t(ut,Sa),t(Sa,nr),t(ut,rr),t(ut,qa),t(qa,ir),t(F,lr),t(F,Oa),t(Oa,dr),t(F,cr),w(Ce,F,null),t(F,pr),w(Se,F,null),u(e,Fs,g),u(e,Y,g),w(_t,Y,null),t(Y,mr),t(Y,Fa),t(Fa,fr),t(Y,gr),w(qe,Y,null),u(e,Ps,g),u(e,z,g),w(vt,z,null),t(z,hr),t(z,Pa),t(Pa,ur),t(z,_r),w(Oe,z,null),u(e,Ts,g),u(e,G,g),w($t,G,null),t(G,vr),t(G,Ta),t(Ta,$r),t(G,yr),w(Fe,G,null),u(e,Is,g),u(e,te,g),w(yt,te,null),t(te,br),t(te,Ia),t(Ia,wr),u(e,Ls,g),u(e,ae,g),t(ae,Pe),t(Pe,La),w(bt,La,null),t(ae,xr),t(ae,Aa),t(Aa,kr),u(e,As,g),w(Te,e,g),u(e,Hs,g),u(e,I,g),w(wt,I,null),t(I,Er),t(I,Ha),t(Ha,Dr),t(I,Nr),w(Ie,I,null),t(I,jr),w(Le,I,null),u(e,Us,g),u(e,L,g),w(xt,L,null),t(L,Cr),t(L,kt),t(kt,Sr),t(kt,Ua),t(Ua,qr),t(kt,Or),t(L,Fr),w(Ae,L,null),t(L,Pr),w(He,L,null),u(e,Vs,g),u(e,X,g),w(Et,X,null),t(X,Tr),t(X,Va),t(Va,Ir),t(X,Lr),w(Ue,X,null),u(e,Ms,g),u(e,se,g),t(se,Ve),t(Ve,Ma),w(Dt,Ma,null),t(se,Ar),t(se,Ba),t(Ba,Hr),u(e,Bs,g),u(e,zt,g),t(zt,Ur),u(e,Rs,g),u(e,Me,g),t(Me,Gt),t(Gt,Vr),t(Gt,Ra),t(Ra,Mr),t(Me,Br),t(Me,Xt),t(Xt,Rr),t(Xt,Ja),t(Ja,Jr),u(e,Js,g),u(e,H,g),t(H,Yr),t(H,Ya),t(Ya,zr),t(H,Gr),t(H,za),t(za,Xr),t(H,Qr),t(H,Qt),t(Qt,Wr),t(H,Kr),u(e,Ys,g),w(Nt,e,g),u(e,zs,g),u(e,oe,g),t(oe,Be),t(Be,Ga),w(jt,Ga,null),t(oe,Zr),t(oe,Xa),t(Xa,ei),u(e,Gs,g),u(e,ne,g),w(Ct,ne,null),t(ne,ti),t(ne,Qa),t(Qa,ai),u(e,Xs,g),u(e,re,g),t(re,Re),t(Re,Wa),w(St,Wa,null),t(re,si),t(re,Ka),t(Ka,oi),u(e,Qs,g),u(e,ie,g),w(qt,ie,null),t(ie,ni),t(ie,Za),t(Za,ri),u(e,Ws,g),u(e,le,g),t(le,Je),t(Je,es),w(Ot,es,null),t(le,ii),t(le,ts),t(ts,li),u(e,Ks,g),u(e,de,g),w(Ft,de,null),t(de,di),t(de,as),t(as,ci),u(e,Zs,g),u(e,ce,g),t(ce,Ye),t(Ye,ss),w(Pt,ss,null),t(ce,pi),t(ce,os),t(os,mi),u(e,eo,g),u(e,pe,g),w(Tt,pe,null),t(pe,fi),t(pe,ns),t(ns,gi),u(e,to,g),u(e,me,g),t(me,ze),t(ze,rs),w(It,rs,null),t(me,hi),t(me,is),t(is,ui),u(e,ao,g),u(e,fe,g),w(Lt,fe,null),t(fe,_i),t(fe,ls),t(ls,vi),u(e,so,g),u(e,ge,g),t(ge,Ge),t(Ge,ds),w(At,ds,null),t(ge,$i),t(ge,cs),t(cs,yi),u(e,oo,g),u(e,he,g),w(Ht,he,null),t(he,bi),t(he,ps),t(ps,wi),u(e,no,g),u(e,ue,g),t(ue,Xe),t(Xe,ms),w(Ut,ms,null),t(ue,xi),t(ue,fs),t(fs,ki),u(e,ro,g),u(e,_e,g),w(Vt,_e,null),t(_e,Ei),t(_e,gs),t(gs,Di),io=!0},p(e,[g]){const Mt={};g&2&&(Mt.$$scope={dirty:g,ctx:e}),be.$set(Mt);const hs={};g&2&&(hs.$$scope={dirty:g,ctx:e}),xe.$set(hs);const us={};g&2&&(us.$$scope={dirty:g,ctx:e}),ke.$set(us);const _s={};g&2&&(_s.$$scope={dirty:g,ctx:e}),Ee.$set(_s);const vs={};g&2&&(vs.$$scope={dirty:g,ctx:e}),De.$set(vs);const Bt={};g&2&&(Bt.$$scope={dirty:g,ctx:e}),Ne.$set(Bt);const $s={};g&2&&($s.$$scope={dirty:g,ctx:e}),je.$set($s);const ys={};g&2&&(ys.$$scope={dirty:g,ctx:e}),Ce.$set(ys);const bs={};g&2&&(bs.$$scope={dirty:g,ctx:e}),Se.$set(bs);const ve={};g&2&&(ve.$$scope={dirty:g,ctx:e}),qe.$set(ve);const ws={};g&2&&(ws.$$scope={dirty:g,ctx:e}),Oe.$set(ws);const S={};g&2&&(S.$$scope={dirty:g,ctx:e}),Fe.$set(S);const xs={};g&2&&(xs.$$scope={dirty:g,ctx:e}),Te.$set(xs);const $e={};g&2&&($e.$$scope={dirty:g,ctx:e}),Ie.$set($e);const ks={};g&2&&(ks.$$scope={dirty:g,ctx:e}),Le.$set(ks);const Es={};g&2&&(Es.$$scope={dirty:g,ctx:e}),Ae.$set(Es);const Ds={};g&2&&(Ds.$$scope={dirty:g,ctx:e}),He.$set(Ds);const Rt={};g&2&&(Rt.$$scope={dirty:g,ctx:e}),Ue.$set(Rt)},i(e){io||(x(s.$$.fragment,e),x(tt.$$.fragment,e),x(at.$$.fragment,e),x(be.$$.fragment,e),x(st.$$.fragment,e),x(xe.$$.fragment,e),x(ke.$$.fragment,e),x(Ee.$$.fragment,e),x(De.$$.fragment,e),x(Ne.$$.fragment,e),x(ft.$$.fragment,e),x(je.$$.fragment,e),x(gt.$$.fragment,e),x(Ce.$$.fragment,e),x(Se.$$.fragment,e),x(_t.$$.fragment,e),x(qe.$$.fragment,e),x(vt.$$.fragment,e),x(Oe.$$.fragment,e),x($t.$$.fragment,e),x(Fe.$$.fragment,e),x(yt.$$.fragment,e),x(bt.$$.fragment,e),x(Te.$$.fragment,e),x(wt.$$.fragment,e),x(Ie.$$.fragment,e),x(Le.$$.fragment,e),x(xt.$$.fragment,e),x(Ae.$$.fragment,e),x(He.$$.fragment,e),x(Et.$$.fragment,e),x(Ue.$$.fragment,e),x(Dt.$$.fragment,e),x(Nt.$$.fragment,e),x(jt.$$.fragment,e),x(Ct.$$.fragment,e),x(St.$$.fragment,e),x(qt.$$.fragment,e),x(Ot.$$.fragment,e),x(Ft.$$.fragment,e),x(Pt.$$.fragment,e),x(Tt.$$.fragment,e),x(It.$$.fragment,e),x(Lt.$$.fragment,e),x(At.$$.fragment,e),x(Ht.$$.fragment,e),x(Ut.$$.fragment,e),x(Vt.$$.fragment,e),io=!0)},o(e){k(s.$$.fragment,e),k(tt.$$.fragment,e),k(at.$$.fragment,e),k(be.$$.fragment,e),k(st.$$.fragment,e),k(xe.$$.fragment,e),k(ke.$$.fragment,e),k(Ee.$$.fragment,e),k(De.$$.fragment,e),k(Ne.$$.fragment,e),k(ft.$$.fragment,e),k(je.$$.fragment,e),k(gt.$$.fragment,e),k(Ce.$$.fragment,e),k(Se.$$.fragment,e),k(_t.$$.fragment,e),k(qe.$$.fragment,e),k(vt.$$.fragment,e),k(Oe.$$.fragment,e),k($t.$$.fragment,e),k(Fe.$$.fragment,e),k(yt.$$.fragment,e),k(bt.$$.fragment,e),k(Te.$$.fragment,e),k(wt.$$.fragment,e),k(Ie.$$.fragment,e),k(Le.$$.fragment,e),k(xt.$$.fragment,e),k(Ae.$$.fragment,e),k(He.$$.fragment,e),k(Et.$$.fragment,e),k(Ue.$$.fragment,e),k(Dt.$$.fragment,e),k(Nt.$$.fragment,e),k(jt.$$.fragment,e),k(Ct.$$.fragment,e),k(St.$$.fragment,e),k(qt.$$.fragment,e),k(Ot.$$.fragment,e),k(Ft.$$.fragment,e),k(Pt.$$.fragment,e),k(Tt.$$.fragment,e),k(It.$$.fragment,e),k(Lt.$$.fragment,e),k(At.$$.fragment,e),k(Ht.$$.fragment,e),k(Ut.$$.fragment,e),k(Vt.$$.fragment,e),io=!1},d(e){a(i),e&&a($),e&&a(h),E(s),e&&a(q),e&&a(O),e&&a(js),e&&a(W),E(tt),e&&a(Cs),e&&a(M),E(at),E(be),e&&a(Ss),e&&a(C),E(st),E(xe),E(ke),E(Ee),E(De),E(Ne),e&&a(qs),e&&a(R),E(ft),E(je),e&&a(Os),e&&a(F),E(gt),E(Ce),E(Se),e&&a(Fs),e&&a(Y),E(_t),E(qe),e&&a(Ps),e&&a(z),E(vt),E(Oe),e&&a(Ts),e&&a(G),E($t),E(Fe),e&&a(Is),e&&a(te),E(yt),e&&a(Ls),e&&a(ae),E(bt),e&&a(As),E(Te,e),e&&a(Hs),e&&a(I),E(wt),E(Ie),E(Le),e&&a(Us),e&&a(L),E(xt),E(Ae),E(He),e&&a(Vs),e&&a(X),E(Et),E(Ue),e&&a(Ms),e&&a(se),E(Dt),e&&a(Bs),e&&a(zt),e&&a(Rs),e&&a(Me),e&&a(Js),e&&a(H),e&&a(Ys),E(Nt,e),e&&a(zs),e&&a(oe),E(jt),e&&a(Gs),e&&a(ne),E(Ct),e&&a(Xs),e&&a(re),E(St),e&&a(Qs),e&&a(ie),E(qt),e&&a(Ws),e&&a(le),E(Ot),e&&a(Ks),e&&a(de),E(Ft),e&&a(Zs),e&&a(ce),E(Pt),e&&a(eo),e&&a(pe),E(Tt),e&&a(to),e&&a(me),E(It),e&&a(ao),e&&a(fe),E(Lt),e&&a(so),e&&a(ge),E(At),e&&a(oo),e&&a(he),E(Ht),e&&a(no),e&&a(ue),E(Ut),e&&a(ro),e&&a(_e),E(Vt)}}}const wd={local:"loading-methods",sections:[{local:"datasets.list_datasets",title:"Datasets"},{local:"datasets.list_metrics",title:"Metrics"},{local:"from-files",sections:[{local:"datasets.packaged_modules.text.TextConfig",title:"Text"},{local:"datasets.packaged_modules.csv.CsvConfig",title:"CSV"},{local:"datasets.packaged_modules.json.JsonConfig",title:"JSON"},{local:"datasets.packaged_modules.parquet.ParquetConfig",title:"Parquet"},{local:"datasets.packaged_modules.sql.SqlConfig",title:"SQL"},{local:"datasets.packaged_modules.imagefolder.ImageFolderConfig",title:"Images"},{local:"datasets.packaged_modules.audiofolder.AudioFolderConfig",title:"Audio"}],title:"From files"}],title:"Loading methods"};function xd(j){return ad(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qd extends Kl{constructor(i){super();Zl(this,i,xd,bd,ed,{})}}export{qd as default,wd as metadata};
