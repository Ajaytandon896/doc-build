import{S as Sl,i as Nl,s as jl,e as s,k as d,w as p,t as r,M as Rl,c as n,d as t,m as l,a as o,x as f,h as i,b as c,F as e,g as v,y as m,L as Cl,q as u,o as g,B as h}from"../../chunks/vendor-e67aec41.js";import{D as b}from"../../chunks/Docstring-ebff01a5.js";import{C as Ht}from"../../chunks/CodeBlock-e2bcf023.js";import{I as kl}from"../../chunks/IconCopyLink-ffd7f84e.js";function Ol(en){let F,ft,N,k,Gt,Ee,tn,Wt,an,ms,M,sn,mt,nn,on,ut,rn,dn,us,$,De,ln,Xt,cn,pn,gt,zt,fn,mn,un,W,ht,Jt,gn,hn,_n,_t,vt,vn,bn,$n,ne,bt,wn,En,Kt,Dn,xn,yn,O,Yt,Tn,In,Qt,Bn,Sn,Zt,Nn,jn,ea,Rn,Cn,oe,xe,kn,ta,On,An,re,ye,Ln,aa,Pn,Vn,ie,Te,Fn,sa,Mn,qn,de,Ie,Un,na,Hn,Gn,le,Be,Wn,oa,Xn,gs,A,Se,zn,ra,Jn,Kn,q,ia,Yn,Qn,da,Zn,eo,la,to,ao,hs,X,Ne,so,ca,no,_s,z,je,oo,pa,ro,vs,S,Re,io,Ce,lo,$t,co,po,fo,ke,mo,wt,uo,go,ho,U,Oe,_o,fa,vo,bo,J,ma,$o,wo,ua,Eo,Do,ga,xo,bs,E,Ae,yo,ce,Le,To,ha,Io,Bo,j,Pe,So,_a,No,jo,va,Ro,Co,Ve,ko,pe,Fe,Oo,Me,Ao,ba,Lo,Po,Vo,fe,qe,Fo,$a,Mo,qo,me,Ue,Uo,wa,Ho,Go,ue,He,Wo,Ea,Xo,zo,ge,Ge,Jo,Da,Ko,$s,B,We,Yo,Et,xa,Qo,Zo,er,Xe,tr,ya,ar,sr,nr,Ta,or,rr,ze,Ia,K,ws,ir,Ba,dr,lr,Sa,cr,pr,Y,Q,Dt,Na,fr,mr,ur,ja,gr,hr,Ra,_r,vr,Z,Ca,ka,br,$r,Oa,wr,Er,Aa,Dr,xr,ee,La,Pa,yr,Tr,Va,Ir,Br,Fa,Sr,Es,L,Je,Nr,Ma,jr,Rr,te,Cr,qa,kr,Or,Ua,Ar,Lr,Ds,y,Ke,Pr,xt,Ha,Vr,Fr,Mr,Ga,qr,Ur,P,yt,Wa,Hr,Gr,Wr,Tt,Xa,Xr,zr,Jr,It,za,Kr,Yr,Qr,Bt,Ja,Zr,ei,ti,St,ai,Ka,si,ni,Ye,oi,Ya,ri,ii,xs,w,Qe,di,Qa,li,ci,Za,pi,fi,Ze,mi,es,ui,gi,et,hi,ts,_i,vi,tt,bi,as,$i,wi,at,ys,ae,st,Ei,ss,Di,Ts,T,nt,xi,ns,yi,Ti,os,Ii,Bi,ot,Si,he,rt,Ni,rs,ji,Ri,H,it,Ci,is,ki,Oi,ds,Ai,Is,se,dt,Li,ls,Pi,Bs,V,lt,Vi,cs,Fi,Mi,_e,ct,qi,ps,Ui,Ss;return Ee=new kl({}),De=new b({props:{name:"class datasets.DatasetBuilder",anchor:"datasets.DatasetBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L170"}}),xe=new b({props:{name:"as_dataset",anchor:"datasets.DatasetBuilder.as_dataset",parameters:[{name:"split",val:": typing.Optional[datasets.splits.Split] = None"},{name:"run_post_process",val:" = True"},{name:"ignore_verifications",val:" = False"},{name:"in_memory",val:" = False"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L734",parametersDescription:[{anchor:"datasets.DatasetBuilder.as_dataset.split",description:"<strong>split</strong> (<code>datasets.Split</code>) &#x2014; Which subset of the data to return.",name:"split"},{anchor:"datasets.DatasetBuilder.as_dataset.run_post_process",description:`<strong>run_post_process</strong> (bool, default=True) &#x2014; Whether to run post-processing dataset transforms and/or add
indexes.`,name:"run_post_process"},{anchor:"datasets.DatasetBuilder.as_dataset.ignore_verifications",description:`<strong>ignore_verifications</strong> (bool, default=False) &#x2014; Whether to ignore the verifications of the
downloaded/processed dataset information (checksums/size/splits/&#x2026;).`,name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.as_dataset.in_memory",description:"<strong>in_memory</strong> (bool, default=False) &#x2014; Whether to copy the data in-memory.",name:"in_memory"}],returnDescription:`
<p>datasets.Dataset</p>
`}}),ye=new b({props:{name:"download_and_prepare",anchor:"datasets.DatasetBuilder.download_and_prepare",parameters:[{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"download_mode",val:": typing.Optional[datasets.utils.download_manager.DownloadMode] = None"},{name:"ignore_verifications",val:": bool = False"},{name:"try_from_hf_gcs",val:": bool = True"},{name:"dl_manager",val:": typing.Optional[datasets.utils.download_manager.DownloadManager] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"**download_and_prepare_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L477",parametersDescription:[{anchor:"datasets.DatasetBuilder.download_and_prepare.download_config",description:'<strong>download_config</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadConfig">DownloadConfig</a>, optional) &#x2014; specific download configuration parameters.',name:"download_config"},{anchor:"datasets.DatasetBuilder.download_and_prepare.download_mode",description:'<strong>download_mode</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadMode">DownloadMode</a>, optional) &#x2014; select the download/generate mode - Default to <code>REUSE_DATASET_IF_EXISTS</code>',name:"download_mode"},{anchor:"datasets.DatasetBuilder.download_and_prepare.ignore_verifications",description:"<strong>ignore_verifications</strong> (<code>bool</code>) &#x2014; Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/&#x2026;)",name:"ignore_verifications"},{anchor:"datasets.DatasetBuilder.download_and_prepare.save_infos",description:"<strong>save_infos</strong> (<code>bool</code>) &#x2014; Save the dataset information (checksums/size/splits/&#x2026;)",name:"save_infos"},{anchor:"datasets.DatasetBuilder.download_and_prepare.try_from_hf_gcs",description:"<strong>try_from_hf_gcs</strong> (<code>bool</code>) &#x2014; If True, it will try to download the already prepared dataset from the Hf google cloud storage",name:"try_from_hf_gcs"},{anchor:"datasets.DatasetBuilder.download_and_prepare.dl_manager",description:'<strong>dl_manager</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadManager">DownloadManager</a>, optional) &#x2014; specific Download Manger to use',name:"dl_manager"},{anchor:"datasets.DatasetBuilder.download_and_prepare.base_path",description:`<strong>base_path</strong> (<code>str</code>, optional) &#x2014; base path for relative paths that are used to download files. This can be a remote url.
If not specified, the value of the <em>base_path</em> attribute (<em>self.base_path</em>) will be used instead.`,name:"base_path"},{anchor:"datasets.DatasetBuilder.download_and_prepare.use_auth_token",description:`<strong>use_auth_token</strong> (<code>Union[str, bool]</code>, optional) &#x2014; Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.
If True, will get token from ~/.huggingface.`,name:"use_auth_token"}]}}),Te=new b({props:{name:"get_all_exported_dataset_infos",anchor:"datasets.DatasetBuilder.get_all_exported_dataset_infos",parameters:[],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L310"}}),Ie=new b({props:{name:"get_exported_dataset_info",anchor:"datasets.DatasetBuilder.get_exported_dataset_info",parameters:[],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L318"}}),Be=new b({props:{name:"get_imported_module_dir",anchor:"datasets.DatasetBuilder.get_imported_module_dir",parameters:[],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L472"}}),Se=new b({props:{name:"class datasets.GeneratorBasedBuilder",anchor:"datasets.GeneratorBasedBuilder",parameters:[{name:"*args",val:""},{name:"writer_batch_size",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L1002"}}),Ne=new b({props:{name:"class datasets.BeamBasedBuilder",anchor:"datasets.BeamBasedBuilder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L1162"}}),je=new b({props:{name:"class datasets.ArrowBasedBuilder",anchor:"datasets.ArrowBasedBuilder",parameters:[{name:"cache_dir",val:": typing.Optional[str] = None"},{name:"name",val:": typing.Optional[str] = None"},{name:"hash",val:": typing.Optional[str] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"info",val:": typing.Optional[datasets.info.DatasetInfo] = None"},{name:"features",val:": typing.Optional[datasets.features.features.Features] = None"},{name:"use_auth_token",val:": typing.Union[str, bool, NoneType] = None"},{name:"namespace",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Union[str, list, dict, datasets.data_files.DataFilesDict, NoneType] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"**config_kwargs",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L1100"}}),Re=new b({props:{name:"class datasets.BuilderConfig",anchor:"datasets.BuilderConfig",parameters:[{name:"name",val:": str = 'default'"},{name:"version",val:": typing.Union[str, datasets.utils.version.Version, NoneType] = '0.0.0'"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"data_files",val:": typing.Optional[datasets.data_files.DataFilesDict] = None"},{name:"description",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L74",parametersDescription:[{anchor:"datasets.BuilderConfig.name",description:"<strong>name</strong> (<code>str</code>, default <code>&quot;default&quot;</code>) &#x2014;",name:"name"},{anchor:"datasets.BuilderConfig.version",description:'<strong>version</strong> (<a href="/docs/datasets/main/en/package_reference/builder_classes#datasets.Version">Version</a> or <code>str</code>, optional) &#x2014;',name:"version"},{anchor:"datasets.BuilderConfig.data_dir",description:"<strong>data_dir</strong> (<code>str</code>, optional) &#x2014;",name:"data_dir"},{anchor:"datasets.BuilderConfig.data_files",description:"<strong>data_files</strong> (<code>str</code> or <code>Sequence</code> or <code>Mapping</code>, optional) &#x2014; Path(s) to source data file(s).",name:"data_files"},{anchor:"datasets.BuilderConfig.description",description:"<strong>description</strong> (<code>str</code>, optional) &#x2014;",name:"description"}]}}),Oe=new b({props:{name:"create_config_id",anchor:"datasets.BuilderConfig.create_config_id",parameters:[{name:"config_kwargs",val:": dict"},{name:"custom_features",val:": typing.Optional[datasets.features.features.Features] = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L113"}}),Ae=new b({props:{name:"class datasets.DownloadManager",anchor:"datasets.DownloadManager",parameters:[{name:"dataset_name",val:": typing.Optional[str] = None"},{name:"data_dir",val:": typing.Optional[str] = None"},{name:"download_config",val:": typing.Optional[datasets.utils.file_utils.DownloadConfig] = None"},{name:"base_path",val:": typing.Optional[str] = None"},{name:"record_checksums",val:": bool = True"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/download_manager.py#L141"}}),Le=new b({props:{name:"download",anchor:"datasets.DownloadManager.download",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/download_manager.py#L259",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),Pe=new b({props:{name:"download_and_extract",anchor:"datasets.DownloadManager.download_and_extract",parameters:[{name:"url_or_urls",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/download_manager.py#L367",returnDescription:`
<p><code>str</code>, extracted paths of given URL(s).</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ve=new Ht({props:{code:"extracted_paths = dl_manager.extract(dl_manager.download(url_or_urls))",highlighted:'<span class="hljs-attr">extracted_paths</span> = dl_manager.extract(dl_manager.download(url_or_urls))'}}),Fe=new b({props:{name:"download_custom",anchor:"datasets.DownloadManager.download_custom",parameters:[{name:"url_or_urls",val:""},{name:"custom_download",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/download_manager.py#L218",returnDescription:`
<p><code>str</code>, The downloaded paths matching the given input
url_or_urls.</p>
`,returnType:`
<p>downloaded_path(s)</p>
`}}),qe=new b({props:{name:"extract",anchor:"datasets.DownloadManager.extract",parameters:[{name:"path_or_paths",val:""},{name:"num_proc",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/download_manager.py#L337",returnDescription:`
<p><code>str</code>, The extracted paths matching the given input
path_or_paths.</p>
`,returnType:`
<p>extracted_path(s)</p>
`}}),Ue=new b({props:{name:"iter_archive",anchor:"datasets.DownloadManager.iter_archive",parameters:[{name:"path_or_buf",val:": typing.Union[str, _io.BufferedReader]"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/download_manager.py#L310",parametersDescription:[{anchor:"datasets.DownloadManager.iter_archive.path_or_buf",description:"<strong>path_or_buf</strong> (<code>str</code> or <code>io.BufferedReader</code>) &#x2014; Archive path or archive binary file object.",name:"path_or_buf"}]}}),He=new b({props:{name:"iter_files",anchor:"datasets.DownloadManager.iter_files",parameters:[{name:"paths",val:": typing.Union[str, typing.List[str]]"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/download_manager.py#L326",parametersDescription:[{anchor:"datasets.DownloadManager.iter_files.paths",description:"<strong>paths</strong> (<code>str</code> or <code>list</code> of <code>str</code>) &#x2014; Root paths.",name:"paths"}]}}),Ge=new b({props:{name:"ship_files_with_pipeline",anchor:"datasets.DownloadManager.ship_files_with_pipeline",parameters:[{name:"downloaded_path_or_paths",val:""},{name:"pipeline",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/download_manager.py#L183"}}),We=new b({props:{name:"class datasets.DownloadMode",anchor:"datasets.DownloadMode",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/download_manager.py#L44"}}),Je=new b({props:{name:"class datasets.SplitGenerator",anchor:"datasets.SplitGenerator",parameters:[{name:"name",val:": str"},{name:"gen_kwargs",val:": typing.Dict = <factory>"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/splits.py#L549",parametersDescription:[{anchor:"datasets.SplitGenerator.name",description:`<strong>name</strong> (str) &#x2014; Name of the Split for which the generator will
create the examples.
**gen_kwargs &#x2014; Keyword arguments to forward to the <code>DatasetBuilder._generate_examples</code> method
of the builder.`,name:"name"}]}}),Ke=new b({props:{name:"class datasets.Split",anchor:"datasets.Split",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/splits.py#L387"}}),Qe=new b({props:{name:"class datasets.NamedSplit",anchor:"datasets.NamedSplit",parameters:[{name:"name",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/splits.py#L303"}}),Ze=new Ht({props:{code:`Each descriptor can be composed with other using addition or slice. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[0:25]) + datasets.Split.TEST

The resulting split will correspond to 25% of the train split merged with
100% of the test split.`,highlighted:`Each descriptor can be composed <span class="hljs-keyword">with</span> other using addition <span class="hljs-keyword">or</span> <span class="hljs-built_in">slice</span>. Ex
split = datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">0</span>:<span class="hljs-number">25</span>]) + datasets.Split.TEST

The resulting split will correspond to <span class="hljs-number">25</span>% of the train split merged <span class="hljs-keyword">with</span>
<span class="hljs-number">100</span>% of the test split.`}}),et=new Ht({props:{code:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[75:])
)  # Error
split = datasets.Split.TEST + datasets.Split.ALL  # Error`,highlighted:`split = (
        datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
        datasets.Split.TRAIN.subsplit(datasets.percent[<span class="hljs-number">75</span>:])
)  <span class="hljs-comment"># Error</span>
split = datasets.Split.TEST + datasets.Split.ALL  <span class="hljs-comment"># Error</span>`}}),tt=new Ht({props:{code:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:25]) +
datasets.Split.TEST.subsplit(datasets.percent[:50])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:50])`,highlighted:`split = (
datasets.Split.TRAIN.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) +
datasets.Split.TEST.subsplit(datasets.percent[:<span class="hljs-number">50</span>])
)
split = (datasets.Split.TRAIN + datasets.Split.TEST).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),at=new Ht({props:{code:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:25]).subsplit(datasets.percent[:25])
split = (train.subsplit(datasets.percent[:25]) + test).subsplit(datasets.percent[:50])`,highlighted:`train = datasets.Split.TRAIN
test = datasets.Split.TEST
split = train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]).subsplit(datasets.percent[:<span class="hljs-number">25</span>])
split = (train.subsplit(datasets.percent[:<span class="hljs-number">25</span>]) + test).subsplit(datasets.percent[:<span class="hljs-number">50</span>])`}}),st=new b({props:{name:"class datasets.NamedSplitAll",anchor:"datasets.NamedSplitAll",parameters:[],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/splits.py#L372"}}),nt=new b({props:{name:"class datasets.ReadInstruction",anchor:"datasets.ReadInstruction",parameters:[{name:"split_name",val:""},{name:"rounding",val:" = None"},{name:"from_",val:" = None"},{name:"to",val:" = None"},{name:"unit",val:" = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_reader.py#L457"}}),ot=new Ht({props:{code:`# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec('test[:33%]'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction('test', to=33, unit='%'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%'))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%]+train[1:-1]')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%]+train[1:-1]'))
ds = datasets.load_dataset('mnist', split=(
datasets.ReadInstruction('test', to=33, unit='%') +
datasets.ReadInstruction('train', from_=1, to=-1, unit='abs')))

# The following lines are equivalent:
ds = datasets.load_dataset('mnist', split='test[:33%](pct1_dropremainder)')
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction.from_spec(
'test[:33%](pct1_dropremainder)'))
ds = datasets.load_dataset('mnist', split=datasets.ReadInstruction(
'test', from_=0, to=33, unit='%', rounding="pct1_dropremainder"))

# 10-fold validation:
tests = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', from_=k, to=k+10, unit='%')
for k in range(0, 100, 10)])
trains = datasets.load_dataset(
'mnist',
[datasets.ReadInstruction('train', to=k, unit='%') + datasets.ReadInstruction('train', from_=k+10, unit='%')
for k in range(0, 100, 10)])`,highlighted:`<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(<span class="hljs-string">&#x27;test[:33%]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%]+train[1:-1]&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=(
datasets.ReadInstruction(<span class="hljs-string">&#x27;test&#x27;</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>) +
datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=<span class="hljs-number">1</span>, to=-<span class="hljs-number">1</span>, unit=<span class="hljs-string">&#x27;abs&#x27;</span>)))

<span class="hljs-comment"># The following lines are equivalent:</span>
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>)
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction.from_spec(
<span class="hljs-string">&#x27;test[:33%](pct1_dropremainder)&#x27;</span>))
ds = datasets.load_dataset(<span class="hljs-string">&#x27;mnist&#x27;</span>, split=datasets.ReadInstruction(
<span class="hljs-string">&#x27;test&#x27;</span>, from_=<span class="hljs-number">0</span>, to=<span class="hljs-number">33</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>, rounding=<span class="hljs-string">&quot;pct1_dropremainder&quot;</span>))

<span class="hljs-comment"># 10-fold validation:</span>
tests = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k, to=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])
trains = datasets.load_dataset(
<span class="hljs-string">&#x27;mnist&#x27;</span>,
[datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, to=k, unit=<span class="hljs-string">&#x27;%&#x27;</span>) + datasets.ReadInstruction(<span class="hljs-string">&#x27;train&#x27;</span>, from_=k+<span class="hljs-number">10</span>, unit=<span class="hljs-string">&#x27;%&#x27;</span>)
<span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>, <span class="hljs-number">10</span>)])`}}),rt=new b({props:{name:"from_spec",anchor:"datasets.ReadInstruction.from_spec",parameters:[{name:"spec",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_reader.py#L537",parametersDescription:[{anchor:"datasets.ReadInstruction.from_spec.spec",description:`<strong>spec</strong> (str) &#x2014; split(s) + optional slice(s) to read + optional rounding
if percents are used as the slicing unit. A slice can be specified,
using absolute numbers (int) or percentages (int). E.g.
<code>test</code>: test split.
<code>test + validation</code>: test split + validation split.
<code>test[10:]</code>: test split, minus its first 10 records.
<code>test[:10%]</code>: first 10% records of test split.
<code>test[:20%](pct1_dropremainder)</code>: first 10% records, rounded with
the <code>pct1_dropremainder</code> rounding.
<code>test[:-5%]+train[40%:60%]</code>: first 95% of test + middle 20% of
train.`,name:"spec"}],returnDescription:`
<p>ReadInstruction instance.</p>
`}}),it=new b({props:{name:"to_absolute",anchor:"datasets.ReadInstruction.to_absolute",parameters:[{name:"name2len",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_reader.py#L605",returnDescription:`
<p>list of _AbsoluteInstruction instances (corresponds to the + in spec).</p>
`}}),dt=new b({props:{name:"class datasets.DownloadConfig",anchor:"datasets.DownloadConfig",parameters:[{name:"cache_dir",val:": typing.Union[pathlib.Path, str, NoneType] = None"},{name:"force_download",val:": bool = False"},{name:"resume_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"proxies",val:": typing.Optional[typing.Dict] = None"},{name:"user_agent",val:": typing.Optional[str] = None"},{name:"extract_compressed_file",val:": bool = False"},{name:"force_extract",val:": bool = False"},{name:"delete_extracted",val:": bool = False"},{name:"use_etag",val:": bool = True"},{name:"num_proc",val:": typing.Optional[int] = None"},{name:"max_retries",val:": int = 1"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"ignore_url_params",val:": bool = False"},{name:"download_desc",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/file_utils.py#L208",parametersDescription:[{anchor:"datasets.DownloadConfig.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>Path</code>, optional) &#x2014; Specify a cache directory to save the file to (overwrite the
default cache dir).`,name:"cache_dir"},{anchor:"datasets.DownloadConfig.force_download",description:`<strong>force_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, re-dowload the file even if it&#x2019;s already cached in
the cache dir.`,name:"force_download"},{anchor:"datasets.DownloadConfig.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True, resume the download if incompletly recieved file is
found.`,name:"resume_download"},{anchor:"datasets.DownloadConfig.proxies",description:"<strong>proxies</strong> (<code>dict</code>, optional) &#x2014;",name:"proxies"},{anchor:"datasets.DownloadConfig.user_agent",description:`<strong>user_agent</strong> (<code>str</code>, optional) &#x2014; Optional string or dict that will be appended to the user-agent on remote
requests.`,name:"user_agent"},{anchor:"datasets.DownloadConfig.extract_compressed_file",description:`<strong>extract_compressed_file</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True and the path point to a zip or tar file,
extract the compressed file in a folder along the archive.`,name:"extract_compressed_file"},{anchor:"datasets.DownloadConfig.force_extract",description:`<strong>force_extract</strong> (<code>bool</code>, default <code>False</code>) &#x2014; If True when extract_compressed_file is True and the archive
was already extracted, re-extract the archive and override the folder where it was extracted.`,name:"force_extract"},{anchor:"datasets.DownloadConfig.delete_extracted",description:"<strong>delete_extracted</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to delete (or keep) the extracted files.",name:"delete_extracted"},{anchor:"datasets.DownloadConfig.use_etag",description:"<strong>use_etag</strong> (<code>bool</code>, default <code>True</code>) &#x2014; Whether to use the ETag HTTP response header to validate the cached files.",name:"use_etag"},{anchor:"datasets.DownloadConfig.num_proc",description:"<strong>num_proc</strong> (<code>int</code>, optional) &#x2014; The number of processes to launch to download the files in parallel.",name:"num_proc"},{anchor:"datasets.DownloadConfig.max_retries",description:"<strong>max_retries</strong> (<code>int</code>, default <code>1</code>) &#x2014; The number of times to retry an HTTP request if it fails.",name:"max_retries"},{anchor:"datasets.DownloadConfig.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <code>bool</code>, optional) &#x2014; Optional string or boolean to use as Bearer token
for remote files on the Datasets Hub. If True, will get token from ~/.huggingface.`,name:"use_auth_token"},{anchor:"datasets.DownloadConfig.ignore_url_params",description:`<strong>ignore_url_params</strong> (<code>bool</code>, default <code>False</code>) &#x2014; Whether to strip all query parameters and #fragments from
the download URL before using it for caching the file.`,name:"ignore_url_params"},{anchor:"datasets.DownloadConfig.download_desc",description:"<strong>download_desc</strong> (<code>str</code>, optional) &#x2014; A description to be displayed alongside with the progress bar while downloading the files.",name:"download_desc"}]}}),lt=new b({props:{name:"class datasets.Version",anchor:"datasets.Version",parameters:[{name:"version_str",val:": str"},{name:"description",val:": typing.Optional[str] = None"},{name:"major",val:": typing.Union[str, int, NoneType] = None"},{name:"minor",val:": typing.Union[str, int, NoneType] = None"},{name:"patch",val:": typing.Union[str, int, NoneType] = None"}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/version.py#L30",parametersDescription:[{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.version_str",description:"<strong>version_str</strong> (<code>str</code>) &#x2014; Eg: &#x201C;1.2.3&#x201D;.",name:"version_str"},{anchor:"datasets.Version.description",description:"<strong>description</strong> (<code>str</code>) &#x2014; A description of what is new in this version.",name:"description"},{anchor:"datasets.Version.major",description:"<strong>major</strong> (<code>str</code>) &#x2014;",name:"major"},{anchor:"datasets.Version.minor",description:"<strong>minor</strong> (<code>str</code>) &#x2014;",name:"minor"},{anchor:"datasets.Version.patch",description:"<strong>patch</strong> (<code>str</code>) &#x2014;",name:"patch"}]}}),ct=new b({props:{name:"match",anchor:"datasets.Version.match",parameters:[{name:"other_version",val:""}],source:"https://github.com/huggingface/datasets/blob/main/src/datasets/utils/version.py#L92"}}),{c(){F=s("meta"),ft=d(),N=s("h1"),k=s("a"),Gt=s("span"),p(Ee.$$.fragment),tn=d(),Wt=s("span"),an=r("Builder classes"),ms=d(),M=s("p"),sn=r("\u{1F917} Datasets relies on two main classes during the dataset building process: "),mt=s("a"),nn=r("datasets.DatasetBuilder"),on=r(" and "),ut=s("a"),rn=r("datasets.BuilderConfig"),dn=r("."),us=d(),$=s("div"),p(De.$$.fragment),ln=d(),Xt=s("p"),cn=r("Abstract base class for all datasets."),pn=d(),gt=s("p"),zt=s("em"),fn=r("DatasetBuilder"),mn=r(" has 3 key methods:"),un=d(),W=s("ul"),ht=s("li"),Jt=s("code"),gn=r("datasets.DatasetBuilder.info"),hn=r(`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),_n=d(),_t=s("li"),vt=s("a"),vn=r("datasets.DatasetBuilder.download_and_prepare()"),bn=r(`: Downloads the source data
and writes it to disk.`),$n=d(),ne=s("li"),bt=s("a"),wn=r("datasets.DatasetBuilder.as_dataset()"),En=r(": Generates a "),Kt=s("em"),Dn=r("Dataset"),xn=r("."),yn=d(),O=s("p"),Yt=s("strong"),Tn=r("Configuration"),In=r(": Some "),Qt=s("em"),Bn=r("DatasetBuilder"),Sn=r(`s expose multiple variants of the
dataset by defining a `),Zt=s("em"),Nn=r("datasets.BuilderConfig"),jn=r(` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),ea=s("code"),Rn=r("datasets.DatasetBuilder.builder_configs()"),Cn=d(),oe=s("div"),p(xe.$$.fragment),kn=d(),ta=s("p"),On=r("Return a Dataset for the specified split."),An=d(),re=s("div"),p(ye.$$.fragment),Ln=d(),aa=s("p"),Pn=r("Downloads and prepares dataset for reading."),Vn=d(),ie=s("div"),p(Te.$$.fragment),Fn=d(),sa=s("p"),Mn=r("Empty dict if doesn\u2019t exist"),qn=d(),de=s("div"),p(Ie.$$.fragment),Un=d(),na=s("p"),Hn=r("Empty DatasetInfo if doesn\u2019t exist"),Gn=d(),le=s("div"),p(Be.$$.fragment),Wn=d(),oa=s("p"),Xn=r("Return the path of the module of this class or subclass."),gs=d(),A=s("div"),p(Se.$$.fragment),zn=d(),ra=s("p"),Jn=r("Base class for datasets with data generation based on dict generators."),Kn=d(),q=s("p"),ia=s("code"),Yn=r("GeneratorBasedBuilder"),Qn=r(` is a convenience class that abstracts away much
of the data writing and reading of `),da=s("code"),Zn=r("DatasetBuilder"),eo=r(`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),la=s("code"),to=r("_split_generators"),ao=r("). See the method docstrings for details."),hs=d(),X=s("div"),p(Ne.$$.fragment),so=d(),ca=s("p"),no=r("Beam based Builder."),_s=d(),z=s("div"),p(je.$$.fragment),oo=d(),pa=s("p"),ro=r("Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),vs=d(),S=s("div"),p(Re.$$.fragment),io=d(),Ce=s("p"),lo=r("Base class for "),$t=s("a"),co=r("DatasetBuilder"),po=r(" data configuration."),fo=d(),ke=s("p"),mo=r(`DatasetBuilder subclasses with data configuration options should subclass
`),wt=s("a"),uo=r("BuilderConfig"),go=r(" and add their own properties."),ho=d(),U=s("div"),p(Oe.$$.fragment),_o=d(),fa=s("p"),vo=r(`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),bo=d(),J=s("ul"),ma=s("li"),$o=r("the config kwargs that can be used to overwrite attributes"),wo=d(),ua=s("li"),Eo=r("the custom features used to write the dataset"),Do=d(),ga=s("li"),xo=r(`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),bs=d(),E=s("div"),p(Ae.$$.fragment),yo=d(),ce=s("div"),p(Le.$$.fragment),To=d(),ha=s("p"),Io=r("Download given url(s)."),Bo=d(),j=s("div"),p(Pe.$$.fragment),So=d(),_a=s("p"),No=r("Download and extract given url_or_urls."),jo=d(),va=s("p"),Ro=r("Is roughly equivalent to:"),Co=d(),p(Ve.$$.fragment),ko=d(),pe=s("div"),p(Fe.$$.fragment),Oo=d(),Me=s("p"),Ao=r("Download given urls(s) by calling "),ba=s("code"),Lo=r("custom_download"),Po=r("."),Vo=d(),fe=s("div"),p(qe.$$.fragment),Fo=d(),$a=s("p"),Mo=r("Extract given path(s)."),qo=d(),me=s("div"),p(Ue.$$.fragment),Uo=d(),wa=s("p"),Ho=r("Iterate over files within an archive."),Go=d(),ue=s("div"),p(He.$$.fragment),Wo=d(),Ea=s("p"),Xo=r("Iterate over file paths."),zo=d(),ge=s("div"),p(Ge.$$.fragment),Jo=d(),Da=s("p"),Ko=r("Ship the files using Beam FileSystems to the pipeline temp dir."),$s=d(),B=s("div"),p(We.$$.fragment),Yo=d(),Et=s("p"),xa=s("code"),Qo=r("Enum"),Zo=r(" for how to treat pre-existing downloads and data."),er=d(),Xe=s("p"),tr=r("The default mode is "),ya=s("code"),ar=r("REUSE_DATASET_IF_EXISTS"),sr=r(`, which will reuse both
raw downloads and the prepared dataset if they exist.`),nr=d(),Ta=s("p"),or=r("The generations modes:"),rr=d(),ze=s("table"),Ia=s("thead"),K=s("tr"),ws=s("th"),ir=d(),Ba=s("th"),dr=r("Downloads"),lr=d(),Sa=s("th"),cr=r("Dataset"),pr=d(),Y=s("tbody"),Q=s("tr"),Dt=s("td"),Na=s("code"),fr=r("REUSE_DATASET_IF_EXISTS"),mr=r(" (default)"),ur=d(),ja=s("td"),gr=r("Reuse"),hr=d(),Ra=s("td"),_r=r("Reuse"),vr=d(),Z=s("tr"),Ca=s("td"),ka=s("code"),br=r("REUSE_CACHE_IF_EXISTS"),$r=d(),Oa=s("td"),wr=r("Reuse"),Er=d(),Aa=s("td"),Dr=r("Fresh"),xr=d(),ee=s("tr"),La=s("td"),Pa=s("code"),yr=r("FORCE_REDOWNLOAD"),Tr=d(),Va=s("td"),Ir=r("Fresh"),Br=d(),Fa=s("td"),Sr=r("Fresh"),Es=d(),L=s("div"),p(Je.$$.fragment),Nr=d(),Ma=s("p"),jr=r("Defines the split information for the generator."),Rr=d(),te=s("p"),Cr=r(`This should be used as returned value of
`),qa=s("code"),kr=r("GeneratorBasedBuilder._split_generators()"),Or=r(`
See `),Ua=s("code"),Ar=r("GeneratorBasedBuilder._split_generators()"),Lr=r(`for more info and example
of usage.`),Ds=d(),y=s("div"),p(Ke.$$.fragment),Pr=d(),xt=s("p"),Ha=s("code"),Vr=r("Enum"),Fr=r(" for dataset splits."),Mr=d(),Ga=s("p"),qr=r(`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),Ur=d(),P=s("ul"),yt=s("li"),Wa=s("code"),Hr=r("TRAIN"),Gr=r(": the training data."),Wr=d(),Tt=s("li"),Xa=s("code"),Xr=r("VALIDATION"),zr=r(`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Jr=d(),It=s("li"),za=s("code"),Kr=r("TEST"),Yr=r(`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Qr=d(),Bt=s("li"),Ja=s("code"),Zr=r("ALL"),ei=r(": the union of all defined dataset splits."),ti=d(),St=s("p"),ai=r("Note: All splits, including compositions inherit from "),Ka=s("code"),si=r("datasets.SplitBase"),ni=d(),Ye=s("p"),oi=r("See the :doc:"),Ya=s("code"),ri=r("guide on splits </loading>"),ii=r(" for more information."),xs=d(),w=s("div"),p(Qe.$$.fragment),di=d(),Qa=s("p"),li=r("Descriptor corresponding to a named split (train, test, \u2026)."),ci=d(),Za=s("p"),pi=r("Example:"),fi=d(),p(Ze.$$.fragment),mi=d(),es=s("p"),ui=r(`Warning:
A split cannot be added twice, so the following will fail:`),gi=d(),p(et.$$.fragment),hi=d(),ts=s("p"),_i=r(`Warning:
The slices can be applied only one time. So the following are valid:`),vi=d(),p(tt.$$.fragment),bi=d(),as=s("p"),$i=r("But not:"),wi=d(),p(at.$$.fragment),ys=d(),ae=s("div"),p(st.$$.fragment),Ei=d(),ss=s("p"),Di=r("Split corresponding to the union of all defined dataset splits."),Ts=d(),T=s("div"),p(nt.$$.fragment),xi=d(),ns=s("p"),yi=r("Reading instruction for a dataset."),Ti=d(),os=s("p"),Ii=r("Examples:"),Bi=d(),p(ot.$$.fragment),Si=d(),he=s("div"),p(rt.$$.fragment),Ni=d(),rs=s("p"),ji=r("Creates a ReadInstruction instance out of a string spec."),Ri=d(),H=s("div"),p(it.$$.fragment),Ci=d(),is=s("p"),ki=r("Translate instruction into a list of absolute instructions."),Oi=d(),ds=s("p"),Ai=r("Those absolute instructions are then to be added together."),Is=d(),se=s("div"),p(dt.$$.fragment),Li=d(),ls=s("p"),Pi=r("Configuration for our cached path manager."),Bs=d(),V=s("div"),p(lt.$$.fragment),Vi=d(),cs=s("p"),Fi=r("Dataset version MAJOR.MINOR.PATCH."),Mi=d(),_e=s("div"),p(ct.$$.fragment),qi=d(),ps=s("p"),Ui=r("Returns True if other_version matches."),this.h()},l(a){const _=Rl('[data-svelte="svelte-1phssyn"]',document.head);F=n(_,"META",{name:!0,content:!0}),_.forEach(t),ft=l(a),N=n(a,"H1",{class:!0});var Ns=o(N);k=n(Ns,"A",{id:!0,class:!0,href:!0});var td=o(k);Gt=n(td,"SPAN",{});var ad=o(Gt);f(Ee.$$.fragment,ad),ad.forEach(t),td.forEach(t),tn=l(Ns),Wt=n(Ns,"SPAN",{});var sd=o(Wt);an=i(sd,"Builder classes"),sd.forEach(t),Ns.forEach(t),ms=l(a),M=n(a,"P",{});var Nt=o(M);sn=i(Nt,"\u{1F917} Datasets relies on two main classes during the dataset building process: "),mt=n(Nt,"A",{href:!0});var nd=o(mt);nn=i(nd,"datasets.DatasetBuilder"),nd.forEach(t),on=i(Nt," and "),ut=n(Nt,"A",{href:!0});var od=o(ut);rn=i(od,"datasets.BuilderConfig"),od.forEach(t),dn=i(Nt,"."),Nt.forEach(t),us=l(a),$=n(a,"DIV",{class:!0});var D=o($);f(De.$$.fragment,D),ln=l(D),Xt=n(D,"P",{});var rd=o(Xt);cn=i(rd,"Abstract base class for all datasets."),rd.forEach(t),pn=l(D),gt=n(D,"P",{});var Hi=o(gt);zt=n(Hi,"EM",{});var id=o(zt);fn=i(id,"DatasetBuilder"),id.forEach(t),mn=i(Hi," has 3 key methods:"),Hi.forEach(t),un=l(D),W=n(D,"UL",{});var jt=o(W);ht=n(jt,"LI",{});var Gi=o(ht);Jt=n(Gi,"CODE",{});var dd=o(Jt);gn=i(dd,"datasets.DatasetBuilder.info"),dd.forEach(t),hn=i(Gi,`: Documents the dataset, including feature
names, types, and shapes, version, splits, citation, etc.`),Gi.forEach(t),_n=l(jt),_t=n(jt,"LI",{});var Wi=o(_t);vt=n(Wi,"A",{href:!0});var ld=o(vt);vn=i(ld,"datasets.DatasetBuilder.download_and_prepare()"),ld.forEach(t),bn=i(Wi,`: Downloads the source data
and writes it to disk.`),Wi.forEach(t),$n=l(jt),ne=n(jt,"LI",{});var fs=o(ne);bt=n(fs,"A",{href:!0});var cd=o(bt);wn=i(cd,"datasets.DatasetBuilder.as_dataset()"),cd.forEach(t),En=i(fs,": Generates a "),Kt=n(fs,"EM",{});var pd=o(Kt);Dn=i(pd,"Dataset"),pd.forEach(t),xn=i(fs,"."),fs.forEach(t),jt.forEach(t),yn=l(D),O=n(D,"P",{});var ve=o(O);Yt=n(ve,"STRONG",{});var fd=o(Yt);Tn=i(fd,"Configuration"),fd.forEach(t),In=i(ve,": Some "),Qt=n(ve,"EM",{});var md=o(Qt);Bn=i(md,"DatasetBuilder"),md.forEach(t),Sn=i(ve,`s expose multiple variants of the
dataset by defining a `),Zt=n(ve,"EM",{});var ud=o(Zt);Nn=i(ud,"datasets.BuilderConfig"),ud.forEach(t),jn=i(ve,` subclass and accepting a
config object (or name) on construction. Configurable datasets expose a
pre-defined set of configurations in `),ea=n(ve,"CODE",{});var gd=o(ea);Rn=i(gd,"datasets.DatasetBuilder.builder_configs()"),gd.forEach(t),ve.forEach(t),Cn=l(D),oe=n(D,"DIV",{class:!0});var js=o(oe);f(xe.$$.fragment,js),kn=l(js),ta=n(js,"P",{});var hd=o(ta);On=i(hd,"Return a Dataset for the specified split."),hd.forEach(t),js.forEach(t),An=l(D),re=n(D,"DIV",{class:!0});var Rs=o(re);f(ye.$$.fragment,Rs),Ln=l(Rs),aa=n(Rs,"P",{});var _d=o(aa);Pn=i(_d,"Downloads and prepares dataset for reading."),_d.forEach(t),Rs.forEach(t),Vn=l(D),ie=n(D,"DIV",{class:!0});var Cs=o(ie);f(Te.$$.fragment,Cs),Fn=l(Cs),sa=n(Cs,"P",{});var vd=o(sa);Mn=i(vd,"Empty dict if doesn\u2019t exist"),vd.forEach(t),Cs.forEach(t),qn=l(D),de=n(D,"DIV",{class:!0});var ks=o(de);f(Ie.$$.fragment,ks),Un=l(ks),na=n(ks,"P",{});var bd=o(na);Hn=i(bd,"Empty DatasetInfo if doesn\u2019t exist"),bd.forEach(t),ks.forEach(t),Gn=l(D),le=n(D,"DIV",{class:!0});var Os=o(le);f(Be.$$.fragment,Os),Wn=l(Os),oa=n(Os,"P",{});var $d=o(oa);Xn=i($d,"Return the path of the module of this class or subclass."),$d.forEach(t),Os.forEach(t),D.forEach(t),gs=l(a),A=n(a,"DIV",{class:!0});var Rt=o(A);f(Se.$$.fragment,Rt),zn=l(Rt),ra=n(Rt,"P",{});var wd=o(ra);Jn=i(wd,"Base class for datasets with data generation based on dict generators."),wd.forEach(t),Kn=l(Rt),q=n(Rt,"P",{});var pt=o(q);ia=n(pt,"CODE",{});var Ed=o(ia);Yn=i(Ed,"GeneratorBasedBuilder"),Ed.forEach(t),Qn=i(pt,` is a convenience class that abstracts away much
of the data writing and reading of `),da=n(pt,"CODE",{});var Dd=o(da);Zn=i(Dd,"DatasetBuilder"),Dd.forEach(t),eo=i(pt,`. It expects subclasses to
implement generators of feature dictionaries across the dataset splits
(`),la=n(pt,"CODE",{});var xd=o(la);to=i(xd,"_split_generators"),xd.forEach(t),ao=i(pt,"). See the method docstrings for details."),pt.forEach(t),Rt.forEach(t),hs=l(a),X=n(a,"DIV",{class:!0});var As=o(X);f(Ne.$$.fragment,As),so=l(As),ca=n(As,"P",{});var yd=o(ca);no=i(yd,"Beam based Builder."),yd.forEach(t),As.forEach(t),_s=l(a),z=n(a,"DIV",{class:!0});var Ls=o(z);f(je.$$.fragment,Ls),oo=l(Ls),pa=n(Ls,"P",{});var Td=o(pa);ro=i(Td,"Base class for datasets with data generation based on Arrow loading functions (CSV/JSON/Parquet)."),Td.forEach(t),Ls.forEach(t),vs=l(a),S=n(a,"DIV",{class:!0});var be=o(S);f(Re.$$.fragment,be),io=l(be),Ce=n(be,"P",{});var Ps=o(Ce);lo=i(Ps,"Base class for "),$t=n(Ps,"A",{href:!0});var Id=o($t);co=i(Id,"DatasetBuilder"),Id.forEach(t),po=i(Ps," data configuration."),Ps.forEach(t),fo=l(be),ke=n(be,"P",{});var Vs=o(ke);mo=i(Vs,`DatasetBuilder subclasses with data configuration options should subclass
`),wt=n(Vs,"A",{href:!0});var Bd=o(wt);uo=i(Bd,"BuilderConfig"),Bd.forEach(t),go=i(Vs," and add their own properties."),Vs.forEach(t),ho=l(be),U=n(be,"DIV",{class:!0});var Ct=o(U);f(Oe.$$.fragment,Ct),_o=l(Ct),fa=n(Ct,"P",{});var Sd=o(fa);vo=i(Sd,`The config id is used to build the cache directory.
By default it is equal to the config name.
However the name of a config is not sufficient to have a unique identifier for the dataset being generated
since it doesn\u2019t take into account:`),Sd.forEach(t),bo=l(Ct),J=n(Ct,"UL",{});var kt=o(J);ma=n(kt,"LI",{});var Nd=o(ma);$o=i(Nd,"the config kwargs that can be used to overwrite attributes"),Nd.forEach(t),wo=l(kt),ua=n(kt,"LI",{});var jd=o(ua);Eo=i(jd,"the custom features used to write the dataset"),jd.forEach(t),Do=l(kt),ga=n(kt,"LI",{});var Rd=o(ga);xo=i(Rd,`the data_files for json/text/csv/pandas datasets
Therefore the config id is just the config name with an optional suffix based on these.`),Rd.forEach(t),kt.forEach(t),Ct.forEach(t),be.forEach(t),bs=l(a),E=n(a,"DIV",{class:!0});var I=o(E);f(Ae.$$.fragment,I),yo=l(I),ce=n(I,"DIV",{class:!0});var Fs=o(ce);f(Le.$$.fragment,Fs),To=l(Fs),ha=n(Fs,"P",{});var Cd=o(ha);Io=i(Cd,"Download given url(s)."),Cd.forEach(t),Fs.forEach(t),Bo=l(I),j=n(I,"DIV",{class:!0});var $e=o(j);f(Pe.$$.fragment,$e),So=l($e),_a=n($e,"P",{});var kd=o(_a);No=i(kd,"Download and extract given url_or_urls."),kd.forEach(t),jo=l($e),va=n($e,"P",{});var Od=o(va);Ro=i(Od,"Is roughly equivalent to:"),Od.forEach(t),Co=l($e),f(Ve.$$.fragment,$e),$e.forEach(t),ko=l(I),pe=n(I,"DIV",{class:!0});var Ms=o(pe);f(Fe.$$.fragment,Ms),Oo=l(Ms),Me=n(Ms,"P",{});var qs=o(Me);Ao=i(qs,"Download given urls(s) by calling "),ba=n(qs,"CODE",{});var Ad=o(ba);Lo=i(Ad,"custom_download"),Ad.forEach(t),Po=i(qs,"."),qs.forEach(t),Ms.forEach(t),Vo=l(I),fe=n(I,"DIV",{class:!0});var Us=o(fe);f(qe.$$.fragment,Us),Fo=l(Us),$a=n(Us,"P",{});var Ld=o($a);Mo=i(Ld,"Extract given path(s)."),Ld.forEach(t),Us.forEach(t),qo=l(I),me=n(I,"DIV",{class:!0});var Hs=o(me);f(Ue.$$.fragment,Hs),Uo=l(Hs),wa=n(Hs,"P",{});var Pd=o(wa);Ho=i(Pd,"Iterate over files within an archive."),Pd.forEach(t),Hs.forEach(t),Go=l(I),ue=n(I,"DIV",{class:!0});var Gs=o(ue);f(He.$$.fragment,Gs),Wo=l(Gs),Ea=n(Gs,"P",{});var Vd=o(Ea);Xo=i(Vd,"Iterate over file paths."),Vd.forEach(t),Gs.forEach(t),zo=l(I),ge=n(I,"DIV",{class:!0});var Ws=o(ge);f(Ge.$$.fragment,Ws),Jo=l(Ws),Da=n(Ws,"P",{});var Fd=o(Da);Ko=i(Fd,"Ship the files using Beam FileSystems to the pipeline temp dir."),Fd.forEach(t),Ws.forEach(t),I.forEach(t),$s=l(a),B=n(a,"DIV",{class:!0});var G=o(B);f(We.$$.fragment,G),Yo=l(G),Et=n(G,"P",{});var Xi=o(Et);xa=n(Xi,"CODE",{});var Md=o(xa);Qo=i(Md,"Enum"),Md.forEach(t),Zo=i(Xi," for how to treat pre-existing downloads and data."),Xi.forEach(t),er=l(G),Xe=n(G,"P",{});var Xs=o(Xe);tr=i(Xs,"The default mode is "),ya=n(Xs,"CODE",{});var qd=o(ya);ar=i(qd,"REUSE_DATASET_IF_EXISTS"),qd.forEach(t),sr=i(Xs,`, which will reuse both
raw downloads and the prepared dataset if they exist.`),Xs.forEach(t),nr=l(G),Ta=n(G,"P",{});var Ud=o(Ta);or=i(Ud,"The generations modes:"),Ud.forEach(t),rr=l(G),ze=n(G,"TABLE",{});var zs=o(ze);Ia=n(zs,"THEAD",{});var Hd=o(Ia);K=n(Hd,"TR",{});var Ot=o(K);ws=n(Ot,"TH",{}),o(ws).forEach(t),ir=l(Ot),Ba=n(Ot,"TH",{});var Gd=o(Ba);dr=i(Gd,"Downloads"),Gd.forEach(t),lr=l(Ot),Sa=n(Ot,"TH",{});var Wd=o(Sa);cr=i(Wd,"Dataset"),Wd.forEach(t),Ot.forEach(t),Hd.forEach(t),pr=l(zs),Y=n(zs,"TBODY",{});var At=o(Y);Q=n(At,"TR",{});var Lt=o(Q);Dt=n(Lt,"TD",{});var zi=o(Dt);Na=n(zi,"CODE",{});var Xd=o(Na);fr=i(Xd,"REUSE_DATASET_IF_EXISTS"),Xd.forEach(t),mr=i(zi," (default)"),zi.forEach(t),ur=l(Lt),ja=n(Lt,"TD",{});var zd=o(ja);gr=i(zd,"Reuse"),zd.forEach(t),hr=l(Lt),Ra=n(Lt,"TD",{});var Jd=o(Ra);_r=i(Jd,"Reuse"),Jd.forEach(t),Lt.forEach(t),vr=l(At),Z=n(At,"TR",{});var Pt=o(Z);Ca=n(Pt,"TD",{});var Kd=o(Ca);ka=n(Kd,"CODE",{});var Yd=o(ka);br=i(Yd,"REUSE_CACHE_IF_EXISTS"),Yd.forEach(t),Kd.forEach(t),$r=l(Pt),Oa=n(Pt,"TD",{});var Qd=o(Oa);wr=i(Qd,"Reuse"),Qd.forEach(t),Er=l(Pt),Aa=n(Pt,"TD",{});var Zd=o(Aa);Dr=i(Zd,"Fresh"),Zd.forEach(t),Pt.forEach(t),xr=l(At),ee=n(At,"TR",{});var Vt=o(ee);La=n(Vt,"TD",{});var el=o(La);Pa=n(el,"CODE",{});var tl=o(Pa);yr=i(tl,"FORCE_REDOWNLOAD"),tl.forEach(t),el.forEach(t),Tr=l(Vt),Va=n(Vt,"TD",{});var al=o(Va);Ir=i(al,"Fresh"),al.forEach(t),Br=l(Vt),Fa=n(Vt,"TD",{});var sl=o(Fa);Sr=i(sl,"Fresh"),sl.forEach(t),Vt.forEach(t),At.forEach(t),zs.forEach(t),G.forEach(t),Es=l(a),L=n(a,"DIV",{class:!0});var Ft=o(L);f(Je.$$.fragment,Ft),Nr=l(Ft),Ma=n(Ft,"P",{});var nl=o(Ma);jr=i(nl,"Defines the split information for the generator."),nl.forEach(t),Rr=l(Ft),te=n(Ft,"P",{});var Mt=o(te);Cr=i(Mt,`This should be used as returned value of
`),qa=n(Mt,"CODE",{});var ol=o(qa);kr=i(ol,"GeneratorBasedBuilder._split_generators()"),ol.forEach(t),Or=i(Mt,`
See `),Ua=n(Mt,"CODE",{});var rl=o(Ua);Ar=i(rl,"GeneratorBasedBuilder._split_generators()"),rl.forEach(t),Lr=i(Mt,`for more info and example
of usage.`),Mt.forEach(t),Ft.forEach(t),Ds=l(a),y=n(a,"DIV",{class:!0});var R=o(y);f(Ke.$$.fragment,R),Pr=l(R),xt=n(R,"P",{});var Ji=o(xt);Ha=n(Ji,"CODE",{});var il=o(Ha);Vr=i(il,"Enum"),il.forEach(t),Fr=i(Ji," for dataset splits."),Ji.forEach(t),Mr=l(R),Ga=n(R,"P",{});var dl=o(Ga);qr=i(dl,`Datasets are typically split into different subsets to be used at various
stages of training and evaluation.`),dl.forEach(t),Ur=l(R),P=n(R,"UL",{});var we=o(P);yt=n(we,"LI",{});var Ki=o(yt);Wa=n(Ki,"CODE",{});var ll=o(Wa);Hr=i(ll,"TRAIN"),ll.forEach(t),Gr=i(Ki,": the training data."),Ki.forEach(t),Wr=l(we),Tt=n(we,"LI",{});var Yi=o(Tt);Xa=n(Yi,"CODE",{});var cl=o(Xa);Xr=i(cl,"VALIDATION"),cl.forEach(t),zr=i(Yi,`: the validation data. If present, this is typically used as
evaluation data while iterating on a model (e.g. changing hyperparameters,
model architecture, etc.).`),Yi.forEach(t),Jr=l(we),It=n(we,"LI",{});var Qi=o(It);za=n(Qi,"CODE",{});var pl=o(za);Kr=i(pl,"TEST"),pl.forEach(t),Yr=i(Qi,`: the testing data. This is the data to report metrics on. Typically
you do not want to use this during model iteration as you may overfit to it.`),Qi.forEach(t),Qr=l(we),Bt=n(we,"LI",{});var Zi=o(Bt);Ja=n(Zi,"CODE",{});var fl=o(Ja);Zr=i(fl,"ALL"),fl.forEach(t),ei=i(Zi,": the union of all defined dataset splits."),Zi.forEach(t),we.forEach(t),ti=l(R),St=n(R,"P",{});var ed=o(St);ai=i(ed,"Note: All splits, including compositions inherit from "),Ka=n(ed,"CODE",{});var ml=o(Ka);si=i(ml,"datasets.SplitBase"),ml.forEach(t),ed.forEach(t),ni=l(R),Ye=n(R,"P",{});var Js=o(Ye);oi=i(Js,"See the :doc:"),Ya=n(Js,"CODE",{});var ul=o(Ya);ri=i(ul,"guide on splits </loading>"),ul.forEach(t),ii=i(Js," for more information."),Js.forEach(t),R.forEach(t),xs=l(a),w=n(a,"DIV",{class:!0});var x=o(w);f(Qe.$$.fragment,x),di=l(x),Qa=n(x,"P",{});var gl=o(Qa);li=i(gl,"Descriptor corresponding to a named split (train, test, \u2026)."),gl.forEach(t),ci=l(x),Za=n(x,"P",{});var hl=o(Za);pi=i(hl,"Example:"),hl.forEach(t),fi=l(x),f(Ze.$$.fragment,x),mi=l(x),es=n(x,"P",{});var _l=o(es);ui=i(_l,`Warning:
A split cannot be added twice, so the following will fail:`),_l.forEach(t),gi=l(x),f(et.$$.fragment,x),hi=l(x),ts=n(x,"P",{});var vl=o(ts);_i=i(vl,`Warning:
The slices can be applied only one time. So the following are valid:`),vl.forEach(t),vi=l(x),f(tt.$$.fragment,x),bi=l(x),as=n(x,"P",{});var bl=o(as);$i=i(bl,"But not:"),bl.forEach(t),wi=l(x),f(at.$$.fragment,x),x.forEach(t),ys=l(a),ae=n(a,"DIV",{class:!0});var Ks=o(ae);f(st.$$.fragment,Ks),Ei=l(Ks),ss=n(Ks,"P",{});var $l=o(ss);Di=i($l,"Split corresponding to the union of all defined dataset splits."),$l.forEach(t),Ks.forEach(t),Ts=l(a),T=n(a,"DIV",{class:!0});var C=o(T);f(nt.$$.fragment,C),xi=l(C),ns=n(C,"P",{});var wl=o(ns);yi=i(wl,"Reading instruction for a dataset."),wl.forEach(t),Ti=l(C),os=n(C,"P",{});var El=o(os);Ii=i(El,"Examples:"),El.forEach(t),Bi=l(C),f(ot.$$.fragment,C),Si=l(C),he=n(C,"DIV",{class:!0});var Ys=o(he);f(rt.$$.fragment,Ys),Ni=l(Ys),rs=n(Ys,"P",{});var Dl=o(rs);ji=i(Dl,"Creates a ReadInstruction instance out of a string spec."),Dl.forEach(t),Ys.forEach(t),Ri=l(C),H=n(C,"DIV",{class:!0});var qt=o(H);f(it.$$.fragment,qt),Ci=l(qt),is=n(qt,"P",{});var xl=o(is);ki=i(xl,"Translate instruction into a list of absolute instructions."),xl.forEach(t),Oi=l(qt),ds=n(qt,"P",{});var yl=o(ds);Ai=i(yl,"Those absolute instructions are then to be added together."),yl.forEach(t),qt.forEach(t),C.forEach(t),Is=l(a),se=n(a,"DIV",{class:!0});var Qs=o(se);f(dt.$$.fragment,Qs),Li=l(Qs),ls=n(Qs,"P",{});var Tl=o(ls);Pi=i(Tl,"Configuration for our cached path manager."),Tl.forEach(t),Qs.forEach(t),Bs=l(a),V=n(a,"DIV",{class:!0});var Ut=o(V);f(lt.$$.fragment,Ut),Vi=l(Ut),cs=n(Ut,"P",{});var Il=o(cs);Fi=i(Il,"Dataset version MAJOR.MINOR.PATCH."),Il.forEach(t),Mi=l(Ut),_e=n(Ut,"DIV",{class:!0});var Zs=o(_e);f(ct.$$.fragment,Zs),qi=l(Zs),ps=n(Zs,"P",{});var Bl=o(ps);Ui=i(Bl,"Returns True if other_version matches."),Bl.forEach(t),Zs.forEach(t),Ut.forEach(t),this.h()},h(){c(F,"name","hf:doc:metadata"),c(F,"content",JSON.stringify(Al)),c(k,"id","datasets.DatasetBuilder"),c(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(k,"href","#datasets.DatasetBuilder"),c(N,"class","relative group"),c(mt,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.DatasetBuilder"),c(ut,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.BuilderConfig"),c(vt,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.DatasetBuilder.download_and_prepare"),c(bt,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.DatasetBuilder.as_dataset"),c(oe,"class","docstring"),c(re,"class","docstring"),c(ie,"class","docstring"),c(de,"class","docstring"),c(le,"class","docstring"),c($,"class","docstring"),c(A,"class","docstring"),c(X,"class","docstring"),c(z,"class","docstring"),c($t,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.DatasetBuilder"),c(wt,"href","/docs/datasets/main/en/package_reference/builder_classes#datasets.BuilderConfig"),c(U,"class","docstring"),c(S,"class","docstring"),c(ce,"class","docstring"),c(j,"class","docstring"),c(pe,"class","docstring"),c(fe,"class","docstring"),c(me,"class","docstring"),c(ue,"class","docstring"),c(ge,"class","docstring"),c(E,"class","docstring"),c(B,"class","docstring"),c(L,"class","docstring"),c(y,"class","docstring"),c(w,"class","docstring"),c(ae,"class","docstring"),c(he,"class","docstring"),c(H,"class","docstring"),c(T,"class","docstring"),c(se,"class","docstring"),c(_e,"class","docstring"),c(V,"class","docstring")},m(a,_){e(document.head,F),v(a,ft,_),v(a,N,_),e(N,k),e(k,Gt),m(Ee,Gt,null),e(N,tn),e(N,Wt),e(Wt,an),v(a,ms,_),v(a,M,_),e(M,sn),e(M,mt),e(mt,nn),e(M,on),e(M,ut),e(ut,rn),e(M,dn),v(a,us,_),v(a,$,_),m(De,$,null),e($,ln),e($,Xt),e(Xt,cn),e($,pn),e($,gt),e(gt,zt),e(zt,fn),e(gt,mn),e($,un),e($,W),e(W,ht),e(ht,Jt),e(Jt,gn),e(ht,hn),e(W,_n),e(W,_t),e(_t,vt),e(vt,vn),e(_t,bn),e(W,$n),e(W,ne),e(ne,bt),e(bt,wn),e(ne,En),e(ne,Kt),e(Kt,Dn),e(ne,xn),e($,yn),e($,O),e(O,Yt),e(Yt,Tn),e(O,In),e(O,Qt),e(Qt,Bn),e(O,Sn),e(O,Zt),e(Zt,Nn),e(O,jn),e(O,ea),e(ea,Rn),e($,Cn),e($,oe),m(xe,oe,null),e(oe,kn),e(oe,ta),e(ta,On),e($,An),e($,re),m(ye,re,null),e(re,Ln),e(re,aa),e(aa,Pn),e($,Vn),e($,ie),m(Te,ie,null),e(ie,Fn),e(ie,sa),e(sa,Mn),e($,qn),e($,de),m(Ie,de,null),e(de,Un),e(de,na),e(na,Hn),e($,Gn),e($,le),m(Be,le,null),e(le,Wn),e(le,oa),e(oa,Xn),v(a,gs,_),v(a,A,_),m(Se,A,null),e(A,zn),e(A,ra),e(ra,Jn),e(A,Kn),e(A,q),e(q,ia),e(ia,Yn),e(q,Qn),e(q,da),e(da,Zn),e(q,eo),e(q,la),e(la,to),e(q,ao),v(a,hs,_),v(a,X,_),m(Ne,X,null),e(X,so),e(X,ca),e(ca,no),v(a,_s,_),v(a,z,_),m(je,z,null),e(z,oo),e(z,pa),e(pa,ro),v(a,vs,_),v(a,S,_),m(Re,S,null),e(S,io),e(S,Ce),e(Ce,lo),e(Ce,$t),e($t,co),e(Ce,po),e(S,fo),e(S,ke),e(ke,mo),e(ke,wt),e(wt,uo),e(ke,go),e(S,ho),e(S,U),m(Oe,U,null),e(U,_o),e(U,fa),e(fa,vo),e(U,bo),e(U,J),e(J,ma),e(ma,$o),e(J,wo),e(J,ua),e(ua,Eo),e(J,Do),e(J,ga),e(ga,xo),v(a,bs,_),v(a,E,_),m(Ae,E,null),e(E,yo),e(E,ce),m(Le,ce,null),e(ce,To),e(ce,ha),e(ha,Io),e(E,Bo),e(E,j),m(Pe,j,null),e(j,So),e(j,_a),e(_a,No),e(j,jo),e(j,va),e(va,Ro),e(j,Co),m(Ve,j,null),e(E,ko),e(E,pe),m(Fe,pe,null),e(pe,Oo),e(pe,Me),e(Me,Ao),e(Me,ba),e(ba,Lo),e(Me,Po),e(E,Vo),e(E,fe),m(qe,fe,null),e(fe,Fo),e(fe,$a),e($a,Mo),e(E,qo),e(E,me),m(Ue,me,null),e(me,Uo),e(me,wa),e(wa,Ho),e(E,Go),e(E,ue),m(He,ue,null),e(ue,Wo),e(ue,Ea),e(Ea,Xo),e(E,zo),e(E,ge),m(Ge,ge,null),e(ge,Jo),e(ge,Da),e(Da,Ko),v(a,$s,_),v(a,B,_),m(We,B,null),e(B,Yo),e(B,Et),e(Et,xa),e(xa,Qo),e(Et,Zo),e(B,er),e(B,Xe),e(Xe,tr),e(Xe,ya),e(ya,ar),e(Xe,sr),e(B,nr),e(B,Ta),e(Ta,or),e(B,rr),e(B,ze),e(ze,Ia),e(Ia,K),e(K,ws),e(K,ir),e(K,Ba),e(Ba,dr),e(K,lr),e(K,Sa),e(Sa,cr),e(ze,pr),e(ze,Y),e(Y,Q),e(Q,Dt),e(Dt,Na),e(Na,fr),e(Dt,mr),e(Q,ur),e(Q,ja),e(ja,gr),e(Q,hr),e(Q,Ra),e(Ra,_r),e(Y,vr),e(Y,Z),e(Z,Ca),e(Ca,ka),e(ka,br),e(Z,$r),e(Z,Oa),e(Oa,wr),e(Z,Er),e(Z,Aa),e(Aa,Dr),e(Y,xr),e(Y,ee),e(ee,La),e(La,Pa),e(Pa,yr),e(ee,Tr),e(ee,Va),e(Va,Ir),e(ee,Br),e(ee,Fa),e(Fa,Sr),v(a,Es,_),v(a,L,_),m(Je,L,null),e(L,Nr),e(L,Ma),e(Ma,jr),e(L,Rr),e(L,te),e(te,Cr),e(te,qa),e(qa,kr),e(te,Or),e(te,Ua),e(Ua,Ar),e(te,Lr),v(a,Ds,_),v(a,y,_),m(Ke,y,null),e(y,Pr),e(y,xt),e(xt,Ha),e(Ha,Vr),e(xt,Fr),e(y,Mr),e(y,Ga),e(Ga,qr),e(y,Ur),e(y,P),e(P,yt),e(yt,Wa),e(Wa,Hr),e(yt,Gr),e(P,Wr),e(P,Tt),e(Tt,Xa),e(Xa,Xr),e(Tt,zr),e(P,Jr),e(P,It),e(It,za),e(za,Kr),e(It,Yr),e(P,Qr),e(P,Bt),e(Bt,Ja),e(Ja,Zr),e(Bt,ei),e(y,ti),e(y,St),e(St,ai),e(St,Ka),e(Ka,si),e(y,ni),e(y,Ye),e(Ye,oi),e(Ye,Ya),e(Ya,ri),e(Ye,ii),v(a,xs,_),v(a,w,_),m(Qe,w,null),e(w,di),e(w,Qa),e(Qa,li),e(w,ci),e(w,Za),e(Za,pi),e(w,fi),m(Ze,w,null),e(w,mi),e(w,es),e(es,ui),e(w,gi),m(et,w,null),e(w,hi),e(w,ts),e(ts,_i),e(w,vi),m(tt,w,null),e(w,bi),e(w,as),e(as,$i),e(w,wi),m(at,w,null),v(a,ys,_),v(a,ae,_),m(st,ae,null),e(ae,Ei),e(ae,ss),e(ss,Di),v(a,Ts,_),v(a,T,_),m(nt,T,null),e(T,xi),e(T,ns),e(ns,yi),e(T,Ti),e(T,os),e(os,Ii),e(T,Bi),m(ot,T,null),e(T,Si),e(T,he),m(rt,he,null),e(he,Ni),e(he,rs),e(rs,ji),e(T,Ri),e(T,H),m(it,H,null),e(H,Ci),e(H,is),e(is,ki),e(H,Oi),e(H,ds),e(ds,Ai),v(a,Is,_),v(a,se,_),m(dt,se,null),e(se,Li),e(se,ls),e(ls,Pi),v(a,Bs,_),v(a,V,_),m(lt,V,null),e(V,Vi),e(V,cs),e(cs,Fi),e(V,Mi),e(V,_e),m(ct,_e,null),e(_e,qi),e(_e,ps),e(ps,Ui),Ss=!0},p:Cl,i(a){Ss||(u(Ee.$$.fragment,a),u(De.$$.fragment,a),u(xe.$$.fragment,a),u(ye.$$.fragment,a),u(Te.$$.fragment,a),u(Ie.$$.fragment,a),u(Be.$$.fragment,a),u(Se.$$.fragment,a),u(Ne.$$.fragment,a),u(je.$$.fragment,a),u(Re.$$.fragment,a),u(Oe.$$.fragment,a),u(Ae.$$.fragment,a),u(Le.$$.fragment,a),u(Pe.$$.fragment,a),u(Ve.$$.fragment,a),u(Fe.$$.fragment,a),u(qe.$$.fragment,a),u(Ue.$$.fragment,a),u(He.$$.fragment,a),u(Ge.$$.fragment,a),u(We.$$.fragment,a),u(Je.$$.fragment,a),u(Ke.$$.fragment,a),u(Qe.$$.fragment,a),u(Ze.$$.fragment,a),u(et.$$.fragment,a),u(tt.$$.fragment,a),u(at.$$.fragment,a),u(st.$$.fragment,a),u(nt.$$.fragment,a),u(ot.$$.fragment,a),u(rt.$$.fragment,a),u(it.$$.fragment,a),u(dt.$$.fragment,a),u(lt.$$.fragment,a),u(ct.$$.fragment,a),Ss=!0)},o(a){g(Ee.$$.fragment,a),g(De.$$.fragment,a),g(xe.$$.fragment,a),g(ye.$$.fragment,a),g(Te.$$.fragment,a),g(Ie.$$.fragment,a),g(Be.$$.fragment,a),g(Se.$$.fragment,a),g(Ne.$$.fragment,a),g(je.$$.fragment,a),g(Re.$$.fragment,a),g(Oe.$$.fragment,a),g(Ae.$$.fragment,a),g(Le.$$.fragment,a),g(Pe.$$.fragment,a),g(Ve.$$.fragment,a),g(Fe.$$.fragment,a),g(qe.$$.fragment,a),g(Ue.$$.fragment,a),g(He.$$.fragment,a),g(Ge.$$.fragment,a),g(We.$$.fragment,a),g(Je.$$.fragment,a),g(Ke.$$.fragment,a),g(Qe.$$.fragment,a),g(Ze.$$.fragment,a),g(et.$$.fragment,a),g(tt.$$.fragment,a),g(at.$$.fragment,a),g(st.$$.fragment,a),g(nt.$$.fragment,a),g(ot.$$.fragment,a),g(rt.$$.fragment,a),g(it.$$.fragment,a),g(dt.$$.fragment,a),g(lt.$$.fragment,a),g(ct.$$.fragment,a),Ss=!1},d(a){t(F),a&&t(ft),a&&t(N),h(Ee),a&&t(ms),a&&t(M),a&&t(us),a&&t($),h(De),h(xe),h(ye),h(Te),h(Ie),h(Be),a&&t(gs),a&&t(A),h(Se),a&&t(hs),a&&t(X),h(Ne),a&&t(_s),a&&t(z),h(je),a&&t(vs),a&&t(S),h(Re),h(Oe),a&&t(bs),a&&t(E),h(Ae),h(Le),h(Pe),h(Ve),h(Fe),h(qe),h(Ue),h(He),h(Ge),a&&t($s),a&&t(B),h(We),a&&t(Es),a&&t(L),h(Je),a&&t(Ds),a&&t(y),h(Ke),a&&t(xs),a&&t(w),h(Qe),h(Ze),h(et),h(tt),h(at),a&&t(ys),a&&t(ae),h(st),a&&t(Ts),a&&t(T),h(nt),h(ot),h(rt),h(it),a&&t(Is),a&&t(se),h(dt),a&&t(Bs),a&&t(V),h(lt),h(ct)}}}const Al={local:"datasets.DatasetBuilder",title:"Builder classes"};function Ll(en,F,ft){let{fw:N}=F;return en.$$set=k=>{"fw"in k&&ft(0,N=k.fw)},[N]}class ql extends Sl{constructor(F){super();Nl(this,F,Ll,Ol,jl,{fw:0})}}export{ql as default,Al as metadata};
