import{S as So,i as zo,s as No,e as a,k as c,w as p,t as s,L as jo,c as o,d as r,m as l,a as n,x as h,h as i,b as d,J as e,g as y,y as g,q as _,o as v,B as x}from"../../../chunks/vendor-b1433968.js";import{T as Lo}from"../../../chunks/Tip-c3840994.js";import{D as P}from"../../../chunks/Docstring-ff504c58.js";import{C as Bo}from"../../../chunks/CodeBlock-a320dbd7.js";import{I as Pt}from"../../../chunks/IconCopyLink-7029626d.js";import"../../../chunks/CopyButton-f65cb278.js";function Ao(pe){let u,b,f,E,D;return{c(){u=a("p"),b=s("Passing "),f=a("code"),E=s("use_auth_token=True"),D=s(" is required when you want to use a private model.")},l($){u=o($,"P",{});var T=n(u);b=i(T,"Passing "),f=o(T,"CODE",{});var z=n(f);E=i(z,"use_auth_token=True"),z.forEach(r),D=i(T," is required when you want to use a private model."),T.forEach(r)},m($,T){y($,u,T),e(u,b),e(u,f),e(f,E),e(u,D)},d($){$&&r(u)}}}function Co(pe){let u,b,f,E,D,$,T,z;return{c(){u=a("p"),b=s("If the "),f=a("code"),E=s("processed_features"),D=s(` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),$=a("code"),T=s("return_tensors"),z=s(`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`)},l(O){u=o(O,"P",{});var M=n(u);b=i(M,"If the "),f=o(M,"CODE",{});var q=n(f);E=i(q,"processed_features"),q.forEach(r),D=i(M,` passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with `),$=o(M,"CODE",{});var Ae=n($);T=i(Ae,"return_tensors"),Ae.forEach(r),z=i(M,`. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`),M.forEach(r)},m(O,M){y(O,u,M),e(u,b),e(u,f),e(f,E),e(u,D),e(u,$),e($,T),e(u,z)},d(O){O&&r(u)}}}function Vo(pe){let u,b,f,E,D,$,T,z,O,M,q,Ae,Ke,tr,rr,Qe,ar,or,Dt,W,Q,Xe,he,nr,Ze,sr,Mt,L,ge,ir,et,cr,lr,k,_e,dr,j,ur,Ce,mr,fr,tt,pr,hr,Ve,gr,_r,vr,X,xr,rt,yr,Er,ve,$r,Z,xe,wr,U,Fr,at,br,Tr,Oe,kr,Ir,qt,H,ee,ot,ye,Pr,nt,Dr,Lt,B,Ee,Mr,st,qr,Lr,N,$e,Sr,it,zr,Nr,R,jr,ct,Br,Ar,lt,Cr,Vr,Or,te,St,J,re,dt,we,Wr,ut,Ur,zt,F,Fe,Hr,Y,Rr,We,Jr,Yr,mt,Gr,Kr,Qr,ft,Xr,Zr,ae,be,ea,pt,ta,ra,oe,Te,aa,ke,oa,ht,na,sa,Nt,G,ne,gt,Ie,ia,_t,ca,jt,w,Pe,la,vt,da,ua,se,De,ma,Me,fa,xt,pa,ha,ga,ie,qe,_a,S,va,yt,xa,ya,Et,Ea,$a,$t,wa,Fa,wt,ba,Ta,ka,ce,Le,Ia,K,Pa,Ft,Da,Ma,bt,qa,La,Sa,le,Se,za,ze,Na,Tt,ja,Ba,Aa,de,Ne,Ca,je,Va,kt,Oa,Wa,Bt;return $=new Pt({}),he=new Pt({}),ge=new P({props:{name:"class transformers.feature_extraction_utils.FeatureExtractionMixin",anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/feature_extraction_utils.py#L202"}}),_e=new P({props:{name:"from_pretrained",anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/feature_extraction_utils.py#L224",parametersDescription:[{anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.feature_extraction_utils.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],returnDescription:`
<p>A feature extractor of type <a
  href="/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.feature_extraction_utils.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),X=new Lo({props:{$$slots:{default:[Ao]},$$scope:{ctx:pe}}}),ve=new Bo({props:{code:`# We can't instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let's show the examples on a
# derived class: *Wav2Vec2FeatureExtractor*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h"
)  # Download feature_extraction_config from huggingface.co and cache.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "./test/saved_model/"
)  # E.g. feature_extractor (or model) was saved using *save_pretrained('./test/saved_model/')*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("./test/saved_model/preprocessor_config.json")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False
)
assert feature_extractor.return_attention_mask is False
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False, return_unused_kwargs=True
)
assert feature_extractor.return_attention_mask is False
assert unused_kwargs == {"foo": False},`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),xe=new P({props:{name:"save_pretrained",anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/feature_extraction_utils.py#L308",parametersDescription:[{anchor:"transformers.feature_extraction_utils.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"}]}}),ye=new Pt({}),Ee=new P({props:{name:"class transformers.SequenceFeatureExtractor",anchor:"transformers.SequenceFeatureExtractor",parameters:[{name:"feature_size",val:": int"},{name:"sampling_rate",val:": int"},{name:"padding_value",val:": float"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/feature_extraction_sequence_utils.py#L38",parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.SequenceFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in Hertz per second (Hz).`,name:"sampling_rate"},{anchor:"transformers.SequenceFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>) &#x2014;
The value that is used to fill the padding values / vectors.`,name:"padding_value"}]}}),$e=new P({props:{name:"pad",anchor:"transformers.SequenceFeatureExtractor.pad",parameters:[{name:"processed_features",val:": typing.Union[transformers.feature_extraction_utils.BatchFeature, typing.List[transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, transformers.feature_extraction_utils.BatchFeature], typing.Dict[str, typing.List[transformers.feature_extraction_utils.BatchFeature]], typing.List[typing.Dict[str, transformers.feature_extraction_utils.BatchFeature]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.file_utils.PaddingStrategy] = True"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.file_utils.TensorType, NoneType] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/feature_extraction_sequence_utils.py#L61",parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.pad.processed_features",description:`<strong>processed_features</strong> (<a href="/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, list of <a href="/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <code>Dict[str, List[float]]</code>, <code>Dict[str, List[List[float]]</code> or <code>List[Dict[str, List[float]]]</code>) &#x2014;
Processed inputs. Can represent one input (<a href="/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a> or <code>Dict[str, List[float]]</code>) or a batch of
input values / vectors (list of <a href="/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.BatchFeature">BatchFeature</a>, <em>Dict[str, List[List[float]]]</em> or <em>List[Dict[str,
List[float]]]</em>) so you can use this method during preprocessing as well as in a PyTorch Dataloader
collate function.</p>
<p>Instead of <code>List[float]</code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.`,name:"processed_features"},{anchor:"transformers.SequenceFeatureExtractor.pad.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.16.2/en/internal/file_utils#transformers.file_utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.SequenceFeatureExtractor.pad.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.SequenceFeatureExtractor.pad.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.SequenceFeatureExtractor.pad.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</p>
<blockquote>
<p>= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</p>
</blockquote>`,name:"pad_to_multiple_of"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.16.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}]}}),te=new Lo({props:{$$slots:{default:[Co]},$$scope:{ctx:pe}}}),we=new Pt({}),Fe=new P({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"tensor_type",val:": typing.Union[NoneType, str, transformers.file_utils.TensorType] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/feature_extraction_utils.py#L60",parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}]}}),be=new P({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": typing.Union[str, transformers.file_utils.TensorType, NoneType] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/feature_extraction_utils.py#L114",parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/v4.16.2/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum
<a href="/docs/transformers/v4.16.2/en/internal/file_utils#transformers.TensorType">TensorType</a>. If <code>None</code>, no modification is done.`,name:"tensor_type"}]}}),Te=new P({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"device",val:": typing.Union[str, ForwardRef('torch.device')]"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/feature_extraction_utils.py#L179",parametersDescription:[{anchor:"transformers.BatchFeature.to.device",description:"<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014; The device to put the tensors on.",name:"device"}],returnDescription:`
<p>The same instance after modification.</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Ie=new Pt({}),Pe=new P({props:{name:"class transformers.ImageFeatureExtractionMixin",anchor:"transformers.ImageFeatureExtractionMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/image_utils.py#L76"}}),De=new P({props:{name:"center_crop",anchor:"transformers.ImageFeatureExtractionMixin.center_crop",parameters:[{name:"image",val:""},{name:"size",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/image_utils.py#L210",parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to which crop the image.`,name:"size"}]}}),qe=new P({props:{name:"normalize",anchor:"transformers.ImageFeatureExtractionMixin.normalize",parameters:[{name:"image",val:""},{name:"mean",val:""},{name:"std",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/image_utils.py#L151",parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.normalize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.mean",description:`<strong>mean</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The mean (per channel) to use for normalization.`,name:"mean"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.std",description:`<strong>std</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The standard deviation (per channel) to use for normalization.`,name:"std"}]}}),Le=new P({props:{name:"resize",anchor:"transformers.ImageFeatureExtractionMixin.resize",parameters:[{name:"image",val:""},{name:"size",val:""},{name:"resample",val:" = 2"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/image_utils.py#L187",parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image.`,name:"size"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"}]}}),Se=new P({props:{name:"to_numpy_array",anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"},{name:"channel_first",val:" = True"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/image_utils.py#L118",parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to a NumPy array.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Will
default to <code>True</code> if the image is a PIL Image or an array/tensor of integers, <code>False</code> otherwise.`,name:"rescale"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.channel_first",description:`<strong>channel_first</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to permute the dimensions of the image to put the channel dimension first.`,name:"channel_first"}]}}),Ne=new P({props:{name:"to_pil_image",anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/v4.16.2/src/transformers/image_utils.py#L88",parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to the PIL Image format.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will
default to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"rescale"}]}}),{c(){u=a("meta"),b=c(),f=a("h1"),E=a("a"),D=a("span"),p($.$$.fragment),T=c(),z=a("span"),O=s("Feature Extractor"),M=c(),q=a("p"),Ae=s(`A feature extractor is in charge of preparing input features for a multi-modal model. This includes feature extraction
from sequences, `),Ke=a("em"),tr=s("e.g."),rr=s(`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),Qe=a("em"),ar=s("e.g."),or=s(` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),Dt=c(),W=a("h2"),Q=a("a"),Xe=a("span"),p(he.$$.fragment),nr=c(),Ze=a("span"),sr=s("FeatureExtractionMixin"),Mt=c(),L=a("div"),p(ge.$$.fragment),ir=c(),et=a("p"),cr=s(`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),lr=c(),k=a("div"),p(_e.$$.fragment),dr=c(),j=a("p"),ur=s("Instantiate a type of "),Ce=a("a"),mr=s("FeatureExtractionMixin"),fr=s(" from a feature extractor, "),tt=a("em"),pr=s("e.g."),hr=s(` a
derived class of `),Ve=a("a"),gr=s("SequenceFeatureExtractor"),_r=s("."),vr=c(),p(X.$$.fragment),xr=c(),rt=a("p"),yr=s("Examples:"),Er=c(),p(ve.$$.fragment),$r=c(),Z=a("div"),p(xe.$$.fragment),wr=c(),U=a("p"),Fr=s("Save a feature_extractor object to the directory "),at=a("code"),br=s("save_directory"),Tr=s(`, so that it can be re-loaded using the
`),Oe=a("a"),kr=s("from_pretrained()"),Ir=s(" class method."),qt=c(),H=a("h2"),ee=a("a"),ot=a("span"),p(ye.$$.fragment),Pr=c(),nt=a("span"),Dr=s("SequenceFeatureExtractor"),Lt=c(),B=a("div"),p(Ee.$$.fragment),Mr=c(),st=a("p"),qr=s("This is a general feature extraction class for speech recognition."),Lr=c(),N=a("div"),p($e.$$.fragment),Sr=c(),it=a("p"),zr=s(`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),Nr=c(),R=a("p"),jr=s("Padding side (left/right) padding values are defined at the feature extractor level (with "),ct=a("code"),Br=s("self.padding_side"),Ar=s(`,
`),lt=a("code"),Cr=s("self.padding_value"),Vr=s(")"),Or=c(),p(te.$$.fragment),St=c(),J=a("h2"),re=a("a"),dt=a("span"),p(we.$$.fragment),Wr=c(),ut=a("span"),Ur=s("BatchFeature"),zt=c(),F=a("div"),p(Fe.$$.fragment),Hr=c(),Y=a("p"),Rr=s("Holds the output of the "),We=a("a"),Jr=s("pad()"),Yr=s(" and feature extractor specific "),mt=a("code"),Gr=s("__call__"),Kr=s(" methods."),Qr=c(),ft=a("p"),Xr=s("This class is derived from a python dictionary and can be used as a dictionary."),Zr=c(),ae=a("div"),p(be.$$.fragment),ea=c(),pt=a("p"),ta=s("Convert the inner content to tensors."),ra=c(),oe=a("div"),p(Te.$$.fragment),aa=c(),ke=a("p"),oa=s("Send all values to device by calling "),ht=a("code"),na=s("v.to(device)"),sa=s(" (PyTorch only)."),Nt=c(),G=a("h2"),ne=a("a"),gt=a("span"),p(Ie.$$.fragment),ia=c(),_t=a("span"),ca=s("ImageFeatureExtractionMixin"),jt=c(),w=a("div"),p(Pe.$$.fragment),la=c(),vt=a("p"),da=s("Mixin that contain utilities for preparing image features."),ua=c(),se=a("div"),p(De.$$.fragment),ma=c(),Me=a("p"),fa=s("Crops "),xt=a("code"),pa=s("image"),ha=s(` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),ga=c(),ie=a("div"),p(qe.$$.fragment),_a=c(),S=a("p"),va=s("Normalizes "),yt=a("code"),xa=s("image"),ya=s(" with "),Et=a("code"),Ea=s("mean"),$a=s(" and "),$t=a("code"),wa=s("std"),Fa=s(". Note that this will trigger a conversion of "),wt=a("code"),ba=s("image"),Ta=s(` to a NumPy array
if it\u2019s a PIL Image.`),ka=c(),ce=a("div"),p(Le.$$.fragment),Ia=c(),K=a("p"),Pa=s("Resizes "),Ft=a("code"),Da=s("image"),Ma=s(". Note that this will trigger a conversion of "),bt=a("code"),qa=s("image"),La=s(" to a PIL Image."),Sa=c(),le=a("div"),p(Se.$$.fragment),za=c(),ze=a("p"),Na=s("Converts "),Tt=a("code"),ja=s("image"),Ba=s(` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),Aa=c(),de=a("div"),p(Ne.$$.fragment),Ca=c(),je=a("p"),Va=s("Converts "),kt=a("code"),Oa=s("image"),Wa=s(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),this.h()},l(t){const m=jo('[data-svelte="svelte-1phssyn"]',document.head);u=o(m,"META",{name:!0,content:!0}),m.forEach(r),b=l(t),f=o(t,"H1",{class:!0});var Be=n(f);E=o(Be,"A",{id:!0,class:!0,href:!0});var It=n(E);D=o(It,"SPAN",{});var Ua=n(D);h($.$$.fragment,Ua),Ua.forEach(r),It.forEach(r),T=l(Be),z=o(Be,"SPAN",{});var Ha=n(z);O=i(Ha,"Feature Extractor"),Ha.forEach(r),Be.forEach(r),M=l(t),q=o(t,"P",{});var Ue=n(q);Ae=i(Ue,`A feature extractor is in charge of preparing input features for a multi-modal model. This includes feature extraction
from sequences, `),Ke=o(Ue,"EM",{});var Ra=n(Ke);tr=i(Ra,"e.g."),Ra.forEach(r),rr=i(Ue,`, pre-processing audio files to Log-Mel Spectrogram features, feature extraction from images
`),Qe=o(Ue,"EM",{});var Ja=n(Qe);ar=i(Ja,"e.g."),Ja.forEach(r),or=i(Ue,` cropping image image files, but also padding, normalization, and conversion to Numpy, PyTorch, and TensorFlow
tensors.`),Ue.forEach(r),Dt=l(t),W=o(t,"H2",{class:!0});var At=n(W);Q=o(At,"A",{id:!0,class:!0,href:!0});var Ya=n(Q);Xe=o(Ya,"SPAN",{});var Ga=n(Xe);h(he.$$.fragment,Ga),Ga.forEach(r),Ya.forEach(r),nr=l(At),Ze=o(At,"SPAN",{});var Ka=n(Ze);sr=i(Ka,"FeatureExtractionMixin"),Ka.forEach(r),At.forEach(r),Mt=l(t),L=o(t,"DIV",{class:!0});var ue=n(L);h(ge.$$.fragment,ue),ir=l(ue),et=o(ue,"P",{});var Qa=n(et);cr=i(Qa,`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),Qa.forEach(r),lr=l(ue),k=o(ue,"DIV",{class:!0});var A=n(k);h(_e.$$.fragment,A),dr=l(A),j=o(A,"P",{});var me=n(j);ur=i(me,"Instantiate a type of "),Ce=o(me,"A",{href:!0});var Xa=n(Ce);mr=i(Xa,"FeatureExtractionMixin"),Xa.forEach(r),fr=i(me," from a feature extractor, "),tt=o(me,"EM",{});var Za=n(tt);pr=i(Za,"e.g."),Za.forEach(r),hr=i(me,` a
derived class of `),Ve=o(me,"A",{href:!0});var eo=n(Ve);gr=i(eo,"SequenceFeatureExtractor"),eo.forEach(r),_r=i(me,"."),me.forEach(r),vr=l(A),h(X.$$.fragment,A),xr=l(A),rt=o(A,"P",{});var to=n(rt);yr=i(to,"Examples:"),to.forEach(r),Er=l(A),h(ve.$$.fragment,A),A.forEach(r),$r=l(ue),Z=o(ue,"DIV",{class:!0});var Ct=n(Z);h(xe.$$.fragment,Ct),wr=l(Ct),U=o(Ct,"P",{});var He=n(U);Fr=i(He,"Save a feature_extractor object to the directory "),at=o(He,"CODE",{});var ro=n(at);br=i(ro,"save_directory"),ro.forEach(r),Tr=i(He,`, so that it can be re-loaded using the
`),Oe=o(He,"A",{href:!0});var ao=n(Oe);kr=i(ao,"from_pretrained()"),ao.forEach(r),Ir=i(He," class method."),He.forEach(r),Ct.forEach(r),ue.forEach(r),qt=l(t),H=o(t,"H2",{class:!0});var Vt=n(H);ee=o(Vt,"A",{id:!0,class:!0,href:!0});var oo=n(ee);ot=o(oo,"SPAN",{});var no=n(ot);h(ye.$$.fragment,no),no.forEach(r),oo.forEach(r),Pr=l(Vt),nt=o(Vt,"SPAN",{});var so=n(nt);Dr=i(so,"SequenceFeatureExtractor"),so.forEach(r),Vt.forEach(r),Lt=l(t),B=o(t,"DIV",{class:!0});var Re=n(B);h(Ee.$$.fragment,Re),Mr=l(Re),st=o(Re,"P",{});var io=n(st);qr=i(io,"This is a general feature extraction class for speech recognition."),io.forEach(r),Lr=l(Re),N=o(Re,"DIV",{class:!0});var fe=n(N);h($e.$$.fragment,fe),Sr=l(fe),it=o(fe,"P",{});var co=n(it);zr=i(co,`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`),co.forEach(r),Nr=l(fe),R=o(fe,"P",{});var Je=n(R);jr=i(Je,"Padding side (left/right) padding values are defined at the feature extractor level (with "),ct=o(Je,"CODE",{});var lo=n(ct);Br=i(lo,"self.padding_side"),lo.forEach(r),Ar=i(Je,`,
`),lt=o(Je,"CODE",{});var uo=n(lt);Cr=i(uo,"self.padding_value"),uo.forEach(r),Vr=i(Je,")"),Je.forEach(r),Or=l(fe),h(te.$$.fragment,fe),fe.forEach(r),Re.forEach(r),St=l(t),J=o(t,"H2",{class:!0});var Ot=n(J);re=o(Ot,"A",{id:!0,class:!0,href:!0});var mo=n(re);dt=o(mo,"SPAN",{});var fo=n(dt);h(we.$$.fragment,fo),fo.forEach(r),mo.forEach(r),Wr=l(Ot),ut=o(Ot,"SPAN",{});var po=n(ut);Ur=i(po,"BatchFeature"),po.forEach(r),Ot.forEach(r),zt=l(t),F=o(t,"DIV",{class:!0});var C=n(F);h(Fe.$$.fragment,C),Hr=l(C),Y=o(C,"P",{});var Ye=n(Y);Rr=i(Ye,"Holds the output of the "),We=o(Ye,"A",{href:!0});var ho=n(We);Jr=i(ho,"pad()"),ho.forEach(r),Yr=i(Ye," and feature extractor specific "),mt=o(Ye,"CODE",{});var go=n(mt);Gr=i(go,"__call__"),go.forEach(r),Kr=i(Ye," methods."),Ye.forEach(r),Qr=l(C),ft=o(C,"P",{});var _o=n(ft);Xr=i(_o,"This class is derived from a python dictionary and can be used as a dictionary."),_o.forEach(r),Zr=l(C),ae=o(C,"DIV",{class:!0});var Wt=n(ae);h(be.$$.fragment,Wt),ea=l(Wt),pt=o(Wt,"P",{});var vo=n(pt);ta=i(vo,"Convert the inner content to tensors."),vo.forEach(r),Wt.forEach(r),ra=l(C),oe=o(C,"DIV",{class:!0});var Ut=n(oe);h(Te.$$.fragment,Ut),aa=l(Ut),ke=o(Ut,"P",{});var Ht=n(ke);oa=i(Ht,"Send all values to device by calling "),ht=o(Ht,"CODE",{});var xo=n(ht);na=i(xo,"v.to(device)"),xo.forEach(r),sa=i(Ht," (PyTorch only)."),Ht.forEach(r),Ut.forEach(r),C.forEach(r),Nt=l(t),G=o(t,"H2",{class:!0});var Rt=n(G);ne=o(Rt,"A",{id:!0,class:!0,href:!0});var yo=n(ne);gt=o(yo,"SPAN",{});var Eo=n(gt);h(Ie.$$.fragment,Eo),Eo.forEach(r),yo.forEach(r),ia=l(Rt),_t=o(Rt,"SPAN",{});var $o=n(_t);ca=i($o,"ImageFeatureExtractionMixin"),$o.forEach(r),Rt.forEach(r),jt=l(t),w=o(t,"DIV",{class:!0});var I=n(w);h(Pe.$$.fragment,I),la=l(I),vt=o(I,"P",{});var wo=n(vt);da=i(wo,"Mixin that contain utilities for preparing image features."),wo.forEach(r),ua=l(I),se=o(I,"DIV",{class:!0});var Jt=n(se);h(De.$$.fragment,Jt),ma=l(Jt),Me=o(Jt,"P",{});var Yt=n(Me);fa=i(Yt,"Crops "),xt=o(Yt,"CODE",{});var Fo=n(xt);pa=i(Fo,"image"),Fo.forEach(r),ha=i(Yt,` to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`),Yt.forEach(r),Jt.forEach(r),ga=l(I),ie=o(I,"DIV",{class:!0});var Gt=n(ie);h(qe.$$.fragment,Gt),_a=l(Gt),S=o(Gt,"P",{});var V=n(S);va=i(V,"Normalizes "),yt=o(V,"CODE",{});var bo=n(yt);xa=i(bo,"image"),bo.forEach(r),ya=i(V," with "),Et=o(V,"CODE",{});var To=n(Et);Ea=i(To,"mean"),To.forEach(r),$a=i(V," and "),$t=o(V,"CODE",{});var ko=n($t);wa=i(ko,"std"),ko.forEach(r),Fa=i(V,". Note that this will trigger a conversion of "),wt=o(V,"CODE",{});var Io=n(wt);ba=i(Io,"image"),Io.forEach(r),Ta=i(V,` to a NumPy array
if it\u2019s a PIL Image.`),V.forEach(r),Gt.forEach(r),ka=l(I),ce=o(I,"DIV",{class:!0});var Kt=n(ce);h(Le.$$.fragment,Kt),Ia=l(Kt),K=o(Kt,"P",{});var Ge=n(K);Pa=i(Ge,"Resizes "),Ft=o(Ge,"CODE",{});var Po=n(Ft);Da=i(Po,"image"),Po.forEach(r),Ma=i(Ge,". Note that this will trigger a conversion of "),bt=o(Ge,"CODE",{});var Do=n(bt);qa=i(Do,"image"),Do.forEach(r),La=i(Ge," to a PIL Image."),Ge.forEach(r),Kt.forEach(r),Sa=l(I),le=o(I,"DIV",{class:!0});var Qt=n(le);h(Se.$$.fragment,Qt),za=l(Qt),ze=o(Qt,"P",{});var Xt=n(ze);Na=i(Xt,"Converts "),Tt=o(Xt,"CODE",{});var Mo=n(Tt);ja=i(Mo,"image"),Mo.forEach(r),Ba=i(Xt,` to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`),Xt.forEach(r),Qt.forEach(r),Aa=l(I),de=o(I,"DIV",{class:!0});var Zt=n(de);h(Ne.$$.fragment,Zt),Ca=l(Zt),je=o(Zt,"P",{});var er=n(je);Va=i(er,"Converts "),kt=o(er,"CODE",{});var qo=n(kt);Oa=i(qo,"image"),qo.forEach(r),Wa=i(er,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),er.forEach(r),Zt.forEach(r),I.forEach(r),this.h()},h(){d(u,"name","hf:doc:metadata"),d(u,"content",JSON.stringify(Oo)),d(E,"id","feature-extractor"),d(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(E,"href","#feature-extractor"),d(f,"class","relative group"),d(Q,"id","transformers.feature_extraction_utils.FeatureExtractionMixin"),d(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Q,"href","#transformers.feature_extraction_utils.FeatureExtractionMixin"),d(W,"class","relative group"),d(Ce,"href","/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.feature_extraction_utils.FeatureExtractionMixin"),d(Ve,"href","/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),d(k,"class","docstring"),d(Oe,"href","/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.feature_extraction_utils.FeatureExtractionMixin.from_pretrained"),d(Z,"class","docstring"),d(L,"class","docstring"),d(ee,"id","transformers.SequenceFeatureExtractor"),d(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ee,"href","#transformers.SequenceFeatureExtractor"),d(H,"class","relative group"),d(N,"class","docstring"),d(B,"class","docstring"),d(re,"id","transformers.BatchFeature"),d(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(re,"href","#transformers.BatchFeature"),d(J,"class","relative group"),d(We,"href","/docs/transformers/v4.16.2/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad"),d(ae,"class","docstring"),d(oe,"class","docstring"),d(F,"class","docstring"),d(ne,"id","transformers.ImageFeatureExtractionMixin"),d(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ne,"href","#transformers.ImageFeatureExtractionMixin"),d(G,"class","relative group"),d(se,"class","docstring"),d(ie,"class","docstring"),d(ce,"class","docstring"),d(le,"class","docstring"),d(de,"class","docstring"),d(w,"class","docstring")},m(t,m){e(document.head,u),y(t,b,m),y(t,f,m),e(f,E),e(E,D),g($,D,null),e(f,T),e(f,z),e(z,O),y(t,M,m),y(t,q,m),e(q,Ae),e(q,Ke),e(Ke,tr),e(q,rr),e(q,Qe),e(Qe,ar),e(q,or),y(t,Dt,m),y(t,W,m),e(W,Q),e(Q,Xe),g(he,Xe,null),e(W,nr),e(W,Ze),e(Ze,sr),y(t,Mt,m),y(t,L,m),g(ge,L,null),e(L,ir),e(L,et),e(et,cr),e(L,lr),e(L,k),g(_e,k,null),e(k,dr),e(k,j),e(j,ur),e(j,Ce),e(Ce,mr),e(j,fr),e(j,tt),e(tt,pr),e(j,hr),e(j,Ve),e(Ve,gr),e(j,_r),e(k,vr),g(X,k,null),e(k,xr),e(k,rt),e(rt,yr),e(k,Er),g(ve,k,null),e(L,$r),e(L,Z),g(xe,Z,null),e(Z,wr),e(Z,U),e(U,Fr),e(U,at),e(at,br),e(U,Tr),e(U,Oe),e(Oe,kr),e(U,Ir),y(t,qt,m),y(t,H,m),e(H,ee),e(ee,ot),g(ye,ot,null),e(H,Pr),e(H,nt),e(nt,Dr),y(t,Lt,m),y(t,B,m),g(Ee,B,null),e(B,Mr),e(B,st),e(st,qr),e(B,Lr),e(B,N),g($e,N,null),e(N,Sr),e(N,it),e(it,zr),e(N,Nr),e(N,R),e(R,jr),e(R,ct),e(ct,Br),e(R,Ar),e(R,lt),e(lt,Cr),e(R,Vr),e(N,Or),g(te,N,null),y(t,St,m),y(t,J,m),e(J,re),e(re,dt),g(we,dt,null),e(J,Wr),e(J,ut),e(ut,Ur),y(t,zt,m),y(t,F,m),g(Fe,F,null),e(F,Hr),e(F,Y),e(Y,Rr),e(Y,We),e(We,Jr),e(Y,Yr),e(Y,mt),e(mt,Gr),e(Y,Kr),e(F,Qr),e(F,ft),e(ft,Xr),e(F,Zr),e(F,ae),g(be,ae,null),e(ae,ea),e(ae,pt),e(pt,ta),e(F,ra),e(F,oe),g(Te,oe,null),e(oe,aa),e(oe,ke),e(ke,oa),e(ke,ht),e(ht,na),e(ke,sa),y(t,Nt,m),y(t,G,m),e(G,ne),e(ne,gt),g(Ie,gt,null),e(G,ia),e(G,_t),e(_t,ca),y(t,jt,m),y(t,w,m),g(Pe,w,null),e(w,la),e(w,vt),e(vt,da),e(w,ua),e(w,se),g(De,se,null),e(se,ma),e(se,Me),e(Me,fa),e(Me,xt),e(xt,pa),e(Me,ha),e(w,ga),e(w,ie),g(qe,ie,null),e(ie,_a),e(ie,S),e(S,va),e(S,yt),e(yt,xa),e(S,ya),e(S,Et),e(Et,Ea),e(S,$a),e(S,$t),e($t,wa),e(S,Fa),e(S,wt),e(wt,ba),e(S,Ta),e(w,ka),e(w,ce),g(Le,ce,null),e(ce,Ia),e(ce,K),e(K,Pa),e(K,Ft),e(Ft,Da),e(K,Ma),e(K,bt),e(bt,qa),e(K,La),e(w,Sa),e(w,le),g(Se,le,null),e(le,za),e(le,ze),e(ze,Na),e(ze,Tt),e(Tt,ja),e(ze,Ba),e(w,Aa),e(w,de),g(Ne,de,null),e(de,Ca),e(de,je),e(je,Va),e(je,kt),e(kt,Oa),e(je,Wa),Bt=!0},p(t,[m]){const Be={};m&2&&(Be.$$scope={dirty:m,ctx:t}),X.$set(Be);const It={};m&2&&(It.$$scope={dirty:m,ctx:t}),te.$set(It)},i(t){Bt||(_($.$$.fragment,t),_(he.$$.fragment,t),_(ge.$$.fragment,t),_(_e.$$.fragment,t),_(X.$$.fragment,t),_(ve.$$.fragment,t),_(xe.$$.fragment,t),_(ye.$$.fragment,t),_(Ee.$$.fragment,t),_($e.$$.fragment,t),_(te.$$.fragment,t),_(we.$$.fragment,t),_(Fe.$$.fragment,t),_(be.$$.fragment,t),_(Te.$$.fragment,t),_(Ie.$$.fragment,t),_(Pe.$$.fragment,t),_(De.$$.fragment,t),_(qe.$$.fragment,t),_(Le.$$.fragment,t),_(Se.$$.fragment,t),_(Ne.$$.fragment,t),Bt=!0)},o(t){v($.$$.fragment,t),v(he.$$.fragment,t),v(ge.$$.fragment,t),v(_e.$$.fragment,t),v(X.$$.fragment,t),v(ve.$$.fragment,t),v(xe.$$.fragment,t),v(ye.$$.fragment,t),v(Ee.$$.fragment,t),v($e.$$.fragment,t),v(te.$$.fragment,t),v(we.$$.fragment,t),v(Fe.$$.fragment,t),v(be.$$.fragment,t),v(Te.$$.fragment,t),v(Ie.$$.fragment,t),v(Pe.$$.fragment,t),v(De.$$.fragment,t),v(qe.$$.fragment,t),v(Le.$$.fragment,t),v(Se.$$.fragment,t),v(Ne.$$.fragment,t),Bt=!1},d(t){r(u),t&&r(b),t&&r(f),x($),t&&r(M),t&&r(q),t&&r(Dt),t&&r(W),x(he),t&&r(Mt),t&&r(L),x(ge),x(_e),x(X),x(ve),x(xe),t&&r(qt),t&&r(H),x(ye),t&&r(Lt),t&&r(B),x(Ee),x($e),x(te),t&&r(St),t&&r(J),x(we),t&&r(zt),t&&r(F),x(Fe),x(be),x(Te),t&&r(Nt),t&&r(G),x(Ie),t&&r(jt),t&&r(w),x(Pe),x(De),x(qe),x(Le),x(Se),x(Ne)}}}const Oo={local:"feature-extractor",sections:[{local:"transformers.feature_extraction_utils.FeatureExtractionMixin",title:"FeatureExtractionMixin"},{local:"transformers.SequenceFeatureExtractor",title:"SequenceFeatureExtractor"},{local:"transformers.BatchFeature",title:"BatchFeature"},{local:"transformers.ImageFeatureExtractionMixin",title:"ImageFeatureExtractionMixin"}],title:"Feature Extractor"};function Wo(pe,u,b){let{fw:f}=u;return pe.$$set=E=>{"fw"in E&&b(0,f=E.fw)},[f]}class Ko extends So{constructor(u){super();zo(this,u,Wo,Vo,No,{fw:0})}}export{Ko as default,Oo as metadata};
