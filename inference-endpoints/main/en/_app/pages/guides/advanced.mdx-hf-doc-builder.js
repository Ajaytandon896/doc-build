import{S as _o,i as Eo,s as wo,e as a,k as f,w as Po,t as i,M as go,c as r,d as t,m as p,a as n,x as To,h as u,b as c,N as bo,G as o,g as l,y as Ao,L as Co,q as So,o as Ro,B as ko,v as Mo}from"../../chunks/vendor-hf-doc-builder.js";import{I as Go}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Io(St){let y,pe,_,E,K,g,qe,Q,ze,ce,w,Fe,R,$e,Oe,me,k,X,He,he,M,Le,de,G,Z,We,ve,I,ee,Ve,ye,T,Rt,_e,D,te,Ye,Ee,U,Be,we,N,oe,Je,Pe,x,ae,je,ge,h,Ke,q,Qe,Xe,z,Ze,et,F,tt,ot,$,at,rt,Te,O,re,st,be,H,se,nt,Ae,L,lt,Ce,W,ne,it,Se,V,le,ut,Re,Y,ft,ke,B,ie,pt,Me,J,ue,ct,Ge,m,mt,fe,ht,dt,b,vt,yt,A,_t,Et,C,wt,Pt,S,gt,Tt,Ie,P,bt,j,At,Ct,De;return g=new Go({}),{c(){y=a("meta"),pe=f(),_=a("h1"),E=a("a"),K=a("span"),Po(g.$$.fragment),qe=f(),Q=a("span"),ze=i("Advanced Setup (Instance Types, Auto Scaling, Versioning)"),ce=f(),w=a("p"),Fe=i("We have seen how fast and easy it is to deploy an Endpoint in "),R=a("a"),$e=i("Create your first Endpoint"),Oe=i(", but that\u2019s not all you can manage. During the creation process and after selecting your Cloud Provider and Region, click on the [Advanced configuration] button to reveal further configuration options for your Endpoint."),me=f(),k=a("p"),X=a("strong"),He=i("Instance type"),he=f(),M=a("p"),Le=i("\u{1F917} Inference Endpoints offers a selection of curated CPU and GPU instances."),de=f(),G=a("p"),Z=a("em"),We=i("Note: Your Hugging Face account comes with a capacity quota for CPU and GPU instances. To increase your quota or request new instance types, please check with us."),ve=f(),I=a("p"),ee=a("em"),Ve=i("Default: CPU-medium"),ye=f(),T=a("img"),_e=f(),D=a("p"),te=a("strong"),Ye=i("Replica autoscaling"),Ee=f(),U=a("p"),Be=i("Set the range (minimum (>=1) and maximum ) of replicas you want your Endpoint to automatically scale within based on utilization."),we=f(),N=a("p"),oe=a("em"),Je=i("Default: min 1; max 2"),Pe=f(),x=a("p"),ae=a("strong"),je=i("Task"),ge=f(),h=a("p"),Ke=i("Select a "),q=a("a"),Qe=i("supported Machine Learning Task"),Xe=i(", or set to "),z=a("a"),Ze=i("Custom"),et=i(". "),F=a("a"),tt=i("Custom"),ot=i(" can/should be used when you are not using a Transformers-based model or when you want to customize the inference pipeline, see "),$=a("a"),at=i("Create your own Inference handler"),rt=i("."),Te=f(),O=a("p"),re=a("em"),st=i("Default: derived from the model repository."),be=f(),H=a("p"),se=a("strong"),nt=i("Framework"),Ae=f(),L=a("p"),lt=i("For Transformers models, if both PyTorch and TensorFlow weights are both available, you can select which model weights to use. This will help reduce the image artifact size and accelerate startups/scaling of your endpoints."),Ce=f(),W=a("p"),ne=a("em"),it=i("Default: PyTorch if available."),Se=f(),V=a("p"),le=a("strong"),ut=i("Revision"),Re=f(),Y=a("p"),ft=i("Create your Endpoint targeting a specific revision commit for its source Hugging Face Model Repository. This allows you to version your endpoint and make sure you are always using the same weights even if you are updating the Model Repository."),ke=f(),B=a("p"),ie=a("em"),pt=i("Default: The most recent commit."),Me=f(),J=a("p"),ue=a("strong"),ct=i("Image"),Ge=f(),m=a("p"),mt=i("Allows you to provide a custom container image you want to deploy into an Endpoint. Those can be public images, e.g "),fe=a("em"),ht=i("tensorflow/serving:2.7.3,"),dt=i(" or private Images hosted on "),b=a("a"),vt=i("Docker hub"),yt=i(", "),A=a("a"),_t=i("AWS ECR"),Et=i(", "),C=a("a"),wt=i("Azure ACR"),Pt=i(", or "),S=a("a"),gt=i("Google GCR"),Tt=i("."),Ie=f(),P=a("p"),bt=i("More on how to "),j=a("a"),At=i("\u201CUse your own custom container\u201D"),Ct=i(" below."),this.h()},l(e){const s=go('[data-svelte="svelte-1phssyn"]',document.head);y=r(s,"META",{name:!0,content:!0}),s.forEach(t),pe=p(e),_=r(e,"H1",{class:!0});var Ue=n(_);E=r(Ue,"A",{id:!0,class:!0,href:!0});var kt=n(E);K=r(kt,"SPAN",{});var Mt=n(K);To(g.$$.fragment,Mt),Mt.forEach(t),kt.forEach(t),qe=p(Ue),Q=r(Ue,"SPAN",{});var Gt=n(Q);ze=u(Gt,"Advanced Setup (Instance Types, Auto Scaling, Versioning)"),Gt.forEach(t),Ue.forEach(t),ce=p(e),w=r(e,"P",{});var Ne=n(w);Fe=u(Ne,"We have seen how fast and easy it is to deploy an Endpoint in "),R=r(Ne,"A",{href:!0});var It=n(R);$e=u(It,"Create your first Endpoint"),It.forEach(t),Oe=u(Ne,", but that\u2019s not all you can manage. During the creation process and after selecting your Cloud Provider and Region, click on the [Advanced configuration] button to reveal further configuration options for your Endpoint."),Ne.forEach(t),me=p(e),k=r(e,"P",{});var Dt=n(k);X=r(Dt,"STRONG",{});var Ut=n(X);He=u(Ut,"Instance type"),Ut.forEach(t),Dt.forEach(t),he=p(e),M=r(e,"P",{});var Nt=n(M);Le=u(Nt,"\u{1F917} Inference Endpoints offers a selection of curated CPU and GPU instances."),Nt.forEach(t),de=p(e),G=r(e,"P",{});var xt=n(G);Z=r(xt,"EM",{});var qt=n(Z);We=u(qt,"Note: Your Hugging Face account comes with a capacity quota for CPU and GPU instances. To increase your quota or request new instance types, please check with us."),qt.forEach(t),xt.forEach(t),ve=p(e),I=r(e,"P",{});var zt=n(I);ee=r(zt,"EM",{});var Ft=n(ee);Ve=u(Ft,"Default: CPU-medium"),Ft.forEach(t),zt.forEach(t),ye=p(e),T=r(e,"IMG",{src:!0,alt:!0}),_e=p(e),D=r(e,"P",{});var $t=n(D);te=r($t,"STRONG",{});var Ot=n(te);Ye=u(Ot,"Replica autoscaling"),Ot.forEach(t),$t.forEach(t),Ee=p(e),U=r(e,"P",{});var Ht=n(U);Be=u(Ht,"Set the range (minimum (>=1) and maximum ) of replicas you want your Endpoint to automatically scale within based on utilization."),Ht.forEach(t),we=p(e),N=r(e,"P",{});var Lt=n(N);oe=r(Lt,"EM",{});var Wt=n(oe);Je=u(Wt,"Default: min 1; max 2"),Wt.forEach(t),Lt.forEach(t),Pe=p(e),x=r(e,"P",{});var Vt=n(x);ae=r(Vt,"STRONG",{});var Yt=n(ae);je=u(Yt,"Task"),Yt.forEach(t),Vt.forEach(t),ge=p(e),h=r(e,"P",{});var v=n(h);Ke=u(v,"Select a "),q=r(v,"A",{href:!0});var Bt=n(q);Qe=u(Bt,"supported Machine Learning Task"),Bt.forEach(t),Xe=u(v,", or set to "),z=r(v,"A",{href:!0});var Jt=n(z);Ze=u(Jt,"Custom"),Jt.forEach(t),et=u(v,". "),F=r(v,"A",{href:!0});var jt=n(F);tt=u(jt,"Custom"),jt.forEach(t),ot=u(v," can/should be used when you are not using a Transformers-based model or when you want to customize the inference pipeline, see "),$=r(v,"A",{href:!0});var Kt=n($);at=u(Kt,"Create your own Inference handler"),Kt.forEach(t),rt=u(v,"."),v.forEach(t),Te=p(e),O=r(e,"P",{});var Qt=n(O);re=r(Qt,"EM",{});var Xt=n(re);st=u(Xt,"Default: derived from the model repository."),Xt.forEach(t),Qt.forEach(t),be=p(e),H=r(e,"P",{});var Zt=n(H);se=r(Zt,"STRONG",{});var eo=n(se);nt=u(eo,"Framework"),eo.forEach(t),Zt.forEach(t),Ae=p(e),L=r(e,"P",{});var to=n(L);lt=u(to,"For Transformers models, if both PyTorch and TensorFlow weights are both available, you can select which model weights to use. This will help reduce the image artifact size and accelerate startups/scaling of your endpoints."),to.forEach(t),Ce=p(e),W=r(e,"P",{});var oo=n(W);ne=r(oo,"EM",{});var ao=n(ne);it=u(ao,"Default: PyTorch if available."),ao.forEach(t),oo.forEach(t),Se=p(e),V=r(e,"P",{});var ro=n(V);le=r(ro,"STRONG",{});var so=n(le);ut=u(so,"Revision"),so.forEach(t),ro.forEach(t),Re=p(e),Y=r(e,"P",{});var no=n(Y);ft=u(no,"Create your Endpoint targeting a specific revision commit for its source Hugging Face Model Repository. This allows you to version your endpoint and make sure you are always using the same weights even if you are updating the Model Repository."),no.forEach(t),ke=p(e),B=r(e,"P",{});var lo=n(B);ie=r(lo,"EM",{});var io=n(ie);pt=u(io,"Default: The most recent commit."),io.forEach(t),lo.forEach(t),Me=p(e),J=r(e,"P",{});var uo=n(J);ue=r(uo,"STRONG",{});var fo=n(ue);ct=u(fo,"Image"),fo.forEach(t),uo.forEach(t),Ge=p(e),m=r(e,"P",{});var d=n(m);mt=u(d,"Allows you to provide a custom container image you want to deploy into an Endpoint. Those can be public images, e.g "),fe=r(d,"EM",{});var po=n(fe);ht=u(po,"tensorflow/serving:2.7.3,"),po.forEach(t),dt=u(d," or private Images hosted on "),b=r(d,"A",{href:!0,rel:!0});var co=n(b);vt=u(co,"Docker hub"),co.forEach(t),yt=u(d,", "),A=r(d,"A",{href:!0,rel:!0});var mo=n(A);_t=u(mo,"AWS ECR"),mo.forEach(t),Et=u(d,", "),C=r(d,"A",{href:!0,rel:!0});var ho=n(C);wt=u(ho,"Azure ACR"),ho.forEach(t),Pt=u(d,", or "),S=r(d,"A",{href:!0,rel:!0});var vo=n(S);gt=u(vo,"Google GCR"),vo.forEach(t),Tt=u(d,"."),d.forEach(t),Ie=p(e),P=r(e,"P",{});var xe=n(P);bt=u(xe,"More on how to "),j=r(xe,"A",{href:!0});var yo=n(j);At=u(yo,"\u201CUse your own custom container\u201D"),yo.forEach(t),Ct=u(xe," below."),xe.forEach(t),this.h()},h(){c(y,"name","hf:doc:metadata"),c(y,"content",JSON.stringify(Do)),c(E,"id","advanced-setup-instance-types-auto-scaling-versioning"),c(E,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(E,"href","#advanced-setup-instance-types-auto-scaling-versioning"),c(_,"class","relative group"),c(R,"href","/guides/create_endpoint"),bo(T.src,Rt="https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/instance_types.png")||c(T,"src",Rt),c(T,"alt","copy curl"),c(q,"href","/supported_tasks"),c(z,"href","/guides/custom_handler"),c(F,"href","/guides/custom_handler"),c($,"href","/guides/custom_handler"),c(b,"href","https://hub.docker.com/"),c(b,"rel","nofollow"),c(A,"href","https://aws.amazon.com/ecr/?nc1=h_ls"),c(A,"rel","nofollow"),c(C,"href","https://azure.microsoft.com/de-de/services/container-registry/"),c(C,"rel","nofollow"),c(S,"href","https://cloud.google.com/container-registry?hl=de"),c(S,"rel","nofollow"),c(j,"href","/guides/custom_handler")},m(e,s){o(document.head,y),l(e,pe,s),l(e,_,s),o(_,E),o(E,K),Ao(g,K,null),o(_,qe),o(_,Q),o(Q,ze),l(e,ce,s),l(e,w,s),o(w,Fe),o(w,R),o(R,$e),o(w,Oe),l(e,me,s),l(e,k,s),o(k,X),o(X,He),l(e,he,s),l(e,M,s),o(M,Le),l(e,de,s),l(e,G,s),o(G,Z),o(Z,We),l(e,ve,s),l(e,I,s),o(I,ee),o(ee,Ve),l(e,ye,s),l(e,T,s),l(e,_e,s),l(e,D,s),o(D,te),o(te,Ye),l(e,Ee,s),l(e,U,s),o(U,Be),l(e,we,s),l(e,N,s),o(N,oe),o(oe,Je),l(e,Pe,s),l(e,x,s),o(x,ae),o(ae,je),l(e,ge,s),l(e,h,s),o(h,Ke),o(h,q),o(q,Qe),o(h,Xe),o(h,z),o(z,Ze),o(h,et),o(h,F),o(F,tt),o(h,ot),o(h,$),o($,at),o(h,rt),l(e,Te,s),l(e,O,s),o(O,re),o(re,st),l(e,be,s),l(e,H,s),o(H,se),o(se,nt),l(e,Ae,s),l(e,L,s),o(L,lt),l(e,Ce,s),l(e,W,s),o(W,ne),o(ne,it),l(e,Se,s),l(e,V,s),o(V,le),o(le,ut),l(e,Re,s),l(e,Y,s),o(Y,ft),l(e,ke,s),l(e,B,s),o(B,ie),o(ie,pt),l(e,Me,s),l(e,J,s),o(J,ue),o(ue,ct),l(e,Ge,s),l(e,m,s),o(m,mt),o(m,fe),o(fe,ht),o(m,dt),o(m,b),o(b,vt),o(m,yt),o(m,A),o(A,_t),o(m,Et),o(m,C),o(C,wt),o(m,Pt),o(m,S),o(S,gt),o(m,Tt),l(e,Ie,s),l(e,P,s),o(P,bt),o(P,j),o(j,At),o(P,Ct),De=!0},p:Co,i(e){De||(So(g.$$.fragment,e),De=!0)},o(e){Ro(g.$$.fragment,e),De=!1},d(e){t(y),e&&t(pe),e&&t(_),ko(g),e&&t(ce),e&&t(w),e&&t(me),e&&t(k),e&&t(he),e&&t(M),e&&t(de),e&&t(G),e&&t(ve),e&&t(I),e&&t(ye),e&&t(T),e&&t(_e),e&&t(D),e&&t(Ee),e&&t(U),e&&t(we),e&&t(N),e&&t(Pe),e&&t(x),e&&t(ge),e&&t(h),e&&t(Te),e&&t(O),e&&t(be),e&&t(H),e&&t(Ae),e&&t(L),e&&t(Ce),e&&t(W),e&&t(Se),e&&t(V),e&&t(Re),e&&t(Y),e&&t(ke),e&&t(B),e&&t(Me),e&&t(J),e&&t(Ge),e&&t(m),e&&t(Ie),e&&t(P)}}}const Do={local:"advanced-setup-instance-types-auto-scaling-versioning",title:"Advanced Setup (Instance Types, Auto Scaling, Versioning)"};function Uo(St){return Mo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qo extends _o{constructor(y){super();Eo(this,y,Uo,Io,wo,{})}}export{qo as default,Do as metadata};
