import{S as qn,i as En,s as Sn,e as n,k as h,w as f,t as l,M as xn,c as r,d as t,m as c,a as o,x as d,h as i,b as p,N as An,G as a,g as u,y as m,L as Pn,q as g,o as _,B as y,v as Tn}from"../chunks/vendor-hf-doc-builder.js";import{I as Ee}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as q}from"../chunks/CodeBlock-hf-doc-builder.js";function zn(As){let E,pt,S,I,Oe,V,la,Le,ia,ut,Se,pa,ht,N,ua,X,ha,ca,ct,k,Ps,ft,F,fa,Z,da,ma,dt,x,H,We,ee,ga,Ye,_a,mt,w,ya,te,ka,wa,ae,ba,va,gt,se,_t,b,$a,ne,ja,qa,re,Ea,Sa,yt,oe,kt,D,xa,le,Aa,Pa,wt,v,Ta,Ue,za,Ma,ie,Ia,Na,bt,C,Fa,Be,Ha,Da,vt,pe,$t,A,O,Ge,ue,Ca,Re,Oa,jt,L,La,he,Wa,Ya,qt,ce,Et,P,W,Je,fe,Ua,Ke,Ba,St,Y,Ga,de,Ra,Ja,xt,me,At,T,U,Qe,ge,Ka,Ve,Qa,Pt,xe,Va,Tt,$,B,Xe,Xa,Za,_e,es,ts,as,G,Ze,ss,ns,ye,rs,os,ls,Ae,et,is,ps,zt,ke,Mt,Pe,us,It,we,Nt,z,R,tt,be,hs,at,cs,Ft,J,fs,st,ds,ms,Ht,ve,Dt,K,gs,nt,_s,ys,Ct,$e,Ot,Te,ks,Lt,je,Wt,M,Q,rt,qe,ws,ot,bs,Yt,ze,vs,Ut,j,$s,Me,js,qs,Ie,Es,Ss,Bt;return V=new Ee({}),ee=new Ee({}),se=new q({props:{code:'pip install "sagemaker>=2.48.0" "transformers==4.6.1" "datasets[s3]==1.6.2" --upgrade',highlighted:'pip install <span class="hljs-string">&quot;sagemaker&gt;=2.48.0&quot;</span> <span class="hljs-string">&quot;transformers==4.6.1&quot;</span> <span class="hljs-string">&quot;datasets[s3]==1.6.2&quot;</span> --upgrade'}}),oe=new q({props:{code:`%%capture
import IPython
!conda install -c conda-forge ipywidgets -y
IPython.Application.instance().kernel.do_shutdown(True)`,highlighted:`%%capture
<span class="hljs-keyword">import</span> IPython
!conda install -c conda-forge ipywidgets -y
IPython.Application.instance().kernel.do_shutdown(<span class="hljs-literal">True</span>)`}}),pe=new q({props:{code:`import sagemaker

sess = sagemaker.Session()
sagemaker_session_bucket = None
if sagemaker_session_bucket is None and sess is not None:
    sagemaker_session_bucket = sess.default_bucket()

role = sagemaker.get_execution_role()
sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)`,highlighted:`<span class="hljs-keyword">import</span> sagemaker

sess = sagemaker.Session()
sagemaker_session_bucket = <span class="hljs-literal">None</span>
<span class="hljs-keyword">if</span> sagemaker_session_bucket <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> sess <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
    sagemaker_session_bucket = sess.default_bucket()

role = sagemaker.get_execution_role()
sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)`}}),ue=new Ee({}),ce=new q({props:{code:`from datasets import load_dataset
from transformers import AutoTokenizer

# load dataset
train_dataset, test_dataset = load_dataset("imdb", split=["train", "test"])

# load tokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")

# create tokenization function
def tokenize(batch):
    return tokenizer(batch["text"], padding="max_length", truncation=True)

# tokenize train and test datasets
train_dataset = train_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# set dataset format for PyTorch
train_dataset =  train_dataset.rename_column("label", "labels")
train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])
test_dataset = test_dataset.rename_column("label", "labels")
test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-comment"># load dataset</span>
train_dataset, test_dataset = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>, split=[<span class="hljs-string">&quot;train&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>])

<span class="hljs-comment"># load tokenizer</span>
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-comment"># create tokenization function</span>
<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">batch</span>):
    <span class="hljs-keyword">return</span> tokenizer(batch[<span class="hljs-string">&quot;text&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># tokenize train and test datasets</span>
train_dataset = train_dataset.<span class="hljs-built_in">map</span>(tokenize, batched=<span class="hljs-literal">True</span>)
test_dataset = test_dataset.<span class="hljs-built_in">map</span>(tokenize, batched=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># set dataset format for PyTorch</span>
train_dataset =  train_dataset.rename_column(<span class="hljs-string">&quot;label&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>)
train_dataset.set_format(<span class="hljs-string">&quot;torch&quot;</span>, columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>])
test_dataset = test_dataset.rename_column(<span class="hljs-string">&quot;label&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>)
test_dataset.set_format(<span class="hljs-string">&quot;torch&quot;</span>, columns=[<span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>])`}}),fe=new Ee({}),me=new q({props:{code:`import botocore
from datasets.filesystems import S3FileSystem

s3_prefix = 'samples/datasets/imdb'
s3 = S3FileSystem()

# save train_dataset to S3
training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'
train_dataset.save_to_disk(training_input_path,fs=s3)

# save test_dataset to S3
test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'
test_dataset.save_to_disk(test_input_path,fs=s3)`,highlighted:`<span class="hljs-keyword">import</span> botocore
<span class="hljs-keyword">from</span> datasets.filesystems <span class="hljs-keyword">import</span> S3FileSystem

s3_prefix = <span class="hljs-string">&#x27;samples/datasets/imdb&#x27;</span>
s3 = S3FileSystem()

<span class="hljs-comment"># save train_dataset to S3</span>
training_input_path = <span class="hljs-string">f&#x27;s3://<span class="hljs-subst">{sess.default_bucket()}</span>/<span class="hljs-subst">{s3_prefix}</span>/train&#x27;</span>
train_dataset.save_to_disk(training_input_path,fs=s3)

<span class="hljs-comment"># save test_dataset to S3</span>
test_input_path = <span class="hljs-string">f&#x27;s3://<span class="hljs-subst">{sess.default_bucket()}</span>/<span class="hljs-subst">{s3_prefix}</span>/test&#x27;</span>
test_dataset.save_to_disk(test_input_path,fs=s3)`}}),ge=new Ee({}),ke=new q({props:{code:`from sagemaker.huggingface import HuggingFace

hyperparameters={
    "epochs": 1,                            # number of training epochs
    "train_batch_size": 32,                 # training batch size
    "model_name":"distilbert-base-uncased"  # name of pretrained model
}

huggingface_estimator = HuggingFace(
    entry_point="train.py",                 # fine-tuning script to use in training job
    source_dir="./scripts",                 # directory where fine-tuning script is stored
    instance_type="ml.p3.2xlarge",          # instance type
    instance_count=1,                       # number of instances
    role=role,                              # IAM role used in training job to acccess AWS resources (S3)
    transformers_version="4.6",             # Transformers version
    pytorch_version="1.7",                  # PyTorch version
    py_version="py36",                      # Python version
    hyperparameters=hyperparameters         # hyperparameters to use in training job
)`,highlighted:`<span class="hljs-keyword">from</span> sagemaker.huggingface <span class="hljs-keyword">import</span> HuggingFace

hyperparameters={
    <span class="hljs-string">&quot;epochs&quot;</span>: <span class="hljs-number">1</span>,                            <span class="hljs-comment"># number of training epochs</span>
    <span class="hljs-string">&quot;train_batch_size&quot;</span>: <span class="hljs-number">32</span>,                 <span class="hljs-comment"># training batch size</span>
    <span class="hljs-string">&quot;model_name&quot;</span>:<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>  <span class="hljs-comment"># name of pretrained model</span>
}

huggingface_estimator = HuggingFace(
    entry_point=<span class="hljs-string">&quot;train.py&quot;</span>,                 <span class="hljs-comment"># fine-tuning script to use in training job</span>
    source_dir=<span class="hljs-string">&quot;./scripts&quot;</span>,                 <span class="hljs-comment"># directory where fine-tuning script is stored</span>
    instance_type=<span class="hljs-string">&quot;ml.p3.2xlarge&quot;</span>,          <span class="hljs-comment"># instance type</span>
    instance_count=<span class="hljs-number">1</span>,                       <span class="hljs-comment"># number of instances</span>
    role=role,                              <span class="hljs-comment"># IAM role used in training job to acccess AWS resources (S3)</span>
    transformers_version=<span class="hljs-string">&quot;4.6&quot;</span>,             <span class="hljs-comment"># Transformers version</span>
    pytorch_version=<span class="hljs-string">&quot;1.7&quot;</span>,                  <span class="hljs-comment"># PyTorch version</span>
    py_version=<span class="hljs-string">&quot;py36&quot;</span>,                      <span class="hljs-comment"># Python version</span>
    hyperparameters=hyperparameters         <span class="hljs-comment"># hyperparameters to use in training job</span>
)`}}),we=new q({props:{code:'huggingface_estimator.fit({"train": training_input_path, "test": test_input_path})',highlighted:'huggingface_estimator.fit({<span class="hljs-string">&quot;train&quot;</span>: training_input_path, <span class="hljs-string">&quot;test&quot;</span>: test_input_path})'}}),be=new Ee({}),ve=new q({props:{code:'predictor = huggingface_estimator.deploy(initial_instance_count=1,"ml.g4dn.xlarge")',highlighted:'predictor = huggingface_estimator.deploy(initial_instance_count=<span class="hljs-number">1</span>,<span class="hljs-string">&quot;ml.g4dn.xlarge&quot;</span>)'}}),$e=new q({props:{code:`sentiment_input = {"inputs": "It feels like a curtain closing...there was an elegance in the way they moved toward conclusion. No fan is going to watch and feel short-changed."}

predictor.predict(sentiment_input)`,highlighted:`sentiment_input = {<span class="hljs-string">&quot;inputs&quot;</span>: <span class="hljs-string">&quot;It feels like a curtain closing...there was an elegance in the way they moved toward conclusion. No fan is going to watch and feel short-changed.&quot;</span>}

predictor.predict(sentiment_input)`}}),je=new q({props:{code:"predictor.delete_endpoint()",highlighted:"predictor.delete_endpoint()"}}),qe=new Ee({}),{c(){E=n("meta"),pt=h(),S=n("h1"),I=n("a"),Oe=n("span"),f(V.$$.fragment),la=h(),Le=n("span"),ia=l("Train and deploy Hugging Face on Amazon SageMaker"),ut=h(),Se=n("p"),pa=l("The get started guide will show you how to quickly use Hugging Face on Amazon SageMaker. Learn how to fine-tune and deploy a pretrained \u{1F917} Transformers model on SageMaker for a binary text classification task."),ht=h(),N=n("p"),ua=l("\u{1F4A1} If you are new to Hugging Face, we recommend first reading the \u{1F917} Transformers "),X=n("a"),ha=l("quick tour"),ca=l("."),ct=h(),k=n("iframe"),ft=h(),F=n("p"),fa=l("\u{1F4D3} Open the "),Z=n("a"),da=l("notebook"),ma=l(" to follow along!"),dt=h(),x=n("h2"),H=n("a"),We=n("span"),f(ee.$$.fragment),ga=h(),Ye=n("span"),_a=l("Installation and setup"),mt=h(),w=n("p"),ya=l("Get started by installing the necessary Hugging Face libraries and SageMaker. You will also need to install "),te=n("a"),ka=l("PyTorch"),wa=l(" and "),ae=n("a"),ba=l("TensorFlow"),va=l(" if you don\u2019t already have it installed."),gt=h(),f(se.$$.fragment),_t=h(),b=n("p"),$a=l("If you want to run this example in "),ne=n("a"),ja=l("SageMaker Studio"),qa=l(", upgrade "),re=n("a"),Ea=l("ipywidgets"),Sa=l(" for the \u{1F917} Datasets library and restart the kernel:"),yt=h(),f(oe.$$.fragment),kt=h(),D=n("p"),xa=l("Next, you should set up your environment: a SageMaker session and an S3 bucket. The S3 bucket will store data, models, and logs. You will need access to an "),le=n("a"),Aa=l("IAM execution role"),Pa=l(" with the required permissions."),wt=h(),v=n("p"),Ta=l("If you are planning on using SageMaker in a local environment, you need to provide the "),Ue=n("code"),za=l("role"),Ma=l(" yourself. Learn more about how to set this up "),ie=n("a"),Ia=l("here"),Na=l("."),bt=h(),C=n("p"),Fa=l("\u26A0\uFE0F The execution role is only available when you run a notebook within SageMaker. If you try to run "),Be=n("code"),Ha=l("get_execution_role"),Da=l(" in a notebook not on SageMaker, you will get a region error."),vt=h(),f(pe.$$.fragment),$t=h(),A=n("h2"),O=n("a"),Ge=n("span"),f(ue.$$.fragment),Ca=h(),Re=n("span"),Oa=l("Preprocess"),jt=h(),L=n("p"),La=l("The \u{1F917} Datasets library makes it easy to download and preprocess a dataset for training. Download and tokenize the "),he=n("a"),Wa=l("IMDb"),Ya=l(" dataset:"),qt=h(),f(ce.$$.fragment),Et=h(),P=n("h2"),W=n("a"),Je=n("span"),f(fe.$$.fragment),Ua=h(),Ke=n("span"),Ba=l("Upload dataset to S3 bucket"),St=h(),Y=n("p"),Ga=l("Next, upload the preprocessed dataset to your S3 session bucket with \u{1F917} Datasets S3 "),de=n("a"),Ra=l("filesystem"),Ja=l(" implementation:"),xt=h(),f(me.$$.fragment),At=h(),T=n("h2"),U=n("a"),Qe=n("span"),f(ge.$$.fragment),Ka=h(),Ve=n("span"),Qa=l("Start a training job"),Pt=h(),xe=n("p"),Va=l("Create a Hugging Face Estimator to handle end-to-end SageMaker training and deployment. The most important parameters to pay attention to are:"),Tt=h(),$=n("ul"),B=n("li"),Xe=n("code"),Xa=l("entry_point"),Za=l(" refers to the fine-tuning script which you can find "),_e=n("a"),es=l("here"),ts=l("."),as=h(),G=n("li"),Ze=n("code"),ss=l("instance_type"),ns=l(" refers to the SageMaker instance that will be launched. Take a look "),ye=n("a"),rs=l("here"),os=l(" for a complete list of instance types."),ls=h(),Ae=n("li"),et=n("code"),is=l("hyperparameters"),ps=l(" refers to the training hyperparameters the model will be fine-tuned with."),zt=h(),f(ke.$$.fragment),Mt=h(),Pe=n("p"),us=l("Begin training with one line of code:"),It=h(),f(we.$$.fragment),Nt=h(),z=n("h2"),R=n("a"),tt=n("span"),f(be.$$.fragment),hs=h(),at=n("span"),cs=l("Deploy model"),Ft=h(),J=n("p"),fs=l("Once the training job is complete, deploy your fine-tuned model by calling "),st=n("code"),ds=l("deploy()"),ms=l(" with the number of instances and instance type:"),Ht=h(),f(ve.$$.fragment),Dt=h(),K=n("p"),gs=l("Call "),nt=n("code"),_s=l("predict()"),ys=l(" on your data:"),Ct=h(),f($e.$$.fragment),Ot=h(),Te=n("p"),ks=l("After running your request, delete the endpoint:"),Lt=h(),f(je.$$.fragment),Wt=h(),M=n("h2"),Q=n("a"),rt=n("span"),f(qe.$$.fragment),ws=h(),ot=n("span"),bs=l("What's next?"),Yt=h(),ze=n("p"),vs=l("Congratulations, you\u2019ve just fine-tuned and deployed a pretrained \u{1F917} Transformers model on SageMaker! \u{1F389}"),Ut=h(),j=n("p"),$s=l("For your next steps, keep reading our documentation for more details about training and deployment. There are many interesting features such as "),Me=n("a"),js=l("distributed training"),qs=l(" and "),Ie=n("a"),Es=l("Spot instances"),Ss=l("."),this.h()},l(e){const s=xn('[data-svelte="svelte-1phssyn"]',document.head);E=r(s,"META",{name:!0,content:!0}),s.forEach(t),pt=c(e),S=r(e,"H1",{class:!0});var Gt=o(S);I=r(Gt,"A",{id:!0,class:!0,href:!0});var Ts=o(I);Oe=r(Ts,"SPAN",{});var zs=o(Oe);d(V.$$.fragment,zs),zs.forEach(t),Ts.forEach(t),la=c(Gt),Le=r(Gt,"SPAN",{});var Ms=o(Le);ia=i(Ms,"Train and deploy Hugging Face on Amazon SageMaker"),Ms.forEach(t),Gt.forEach(t),ut=c(e),Se=r(e,"P",{});var Is=o(Se);pa=i(Is,"The get started guide will show you how to quickly use Hugging Face on Amazon SageMaker. Learn how to fine-tune and deploy a pretrained \u{1F917} Transformers model on SageMaker for a binary text classification task."),Is.forEach(t),ht=c(e),N=r(e,"P",{});var Rt=o(N);ua=i(Rt,"\u{1F4A1} If you are new to Hugging Face, we recommend first reading the \u{1F917} Transformers "),X=r(Rt,"A",{href:!0,rel:!0});var Ns=o(X);ha=i(Ns,"quick tour"),Ns.forEach(t),ca=i(Rt,"."),Rt.forEach(t),ct=c(e),k=r(e,"IFRAME",{width:!0,height:!0,src:!0,title:!0,frameborder:!0,allow:!0}),o(k).forEach(t),ft=c(e),F=r(e,"P",{});var Jt=o(F);fa=i(Jt,"\u{1F4D3} Open the "),Z=r(Jt,"A",{href:!0,rel:!0});var Fs=o(Z);da=i(Fs,"notebook"),Fs.forEach(t),ma=i(Jt," to follow along!"),Jt.forEach(t),dt=c(e),x=r(e,"H2",{class:!0});var Kt=o(x);H=r(Kt,"A",{id:!0,class:!0,href:!0});var Hs=o(H);We=r(Hs,"SPAN",{});var Ds=o(We);d(ee.$$.fragment,Ds),Ds.forEach(t),Hs.forEach(t),ga=c(Kt),Ye=r(Kt,"SPAN",{});var Cs=o(Ye);_a=i(Cs,"Installation and setup"),Cs.forEach(t),Kt.forEach(t),mt=c(e),w=r(e,"P",{});var Ne=o(w);ya=i(Ne,"Get started by installing the necessary Hugging Face libraries and SageMaker. You will also need to install "),te=r(Ne,"A",{href:!0,rel:!0});var Os=o(te);ka=i(Os,"PyTorch"),Os.forEach(t),wa=i(Ne," and "),ae=r(Ne,"A",{href:!0,rel:!0});var Ls=o(ae);ba=i(Ls,"TensorFlow"),Ls.forEach(t),va=i(Ne," if you don\u2019t already have it installed."),Ne.forEach(t),gt=c(e),d(se.$$.fragment,e),_t=c(e),b=r(e,"P",{});var Fe=o(b);$a=i(Fe,"If you want to run this example in "),ne=r(Fe,"A",{href:!0,rel:!0});var Ws=o(ne);ja=i(Ws,"SageMaker Studio"),Ws.forEach(t),qa=i(Fe,", upgrade "),re=r(Fe,"A",{href:!0,rel:!0});var Ys=o(re);Ea=i(Ys,"ipywidgets"),Ys.forEach(t),Sa=i(Fe," for the \u{1F917} Datasets library and restart the kernel:"),Fe.forEach(t),yt=c(e),d(oe.$$.fragment,e),kt=c(e),D=r(e,"P",{});var Qt=o(D);xa=i(Qt,"Next, you should set up your environment: a SageMaker session and an S3 bucket. The S3 bucket will store data, models, and logs. You will need access to an "),le=r(Qt,"A",{href:!0,rel:!0});var Us=o(le);Aa=i(Us,"IAM execution role"),Us.forEach(t),Pa=i(Qt," with the required permissions."),Qt.forEach(t),wt=c(e),v=r(e,"P",{});var He=o(v);Ta=i(He,"If you are planning on using SageMaker in a local environment, you need to provide the "),Ue=r(He,"CODE",{});var Bs=o(Ue);za=i(Bs,"role"),Bs.forEach(t),Ma=i(He," yourself. Learn more about how to set this up "),ie=r(He,"A",{href:!0,rel:!0});var Gs=o(ie);Ia=i(Gs,"here"),Gs.forEach(t),Na=i(He,"."),He.forEach(t),bt=c(e),C=r(e,"P",{});var Vt=o(C);Fa=i(Vt,"\u26A0\uFE0F The execution role is only available when you run a notebook within SageMaker. If you try to run "),Be=r(Vt,"CODE",{});var Rs=o(Be);Ha=i(Rs,"get_execution_role"),Rs.forEach(t),Da=i(Vt," in a notebook not on SageMaker, you will get a region error."),Vt.forEach(t),vt=c(e),d(pe.$$.fragment,e),$t=c(e),A=r(e,"H2",{class:!0});var Xt=o(A);O=r(Xt,"A",{id:!0,class:!0,href:!0});var Js=o(O);Ge=r(Js,"SPAN",{});var Ks=o(Ge);d(ue.$$.fragment,Ks),Ks.forEach(t),Js.forEach(t),Ca=c(Xt),Re=r(Xt,"SPAN",{});var Qs=o(Re);Oa=i(Qs,"Preprocess"),Qs.forEach(t),Xt.forEach(t),jt=c(e),L=r(e,"P",{});var Zt=o(L);La=i(Zt,"The \u{1F917} Datasets library makes it easy to download and preprocess a dataset for training. Download and tokenize the "),he=r(Zt,"A",{href:!0,rel:!0});var Vs=o(he);Wa=i(Vs,"IMDb"),Vs.forEach(t),Ya=i(Zt," dataset:"),Zt.forEach(t),qt=c(e),d(ce.$$.fragment,e),Et=c(e),P=r(e,"H2",{class:!0});var ea=o(P);W=r(ea,"A",{id:!0,class:!0,href:!0});var Xs=o(W);Je=r(Xs,"SPAN",{});var Zs=o(Je);d(fe.$$.fragment,Zs),Zs.forEach(t),Xs.forEach(t),Ua=c(ea),Ke=r(ea,"SPAN",{});var en=o(Ke);Ba=i(en,"Upload dataset to S3 bucket"),en.forEach(t),ea.forEach(t),St=c(e),Y=r(e,"P",{});var ta=o(Y);Ga=i(ta,"Next, upload the preprocessed dataset to your S3 session bucket with \u{1F917} Datasets S3 "),de=r(ta,"A",{href:!0,rel:!0});var tn=o(de);Ra=i(tn,"filesystem"),tn.forEach(t),Ja=i(ta," implementation:"),ta.forEach(t),xt=c(e),d(me.$$.fragment,e),At=c(e),T=r(e,"H2",{class:!0});var aa=o(T);U=r(aa,"A",{id:!0,class:!0,href:!0});var an=o(U);Qe=r(an,"SPAN",{});var sn=o(Qe);d(ge.$$.fragment,sn),sn.forEach(t),an.forEach(t),Ka=c(aa),Ve=r(aa,"SPAN",{});var nn=o(Ve);Qa=i(nn,"Start a training job"),nn.forEach(t),aa.forEach(t),Pt=c(e),xe=r(e,"P",{});var rn=o(xe);Va=i(rn,"Create a Hugging Face Estimator to handle end-to-end SageMaker training and deployment. The most important parameters to pay attention to are:"),rn.forEach(t),Tt=c(e),$=r(e,"UL",{});var De=o($);B=r(De,"LI",{});var lt=o(B);Xe=r(lt,"CODE",{});var on=o(Xe);Xa=i(on,"entry_point"),on.forEach(t),Za=i(lt," refers to the fine-tuning script which you can find "),_e=r(lt,"A",{href:!0,rel:!0});var ln=o(_e);es=i(ln,"here"),ln.forEach(t),ts=i(lt,"."),lt.forEach(t),as=c(De),G=r(De,"LI",{});var it=o(G);Ze=r(it,"CODE",{});var pn=o(Ze);ss=i(pn,"instance_type"),pn.forEach(t),ns=i(it," refers to the SageMaker instance that will be launched. Take a look "),ye=r(it,"A",{href:!0,rel:!0});var un=o(ye);rs=i(un,"here"),un.forEach(t),os=i(it," for a complete list of instance types."),it.forEach(t),ls=c(De),Ae=r(De,"LI",{});var xs=o(Ae);et=r(xs,"CODE",{});var hn=o(et);is=i(hn,"hyperparameters"),hn.forEach(t),ps=i(xs," refers to the training hyperparameters the model will be fine-tuned with."),xs.forEach(t),De.forEach(t),zt=c(e),d(ke.$$.fragment,e),Mt=c(e),Pe=r(e,"P",{});var cn=o(Pe);us=i(cn,"Begin training with one line of code:"),cn.forEach(t),It=c(e),d(we.$$.fragment,e),Nt=c(e),z=r(e,"H2",{class:!0});var sa=o(z);R=r(sa,"A",{id:!0,class:!0,href:!0});var fn=o(R);tt=r(fn,"SPAN",{});var dn=o(tt);d(be.$$.fragment,dn),dn.forEach(t),fn.forEach(t),hs=c(sa),at=r(sa,"SPAN",{});var mn=o(at);cs=i(mn,"Deploy model"),mn.forEach(t),sa.forEach(t),Ft=c(e),J=r(e,"P",{});var na=o(J);fs=i(na,"Once the training job is complete, deploy your fine-tuned model by calling "),st=r(na,"CODE",{});var gn=o(st);ds=i(gn,"deploy()"),gn.forEach(t),ms=i(na," with the number of instances and instance type:"),na.forEach(t),Ht=c(e),d(ve.$$.fragment,e),Dt=c(e),K=r(e,"P",{});var ra=o(K);gs=i(ra,"Call "),nt=r(ra,"CODE",{});var _n=o(nt);_s=i(_n,"predict()"),_n.forEach(t),ys=i(ra," on your data:"),ra.forEach(t),Ct=c(e),d($e.$$.fragment,e),Ot=c(e),Te=r(e,"P",{});var yn=o(Te);ks=i(yn,"After running your request, delete the endpoint:"),yn.forEach(t),Lt=c(e),d(je.$$.fragment,e),Wt=c(e),M=r(e,"H2",{class:!0});var oa=o(M);Q=r(oa,"A",{id:!0,class:!0,href:!0});var kn=o(Q);rt=r(kn,"SPAN",{});var wn=o(rt);d(qe.$$.fragment,wn),wn.forEach(t),kn.forEach(t),ws=c(oa),ot=r(oa,"SPAN",{});var bn=o(ot);bs=i(bn,"What's next?"),bn.forEach(t),oa.forEach(t),Yt=c(e),ze=r(e,"P",{});var vn=o(ze);vs=i(vn,"Congratulations, you\u2019ve just fine-tuned and deployed a pretrained \u{1F917} Transformers model on SageMaker! \u{1F389}"),vn.forEach(t),Ut=c(e),j=r(e,"P",{});var Ce=o(j);$s=i(Ce,"For your next steps, keep reading our documentation for more details about training and deployment. There are many interesting features such as "),Me=r(Ce,"A",{href:!0});var $n=o(Me);js=i($n,"distributed training"),$n.forEach(t),qs=i(Ce," and "),Ie=r(Ce,"A",{href:!0});var jn=o(Ie);Es=i(jn,"Spot instances"),jn.forEach(t),Ss=i(Ce,"."),Ce.forEach(t),this.h()},h(){p(E,"name","hf:doc:metadata"),p(E,"content",JSON.stringify(Mn)),p(I,"id","train-and-deploy-hugging-face-on-amazon-sagemaker"),p(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(I,"href","#train-and-deploy-hugging-face-on-amazon-sagemaker"),p(S,"class","relative group"),p(X,"href","https://huggingface.co/transformers/quicktour.html"),p(X,"rel","nofollow"),p(k,"width","560"),p(k,"height","315"),An(k.src,Ps="https://www.youtube.com/embed/pYqjCzoyWyo")||p(k,"src",Ps),p(k,"title","YouTube video player"),p(k,"frameborder","0"),p(k,"allow","accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"),k.allowFullscreen=!0,p(Z,"href","https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/sagemaker-notebook.ipynb"),p(Z,"rel","nofollow"),p(H,"id","installation-and-setup"),p(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(H,"href","#installation-and-setup"),p(x,"class","relative group"),p(te,"href","https://pytorch.org/get-started/locally/"),p(te,"rel","nofollow"),p(ae,"href","https://www.tensorflow.org/install/pip#tensorflow-2-packages-are-available"),p(ae,"rel","nofollow"),p(ne,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html"),p(ne,"rel","nofollow"),p(re,"href","https://ipywidgets.readthedocs.io/en/latest/"),p(re,"rel","nofollow"),p(le,"href","https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html"),p(le,"rel","nofollow"),p(ie,"href","https://huggingface.co/docs/sagemaker/train#installation-and-setup"),p(ie,"rel","nofollow"),p(O,"id","preprocess"),p(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(O,"href","#preprocess"),p(A,"class","relative group"),p(he,"href","https://huggingface.co/datasets/imdb"),p(he,"rel","nofollow"),p(W,"id","upload-dataset-to-s3-bucket"),p(W,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(W,"href","#upload-dataset-to-s3-bucket"),p(P,"class","relative group"),p(de,"href","https://huggingface.co/docs/datasets/filesystems.html"),p(de,"rel","nofollow"),p(U,"id","start-a-training-job"),p(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(U,"href","#start-a-training-job"),p(T,"class","relative group"),p(_e,"href","https://github.com/huggingface/notebooks/blob/main/sagemaker/01_getting_started_pytorch/scripts/train.py"),p(_e,"rel","nofollow"),p(ye,"href","https://aws.amazon.com/sagemaker/pricing/"),p(ye,"rel","nofollow"),p(R,"id","deploy-model"),p(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(R,"href","#deploy-model"),p(z,"class","relative group"),p(Q,"id","whats-next"),p(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Q,"href","#whats-next"),p(M,"class","relative group"),p(Me,"href","/docs/sagemaker/train#distributed-training"),p(Ie,"href","/docs/sagemaker/train#spot-instances")},m(e,s){a(document.head,E),u(e,pt,s),u(e,S,s),a(S,I),a(I,Oe),m(V,Oe,null),a(S,la),a(S,Le),a(Le,ia),u(e,ut,s),u(e,Se,s),a(Se,pa),u(e,ht,s),u(e,N,s),a(N,ua),a(N,X),a(X,ha),a(N,ca),u(e,ct,s),u(e,k,s),u(e,ft,s),u(e,F,s),a(F,fa),a(F,Z),a(Z,da),a(F,ma),u(e,dt,s),u(e,x,s),a(x,H),a(H,We),m(ee,We,null),a(x,ga),a(x,Ye),a(Ye,_a),u(e,mt,s),u(e,w,s),a(w,ya),a(w,te),a(te,ka),a(w,wa),a(w,ae),a(ae,ba),a(w,va),u(e,gt,s),m(se,e,s),u(e,_t,s),u(e,b,s),a(b,$a),a(b,ne),a(ne,ja),a(b,qa),a(b,re),a(re,Ea),a(b,Sa),u(e,yt,s),m(oe,e,s),u(e,kt,s),u(e,D,s),a(D,xa),a(D,le),a(le,Aa),a(D,Pa),u(e,wt,s),u(e,v,s),a(v,Ta),a(v,Ue),a(Ue,za),a(v,Ma),a(v,ie),a(ie,Ia),a(v,Na),u(e,bt,s),u(e,C,s),a(C,Fa),a(C,Be),a(Be,Ha),a(C,Da),u(e,vt,s),m(pe,e,s),u(e,$t,s),u(e,A,s),a(A,O),a(O,Ge),m(ue,Ge,null),a(A,Ca),a(A,Re),a(Re,Oa),u(e,jt,s),u(e,L,s),a(L,La),a(L,he),a(he,Wa),a(L,Ya),u(e,qt,s),m(ce,e,s),u(e,Et,s),u(e,P,s),a(P,W),a(W,Je),m(fe,Je,null),a(P,Ua),a(P,Ke),a(Ke,Ba),u(e,St,s),u(e,Y,s),a(Y,Ga),a(Y,de),a(de,Ra),a(Y,Ja),u(e,xt,s),m(me,e,s),u(e,At,s),u(e,T,s),a(T,U),a(U,Qe),m(ge,Qe,null),a(T,Ka),a(T,Ve),a(Ve,Qa),u(e,Pt,s),u(e,xe,s),a(xe,Va),u(e,Tt,s),u(e,$,s),a($,B),a(B,Xe),a(Xe,Xa),a(B,Za),a(B,_e),a(_e,es),a(B,ts),a($,as),a($,G),a(G,Ze),a(Ze,ss),a(G,ns),a(G,ye),a(ye,rs),a(G,os),a($,ls),a($,Ae),a(Ae,et),a(et,is),a(Ae,ps),u(e,zt,s),m(ke,e,s),u(e,Mt,s),u(e,Pe,s),a(Pe,us),u(e,It,s),m(we,e,s),u(e,Nt,s),u(e,z,s),a(z,R),a(R,tt),m(be,tt,null),a(z,hs),a(z,at),a(at,cs),u(e,Ft,s),u(e,J,s),a(J,fs),a(J,st),a(st,ds),a(J,ms),u(e,Ht,s),m(ve,e,s),u(e,Dt,s),u(e,K,s),a(K,gs),a(K,nt),a(nt,_s),a(K,ys),u(e,Ct,s),m($e,e,s),u(e,Ot,s),u(e,Te,s),a(Te,ks),u(e,Lt,s),m(je,e,s),u(e,Wt,s),u(e,M,s),a(M,Q),a(Q,rt),m(qe,rt,null),a(M,ws),a(M,ot),a(ot,bs),u(e,Yt,s),u(e,ze,s),a(ze,vs),u(e,Ut,s),u(e,j,s),a(j,$s),a(j,Me),a(Me,js),a(j,qs),a(j,Ie),a(Ie,Es),a(j,Ss),Bt=!0},p:Pn,i(e){Bt||(g(V.$$.fragment,e),g(ee.$$.fragment,e),g(se.$$.fragment,e),g(oe.$$.fragment,e),g(pe.$$.fragment,e),g(ue.$$.fragment,e),g(ce.$$.fragment,e),g(fe.$$.fragment,e),g(me.$$.fragment,e),g(ge.$$.fragment,e),g(ke.$$.fragment,e),g(we.$$.fragment,e),g(be.$$.fragment,e),g(ve.$$.fragment,e),g($e.$$.fragment,e),g(je.$$.fragment,e),g(qe.$$.fragment,e),Bt=!0)},o(e){_(V.$$.fragment,e),_(ee.$$.fragment,e),_(se.$$.fragment,e),_(oe.$$.fragment,e),_(pe.$$.fragment,e),_(ue.$$.fragment,e),_(ce.$$.fragment,e),_(fe.$$.fragment,e),_(me.$$.fragment,e),_(ge.$$.fragment,e),_(ke.$$.fragment,e),_(we.$$.fragment,e),_(be.$$.fragment,e),_(ve.$$.fragment,e),_($e.$$.fragment,e),_(je.$$.fragment,e),_(qe.$$.fragment,e),Bt=!1},d(e){t(E),e&&t(pt),e&&t(S),y(V),e&&t(ut),e&&t(Se),e&&t(ht),e&&t(N),e&&t(ct),e&&t(k),e&&t(ft),e&&t(F),e&&t(dt),e&&t(x),y(ee),e&&t(mt),e&&t(w),e&&t(gt),y(se,e),e&&t(_t),e&&t(b),e&&t(yt),y(oe,e),e&&t(kt),e&&t(D),e&&t(wt),e&&t(v),e&&t(bt),e&&t(C),e&&t(vt),y(pe,e),e&&t($t),e&&t(A),y(ue),e&&t(jt),e&&t(L),e&&t(qt),y(ce,e),e&&t(Et),e&&t(P),y(fe),e&&t(St),e&&t(Y),e&&t(xt),y(me,e),e&&t(At),e&&t(T),y(ge),e&&t(Pt),e&&t(xe),e&&t(Tt),e&&t($),e&&t(zt),y(ke,e),e&&t(Mt),e&&t(Pe),e&&t(It),y(we,e),e&&t(Nt),e&&t(z),y(be),e&&t(Ft),e&&t(J),e&&t(Ht),y(ve,e),e&&t(Dt),e&&t(K),e&&t(Ct),y($e,e),e&&t(Ot),e&&t(Te),e&&t(Lt),y(je,e),e&&t(Wt),e&&t(M),y(qe),e&&t(Yt),e&&t(ze),e&&t(Ut),e&&t(j)}}}const Mn={local:"train-and-deploy-hugging-face-on-amazon-sagemaker",sections:[{local:"installation-and-setup",title:"Installation and setup"},{local:"preprocess",title:"Preprocess"},{local:"upload-dataset-to-s3-bucket",title:"Upload dataset to S3 bucket"},{local:"start-a-training-job",title:"Start a training job"},{local:"deploy-model",title:"Deploy model"},{local:"whats-next",title:"What's next?"}],title:"Train and deploy Hugging Face on Amazon SageMaker"};function In(As){return Tn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Dn extends qn{constructor(E){super();En(this,E,In,zn,Sn,{})}}export{Dn as default,Mn as metadata};
