import{S as Ye,i as Ze,s as et,e as n,k as l,w as Ke,t as s,M as tt,c as r,d as t,m as c,a as i,x as We,h as d,b as g,G as a,g as f,y as He,L as at,q as Je,o as Qe,B as Xe,v as nt}from"../../chunks/vendor-hf-doc-builder.js";import{D as rt}from"../../chunks/Docstring-hf-doc-builder.js";import{I as it}from"../../chunks/IconCopyLink-hf-doc-builder.js";function ot(Re){let m,z,w,y,$,E,ee,N,te,U,I,ae,q,T,ne,B,_,re,x,ie,oe,G,R,le,K,p,O,se,ce,F,de,fe,P,pe,he,S,ue,me,D,we,W,L,ve,H,h,V,ge,ye,j,_e,be,M,Ee,xe,C,Ae,J,v,A,Ie,k,Te,Q;return E=new it({}),A=new rt({props:{name:"class simulate.RewardFunction",anchor:"simulate.RewardFunction",parameters:[{name:"type",val:": typing.Optional[str] = None"},{name:"entity_a",val:": typing.Optional[typing.Any] = None"},{name:"entity_b",val:": typing.Optional[typing.Any] = None"},{name:"distance_metric",val:": typing.Optional[str] = None"},{name:"direction",val:": typing.Optional[typing.List[float]] = None"},{name:"scalar",val:": float = 1.0"},{name:"threshold",val:": float = 1.0"},{name:"is_terminal",val:": bool = False"},{name:"is_collectable",val:": bool = False"},{name:"trigger_once",val:": bool = True"},{name:"reward_function_a",val:": dataclasses.InitVar[typing.Optional[ForwardRef('RewardFunction')]] = None"},{name:"reward_function_b",val:": dataclasses.InitVar[typing.Optional[ForwardRef('RewardFunction')]] = None"},{name:"name",val:": dataclasses.InitVar[typing.Optional[str]] = None"},{name:"position",val:": dataclasses.InitVar[typing.Optional[typing.List[float]]] = <property object at 0x7fca120c6450>"},{name:"rotation",val:": dataclasses.InitVar[typing.Optional[typing.List[float]]] = <property object at 0x7fca120c62c0>"},{name:"scaling",val:": dataclasses.InitVar[typing.Union[float, typing.List[float], NoneType]] = <property object at 0x7fca120c6310>"},{name:"transformation_matrix",val:": dataclasses.InitVar[typing.Optional[typing.List[float]]] = <property object at 0x7fca120c6360>"},{name:"parent",val:": dataclasses.InitVar[typing.Optional[typing.Any]] = None"},{name:"children",val:": dataclasses.InitVar[typing.Optional[typing.List[typing.Any]]] = None"},{name:"created_from_file",val:": dataclasses.InitVar[typing.Optional[str]] = None"}],source:"https://github.com/huggingface/simulate/blob/v0.0.1/src/simulate/assets/reward_functions.py#L16"}}),{c(){m=n("meta"),z=l(),w=n("h1"),y=n("a"),$=n("span"),Ke(E.$$.fragment),ee=l(),N=n("span"),te=s("Reward Functions"),U=l(),I=n("p"),ae=s(`\u{1F917} Simulate provides a system to define simple and complex reward functions. This is achieved through the combination of \u201Cleaf\u201D reward
functions, such as Sparse and Dense rewards functions, and predicate reward functions.`),q=l(),T=n("p"),ne=s("(LINK TO REWARD PREDICATE DIAGRAM)"),B=l(),_=n("p"),re=s(`Reward functions can be parameterized with a variety of distance metrics. Currently \u201Ceuclidean\u201D, \u201Ccosine\u201D and \u201Cbest_euclidean\u201D are supported.
Through the combination of predicates and leaf rewards, complex reward functions can be created. A good example of the is the
`),x=n("a"),ie=s("Move Boxes"),oe=s(" example."),G=l(),R=n("p"),le=s("The following \u201Cleaf\u201D rewards are available in Simulate:"),K=l(),p=n("ul"),O=n("li"),se=s("\u201Cdense\u201D: A reward that is non-zero at every time-step."),ce=l(),F=n("li"),de=s("\u201Csparse\u201D: A reward that is triggered by the proximity of another object."),fe=l(),P=n("li"),pe=s("\u201Ctimeout\u201D: A timeout reward that is triggered after a certain number of time-steps."),he=l(),S=n("li"),ue=s("\u201Csee\u201D: Triggered when an object is in the field of view of an Actor."),me=l(),D=n("li"),we=s("\u201Cangle_to\u201D: Triggered when the angle between two objects and a certain direction is less that a threshold."),W=l(),L=n("p"),ve=s("The \u201Cleaf\u201D reward functions can be combined in a tree structure with the following predicate functions:"),H=l(),h=n("ul"),V=n("li"),ge=s("\u201Cnot\u201D: Triggers when a reward is not triggered."),ye=l(),j=n("li"),_e=s("\u201Cand\u201D: Triggers when both children of this node are returning a positive reward."),be=l(),M=n("li"),Ee=s("\u201Cor\u201D: Triggers when one or both of the children of this node are returning a positive reward."),xe=l(),C=n("li"),Ae=s("\u201Cxor\u201D: Triggers when  only one of the children of this node are returning a positive reward."),J=l(),v=n("div"),Ke(A.$$.fragment),Ie=l(),k=n("p"),Te=s("An RL reward function"),this.h()},l(e){const o=tt('[data-svelte="svelte-1phssyn"]',document.head);m=r(o,"META",{name:!0,content:!0}),o.forEach(t),z=c(e),w=r(e,"H1",{class:!0});var X=i(w);y=r(X,"A",{id:!0,class:!0,href:!0});var Le=i(y);$=r(Le,"SPAN",{});var $e=i($);We(E.$$.fragment,$e),$e.forEach(t),Le.forEach(t),ee=c(X),N=r(X,"SPAN",{});var Ne=i(N);te=d(Ne,"Reward Functions"),Ne.forEach(t),X.forEach(t),U=c(e),I=r(e,"P",{});var Oe=i(I);ae=d(Oe,`\u{1F917} Simulate provides a system to define simple and complex reward functions. This is achieved through the combination of \u201Cleaf\u201D reward
functions, such as Sparse and Dense rewards functions, and predicate reward functions.`),Oe.forEach(t),q=c(e),T=r(e,"P",{});var Fe=i(T);ne=d(Fe,"(LINK TO REWARD PREDICATE DIAGRAM)"),Fe.forEach(t),B=c(e),_=r(e,"P",{});var Y=i(_);re=d(Y,`Reward functions can be parameterized with a variety of distance metrics. Currently \u201Ceuclidean\u201D, \u201Ccosine\u201D and \u201Cbest_euclidean\u201D are supported.
Through the combination of predicates and leaf rewards, complex reward functions can be created. A good example of the is the
`),x=r(Y,"A",{href:!0,rel:!0});var Pe=i(x);ie=d(Pe,"Move Boxes"),Pe.forEach(t),oe=d(Y," example."),Y.forEach(t),G=c(e),R=r(e,"P",{});var Se=i(R);le=d(Se,"The following \u201Cleaf\u201D rewards are available in Simulate:"),Se.forEach(t),K=c(e),p=r(e,"UL",{});var u=i(p);O=r(u,"LI",{});var De=i(O);se=d(De,"\u201Cdense\u201D: A reward that is non-zero at every time-step."),De.forEach(t),ce=c(u),F=r(u,"LI",{});var Ve=i(F);de=d(Ve,"\u201Csparse\u201D: A reward that is triggered by the proximity of another object."),Ve.forEach(t),fe=c(u),P=r(u,"LI",{});var je=i(P);pe=d(je,"\u201Ctimeout\u201D: A timeout reward that is triggered after a certain number of time-steps."),je.forEach(t),he=c(u),S=r(u,"LI",{});var Me=i(S);ue=d(Me,"\u201Csee\u201D: Triggered when an object is in the field of view of an Actor."),Me.forEach(t),me=c(u),D=r(u,"LI",{});var Ce=i(D);we=d(Ce,"\u201Cangle_to\u201D: Triggered when the angle between two objects and a certain direction is less that a threshold."),Ce.forEach(t),u.forEach(t),W=c(e),L=r(e,"P",{});var ke=i(L);ve=d(ke,"The \u201Cleaf\u201D reward functions can be combined in a tree structure with the following predicate functions:"),ke.forEach(t),H=c(e),h=r(e,"UL",{});var b=i(h);V=r(b,"LI",{});var ze=i(V);ge=d(ze,"\u201Cnot\u201D: Triggers when a reward is not triggered."),ze.forEach(t),ye=c(b),j=r(b,"LI",{});var Ue=i(j);_e=d(Ue,"\u201Cand\u201D: Triggers when both children of this node are returning a positive reward."),Ue.forEach(t),be=c(b),M=r(b,"LI",{});var qe=i(M);Ee=d(qe,"\u201Cor\u201D: Triggers when one or both of the children of this node are returning a positive reward."),qe.forEach(t),xe=c(b),C=r(b,"LI",{});var Be=i(C);Ae=d(Be,"\u201Cxor\u201D: Triggers when  only one of the children of this node are returning a positive reward."),Be.forEach(t),b.forEach(t),J=c(e),v=r(e,"DIV",{class:!0});var Z=i(v);We(A.$$.fragment,Z),Ie=c(Z),k=r(Z,"P",{});var Ge=i(k);Te=d(Ge,"An RL reward function"),Ge.forEach(t),Z.forEach(t),this.h()},h(){g(m,"name","hf:doc:metadata"),g(m,"content",JSON.stringify(lt)),g(y,"id","simulate.RewardFunction"),g(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(y,"href","#simulate.RewardFunction"),g(w,"class","relative group"),g(x,"href","https://github.com/huggingface/simulate/blob/main/examples/rl/sb3_move_boxes.py"),g(x,"rel","nofollow"),g(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){a(document.head,m),f(e,z,o),f(e,w,o),a(w,y),a(y,$),He(E,$,null),a(w,ee),a(w,N),a(N,te),f(e,U,o),f(e,I,o),a(I,ae),f(e,q,o),f(e,T,o),a(T,ne),f(e,B,o),f(e,_,o),a(_,re),a(_,x),a(x,ie),a(_,oe),f(e,G,o),f(e,R,o),a(R,le),f(e,K,o),f(e,p,o),a(p,O),a(O,se),a(p,ce),a(p,F),a(F,de),a(p,fe),a(p,P),a(P,pe),a(p,he),a(p,S),a(S,ue),a(p,me),a(p,D),a(D,we),f(e,W,o),f(e,L,o),a(L,ve),f(e,H,o),f(e,h,o),a(h,V),a(V,ge),a(h,ye),a(h,j),a(j,_e),a(h,be),a(h,M),a(M,Ee),a(h,xe),a(h,C),a(C,Ae),f(e,J,o),f(e,v,o),He(A,v,null),a(v,Ie),a(v,k),a(k,Te),Q=!0},p:at,i(e){Q||(Je(E.$$.fragment,e),Je(A.$$.fragment,e),Q=!0)},o(e){Qe(E.$$.fragment,e),Qe(A.$$.fragment,e),Q=!1},d(e){t(m),e&&t(z),e&&t(w),Xe(E),e&&t(U),e&&t(I),e&&t(q),e&&t(T),e&&t(B),e&&t(_),e&&t(G),e&&t(R),e&&t(K),e&&t(p),e&&t(W),e&&t(L),e&&t(H),e&&t(h),e&&t(J),e&&t(v),Xe(A)}}}const lt={local:"simulate.RewardFunction",title:"Reward Functions"};function st(Re){return nt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pt extends Ye{constructor(m){super();Ze(this,m,st,ot,et,{})}}export{pt as default,lt as metadata};
