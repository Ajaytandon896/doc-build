import{S as cs,i as hs,s as ds,e as n,k as l,w as b,t as r,L as ms,c as o,d as s,m as p,a,x as T,h as i,b as c,J as t,g as m,y,K as fs,q as E,o as w,B as $}from"../../../chunks/vendor-b1433968.js";import{D as re}from"../../../chunks/Docstring-ff504c58.js";import{C as gs}from"../../../chunks/CodeBlock-a320dbd7.js";import{I as Ct}from"../../../chunks/IconCopyLink-7029626d.js";import"../../../chunks/CopyButton-f65cb278.js";function us(Fe){let _,Y,g,u,ie,j,Ue,le,We,ye,P,q,pe,N,Xe,ce,He,Ee,L,Ge,I,Je,Qe,we,K,Ye,$e,Z,he,Ke,Pe,ee,Ze,ze,C,qe,k,et,M,tt,st,O,nt,ot,Le,z,A,de,S,at,me,rt,Ae,d,V,it,fe,lt,pt,F,ct,te,ht,dt,mt,D,U,ft,ge,gt,ut,v,W,_t,ue,kt,vt,X,se,bt,_e,Tt,yt,ne,Et,ke,wt,$t,B,H,Pt,ve,zt,qt,R,G,Lt,be,At,Dt,x,J,Bt,Q,Rt,Te,xt,jt,De;return j=new Ct({}),N=new Ct({}),C=new gs({props:{code:`import torch
from transformers import AutoModel, AutoTokenizer

phobert = AutoModel.from_pretrained("vinai/phobert-base")
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")

# INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!
line = "T\xF4i l\xE0 sinh_vi\xEAn tr\u01B0\u1EDDng \u0111\u1EA1i_h\u1ECDc C\xF4ng_ngh\u1EC7 ."

input_ids = torch.tensor([tokenizer.encode(line)])

with torch.no_grad():
    features = phobert(input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
# from transformers import TFAutoModel
# phobert = TFAutoModel.from_pretrained("vinai/phobert-base"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> torch</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">phobert = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">line = <span class="hljs-string">&quot;T\xF4i l\xE0 sinh_vi\xEAn tr\u01B0\u1EDDng \u0111\u1EA1i_h\u1ECDc C\xF4ng_ngh\u1EC7 .&quot;</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">input_ids = torch.tensor([tokenizer.encode(line)])</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">with</span> torch.no_grad():</span>
<span class="hljs-meta">...</span> <span class="language-python">    features = phobert(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># With TensorFlow 2.0+:</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># from transformers import TFAutoModel</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># phobert = TFAutoModel.from_pretrained(&quot;vinai/phobert-base&quot;)</span></span>`}}),S=new Ct({}),V=new re({props:{name:"class transformers.PhobertTokenizer",anchor:"transformers.PhobertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/phobert/tokenization_phobert.py#L68",parametersDescription:[{anchor:"transformers.PhobertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.PhobertTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.PhobertTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>st</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.`,name:"bos_token"}]}}),U=new re({props:{name:"add_from_file",anchor:"transformers.PhobertTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/phobert/tokenization_phobert.py#L335"}}),W=new re({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/phobert/tokenization_phobert.py#L158",parametersDescription:[{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),H=new re({props:{name:"convert_tokens_to_string",anchor:"transformers.PhobertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/phobert/tokenization_phobert.py#L305"}}),G=new re({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/phobert/tokenization_phobert.py#L212",parametersDescription:[{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),J=new re({props:{name:"get_special_tokens_mask",anchor:"transformers.PhobertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/phobert/tokenization_phobert.py#L184",parametersDescription:[{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),{c(){_=n("meta"),Y=l(),g=n("h1"),u=n("a"),ie=n("span"),b(j.$$.fragment),Ue=l(),le=n("span"),We=r("PhoBERT"),ye=l(),P=n("h2"),q=n("a"),pe=n("span"),b(N.$$.fragment),Xe=l(),ce=n("span"),He=r("Overview"),Ee=l(),L=n("p"),Ge=r("The PhoBERT model was proposed in "),I=n("a"),Je=r("PhoBERT: Pre-trained language models for Vietnamese"),Qe=r(" by Dat Quoc Nguyen, Anh Tuan Nguyen."),we=l(),K=n("p"),Ye=r("The abstract from the paper is the following:"),$e=l(),Z=n("p"),he=n("em"),Ke=r(`We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.`),Pe=l(),ee=n("p"),Ze=r("Example of use:"),ze=l(),b(C.$$.fragment),qe=l(),k=n("p"),et=r("This model was contributed by "),M=n("a"),tt=r("dqnguyen"),st=r(". The original code can be found "),O=n("a"),nt=r("here"),ot=r("."),Le=l(),z=n("h2"),A=n("a"),de=n("span"),b(S.$$.fragment),at=l(),me=n("span"),rt=r("PhobertTokenizer"),Ae=l(),d=n("div"),b(V.$$.fragment),it=l(),fe=n("p"),lt=r("Construct a PhoBERT tokenizer. Based on Byte-Pair-Encoding."),pt=l(),F=n("p"),ct=r("This tokenizer inherits from "),te=n("a"),ht=r("PreTrainedTokenizer"),dt=r(` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),mt=l(),D=n("div"),b(U.$$.fragment),ft=l(),ge=n("p"),gt=r("Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),ut=l(),v=n("div"),b(W.$$.fragment),_t=l(),ue=n("p"),kt=r(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),vt=l(),X=n("ul"),se=n("li"),bt=r("single sequence: "),_e=n("code"),Tt=r("<s> X </s>"),yt=l(),ne=n("li"),Et=r("pair of sequences: "),ke=n("code"),wt=r("<s> A </s></s> B </s>"),$t=l(),B=n("div"),b(H.$$.fragment),Pt=l(),ve=n("p"),zt=r("Converts a sequence of tokens (string) in a single string."),qt=l(),R=n("div"),b(G.$$.fragment),Lt=l(),be=n("p"),At=r(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),Dt=l(),x=n("div"),b(J.$$.fragment),Bt=l(),Q=n("p"),Rt=r(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Te=n("code"),xt=r("prepare_for_model"),jt=r(" method."),this.h()},l(e){const h=ms('[data-svelte="svelte-1phssyn"]',document.head);_=o(h,"META",{name:!0,content:!0}),h.forEach(s),Y=p(e),g=o(e,"H1",{class:!0});var Be=a(g);u=o(Be,"A",{id:!0,class:!0,href:!0});var Mt=a(u);ie=o(Mt,"SPAN",{});var Ot=a(ie);T(j.$$.fragment,Ot),Ot.forEach(s),Mt.forEach(s),Ue=p(Be),le=o(Be,"SPAN",{});var St=a(le);We=i(St,"PhoBERT"),St.forEach(s),Be.forEach(s),ye=p(e),P=o(e,"H2",{class:!0});var Re=a(P);q=o(Re,"A",{id:!0,class:!0,href:!0});var Vt=a(q);pe=o(Vt,"SPAN",{});var Ft=a(pe);T(N.$$.fragment,Ft),Ft.forEach(s),Vt.forEach(s),Xe=p(Re),ce=o(Re,"SPAN",{});var Ut=a(ce);He=i(Ut,"Overview"),Ut.forEach(s),Re.forEach(s),Ee=p(e),L=o(e,"P",{});var xe=a(L);Ge=i(xe,"The PhoBERT model was proposed in "),I=o(xe,"A",{href:!0,rel:!0});var Wt=a(I);Je=i(Wt,"PhoBERT: Pre-trained language models for Vietnamese"),Wt.forEach(s),Qe=i(xe," by Dat Quoc Nguyen, Anh Tuan Nguyen."),xe.forEach(s),we=p(e),K=o(e,"P",{});var Xt=a(K);Ye=i(Xt,"The abstract from the paper is the following:"),Xt.forEach(s),$e=p(e),Z=o(e,"P",{});var Ht=a(Z);he=o(Ht,"EM",{});var Gt=a(he);Ke=i(Gt,`We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.`),Gt.forEach(s),Ht.forEach(s),Pe=p(e),ee=o(e,"P",{});var Jt=a(ee);Ze=i(Jt,"Example of use:"),Jt.forEach(s),ze=p(e),T(C.$$.fragment,e),qe=p(e),k=o(e,"P",{});var oe=a(k);et=i(oe,"This model was contributed by "),M=o(oe,"A",{href:!0,rel:!0});var Qt=a(M);tt=i(Qt,"dqnguyen"),Qt.forEach(s),st=i(oe,". The original code can be found "),O=o(oe,"A",{href:!0,rel:!0});var Yt=a(O);nt=i(Yt,"here"),Yt.forEach(s),ot=i(oe,"."),oe.forEach(s),Le=p(e),z=o(e,"H2",{class:!0});var je=a(z);A=o(je,"A",{id:!0,class:!0,href:!0});var Kt=a(A);de=o(Kt,"SPAN",{});var Zt=a(de);T(S.$$.fragment,Zt),Zt.forEach(s),Kt.forEach(s),at=p(je),me=o(je,"SPAN",{});var es=a(me);rt=i(es,"PhobertTokenizer"),es.forEach(s),je.forEach(s),Ae=p(e),d=o(e,"DIV",{class:!0});var f=a(d);T(V.$$.fragment,f),it=p(f),fe=o(f,"P",{});var ts=a(fe);lt=i(ts,"Construct a PhoBERT tokenizer. Based on Byte-Pair-Encoding."),ts.forEach(s),pt=p(f),F=o(f,"P",{});var Ne=a(F);ct=i(Ne,"This tokenizer inherits from "),te=o(Ne,"A",{href:!0});var ss=a(te);ht=i(ss,"PreTrainedTokenizer"),ss.forEach(s),dt=i(Ne,` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Ne.forEach(s),mt=p(f),D=o(f,"DIV",{class:!0});var Ie=a(D);T(U.$$.fragment,Ie),ft=p(Ie),ge=o(Ie,"P",{});var ns=a(ge);gt=i(ns,"Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),ns.forEach(s),Ie.forEach(s),ut=p(f),v=o(f,"DIV",{class:!0});var ae=a(v);T(W.$$.fragment,ae),_t=p(ae),ue=o(ae,"P",{});var os=a(ue);kt=i(os,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`),os.forEach(s),vt=p(ae),X=o(ae,"UL",{});var Ce=a(X);se=o(Ce,"LI",{});var Nt=a(se);bt=i(Nt,"single sequence: "),_e=o(Nt,"CODE",{});var as=a(_e);Tt=i(as,"<s> X </s>"),as.forEach(s),Nt.forEach(s),yt=p(Ce),ne=o(Ce,"LI",{});var It=a(ne);Et=i(It,"pair of sequences: "),ke=o(It,"CODE",{});var rs=a(ke);wt=i(rs,"<s> A </s></s> B </s>"),rs.forEach(s),It.forEach(s),Ce.forEach(s),ae.forEach(s),$t=p(f),B=o(f,"DIV",{class:!0});var Me=a(B);T(H.$$.fragment,Me),Pt=p(Me),ve=o(Me,"P",{});var is=a(ve);zt=i(is,"Converts a sequence of tokens (string) in a single string."),is.forEach(s),Me.forEach(s),qt=p(f),R=o(f,"DIV",{class:!0});var Oe=a(R);T(G.$$.fragment,Oe),Lt=p(Oe),be=o(Oe,"P",{});var ls=a(be);At=i(ls,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`),ls.forEach(s),Oe.forEach(s),Dt=p(f),x=o(f,"DIV",{class:!0});var Se=a(x);T(J.$$.fragment,Se),Bt=p(Se),Q=o(Se,"P",{});var Ve=a(Q);Rt=i(Ve,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Te=o(Ve,"CODE",{});var ps=a(Te);xt=i(ps,"prepare_for_model"),ps.forEach(s),jt=i(Ve," method."),Ve.forEach(s),Se.forEach(s),f.forEach(s),this.h()},h(){c(_,"name","hf:doc:metadata"),c(_,"content",JSON.stringify(_s)),c(u,"id","phobert"),c(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(u,"href","#phobert"),c(g,"class","relative group"),c(q,"id","overview"),c(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q,"href","#overview"),c(P,"class","relative group"),c(I,"href","https://www.aclweb.org/anthology/2020.findings-emnlp.92.pdf"),c(I,"rel","nofollow"),c(M,"href","https://huggingface.co/dqnguyen"),c(M,"rel","nofollow"),c(O,"href","https://github.com/VinAIResearch/PhoBERT"),c(O,"rel","nofollow"),c(A,"id","transformers.PhobertTokenizer"),c(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A,"href","#transformers.PhobertTokenizer"),c(z,"class","relative group"),c(te,"href","/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(D,"class","docstring"),c(v,"class","docstring"),c(B,"class","docstring"),c(R,"class","docstring"),c(x,"class","docstring"),c(d,"class","docstring")},m(e,h){t(document.head,_),m(e,Y,h),m(e,g,h),t(g,u),t(u,ie),y(j,ie,null),t(g,Ue),t(g,le),t(le,We),m(e,ye,h),m(e,P,h),t(P,q),t(q,pe),y(N,pe,null),t(P,Xe),t(P,ce),t(ce,He),m(e,Ee,h),m(e,L,h),t(L,Ge),t(L,I),t(I,Je),t(L,Qe),m(e,we,h),m(e,K,h),t(K,Ye),m(e,$e,h),m(e,Z,h),t(Z,he),t(he,Ke),m(e,Pe,h),m(e,ee,h),t(ee,Ze),m(e,ze,h),y(C,e,h),m(e,qe,h),m(e,k,h),t(k,et),t(k,M),t(M,tt),t(k,st),t(k,O),t(O,nt),t(k,ot),m(e,Le,h),m(e,z,h),t(z,A),t(A,de),y(S,de,null),t(z,at),t(z,me),t(me,rt),m(e,Ae,h),m(e,d,h),y(V,d,null),t(d,it),t(d,fe),t(fe,lt),t(d,pt),t(d,F),t(F,ct),t(F,te),t(te,ht),t(F,dt),t(d,mt),t(d,D),y(U,D,null),t(D,ft),t(D,ge),t(ge,gt),t(d,ut),t(d,v),y(W,v,null),t(v,_t),t(v,ue),t(ue,kt),t(v,vt),t(v,X),t(X,se),t(se,bt),t(se,_e),t(_e,Tt),t(X,yt),t(X,ne),t(ne,Et),t(ne,ke),t(ke,wt),t(d,$t),t(d,B),y(H,B,null),t(B,Pt),t(B,ve),t(ve,zt),t(d,qt),t(d,R),y(G,R,null),t(R,Lt),t(R,be),t(be,At),t(d,Dt),t(d,x),y(J,x,null),t(x,Bt),t(x,Q),t(Q,Rt),t(Q,Te),t(Te,xt),t(Q,jt),De=!0},p:fs,i(e){De||(E(j.$$.fragment,e),E(N.$$.fragment,e),E(C.$$.fragment,e),E(S.$$.fragment,e),E(V.$$.fragment,e),E(U.$$.fragment,e),E(W.$$.fragment,e),E(H.$$.fragment,e),E(G.$$.fragment,e),E(J.$$.fragment,e),De=!0)},o(e){w(j.$$.fragment,e),w(N.$$.fragment,e),w(C.$$.fragment,e),w(S.$$.fragment,e),w(V.$$.fragment,e),w(U.$$.fragment,e),w(W.$$.fragment,e),w(H.$$.fragment,e),w(G.$$.fragment,e),w(J.$$.fragment,e),De=!1},d(e){s(_),e&&s(Y),e&&s(g),$(j),e&&s(ye),e&&s(P),$(N),e&&s(Ee),e&&s(L),e&&s(we),e&&s(K),e&&s($e),e&&s(Z),e&&s(Pe),e&&s(ee),e&&s(ze),$(C,e),e&&s(qe),e&&s(k),e&&s(Le),e&&s(z),$(S),e&&s(Ae),e&&s(d),$(V),$(U),$(W),$(H),$(G),$(J)}}}const _s={local:"phobert",sections:[{local:"overview",title:"Overview"},{local:"transformers.PhobertTokenizer",title:"PhobertTokenizer"}],title:"PhoBERT"};function ks(Fe,_,Y){let{fw:g}=_;return Fe.$$set=u=>{"fw"in u&&Y(0,g=u.fw)},[g]}class ws extends cs{constructor(_){super();hs(this,_,ks,us,ds,{fw:0})}}export{ws as default,_s as metadata};
