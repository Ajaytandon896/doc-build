import{S as nv,i as rv,s as av,e as r,k as i,w as v,t as o,L as sv,c as a,d as n,m as l,a as s,x as b,h as t,b as c,J as e,g as m,y as E,q as y,o as k,B as T}from"../../../chunks/vendor-b1433968.js";import{T as Kf}from"../../../chunks/Tip-c3840994.js";import{D as Z}from"../../../chunks/Docstring-ff504c58.js";import{C as zo}from"../../../chunks/CodeBlock-a320dbd7.js";import{I as Za}from"../../../chunks/IconCopyLink-7029626d.js";import"../../../chunks/CopyButton-f65cb278.js";function dv(Te){let h,F,f,j,L;return{c(){h=r("p"),F=o(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=r("code"),j=o("Module"),L=o(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(M){h=a(M,"P",{});var D=s(h);F=t(D,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=a(D,"CODE",{});var B=s(f);j=t(B,"Module"),B.forEach(n),L=t(D,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),D.forEach(n)},m(M,D){m(M,h,D),e(h,F),e(h,f),e(f,j),e(h,L)},d(M){M&&n(h)}}}function iv(Te){let h,F,f,j,L;return{c(){h=r("p"),F=o(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=r("code"),j=o("Module"),L=o(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(M){h=a(M,"P",{});var D=s(h);F=t(D,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=a(D,"CODE",{});var B=s(f);j=t(B,"Module"),B.forEach(n),L=t(D,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),D.forEach(n)},m(M,D){m(M,h,D),e(h,F),e(h,f),e(f,j),e(h,L)},d(M){M&&n(h)}}}function lv(Te){let h,F,f,j,L;return{c(){h=r("p"),F=o(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=r("code"),j=o("Module"),L=o(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(M){h=a(M,"P",{});var D=s(h);F=t(D,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=a(D,"CODE",{});var B=s(f);j=t(B,"Module"),B.forEach(n),L=t(D,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),D.forEach(n)},m(M,D){m(M,h,D),e(h,F),e(h,f),e(f,j),e(h,L)},d(M){M&&n(h)}}}function cv(Te){let h,F,f,j,L,M,D,B,Ss,Ka,Xe,Ls,zt,Os,Ns,Qa,eo,Bs,jo,Us,Ws,Xa,oo,Vs,jt,Rs,Gs,es,he,Hs,$t,Js,Ys,$o,Zs,Ks,os,me,Qs,Mn,Xs,ed,Dn,od,td,ts,Po,ns,fe,nd,qo,rd,ad,Co,sd,dd,rs,we,to,xn,Fo,id,zn,ld,as,C,Ao,cd,no,Pt,pd,hd,qt,md,fd,ud,Me,gd,Ct,_d,vd,Ft,bd,Ed,yd,jn,kd,Td,Io,wd,ro,So,Md,Lo,Dd,At,xd,zd,jd,ao,Oo,$d,De,Pd,$n,qd,Cd,Pn,Fd,Ad,ss,xe,so,qn,No,Id,Cn,Sd,ds,$,Bo,Ld,ze,Od,Fn,Nd,Bd,An,Ud,Wd,Vd,Uo,Rd,Wo,Gd,Hd,Jd,In,Yd,Zd,Vo,Kd,It,Qd,Xd,ei,Ro,oi,Go,ti,ni,ri,ue,St,ai,si,Sn,di,ii,Ln,li,ci,pi,U,Ho,hi,je,mi,Lt,fi,ui,On,gi,_i,vi,io,bi,Nn,Ei,yi,Jo,ki,u,Yo,Ti,Bn,wi,Mi,$e,Di,Un,xi,zi,Wn,ji,$i,Pi,Pe,qi,Vn,Ci,Fi,Rn,Ai,Ii,Si,qe,K,Li,Gn,Oi,Ni,Hn,Bi,Ui,Jn,Wi,Vi,Ri,Q,Gi,Yn,Hi,Ji,Ot,Yi,Zi,Zn,Ki,Qi,Xi,O,el,Kn,ol,tl,Qn,nl,rl,Xn,al,sl,er,dl,il,or,ll,cl,pl,X,hl,tr,ml,fl,nr,ul,gl,rr,_l,vl,bl,Ce,ee,El,ar,yl,kl,sr,Tl,wl,dr,Ml,Dl,xl,oe,zl,ir,jl,$l,Nt,Pl,ql,lr,Cl,Fl,Al,N,Il,cr,Sl,Ll,pr,Ol,Nl,hr,Bl,Ul,mr,Wl,Vl,fr,Rl,Gl,Hl,Fe,Jl,ur,Yl,Zl,gr,Kl,Ql,Xl,Ae,ec,_r,oc,tc,vr,nc,rc,ac,Ie,br,sc,dc,Er,ic,lc,yr,cc,pc,Zo,hc,kr,mc,fc,uc,Tr,gc,_c,Ko,is,Se,lo,wr,Qo,vc,Mr,bc,ls,P,Xo,Ec,Le,yc,Dr,kc,Tc,xr,wc,Mc,Dc,et,xc,ot,zc,jc,$c,zr,Pc,qc,tt,Cc,Bt,Fc,Ac,Ic,nt,Sc,rt,Lc,Oc,Nc,ge,jr,Bc,Uc,$r,Wc,Vc,Pr,Rc,Gc,Hc,W,at,Jc,Oe,Yc,Ut,Zc,Kc,qr,Qc,Xc,ep,co,op,Cr,tp,np,st,rp,g,dt,ap,Fr,sp,dp,Ne,ip,Ar,lp,cp,Ir,pp,hp,mp,Be,te,fp,Sr,up,gp,Lr,_p,vp,Or,bp,Ep,yp,ne,kp,Nr,Tp,wp,Wt,Mp,Dp,Br,xp,zp,jp,G,$p,Ur,Pp,qp,Wr,Cp,Fp,Vr,Ap,Ip,Rr,Sp,Lp,Op,re,Np,Gr,Bp,Up,Hr,Wp,Vp,Jr,Rp,Gp,Hp,Ue,ae,Jp,Yr,Yp,Zp,Zr,Kp,Qp,Kr,Xp,eh,oh,se,th,Qr,nh,rh,Vt,ah,sh,Xr,dh,ih,lh,H,ch,ea,ph,hh,oa,mh,fh,ta,uh,gh,na,_h,vh,bh,We,Eh,ra,yh,kh,aa,Th,wh,Mh,Ve,Dh,sa,xh,zh,da,jh,$h,Ph,Re,ia,qh,Ch,la,Fh,Ah,ca,Ih,Sh,it,Lh,pa,Oh,Nh,Bh,ha,Uh,Wh,lt,cs,Ge,po,ma,ct,Vh,fa,Rh,ps,q,pt,Gh,He,Hh,ua,Jh,Yh,ga,Zh,Kh,Qh,ht,Xh,mt,em,om,tm,_a,nm,rm,ft,am,Rt,sm,dm,im,ut,lm,gt,cm,pm,hm,_e,Gt,mm,fm,va,um,gm,ba,_m,vm,bm,V,_t,Em,Je,ym,Ht,km,Tm,Ea,wm,Mm,Dm,ho,xm,ya,zm,jm,vt,$m,_,bt,Pm,ka,qm,Cm,Ye,Fm,Ta,Am,Im,wa,Sm,Lm,Om,Et,de,Nm,Ma,Bm,Um,Da,Wm,Vm,xa,Rm,Gm,Hm,ie,Jm,za,Ym,Zm,Jt,Km,Qm,ja,Xm,ef,of,le,tf,$a,nf,rf,Pa,af,sf,qa,df,lf,cf,yt,ce,pf,Ca,hf,mf,Fa,ff,uf,Aa,gf,_f,vf,pe,bf,Ia,Ef,yf,Yt,kf,Tf,Sa,wf,Mf,Df,Ze,xf,La,zf,jf,Oa,$f,Pf,qf,Ke,Cf,Na,Ff,Af,Ba,If,Sf,Lf,Qe,Ua,Of,Nf,Wa,Bf,Uf,Va,Wf,Vf,kt,Rf,Ra,Gf,Hf,Jf,Ga,Yf,Zf,Tt,hs;return M=new Za({}),Po=new zo({props:{code:`# a workaround to load from pytorch checkpoint
_model = EncoderDecoderModel.from_pretrained("patrickvonplaten/bert2bert-cnn_dailymail-fp16")
_model.encoder.save_pretrained("./encoder")
_model.decoder.save_pretrained("./decoder")
model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(
    "./encoder", "./decoder", encoder_from_pt=True, decoder_from_pt=True
)
# This is only for copying some specific attributes of this particular model.
model.config = _model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># a workaround to load from pytorch checkpoint</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">_model = EncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/bert2bert-cnn_dailymail-fp16&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">_model.encoder.save_pretrained(<span class="hljs-string">&quot;./encoder&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">_model.decoder.save_pretrained(<span class="hljs-string">&quot;./decoder&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(</span>
<span class="hljs-meta">...</span> <span class="language-python">    <span class="hljs-string">&quot;./encoder&quot;</span>, <span class="hljs-string">&quot;./decoder&quot;</span>, encoder_from_pt=<span class="hljs-literal">True</span>, decoder_from_pt=<span class="hljs-literal">True</span></span>
<span class="hljs-meta">...</span> <span class="language-python">)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># This is only for copying some specific attributes of this particular model.</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model.config = _model.config</span>`}}),Fo=new Za({}),Ao=new Z({props:{name:"class transformers.EncoderDecoderConfig",anchor:"transformers.EncoderDecoderConfig",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/configuration_encoder_decoder.py#L26",parametersDescription:[{anchor:"transformers.EncoderDecoderConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments. Notably:</p>
<ul>
<li><strong>encoder</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration
object that defines the encoder config.</li>
<li><strong>decoder</strong> (<a href="/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration
object that defines the decoder config.</li>
</ul>`,name:"kwargs"}]}}),Io=new zo({props:{code:`from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel

# Initializing a BERT bert-base-uncased style configuration
config_encoder = BertConfig()
config_decoder = BertConfig()

config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

# Initializing a Bert2Bert model from the bert-base-uncased style configurations
model = EncoderDecoderModel(config=config)

# Accessing the model configuration
config_encoder = model.config.encoder
config_decoder  = model.config.decoder
# set decoder config to causal lm
config_decoder.is_decoder = True
config_decoder.add_cross_attention = True

# Saving the model, including its configuration
model.save_pretrained('my-model')

# loading model and config from pretrained folder
encoder_decoder_config = EncoderDecoderConfig.from_pretrained('my-model')
model = EncoderDecoderModel.from_pretrained('my-model', config=encoder_decoder_config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, EncoderDecoderConfig, EncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BERT bert-base-uncased style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = BertConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder = BertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bert2Bert model from the bert-base-uncased style configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel(config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = model.config.encoder
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder  = model.config.decoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set decoder config to causal lm</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.is_decoder = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.add_cross_attention = <span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Saving the model, including its configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&#x27;my-model&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loading model and config from pretrained folder</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_decoder_config = EncoderDecoderConfig.from_pretrained(<span class="hljs-string">&#x27;my-model&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_pretrained(<span class="hljs-string">&#x27;my-model&#x27;</span>, config=encoder_decoder_config)`}}),So=new Z({props:{name:"from_encoder_decoder_configs",anchor:"transformers.EncoderDecoderConfig.from_encoder_decoder_configs",parameters:[{name:"encoder_config",val:": PretrainedConfig"},{name:"decoder_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/configuration_encoder_decoder.py#L90",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"
>EncoderDecoderConfig</a></p>
`}}),Oo=new Z({props:{name:"to_dict",anchor:"transformers.EncoderDecoderConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/configuration_encoder_decoder.py#L107",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),No=new Za({}),Bo=new Z({props:{name:"class transformers.EncoderDecoderModel",anchor:"transformers.EncoderDecoderModel",parameters:[{name:"config",val:": typing.Optional[transformers.configuration_utils.PretrainedConfig] = None"},{name:"encoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"},{name:"decoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py#L171",parametersDescription:[{anchor:"transformers.EncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Ho=new Z({props:{name:"forward",anchor:"transformers.EncoderDecoderModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py#L428",parametersDescription:[{anchor:"transformers.EncoderDecoderModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.EncoderDecoderModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.EncoderDecoderModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>For training, <code>decoder_input_ids</code> are automatically created by the model by shifting the <code>labels</code>
to the right, replacing -100 by the <code>pad_token_id</code> and prepending them with the
<code>decoder_start_token_id</code>.`,name:"decoder_input_ids"},{anchor:"transformers.EncoderDecoderModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.EncoderDecoderModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
This tuple must consist of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>:
<code>attentions</code>) <code>last_hidden_state</code> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) is a tensor of hidden-states at the output of the last layer of the
encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.EncoderDecoderModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.EncoderDecoderModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.EncoderDecoderModel.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>decoder_input_ids</code>
indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.EncoderDecoderModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss for the decoder. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.EncoderDecoderModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.EncoderDecoderModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.EncoderDecoderModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.EncoderDecoderModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>Seq2SeqLMOutput</code> instead of a
plain tuple.
kwargs &#x2014; (<em>optional</em>) Remaining dictionary of keyword arguments. Keyword arguments come in two flavors:</p>
<ul>
<li>Without a prefix which will be input as <code>**encoder_kwargs</code> for the encoder forward function.</li>
<li>With a _decoder__ prefix which will be input as <code>**decoder_kwargs</code> for the decoder forward function.</li>
</ul>`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"
>EncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),io=new Kf({props:{$$slots:{default:[dv]},$$scope:{ctx:Te}}}),Jo=new zo({props:{code:`from transformers import EncoderDecoderModel, BertTokenizer
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints

# training
model.config.decoder_start_token_id = tokenizer.cls_token_id
model.config.pad_token_id = tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

input_ids = tokenizer("This is a really long text", return_tensors="pt").input_ids
labels = tokenizer("This is the corresponding summary", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, labels=input_ids)
loss, logits = outputs.loss, outputs.logits

# save and load from pretrained
model.save_pretrained("bert2bert")
model = EncoderDecoderModel.from_pretrained("bert2bert")

# generation
generated = model.generate(input_ids),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EncoderDecoderModel, BertTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, <span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>) <span class="hljs-comment"># initialize Bert2Bert from pre-trained checkpoints</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.decoder_start_token_id = tokenizer.cls_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = tokenizer.pad_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.vocab_size = model.config.decoder.vocab_size

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;This is a really long text&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;This is the corresponding summary&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, labels=input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, logits = outputs.loss, outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save and load from pretrained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;bert2bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;bert2bert&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated = model.generate(input_ids)`}}),Yo=new Z({props:{name:"from_encoder_decoder_pretrained",anchor:"transformers.EncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": str = None"},{name:"decoder_pretrained_model_name_or_path",val:": str = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py#L284"}}),Ko=new zo({props:{code:`from transformers import EncoderDecoderModel
# initialize a bert2bert from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized
model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased')
# saving model after fine-tuning
model.save_pretrained("./bert2bert")
# load fine-tuned model
model = EncoderDecoderModel.from_pretrained("./bert2bert"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a bert2bert from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, <span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./bert2bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./bert2bert&quot;</span>)`}}),Qo=new Za({}),Xo=new Z({props:{name:"class transformers.TFEncoderDecoderModel",anchor:"transformers.TFEncoderDecoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py#L154",parametersDescription:[{anchor:"transformers.TFEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the
model weights.`,name:"config"}]}}),at=new Z({props:{name:"call",anchor:"transformers.TFEncoderDecoderModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py#L463",parametersDescription:[{anchor:"transformers.TFEncoderDecoderModel.call.input_ids",description:`<strong>input_ids</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFEncoderDecoderModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFEncoderDecoderModel.call.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>Provide for sequence to sequence training to the decoder. Indices can be obtained using
<a href="/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for details.`,name:"decoder_input_ids"},{anchor:"transformers.TFEncoderDecoderModel.call.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.TFEncoderDecoderModel.call.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(tf.Tensor)</code>, <em>optional</em>) &#x2014;
This tuple must consist of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>:
<code>attentions</code>) <code>last_hidden_state</code> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) is a
tensor of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the
decoder.`,name:"encoder_outputs"},{anchor:"transformers.TFEncoderDecoderModel.call.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(tf.Tensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.TFEncoderDecoderModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFEncoderDecoderModel.call.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>decoder_input_ids</code>
indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.TFEncoderDecoderModel.call.labels",description:`<strong>labels</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss for the decoder. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.TFEncoderDecoderModel.call.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.TFEncoderDecoderModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TFEncoderDecoderModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TFEncoderDecoderModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>Seq2SeqLMOutput</code> instead of a
plain tuple.`,name:"return_dict"},{anchor:"transformers.TFEncoderDecoderModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).
kwargs &#x2014; (<em>optional</em>) Remaining dictionary of keyword arguments. Keyword arguments come in two flavors:</p>
<ul>
<li>Without a prefix which will be input as <code>**encoder_kwargs</code> for the encoder forward function.</li>
<li>With a _decoder__ prefix which will be input as <code>**decoder_kwargs</code> for the decoder forward function.</li>
</ul>`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"
>TFSeq2SeqLMOutput</a> or a tuple of
<code>tf.Tensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"
>EncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape \`(batch_size, num_heads, sequence_length, sequence_length)\u201C.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"
>TFSeq2SeqLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),co=new Kf({props:{$$slots:{default:[iv]},$$scope:{ctx:Te}}}),st=new zo({props:{code:`from transformers import TFEncoderDecoderModel, BertTokenizer

# initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = TFEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

# forward
input_ids = tokenizer.encode("Hello, my dog is cute", add_special_tokens=True, return_tensors='tf')  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)

# training
outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)
loss, logits = outputs.loss, outputs.logits

# save and load from pretrained
model.save_pretrained("bert2gpt2")
model = TFEncoderDecoderModel.from_pretrained("bert2gpt2")

# generation
generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.bos_token_id),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFEncoderDecoderModel, BertTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, <span class="hljs-string">&#x27;gpt2&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer.encode(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, add_special_tokens=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;tf&#x27;</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, logits = outputs.loss, outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save and load from pretrained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;bert2gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;bert2gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.bos_token_id)`}}),dt=new Z({props:{name:"from_encoder_decoder_pretrained",anchor:"transformers.TFEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": str = None"},{name:"decoder_pretrained_model_name_or_path",val:": str = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py#L300"}}),lt=new zo({props:{code:`from transformers import TFEncoderDecoderModel
# initialize a bert2gpt2 from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized
model = TFEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'gpt2')
# saving model after fine-tuning
model.save_pretrained("./bert2gpt2")
# load fine-tuned model
model = TFEncoderDecoderModel.from_pretrained("./bert2gpt2"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a bert2gpt2 from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, <span class="hljs-string">&#x27;gpt2&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./bert2gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./bert2gpt2&quot;</span>)`}}),ct=new Za({}),pt=new Z({props:{name:"class transformers.FlaxEncoderDecoderModel",anchor:"transformers.FlaxEncoderDecoderModel",parameters:[{name:"config",val:": EncoderDecoderConfig"},{name:"input_shape",val:": typing.Optional[typing.Tuple] = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py#L310",parametersDescription:[{anchor:"transformers.FlaxEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the
model weights.`,name:"config"},{anchor:"transformers.FlaxEncoderDecoderModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on
GPUs) and <code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see
<a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and <a href="/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),_t=new Z({props:{name:"__call__",anchor:"transformers.FlaxEncoderDecoderModel.__call__",parameters:[{name:"input_ids",val:": ndarray"},{name:"attention_mask",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"decoder_input_ids",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"decoder_attention_mask",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"position_ids",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"decoder_position_ids",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"train",val:": bool = False"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py#L619",parametersDescription:[{anchor:"transformers.FlaxEncoderDecoderModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.14.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/v4.14.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>For sequence to sequence training, <code>decoder_input_ids</code> should be provided. If no
<code>decoder_input_ids</code> is provided, the model will create this tensor by shifting the <code>input_ids</code> to
the right for denoising pre-training.`,name:"decoder_input_ids"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.encoder.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.decoder_position_ids",description:`<strong>decoder_position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
range <code>[0, config.decoder.max_position_embeddings - 1]</code>.`,name:"decoder_position_ids"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>FlaxSeq2SeqLMOutput</code> instead
of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"
>FlaxSeq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"
>EncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.14.1/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"
>FlaxSeq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ho=new Kf({props:{$$slots:{default:[lv]},$$scope:{ctx:Te}}}),vt=new zo({props:{code:`from transformers import FlaxEncoderDecoderModel, BertTokenizer, GPT2Tokenizer

# load a fine-tuned bert2gpt2 model
model = FlaxEncoderDecoderModel.from_pretrained("patrickvonplaten/bert2gpt2-cnn_dailymail-fp16")
# load input & output tokenizer
tokenizer_input = BertTokenizer.from_pretrained('bert-base-cased')
tokenizer_output = GPT2Tokenizer.from_pretrained('gpt2')

article = '''Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members
singing a racist chant. SAE's national chapter suspended the students,
but University of Oklahoma President David Boren took it a step further,
saying the university's affiliation with the fraternity is permanently done.'''

input_ids = tokenizer_input(article, add_special_tokens=True, return_tensors='np').input_ids

# use GPT2's eos_token as the pad as well as eos token
model.config.eos_token_id = model.config.decoder.eos_token_id
model.config.pad_token_id = model.config.eos_token_id

sequences = model.generate(input_ids, num_beams=4, max_length=12).sequences

summary = tokenizer_output.batch_decode(sequences, skip_special_tokens=True)[0]
assert summary == "SAS Alpha Epsilon suspended Sigma Alpha Epsilon members",`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxEncoderDecoderModel, BertTokenizer, GPT2Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load a fine-tuned bert2gpt2 model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/bert2gpt2-cnn_dailymail-fp16&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load input &amp; output tokenizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_input = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_output = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&#x27;gpt2&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&#x27;&#x27;&#x27;Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members
<span class="hljs-meta">... </span>singing a racist chant. SAE&#x27;s national chapter suspended the students,
<span class="hljs-meta">... </span>but University of Oklahoma President David Boren took it a step further,
<span class="hljs-meta">... </span>saying the university&#x27;s affiliation with the fraternity is permanently done.&#x27;&#x27;&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_input(article, add_special_tokens=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;np&#x27;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># use GPT2&#x27;s eos_token as the pad as well as eos token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.eos_token_id = model.config.decoder.eos_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>sequences = model.generate(input_ids, num_beams=<span class="hljs-number">4</span>, max_length=<span class="hljs-number">12</span>).sequences

<span class="hljs-meta">&gt;&gt;&gt; </span>summary = tokenizer_output.batch_decode(sequences, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> summary == <span class="hljs-string">&quot;SAS Alpha Epsilon suspended Sigma Alpha Epsilon members&quot;</span>`}}),bt=new Z({props:{name:"from_encoder_decoder_pretrained",anchor:"transformers.FlaxEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"decoder_pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.14.1/src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py#L743"}}),Tt=new zo({props:{code:`from transformers import FlaxEncoderDecoderModel
# initialize a bert2gpt2 from pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')
# saving model after fine-tuning
model.save_pretrained("./bert2gpt2")
# load fine-tuned model
model = FlaxEncoderDecoderModel.from_pretrained("./bert2gpt2"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a bert2gpt2 from pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, <span class="hljs-string">&#x27;gpt2&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./bert2gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./bert2gpt2&quot;</span>)`}}),{c(){h=r("meta"),F=i(),f=r("h1"),j=r("a"),L=r("span"),v(M.$$.fragment),D=i(),B=r("span"),Ss=o("Encoder Decoder Models"),Ka=i(),Xe=r("p"),Ls=o("The "),zt=r("a"),Os=o("EncoderDecoderModel"),Ns=o(` can be used to initialize a sequence-to-sequence model with any
pretrained autoencoding model as the encoder and any pretrained autoregressive model as the decoder.`),Qa=i(),eo=r("p"),Bs=o(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation tasks
was shown in `),jo=r("a"),Us=o("Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),Ws=o(` by
Sascha Rothe, Shashi Narayan, Aliaksei Severyn.`),Xa=i(),oo=r("p"),Vs=o("After such an "),jt=r("a"),Rs=o("EncoderDecoderModel"),Gs=o(` has been trained/fine-tuned, it can be saved/loaded just like
any other models (see the examples for more information).`),es=i(),he=r("p"),Hs=o("An application of this architecture could be to leverage two pretrained "),$t=r("a"),Js=o("BertModel"),Ys=o(` as the encoder
and decoder for a summarization model as was shown in: `),$o=r("a"),Zs=o("Text Summarization with Pretrained Encoders"),Ks=o(" by Yang Liu and Mirella Lapata."),os=i(),me=r("p"),Qs=o("The "),Mn=r("code"),Xs=o("from_pretrained()"),ed=o(` currently doesn\u2019t support initializing the model from a
pytorch checkpoint. Passing `),Dn=r("code"),od=o("from_pt=True"),td=o(` to this method will throw an exception. If there are only pytorch
checkpoints for a particular encoder-decoder model, a workaround is:`),ts=i(),v(Po.$$.fragment),ns=i(),fe=r("p"),nd=o("This model was contributed by "),qo=r("a"),rd=o("thomwolf"),ad=o(`. This model\u2019s TensorFlow and Flax versions
were contributed by `),Co=r("a"),sd=o("ydshieh"),dd=o("."),rs=i(),we=r("h2"),to=r("a"),xn=r("span"),v(Fo.$$.fragment),id=i(),zn=r("span"),ld=o("EncoderDecoderConfig"),as=i(),C=r("div"),v(Ao.$$.fragment),cd=i(),no=r("p"),Pt=r("a"),pd=o("EncoderDecoderConfig"),hd=o(` is the configuration class to store the configuration of a
`),qt=r("a"),md=o("EncoderDecoderModel"),fd=o(`. It is used to instantiate an Encoder Decoder model according to the
specified arguments, defining the encoder and decoder configs.`),ud=i(),Me=r("p"),gd=o("Configuration objects inherit from "),Ct=r("a"),_d=o("PretrainedConfig"),vd=o(` and can be used to control the model
outputs. Read the documentation from `),Ft=r("a"),bd=o("PretrainedConfig"),Ed=o(" for more information."),yd=i(),jn=r("p"),kd=o("Examples:"),Td=i(),v(Io.$$.fragment),wd=i(),ro=r("div"),v(So.$$.fragment),Md=i(),Lo=r("p"),Dd=o("Instantiate a "),At=r("a"),xd=o("EncoderDecoderConfig"),zd=o(` (or a derived class) from a pre-trained encoder model
configuration and decoder model configuration.`),jd=i(),ao=r("div"),v(Oo.$$.fragment),$d=i(),De=r("p"),Pd=o("Serializes this instance to a Python dictionary. Override the default "),$n=r("em"),qd=o("to_dict()"),Cd=o(" from "),Pn=r("em"),Fd=o("PretrainedConfig"),Ad=o("."),ss=i(),xe=r("h2"),so=r("a"),qn=r("span"),v(No.$$.fragment),Id=i(),Cn=r("span"),Sd=o("EncoderDecoderModel"),ds=i(),$=r("div"),v(Bo.$$.fragment),Ld=i(),ze=r("p"),Od=o(`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),Fn=r("code"),Nd=o("from_pretrained()"),Bd=o(` function and the decoder is loaded via
`),An=r("code"),Ud=o("from_pretrained()"),Wd=o(` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),Vd=i(),Uo=r("p"),Rd=o(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Wo=r("a"),Gd=o("Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),Hd=o(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Jd=i(),In=r("p"),Yd=o(`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),Zd=i(),Vo=r("p"),Kd=o("This model inherits from "),It=r("a"),Qd=o("PreTrainedModel"),Xd=o(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ei=i(),Ro=r("p"),oi=o("This model is also a PyTorch "),Go=r("a"),ti=o("torch.nn.Module"),ni=o(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),ri=i(),ue=r("p"),St=r("a"),ai=o("EncoderDecoderModel"),si=o(` is a generic model class that will be instantiated as a transformer
architecture with one of the base model classes of the library as encoder and another one as decoder when created
with the :meth`),Sn=r("em"),di=o("~transformers.AutoModel.from_pretrained"),ii=o(` class method for the encoder and
:meth`),Ln=r("em"),li=o("~transformers.AutoModelForCausalLM.from_pretrained"),ci=o(" class method for the decoder."),pi=i(),U=r("div"),v(Ho.$$.fragment),hi=i(),je=r("p"),mi=o("The "),Lt=r("a"),fi=o("EncoderDecoderModel"),ui=o(" forward method, overrides the "),On=r("code"),gi=o("__call__"),_i=o(" special method."),vi=i(),v(io.$$.fragment),bi=i(),Nn=r("p"),Ei=o("Examples:"),yi=i(),v(Jo.$$.fragment),ki=i(),u=r("div"),v(Yo.$$.fragment),Ti=i(),Bn=r("p"),wi=o(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Mi=i(),$e=r("p"),Di=o("The model is set in evaluation mode by default using "),Un=r("code"),xi=o("model.eval()"),zi=o(` (Dropout modules are deactivated). To
train the model, you need to first set it back in training mode with `),Wn=r("code"),ji=o("model.train()"),$i=o("."),Pi=i(),Pe=r("p"),qi=o(`Params:
encoder`),Vn=r("em"),Ci=o("pretrained_model_name_or_path (:obj: _str"),Fi=o(", "),Rn=r("em"),Ai=o("optional"),Ii=o(`):
Information necessary to initiate the encoder. Can be either:`),Si=i(),qe=r("ul"),K=r("li"),Li=o("A string, the "),Gn=r("em"),Oi=o("model id"),Ni=o(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Hn=r("code"),Bi=o("bert-base-uncased"),Ui=o(`, or namespaced under
a user or organization name, like `),Jn=r("code"),Wi=o("dbmdz/bert-base-german-cased"),Vi=o("."),Ri=i(),Q=r("li"),Gi=o("A path to a "),Yn=r("em"),Hi=o("directory"),Ji=o(` containing model weights saved using
`),Ot=r("a"),Yi=o("save_pretrained()"),Zi=o(", e.g., "),Zn=r("code"),Ki=o("./my_model_directory/"),Qi=o("."),Xi=i(),O=r("li"),el=o("A path or url to a "),Kn=r("em"),ol=o("tensorflow index checkpoint file"),tl=o(" (e.g, "),Qn=r("code"),nl=o("./tf_model/model.ckpt.index"),rl=o(`). In
this case, `),Xn=r("code"),al=o("from_tf"),sl=o(" should be set to "),er=r("code"),dl=o("True"),il=o(` and a configuration object should be provided
as `),or=r("code"),ll=o("config"),cl=o(` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),pl=i(),X=r("p"),hl=o("decoder"),tr=r("em"),ml=o("pretrained_model_name_or_path (:obj: _str"),fl=o(", "),nr=r("em"),ul=o("optional"),gl=o(", defaults to "),rr=r("em"),_l=o("None"),vl=o(`):
Information necessary to initiate the decoder. Can be either:`),bl=i(),Ce=r("ul"),ee=r("li"),El=o("A string, the "),ar=r("em"),yl=o("model id"),kl=o(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),sr=r("code"),Tl=o("bert-base-uncased"),wl=o(`, or namespaced under
a user or organization name, like `),dr=r("code"),Ml=o("dbmdz/bert-base-german-cased"),Dl=o("."),xl=i(),oe=r("li"),zl=o("A path to a "),ir=r("em"),jl=o("directory"),$l=o(` containing model weights saved using
`),Nt=r("a"),Pl=o("save_pretrained()"),ql=o(", e.g., "),lr=r("code"),Cl=o("./my_model_directory/"),Fl=o("."),Al=i(),N=r("li"),Il=o("A path or url to a "),cr=r("em"),Sl=o("tensorflow index checkpoint file"),Ll=o(" (e.g, "),pr=r("code"),Ol=o("./tf_model/model.ckpt.index"),Nl=o(`). In
this case, `),hr=r("code"),Bl=o("from_tf"),Ul=o(" should be set to "),mr=r("code"),Wl=o("True"),Vl=o(` and a configuration object should be provided
as `),fr=r("code"),Rl=o("config"),Gl=o(` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),Hl=i(),Fe=r("p"),Jl=o("model"),ur=r("em"),Yl=o("args (remaining positional arguments, _optional"),Zl=o(`):
All remaining positional arguments will be passed to the underlying model\u2019s `),gr=r("code"),Kl=o("__init__"),Ql=o(" method."),Xl=i(),Ae=r("p"),ec=o("kwargs (remaining dictionary of keyword arguments, "),_r=r("em"),oc=o("optional"),tc=o(`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),vr=r("code"),nc=o("output_attentions=True"),rc=o(")."),ac=i(),Ie=r("ul"),br=r("li"),sc=o("To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),dc=i(),Er=r("li"),ic=o("To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),lc=i(),yr=r("li"),cc=o("To update the parent model configuration, do not use a prefix for each configuration parameter."),pc=i(),Zo=r("p"),hc=o("Behaves differently depending on whether a "),kr=r("code"),mc=o("config"),fc=o(" is provided or automatically loaded."),uc=i(),Tr=r("p"),gc=o("Example:"),_c=i(),v(Ko.$$.fragment),is=i(),Se=r("h2"),lo=r("a"),wr=r("span"),v(Qo.$$.fragment),vc=i(),Mr=r("span"),bc=o("TFEncoderDecoderModel"),ls=i(),P=r("div"),v(Xo.$$.fragment),Ec=i(),Le=r("p"),yc=o(`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),Dr=r("code"),kc=o("from_pretrained()"),Tc=o(` function and the decoder is loaded via
`),xr=r("code"),wc=o("from_pretrained()"),Mc=o(` function. Cross-attention layers are automatically
added to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),Dc=i(),et=r("p"),xc=o(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),ot=r("a"),zc=o("Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),jc=o(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),$c=i(),zr=r("p"),Pc=o(`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),qc=i(),tt=r("p"),Cc=o("This model inherits from "),Bt=r("a"),Fc=o("TFPreTrainedModel"),Ac=o(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Ic=i(),nt=r("p"),Sc=o("This model is also a "),rt=r("a"),Lc=o("tf.keras.Model"),Oc=o(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Nc=i(),ge=r("p"),jr=r("code"),Bc=o("TFEncoderDecoder"),Uc=o(` is a generic model class that will be instantiated as a transformer
architecture with one of the base model classes of the library as encoder and another one as decoder when created
with the :meth`),$r=r("em"),Wc=o("~transformers.TFAutoModel.from_pretrained"),Vc=o(` class method for the encoder and
:meth`),Pr=r("em"),Rc=o("~transformers.TFAutoModelForCausalLM.from_pretrained"),Gc=o(" class method for the decoder."),Hc=i(),W=r("div"),v(at.$$.fragment),Jc=i(),Oe=r("p"),Yc=o("The "),Ut=r("a"),Zc=o("TFEncoderDecoderModel"),Kc=o(" forward method, overrides the "),qr=r("code"),Qc=o("__call__"),Xc=o(" special method."),ep=i(),v(co.$$.fragment),op=i(),Cr=r("p"),tp=o("Examples:"),np=i(),v(st.$$.fragment),rp=i(),g=r("div"),v(dt.$$.fragment),ap=i(),Fr=r("p"),sp=o(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),dp=i(),Ne=r("p"),ip=o(`Params:
encoder`),Ar=r("em"),lp=o("pretrained_model_name_or_path (:obj: _str"),cp=o(", "),Ir=r("em"),pp=o("optional"),hp=o(`):
Information necessary to initiate the encoder. Can be either:`),mp=i(),Be=r("ul"),te=r("li"),fp=o("A string, the "),Sr=r("em"),up=o("model id"),gp=o(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Lr=r("code"),_p=o("bert-base-uncased"),vp=o(`, or namespaced under
a user or organization name, like `),Or=r("code"),bp=o("dbmdz/bert-base-german-cased"),Ep=o("."),yp=i(),ne=r("li"),kp=o("A path to a "),Nr=r("em"),Tp=o("directory"),wp=o(` containing model weights saved using
`),Wt=r("a"),Mp=o("save_pretrained()"),Dp=o(", e.g., "),Br=r("code"),xp=o("./my_model_directory/"),zp=o("."),jp=i(),G=r("li"),$p=o("A path or url to a "),Ur=r("em"),Pp=o("pytorch index checkpoint file"),qp=o(" (e.g, "),Wr=r("code"),Cp=o("./pt_model/"),Fp=o(`). In this case,
`),Vr=r("code"),Ap=o("encoder_from_pt"),Ip=o(" should be set to "),Rr=r("code"),Sp=o("True"),Lp=o("."),Op=i(),re=r("p"),Np=o("decoder"),Gr=r("em"),Bp=o("pretrained_model_name_or_path (:obj: _str"),Up=o(", "),Hr=r("em"),Wp=o("optional"),Vp=o(", defaults to "),Jr=r("em"),Rp=o("None"),Gp=o(`):
Information necessary to initiate the decoder. Can be either:`),Hp=i(),Ue=r("ul"),ae=r("li"),Jp=o("A string, the "),Yr=r("em"),Yp=o("model id"),Zp=o(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Zr=r("code"),Kp=o("bert-base-uncased"),Qp=o(`, or namespaced under
a user or organization name, like `),Kr=r("code"),Xp=o("dbmdz/bert-base-german-cased"),eh=o("."),oh=i(),se=r("li"),th=o("A path to a "),Qr=r("em"),nh=o("directory"),rh=o(` containing model weights saved using
`),Vt=r("a"),ah=o("save_pretrained()"),sh=o(", e.g., "),Xr=r("code"),dh=o("./my_model_directory/"),ih=o("."),lh=i(),H=r("li"),ch=o("A path or url to a "),ea=r("em"),ph=o("pytorch checkpoint file"),hh=o(" (e.g, "),oa=r("code"),mh=o("./pt_model/"),fh=o(`). In this case,
`),ta=r("code"),uh=o("decoder_from_pt"),gh=o(" should be set to "),na=r("code"),_h=o("True"),vh=o("."),bh=i(),We=r("p"),Eh=o("model"),ra=r("em"),yh=o("args (remaining positional arguments, _optional"),kh=o(`):
All remaning positional arguments will be passed to the underlying model\u2019s `),aa=r("code"),Th=o("__init__"),wh=o(" method."),Mh=i(),Ve=r("p"),Dh=o("kwargs (remaining dictionary of keyword arguments, "),sa=r("em"),xh=o("optional"),zh=o(`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),da=r("code"),jh=o("output_attentions=True"),$h=o(")."),Ph=i(),Re=r("ul"),ia=r("li"),qh=o("To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),Ch=i(),la=r("li"),Fh=o("To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),Ah=i(),ca=r("li"),Ih=o("To update the parent model configuration, do not use a prefix for each configuration parameter."),Sh=i(),it=r("p"),Lh=o("Behaves differently depending on whether a "),pa=r("code"),Oh=o("config"),Nh=o(" is provided or automatically loaded."),Bh=i(),ha=r("p"),Uh=o("Example:"),Wh=i(),v(lt.$$.fragment),cs=i(),Ge=r("h2"),po=r("a"),ma=r("span"),v(ct.$$.fragment),Vh=i(),fa=r("span"),Rh=o("FlaxEncoderDecoderModel"),ps=i(),q=r("div"),v(pt.$$.fragment),Gh=i(),He=r("p"),Hh=o(`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),ua=r("code"),Jh=o("from_pretrained()"),Yh=o(` function and the decoder is loaded via
`),ga=r("code"),Zh=o("from_pretrained()"),Kh=o(` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),Qh=i(),ht=r("p"),Xh=o(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),mt=r("a"),em=o("Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),om=o(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),tm=i(),_a=r("p"),nm=o(`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),rm=i(),ft=r("p"),am=o("This model inherits from "),Rt=r("a"),sm=o("FlaxPreTrainedModel"),dm=o(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),im=i(),ut=r("p"),lm=o("This model is also a Flax Linen "),gt=r("a"),cm=o("flax.nn.Module"),pm=o(` subclass. Use it as a regular Flax
Module and refer to the Flax documentation for all matter related to general usage and behavior.`),hm=i(),_e=r("p"),Gt=r("a"),mm=o("FlaxEncoderDecoderModel"),fm=o(` is a generic model class that will be instantiated as a transformer
architecture with the module (flax.nn.Module) of one of the base model classes of the library as encoder module and
another one as decoder module when created with the :meth`),va=r("em"),um=o("~transformers.FlaxAutoModel.from_pretrained"),gm=o(` class method
for the encoder and :meth`),ba=r("em"),_m=o("~transformers.FlaxAutoModelForCausalLM.from_pretrained"),vm=o(" class method for the decoder."),bm=i(),V=r("div"),v(_t.$$.fragment),Em=i(),Je=r("p"),ym=o("The "),Ht=r("a"),km=o("FlaxEncoderDecoderModel"),Tm=o(" forward method, overrides the "),Ea=r("code"),wm=o("__call__"),Mm=o(" special method."),Dm=i(),v(ho.$$.fragment),xm=i(),ya=r("p"),zm=o("Examples:"),jm=i(),v(vt.$$.fragment),$m=i(),_=r("div"),v(bt.$$.fragment),Pm=i(),ka=r("p"),qm=o(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Cm=i(),Ye=r("p"),Fm=o(`Params:
encoder`),Ta=r("em"),Am=o("pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),Im=o(", "),wa=r("em"),Sm=o("optional"),Lm=o(`):
Information necessary to initiate the encoder. Can be either:`),Om=i(),Et=r("ul"),de=r("li"),Nm=o("A string, the "),Ma=r("em"),Bm=o("model id"),Um=o(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Da=r("code"),Wm=o("bert-base-uncased"),Vm=o(`, or namespaced under
a user or organization name, like `),xa=r("code"),Rm=o("dbmdz/bert-base-german-cased"),Gm=o("."),Hm=i(),ie=r("li"),Jm=o("A path to a "),za=r("em"),Ym=o("directory"),Zm=o(` containing model weights saved using
`),Jt=r("a"),Km=o("save_pretrained()"),Qm=o(", e.g., "),ja=r("code"),Xm=o("./my_model_directory/"),ef=o("."),of=i(),le=r("p"),tf=o("decoder"),$a=r("em"),nf=o("pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),rf=o(", "),Pa=r("em"),af=o("optional"),sf=o(", defaults to "),qa=r("em"),df=o("None"),lf=o(`):
Information necessary to initiate the decoder. Can be either:`),cf=i(),yt=r("ul"),ce=r("li"),pf=o("A string, the "),Ca=r("em"),hf=o("model id"),mf=o(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Fa=r("code"),ff=o("bert-base-uncased"),uf=o(`, or namespaced under
a user or organization name, like `),Aa=r("code"),gf=o("dbmdz/bert-base-german-cased"),_f=o("."),vf=i(),pe=r("li"),bf=o("A path to a "),Ia=r("em"),Ef=o("directory"),yf=o(` containing model weights saved using
`),Yt=r("a"),kf=o("save_pretrained()"),Tf=o(", e.g., "),Sa=r("code"),wf=o("./my_model_directory/"),Mf=o("."),Df=i(),Ze=r("p"),xf=o("model"),La=r("em"),zf=o("args (remaining positional arguments, _optional"),jf=o(`):
All remaning positional arguments will be passed to the underlying model\u2019s `),Oa=r("code"),$f=o("__init__"),Pf=o(" method."),qf=i(),Ke=r("p"),Cf=o("kwargs (remaining dictionary of keyword arguments, "),Na=r("em"),Ff=o("optional"),Af=o(`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),Ba=r("code"),If=o("output_attentions=True"),Sf=o(")."),Lf=i(),Qe=r("ul"),Ua=r("li"),Of=o("To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),Nf=i(),Wa=r("li"),Bf=o("To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),Uf=i(),Va=r("li"),Wf=o("To update the parent model configuration, do not use a prefix for each configuration parameter."),Vf=i(),kt=r("p"),Rf=o("Behaves differently depending on whether a "),Ra=r("code"),Gf=o("config"),Hf=o(" is provided or automatically loaded."),Jf=i(),Ga=r("p"),Yf=o("Example:"),Zf=i(),v(Tt.$$.fragment),this.h()},l(d){const p=sv('[data-svelte="svelte-1phssyn"]',document.head);h=a(p,"META",{name:!0,content:!0}),p.forEach(n),F=l(d),f=a(d,"H1",{class:!0});var wt=s(f);j=a(wt,"A",{id:!0,class:!0,href:!0});var Ha=s(j);L=a(Ha,"SPAN",{});var Ja=s(L);b(M.$$.fragment,Ja),Ja.forEach(n),Ha.forEach(n),D=l(wt),B=a(wt,"SPAN",{});var Qf=s(B);Ss=t(Qf,"Encoder Decoder Models"),Qf.forEach(n),wt.forEach(n),Ka=l(d),Xe=a(d,"P",{});var ms=s(Xe);Ls=t(ms,"The "),zt=a(ms,"A",{href:!0});var Xf=s(zt);Os=t(Xf,"EncoderDecoderModel"),Xf.forEach(n),Ns=t(ms,` can be used to initialize a sequence-to-sequence model with any
pretrained autoencoding model as the encoder and any pretrained autoregressive model as the decoder.`),ms.forEach(n),Qa=l(d),eo=a(d,"P",{});var fs=s(eo);Bs=t(fs,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation tasks
was shown in `),jo=a(fs,"A",{href:!0,rel:!0});var eu=s(jo);Us=t(eu,"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),eu.forEach(n),Ws=t(fs,` by
Sascha Rothe, Shashi Narayan, Aliaksei Severyn.`),fs.forEach(n),Xa=l(d),oo=a(d,"P",{});var us=s(oo);Vs=t(us,"After such an "),jt=a(us,"A",{href:!0});var ou=s(jt);Rs=t(ou,"EncoderDecoderModel"),ou.forEach(n),Gs=t(us,` has been trained/fine-tuned, it can be saved/loaded just like
any other models (see the examples for more information).`),us.forEach(n),es=l(d),he=a(d,"P",{});var Zt=s(he);Hs=t(Zt,"An application of this architecture could be to leverage two pretrained "),$t=a(Zt,"A",{href:!0});var tu=s($t);Js=t(tu,"BertModel"),tu.forEach(n),Ys=t(Zt,` as the encoder
and decoder for a summarization model as was shown in: `),$o=a(Zt,"A",{href:!0,rel:!0});var nu=s($o);Zs=t(nu,"Text Summarization with Pretrained Encoders"),nu.forEach(n),Ks=t(Zt," by Yang Liu and Mirella Lapata."),Zt.forEach(n),os=l(d),me=a(d,"P",{});var Kt=s(me);Qs=t(Kt,"The "),Mn=a(Kt,"CODE",{});var ru=s(Mn);Xs=t(ru,"from_pretrained()"),ru.forEach(n),ed=t(Kt,` currently doesn\u2019t support initializing the model from a
pytorch checkpoint. Passing `),Dn=a(Kt,"CODE",{});var au=s(Dn);od=t(au,"from_pt=True"),au.forEach(n),td=t(Kt,` to this method will throw an exception. If there are only pytorch
checkpoints for a particular encoder-decoder model, a workaround is:`),Kt.forEach(n),ts=l(d),b(Po.$$.fragment,d),ns=l(d),fe=a(d,"P",{});var Qt=s(fe);nd=t(Qt,"This model was contributed by "),qo=a(Qt,"A",{href:!0,rel:!0});var su=s(qo);rd=t(su,"thomwolf"),su.forEach(n),ad=t(Qt,`. This model\u2019s TensorFlow and Flax versions
were contributed by `),Co=a(Qt,"A",{href:!0,rel:!0});var du=s(Co);sd=t(du,"ydshieh"),du.forEach(n),dd=t(Qt,"."),Qt.forEach(n),rs=l(d),we=a(d,"H2",{class:!0});var gs=s(we);to=a(gs,"A",{id:!0,class:!0,href:!0});var iu=s(to);xn=a(iu,"SPAN",{});var lu=s(xn);b(Fo.$$.fragment,lu),lu.forEach(n),iu.forEach(n),id=l(gs),zn=a(gs,"SPAN",{});var cu=s(zn);ld=t(cu,"EncoderDecoderConfig"),cu.forEach(n),gs.forEach(n),as=l(d),C=a(d,"DIV",{class:!0});var R=s(C);b(Ao.$$.fragment,R),cd=l(R),no=a(R,"P",{});var Ya=s(no);Pt=a(Ya,"A",{href:!0});var pu=s(Pt);pd=t(pu,"EncoderDecoderConfig"),pu.forEach(n),hd=t(Ya,` is the configuration class to store the configuration of a
`),qt=a(Ya,"A",{href:!0});var hu=s(qt);md=t(hu,"EncoderDecoderModel"),hu.forEach(n),fd=t(Ya,`. It is used to instantiate an Encoder Decoder model according to the
specified arguments, defining the encoder and decoder configs.`),Ya.forEach(n),ud=l(R),Me=a(R,"P",{});var Xt=s(Me);gd=t(Xt,"Configuration objects inherit from "),Ct=a(Xt,"A",{href:!0});var mu=s(Ct);_d=t(mu,"PretrainedConfig"),mu.forEach(n),vd=t(Xt,` and can be used to control the model
outputs. Read the documentation from `),Ft=a(Xt,"A",{href:!0});var fu=s(Ft);bd=t(fu,"PretrainedConfig"),fu.forEach(n),Ed=t(Xt," for more information."),Xt.forEach(n),yd=l(R),jn=a(R,"P",{});var uu=s(jn);kd=t(uu,"Examples:"),uu.forEach(n),Td=l(R),b(Io.$$.fragment,R),wd=l(R),ro=a(R,"DIV",{class:!0});var _s=s(ro);b(So.$$.fragment,_s),Md=l(_s),Lo=a(_s,"P",{});var vs=s(Lo);Dd=t(vs,"Instantiate a "),At=a(vs,"A",{href:!0});var gu=s(At);xd=t(gu,"EncoderDecoderConfig"),gu.forEach(n),zd=t(vs,` (or a derived class) from a pre-trained encoder model
configuration and decoder model configuration.`),vs.forEach(n),_s.forEach(n),jd=l(R),ao=a(R,"DIV",{class:!0});var bs=s(ao);b(Oo.$$.fragment,bs),$d=l(bs),De=a(bs,"P",{});var en=s(De);Pd=t(en,"Serializes this instance to a Python dictionary. Override the default "),$n=a(en,"EM",{});var _u=s($n);qd=t(_u,"to_dict()"),_u.forEach(n),Cd=t(en," from "),Pn=a(en,"EM",{});var vu=s(Pn);Fd=t(vu,"PretrainedConfig"),vu.forEach(n),Ad=t(en,"."),en.forEach(n),bs.forEach(n),R.forEach(n),ss=l(d),xe=a(d,"H2",{class:!0});var Es=s(xe);so=a(Es,"A",{id:!0,class:!0,href:!0});var bu=s(so);qn=a(bu,"SPAN",{});var Eu=s(qn);b(No.$$.fragment,Eu),Eu.forEach(n),bu.forEach(n),Id=l(Es),Cn=a(Es,"SPAN",{});var yu=s(Cn);Sd=t(yu,"EncoderDecoderModel"),yu.forEach(n),Es.forEach(n),ds=l(d),$=a(d,"DIV",{class:!0});var A=s($);b(Bo.$$.fragment,A),Ld=l(A),ze=a(A,"P",{});var on=s(ze);Od=t(on,`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),Fn=a(on,"CODE",{});var ku=s(Fn);Nd=t(ku,"from_pretrained()"),ku.forEach(n),Bd=t(on,` function and the decoder is loaded via
`),An=a(on,"CODE",{});var Tu=s(An);Ud=t(Tu,"from_pretrained()"),Tu.forEach(n),Wd=t(on,` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),on.forEach(n),Vd=l(A),Uo=a(A,"P",{});var ys=s(Uo);Rd=t(ys,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Wo=a(ys,"A",{href:!0,rel:!0});var wu=s(Wo);Gd=t(wu,"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),wu.forEach(n),Hd=t(ys,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),ys.forEach(n),Jd=l(A),In=a(A,"P",{});var Mu=s(In);Yd=t(Mu,`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),Mu.forEach(n),Zd=l(A),Vo=a(A,"P",{});var ks=s(Vo);Kd=t(ks,"This model inherits from "),It=a(ks,"A",{href:!0});var Du=s(It);Qd=t(Du,"PreTrainedModel"),Du.forEach(n),Xd=t(ks,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ks.forEach(n),ei=l(A),Ro=a(A,"P",{});var Ts=s(Ro);oi=t(Ts,"This model is also a PyTorch "),Go=a(Ts,"A",{href:!0,rel:!0});var xu=s(Go);ti=t(xu,"torch.nn.Module"),xu.forEach(n),ni=t(Ts,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Ts.forEach(n),ri=l(A),ue=a(A,"P",{});var Mt=s(ue);St=a(Mt,"A",{href:!0});var zu=s(St);ai=t(zu,"EncoderDecoderModel"),zu.forEach(n),si=t(Mt,` is a generic model class that will be instantiated as a transformer
architecture with one of the base model classes of the library as encoder and another one as decoder when created
with the :meth`),Sn=a(Mt,"EM",{});var ju=s(Sn);di=t(ju,"~transformers.AutoModel.from_pretrained"),ju.forEach(n),ii=t(Mt,` class method for the encoder and
:meth`),Ln=a(Mt,"EM",{});var $u=s(Ln);li=t($u,"~transformers.AutoModelForCausalLM.from_pretrained"),$u.forEach(n),ci=t(Mt," class method for the decoder."),Mt.forEach(n),pi=l(A),U=a(A,"DIV",{class:!0});var ve=s(U);b(Ho.$$.fragment,ve),hi=l(ve),je=a(ve,"P",{});var tn=s(je);mi=t(tn,"The "),Lt=a(tn,"A",{href:!0});var Pu=s(Lt);fi=t(Pu,"EncoderDecoderModel"),Pu.forEach(n),ui=t(tn," forward method, overrides the "),On=a(tn,"CODE",{});var qu=s(On);gi=t(qu,"__call__"),qu.forEach(n),_i=t(tn," special method."),tn.forEach(n),vi=l(ve),b(io.$$.fragment,ve),bi=l(ve),Nn=a(ve,"P",{});var Cu=s(Nn);Ei=t(Cu,"Examples:"),Cu.forEach(n),yi=l(ve),b(Jo.$$.fragment,ve),ve.forEach(n),ki=l(A),u=a(A,"DIV",{class:!0});var w=s(u);b(Yo.$$.fragment,w),Ti=l(w),Bn=a(w,"P",{});var Fu=s(Bn);wi=t(Fu,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Fu.forEach(n),Mi=l(w),$e=a(w,"P",{});var nn=s($e);Di=t(nn,"The model is set in evaluation mode by default using "),Un=a(nn,"CODE",{});var Au=s(Un);xi=t(Au,"model.eval()"),Au.forEach(n),zi=t(nn,` (Dropout modules are deactivated). To
train the model, you need to first set it back in training mode with `),Wn=a(nn,"CODE",{});var Iu=s(Wn);ji=t(Iu,"model.train()"),Iu.forEach(n),$i=t(nn,"."),nn.forEach(n),Pi=l(w),Pe=a(w,"P",{});var rn=s(Pe);qi=t(rn,`Params:
encoder`),Vn=a(rn,"EM",{});var Su=s(Vn);Ci=t(Su,"pretrained_model_name_or_path (:obj: _str"),Su.forEach(n),Fi=t(rn,", "),Rn=a(rn,"EM",{});var Lu=s(Rn);Ai=t(Lu,"optional"),Lu.forEach(n),Ii=t(rn,`):
Information necessary to initiate the encoder. Can be either:`),rn.forEach(n),Si=l(w),qe=a(w,"UL",{});var an=s(qe);K=a(an,"LI",{});var mo=s(K);Li=t(mo,"A string, the "),Gn=a(mo,"EM",{});var Ou=s(Gn);Oi=t(Ou,"model id"),Ou.forEach(n),Ni=t(mo,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Hn=a(mo,"CODE",{});var Nu=s(Hn);Bi=t(Nu,"bert-base-uncased"),Nu.forEach(n),Ui=t(mo,`, or namespaced under
a user or organization name, like `),Jn=a(mo,"CODE",{});var Bu=s(Jn);Wi=t(Bu,"dbmdz/bert-base-german-cased"),Bu.forEach(n),Vi=t(mo,"."),mo.forEach(n),Ri=l(an),Q=a(an,"LI",{});var fo=s(Q);Gi=t(fo,"A path to a "),Yn=a(fo,"EM",{});var Uu=s(Yn);Hi=t(Uu,"directory"),Uu.forEach(n),Ji=t(fo,` containing model weights saved using
`),Ot=a(fo,"A",{href:!0});var Wu=s(Ot);Yi=t(Wu,"save_pretrained()"),Wu.forEach(n),Zi=t(fo,", e.g., "),Zn=a(fo,"CODE",{});var Vu=s(Zn);Ki=t(Vu,"./my_model_directory/"),Vu.forEach(n),Qi=t(fo,"."),fo.forEach(n),Xi=l(an),O=a(an,"LI",{});var J=s(O);el=t(J,"A path or url to a "),Kn=a(J,"EM",{});var Ru=s(Kn);ol=t(Ru,"tensorflow index checkpoint file"),Ru.forEach(n),tl=t(J," (e.g, "),Qn=a(J,"CODE",{});var Gu=s(Qn);nl=t(Gu,"./tf_model/model.ckpt.index"),Gu.forEach(n),rl=t(J,`). In
this case, `),Xn=a(J,"CODE",{});var Hu=s(Xn);al=t(Hu,"from_tf"),Hu.forEach(n),sl=t(J," should be set to "),er=a(J,"CODE",{});var Ju=s(er);dl=t(Ju,"True"),Ju.forEach(n),il=t(J,` and a configuration object should be provided
as `),or=a(J,"CODE",{});var Yu=s(or);ll=t(Yu,"config"),Yu.forEach(n),cl=t(J,` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),J.forEach(n),an.forEach(n),pl=l(w),X=a(w,"P",{});var uo=s(X);hl=t(uo,"decoder"),tr=a(uo,"EM",{});var Zu=s(tr);ml=t(Zu,"pretrained_model_name_or_path (:obj: _str"),Zu.forEach(n),fl=t(uo,", "),nr=a(uo,"EM",{});var Ku=s(nr);ul=t(Ku,"optional"),Ku.forEach(n),gl=t(uo,", defaults to "),rr=a(uo,"EM",{});var Qu=s(rr);_l=t(Qu,"None"),Qu.forEach(n),vl=t(uo,`):
Information necessary to initiate the decoder. Can be either:`),uo.forEach(n),bl=l(w),Ce=a(w,"UL",{});var sn=s(Ce);ee=a(sn,"LI",{});var go=s(ee);El=t(go,"A string, the "),ar=a(go,"EM",{});var Xu=s(ar);yl=t(Xu,"model id"),Xu.forEach(n),kl=t(go,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),sr=a(go,"CODE",{});var eg=s(sr);Tl=t(eg,"bert-base-uncased"),eg.forEach(n),wl=t(go,`, or namespaced under
a user or organization name, like `),dr=a(go,"CODE",{});var og=s(dr);Ml=t(og,"dbmdz/bert-base-german-cased"),og.forEach(n),Dl=t(go,"."),go.forEach(n),xl=l(sn),oe=a(sn,"LI",{});var _o=s(oe);zl=t(_o,"A path to a "),ir=a(_o,"EM",{});var tg=s(ir);jl=t(tg,"directory"),tg.forEach(n),$l=t(_o,` containing model weights saved using
`),Nt=a(_o,"A",{href:!0});var ng=s(Nt);Pl=t(ng,"save_pretrained()"),ng.forEach(n),ql=t(_o,", e.g., "),lr=a(_o,"CODE",{});var rg=s(lr);Cl=t(rg,"./my_model_directory/"),rg.forEach(n),Fl=t(_o,"."),_o.forEach(n),Al=l(sn),N=a(sn,"LI",{});var Y=s(N);Il=t(Y,"A path or url to a "),cr=a(Y,"EM",{});var ag=s(cr);Sl=t(ag,"tensorflow index checkpoint file"),ag.forEach(n),Ll=t(Y," (e.g, "),pr=a(Y,"CODE",{});var sg=s(pr);Ol=t(sg,"./tf_model/model.ckpt.index"),sg.forEach(n),Nl=t(Y,`). In
this case, `),hr=a(Y,"CODE",{});var dg=s(hr);Bl=t(dg,"from_tf"),dg.forEach(n),Ul=t(Y," should be set to "),mr=a(Y,"CODE",{});var ig=s(mr);Wl=t(ig,"True"),ig.forEach(n),Vl=t(Y,` and a configuration object should be provided
as `),fr=a(Y,"CODE",{});var lg=s(fr);Rl=t(lg,"config"),lg.forEach(n),Gl=t(Y,` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),Y.forEach(n),sn.forEach(n),Hl=l(w),Fe=a(w,"P",{});var dn=s(Fe);Jl=t(dn,"model"),ur=a(dn,"EM",{});var cg=s(ur);Yl=t(cg,"args (remaining positional arguments, _optional"),cg.forEach(n),Zl=t(dn,`):
All remaining positional arguments will be passed to the underlying model\u2019s `),gr=a(dn,"CODE",{});var pg=s(gr);Kl=t(pg,"__init__"),pg.forEach(n),Ql=t(dn," method."),dn.forEach(n),Xl=l(w),Ae=a(w,"P",{});var ln=s(Ae);ec=t(ln,"kwargs (remaining dictionary of keyword arguments, "),_r=a(ln,"EM",{});var hg=s(_r);oc=t(hg,"optional"),hg.forEach(n),tc=t(ln,`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),vr=a(ln,"CODE",{});var mg=s(vr);nc=t(mg,"output_attentions=True"),mg.forEach(n),rc=t(ln,")."),ln.forEach(n),ac=l(w),Ie=a(w,"UL",{});var cn=s(Ie);br=a(cn,"LI",{});var fg=s(br);sc=t(fg,"To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),fg.forEach(n),dc=l(cn),Er=a(cn,"LI",{});var ug=s(Er);ic=t(ug,"To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),ug.forEach(n),lc=l(cn),yr=a(cn,"LI",{});var gg=s(yr);cc=t(gg,"To update the parent model configuration, do not use a prefix for each configuration parameter."),gg.forEach(n),cn.forEach(n),pc=l(w),Zo=a(w,"P",{});var ws=s(Zo);hc=t(ws,"Behaves differently depending on whether a "),kr=a(ws,"CODE",{});var _g=s(kr);mc=t(_g,"config"),_g.forEach(n),fc=t(ws," is provided or automatically loaded."),ws.forEach(n),uc=l(w),Tr=a(w,"P",{});var vg=s(Tr);gc=t(vg,"Example:"),vg.forEach(n),_c=l(w),b(Ko.$$.fragment,w),w.forEach(n),A.forEach(n),is=l(d),Se=a(d,"H2",{class:!0});var Ms=s(Se);lo=a(Ms,"A",{id:!0,class:!0,href:!0});var bg=s(lo);wr=a(bg,"SPAN",{});var Eg=s(wr);b(Qo.$$.fragment,Eg),Eg.forEach(n),bg.forEach(n),vc=l(Ms),Mr=a(Ms,"SPAN",{});var yg=s(Mr);bc=t(yg,"TFEncoderDecoderModel"),yg.forEach(n),Ms.forEach(n),ls=l(d),P=a(d,"DIV",{class:!0});var I=s(P);b(Xo.$$.fragment,I),Ec=l(I),Le=a(I,"P",{});var pn=s(Le);yc=t(pn,`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),Dr=a(pn,"CODE",{});var kg=s(Dr);kc=t(kg,"from_pretrained()"),kg.forEach(n),Tc=t(pn,` function and the decoder is loaded via
`),xr=a(pn,"CODE",{});var Tg=s(xr);wc=t(Tg,"from_pretrained()"),Tg.forEach(n),Mc=t(pn,` function. Cross-attention layers are automatically
added to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),pn.forEach(n),Dc=l(I),et=a(I,"P",{});var Ds=s(et);xc=t(Ds,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),ot=a(Ds,"A",{href:!0,rel:!0});var wg=s(ot);zc=t(wg,"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),wg.forEach(n),jc=t(Ds,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Ds.forEach(n),$c=l(I),zr=a(I,"P",{});var Mg=s(zr);Pc=t(Mg,`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),Mg.forEach(n),qc=l(I),tt=a(I,"P",{});var xs=s(tt);Cc=t(xs,"This model inherits from "),Bt=a(xs,"A",{href:!0});var Dg=s(Bt);Fc=t(Dg,"TFPreTrainedModel"),Dg.forEach(n),Ac=t(xs,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),xs.forEach(n),Ic=l(I),nt=a(I,"P",{});var zs=s(nt);Sc=t(zs,"This model is also a "),rt=a(zs,"A",{href:!0,rel:!0});var xg=s(rt);Lc=t(xg,"tf.keras.Model"),xg.forEach(n),Oc=t(zs,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),zs.forEach(n),Nc=l(I),ge=a(I,"P",{});var Dt=s(ge);jr=a(Dt,"CODE",{});var zg=s(jr);Bc=t(zg,"TFEncoderDecoder"),zg.forEach(n),Uc=t(Dt,` is a generic model class that will be instantiated as a transformer
architecture with one of the base model classes of the library as encoder and another one as decoder when created
with the :meth`),$r=a(Dt,"EM",{});var jg=s($r);Wc=t(jg,"~transformers.TFAutoModel.from_pretrained"),jg.forEach(n),Vc=t(Dt,` class method for the encoder and
:meth`),Pr=a(Dt,"EM",{});var $g=s(Pr);Rc=t($g,"~transformers.TFAutoModelForCausalLM.from_pretrained"),$g.forEach(n),Gc=t(Dt," class method for the decoder."),Dt.forEach(n),Hc=l(I),W=a(I,"DIV",{class:!0});var be=s(W);b(at.$$.fragment,be),Jc=l(be),Oe=a(be,"P",{});var hn=s(Oe);Yc=t(hn,"The "),Ut=a(hn,"A",{href:!0});var Pg=s(Ut);Zc=t(Pg,"TFEncoderDecoderModel"),Pg.forEach(n),Kc=t(hn," forward method, overrides the "),qr=a(hn,"CODE",{});var qg=s(qr);Qc=t(qg,"__call__"),qg.forEach(n),Xc=t(hn," special method."),hn.forEach(n),ep=l(be),b(co.$$.fragment,be),op=l(be),Cr=a(be,"P",{});var Cg=s(Cr);tp=t(Cg,"Examples:"),Cg.forEach(n),np=l(be),b(st.$$.fragment,be),be.forEach(n),rp=l(I),g=a(I,"DIV",{class:!0});var x=s(g);b(dt.$$.fragment,x),ap=l(x),Fr=a(x,"P",{});var Fg=s(Fr);sp=t(Fg,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Fg.forEach(n),dp=l(x),Ne=a(x,"P",{});var mn=s(Ne);ip=t(mn,`Params:
encoder`),Ar=a(mn,"EM",{});var Ag=s(Ar);lp=t(Ag,"pretrained_model_name_or_path (:obj: _str"),Ag.forEach(n),cp=t(mn,", "),Ir=a(mn,"EM",{});var Ig=s(Ir);pp=t(Ig,"optional"),Ig.forEach(n),hp=t(mn,`):
Information necessary to initiate the encoder. Can be either:`),mn.forEach(n),mp=l(x),Be=a(x,"UL",{});var fn=s(Be);te=a(fn,"LI",{});var vo=s(te);fp=t(vo,"A string, the "),Sr=a(vo,"EM",{});var Sg=s(Sr);up=t(Sg,"model id"),Sg.forEach(n),gp=t(vo,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Lr=a(vo,"CODE",{});var Lg=s(Lr);_p=t(Lg,"bert-base-uncased"),Lg.forEach(n),vp=t(vo,`, or namespaced under
a user or organization name, like `),Or=a(vo,"CODE",{});var Og=s(Or);bp=t(Og,"dbmdz/bert-base-german-cased"),Og.forEach(n),Ep=t(vo,"."),vo.forEach(n),yp=l(fn),ne=a(fn,"LI",{});var bo=s(ne);kp=t(bo,"A path to a "),Nr=a(bo,"EM",{});var Ng=s(Nr);Tp=t(Ng,"directory"),Ng.forEach(n),wp=t(bo,` containing model weights saved using
`),Wt=a(bo,"A",{href:!0});var Bg=s(Wt);Mp=t(Bg,"save_pretrained()"),Bg.forEach(n),Dp=t(bo,", e.g., "),Br=a(bo,"CODE",{});var Ug=s(Br);xp=t(Ug,"./my_model_directory/"),Ug.forEach(n),zp=t(bo,"."),bo.forEach(n),jp=l(fn),G=a(fn,"LI",{});var Ee=s(G);$p=t(Ee,"A path or url to a "),Ur=a(Ee,"EM",{});var Wg=s(Ur);Pp=t(Wg,"pytorch index checkpoint file"),Wg.forEach(n),qp=t(Ee," (e.g, "),Wr=a(Ee,"CODE",{});var Vg=s(Wr);Cp=t(Vg,"./pt_model/"),Vg.forEach(n),Fp=t(Ee,`). In this case,
`),Vr=a(Ee,"CODE",{});var Rg=s(Vr);Ap=t(Rg,"encoder_from_pt"),Rg.forEach(n),Ip=t(Ee," should be set to "),Rr=a(Ee,"CODE",{});var Gg=s(Rr);Sp=t(Gg,"True"),Gg.forEach(n),Lp=t(Ee,"."),Ee.forEach(n),fn.forEach(n),Op=l(x),re=a(x,"P",{});var Eo=s(re);Np=t(Eo,"decoder"),Gr=a(Eo,"EM",{});var Hg=s(Gr);Bp=t(Hg,"pretrained_model_name_or_path (:obj: _str"),Hg.forEach(n),Up=t(Eo,", "),Hr=a(Eo,"EM",{});var Jg=s(Hr);Wp=t(Jg,"optional"),Jg.forEach(n),Vp=t(Eo,", defaults to "),Jr=a(Eo,"EM",{});var Yg=s(Jr);Rp=t(Yg,"None"),Yg.forEach(n),Gp=t(Eo,`):
Information necessary to initiate the decoder. Can be either:`),Eo.forEach(n),Hp=l(x),Ue=a(x,"UL",{});var un=s(Ue);ae=a(un,"LI",{});var yo=s(ae);Jp=t(yo,"A string, the "),Yr=a(yo,"EM",{});var Zg=s(Yr);Yp=t(Zg,"model id"),Zg.forEach(n),Zp=t(yo,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Zr=a(yo,"CODE",{});var Kg=s(Zr);Kp=t(Kg,"bert-base-uncased"),Kg.forEach(n),Qp=t(yo,`, or namespaced under
a user or organization name, like `),Kr=a(yo,"CODE",{});var Qg=s(Kr);Xp=t(Qg,"dbmdz/bert-base-german-cased"),Qg.forEach(n),eh=t(yo,"."),yo.forEach(n),oh=l(un),se=a(un,"LI",{});var ko=s(se);th=t(ko,"A path to a "),Qr=a(ko,"EM",{});var Xg=s(Qr);nh=t(Xg,"directory"),Xg.forEach(n),rh=t(ko,` containing model weights saved using
`),Vt=a(ko,"A",{href:!0});var e_=s(Vt);ah=t(e_,"save_pretrained()"),e_.forEach(n),sh=t(ko,", e.g., "),Xr=a(ko,"CODE",{});var o_=s(Xr);dh=t(o_,"./my_model_directory/"),o_.forEach(n),ih=t(ko,"."),ko.forEach(n),lh=l(un),H=a(un,"LI",{});var ye=s(H);ch=t(ye,"A path or url to a "),ea=a(ye,"EM",{});var t_=s(ea);ph=t(t_,"pytorch checkpoint file"),t_.forEach(n),hh=t(ye," (e.g, "),oa=a(ye,"CODE",{});var n_=s(oa);mh=t(n_,"./pt_model/"),n_.forEach(n),fh=t(ye,`). In this case,
`),ta=a(ye,"CODE",{});var r_=s(ta);uh=t(r_,"decoder_from_pt"),r_.forEach(n),gh=t(ye," should be set to "),na=a(ye,"CODE",{});var a_=s(na);_h=t(a_,"True"),a_.forEach(n),vh=t(ye,"."),ye.forEach(n),un.forEach(n),bh=l(x),We=a(x,"P",{});var gn=s(We);Eh=t(gn,"model"),ra=a(gn,"EM",{});var s_=s(ra);yh=t(s_,"args (remaining positional arguments, _optional"),s_.forEach(n),kh=t(gn,`):
All remaning positional arguments will be passed to the underlying model\u2019s `),aa=a(gn,"CODE",{});var d_=s(aa);Th=t(d_,"__init__"),d_.forEach(n),wh=t(gn," method."),gn.forEach(n),Mh=l(x),Ve=a(x,"P",{});var _n=s(Ve);Dh=t(_n,"kwargs (remaining dictionary of keyword arguments, "),sa=a(_n,"EM",{});var i_=s(sa);xh=t(i_,"optional"),i_.forEach(n),zh=t(_n,`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),da=a(_n,"CODE",{});var l_=s(da);jh=t(l_,"output_attentions=True"),l_.forEach(n),$h=t(_n,")."),_n.forEach(n),Ph=l(x),Re=a(x,"UL",{});var vn=s(Re);ia=a(vn,"LI",{});var c_=s(ia);qh=t(c_,"To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),c_.forEach(n),Ch=l(vn),la=a(vn,"LI",{});var p_=s(la);Fh=t(p_,"To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),p_.forEach(n),Ah=l(vn),ca=a(vn,"LI",{});var h_=s(ca);Ih=t(h_,"To update the parent model configuration, do not use a prefix for each configuration parameter."),h_.forEach(n),vn.forEach(n),Sh=l(x),it=a(x,"P",{});var js=s(it);Lh=t(js,"Behaves differently depending on whether a "),pa=a(js,"CODE",{});var m_=s(pa);Oh=t(m_,"config"),m_.forEach(n),Nh=t(js," is provided or automatically loaded."),js.forEach(n),Bh=l(x),ha=a(x,"P",{});var f_=s(ha);Uh=t(f_,"Example:"),f_.forEach(n),Wh=l(x),b(lt.$$.fragment,x),x.forEach(n),I.forEach(n),cs=l(d),Ge=a(d,"H2",{class:!0});var $s=s(Ge);po=a($s,"A",{id:!0,class:!0,href:!0});var u_=s(po);ma=a(u_,"SPAN",{});var g_=s(ma);b(ct.$$.fragment,g_),g_.forEach(n),u_.forEach(n),Vh=l($s),fa=a($s,"SPAN",{});var __=s(fa);Rh=t(__,"FlaxEncoderDecoderModel"),__.forEach(n),$s.forEach(n),ps=l(d),q=a(d,"DIV",{class:!0});var S=s(q);b(pt.$$.fragment,S),Gh=l(S),He=a(S,"P",{});var bn=s(He);Hh=t(bn,`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),ua=a(bn,"CODE",{});var v_=s(ua);Jh=t(v_,"from_pretrained()"),v_.forEach(n),Yh=t(bn,` function and the decoder is loaded via
`),ga=a(bn,"CODE",{});var b_=s(ga);Zh=t(b_,"from_pretrained()"),b_.forEach(n),Kh=t(bn,` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),bn.forEach(n),Qh=l(S),ht=a(S,"P",{});var Ps=s(ht);Xh=t(Ps,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),mt=a(Ps,"A",{href:!0,rel:!0});var E_=s(mt);em=t(E_,"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),E_.forEach(n),om=t(Ps,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Ps.forEach(n),tm=l(S),_a=a(S,"P",{});var y_=s(_a);nm=t(y_,`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),y_.forEach(n),rm=l(S),ft=a(S,"P",{});var qs=s(ft);am=t(qs,"This model inherits from "),Rt=a(qs,"A",{href:!0});var k_=s(Rt);sm=t(k_,"FlaxPreTrainedModel"),k_.forEach(n),dm=t(qs,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),qs.forEach(n),im=l(S),ut=a(S,"P",{});var Cs=s(ut);lm=t(Cs,"This model is also a Flax Linen "),gt=a(Cs,"A",{href:!0,rel:!0});var T_=s(gt);cm=t(T_,"flax.nn.Module"),T_.forEach(n),pm=t(Cs,` subclass. Use it as a regular Flax
Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Cs.forEach(n),hm=l(S),_e=a(S,"P",{});var xt=s(_e);Gt=a(xt,"A",{href:!0});var w_=s(Gt);mm=t(w_,"FlaxEncoderDecoderModel"),w_.forEach(n),fm=t(xt,` is a generic model class that will be instantiated as a transformer
architecture with the module (flax.nn.Module) of one of the base model classes of the library as encoder module and
another one as decoder module when created with the :meth`),va=a(xt,"EM",{});var M_=s(va);um=t(M_,"~transformers.FlaxAutoModel.from_pretrained"),M_.forEach(n),gm=t(xt,` class method
for the encoder and :meth`),ba=a(xt,"EM",{});var D_=s(ba);_m=t(D_,"~transformers.FlaxAutoModelForCausalLM.from_pretrained"),D_.forEach(n),vm=t(xt," class method for the decoder."),xt.forEach(n),bm=l(S),V=a(S,"DIV",{class:!0});var ke=s(V);b(_t.$$.fragment,ke),Em=l(ke),Je=a(ke,"P",{});var En=s(Je);ym=t(En,"The "),Ht=a(En,"A",{href:!0});var x_=s(Ht);km=t(x_,"FlaxEncoderDecoderModel"),x_.forEach(n),Tm=t(En," forward method, overrides the "),Ea=a(En,"CODE",{});var z_=s(Ea);wm=t(z_,"__call__"),z_.forEach(n),Mm=t(En," special method."),En.forEach(n),Dm=l(ke),b(ho.$$.fragment,ke),xm=l(ke),ya=a(ke,"P",{});var j_=s(ya);zm=t(j_,"Examples:"),j_.forEach(n),jm=l(ke),b(vt.$$.fragment,ke),ke.forEach(n),$m=l(S),_=a(S,"DIV",{class:!0});var z=s(_);b(bt.$$.fragment,z),Pm=l(z),ka=a(z,"P",{});var $_=s(ka);qm=t($_,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),$_.forEach(n),Cm=l(z),Ye=a(z,"P",{});var yn=s(Ye);Fm=t(yn,`Params:
encoder`),Ta=a(yn,"EM",{});var P_=s(Ta);Am=t(P_,"pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),P_.forEach(n),Im=t(yn,", "),wa=a(yn,"EM",{});var q_=s(wa);Sm=t(q_,"optional"),q_.forEach(n),Lm=t(yn,`):
Information necessary to initiate the encoder. Can be either:`),yn.forEach(n),Om=l(z),Et=a(z,"UL",{});var Fs=s(Et);de=a(Fs,"LI",{});var To=s(de);Nm=t(To,"A string, the "),Ma=a(To,"EM",{});var C_=s(Ma);Bm=t(C_,"model id"),C_.forEach(n),Um=t(To,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Da=a(To,"CODE",{});var F_=s(Da);Wm=t(F_,"bert-base-uncased"),F_.forEach(n),Vm=t(To,`, or namespaced under
a user or organization name, like `),xa=a(To,"CODE",{});var A_=s(xa);Rm=t(A_,"dbmdz/bert-base-german-cased"),A_.forEach(n),Gm=t(To,"."),To.forEach(n),Hm=l(Fs),ie=a(Fs,"LI",{});var wo=s(ie);Jm=t(wo,"A path to a "),za=a(wo,"EM",{});var I_=s(za);Ym=t(I_,"directory"),I_.forEach(n),Zm=t(wo,` containing model weights saved using
`),Jt=a(wo,"A",{href:!0});var S_=s(Jt);Km=t(S_,"save_pretrained()"),S_.forEach(n),Qm=t(wo,", e.g., "),ja=a(wo,"CODE",{});var L_=s(ja);Xm=t(L_,"./my_model_directory/"),L_.forEach(n),ef=t(wo,"."),wo.forEach(n),Fs.forEach(n),of=l(z),le=a(z,"P",{});var Mo=s(le);tf=t(Mo,"decoder"),$a=a(Mo,"EM",{});var O_=s($a);nf=t(O_,"pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),O_.forEach(n),rf=t(Mo,", "),Pa=a(Mo,"EM",{});var N_=s(Pa);af=t(N_,"optional"),N_.forEach(n),sf=t(Mo,", defaults to "),qa=a(Mo,"EM",{});var B_=s(qa);df=t(B_,"None"),B_.forEach(n),lf=t(Mo,`):
Information necessary to initiate the decoder. Can be either:`),Mo.forEach(n),cf=l(z),yt=a(z,"UL",{});var As=s(yt);ce=a(As,"LI",{});var Do=s(ce);pf=t(Do,"A string, the "),Ca=a(Do,"EM",{});var U_=s(Ca);hf=t(U_,"model id"),U_.forEach(n),mf=t(Do,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Fa=a(Do,"CODE",{});var W_=s(Fa);ff=t(W_,"bert-base-uncased"),W_.forEach(n),uf=t(Do,`, or namespaced under
a user or organization name, like `),Aa=a(Do,"CODE",{});var V_=s(Aa);gf=t(V_,"dbmdz/bert-base-german-cased"),V_.forEach(n),_f=t(Do,"."),Do.forEach(n),vf=l(As),pe=a(As,"LI",{});var xo=s(pe);bf=t(xo,"A path to a "),Ia=a(xo,"EM",{});var R_=s(Ia);Ef=t(R_,"directory"),R_.forEach(n),yf=t(xo,` containing model weights saved using
`),Yt=a(xo,"A",{href:!0});var G_=s(Yt);kf=t(G_,"save_pretrained()"),G_.forEach(n),Tf=t(xo,", e.g., "),Sa=a(xo,"CODE",{});var H_=s(Sa);wf=t(H_,"./my_model_directory/"),H_.forEach(n),Mf=t(xo,"."),xo.forEach(n),As.forEach(n),Df=l(z),Ze=a(z,"P",{});var kn=s(Ze);xf=t(kn,"model"),La=a(kn,"EM",{});var J_=s(La);zf=t(J_,"args (remaining positional arguments, _optional"),J_.forEach(n),jf=t(kn,`):
All remaning positional arguments will be passed to the underlying model\u2019s `),Oa=a(kn,"CODE",{});var Y_=s(Oa);$f=t(Y_,"__init__"),Y_.forEach(n),Pf=t(kn," method."),kn.forEach(n),qf=l(z),Ke=a(z,"P",{});var Tn=s(Ke);Cf=t(Tn,"kwargs (remaining dictionary of keyword arguments, "),Na=a(Tn,"EM",{});var Z_=s(Na);Ff=t(Z_,"optional"),Z_.forEach(n),Af=t(Tn,`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),Ba=a(Tn,"CODE",{});var K_=s(Ba);If=t(K_,"output_attentions=True"),K_.forEach(n),Sf=t(Tn,")."),Tn.forEach(n),Lf=l(z),Qe=a(z,"UL",{});var wn=s(Qe);Ua=a(wn,"LI",{});var Q_=s(Ua);Of=t(Q_,"To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),Q_.forEach(n),Nf=l(wn),Wa=a(wn,"LI",{});var X_=s(Wa);Bf=t(X_,"To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),X_.forEach(n),Uf=l(wn),Va=a(wn,"LI",{});var ev=s(Va);Wf=t(ev,"To update the parent model configuration, do not use a prefix for each configuration parameter."),ev.forEach(n),wn.forEach(n),Vf=l(z),kt=a(z,"P",{});var Is=s(kt);Rf=t(Is,"Behaves differently depending on whether a "),Ra=a(Is,"CODE",{});var ov=s(Ra);Gf=t(ov,"config"),ov.forEach(n),Hf=t(Is," is provided or automatically loaded."),Is.forEach(n),Jf=l(z),Ga=a(z,"P",{});var tv=s(Ga);Yf=t(tv,"Example:"),tv.forEach(n),Zf=l(z),b(Tt.$$.fragment,z),z.forEach(n),S.forEach(n),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(pv)),c(j,"id","encoder-decoder-models"),c(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(j,"href","#encoder-decoder-models"),c(f,"class","relative group"),c(zt,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c(jo,"href","https://arxiv.org/abs/1907.12461"),c(jo,"rel","nofollow"),c(jt,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c($t,"href","/docs/transformers/v4.14.1/en/model_doc/bert#transformers.BertModel"),c($o,"href","https://arxiv.org/abs/1908.08345"),c($o,"rel","nofollow"),c(qo,"href","https://github.com/thomwolf"),c(qo,"rel","nofollow"),c(Co,"href","https://github.com/ydshieh"),c(Co,"rel","nofollow"),c(to,"id","transformers.EncoderDecoderConfig"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.EncoderDecoderConfig"),c(we,"class","relative group"),c(Pt,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"),c(qt,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c(Ct,"href","/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ft,"href","/docs/transformers/v4.14.1/en/main_classes/configuration#transformers.PretrainedConfig"),c(At,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"),c(ro,"class","docstring"),c(ao,"class","docstring"),c(C,"class","docstring"),c(so,"id","transformers.EncoderDecoderModel"),c(so,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(so,"href","#transformers.EncoderDecoderModel"),c(xe,"class","relative group"),c(Wo,"href","https://arxiv.org/abs/1907.12461"),c(Wo,"rel","nofollow"),c(It,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel"),c(Go,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Go,"rel","nofollow"),c(St,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c(Lt,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c(U,"class","docstring"),c(Ot,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),c(Nt,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),c(u,"class","docstring"),c($,"class","docstring"),c(lo,"id","transformers.TFEncoderDecoderModel"),c(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lo,"href","#transformers.TFEncoderDecoderModel"),c(Se,"class","relative group"),c(ot,"href","https://arxiv.org/abs/1907.12461"),c(ot,"rel","nofollow"),c(Bt,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.TFPreTrainedModel"),c(rt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(rt,"rel","nofollow"),c(Ut,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.TFEncoderDecoderModel"),c(W,"class","docstring"),c(Wt,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained"),c(Vt,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained"),c(g,"class","docstring"),c(P,"class","docstring"),c(po,"id","transformers.FlaxEncoderDecoderModel"),c(po,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(po,"href","#transformers.FlaxEncoderDecoderModel"),c(Ge,"class","relative group"),c(mt,"href","https://arxiv.org/abs/1907.12461"),c(mt,"rel","nofollow"),c(Rt,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(gt,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),c(gt,"rel","nofollow"),c(Gt,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.FlaxEncoderDecoderModel"),c(Ht,"href","/docs/transformers/v4.14.1/en/model_doc/encoderdecoder#transformers.FlaxEncoderDecoderModel"),c(V,"class","docstring"),c(Jt,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained"),c(Yt,"href","/docs/transformers/v4.14.1/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained"),c(_,"class","docstring"),c(q,"class","docstring")},m(d,p){e(document.head,h),m(d,F,p),m(d,f,p),e(f,j),e(j,L),E(M,L,null),e(f,D),e(f,B),e(B,Ss),m(d,Ka,p),m(d,Xe,p),e(Xe,Ls),e(Xe,zt),e(zt,Os),e(Xe,Ns),m(d,Qa,p),m(d,eo,p),e(eo,Bs),e(eo,jo),e(jo,Us),e(eo,Ws),m(d,Xa,p),m(d,oo,p),e(oo,Vs),e(oo,jt),e(jt,Rs),e(oo,Gs),m(d,es,p),m(d,he,p),e(he,Hs),e(he,$t),e($t,Js),e(he,Ys),e(he,$o),e($o,Zs),e(he,Ks),m(d,os,p),m(d,me,p),e(me,Qs),e(me,Mn),e(Mn,Xs),e(me,ed),e(me,Dn),e(Dn,od),e(me,td),m(d,ts,p),E(Po,d,p),m(d,ns,p),m(d,fe,p),e(fe,nd),e(fe,qo),e(qo,rd),e(fe,ad),e(fe,Co),e(Co,sd),e(fe,dd),m(d,rs,p),m(d,we,p),e(we,to),e(to,xn),E(Fo,xn,null),e(we,id),e(we,zn),e(zn,ld),m(d,as,p),m(d,C,p),E(Ao,C,null),e(C,cd),e(C,no),e(no,Pt),e(Pt,pd),e(no,hd),e(no,qt),e(qt,md),e(no,fd),e(C,ud),e(C,Me),e(Me,gd),e(Me,Ct),e(Ct,_d),e(Me,vd),e(Me,Ft),e(Ft,bd),e(Me,Ed),e(C,yd),e(C,jn),e(jn,kd),e(C,Td),E(Io,C,null),e(C,wd),e(C,ro),E(So,ro,null),e(ro,Md),e(ro,Lo),e(Lo,Dd),e(Lo,At),e(At,xd),e(Lo,zd),e(C,jd),e(C,ao),E(Oo,ao,null),e(ao,$d),e(ao,De),e(De,Pd),e(De,$n),e($n,qd),e(De,Cd),e(De,Pn),e(Pn,Fd),e(De,Ad),m(d,ss,p),m(d,xe,p),e(xe,so),e(so,qn),E(No,qn,null),e(xe,Id),e(xe,Cn),e(Cn,Sd),m(d,ds,p),m(d,$,p),E(Bo,$,null),e($,Ld),e($,ze),e(ze,Od),e(ze,Fn),e(Fn,Nd),e(ze,Bd),e(ze,An),e(An,Ud),e(ze,Wd),e($,Vd),e($,Uo),e(Uo,Rd),e(Uo,Wo),e(Wo,Gd),e(Uo,Hd),e($,Jd),e($,In),e(In,Yd),e($,Zd),e($,Vo),e(Vo,Kd),e(Vo,It),e(It,Qd),e(Vo,Xd),e($,ei),e($,Ro),e(Ro,oi),e(Ro,Go),e(Go,ti),e(Ro,ni),e($,ri),e($,ue),e(ue,St),e(St,ai),e(ue,si),e(ue,Sn),e(Sn,di),e(ue,ii),e(ue,Ln),e(Ln,li),e(ue,ci),e($,pi),e($,U),E(Ho,U,null),e(U,hi),e(U,je),e(je,mi),e(je,Lt),e(Lt,fi),e(je,ui),e(je,On),e(On,gi),e(je,_i),e(U,vi),E(io,U,null),e(U,bi),e(U,Nn),e(Nn,Ei),e(U,yi),E(Jo,U,null),e($,ki),e($,u),E(Yo,u,null),e(u,Ti),e(u,Bn),e(Bn,wi),e(u,Mi),e(u,$e),e($e,Di),e($e,Un),e(Un,xi),e($e,zi),e($e,Wn),e(Wn,ji),e($e,$i),e(u,Pi),e(u,Pe),e(Pe,qi),e(Pe,Vn),e(Vn,Ci),e(Pe,Fi),e(Pe,Rn),e(Rn,Ai),e(Pe,Ii),e(u,Si),e(u,qe),e(qe,K),e(K,Li),e(K,Gn),e(Gn,Oi),e(K,Ni),e(K,Hn),e(Hn,Bi),e(K,Ui),e(K,Jn),e(Jn,Wi),e(K,Vi),e(qe,Ri),e(qe,Q),e(Q,Gi),e(Q,Yn),e(Yn,Hi),e(Q,Ji),e(Q,Ot),e(Ot,Yi),e(Q,Zi),e(Q,Zn),e(Zn,Ki),e(Q,Qi),e(qe,Xi),e(qe,O),e(O,el),e(O,Kn),e(Kn,ol),e(O,tl),e(O,Qn),e(Qn,nl),e(O,rl),e(O,Xn),e(Xn,al),e(O,sl),e(O,er),e(er,dl),e(O,il),e(O,or),e(or,ll),e(O,cl),e(u,pl),e(u,X),e(X,hl),e(X,tr),e(tr,ml),e(X,fl),e(X,nr),e(nr,ul),e(X,gl),e(X,rr),e(rr,_l),e(X,vl),e(u,bl),e(u,Ce),e(Ce,ee),e(ee,El),e(ee,ar),e(ar,yl),e(ee,kl),e(ee,sr),e(sr,Tl),e(ee,wl),e(ee,dr),e(dr,Ml),e(ee,Dl),e(Ce,xl),e(Ce,oe),e(oe,zl),e(oe,ir),e(ir,jl),e(oe,$l),e(oe,Nt),e(Nt,Pl),e(oe,ql),e(oe,lr),e(lr,Cl),e(oe,Fl),e(Ce,Al),e(Ce,N),e(N,Il),e(N,cr),e(cr,Sl),e(N,Ll),e(N,pr),e(pr,Ol),e(N,Nl),e(N,hr),e(hr,Bl),e(N,Ul),e(N,mr),e(mr,Wl),e(N,Vl),e(N,fr),e(fr,Rl),e(N,Gl),e(u,Hl),e(u,Fe),e(Fe,Jl),e(Fe,ur),e(ur,Yl),e(Fe,Zl),e(Fe,gr),e(gr,Kl),e(Fe,Ql),e(u,Xl),e(u,Ae),e(Ae,ec),e(Ae,_r),e(_r,oc),e(Ae,tc),e(Ae,vr),e(vr,nc),e(Ae,rc),e(u,ac),e(u,Ie),e(Ie,br),e(br,sc),e(Ie,dc),e(Ie,Er),e(Er,ic),e(Ie,lc),e(Ie,yr),e(yr,cc),e(u,pc),e(u,Zo),e(Zo,hc),e(Zo,kr),e(kr,mc),e(Zo,fc),e(u,uc),e(u,Tr),e(Tr,gc),e(u,_c),E(Ko,u,null),m(d,is,p),m(d,Se,p),e(Se,lo),e(lo,wr),E(Qo,wr,null),e(Se,vc),e(Se,Mr),e(Mr,bc),m(d,ls,p),m(d,P,p),E(Xo,P,null),e(P,Ec),e(P,Le),e(Le,yc),e(Le,Dr),e(Dr,kc),e(Le,Tc),e(Le,xr),e(xr,wc),e(Le,Mc),e(P,Dc),e(P,et),e(et,xc),e(et,ot),e(ot,zc),e(et,jc),e(P,$c),e(P,zr),e(zr,Pc),e(P,qc),e(P,tt),e(tt,Cc),e(tt,Bt),e(Bt,Fc),e(tt,Ac),e(P,Ic),e(P,nt),e(nt,Sc),e(nt,rt),e(rt,Lc),e(nt,Oc),e(P,Nc),e(P,ge),e(ge,jr),e(jr,Bc),e(ge,Uc),e(ge,$r),e($r,Wc),e(ge,Vc),e(ge,Pr),e(Pr,Rc),e(ge,Gc),e(P,Hc),e(P,W),E(at,W,null),e(W,Jc),e(W,Oe),e(Oe,Yc),e(Oe,Ut),e(Ut,Zc),e(Oe,Kc),e(Oe,qr),e(qr,Qc),e(Oe,Xc),e(W,ep),E(co,W,null),e(W,op),e(W,Cr),e(Cr,tp),e(W,np),E(st,W,null),e(P,rp),e(P,g),E(dt,g,null),e(g,ap),e(g,Fr),e(Fr,sp),e(g,dp),e(g,Ne),e(Ne,ip),e(Ne,Ar),e(Ar,lp),e(Ne,cp),e(Ne,Ir),e(Ir,pp),e(Ne,hp),e(g,mp),e(g,Be),e(Be,te),e(te,fp),e(te,Sr),e(Sr,up),e(te,gp),e(te,Lr),e(Lr,_p),e(te,vp),e(te,Or),e(Or,bp),e(te,Ep),e(Be,yp),e(Be,ne),e(ne,kp),e(ne,Nr),e(Nr,Tp),e(ne,wp),e(ne,Wt),e(Wt,Mp),e(ne,Dp),e(ne,Br),e(Br,xp),e(ne,zp),e(Be,jp),e(Be,G),e(G,$p),e(G,Ur),e(Ur,Pp),e(G,qp),e(G,Wr),e(Wr,Cp),e(G,Fp),e(G,Vr),e(Vr,Ap),e(G,Ip),e(G,Rr),e(Rr,Sp),e(G,Lp),e(g,Op),e(g,re),e(re,Np),e(re,Gr),e(Gr,Bp),e(re,Up),e(re,Hr),e(Hr,Wp),e(re,Vp),e(re,Jr),e(Jr,Rp),e(re,Gp),e(g,Hp),e(g,Ue),e(Ue,ae),e(ae,Jp),e(ae,Yr),e(Yr,Yp),e(ae,Zp),e(ae,Zr),e(Zr,Kp),e(ae,Qp),e(ae,Kr),e(Kr,Xp),e(ae,eh),e(Ue,oh),e(Ue,se),e(se,th),e(se,Qr),e(Qr,nh),e(se,rh),e(se,Vt),e(Vt,ah),e(se,sh),e(se,Xr),e(Xr,dh),e(se,ih),e(Ue,lh),e(Ue,H),e(H,ch),e(H,ea),e(ea,ph),e(H,hh),e(H,oa),e(oa,mh),e(H,fh),e(H,ta),e(ta,uh),e(H,gh),e(H,na),e(na,_h),e(H,vh),e(g,bh),e(g,We),e(We,Eh),e(We,ra),e(ra,yh),e(We,kh),e(We,aa),e(aa,Th),e(We,wh),e(g,Mh),e(g,Ve),e(Ve,Dh),e(Ve,sa),e(sa,xh),e(Ve,zh),e(Ve,da),e(da,jh),e(Ve,$h),e(g,Ph),e(g,Re),e(Re,ia),e(ia,qh),e(Re,Ch),e(Re,la),e(la,Fh),e(Re,Ah),e(Re,ca),e(ca,Ih),e(g,Sh),e(g,it),e(it,Lh),e(it,pa),e(pa,Oh),e(it,Nh),e(g,Bh),e(g,ha),e(ha,Uh),e(g,Wh),E(lt,g,null),m(d,cs,p),m(d,Ge,p),e(Ge,po),e(po,ma),E(ct,ma,null),e(Ge,Vh),e(Ge,fa),e(fa,Rh),m(d,ps,p),m(d,q,p),E(pt,q,null),e(q,Gh),e(q,He),e(He,Hh),e(He,ua),e(ua,Jh),e(He,Yh),e(He,ga),e(ga,Zh),e(He,Kh),e(q,Qh),e(q,ht),e(ht,Xh),e(ht,mt),e(mt,em),e(ht,om),e(q,tm),e(q,_a),e(_a,nm),e(q,rm),e(q,ft),e(ft,am),e(ft,Rt),e(Rt,sm),e(ft,dm),e(q,im),e(q,ut),e(ut,lm),e(ut,gt),e(gt,cm),e(ut,pm),e(q,hm),e(q,_e),e(_e,Gt),e(Gt,mm),e(_e,fm),e(_e,va),e(va,um),e(_e,gm),e(_e,ba),e(ba,_m),e(_e,vm),e(q,bm),e(q,V),E(_t,V,null),e(V,Em),e(V,Je),e(Je,ym),e(Je,Ht),e(Ht,km),e(Je,Tm),e(Je,Ea),e(Ea,wm),e(Je,Mm),e(V,Dm),E(ho,V,null),e(V,xm),e(V,ya),e(ya,zm),e(V,jm),E(vt,V,null),e(q,$m),e(q,_),E(bt,_,null),e(_,Pm),e(_,ka),e(ka,qm),e(_,Cm),e(_,Ye),e(Ye,Fm),e(Ye,Ta),e(Ta,Am),e(Ye,Im),e(Ye,wa),e(wa,Sm),e(Ye,Lm),e(_,Om),e(_,Et),e(Et,de),e(de,Nm),e(de,Ma),e(Ma,Bm),e(de,Um),e(de,Da),e(Da,Wm),e(de,Vm),e(de,xa),e(xa,Rm),e(de,Gm),e(Et,Hm),e(Et,ie),e(ie,Jm),e(ie,za),e(za,Ym),e(ie,Zm),e(ie,Jt),e(Jt,Km),e(ie,Qm),e(ie,ja),e(ja,Xm),e(ie,ef),e(_,of),e(_,le),e(le,tf),e(le,$a),e($a,nf),e(le,rf),e(le,Pa),e(Pa,af),e(le,sf),e(le,qa),e(qa,df),e(le,lf),e(_,cf),e(_,yt),e(yt,ce),e(ce,pf),e(ce,Ca),e(Ca,hf),e(ce,mf),e(ce,Fa),e(Fa,ff),e(ce,uf),e(ce,Aa),e(Aa,gf),e(ce,_f),e(yt,vf),e(yt,pe),e(pe,bf),e(pe,Ia),e(Ia,Ef),e(pe,yf),e(pe,Yt),e(Yt,kf),e(pe,Tf),e(pe,Sa),e(Sa,wf),e(pe,Mf),e(_,Df),e(_,Ze),e(Ze,xf),e(Ze,La),e(La,zf),e(Ze,jf),e(Ze,Oa),e(Oa,$f),e(Ze,Pf),e(_,qf),e(_,Ke),e(Ke,Cf),e(Ke,Na),e(Na,Ff),e(Ke,Af),e(Ke,Ba),e(Ba,If),e(Ke,Sf),e(_,Lf),e(_,Qe),e(Qe,Ua),e(Ua,Of),e(Qe,Nf),e(Qe,Wa),e(Wa,Bf),e(Qe,Uf),e(Qe,Va),e(Va,Wf),e(_,Vf),e(_,kt),e(kt,Rf),e(kt,Ra),e(Ra,Gf),e(kt,Hf),e(_,Jf),e(_,Ga),e(Ga,Yf),e(_,Zf),E(Tt,_,null),hs=!0},p(d,[p]){const wt={};p&2&&(wt.$$scope={dirty:p,ctx:d}),io.$set(wt);const Ha={};p&2&&(Ha.$$scope={dirty:p,ctx:d}),co.$set(Ha);const Ja={};p&2&&(Ja.$$scope={dirty:p,ctx:d}),ho.$set(Ja)},i(d){hs||(y(M.$$.fragment,d),y(Po.$$.fragment,d),y(Fo.$$.fragment,d),y(Ao.$$.fragment,d),y(Io.$$.fragment,d),y(So.$$.fragment,d),y(Oo.$$.fragment,d),y(No.$$.fragment,d),y(Bo.$$.fragment,d),y(Ho.$$.fragment,d),y(io.$$.fragment,d),y(Jo.$$.fragment,d),y(Yo.$$.fragment,d),y(Ko.$$.fragment,d),y(Qo.$$.fragment,d),y(Xo.$$.fragment,d),y(at.$$.fragment,d),y(co.$$.fragment,d),y(st.$$.fragment,d),y(dt.$$.fragment,d),y(lt.$$.fragment,d),y(ct.$$.fragment,d),y(pt.$$.fragment,d),y(_t.$$.fragment,d),y(ho.$$.fragment,d),y(vt.$$.fragment,d),y(bt.$$.fragment,d),y(Tt.$$.fragment,d),hs=!0)},o(d){k(M.$$.fragment,d),k(Po.$$.fragment,d),k(Fo.$$.fragment,d),k(Ao.$$.fragment,d),k(Io.$$.fragment,d),k(So.$$.fragment,d),k(Oo.$$.fragment,d),k(No.$$.fragment,d),k(Bo.$$.fragment,d),k(Ho.$$.fragment,d),k(io.$$.fragment,d),k(Jo.$$.fragment,d),k(Yo.$$.fragment,d),k(Ko.$$.fragment,d),k(Qo.$$.fragment,d),k(Xo.$$.fragment,d),k(at.$$.fragment,d),k(co.$$.fragment,d),k(st.$$.fragment,d),k(dt.$$.fragment,d),k(lt.$$.fragment,d),k(ct.$$.fragment,d),k(pt.$$.fragment,d),k(_t.$$.fragment,d),k(ho.$$.fragment,d),k(vt.$$.fragment,d),k(bt.$$.fragment,d),k(Tt.$$.fragment,d),hs=!1},d(d){n(h),d&&n(F),d&&n(f),T(M),d&&n(Ka),d&&n(Xe),d&&n(Qa),d&&n(eo),d&&n(Xa),d&&n(oo),d&&n(es),d&&n(he),d&&n(os),d&&n(me),d&&n(ts),T(Po,d),d&&n(ns),d&&n(fe),d&&n(rs),d&&n(we),T(Fo),d&&n(as),d&&n(C),T(Ao),T(Io),T(So),T(Oo),d&&n(ss),d&&n(xe),T(No),d&&n(ds),d&&n($),T(Bo),T(Ho),T(io),T(Jo),T(Yo),T(Ko),d&&n(is),d&&n(Se),T(Qo),d&&n(ls),d&&n(P),T(Xo),T(at),T(co),T(st),T(dt),T(lt),d&&n(cs),d&&n(Ge),T(ct),d&&n(ps),d&&n(q),T(pt),T(_t),T(ho),T(vt),T(bt),T(Tt)}}}const pv={local:"encoder-decoder-models",sections:[{local:"transformers.EncoderDecoderConfig",title:"EncoderDecoderConfig"},{local:"transformers.EncoderDecoderModel",title:"EncoderDecoderModel"},{local:"transformers.TFEncoderDecoderModel",title:"TFEncoderDecoderModel"},{local:"transformers.FlaxEncoderDecoderModel",title:"FlaxEncoderDecoderModel"}],title:"Encoder Decoder Models"};function hv(Te,h,F){let{fw:f}=h;return Te.$$set=j=>{"fw"in j&&F(0,f=j.fw)},[f]}class bv extends nv{constructor(h){super();rv(this,h,hv,cv,av,{fw:0})}}export{bv as default,pv as metadata};
