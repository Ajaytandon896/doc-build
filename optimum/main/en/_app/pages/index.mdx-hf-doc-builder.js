import{S as Tl,i as Al,s as Ol,e as a,k as h,w as ge,t as i,M as Pl,c as o,d as r,m as p,a as l,x as we,h as s,b as n,G as e,g as d,y as _e,L as Il,q as Ee,o as ye,B as be,v as Dl}from"../chunks/vendor-hf-doc-builder.js";import{I as pr}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as ao}from"../chunks/CodeBlock-hf-doc-builder.js";function Nl(oo){let y,Nt,b,k,pt,M,fr,ft,dr,kt,$e,cr,xt,Te,ur,zt,$,x,dt,L,mr,ct,vr,Ht,Ae,gr,Ct,Oe,wr,Rt,u,z,Q,_r,Er,B,yr,br,$r,v,X,Tr,Ar,F,Or,Pr,V,Ir,Dr,Nr,g,Y,kr,xr,j,zr,Hr,J,Cr,Rr,Sr,ut,Ur,St,T,H,mt,K,qr,vt,Gr,Ut,Pe,Mr,qt,w,Lr,W,Qr,Br,Z,Xr,Fr,Gt,C,gt,A,Ie,Vr,Yr,De,jr,Jr,Ne,Kr,Wr,m,O,ke,Zr,ea,xe,ta,ra,ze,aa,oa,P,He,la,na,Ce,ia,sa,Re,ha,pa,I,Se,fa,da,Ue,ca,ua,qe,ma,va,D,Ge,ga,wa,Me,_a,Ea,Le,ya,Mt,N,R,wt,ee,ba,_t,$a,Lt,S,Ta,Et,Aa,Oa,Qt,te,Bt,Qe,Pa,Xt,U,yt,re,Be,Ia,Da,Xe,Na,ka,c,ae,Fe,oe,xa,za,Ve,bt,Ha,Ca,le,Ye,ne,Ra,Sa,je,$t,Ua,qa,ie,Je,se,Ga,Ma,Ke,Tt,La,Qa,he,We,pe,Ba,Xa,Ze,At,Fa,Va,fe,et,de,Ya,ja,tt,Ot,Ja,Ft,rt,Ka,Vt,ce,Yt,_,Wa,Pt,Za,eo,It,to,ro,jt,ue,Jt;return M=new pr({}),L=new pr({}),K=new pr({}),ee=new pr({}),te=new ao({props:{code:"python -m pip install optimum",highlighted:"python -m pip install optimum"}}),ce=new ao({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git",highlighted:"python -m pip install git+https://github.com/huggingface/optimum.git"}}),ue=new ao({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]",highlighted:'python -m pip install git+https://github.com/huggingface/optimum.git<span class="hljs-comment">#egg=optimum[onnxruntime]</span>'}}),{c(){y=a("meta"),Nt=h(),b=a("h1"),k=a("a"),pt=a("span"),ge(M.$$.fragment),fr=h(),ft=a("span"),dr=i("\u{1F917} Optimum"),kt=h(),$e=a("p"),cr=i("\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),xt=h(),Te=a("p"),ur=i(`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),zt=h(),$=a("h2"),x=a("a"),dt=a("span"),ge(L.$$.fragment),mr=h(),ct=a("span"),vr=i("Integration with Hardware Partners"),Ht=h(),Ae=a("p"),gr=i("\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),Ct=h(),Oe=a("p"),wr=i("To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),Rt=h(),u=a("ul"),z=a("li"),Q=a("a"),_r=i("Graphcore IPUs"),Er=i(" - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),B=a("a"),yr=i("More information here"),br=i("."),$r=h(),v=a("li"),X=a("a"),Tr=i("Habana Gaudi Processor (HPU)"),Ar=i(" - "),F=a("a"),Or=i("HPUs"),Pr=i(" are designed to maximize training throughput and efficiency. "),V=a("a"),Ir=i("More information here"),Dr=i("."),Nr=h(),g=a("li"),Y=a("a"),kr=i("Intel"),xr=i(" - Enabling the usage of Intel tools to accelerate end-to-end pipelines on Intel architectures. More information about "),j=a("a"),zr=i("Neural Compressor"),Hr=i(" and "),J=a("a"),Cr=i("OpenVINO"),Rr=i("."),Sr=h(),ut=a("li"),Ur=i("More to come soon! \u2B50"),St=h(),T=a("h2"),H=a("a"),mt=a("span"),ge(K.$$.fragment),qr=h(),vt=a("span"),Gr=i("Optimizing models towards inference"),Ut=h(),Pe=a("p"),Mr=i(`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),qt=h(),w=a("p"),Lr=i("Optimum enables the usage of popular compression techniques such as quantization and pruning by supporting "),W=a("a"),Qr=i("ONNX Runtime"),Br=i(" along with "),Z=a("a"),Xr=i("Intel Neural Compressor"),Fr=i("."),Gt=h(),C=a("table"),gt=a("thead"),A=a("tr"),Ie=a("th"),Vr=i("Features"),Yr=h(),De=a("th"),jr=i("ONNX Runtime"),Jr=h(),Ne=a("th"),Kr=i("Intel Neural Compressor"),Wr=h(),m=a("tbody"),O=a("tr"),ke=a("td"),Zr=i("Post-training Dynamic Quantization"),ea=h(),xe=a("td"),ta=i("\u2705"),ra=h(),ze=a("td"),aa=i("\u2705"),oa=h(),P=a("tr"),He=a("td"),la=i("Post-training Static Quantization"),na=h(),Ce=a("td"),ia=i("\u2705"),sa=h(),Re=a("td"),ha=i("\u2705"),pa=h(),I=a("tr"),Se=a("td"),fa=i("Quantization Aware Training (QAT)"),da=h(),Ue=a("td"),ca=i("Stay tuned! \u2B50"),ua=h(),qe=a("td"),ma=i("\u2705"),va=h(),D=a("tr"),Ge=a("td"),ga=i("Pruning"),wa=h(),Me=a("td"),_a=i("N/A"),Ea=h(),Le=a("td"),ya=i("\u2705"),Mt=h(),N=a("h2"),R=a("a"),wt=a("span"),ge(ee.$$.fragment),ba=h(),_t=a("span"),$a=i("Installation"),Lt=h(),S=a("p"),Ta=i("\u{1F917} Optimum can be installed using "),Et=a("code"),Aa=i("pip"),Oa=i(" as follows:"),Qt=h(),ge(te.$$.fragment),Bt=h(),Qe=a("p"),Pa=i("If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),Xt=h(),U=a("table"),yt=a("thead"),re=a("tr"),Be=a("th"),Ia=i("Accelerator"),Da=h(),Xe=a("th"),Na=i("Installation"),ka=h(),c=a("tbody"),ae=a("tr"),Fe=a("td"),oe=a("a"),xa=i("ONNX runtime"),za=h(),Ve=a("td"),bt=a("code"),Ha=i("python -m pip install optimum[onnxruntime]"),Ca=h(),le=a("tr"),Ye=a("td"),ne=a("a"),Ra=i("Intel Neural Compressor"),Sa=h(),je=a("td"),$t=a("code"),Ua=i("python -m pip install optimum[neural-compressor]"),qa=h(),ie=a("tr"),Je=a("td"),se=a("a"),Ga=i("OpenVINO"),Ma=h(),Ke=a("td"),Tt=a("code"),La=i("python -m pip install optimum[openvino]"),Qa=h(),he=a("tr"),We=a("td"),pe=a("a"),Ba=i("Graphcore IPU"),Xa=h(),Ze=a("td"),At=a("code"),Fa=i("python -m pip install optimum[graphcore]"),Va=h(),fe=a("tr"),et=a("td"),de=a("a"),Ya=i("Habana Gaudi Processor (HPU)"),ja=h(),tt=a("td"),Ot=a("code"),Ja=i("python -m pip install optimum[habana]"),Ft=h(),rt=a("p"),Ka=i("If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),Vt=h(),ge(ce.$$.fragment),Yt=h(),_=a("p"),Wa=i("For the accelerator-specific features, you can install them by appending "),Pt=a("code"),Za=i("#egg=optimum[accelerator_type]"),eo=i(" to the "),It=a("code"),to=i("pip"),ro=i(" command, e.g."),jt=h(),ge(ue.$$.fragment),this.h()},l(t){const f=Pl('[data-svelte="svelte-1phssyn"]',document.head);y=o(f,"META",{name:!0,content:!0}),f.forEach(r),Nt=p(t),b=o(t,"H1",{class:!0});var Kt=l(b);k=o(Kt,"A",{id:!0,class:!0,href:!0});var lo=l(k);pt=o(lo,"SPAN",{});var no=l(pt);we(M.$$.fragment,no),no.forEach(r),lo.forEach(r),fr=p(Kt),ft=o(Kt,"SPAN",{});var io=l(ft);dr=s(io,"\u{1F917} Optimum"),io.forEach(r),Kt.forEach(r),kt=p(t),$e=o(t,"P",{});var so=l($e);cr=s(so,"\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),so.forEach(r),xt=p(t),Te=o(t,"P",{});var ho=l(Te);ur=s(ho,`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),ho.forEach(r),zt=p(t),$=o(t,"H2",{class:!0});var Wt=l($);x=o(Wt,"A",{id:!0,class:!0,href:!0});var po=l(x);dt=o(po,"SPAN",{});var fo=l(dt);we(L.$$.fragment,fo),fo.forEach(r),po.forEach(r),mr=p(Wt),ct=o(Wt,"SPAN",{});var co=l(ct);vr=s(co,"Integration with Hardware Partners"),co.forEach(r),Wt.forEach(r),Ht=p(t),Ae=o(t,"P",{});var uo=l(Ae);gr=s(uo,"\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),uo.forEach(r),Ct=p(t),Oe=o(t,"P",{});var mo=l(Oe);wr=s(mo,"To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),mo.forEach(r),Rt=p(t),u=o(t,"UL",{});var q=l(u);z=o(q,"LI",{});var Dt=l(z);Q=o(Dt,"A",{href:!0,rel:!0});var vo=l(Q);_r=s(vo,"Graphcore IPUs"),vo.forEach(r),Er=s(Dt," - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),B=o(Dt,"A",{href:!0,rel:!0});var go=l(B);yr=s(go,"More information here"),go.forEach(r),br=s(Dt,"."),Dt.forEach(r),$r=p(q),v=o(q,"LI",{});var me=l(v);X=o(me,"A",{href:!0,rel:!0});var wo=l(X);Tr=s(wo,"Habana Gaudi Processor (HPU)"),wo.forEach(r),Ar=s(me," - "),F=o(me,"A",{href:!0,rel:!0});var _o=l(F);Or=s(_o,"HPUs"),_o.forEach(r),Pr=s(me," are designed to maximize training throughput and efficiency. "),V=o(me,"A",{href:!0,rel:!0});var Eo=l(V);Ir=s(Eo,"More information here"),Eo.forEach(r),Dr=s(me,"."),me.forEach(r),Nr=p(q),g=o(q,"LI",{});var ve=l(g);Y=o(ve,"A",{href:!0,rel:!0});var yo=l(Y);kr=s(yo,"Intel"),yo.forEach(r),xr=s(ve," - Enabling the usage of Intel tools to accelerate end-to-end pipelines on Intel architectures. More information about "),j=o(ve,"A",{href:!0,rel:!0});var bo=l(j);zr=s(bo,"Neural Compressor"),bo.forEach(r),Hr=s(ve," and "),J=o(ve,"A",{href:!0,rel:!0});var $o=l(J);Cr=s($o,"OpenVINO"),$o.forEach(r),Rr=s(ve,"."),ve.forEach(r),Sr=p(q),ut=o(q,"LI",{});var To=l(ut);Ur=s(To,"More to come soon! \u2B50"),To.forEach(r),q.forEach(r),St=p(t),T=o(t,"H2",{class:!0});var Zt=l(T);H=o(Zt,"A",{id:!0,class:!0,href:!0});var Ao=l(H);mt=o(Ao,"SPAN",{});var Oo=l(mt);we(K.$$.fragment,Oo),Oo.forEach(r),Ao.forEach(r),qr=p(Zt),vt=o(Zt,"SPAN",{});var Po=l(vt);Gr=s(Po,"Optimizing models towards inference"),Po.forEach(r),Zt.forEach(r),Ut=p(t),Pe=o(t,"P",{});var Io=l(Pe);Mr=s(Io,`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),Io.forEach(r),qt=p(t),w=o(t,"P",{});var at=l(w);Lr=s(at,"Optimum enables the usage of popular compression techniques such as quantization and pruning by supporting "),W=o(at,"A",{href:!0,rel:!0});var Do=l(W);Qr=s(Do,"ONNX Runtime"),Do.forEach(r),Br=s(at," along with "),Z=o(at,"A",{href:!0,rel:!0});var No=l(Z);Xr=s(No,"Intel Neural Compressor"),No.forEach(r),Fr=s(at,"."),at.forEach(r),Gt=p(t),C=o(t,"TABLE",{});var er=l(C);gt=o(er,"THEAD",{});var ko=l(gt);A=o(ko,"TR",{});var ot=l(A);Ie=o(ot,"TH",{align:!0});var xo=l(Ie);Vr=s(xo,"Features"),xo.forEach(r),Yr=p(ot),De=o(ot,"TH",{align:!0});var zo=l(De);jr=s(zo,"ONNX Runtime"),zo.forEach(r),Jr=p(ot),Ne=o(ot,"TH",{align:!0});var Ho=l(Ne);Kr=s(Ho,"Intel Neural Compressor"),Ho.forEach(r),ot.forEach(r),ko.forEach(r),Wr=p(er),m=o(er,"TBODY",{});var G=l(m);O=o(G,"TR",{});var lt=l(O);ke=o(lt,"TD",{align:!0});var Co=l(ke);Zr=s(Co,"Post-training Dynamic Quantization"),Co.forEach(r),ea=p(lt),xe=o(lt,"TD",{align:!0});var Ro=l(xe);ta=s(Ro,"\u2705"),Ro.forEach(r),ra=p(lt),ze=o(lt,"TD",{align:!0});var So=l(ze);aa=s(So,"\u2705"),So.forEach(r),lt.forEach(r),oa=p(G),P=o(G,"TR",{});var nt=l(P);He=o(nt,"TD",{align:!0});var Uo=l(He);la=s(Uo,"Post-training Static Quantization"),Uo.forEach(r),na=p(nt),Ce=o(nt,"TD",{align:!0});var qo=l(Ce);ia=s(qo,"\u2705"),qo.forEach(r),sa=p(nt),Re=o(nt,"TD",{align:!0});var Go=l(Re);ha=s(Go,"\u2705"),Go.forEach(r),nt.forEach(r),pa=p(G),I=o(G,"TR",{});var it=l(I);Se=o(it,"TD",{align:!0});var Mo=l(Se);fa=s(Mo,"Quantization Aware Training (QAT)"),Mo.forEach(r),da=p(it),Ue=o(it,"TD",{align:!0});var Lo=l(Ue);ca=s(Lo,"Stay tuned! \u2B50"),Lo.forEach(r),ua=p(it),qe=o(it,"TD",{align:!0});var Qo=l(qe);ma=s(Qo,"\u2705"),Qo.forEach(r),it.forEach(r),va=p(G),D=o(G,"TR",{});var st=l(D);Ge=o(st,"TD",{align:!0});var Bo=l(Ge);ga=s(Bo,"Pruning"),Bo.forEach(r),wa=p(st),Me=o(st,"TD",{align:!0});var Xo=l(Me);_a=s(Xo,"N/A"),Xo.forEach(r),Ea=p(st),Le=o(st,"TD",{align:!0});var Fo=l(Le);ya=s(Fo,"\u2705"),Fo.forEach(r),st.forEach(r),G.forEach(r),er.forEach(r),Mt=p(t),N=o(t,"H2",{class:!0});var tr=l(N);R=o(tr,"A",{id:!0,class:!0,href:!0});var Vo=l(R);wt=o(Vo,"SPAN",{});var Yo=l(wt);we(ee.$$.fragment,Yo),Yo.forEach(r),Vo.forEach(r),ba=p(tr),_t=o(tr,"SPAN",{});var jo=l(_t);$a=s(jo,"Installation"),jo.forEach(r),tr.forEach(r),Lt=p(t),S=o(t,"P",{});var rr=l(S);Ta=s(rr,"\u{1F917} Optimum can be installed using "),Et=o(rr,"CODE",{});var Jo=l(Et);Aa=s(Jo,"pip"),Jo.forEach(r),Oa=s(rr," as follows:"),rr.forEach(r),Qt=p(t),we(te.$$.fragment,t),Bt=p(t),Qe=o(t,"P",{});var Ko=l(Qe);Pa=s(Ko,"If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),Ko.forEach(r),Xt=p(t),U=o(t,"TABLE",{});var ar=l(U);yt=o(ar,"THEAD",{});var Wo=l(yt);re=o(Wo,"TR",{});var or=l(re);Be=o(or,"TH",{align:!0});var Zo=l(Be);Ia=s(Zo,"Accelerator"),Zo.forEach(r),Da=p(or),Xe=o(or,"TH",{align:!0});var el=l(Xe);Na=s(el,"Installation"),el.forEach(r),or.forEach(r),Wo.forEach(r),ka=p(ar),c=o(ar,"TBODY",{});var E=l(c);ae=o(E,"TR",{});var lr=l(ae);Fe=o(lr,"TD",{align:!0});var tl=l(Fe);oe=o(tl,"A",{href:!0,rel:!0});var rl=l(oe);xa=s(rl,"ONNX runtime"),rl.forEach(r),tl.forEach(r),za=p(lr),Ve=o(lr,"TD",{align:!0});var al=l(Ve);bt=o(al,"CODE",{});var ol=l(bt);Ha=s(ol,"python -m pip install optimum[onnxruntime]"),ol.forEach(r),al.forEach(r),lr.forEach(r),Ca=p(E),le=o(E,"TR",{});var nr=l(le);Ye=o(nr,"TD",{align:!0});var ll=l(Ye);ne=o(ll,"A",{href:!0,rel:!0});var nl=l(ne);Ra=s(nl,"Intel Neural Compressor"),nl.forEach(r),ll.forEach(r),Sa=p(nr),je=o(nr,"TD",{align:!0});var il=l(je);$t=o(il,"CODE",{});var sl=l($t);Ua=s(sl,"python -m pip install optimum[neural-compressor]"),sl.forEach(r),il.forEach(r),nr.forEach(r),qa=p(E),ie=o(E,"TR",{});var ir=l(ie);Je=o(ir,"TD",{align:!0});var hl=l(Je);se=o(hl,"A",{href:!0,rel:!0});var pl=l(se);Ga=s(pl,"OpenVINO"),pl.forEach(r),hl.forEach(r),Ma=p(ir),Ke=o(ir,"TD",{align:!0});var fl=l(Ke);Tt=o(fl,"CODE",{});var dl=l(Tt);La=s(dl,"python -m pip install optimum[openvino]"),dl.forEach(r),fl.forEach(r),ir.forEach(r),Qa=p(E),he=o(E,"TR",{});var sr=l(he);We=o(sr,"TD",{align:!0});var cl=l(We);pe=o(cl,"A",{href:!0,rel:!0});var ul=l(pe);Ba=s(ul,"Graphcore IPU"),ul.forEach(r),cl.forEach(r),Xa=p(sr),Ze=o(sr,"TD",{align:!0});var ml=l(Ze);At=o(ml,"CODE",{});var vl=l(At);Fa=s(vl,"python -m pip install optimum[graphcore]"),vl.forEach(r),ml.forEach(r),sr.forEach(r),Va=p(E),fe=o(E,"TR",{});var hr=l(fe);et=o(hr,"TD",{align:!0});var gl=l(et);de=o(gl,"A",{href:!0,rel:!0});var wl=l(de);Ya=s(wl,"Habana Gaudi Processor (HPU)"),wl.forEach(r),gl.forEach(r),ja=p(hr),tt=o(hr,"TD",{align:!0});var _l=l(tt);Ot=o(_l,"CODE",{});var El=l(Ot);Ja=s(El,"python -m pip install optimum[habana]"),El.forEach(r),_l.forEach(r),hr.forEach(r),E.forEach(r),ar.forEach(r),Ft=p(t),rt=o(t,"P",{});var yl=l(rt);Ka=s(yl,"If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),yl.forEach(r),Vt=p(t),we(ce.$$.fragment,t),Yt=p(t),_=o(t,"P",{});var ht=l(_);Wa=s(ht,"For the accelerator-specific features, you can install them by appending "),Pt=o(ht,"CODE",{});var bl=l(Pt);Za=s(bl,"#egg=optimum[accelerator_type]"),bl.forEach(r),eo=s(ht," to the "),It=o(ht,"CODE",{});var $l=l(It);to=s($l,"pip"),$l.forEach(r),ro=s(ht," command, e.g."),ht.forEach(r),jt=p(t),we(ue.$$.fragment,t),this.h()},h(){n(y,"name","hf:doc:metadata"),n(y,"content",JSON.stringify(kl)),n(k,"id","optimum"),n(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(k,"href","#optimum"),n(b,"class","relative group"),n(x,"id","integration-with-hardware-partners"),n(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(x,"href","#integration-with-hardware-partners"),n($,"class","relative group"),n(Q,"href","https://github.com/huggingface/optimum-graphcore"),n(Q,"rel","nofollow"),n(B,"href","https://www.graphcore.ai/products/ipu"),n(B,"rel","nofollow"),n(X,"href","https://github.com/huggingface/optimum-habana"),n(X,"rel","nofollow"),n(F,"href","https://docs.habana.ai/en/latest/Gaudi_Overview/Gaudi_Architecture.html"),n(F,"rel","nofollow"),n(V,"href","https://habana.ai/training/"),n(V,"rel","nofollow"),n(Y,"href","https://github.com/huggingface/optimum-intel"),n(Y,"rel","nofollow"),n(j,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),n(j,"rel","nofollow"),n(J,"href","https://docs.openvino.ai/latest/index.html"),n(J,"rel","nofollow"),n(H,"id","optimizing-models-towards-inference"),n(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(H,"href","#optimizing-models-towards-inference"),n(T,"class","relative group"),n(W,"href","https://onnxruntime.ai/docs/"),n(W,"rel","nofollow"),n(Z,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),n(Z,"rel","nofollow"),n(Ie,"align","center"),n(De,"align","center"),n(Ne,"align","center"),n(ke,"align","center"),n(xe,"align","center"),n(ze,"align","center"),n(He,"align","center"),n(Ce,"align","center"),n(Re,"align","center"),n(Se,"align","center"),n(Ue,"align","center"),n(qe,"align","center"),n(Ge,"align","center"),n(Me,"align","center"),n(Le,"align","center"),n(R,"id","installation"),n(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(R,"href","#installation"),n(N,"class","relative group"),n(Be,"align","left"),n(Xe,"align","left"),n(oe,"href","https://onnxruntime.ai/docs/"),n(oe,"rel","nofollow"),n(Fe,"align","left"),n(Ve,"align","left"),n(ne,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),n(ne,"rel","nofollow"),n(Ye,"align","left"),n(je,"align","left"),n(se,"href","https://docs.openvino.ai/latest/index.html"),n(se,"rel","nofollow"),n(Je,"align","left"),n(Ke,"align","left"),n(pe,"href","https://www.graphcore.ai/products/ipu"),n(pe,"rel","nofollow"),n(We,"align","left"),n(Ze,"align","left"),n(de,"href","https://habana.ai/training/"),n(de,"rel","nofollow"),n(et,"align","left"),n(tt,"align","left")},m(t,f){e(document.head,y),d(t,Nt,f),d(t,b,f),e(b,k),e(k,pt),_e(M,pt,null),e(b,fr),e(b,ft),e(ft,dr),d(t,kt,f),d(t,$e,f),e($e,cr),d(t,xt,f),d(t,Te,f),e(Te,ur),d(t,zt,f),d(t,$,f),e($,x),e(x,dt),_e(L,dt,null),e($,mr),e($,ct),e(ct,vr),d(t,Ht,f),d(t,Ae,f),e(Ae,gr),d(t,Ct,f),d(t,Oe,f),e(Oe,wr),d(t,Rt,f),d(t,u,f),e(u,z),e(z,Q),e(Q,_r),e(z,Er),e(z,B),e(B,yr),e(z,br),e(u,$r),e(u,v),e(v,X),e(X,Tr),e(v,Ar),e(v,F),e(F,Or),e(v,Pr),e(v,V),e(V,Ir),e(v,Dr),e(u,Nr),e(u,g),e(g,Y),e(Y,kr),e(g,xr),e(g,j),e(j,zr),e(g,Hr),e(g,J),e(J,Cr),e(g,Rr),e(u,Sr),e(u,ut),e(ut,Ur),d(t,St,f),d(t,T,f),e(T,H),e(H,mt),_e(K,mt,null),e(T,qr),e(T,vt),e(vt,Gr),d(t,Ut,f),d(t,Pe,f),e(Pe,Mr),d(t,qt,f),d(t,w,f),e(w,Lr),e(w,W),e(W,Qr),e(w,Br),e(w,Z),e(Z,Xr),e(w,Fr),d(t,Gt,f),d(t,C,f),e(C,gt),e(gt,A),e(A,Ie),e(Ie,Vr),e(A,Yr),e(A,De),e(De,jr),e(A,Jr),e(A,Ne),e(Ne,Kr),e(C,Wr),e(C,m),e(m,O),e(O,ke),e(ke,Zr),e(O,ea),e(O,xe),e(xe,ta),e(O,ra),e(O,ze),e(ze,aa),e(m,oa),e(m,P),e(P,He),e(He,la),e(P,na),e(P,Ce),e(Ce,ia),e(P,sa),e(P,Re),e(Re,ha),e(m,pa),e(m,I),e(I,Se),e(Se,fa),e(I,da),e(I,Ue),e(Ue,ca),e(I,ua),e(I,qe),e(qe,ma),e(m,va),e(m,D),e(D,Ge),e(Ge,ga),e(D,wa),e(D,Me),e(Me,_a),e(D,Ea),e(D,Le),e(Le,ya),d(t,Mt,f),d(t,N,f),e(N,R),e(R,wt),_e(ee,wt,null),e(N,ba),e(N,_t),e(_t,$a),d(t,Lt,f),d(t,S,f),e(S,Ta),e(S,Et),e(Et,Aa),e(S,Oa),d(t,Qt,f),_e(te,t,f),d(t,Bt,f),d(t,Qe,f),e(Qe,Pa),d(t,Xt,f),d(t,U,f),e(U,yt),e(yt,re),e(re,Be),e(Be,Ia),e(re,Da),e(re,Xe),e(Xe,Na),e(U,ka),e(U,c),e(c,ae),e(ae,Fe),e(Fe,oe),e(oe,xa),e(ae,za),e(ae,Ve),e(Ve,bt),e(bt,Ha),e(c,Ca),e(c,le),e(le,Ye),e(Ye,ne),e(ne,Ra),e(le,Sa),e(le,je),e(je,$t),e($t,Ua),e(c,qa),e(c,ie),e(ie,Je),e(Je,se),e(se,Ga),e(ie,Ma),e(ie,Ke),e(Ke,Tt),e(Tt,La),e(c,Qa),e(c,he),e(he,We),e(We,pe),e(pe,Ba),e(he,Xa),e(he,Ze),e(Ze,At),e(At,Fa),e(c,Va),e(c,fe),e(fe,et),e(et,de),e(de,Ya),e(fe,ja),e(fe,tt),e(tt,Ot),e(Ot,Ja),d(t,Ft,f),d(t,rt,f),e(rt,Ka),d(t,Vt,f),_e(ce,t,f),d(t,Yt,f),d(t,_,f),e(_,Wa),e(_,Pt),e(Pt,Za),e(_,eo),e(_,It),e(It,to),e(_,ro),d(t,jt,f),_e(ue,t,f),Jt=!0},p:Il,i(t){Jt||(Ee(M.$$.fragment,t),Ee(L.$$.fragment,t),Ee(K.$$.fragment,t),Ee(ee.$$.fragment,t),Ee(te.$$.fragment,t),Ee(ce.$$.fragment,t),Ee(ue.$$.fragment,t),Jt=!0)},o(t){ye(M.$$.fragment,t),ye(L.$$.fragment,t),ye(K.$$.fragment,t),ye(ee.$$.fragment,t),ye(te.$$.fragment,t),ye(ce.$$.fragment,t),ye(ue.$$.fragment,t),Jt=!1},d(t){r(y),t&&r(Nt),t&&r(b),be(M),t&&r(kt),t&&r($e),t&&r(xt),t&&r(Te),t&&r(zt),t&&r($),be(L),t&&r(Ht),t&&r(Ae),t&&r(Ct),t&&r(Oe),t&&r(Rt),t&&r(u),t&&r(St),t&&r(T),be(K),t&&r(Ut),t&&r(Pe),t&&r(qt),t&&r(w),t&&r(Gt),t&&r(C),t&&r(Mt),t&&r(N),be(ee),t&&r(Lt),t&&r(S),t&&r(Qt),be(te,t),t&&r(Bt),t&&r(Qe),t&&r(Xt),t&&r(U),t&&r(Ft),t&&r(rt),t&&r(Vt),be(ce,t),t&&r(Yt),t&&r(_),t&&r(jt),be(ue,t)}}}const kl={local:"optimum",sections:[{local:"integration-with-hardware-partners",title:"Integration with Hardware Partners"},{local:"optimizing-models-towards-inference",title:"Optimizing models towards inference"},{local:"installation",title:"Installation"}],title:"\u{1F917} Optimum"};function xl(oo){return Dl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rl extends Tl{constructor(y){super();Al(this,y,xl,Nl,Ol,{})}}export{Rl as default,kl as metadata};
