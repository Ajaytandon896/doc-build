import{S as fl,i as pl,s as dl,e as a,k as h,w as me,t as i,M as cl,c as o,d as r,m as f,a as l,x as ge,h as s,b as n,G as e,g as d,y as ve,L as ul,q as we,o as _e,B as Ee,v as ml}from"../chunks/vendor-hf-doc-builder.js";import{I as or}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as Va}from"../chunks/CodeBlock-hf-doc-builder.js";function gl(Ya){let E,At,y,N,nt,M,lr,it,nr,Pt,ye,ir,It,be,sr,Ot,b,k,st,L,hr,ht,fr,Dt,$e,pr,Nt,Te,dr,kt,c,x,Q,cr,ur,B,mr,gr,vr,g,X,wr,_r,F,Er,yr,V,br,$r,Tr,v,Y,Ar,Pr,j,Ir,Or,J,Dr,Nr,kr,ft,xr,xt,$,z,pt,K,zr,dt,Hr,zt,Ae,Cr,Ht,w,Rr,W,Sr,Ur,Z,qr,Gr,Ct,H,ct,T,Pe,Mr,Lr,Ie,Qr,Br,Oe,Xr,Fr,u,A,De,Vr,Yr,Ne,jr,Jr,ke,Kr,Wr,P,xe,Zr,ea,ze,ta,ra,He,aa,oa,I,Ce,la,na,Re,ia,sa,Se,ha,fa,O,Ue,pa,da,qe,ca,ua,Ge,ma,Rt,D,C,ut,ee,ga,mt,va,St,R,wa,gt,_a,Ea,Ut,te,qt,Me,ya,Gt,S,vt,re,Le,ba,$a,Qe,Ta,Aa,m,ae,Be,oe,Pa,Ia,Xe,wt,Oa,Da,le,Fe,ne,Na,ka,Ve,_t,xa,za,ie,Ye,se,Ha,Ca,je,Et,Ra,Sa,he,Je,fe,Ua,qa,Ke,yt,Ga,Mt,We,Ma,Lt,pe,Qt,_,La,bt,Qa,Ba,$t,Xa,Fa,Bt,de,Xt;return M=new or({}),L=new or({}),K=new or({}),ee=new or({}),te=new Va({props:{code:"python -m pip install optimum",highlighted:"python -m pip install optimum"}}),pe=new Va({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git",highlighted:"python -m pip install git+https://github.com/huggingface/optimum.git"}}),de=new Va({props:{code:"python -m pip install git+https://github.com/huggingface/optimum.git#egg=optimum[onnxruntime]",highlighted:'python -m pip install git+https://github.com/huggingface/optimum.git<span class="hljs-comment">#egg=optimum[onnxruntime]</span>'}}),{c(){E=a("meta"),At=h(),y=a("h1"),N=a("a"),nt=a("span"),me(M.$$.fragment),lr=h(),it=a("span"),nr=i("\u{1F917} Optimum"),Pt=h(),ye=a("p"),ir=i("\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),It=h(),be=a("p"),sr=i(`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),Ot=h(),b=a("h2"),k=a("a"),st=a("span"),me(L.$$.fragment),hr=h(),ht=a("span"),fr=i("Integration with Hardware Partners"),Dt=h(),$e=a("p"),pr=i("\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),Nt=h(),Te=a("p"),dr=i("To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),kt=h(),c=a("ul"),x=a("li"),Q=a("a"),cr=i("Graphcore IPUs"),ur=i(" - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),B=a("a"),mr=i("More information here"),gr=i("."),vr=h(),g=a("li"),X=a("a"),wr=i("Habana Gaudi Processor (HPU)"),_r=i(" - "),F=a("a"),Er=i("HPUs"),yr=i(" are designed to maximize training throughput and efficiency. "),V=a("a"),br=i("More information here"),$r=i("."),Tr=h(),v=a("li"),Y=a("a"),Ar=i("Intel"),Pr=i(" - Enabling the usage of Intel tools to accelerate end-to-end pipelines on Intel architectures. More information about "),j=a("a"),Ir=i("Neural Compressor"),Or=i(" and "),J=a("a"),Dr=i("OpenVINO"),Nr=i("."),kr=h(),ft=a("li"),xr=i("More to come soon! \u2B50"),xt=h(),$=a("h2"),z=a("a"),pt=a("span"),me(K.$$.fragment),zr=h(),dt=a("span"),Hr=i("Optimizing models towards inference"),zt=h(),Ae=a("p"),Cr=i(`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),Ht=h(),w=a("p"),Rr=i("Optimum enables the usage of popular compression techniques such as quantization and pruning by supporting "),W=a("a"),Sr=i("ONNX Runtime"),Ur=i(" along with "),Z=a("a"),qr=i("Intel Neural Compressor"),Gr=i("."),Ct=h(),H=a("table"),ct=a("thead"),T=a("tr"),Pe=a("th"),Mr=i("Features"),Lr=h(),Ie=a("th"),Qr=i("ONNX Runtime"),Br=h(),Oe=a("th"),Xr=i("Intel Neural Compressor"),Fr=h(),u=a("tbody"),A=a("tr"),De=a("td"),Vr=i("Post-training Dynamic Quantization"),Yr=h(),Ne=a("td"),jr=i("\u2705"),Jr=h(),ke=a("td"),Kr=i("\u2705"),Wr=h(),P=a("tr"),xe=a("td"),Zr=i("Post-training Static Quantization"),ea=h(),ze=a("td"),ta=i("\u2705"),ra=h(),He=a("td"),aa=i("\u2705"),oa=h(),I=a("tr"),Ce=a("td"),la=i("Quantization Aware Training (QAT)"),na=h(),Re=a("td"),ia=i("Stay tuned! \u2B50"),sa=h(),Se=a("td"),ha=i("\u2705"),fa=h(),O=a("tr"),Ue=a("td"),pa=i("Pruning"),da=h(),qe=a("td"),ca=i("N/A"),ua=h(),Ge=a("td"),ma=i("\u2705"),Rt=h(),D=a("h2"),C=a("a"),ut=a("span"),me(ee.$$.fragment),ga=h(),mt=a("span"),va=i("Installation"),St=h(),R=a("p"),wa=i("\u{1F917} Optimum can be installed using "),gt=a("code"),_a=i("pip"),Ea=i(" as follows:"),Ut=h(),me(te.$$.fragment),qt=h(),Me=a("p"),ya=i("If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),Gt=h(),S=a("table"),vt=a("thead"),re=a("tr"),Le=a("th"),ba=i("Accelerator"),$a=h(),Qe=a("th"),Ta=i("Installation"),Aa=h(),m=a("tbody"),ae=a("tr"),Be=a("td"),oe=a("a"),Pa=i("ONNX runtime"),Ia=h(),Xe=a("td"),wt=a("code"),Oa=i("python -m pip install optimum[onnxruntime]"),Da=h(),le=a("tr"),Fe=a("td"),ne=a("a"),Na=i("Intel Neural Compressor"),ka=h(),Ve=a("td"),_t=a("code"),xa=i("python -m pip install optimum[intel]"),za=h(),ie=a("tr"),Ye=a("td"),se=a("a"),Ha=i("Graphcore IPU"),Ca=h(),je=a("td"),Et=a("code"),Ra=i("python -m pip install optimum[graphcore]"),Sa=h(),he=a("tr"),Je=a("td"),fe=a("a"),Ua=i("Habana Gaudi Processor (HPU)"),qa=h(),Ke=a("td"),yt=a("code"),Ga=i("python -m pip install optimum[habana]"),Mt=h(),We=a("p"),Ma=i("If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),Lt=h(),me(pe.$$.fragment),Qt=h(),_=a("p"),La=i("For the accelerator-specific features, you can install them by appending "),bt=a("code"),Qa=i("#egg=optimum[accelerator_type]"),Ba=i(" to the "),$t=a("code"),Xa=i("pip"),Fa=i(" command, e.g."),Bt=h(),me(de.$$.fragment),this.h()},l(t){const p=cl('[data-svelte="svelte-1phssyn"]',document.head);E=o(p,"META",{name:!0,content:!0}),p.forEach(r),At=f(t),y=o(t,"H1",{class:!0});var Ft=l(y);N=o(Ft,"A",{id:!0,class:!0,href:!0});var ja=l(N);nt=o(ja,"SPAN",{});var Ja=l(nt);ge(M.$$.fragment,Ja),Ja.forEach(r),ja.forEach(r),lr=f(Ft),it=o(Ft,"SPAN",{});var Ka=l(it);nr=s(Ka,"\u{1F917} Optimum"),Ka.forEach(r),Ft.forEach(r),Pt=f(t),ye=o(t,"P",{});var Wa=l(ye);ir=s(Wa,"\u{1F917} Optimum is an extension of \u{1F917} Transformers, providing a set of performance optimization tools enabling maximum efficiency to train and run models on targeted hardware."),Wa.forEach(r),It=f(t),be=o(t,"P",{});var Za=l(be);sr=s(Za,`The AI ecosystem evolves quickly and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers.`),Za.forEach(r),Ot=f(t),b=o(t,"H2",{class:!0});var Vt=l(b);k=o(Vt,"A",{id:!0,class:!0,href:!0});var eo=l(k);st=o(eo,"SPAN",{});var to=l(st);ge(L.$$.fragment,to),to.forEach(r),eo.forEach(r),hr=f(Vt),ht=o(Vt,"SPAN",{});var ro=l(ht);fr=s(ro,"Integration with Hardware Partners"),ro.forEach(r),Vt.forEach(r),Dt=f(t),$e=o(t,"P",{});var ao=l($e);pr=s(ao,"\u{1F917} Optimum aims at providing more diversity towards the kind of hardware users can target to train and finetune their models."),ao.forEach(r),Nt=f(t),Te=o(t,"P",{});var oo=l(Te);dr=s(oo,"To achieve this, we are collaborating with the following hardware manufacturers in order to provide the best transformers integration:"),oo.forEach(r),kt=f(t),c=o(t,"UL",{});var U=l(c);x=o(U,"LI",{});var Tt=l(x);Q=o(Tt,"A",{href:!0,rel:!0});var lo=l(Q);cr=s(lo,"Graphcore IPUs"),lo.forEach(r),ur=s(Tt," - IPUs are a completely new kind of massively parallel processor to accelerate machine intelligence. "),B=o(Tt,"A",{href:!0,rel:!0});var no=l(B);mr=s(no,"More information here"),no.forEach(r),gr=s(Tt,"."),Tt.forEach(r),vr=f(U),g=o(U,"LI",{});var ce=l(g);X=o(ce,"A",{href:!0,rel:!0});var io=l(X);wr=s(io,"Habana Gaudi Processor (HPU)"),io.forEach(r),_r=s(ce," - "),F=o(ce,"A",{href:!0,rel:!0});var so=l(F);Er=s(so,"HPUs"),so.forEach(r),yr=s(ce," are designed to maximize training throughput and efficiency. "),V=o(ce,"A",{href:!0,rel:!0});var ho=l(V);br=s(ho,"More information here"),ho.forEach(r),$r=s(ce,"."),ce.forEach(r),Tr=f(U),v=o(U,"LI",{});var ue=l(v);Y=o(ue,"A",{href:!0,rel:!0});var fo=l(Y);Ar=s(fo,"Intel"),fo.forEach(r),Pr=s(ue," - Enabling the usage of Intel tools to accelerate end-to-end pipelines on Intel architectures. More information about "),j=o(ue,"A",{href:!0,rel:!0});var po=l(j);Ir=s(po,"Neural Compressor"),po.forEach(r),Or=s(ue," and "),J=o(ue,"A",{href:!0,rel:!0});var co=l(J);Dr=s(co,"OpenVINO"),co.forEach(r),Nr=s(ue,"."),ue.forEach(r),kr=f(U),ft=o(U,"LI",{});var uo=l(ft);xr=s(uo,"More to come soon! \u2B50"),uo.forEach(r),U.forEach(r),xt=f(t),$=o(t,"H2",{class:!0});var Yt=l($);z=o(Yt,"A",{id:!0,class:!0,href:!0});var mo=l(z);pt=o(mo,"SPAN",{});var go=l(pt);ge(K.$$.fragment,go),go.forEach(r),mo.forEach(r),zr=f(Yt),dt=o(Yt,"SPAN",{});var vo=l(dt);Hr=s(vo,"Optimizing models towards inference"),vo.forEach(r),Yt.forEach(r),zt=f(t),Ae=o(t,"P",{});var wo=l(Ae);Cr=s(wo,`Along with supporting dedicated AI hardware for training, Optimum also provides inference optimizations towards various frameworks and
platforms.`),wo.forEach(r),Ht=f(t),w=o(t,"P",{});var Ze=l(w);Rr=s(Ze,"Optimum enables the usage of popular compression techniques such as quantization and pruning by supporting "),W=o(Ze,"A",{href:!0,rel:!0});var _o=l(W);Sr=s(_o,"ONNX Runtime"),_o.forEach(r),Ur=s(Ze," along with "),Z=o(Ze,"A",{href:!0,rel:!0});var Eo=l(Z);qr=s(Eo,"Intel Neural Compressor"),Eo.forEach(r),Gr=s(Ze,"."),Ze.forEach(r),Ct=f(t),H=o(t,"TABLE",{});var jt=l(H);ct=o(jt,"THEAD",{});var yo=l(ct);T=o(yo,"TR",{});var et=l(T);Pe=o(et,"TH",{align:!0});var bo=l(Pe);Mr=s(bo,"Features"),bo.forEach(r),Lr=f(et),Ie=o(et,"TH",{align:!0});var $o=l(Ie);Qr=s($o,"ONNX Runtime"),$o.forEach(r),Br=f(et),Oe=o(et,"TH",{align:!0});var To=l(Oe);Xr=s(To,"Intel Neural Compressor"),To.forEach(r),et.forEach(r),yo.forEach(r),Fr=f(jt),u=o(jt,"TBODY",{});var q=l(u);A=o(q,"TR",{});var tt=l(A);De=o(tt,"TD",{align:!0});var Ao=l(De);Vr=s(Ao,"Post-training Dynamic Quantization"),Ao.forEach(r),Yr=f(tt),Ne=o(tt,"TD",{align:!0});var Po=l(Ne);jr=s(Po,"\u2705"),Po.forEach(r),Jr=f(tt),ke=o(tt,"TD",{align:!0});var Io=l(ke);Kr=s(Io,"\u2705"),Io.forEach(r),tt.forEach(r),Wr=f(q),P=o(q,"TR",{});var rt=l(P);xe=o(rt,"TD",{align:!0});var Oo=l(xe);Zr=s(Oo,"Post-training Static Quantization"),Oo.forEach(r),ea=f(rt),ze=o(rt,"TD",{align:!0});var Do=l(ze);ta=s(Do,"\u2705"),Do.forEach(r),ra=f(rt),He=o(rt,"TD",{align:!0});var No=l(He);aa=s(No,"\u2705"),No.forEach(r),rt.forEach(r),oa=f(q),I=o(q,"TR",{});var at=l(I);Ce=o(at,"TD",{align:!0});var ko=l(Ce);la=s(ko,"Quantization Aware Training (QAT)"),ko.forEach(r),na=f(at),Re=o(at,"TD",{align:!0});var xo=l(Re);ia=s(xo,"Stay tuned! \u2B50"),xo.forEach(r),sa=f(at),Se=o(at,"TD",{align:!0});var zo=l(Se);ha=s(zo,"\u2705"),zo.forEach(r),at.forEach(r),fa=f(q),O=o(q,"TR",{});var ot=l(O);Ue=o(ot,"TD",{align:!0});var Ho=l(Ue);pa=s(Ho,"Pruning"),Ho.forEach(r),da=f(ot),qe=o(ot,"TD",{align:!0});var Co=l(qe);ca=s(Co,"N/A"),Co.forEach(r),ua=f(ot),Ge=o(ot,"TD",{align:!0});var Ro=l(Ge);ma=s(Ro,"\u2705"),Ro.forEach(r),ot.forEach(r),q.forEach(r),jt.forEach(r),Rt=f(t),D=o(t,"H2",{class:!0});var Jt=l(D);C=o(Jt,"A",{id:!0,class:!0,href:!0});var So=l(C);ut=o(So,"SPAN",{});var Uo=l(ut);ge(ee.$$.fragment,Uo),Uo.forEach(r),So.forEach(r),ga=f(Jt),mt=o(Jt,"SPAN",{});var qo=l(mt);va=s(qo,"Installation"),qo.forEach(r),Jt.forEach(r),St=f(t),R=o(t,"P",{});var Kt=l(R);wa=s(Kt,"\u{1F917} Optimum can be installed using "),gt=o(Kt,"CODE",{});var Go=l(gt);_a=s(Go,"pip"),Go.forEach(r),Ea=s(Kt," as follows:"),Kt.forEach(r),Ut=f(t),ge(te.$$.fragment,t),qt=f(t),Me=o(t,"P",{});var Mo=l(Me);ya=s(Mo,"If you\u2019d like to use the accelerator-specific features of \u{1F917} Optimum, you can install the required dependencies according to the table below:"),Mo.forEach(r),Gt=f(t),S=o(t,"TABLE",{});var Wt=l(S);vt=o(Wt,"THEAD",{});var Lo=l(vt);re=o(Lo,"TR",{});var Zt=l(re);Le=o(Zt,"TH",{align:!0});var Qo=l(Le);ba=s(Qo,"Accelerator"),Qo.forEach(r),$a=f(Zt),Qe=o(Zt,"TH",{align:!0});var Bo=l(Qe);Ta=s(Bo,"Installation"),Bo.forEach(r),Zt.forEach(r),Lo.forEach(r),Aa=f(Wt),m=o(Wt,"TBODY",{});var G=l(m);ae=o(G,"TR",{});var er=l(ae);Be=o(er,"TD",{align:!0});var Xo=l(Be);oe=o(Xo,"A",{href:!0,rel:!0});var Fo=l(oe);Pa=s(Fo,"ONNX runtime"),Fo.forEach(r),Xo.forEach(r),Ia=f(er),Xe=o(er,"TD",{align:!0});var Vo=l(Xe);wt=o(Vo,"CODE",{});var Yo=l(wt);Oa=s(Yo,"python -m pip install optimum[onnxruntime]"),Yo.forEach(r),Vo.forEach(r),er.forEach(r),Da=f(G),le=o(G,"TR",{});var tr=l(le);Fe=o(tr,"TD",{align:!0});var jo=l(Fe);ne=o(jo,"A",{href:!0,rel:!0});var Jo=l(ne);Na=s(Jo,"Intel Neural Compressor"),Jo.forEach(r),jo.forEach(r),ka=f(tr),Ve=o(tr,"TD",{align:!0});var Ko=l(Ve);_t=o(Ko,"CODE",{});var Wo=l(_t);xa=s(Wo,"python -m pip install optimum[intel]"),Wo.forEach(r),Ko.forEach(r),tr.forEach(r),za=f(G),ie=o(G,"TR",{});var rr=l(ie);Ye=o(rr,"TD",{align:!0});var Zo=l(Ye);se=o(Zo,"A",{href:!0,rel:!0});var el=l(se);Ha=s(el,"Graphcore IPU"),el.forEach(r),Zo.forEach(r),Ca=f(rr),je=o(rr,"TD",{align:!0});var tl=l(je);Et=o(tl,"CODE",{});var rl=l(Et);Ra=s(rl,"python -m pip install optimum[graphcore]"),rl.forEach(r),tl.forEach(r),rr.forEach(r),Sa=f(G),he=o(G,"TR",{});var ar=l(he);Je=o(ar,"TD",{align:!0});var al=l(Je);fe=o(al,"A",{href:!0,rel:!0});var ol=l(fe);Ua=s(ol,"Habana Gaudi Processor (HPU)"),ol.forEach(r),al.forEach(r),qa=f(ar),Ke=o(ar,"TD",{align:!0});var ll=l(Ke);yt=o(ll,"CODE",{});var nl=l(yt);Ga=s(nl,"python -m pip install optimum[habana]"),nl.forEach(r),ll.forEach(r),ar.forEach(r),G.forEach(r),Wt.forEach(r),Mt=f(t),We=o(t,"P",{});var il=l(We);Ma=s(il,"If you\u2019d like to play with the examples or need the bleeding edge of the code and can\u2019t wait for a new release, you can install the base library from source as follows:"),il.forEach(r),Lt=f(t),ge(pe.$$.fragment,t),Qt=f(t),_=o(t,"P",{});var lt=l(_);La=s(lt,"For the accelerator-specific features, you can install them by appending "),bt=o(lt,"CODE",{});var sl=l(bt);Qa=s(sl,"#egg=optimum[accelerator_type]"),sl.forEach(r),Ba=s(lt," to the "),$t=o(lt,"CODE",{});var hl=l($t);Xa=s(hl,"pip"),hl.forEach(r),Fa=s(lt," command, e.g."),lt.forEach(r),Bt=f(t),ge(de.$$.fragment,t),this.h()},h(){n(E,"name","hf:doc:metadata"),n(E,"content",JSON.stringify(vl)),n(N,"id","optimum"),n(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(N,"href","#optimum"),n(y,"class","relative group"),n(k,"id","integration-with-hardware-partners"),n(k,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(k,"href","#integration-with-hardware-partners"),n(b,"class","relative group"),n(Q,"href","https://github.com/huggingface/optimum-graphcore"),n(Q,"rel","nofollow"),n(B,"href","https://www.graphcore.ai/products/ipu"),n(B,"rel","nofollow"),n(X,"href","https://github.com/huggingface/optimum-habana"),n(X,"rel","nofollow"),n(F,"href","https://docs.habana.ai/en/latest/Gaudi_Overview/Gaudi_Architecture.html"),n(F,"rel","nofollow"),n(V,"href","https://habana.ai/training/"),n(V,"rel","nofollow"),n(Y,"href","https://github.com/huggingface/optimum-intel"),n(Y,"rel","nofollow"),n(j,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),n(j,"rel","nofollow"),n(J,"href","https://docs.openvino.ai/latest/index.html"),n(J,"rel","nofollow"),n(z,"id","optimizing-models-towards-inference"),n(z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(z,"href","#optimizing-models-towards-inference"),n($,"class","relative group"),n(W,"href","https://onnxruntime.ai/docs/"),n(W,"rel","nofollow"),n(Z,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),n(Z,"rel","nofollow"),n(Pe,"align","center"),n(Ie,"align","center"),n(Oe,"align","center"),n(De,"align","center"),n(Ne,"align","center"),n(ke,"align","center"),n(xe,"align","center"),n(ze,"align","center"),n(He,"align","center"),n(Ce,"align","center"),n(Re,"align","center"),n(Se,"align","center"),n(Ue,"align","center"),n(qe,"align","center"),n(Ge,"align","center"),n(C,"id","installation"),n(C,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),n(C,"href","#installation"),n(D,"class","relative group"),n(Le,"align","left"),n(Qe,"align","left"),n(oe,"href","https://onnxruntime.ai/docs/"),n(oe,"rel","nofollow"),n(Be,"align","left"),n(Xe,"align","left"),n(ne,"href","https://www.intel.com/content/www/us/en/developer/tools/oneapi/neural-compressor.html"),n(ne,"rel","nofollow"),n(Fe,"align","left"),n(Ve,"align","left"),n(se,"href","https://www.graphcore.ai/products/ipu"),n(se,"rel","nofollow"),n(Ye,"align","left"),n(je,"align","left"),n(fe,"href","https://habana.ai/training/"),n(fe,"rel","nofollow"),n(Je,"align","left"),n(Ke,"align","left")},m(t,p){e(document.head,E),d(t,At,p),d(t,y,p),e(y,N),e(N,nt),ve(M,nt,null),e(y,lr),e(y,it),e(it,nr),d(t,Pt,p),d(t,ye,p),e(ye,ir),d(t,It,p),d(t,be,p),e(be,sr),d(t,Ot,p),d(t,b,p),e(b,k),e(k,st),ve(L,st,null),e(b,hr),e(b,ht),e(ht,fr),d(t,Dt,p),d(t,$e,p),e($e,pr),d(t,Nt,p),d(t,Te,p),e(Te,dr),d(t,kt,p),d(t,c,p),e(c,x),e(x,Q),e(Q,cr),e(x,ur),e(x,B),e(B,mr),e(x,gr),e(c,vr),e(c,g),e(g,X),e(X,wr),e(g,_r),e(g,F),e(F,Er),e(g,yr),e(g,V),e(V,br),e(g,$r),e(c,Tr),e(c,v),e(v,Y),e(Y,Ar),e(v,Pr),e(v,j),e(j,Ir),e(v,Or),e(v,J),e(J,Dr),e(v,Nr),e(c,kr),e(c,ft),e(ft,xr),d(t,xt,p),d(t,$,p),e($,z),e(z,pt),ve(K,pt,null),e($,zr),e($,dt),e(dt,Hr),d(t,zt,p),d(t,Ae,p),e(Ae,Cr),d(t,Ht,p),d(t,w,p),e(w,Rr),e(w,W),e(W,Sr),e(w,Ur),e(w,Z),e(Z,qr),e(w,Gr),d(t,Ct,p),d(t,H,p),e(H,ct),e(ct,T),e(T,Pe),e(Pe,Mr),e(T,Lr),e(T,Ie),e(Ie,Qr),e(T,Br),e(T,Oe),e(Oe,Xr),e(H,Fr),e(H,u),e(u,A),e(A,De),e(De,Vr),e(A,Yr),e(A,Ne),e(Ne,jr),e(A,Jr),e(A,ke),e(ke,Kr),e(u,Wr),e(u,P),e(P,xe),e(xe,Zr),e(P,ea),e(P,ze),e(ze,ta),e(P,ra),e(P,He),e(He,aa),e(u,oa),e(u,I),e(I,Ce),e(Ce,la),e(I,na),e(I,Re),e(Re,ia),e(I,sa),e(I,Se),e(Se,ha),e(u,fa),e(u,O),e(O,Ue),e(Ue,pa),e(O,da),e(O,qe),e(qe,ca),e(O,ua),e(O,Ge),e(Ge,ma),d(t,Rt,p),d(t,D,p),e(D,C),e(C,ut),ve(ee,ut,null),e(D,ga),e(D,mt),e(mt,va),d(t,St,p),d(t,R,p),e(R,wa),e(R,gt),e(gt,_a),e(R,Ea),d(t,Ut,p),ve(te,t,p),d(t,qt,p),d(t,Me,p),e(Me,ya),d(t,Gt,p),d(t,S,p),e(S,vt),e(vt,re),e(re,Le),e(Le,ba),e(re,$a),e(re,Qe),e(Qe,Ta),e(S,Aa),e(S,m),e(m,ae),e(ae,Be),e(Be,oe),e(oe,Pa),e(ae,Ia),e(ae,Xe),e(Xe,wt),e(wt,Oa),e(m,Da),e(m,le),e(le,Fe),e(Fe,ne),e(ne,Na),e(le,ka),e(le,Ve),e(Ve,_t),e(_t,xa),e(m,za),e(m,ie),e(ie,Ye),e(Ye,se),e(se,Ha),e(ie,Ca),e(ie,je),e(je,Et),e(Et,Ra),e(m,Sa),e(m,he),e(he,Je),e(Je,fe),e(fe,Ua),e(he,qa),e(he,Ke),e(Ke,yt),e(yt,Ga),d(t,Mt,p),d(t,We,p),e(We,Ma),d(t,Lt,p),ve(pe,t,p),d(t,Qt,p),d(t,_,p),e(_,La),e(_,bt),e(bt,Qa),e(_,Ba),e(_,$t),e($t,Xa),e(_,Fa),d(t,Bt,p),ve(de,t,p),Xt=!0},p:ul,i(t){Xt||(we(M.$$.fragment,t),we(L.$$.fragment,t),we(K.$$.fragment,t),we(ee.$$.fragment,t),we(te.$$.fragment,t),we(pe.$$.fragment,t),we(de.$$.fragment,t),Xt=!0)},o(t){_e(M.$$.fragment,t),_e(L.$$.fragment,t),_e(K.$$.fragment,t),_e(ee.$$.fragment,t),_e(te.$$.fragment,t),_e(pe.$$.fragment,t),_e(de.$$.fragment,t),Xt=!1},d(t){r(E),t&&r(At),t&&r(y),Ee(M),t&&r(Pt),t&&r(ye),t&&r(It),t&&r(be),t&&r(Ot),t&&r(b),Ee(L),t&&r(Dt),t&&r($e),t&&r(Nt),t&&r(Te),t&&r(kt),t&&r(c),t&&r(xt),t&&r($),Ee(K),t&&r(zt),t&&r(Ae),t&&r(Ht),t&&r(w),t&&r(Ct),t&&r(H),t&&r(Rt),t&&r(D),Ee(ee),t&&r(St),t&&r(R),t&&r(Ut),Ee(te,t),t&&r(qt),t&&r(Me),t&&r(Gt),t&&r(S),t&&r(Mt),t&&r(We),t&&r(Lt),Ee(pe,t),t&&r(Qt),t&&r(_),t&&r(Bt),Ee(de,t)}}}const vl={local:"optimum",sections:[{local:"integration-with-hardware-partners",title:"Integration with Hardware Partners"},{local:"optimizing-models-towards-inference",title:"Optimizing models towards inference"},{local:"installation",title:"Installation"}],title:"\u{1F917} Optimum"};function wl(Ya){return ml(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bl extends fl{constructor(E){super();pl(this,E,wl,gl,dl,{})}}export{bl as default,vl as metadata};
