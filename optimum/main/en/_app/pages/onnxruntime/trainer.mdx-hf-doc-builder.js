import{S as Ou,i as wu,s as $u,e as r,k as s,w as f,t as n,M as xu,c as i,d as a,m,a as l,x as _,h as o,b as p,G as e,g as c,y as g,q as v,o as b,B as y,v as Eu}from"../../chunks/vendor-hf-doc-builder.js";import{T as Hr}from"../../chunks/Tip-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ee}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as Re}from"../../chunks/IconCopyLink-hf-doc-builder.js";function Ru(Z){let u,$;return{c(){u=r("p"),$=n(`If your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic padding
in a token classification task) the predictions will be padded (on the right) to allow for concatenation into
one array. The padding index is -100.`)},l(h){u=i(h,"P",{});var O=l(u);$=o(O,`If your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic padding
in a token classification task) the predictions will be padded (on the right) to allow for concatenation into
one array. The padding index is -100.`),O.forEach(a)},m(h,O){c(h,u,O),e(u,$)},d(h){h&&a(u)}}}function Nu(Z){let u,$;return{c(){u=r("p"),$=n(`If your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic padding
in a token classification task) the predictions will be padded (on the right) to allow for concatenation into
one array. The padding index is -100.`)},l(h){u=i(h,"P",{});var O=l(u);$=o(O,`If your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic padding
in a token classification task) the predictions will be padded (on the right) to allow for concatenation into
one array. The padding index is -100.`),O.forEach(a)},m(h,O){c(h,u,O),e(u,$)},d(h){h&&a(u)}}}function ku(Z){let u,$,h,O,X;return{c(){u=r("p"),$=n(`DeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the moment).
You can find some `),h=r("a"),O=n("DeepSpeed configuration examples"),X=n(`
in the Optimum repository.`),this.h()},l(x){u=i(x,"P",{});var S=l(u);$=o(S,`DeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the moment).
You can find some `),h=i(S,"A",{href:!0,rel:!0});var Y=l(h);O=o(Y,"DeepSpeed configuration examples"),Y.forEach(a),X=o(S,`
in the Optimum repository.`),S.forEach(a),this.h()},h(){p(h,"href","https://github.com/huggingface/optimum/tree/main/tests/onnxruntime/ds_configs"),p(h,"rel","nofollow")},m(x,S){c(x,u,S),e(u,$),e(u,h),e(h,O),e(u,X)},d(x){x&&a(u)}}}function Su(Z){let u,$,h,O,X;return{c(){u=r("p"),$=n(`DeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the moment).
You can find some `),h=r("a"),O=n("DeepSpeed configuration examples"),X=n(`
in the Optimum repository.`),this.h()},l(x){u=i(x,"P",{});var S=l(u);$=o(S,`DeepSpeed is supported by ONNX Runtime(only ZeRO stage 1 and 2 for the moment).
You can find some `),h=i(S,"A",{href:!0,rel:!0});var Y=l(h);O=o(Y,"DeepSpeed configuration examples"),Y.forEach(a),X=o(S,`
in the Optimum repository.`),S.forEach(a),this.h()},h(){p(h,"href","https://github.com/huggingface/optimum/tree/main/tests/onnxruntime/ds_configs"),p(h,"rel","nofollow")},m(x,S){c(x,u,S),e(u,$),e(u,h),e(h,O),e(u,X)},d(x){x&&a(u)}}}function qu(Z){let u,$,h,O,X,x,S,Y,jr,So,E,Wr,ka,Gr,Vr,Sa,Yr,Br,qa,Qe,Zr,Jr,Da,Kr,Qr,Aa,ei,ti,Pa,ai,ni,qo,re,Ne,za,et,oi,Ca,ri,Do,ra,ii,Ao,J,li,Ia,si,mi,Fa,di,pi,Po,ie,ke,La,tt,ci,Xa,ui,zo,K,hi,Ua,fi,_i,at,gi,vi,Co,Se,bi,nt,yi,Ti,Io,ot,Fo,ia,Oi,Lo,rt,Xo,la,wi,Uo,it,Mo,le,qe,Ma,lt,$i,Ha,xi,Ho,sa,Ei,jo,st,Wo,ma,Ri,Go,mt,Vo,De,Ni,dt,ki,Si,Yo,se,Ae,ja,pt,qi,Wa,Di,Bo,R,Ai,Ga,Pi,zi,ct,Va,Ci,Ii,Ya,Fi,Li,Ba,Xi,Ui,Za,Mi,Hi,Ja,ji,Wi,Zo,ut,Jo,Pe,Gi,ht,Vi,Yi,Ko,T,ft,Bi,Ka,Zi,Ji,Qa,Ki,Qi,U,ze,en,el,tl,_t,al,nl,ol,N,tn,rl,il,an,ll,sl,nn,ml,dl,on,pl,cl,rn,ul,hl,ln,fl,_l,sn,gl,vl,bl,da,mn,yl,Tl,Ol,M,dn,wl,$l,pn,xl,El,cn,Rl,Nl,un,kl,Sl,ql,H,hn,Dl,Al,fn,Pl,zl,_n,Cl,Il,gn,Fl,Ll,Xl,Ce,gt,Ul,vn,Ml,Hl,Q,vt,jl,bn,Wl,Gl,bt,Vl,yn,Yl,Bl,Zl,Ie,yt,Jl,Tn,Kl,Ql,ee,Tt,es,me,ts,On,as,ns,wn,os,rs,is,$n,ls,ss,Fe,Ot,ms,wt,ds,xn,ps,cs,us,q,$t,hs,En,fs,_s,xt,gs,Rn,vs,bs,ys,Le,Ts,Et,Os,Nn,ws,$s,xs,de,pe,Es,kn,Rs,Ns,Sn,ks,Ss,qs,ce,Ds,qn,As,Ps,Dn,zs,Cs,Is,ue,Fs,An,Ls,Xs,Pn,Us,Ms,Hs,te,Rt,js,he,Ws,zn,Gs,Vs,Cn,Ys,Bs,Zs,In,Js,Ks,Xe,Nt,Qs,fe,em,Fn,tm,am,Ln,nm,om,rm,Ue,kt,im,Xn,lm,Qo,_e,Me,Un,St,sm,Mn,mm,er,k,dm,Hn,pm,cm,qt,jn,um,hm,Wn,fm,_m,Gn,gm,vm,Vn,bm,ym,Yn,Tm,Om,tr,Dt,ar,He,wm,At,$m,xm,nr,B,Pt,Em,je,zt,Rm,Bn,Nm,km,D,Ct,Sm,Zn,qm,Dm,It,Am,Jn,Pm,zm,Cm,We,Im,Ft,Fm,Kn,Lm,Xm,Um,ge,ve,Mm,Qn,Hm,jm,eo,Wm,Gm,Vm,be,Ym,to,Bm,Zm,ao,Jm,Km,Qm,ye,ed,no,td,ad,oo,nd,od,or,Te,Ge,ro,Lt,rd,io,id,rr,z,ld,lo,sd,md,Xt,so,dd,pd,mo,cd,ud,po,hd,fd,ir,Ut,lr,Mt,Ht,sr,Ve,mr,Oe,Ye,co,jt,_d,uo,gd,dr,C,vd,ho,bd,yd,Wt,fo,Td,Od,_o,wd,$d,go,xd,Ed,pr,Gt,cr,Vt,Yt,ur,Be,hr,we,Ze,vo,Bt,Rd,bo,Nd,fr,j,yo,Zt,kd,Sd,To,Jt,qd,Dd,Oo,Kt,Ad,Pd,pa,zd,Qt,wo,ea,Cd,Id,$o,ta,Fd,_r,W,Ld,xo,Xd,Ud,aa,Md,Hd,na,jd,Wd,gr;return x=new Re({}),et=new Re({}),tt=new Re({}),ot=new Ee({props:{code:"docker build -f Dockerfile-ort1.12.0-cu113 -t <imagename:tag> .",highlighted:"docker build -f Dockerfile-ort1.12.0-cu113 -t &lt;imagename:tag&gt; ."}}),rt=new Ee({props:{code:`pip install onnx==1.12.0 ninja
pip install onnxruntime-training==1.12.0+cu113 -f https://download.onnxruntime.ai/onnxruntime_stable_cu113.html
pip install torch==1.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html
pip install torch-ort
pip install transformers datasets accelerate
pip install --upgrade protobuf==3.20.1`,highlighted:`pip install onnx==1.12.0 ninja
pip install onnxruntime-training==1.12.0+cu113 -f https://download.onnxruntime.ai/onnxruntime_stable_cu113.html
pip install torch==1.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html
pip install torch-ort
pip install transformers datasets accelerate
pip install --upgrade protobuf==3.20.1`}}),it=new Ee({props:{code:"python -m torch_ort.configure",highlighted:"python -m torch_ort.configure"}}),lt=new Re({}),st=new Ee({props:{code:"pip install optimum",highlighted:"pip install optimum"}}),mt=new Ee({props:{code:"pip install git+https://github.com/huggingface/optimum.git",highlighted:"pip install git+https://github.com/huggingface/optimum.git"}}),pt=new Re({}),ut=new Ee({props:{code:`-from transformers import Trainer
+from optimum.onnxruntime import ORTTrainer

# Step 1: Create your ONNX Runtime Trainer
-trainer = Trainer(
+trainer = ORTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
+   feature="sequence-classification",
)

# Step 2: Use ONNX Runtime for training!\u{1F917}
train_result = trainer.train()`,highlighted:`<span class="hljs-deletion">-from transformers import Trainer</span>
<span class="hljs-addition">+from optimum.onnxruntime import ORTTrainer</span>

# Step 1: Create your ONNX Runtime Trainer
<span class="hljs-deletion">-trainer = Trainer(</span>
<span class="hljs-addition">+trainer = ORTTrainer(</span>
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    tokenizer=tokenizer,
    data_collator=default_data_collator,
<span class="hljs-addition">+   feature=&quot;sequence-classification&quot;,</span>
)

# Step 2: Use ONNX Runtime for training!\u{1F917}
train_result = trainer.train()`}}),ft=new P({props:{name:"class optimum.onnxruntime.ORTTrainer",anchor:"optimum.onnxruntime.ORTTrainer",parameters:[{name:"model",val:": typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None"},{name:"tokenizer",val:": typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None"},{name:"feature",val:": str = 'default'"},{name:"args",val:": ORTTrainingArguments = None"},{name:"data_collator",val:": typing.Optional[DataCollator] = None"},{name:"train_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"model_init",val:": typing.Callable[[], transformers.modeling_utils.PreTrainedModel] = None"},{name:"compute_metrics",val:": typing.Union[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict], NoneType] = None"},{name:"callbacks",val:": typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]] = None"},{name:"optimizers",val:": typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None)"},{name:"preprocess_logits_for_metrics",val:": typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None"},{name:"onnx_model_path",val:": typing.Union[str, os.PathLike] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.model",description:`<strong>model</strong> (<a href="https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel" rel="nofollow">PreTrainedModel</a> or <code>torch.nn.Module</code>, <em>optional</em>) &#x2014;
The model to train, evaluate or use for predictions. If not provided, a <code>model_init</code> must be passed.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p><code>ORTTrainer</code> is optimized to work with the <a href="https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel" rel="nofollow">PreTrainedModel</a> provided by the transformers library.
You can still use your own models defined as <code>torch.nn.Module</code> for training with ONNX Runtime backend
and inference with PyTorch backend as long as they work the same way as the &#x1F917; Transformers models.</p>

					</div>`,name:"model"},{anchor:"optimum.onnxruntime.ORTTrainer.args",description:`<strong>args</strong> (<code>ORTTrainingArguments</code>, <em>optional</em>) &#x2014;
The arguments to tweak for training. Will default to a basic instance of <code>ORTTrainingArguments</code> with the
<code>output_dir</code> set to a directory named <em>tmp_trainer</em> in the current directory if not provided.`,name:"args"},{anchor:"optimum.onnxruntime.ORTTrainer.data_collator",description:`<strong>data_collator</strong> (<code>DataCollator</code>, <em>optional</em>) &#x2014;
The function to use to form a batch from a list of elements of <code>train_dataset</code> or <code>eval_dataset</code>. Will
default to <a href="https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.default_data_collator" rel="nofollow">default_data_collator</a> if no <code>tokenizer</code> is provided, an instance of
<a href="https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding" rel="nofollow">DataCollatorWithPadding</a> otherwise.`,name:"data_collator"},{anchor:"optimum.onnxruntime.ORTTrainer.train_dataset",description:`<strong>train_dataset</strong> (<code>torch.utils.data.Dataset</code> or <code>torch.utils.data.IterableDataset</code>, <em>optional</em>) &#x2014;
The dataset to use for training. If it is a <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset" rel="nofollow">Dataset</a>, columns not accepted by the
<code>model.forward()</code> method are automatically removed.
Note that if it&#x2019;s a <code>torch.utils.data.IterableDataset</code> with some randomization and you are training in a
distributed fashion, your iterable dataset should either use a internal attribute <code>generator</code> that is a
<code>torch.Generator</code> for the randomization that must be identical on all processes (and the ORTTrainer will
manually set the seed of this <code>generator</code> at each epoch) or have a <code>set_epoch()</code> method that internally
sets the seed of the RNGs used.`,name:"train_dataset"},{anchor:"optimum.onnxruntime.ORTTrainer.eval_dataset",description:`<strong>eval_dataset</strong> (<code>torch.utils.data.Dataset</code>, <em>optional</em>) &#x2014;
The dataset to use for evaluation. If it is a <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset" rel="nofollow">Dataset</a>, columns not accepted by the
<code>model.forward()</code> method are automatically removed.`,name:"eval_dataset"},{anchor:"optimum.onnxruntime.ORTTrainer.tokenizer",description:`<strong>tokenizer</strong> (<a href="https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase" rel="nofollow">PreTrainedTokenizerBase</a>, <em>optional</em>) &#x2014;
The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs the
maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an
interrupted training or reuse the fine-tuned model.`,name:"tokenizer"},{anchor:"optimum.onnxruntime.ORTTrainer.model_init",description:`<strong>model_init</strong> (<code>Callable[[], PreTrainedModel]</code>, <em>optional</em>) &#x2014;
A function that instantiates the model to be used. If provided, each call to <code>ORTTrainer.train</code> will start
from a new instance of the model as given by this function.
The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to
be able to choose different architectures according to hyper parameters (such as layer count, sizes of
inner layers, dropout probabilities etc).`,name:"model_init"},{anchor:"optimum.onnxruntime.ORTTrainer.compute_metrics",description:`<strong>compute_metrics</strong> (<code>Callable[[EvalPrediction], Dict]</code>, <em>optional</em>) &#x2014;
The function that will be used to compute metrics at evaluation. Must take a <code>EvalPrediction</code> and return
a dictionary string to metric values.`,name:"compute_metrics"},{anchor:"optimum.onnxruntime.ORTTrainer.callbacks",description:`<strong>callbacks</strong> (List of <code>TrainerCallback</code>, <em>optional</em>) &#x2014;
A list of callbacks to customize the training loop. Will add those to the list of default callbacks
detailed in <a href="callback">here</a>.
If you want to remove one of the default callbacks used, use the <code>ORTTrainer.remove_callback</code> method.`,name:"callbacks"},{anchor:"optimum.onnxruntime.ORTTrainer.optimizers",description:`<strong>optimizers</strong> (<code>Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]</code>, <em>optional</em>) &#x2014; A tuple
containing the optimizer and the scheduler to use. Will default to an instance of <code>AdamW</code> on your model
and a scheduler given by <code>get_linear_schedule_with_warmup</code> controlled by <code>args</code>.`,name:"optimizers"},{anchor:"optimum.onnxruntime.ORTTrainer.preprocess_logits_for_metrics",description:`<strong>preprocess_logits_for_metrics</strong> (<code>Callable[[torch.Tensor, torch.Tensor], torch.Tensor]</code>, <em>optional</em>) &#x2014;
A function that preprocess the logits right before caching them at each evaluation step. Must take two
tensors, the logits and the labels, and return the logits once processed as desired. The modifications made
by this function will be reflected in the predictions received by <code>compute_metrics</code>.
Note that the labels (second parameter) will be <code>None</code> if the dataset does not have them.`,name:"preprocess_logits_for_metrics"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L154"}}),gt=new P({props:{name:"compute_loss_ort",anchor:"optimum.onnxruntime.ORTTrainer.compute_loss_ort",parameters:[{name:"model",val:""},{name:"inputs",val:""},{name:"return_outputs",val:" = False"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L1369"}}),vt=new P({props:{name:"create_optimizer",anchor:"optimum.onnxruntime.ORTTrainer.create_optimizer",parameters:[],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L1573"}}),yt=new P({props:{name:"evaluate",anchor:"optimum.onnxruntime.ORTTrainer.evaluate",parameters:[{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"},{name:"inference_with_ort",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.evaluate.eval_dataset",description:`<strong>eval_dataset</strong> (<code>Dataset</code>, <em>optional</em>) &#x2014;
Pass a dataset if you wish to override <code>self.eval_dataset</code>. If it is a <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset" rel="nofollow">Dataset</a>, columns
not accepted by the <code>model.forward()</code> method are automatically removed. It must implement the <code>__len__</code>
method.`,name:"eval_dataset"},{anchor:"optimum.onnxruntime.ORTTrainer.evaluate.ignore_keys",description:`<strong>ignore_keys</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"optimum.onnxruntime.ORTTrainer.evaluate.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;eval&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;eval_bleu&#x201D; if the prefix is &#x201C;eval&#x201D; (default)`,name:"metric_key_prefix"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L736",returnDescription:`
<p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The
dictionary also contains the epoch number which comes from the training state.</p>
`}}),Tt=new P({props:{name:"evaluation_loop_ort",anchor:"optimum.onnxruntime.ORTTrainer.evaluation_loop_ort",parameters:[{name:"dataloader",val:": DataLoader"},{name:"description",val:": str"},{name:"prediction_loss_only",val:": typing.Optional[bool] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L896"}}),Ot=new P({props:{name:"get_ort_optimizer_cls_and_kwargs",anchor:"optimum.onnxruntime.ORTTrainer.get_ort_optimizer_cls_and_kwargs",parameters:[{name:"args",val:": ORTTrainingArguments"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.get_ort_optimizer_cls_and_kwargs.args",description:`<strong>args</strong> (<code>ORTTrainingArguments</code>) &#x2014;
The training arguments for the training session.`,name:"args"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L1626"}}),$t=new P({props:{name:"predict",anchor:"optimum.onnxruntime.ORTTrainer.predict",parameters:[{name:"test_dataset",val:": Dataset"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'test'"},{name:"inference_with_ort",val:": bool = False"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.predict.test_dataset",description:`<strong>test_dataset</strong> (<code>Dataset</code>) &#x2014;
Dataset to run the predictions on. If it is an <code>datasets.Dataset</code>, columns not accepted by the
<code>model.forward()</code> method are automatically removed. Has to implement the method <code>__len__</code>`,name:"test_dataset"},{anchor:"optimum.onnxruntime.ORTTrainer.predict.ignore_keys",description:`<strong>ignore_keys</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"optimum.onnxruntime.ORTTrainer.predict.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;test&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;test_bleu&#x201D; if the prefix is &#x201C;test&#x201D; (default)`,name:"metric_key_prefix"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L813"}}),Le=new Hr({props:{$$slots:{default:[Ru]},$$scope:{ctx:Z}}}),Rt=new P({props:{name:"prediction_loop_ort",anchor:"optimum.onnxruntime.ORTTrainer.prediction_loop_ort",parameters:[{name:"dataloader",val:": DataLoader"},{name:"description",val:": str"},{name:"prediction_loss_only",val:": typing.Optional[bool] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L1120"}}),Nt=new P({props:{name:"prediction_step_ort",anchor:"optimum.onnxruntime.ORTTrainer.prediction_step_ort",parameters:[{name:"model",val:": ORTModel"},{name:"inputs",val:": typing.Dict[str, typing.Union[torch.Tensor, typing.Any]]"},{name:"prediction_loss_only",val:": bool"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.prediction_step_ort.model",description:`<strong>model</strong> (<code>ORTModel</code>) &#x2014;
The model to evaluate.`,name:"model"},{anchor:"optimum.onnxruntime.ORTTrainer.prediction_step_ort.inputs",description:`<strong>inputs</strong> (<code>Dict[str, Union[torch.Tensor, Any]]</code>) &#x2014;
The inputs and targets of the model.</p>
<p>The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
argument <code>labels</code>. Check your model&#x2019;s documentation for all accepted arguments.`,name:"inputs"},{anchor:"optimum.onnxruntime.ORTTrainer.prediction_step_ort.prediction_loss_only",description:`<strong>prediction_loss_only</strong> (<code>bool</code>) &#x2014;
Whether or not to return the loss only.`,name:"prediction_loss_only"},{anchor:"optimum.onnxruntime.ORTTrainer.prediction_step_ort.ignore_keys",description:`<strong>ignore_keys</strong> (<code>Lst[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L1283",returnDescription:`
<p>A tuple with the loss,
logits and labels (each being optional).</p>
`,returnType:`
<p>Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]</p>
`}}),kt=new P({props:{name:"train",anchor:"optimum.onnxruntime.ORTTrainer.train",parameters:[{name:"resume_from_checkpoint",val:": typing.Union[str, bool, NoneType] = None"},{name:"trial",val:": typing.Union[ForwardRef('optuna.Trial'), typing.Dict[str, typing.Any]] = None"},{name:"ignore_keys_for_eval",val:": typing.Optional[typing.List[str]] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainer.train.resume_from_checkpoint",description:`<strong>resume_from_checkpoint</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
If a <code>str</code>, local path to a saved checkpoint as saved by a previous instance of <code>ORTTrainer</code>. If a
<code>bool</code> and equals <code>True</code>, load the last checkpoint in <em>args.output_dir</em> as saved by a previous instance
of <code>ORTTrainer</code>. If present, training will resume from the model/optimizer/scheduler states loaded here.`,name:"resume_from_checkpoint"},{anchor:"optimum.onnxruntime.ORTTrainer.train.trial",description:`<strong>trial</strong> (<code>optuna.Trial</code> or <code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The trial run or the hyperparameter dictionary for hyperparameter search.`,name:"trial"},{anchor:"optimum.onnxruntime.ORTTrainer.train.ignore_keys_for_eval",description:`<strong>ignore_keys_for_eval</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions for evaluation during the training.
kwargs &#x2014;
Additional keyword arguments used to hide deprecated arguments`,name:"ignore_keys_for_eval"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer.py#L267"}}),St=new Re({}),Dt=new Ee({props:{code:`-from transformers import Seq2SeqTrainer
+from optimum.onnxruntime import ORTSeq2SeqTrainer

# Step 1: Create your ONNX Runtime Seq2SeqTrainer
-trainer = Seq2SeqTrainer(
trainer = ORTSeq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    compute_metrics=compute_metrics if training_args.predict_with_generate else None,
    tokenizer=tokenizer,
    data_collator=data_collator,
+   feature="seq2seq-lm",
)

# Step 2: Use ONNX Runtime for training!\u{1F917}
train_result = trainer.train()`,highlighted:`<span class="hljs-deletion">-from transformers import Seq2SeqTrainer</span>
<span class="hljs-addition">+from optimum.onnxruntime import ORTSeq2SeqTrainer</span>

# Step 1: Create your ONNX Runtime Seq2SeqTrainer
<span class="hljs-deletion">-trainer = Seq2SeqTrainer(</span>
trainer = ORTSeq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=valid_dataset,
    compute_metrics=compute_metrics if training_args.predict_with_generate else None,
    tokenizer=tokenizer,
    data_collator=data_collator,
<span class="hljs-addition">+   feature=&quot;seq2seq-lm&quot;,</span>
)

# Step 2: Use ONNX Runtime for training!\u{1F917}
train_result = trainer.train()`}}),Pt=new P({props:{name:"class optimum.onnxruntime.ORTSeq2SeqTrainer",anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer",parameters:[{name:"model",val:": typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None"},{name:"tokenizer",val:": typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None"},{name:"feature",val:": str = 'default'"},{name:"args",val:": ORTTrainingArguments = None"},{name:"data_collator",val:": typing.Optional[DataCollator] = None"},{name:"train_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"model_init",val:": typing.Callable[[], transformers.modeling_utils.PreTrainedModel] = None"},{name:"compute_metrics",val:": typing.Union[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict], NoneType] = None"},{name:"callbacks",val:": typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]] = None"},{name:"optimizers",val:": typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None)"},{name:"preprocess_logits_for_metrics",val:": typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None"},{name:"onnx_model_path",val:": typing.Union[str, os.PathLike] = None"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer_seq2seq.py#L72"}}),zt=new P({props:{name:"evaluate",anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.evaluate",parameters:[{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"},{name:"inference_with_ort",val:": bool = False"},{name:"**gen_kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.evaluate.eval_dataset",description:`<strong>eval_dataset</strong> (<code>Dataset</code>, <em>optional</em>) &#x2014;
Pass a dataset if you wish to override <code>self.eval_dataset</code>. If it is a <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset" rel="nofollow">Dataset</a>, columns
not accepted by the <code>model.forward()</code> method are automatically removed. It must implement the <code>__len__</code>
method.`,name:"eval_dataset"},{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.evaluate.ignore_keys",description:`<strong>ignore_keys</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.evaluate.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;eval&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;eval_bleu&#x201D; if the prefix is &#x201C;eval&#x201D; (default)`,name:"metric_key_prefix"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer_seq2seq.py#L73",returnDescription:`
<p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The
dictionary also contains the epoch number which comes from the training state.</p>
`}}),Ct=new P({props:{name:"predict",anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.predict",parameters:[{name:"test_dataset",val:": Dataset"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"},{name:"inference_with_ort",val:": bool = False"},{name:"**gen_kwargs",val:""}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.predict.test_dataset",description:`<strong>test_dataset</strong> (<code>Dataset</code>) &#x2014;
Dataset to run the predictions on. If it is an <code>datasets.Dataset</code>, columns not accepted by the
<code>model.forward()</code> method are automatically removed. Has to implement the method <code>__len__</code>`,name:"test_dataset"},{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.predict.ignore_keys",description:`<strong>ignore_keys</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainer.predict.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;test&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;test_bleu&#x201D; if the prefix is &#x201C;test&#x201D; (default)`,name:"metric_key_prefix"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/trainer_seq2seq.py#L116"}}),We=new Hr({props:{$$slots:{default:[Nu]},$$scope:{ctx:Z}}}),Lt=new Re({}),Ut=new Ee({props:{code:`-from transformers import TrainingArguments
+from optimum.onnxruntime import ORTTrainingArguments

-training_args = TrainingArguments(
+training_args =  ORTTrainingArguments(
    output_dir=tmp_dir,
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir=tmp_dir,
    optim="adamw_ort_fused",  # Fused Adam optimizer implemented by ORT
)`,highlighted:`<span class="hljs-deletion">-from transformers import TrainingArguments</span>
<span class="hljs-addition">+from optimum.onnxruntime import ORTTrainingArguments</span>

<span class="hljs-deletion">-training_args = TrainingArguments(</span>
<span class="hljs-addition">+training_args =  ORTTrainingArguments(</span>
    output_dir=tmp_dir,
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir=tmp_dir,
    optim=&quot;adamw_ort_fused&quot;,  # Fused Adam optimizer implemented by ORT
)`}}),Ht=new P({props:{name:"class optimum.onnxruntime.ORTTrainingArguments",anchor:"optimum.onnxruntime.ORTTrainingArguments",parameters:[{name:"output_dir",val:": str"},{name:"overwrite_output_dir",val:": bool = False"},{name:"do_train",val:": bool = False"},{name:"do_eval",val:": bool = False"},{name:"do_predict",val:": bool = False"},{name:"evaluation_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'"},{name:"prediction_loss_only",val:": bool = False"},{name:"per_device_train_batch_size",val:": int = 8"},{name:"per_device_eval_batch_size",val:": int = 8"},{name:"per_gpu_train_batch_size",val:": typing.Optional[int] = None"},{name:"per_gpu_eval_batch_size",val:": typing.Optional[int] = None"},{name:"gradient_accumulation_steps",val:": int = 1"},{name:"eval_accumulation_steps",val:": typing.Optional[int] = None"},{name:"eval_delay",val:": typing.Optional[float] = 0"},{name:"learning_rate",val:": float = 5e-05"},{name:"weight_decay",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"max_grad_norm",val:": float = 1.0"},{name:"num_train_epochs",val:": float = 3.0"},{name:"max_steps",val:": int = -1"},{name:"lr_scheduler_type",val:": typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear'"},{name:"warmup_ratio",val:": float = 0.0"},{name:"warmup_steps",val:": int = 0"},{name:"log_level",val:": typing.Optional[str] = 'passive'"},{name:"log_level_replica",val:": typing.Optional[str] = 'passive'"},{name:"log_on_each_node",val:": bool = True"},{name:"logging_dir",val:": typing.Optional[str] = None"},{name:"logging_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'"},{name:"logging_first_step",val:": bool = False"},{name:"logging_steps",val:": int = 500"},{name:"logging_nan_inf_filter",val:": bool = True"},{name:"save_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'"},{name:"save_steps",val:": int = 500"},{name:"save_total_limit",val:": typing.Optional[int] = None"},{name:"save_on_each_node",val:": bool = False"},{name:"no_cuda",val:": bool = False"},{name:"use_mps_device",val:": bool = False"},{name:"seed",val:": int = 42"},{name:"data_seed",val:": typing.Optional[int] = None"},{name:"jit_mode_eval",val:": bool = False"},{name:"use_ipex",val:": bool = False"},{name:"bf16",val:": bool = False"},{name:"fp16",val:": bool = False"},{name:"fp16_opt_level",val:": str = 'O1'"},{name:"half_precision_backend",val:": str = 'auto'"},{name:"bf16_full_eval",val:": bool = False"},{name:"fp16_full_eval",val:": bool = False"},{name:"tf32",val:": typing.Optional[bool] = None"},{name:"local_rank",val:": int = -1"},{name:"xpu_backend",val:": typing.Optional[str] = None"},{name:"tpu_num_cores",val:": typing.Optional[int] = None"},{name:"tpu_metrics_debug",val:": bool = False"},{name:"debug",val:": str = ''"},{name:"dataloader_drop_last",val:": bool = False"},{name:"eval_steps",val:": typing.Optional[int] = None"},{name:"dataloader_num_workers",val:": int = 0"},{name:"past_index",val:": int = -1"},{name:"run_name",val:": typing.Optional[str] = None"},{name:"disable_tqdm",val:": typing.Optional[bool] = None"},{name:"remove_unused_columns",val:": typing.Optional[bool] = True"},{name:"label_names",val:": typing.Optional[typing.List[str]] = None"},{name:"load_best_model_at_end",val:": typing.Optional[bool] = False"},{name:"metric_for_best_model",val:": typing.Optional[str] = None"},{name:"greater_is_better",val:": typing.Optional[bool] = None"},{name:"ignore_data_skip",val:": bool = False"},{name:"sharded_ddp",val:": str = ''"},{name:"fsdp",val:": str = ''"},{name:"fsdp_min_num_params",val:": int = 0"},{name:"fsdp_transformer_layer_cls_to_wrap",val:": typing.Optional[str] = None"},{name:"deepspeed",val:": typing.Optional[str] = None"},{name:"label_smoothing_factor",val:": float = 0.0"},{name:"optim",val:": typing.Optional[str] = 'adamw_hf'"},{name:"adafactor",val:": bool = False"},{name:"group_by_length",val:": bool = False"},{name:"length_column_name",val:": typing.Optional[str] = 'length'"},{name:"report_to",val:": typing.Optional[typing.List[str]] = None"},{name:"ddp_find_unused_parameters",val:": typing.Optional[bool] = None"},{name:"ddp_bucket_cap_mb",val:": typing.Optional[int] = None"},{name:"dataloader_pin_memory",val:": bool = True"},{name:"skip_memory_metrics",val:": bool = True"},{name:"use_legacy_prediction_loop",val:": bool = False"},{name:"push_to_hub",val:": bool = False"},{name:"resume_from_checkpoint",val:": typing.Optional[str] = None"},{name:"hub_model_id",val:": typing.Optional[str] = None"},{name:"hub_strategy",val:": typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'"},{name:"hub_token",val:": typing.Optional[str] = None"},{name:"hub_private_repo",val:": bool = False"},{name:"gradient_checkpointing",val:": bool = False"},{name:"include_inputs_for_metrics",val:": bool = False"},{name:"fp16_backend",val:": str = 'auto'"},{name:"push_to_hub_model_id",val:": typing.Optional[str] = None"},{name:"push_to_hub_organization",val:": typing.Optional[str] = None"},{name:"push_to_hub_token",val:": typing.Optional[str] = None"},{name:"mp_parameters",val:": str = ''"},{name:"auto_find_batch_size",val:": bool = False"},{name:"full_determinism",val:": bool = False"},{name:"torchdynamo",val:": typing.Optional[str] = None"},{name:"ray_scope",val:": typing.Optional[str] = 'last'"},{name:"ddp_timeout",val:": typing.Optional[int] = 1800"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTTrainingArguments.optim",description:`<strong>optim</strong> (<code>str</code> or <code>training_args.ORTOptimizerNames</code> or <code>transformers.training_args.OptimizerNames</code>, <em>optional</em>, defaults to <code>&quot;adamw_hf&quot;</code>) &#x2014;
The optimizer to use, including optimizers in Transformers: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor. And optimizers implemented by ONNX Runtime: adamw_ort_fused.`,name:"optim"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/training_args.py#L56"}}),Ve=new Hr({props:{warning:!1,$$slots:{default:[ku]},$$scope:{ctx:Z}}}),jt=new Re({}),Gt=new Ee({props:{code:`-from transformers import Seq2SeqTrainingArguments
+from optimum.onnxruntime import ORTSeq2SeqTrainingArguments

-training_args = Seq2SeqTrainingArguments(
+training_args =  ORTSeq2SeqTrainingArguments(
    output_dir=tmp_dir,
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir=tmp_dir,
    optim="adamw_ort_fused",  # Fused Adam optimizer implemented by ORT
)`,highlighted:`<span class="hljs-deletion">-from transformers import Seq2SeqTrainingArguments</span>
<span class="hljs-addition">+from optimum.onnxruntime import ORTSeq2SeqTrainingArguments</span>

<span class="hljs-deletion">-training_args = Seq2SeqTrainingArguments(</span>
<span class="hljs-addition">+training_args =  ORTSeq2SeqTrainingArguments(</span>
    output_dir=tmp_dir,
    num_train_epochs=1,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir=tmp_dir,
    optim=&quot;adamw_ort_fused&quot;,  # Fused Adam optimizer implemented by ORT
)`}}),Yt=new P({props:{name:"class optimum.onnxruntime.ORTSeq2SeqTrainingArguments",anchor:"optimum.onnxruntime.ORTSeq2SeqTrainingArguments",parameters:[{name:"output_dir",val:": str"},{name:"overwrite_output_dir",val:": bool = False"},{name:"do_train",val:": bool = False"},{name:"do_eval",val:": bool = False"},{name:"do_predict",val:": bool = False"},{name:"evaluation_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'no'"},{name:"prediction_loss_only",val:": bool = False"},{name:"per_device_train_batch_size",val:": int = 8"},{name:"per_device_eval_batch_size",val:": int = 8"},{name:"per_gpu_train_batch_size",val:": typing.Optional[int] = None"},{name:"per_gpu_eval_batch_size",val:": typing.Optional[int] = None"},{name:"gradient_accumulation_steps",val:": int = 1"},{name:"eval_accumulation_steps",val:": typing.Optional[int] = None"},{name:"eval_delay",val:": typing.Optional[float] = 0"},{name:"learning_rate",val:": float = 5e-05"},{name:"weight_decay",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"max_grad_norm",val:": float = 1.0"},{name:"num_train_epochs",val:": float = 3.0"},{name:"max_steps",val:": int = -1"},{name:"lr_scheduler_type",val:": typing.Union[transformers.trainer_utils.SchedulerType, str] = 'linear'"},{name:"warmup_ratio",val:": float = 0.0"},{name:"warmup_steps",val:": int = 0"},{name:"log_level",val:": typing.Optional[str] = 'passive'"},{name:"log_level_replica",val:": typing.Optional[str] = 'passive'"},{name:"log_on_each_node",val:": bool = True"},{name:"logging_dir",val:": typing.Optional[str] = None"},{name:"logging_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'"},{name:"logging_first_step",val:": bool = False"},{name:"logging_steps",val:": int = 500"},{name:"logging_nan_inf_filter",val:": bool = True"},{name:"save_strategy",val:": typing.Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps'"},{name:"save_steps",val:": int = 500"},{name:"save_total_limit",val:": typing.Optional[int] = None"},{name:"save_on_each_node",val:": bool = False"},{name:"no_cuda",val:": bool = False"},{name:"use_mps_device",val:": bool = False"},{name:"seed",val:": int = 42"},{name:"data_seed",val:": typing.Optional[int] = None"},{name:"jit_mode_eval",val:": bool = False"},{name:"use_ipex",val:": bool = False"},{name:"bf16",val:": bool = False"},{name:"fp16",val:": bool = False"},{name:"fp16_opt_level",val:": str = 'O1'"},{name:"half_precision_backend",val:": str = 'auto'"},{name:"bf16_full_eval",val:": bool = False"},{name:"fp16_full_eval",val:": bool = False"},{name:"tf32",val:": typing.Optional[bool] = None"},{name:"local_rank",val:": int = -1"},{name:"xpu_backend",val:": typing.Optional[str] = None"},{name:"tpu_num_cores",val:": typing.Optional[int] = None"},{name:"tpu_metrics_debug",val:": bool = False"},{name:"debug",val:": str = ''"},{name:"dataloader_drop_last",val:": bool = False"},{name:"eval_steps",val:": typing.Optional[int] = None"},{name:"dataloader_num_workers",val:": int = 0"},{name:"past_index",val:": int = -1"},{name:"run_name",val:": typing.Optional[str] = None"},{name:"disable_tqdm",val:": typing.Optional[bool] = None"},{name:"remove_unused_columns",val:": typing.Optional[bool] = True"},{name:"label_names",val:": typing.Optional[typing.List[str]] = None"},{name:"load_best_model_at_end",val:": typing.Optional[bool] = False"},{name:"metric_for_best_model",val:": typing.Optional[str] = None"},{name:"greater_is_better",val:": typing.Optional[bool] = None"},{name:"ignore_data_skip",val:": bool = False"},{name:"sharded_ddp",val:": str = ''"},{name:"fsdp",val:": str = ''"},{name:"fsdp_min_num_params",val:": int = 0"},{name:"fsdp_transformer_layer_cls_to_wrap",val:": typing.Optional[str] = None"},{name:"deepspeed",val:": typing.Optional[str] = None"},{name:"label_smoothing_factor",val:": float = 0.0"},{name:"optim",val:": typing.Optional[str] = 'adamw_hf'"},{name:"adafactor",val:": bool = False"},{name:"group_by_length",val:": bool = False"},{name:"length_column_name",val:": typing.Optional[str] = 'length'"},{name:"report_to",val:": typing.Optional[typing.List[str]] = None"},{name:"ddp_find_unused_parameters",val:": typing.Optional[bool] = None"},{name:"ddp_bucket_cap_mb",val:": typing.Optional[int] = None"},{name:"dataloader_pin_memory",val:": bool = True"},{name:"skip_memory_metrics",val:": bool = True"},{name:"use_legacy_prediction_loop",val:": bool = False"},{name:"push_to_hub",val:": bool = False"},{name:"resume_from_checkpoint",val:": typing.Optional[str] = None"},{name:"hub_model_id",val:": typing.Optional[str] = None"},{name:"hub_strategy",val:": typing.Union[transformers.trainer_utils.HubStrategy, str] = 'every_save'"},{name:"hub_token",val:": typing.Optional[str] = None"},{name:"hub_private_repo",val:": bool = False"},{name:"gradient_checkpointing",val:": bool = False"},{name:"include_inputs_for_metrics",val:": bool = False"},{name:"fp16_backend",val:": str = 'auto'"},{name:"push_to_hub_model_id",val:": typing.Optional[str] = None"},{name:"push_to_hub_organization",val:": typing.Optional[str] = None"},{name:"push_to_hub_token",val:": typing.Optional[str] = None"},{name:"mp_parameters",val:": str = ''"},{name:"auto_find_batch_size",val:": bool = False"},{name:"full_determinism",val:": bool = False"},{name:"torchdynamo",val:": typing.Optional[str] = None"},{name:"ray_scope",val:": typing.Optional[str] = 'last'"},{name:"ddp_timeout",val:": typing.Optional[int] = 1800"},{name:"sortish_sampler",val:": bool = False"},{name:"predict_with_generate",val:": bool = False"},{name:"generation_max_length",val:": typing.Optional[int] = None"},{name:"generation_num_beams",val:": typing.Optional[int] = None"}],parametersDescription:[{anchor:"optimum.onnxruntime.ORTSeq2SeqTrainingArguments.optim",description:`<strong>optim</strong> (<code>str</code> or <code>training_args.ORTOptimizerNames</code> or <code>transformers.training_args.OptimizerNames</code>, <em>optional</em>, defaults to <code>&quot;adamw_hf&quot;</code>) &#x2014;
The optimizer to use, including optimizers in Transformers: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor. And optimizers implemented by ONNX Runtime: adamw_ort_fused.`,name:"optim"}],source:"https://github.com/huggingface/optimum/blob/main/optimum/onnxruntime/training_args_seq2seq.py#L25"}}),Be=new Hr({props:{warning:!1,$$slots:{default:[Su]},$$scope:{ctx:Z}}}),Bt=new Re({}),{c(){u=r("meta"),$=s(),h=r("h1"),O=r("a"),X=r("span"),f(x.$$.fragment),S=s(),Y=r("span"),jr=n("Trainer"),So=s(),E=r("p"),Wr=n("The "),ka=r("code"),Gr=n("ORTTrainer"),Vr=n(" and "),Sa=r("code"),Yr=n("ORTSeq2SeqTrainer"),Br=n(" classes provide APIs for training PyTorch models with "),qa=r("strong"),Qe=r("a"),Zr=n("ONNX Runtime (ORT)"),Jr=n(`.
Taking ONNX Runtime as backend, `),Da=r("code"),Kr=n("ORTTrainer"),Qr=n(" and "),Aa=r("code"),ei=n("ORTSeq2SeqTrainer"),ti=n(` optimize the computation graph and the memory usage. They also support
mixed precision training implemented by ORT, as well as distributed training on multiple GPUs. With them, you will be able to achieve
`),Pa=r("strong"),ai=n("lower latency, higher throughput, and larger maximum batch size"),ni=n(" while training large transformers models."),qo=s(),re=r("h2"),Ne=r("a"),za=r("span"),f(et.$$.fragment),oi=s(),Ca=r("span"),ri=n("Prerequisite"),Do=s(),ra=r("p"),ii=n("To use ONNX Runtime for training, you need a machine with at least one NVIDIA or AMD GPU."),Ao=s(),J=r("p"),li=n("To use "),Ia=r("code"),si=n("ORTTrainer"),mi=n(" or "),Fa=r("code"),di=n("ORTSeq2SeqTrainer"),pi=n(", you need to install ONNX Runtime Training module and Optimum."),Po=s(),ie=r("h3"),ke=r("a"),La=r("span"),f(tt.$$.fragment),ci=s(),Xa=r("span"),ui=n("Install ONNX Runtime"),zo=s(),K=r("p"),hi=n("To set up the environment, we "),Ua=r("strong"),fi=n("strongly recommend"),_i=n(` you install the dependencies with Docker to ensure that the versions are correct and well
configured. You can find dockerfiles with various combinations `),at=r("a"),gi=n("here"),vi=n("."),Co=s(),Se=r("p"),bi=n("For example, if you want to install "),nt=r("a"),yi=n("onnxruntime-training 1.12.0"),Ti=n(" via Dockerfile:"),Io=s(),f(ot.$$.fragment),Fo=s(),ia=r("p"),Oi=n("If you want to install the dependencies beyond in a local Python environment. You can pip install them once you have CUDA 11.3 and cuDNN 8 well installed."),Lo=s(),f(rt.$$.fragment),Xo=s(),la=r("p"),wi=n("And run post-installation configuration:"),Uo=s(),f(it.$$.fragment),Mo=s(),le=r("h3"),qe=r("a"),Ma=r("span"),f(lt.$$.fragment),$i=s(),Ha=r("span"),xi=n("Install Optimum"),Ho=s(),sa=r("p"),Ei=n("You can install Optimum via pypi:"),jo=s(),f(st.$$.fragment),Wo=s(),ma=r("p"),Ri=n("Or install from source:"),Go=s(),f(mt.$$.fragment),Vo=s(),De=r("p"),Ni=n(`This command installs the current main dev version of Optimum, which could include latest developments(new features, bug fixes). However, the
main version might not be very stable. If you run into any problem, please open an `),dt=r("a"),ki=n("issue"),Si=n(` so
that we can fix it as soon as possible.`),Yo=s(),se=r("h2"),Ae=r("a"),ja=r("span"),f(pt.$$.fragment),qi=s(),Wa=r("span"),Di=n("ORTTrainer"),Bo=s(),R=r("p"),Ai=n("The "),Ga=r("code"),Pi=n("ORTTrainer"),zi=n(" class inherits the "),ct=r("a"),Va=r("code"),Ci=n("Trainer"),Ii=n(`
of Transformers. You can easily adapt the codes by replacing `),Ya=r("code"),Fi=n("Trainer"),Li=n(" of transformers with "),Ba=r("code"),Xi=n("ORTTrainer"),Ui=n(` to take advantage of the acceleration
empowered by ONNX Runtime. Here is an example of how to use `),Za=r("code"),Mi=n("ORTTrainer"),Hi=n(" compared with "),Ja=r("code"),ji=n("Trainer"),Wi=n(":"),Zo=s(),f(ut.$$.fragment),Jo=s(),Pe=r("p"),Gi=n("Check out more detailed "),ht=r("a"),Vi=n("example scripts"),Yi=n(" in the optimum repository."),Ko=s(),T=r("div"),f(ft.$$.fragment),Bi=s(),Ka=r("p"),Zi=n("ORTTrainer is a simple but feature-complete training and eval loop for ONNX Runtime, optimized for \u{1F917} Transformers."),Ji=s(),Qa=r("p"),Ki=n("Important attributes:"),Qi=s(),U=r("ul"),ze=r("li"),en=r("strong"),el=n("model"),tl=n(" \u2014 Always points to the core model. If using a transformers model, it will be a "),_t=r("a"),al=n("PreTrainedModel"),nl=n(`
subclass.`),ol=s(),N=r("li"),tn=r("strong"),rl=n("model_wrapped"),il=n(` \u2014 Always points to the most external model in case one or more other modules wrap the
original model. This is the model that should be used for the forward pass. For example, under `),an=r("code"),ll=n("DeepSpeed"),sl=n(`,
the inner model is first wrapped in `),nn=r("code"),ml=n("ORTModule"),dl=n(" and then in "),on=r("code"),pl=n("DeepSpeed"),cl=n(` and then again in
`),rn=r("code"),ul=n("torch.nn.DistributedDataParallel"),hl=n(". If the inner model hasn\u2019t been wrapped, then "),ln=r("code"),fl=n("self.model_wrapped"),_l=n(` is the
same as `),sn=r("code"),gl=n("self.model"),vl=n("."),bl=s(),da=r("li"),mn=r("strong"),yl=n("is_model_parallel"),Tl=n(` \u2014 Whether or not a model has been switched to a model parallel mode (different from
data parallelism, this means some of the model layers are split on different GPUs).`),Ol=s(),M=r("li"),dn=r("strong"),wl=n("place_model_on_device"),$l=n(` \u2014 Whether or not to automatically place the model on the device - it will be set
to `),pn=r("code"),xl=n("False"),El=n(` if model parallel or deepspeed is used, or if the default
`),cn=r("code"),Rl=n("ORTTrainingArguments.place_model_on_device"),Nl=n(" is overridden to return "),un=r("code"),kl=n("False"),Sl=n(" ."),ql=s(),H=r("li"),hn=r("strong"),Dl=n("is_in_train"),Al=n(" \u2014 Whether or not a model is currently running "),fn=r("code"),Pl=n("train"),zl=n(" (e.g. when "),_n=r("code"),Cl=n("evaluate"),Il=n(` is called while
in `),gn=r("code"),Fl=n("train"),Ll=n(")"),Xl=s(),Ce=r("div"),f(gt.$$.fragment),Ul=s(),vn=r("p"),Ml=n(`How the loss is computed by ORTTrainer. By default, all models return the loss in the first element.
Subclass and override for custom behavior.`),Hl=s(),Q=r("div"),f(vt.$$.fragment),jl=s(),bn=r("p"),Wl=n("Setup the optimizer."),Gl=s(),bt=r("p"),Vl=n(`We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
ORTTrainer\u2019s init through `),yn=r("code"),Yl=n("optimizers"),Bl=n(", or subclass and override this method in a subclass."),Zl=s(),Ie=r("div"),f(yt.$$.fragment),Jl=s(),Tn=r("p"),Kl=n("Run evaluation with ONNX Runtime or PyTorch backend and returns metrics."),Ql=s(),ee=r("div"),f(Tt.$$.fragment),es=s(),me=r("p"),ts=n("Prediction/evaluation loop, shared by "),On=r("code"),as=n("ORTTrainer.evaluate()"),ns=n(" and "),wn=r("code"),os=n("ORTTrainer.predict()"),rs=n("."),is=s(),$n=r("p"),ls=n("Works both with or without labels."),ss=s(),Fe=r("div"),f(Ot.$$.fragment),ms=s(),wt=r("p"),ds=n("Returns the optimizer class and optimizer parameters implemented in ONNX Runtime based on "),xn=r("code"),ps=n("ORTTrainingArguments"),cs=n("."),us=s(),q=r("div"),f($t.$$.fragment),hs=s(),En=r("p"),fs=n("Run prediction and returns predictions and potential metrics."),_s=s(),xt=r("p"),gs=n(`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Rn=r("code"),vs=n("evaluate()"),bs=n("."),ys=s(),f(Le.$$.fragment),Ts=s(),Et=r("p"),Os=n("Returns: "),Nn=r("em"),ws=n("NamedTuple"),$s=n(" A namedtuple with the following keys:"),xs=s(),de=r("ul"),pe=r("li"),Es=n("predictions ("),kn=r("code"),Rs=n("np.ndarray"),Ns=n("): The predictions on "),Sn=r("code"),ks=n("test_dataset"),Ss=n("."),qs=s(),ce=r("li"),Ds=n("label_ids ("),qn=r("code"),As=n("np.ndarray"),Ps=n(", "),Dn=r("em"),zs=n("optional"),Cs=n("): The labels (if the dataset contained some)."),Is=s(),ue=r("li"),Fs=n("metrics ("),An=r("code"),Ls=n("Dict[str, float]"),Xs=n(", "),Pn=r("em"),Us=n("optional"),Ms=n(`): The potential dictionary of metrics (if the dataset contained
labels).`),Hs=s(),te=r("div"),f(Rt.$$.fragment),js=s(),he=r("p"),Ws=n("Prediction/evaluation loop, shared by "),zn=r("code"),Gs=n("ORTTrainer.evaluate()"),Vs=n(" and "),Cn=r("code"),Ys=n("ORTTrainer.predict()"),Bs=n("."),Zs=s(),In=r("p"),Js=n("Works both with or without labels."),Ks=s(),Xe=r("div"),f(Nt.$$.fragment),Qs=s(),fe=r("p"),em=n("Perform an evaluation step on "),Fn=r("code"),tm=n("model"),am=n(" using "),Ln=r("code"),nm=n("inputs"),om=n("."),rm=s(),Ue=r("div"),f(kt.$$.fragment),im=s(),Xn=r("p"),lm=n("Main entry point for training with ONNX Runtime accelerator."),Qo=s(),_e=r("h2"),Me=r("a"),Un=r("span"),f(St.$$.fragment),sm=s(),Mn=r("span"),mm=n("ORTSeq2SeqTrainer"),er=s(),k=r("p"),dm=n("The "),Hn=r("code"),pm=n("ORTSeq2SeqTrainer"),cm=n(" class is similar to the "),qt=r("a"),jn=r("code"),um=n("Seq2SeqTrainer"),hm=n(`
of Transformers. You can easily adapt the codes by replacing `),Wn=r("code"),fm=n("Seq2SeqTrainer"),_m=n(" of transformers with "),Gn=r("code"),gm=n("ORTSeq2SeqTrainer"),vm=n(` to take advantage of the acceleration
empowered by ONNX Runtime. Here is an example of how to use `),Vn=r("code"),bm=n("ORTSeq2SeqTrainer"),ym=n(" compared with "),Yn=r("code"),Tm=n("Seq2SeqTrainer"),Om=n(":"),tr=s(),f(Dt.$$.fragment),ar=s(),He=r("p"),wm=n("Check out more detailed "),At=r("a"),$m=n("example scripts"),xm=n(" in the optimum repository."),nr=s(),B=r("div"),f(Pt.$$.fragment),Em=s(),je=r("div"),f(zt.$$.fragment),Rm=s(),Bn=r("p"),Nm=n("Run evaluation with ONNX Runtime or PyTorch backend and returns metrics."),km=s(),D=r("div"),f(Ct.$$.fragment),Sm=s(),Zn=r("p"),qm=n("Run prediction and returns predictions and potential metrics."),Dm=s(),It=r("p"),Am=n(`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Jn=r("code"),Pm=n("evaluate()"),zm=n("."),Cm=s(),f(We.$$.fragment),Im=s(),Ft=r("p"),Fm=n("Returns: "),Kn=r("em"),Lm=n("NamedTuple"),Xm=n(" A namedtuple with the following keys:"),Um=s(),ge=r("ul"),ve=r("li"),Mm=n("predictions ("),Qn=r("code"),Hm=n("np.ndarray"),jm=n("): The predictions on "),eo=r("code"),Wm=n("test_dataset"),Gm=n("."),Vm=s(),be=r("li"),Ym=n("label_ids ("),to=r("code"),Bm=n("np.ndarray"),Zm=n(", "),ao=r("em"),Jm=n("optional"),Km=n("): The labels (if the dataset contained some)."),Qm=s(),ye=r("li"),ed=n("metrics ("),no=r("code"),td=n("Dict[str, float]"),ad=n(", "),oo=r("em"),nd=n("optional"),od=n(`): The potential dictionary of metrics (if the dataset contained
labels).`),or=s(),Te=r("h2"),Ge=r("a"),ro=r("span"),f(Lt.$$.fragment),rd=s(),io=r("span"),id=n("ORTTrainingArguments"),rr=s(),z=r("p"),ld=n("The "),lo=r("code"),sd=n("ORTTrainingArguments"),md=n(" class inherits the "),Xt=r("a"),so=r("code"),dd=n("TrainingArguments"),pd=n(`
class in Transformers. Besides the optimizers implemented in Transformers, it allows you to use the optimizers implemented in ONNX Runtime.
Replace `),mo=r("code"),cd=n("Seq2SeqTrainingArguments"),ud=n(" with "),po=r("code"),hd=n("ORTSeq2SeqTrainingArguments"),fd=n(":"),ir=s(),f(Ut.$$.fragment),lr=s(),Mt=r("div"),f(Ht.$$.fragment),sr=s(),f(Ve.$$.fragment),mr=s(),Oe=r("h2"),Ye=r("a"),co=r("span"),f(jt.$$.fragment),_d=s(),uo=r("span"),gd=n("ORTSeq2SeqTrainingArguments"),dr=s(),C=r("p"),vd=n("The "),ho=r("code"),bd=n("ORTSeq2SeqTrainingArguments"),yd=n(" class inherits the "),Wt=r("a"),fo=r("code"),Td=n("Seq2SeqTrainingArguments"),Od=n(`
class in Transformers. Besides the optimizers implemented in Transformers, it allows you to use the optimizers implemented in ONNX Runtime.
Replace `),_o=r("code"),wd=n("Seq2SeqTrainingArguments"),$d=n(" with "),go=r("code"),xd=n("ORTSeq2SeqTrainingArguments"),Ed=n(":"),pr=s(),f(Gt.$$.fragment),cr=s(),Vt=r("div"),f(Yt.$$.fragment),ur=s(),f(Be.$$.fragment),hr=s(),we=r("h2"),Ze=r("a"),vo=r("span"),f(Bt.$$.fragment),Rd=s(),bo=r("span"),Nd=n("Other Resources"),fr=s(),j=r("ul"),yo=r("li"),Zt=r("a"),kd=n("ONNX Runtime github"),Sd=s(),To=r("li"),Jt=r("a"),qd=n("Torch ORT github"),Dd=s(),Oo=r("li"),Kt=r("a"),Ad=n("Download ONNX Runtime stable versions"),Pd=s(),pa=r("li"),zd=n("Blog posts"),Qt=r("ul"),wo=r("li"),ea=r("a"),Cd=n("Accelerate PyTorch transformer model training with ONNX Runtime \u2013 a deep dive"),Id=s(),$o=r("li"),ta=r("a"),Fd=n("ONNX Runtime Training Technical Deep Dive"),_r=s(),W=r("p"),Ld=n("If you have any problems or questions regarding "),xo=r("code"),Xd=n("ORTTrainer"),Ud=n(", please file an issue with "),aa=r("a"),Md=n("Optimum Github"),Hd=n(`
or discuss with us on `),na=r("a"),jd=n("HuggingFace\u2019s community forum"),Wd=n(", cheers \u{1F917} !"),this.h()},l(t){const d=xu('[data-svelte="svelte-1phssyn"]',document.head);u=i(d,"META",{name:!0,content:!0}),d.forEach(a),$=m(t),h=i(t,"H1",{class:!0});var oa=l(h);O=i(oa,"A",{id:!0,class:!0,href:!0});var Eo=l(O);X=i(Eo,"SPAN",{});var Ro=l(X);_(x.$$.fragment,Ro),Ro.forEach(a),Eo.forEach(a),S=m(oa),Y=i(oa,"SPAN",{});var No=l(Y);jr=o(No,"Trainer"),No.forEach(a),oa.forEach(a),So=m(t),E=i(t,"P",{});var I=l(E);Wr=o(I,"The "),ka=i(I,"CODE",{});var Yd=l(ka);Gr=o(Yd,"ORTTrainer"),Yd.forEach(a),Vr=o(I," and "),Sa=i(I,"CODE",{});var Bd=l(Sa);Yr=o(Bd,"ORTSeq2SeqTrainer"),Bd.forEach(a),Br=o(I," classes provide APIs for training PyTorch models with "),qa=i(I,"STRONG",{});var Zd=l(qa);Qe=i(Zd,"A",{href:!0,rel:!0});var Jd=l(Qe);Zr=o(Jd,"ONNX Runtime (ORT)"),Jd.forEach(a),Zd.forEach(a),Jr=o(I,`.
Taking ONNX Runtime as backend, `),Da=i(I,"CODE",{});var Kd=l(Da);Kr=o(Kd,"ORTTrainer"),Kd.forEach(a),Qr=o(I," and "),Aa=i(I,"CODE",{});var Qd=l(Aa);ei=o(Qd,"ORTSeq2SeqTrainer"),Qd.forEach(a),ti=o(I,` optimize the computation graph and the memory usage. They also support
mixed precision training implemented by ORT, as well as distributed training on multiple GPUs. With them, you will be able to achieve
`),Pa=i(I,"STRONG",{});var ep=l(Pa);ai=o(ep,"lower latency, higher throughput, and larger maximum batch size"),ep.forEach(a),ni=o(I," while training large transformers models."),I.forEach(a),qo=m(t),re=i(t,"H2",{class:!0});var vr=l(re);Ne=i(vr,"A",{id:!0,class:!0,href:!0});var tp=l(Ne);za=i(tp,"SPAN",{});var ap=l(za);_(et.$$.fragment,ap),ap.forEach(a),tp.forEach(a),oi=m(vr),Ca=i(vr,"SPAN",{});var np=l(Ca);ri=o(np,"Prerequisite"),np.forEach(a),vr.forEach(a),Do=m(t),ra=i(t,"P",{});var op=l(ra);ii=o(op,"To use ONNX Runtime for training, you need a machine with at least one NVIDIA or AMD GPU."),op.forEach(a),Ao=m(t),J=i(t,"P",{});var ca=l(J);li=o(ca,"To use "),Ia=i(ca,"CODE",{});var rp=l(Ia);si=o(rp,"ORTTrainer"),rp.forEach(a),mi=o(ca," or "),Fa=i(ca,"CODE",{});var ip=l(Fa);di=o(ip,"ORTSeq2SeqTrainer"),ip.forEach(a),pi=o(ca,", you need to install ONNX Runtime Training module and Optimum."),ca.forEach(a),Po=m(t),ie=i(t,"H3",{class:!0});var br=l(ie);ke=i(br,"A",{id:!0,class:!0,href:!0});var lp=l(ke);La=i(lp,"SPAN",{});var sp=l(La);_(tt.$$.fragment,sp),sp.forEach(a),lp.forEach(a),ci=m(br),Xa=i(br,"SPAN",{});var mp=l(Xa);ui=o(mp,"Install ONNX Runtime"),mp.forEach(a),br.forEach(a),zo=m(t),K=i(t,"P",{});var ua=l(K);hi=o(ua,"To set up the environment, we "),Ua=i(ua,"STRONG",{});var dp=l(Ua);fi=o(dp,"strongly recommend"),dp.forEach(a),_i=o(ua,` you install the dependencies with Docker to ensure that the versions are correct and well
configured. You can find dockerfiles with various combinations `),at=i(ua,"A",{href:!0,rel:!0});var pp=l(at);gi=o(pp,"here"),pp.forEach(a),vi=o(ua,"."),ua.forEach(a),Co=m(t),Se=i(t,"P",{});var yr=l(Se);bi=o(yr,"For example, if you want to install "),nt=i(yr,"A",{href:!0,rel:!0});var cp=l(nt);yi=o(cp,"onnxruntime-training 1.12.0"),cp.forEach(a),Ti=o(yr," via Dockerfile:"),yr.forEach(a),Io=m(t),_(ot.$$.fragment,t),Fo=m(t),ia=i(t,"P",{});var up=l(ia);Oi=o(up,"If you want to install the dependencies beyond in a local Python environment. You can pip install them once you have CUDA 11.3 and cuDNN 8 well installed."),up.forEach(a),Lo=m(t),_(rt.$$.fragment,t),Xo=m(t),la=i(t,"P",{});var hp=l(la);wi=o(hp,"And run post-installation configuration:"),hp.forEach(a),Uo=m(t),_(it.$$.fragment,t),Mo=m(t),le=i(t,"H3",{class:!0});var Tr=l(le);qe=i(Tr,"A",{id:!0,class:!0,href:!0});var fp=l(qe);Ma=i(fp,"SPAN",{});var _p=l(Ma);_(lt.$$.fragment,_p),_p.forEach(a),fp.forEach(a),$i=m(Tr),Ha=i(Tr,"SPAN",{});var gp=l(Ha);xi=o(gp,"Install Optimum"),gp.forEach(a),Tr.forEach(a),Ho=m(t),sa=i(t,"P",{});var vp=l(sa);Ei=o(vp,"You can install Optimum via pypi:"),vp.forEach(a),jo=m(t),_(st.$$.fragment,t),Wo=m(t),ma=i(t,"P",{});var bp=l(ma);Ri=o(bp,"Or install from source:"),bp.forEach(a),Go=m(t),_(mt.$$.fragment,t),Vo=m(t),De=i(t,"P",{});var Or=l(De);Ni=o(Or,`This command installs the current main dev version of Optimum, which could include latest developments(new features, bug fixes). However, the
main version might not be very stable. If you run into any problem, please open an `),dt=i(Or,"A",{href:!0,rel:!0});var yp=l(dt);ki=o(yp,"issue"),yp.forEach(a),Si=o(Or,` so
that we can fix it as soon as possible.`),Or.forEach(a),Yo=m(t),se=i(t,"H2",{class:!0});var wr=l(se);Ae=i(wr,"A",{id:!0,class:!0,href:!0});var Tp=l(Ae);ja=i(Tp,"SPAN",{});var Op=l(ja);_(pt.$$.fragment,Op),Op.forEach(a),Tp.forEach(a),qi=m(wr),Wa=i(wr,"SPAN",{});var wp=l(Wa);Di=o(wp,"ORTTrainer"),wp.forEach(a),wr.forEach(a),Bo=m(t),R=i(t,"P",{});var F=l(R);Ai=o(F,"The "),Ga=i(F,"CODE",{});var $p=l(Ga);Pi=o($p,"ORTTrainer"),$p.forEach(a),zi=o(F," class inherits the "),ct=i(F,"A",{href:!0,rel:!0});var xp=l(ct);Va=i(xp,"CODE",{});var Ep=l(Va);Ci=o(Ep,"Trainer"),Ep.forEach(a),xp.forEach(a),Ii=o(F,`
of Transformers. You can easily adapt the codes by replacing `),Ya=i(F,"CODE",{});var Rp=l(Ya);Fi=o(Rp,"Trainer"),Rp.forEach(a),Li=o(F," of transformers with "),Ba=i(F,"CODE",{});var Np=l(Ba);Xi=o(Np,"ORTTrainer"),Np.forEach(a),Ui=o(F,` to take advantage of the acceleration
empowered by ONNX Runtime. Here is an example of how to use `),Za=i(F,"CODE",{});var kp=l(Za);Mi=o(kp,"ORTTrainer"),kp.forEach(a),Hi=o(F," compared with "),Ja=i(F,"CODE",{});var Sp=l(Ja);ji=o(Sp,"Trainer"),Sp.forEach(a),Wi=o(F,":"),F.forEach(a),Zo=m(t),_(ut.$$.fragment,t),Jo=m(t),Pe=i(t,"P",{});var $r=l(Pe);Gi=o($r,"Check out more detailed "),ht=i($r,"A",{href:!0,rel:!0});var qp=l(ht);Vi=o(qp,"example scripts"),qp.forEach(a),Yi=o($r," in the optimum repository."),$r.forEach(a),Ko=m(t),T=i(t,"DIV",{class:!0});var w=l(T);_(ft.$$.fragment,w),Bi=m(w),Ka=i(w,"P",{});var Dp=l(Ka);Zi=o(Dp,"ORTTrainer is a simple but feature-complete training and eval loop for ONNX Runtime, optimized for \u{1F917} Transformers."),Dp.forEach(a),Ji=m(w),Qa=i(w,"P",{});var Ap=l(Qa);Ki=o(Ap,"Important attributes:"),Ap.forEach(a),Qi=m(w),U=i(w,"UL",{});var ae=l(U);ze=i(ae,"LI",{});var ko=l(ze);en=i(ko,"STRONG",{});var Pp=l(en);el=o(Pp,"model"),Pp.forEach(a),tl=o(ko," \u2014 Always points to the core model. If using a transformers model, it will be a "),_t=i(ko,"A",{href:!0,rel:!0});var zp=l(_t);al=o(zp,"PreTrainedModel"),zp.forEach(a),nl=o(ko,`
subclass.`),ko.forEach(a),ol=m(ae),N=i(ae,"LI",{});var A=l(N);tn=i(A,"STRONG",{});var Cp=l(tn);rl=o(Cp,"model_wrapped"),Cp.forEach(a),il=o(A,` \u2014 Always points to the most external model in case one or more other modules wrap the
original model. This is the model that should be used for the forward pass. For example, under `),an=i(A,"CODE",{});var Ip=l(an);ll=o(Ip,"DeepSpeed"),Ip.forEach(a),sl=o(A,`,
the inner model is first wrapped in `),nn=i(A,"CODE",{});var Fp=l(nn);ml=o(Fp,"ORTModule"),Fp.forEach(a),dl=o(A," and then in "),on=i(A,"CODE",{});var Lp=l(on);pl=o(Lp,"DeepSpeed"),Lp.forEach(a),cl=o(A,` and then again in
`),rn=i(A,"CODE",{});var Xp=l(rn);ul=o(Xp,"torch.nn.DistributedDataParallel"),Xp.forEach(a),hl=o(A,". If the inner model hasn\u2019t been wrapped, then "),ln=i(A,"CODE",{});var Up=l(ln);fl=o(Up,"self.model_wrapped"),Up.forEach(a),_l=o(A,` is the
same as `),sn=i(A,"CODE",{});var Mp=l(sn);gl=o(Mp,"self.model"),Mp.forEach(a),vl=o(A,"."),A.forEach(a),bl=m(ae),da=i(ae,"LI",{});var Gd=l(da);mn=i(Gd,"STRONG",{});var Hp=l(mn);yl=o(Hp,"is_model_parallel"),Hp.forEach(a),Tl=o(Gd,` \u2014 Whether or not a model has been switched to a model parallel mode (different from
data parallelism, this means some of the model layers are split on different GPUs).`),Gd.forEach(a),Ol=m(ae),M=i(ae,"LI",{});var $e=l(M);dn=i($e,"STRONG",{});var jp=l(dn);wl=o(jp,"place_model_on_device"),jp.forEach(a),$l=o($e,` \u2014 Whether or not to automatically place the model on the device - it will be set
to `),pn=i($e,"CODE",{});var Wp=l(pn);xl=o(Wp,"False"),Wp.forEach(a),El=o($e,` if model parallel or deepspeed is used, or if the default
`),cn=i($e,"CODE",{});var Gp=l(cn);Rl=o(Gp,"ORTTrainingArguments.place_model_on_device"),Gp.forEach(a),Nl=o($e," is overridden to return "),un=i($e,"CODE",{});var Vp=l(un);kl=o(Vp,"False"),Vp.forEach(a),Sl=o($e," ."),$e.forEach(a),ql=m(ae),H=i(ae,"LI",{});var xe=l(H);hn=i(xe,"STRONG",{});var Yp=l(hn);Dl=o(Yp,"is_in_train"),Yp.forEach(a),Al=o(xe," \u2014 Whether or not a model is currently running "),fn=i(xe,"CODE",{});var Bp=l(fn);Pl=o(Bp,"train"),Bp.forEach(a),zl=o(xe," (e.g. when "),_n=i(xe,"CODE",{});var Zp=l(_n);Cl=o(Zp,"evaluate"),Zp.forEach(a),Il=o(xe,` is called while
in `),gn=i(xe,"CODE",{});var Jp=l(gn);Fl=o(Jp,"train"),Jp.forEach(a),Ll=o(xe,")"),xe.forEach(a),ae.forEach(a),Xl=m(w),Ce=i(w,"DIV",{class:!0});var xr=l(Ce);_(gt.$$.fragment,xr),Ul=m(xr),vn=i(xr,"P",{});var Kp=l(vn);Ml=o(Kp,`How the loss is computed by ORTTrainer. By default, all models return the loss in the first element.
Subclass and override for custom behavior.`),Kp.forEach(a),xr.forEach(a),Hl=m(w),Q=i(w,"DIV",{class:!0});var ha=l(Q);_(vt.$$.fragment,ha),jl=m(ha),bn=i(ha,"P",{});var Qp=l(bn);Wl=o(Qp,"Setup the optimizer."),Qp.forEach(a),Gl=m(ha),bt=i(ha,"P",{});var Er=l(bt);Vl=o(Er,`We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
ORTTrainer\u2019s init through `),yn=i(Er,"CODE",{});var ec=l(yn);Yl=o(ec,"optimizers"),ec.forEach(a),Bl=o(Er,", or subclass and override this method in a subclass."),Er.forEach(a),ha.forEach(a),Zl=m(w),Ie=i(w,"DIV",{class:!0});var Rr=l(Ie);_(yt.$$.fragment,Rr),Jl=m(Rr),Tn=i(Rr,"P",{});var tc=l(Tn);Kl=o(tc,"Run evaluation with ONNX Runtime or PyTorch backend and returns metrics."),tc.forEach(a),Rr.forEach(a),Ql=m(w),ee=i(w,"DIV",{class:!0});var fa=l(ee);_(Tt.$$.fragment,fa),es=m(fa),me=i(fa,"P",{});var _a=l(me);ts=o(_a,"Prediction/evaluation loop, shared by "),On=i(_a,"CODE",{});var ac=l(On);as=o(ac,"ORTTrainer.evaluate()"),ac.forEach(a),ns=o(_a," and "),wn=i(_a,"CODE",{});var nc=l(wn);os=o(nc,"ORTTrainer.predict()"),nc.forEach(a),rs=o(_a,"."),_a.forEach(a),is=m(fa),$n=i(fa,"P",{});var oc=l($n);ls=o(oc,"Works both with or without labels."),oc.forEach(a),fa.forEach(a),ss=m(w),Fe=i(w,"DIV",{class:!0});var Nr=l(Fe);_(Ot.$$.fragment,Nr),ms=m(Nr),wt=i(Nr,"P",{});var kr=l(wt);ds=o(kr,"Returns the optimizer class and optimizer parameters implemented in ONNX Runtime based on "),xn=i(kr,"CODE",{});var rc=l(xn);ps=o(rc,"ORTTrainingArguments"),rc.forEach(a),cs=o(kr,"."),kr.forEach(a),Nr.forEach(a),us=m(w),q=i(w,"DIV",{class:!0});var G=l(q);_($t.$$.fragment,G),hs=m(G),En=i(G,"P",{});var ic=l(En);fs=o(ic,"Run prediction and returns predictions and potential metrics."),ic.forEach(a),_s=m(G),xt=i(G,"P",{});var Sr=l(xt);gs=o(Sr,`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Rn=i(Sr,"CODE",{});var lc=l(Rn);vs=o(lc,"evaluate()"),lc.forEach(a),bs=o(Sr,"."),Sr.forEach(a),ys=m(G),_(Le.$$.fragment,G),Ts=m(G),Et=i(G,"P",{});var qr=l(Et);Os=o(qr,"Returns: "),Nn=i(qr,"EM",{});var sc=l(Nn);ws=o(sc,"NamedTuple"),sc.forEach(a),$s=o(qr," A namedtuple with the following keys:"),qr.forEach(a),xs=m(G),de=i(G,"UL",{});var ga=l(de);pe=i(ga,"LI",{});var va=l(pe);Es=o(va,"predictions ("),kn=i(va,"CODE",{});var mc=l(kn);Rs=o(mc,"np.ndarray"),mc.forEach(a),Ns=o(va,"): The predictions on "),Sn=i(va,"CODE",{});var dc=l(Sn);ks=o(dc,"test_dataset"),dc.forEach(a),Ss=o(va,"."),va.forEach(a),qs=m(ga),ce=i(ga,"LI",{});var ba=l(ce);Ds=o(ba,"label_ids ("),qn=i(ba,"CODE",{});var pc=l(qn);As=o(pc,"np.ndarray"),pc.forEach(a),Ps=o(ba,", "),Dn=i(ba,"EM",{});var cc=l(Dn);zs=o(cc,"optional"),cc.forEach(a),Cs=o(ba,"): The labels (if the dataset contained some)."),ba.forEach(a),Is=m(ga),ue=i(ga,"LI",{});var ya=l(ue);Fs=o(ya,"metrics ("),An=i(ya,"CODE",{});var uc=l(An);Ls=o(uc,"Dict[str, float]"),uc.forEach(a),Xs=o(ya,", "),Pn=i(ya,"EM",{});var hc=l(Pn);Us=o(hc,"optional"),hc.forEach(a),Ms=o(ya,`): The potential dictionary of metrics (if the dataset contained
labels).`),ya.forEach(a),ga.forEach(a),G.forEach(a),Hs=m(w),te=i(w,"DIV",{class:!0});var Ta=l(te);_(Rt.$$.fragment,Ta),js=m(Ta),he=i(Ta,"P",{});var Oa=l(he);Ws=o(Oa,"Prediction/evaluation loop, shared by "),zn=i(Oa,"CODE",{});var fc=l(zn);Gs=o(fc,"ORTTrainer.evaluate()"),fc.forEach(a),Vs=o(Oa," and "),Cn=i(Oa,"CODE",{});var _c=l(Cn);Ys=o(_c,"ORTTrainer.predict()"),_c.forEach(a),Bs=o(Oa,"."),Oa.forEach(a),Zs=m(Ta),In=i(Ta,"P",{});var gc=l(In);Js=o(gc,"Works both with or without labels."),gc.forEach(a),Ta.forEach(a),Ks=m(w),Xe=i(w,"DIV",{class:!0});var Dr=l(Xe);_(Nt.$$.fragment,Dr),Qs=m(Dr),fe=i(Dr,"P",{});var wa=l(fe);em=o(wa,"Perform an evaluation step on "),Fn=i(wa,"CODE",{});var vc=l(Fn);tm=o(vc,"model"),vc.forEach(a),am=o(wa," using "),Ln=i(wa,"CODE",{});var bc=l(Ln);nm=o(bc,"inputs"),bc.forEach(a),om=o(wa,"."),wa.forEach(a),Dr.forEach(a),rm=m(w),Ue=i(w,"DIV",{class:!0});var Ar=l(Ue);_(kt.$$.fragment,Ar),im=m(Ar),Xn=i(Ar,"P",{});var yc=l(Xn);lm=o(yc,"Main entry point for training with ONNX Runtime accelerator."),yc.forEach(a),Ar.forEach(a),w.forEach(a),Qo=m(t),_e=i(t,"H2",{class:!0});var Pr=l(_e);Me=i(Pr,"A",{id:!0,class:!0,href:!0});var Tc=l(Me);Un=i(Tc,"SPAN",{});var Oc=l(Un);_(St.$$.fragment,Oc),Oc.forEach(a),Tc.forEach(a),sm=m(Pr),Mn=i(Pr,"SPAN",{});var wc=l(Mn);mm=o(wc,"ORTSeq2SeqTrainer"),wc.forEach(a),Pr.forEach(a),er=m(t),k=i(t,"P",{});var L=l(k);dm=o(L,"The "),Hn=i(L,"CODE",{});var $c=l(Hn);pm=o($c,"ORTSeq2SeqTrainer"),$c.forEach(a),cm=o(L," class is similar to the "),qt=i(L,"A",{href:!0,rel:!0});var xc=l(qt);jn=i(xc,"CODE",{});var Ec=l(jn);um=o(Ec,"Seq2SeqTrainer"),Ec.forEach(a),xc.forEach(a),hm=o(L,`
of Transformers. You can easily adapt the codes by replacing `),Wn=i(L,"CODE",{});var Rc=l(Wn);fm=o(Rc,"Seq2SeqTrainer"),Rc.forEach(a),_m=o(L," of transformers with "),Gn=i(L,"CODE",{});var Nc=l(Gn);gm=o(Nc,"ORTSeq2SeqTrainer"),Nc.forEach(a),vm=o(L,` to take advantage of the acceleration
empowered by ONNX Runtime. Here is an example of how to use `),Vn=i(L,"CODE",{});var kc=l(Vn);bm=o(kc,"ORTSeq2SeqTrainer"),kc.forEach(a),ym=o(L," compared with "),Yn=i(L,"CODE",{});var Sc=l(Yn);Tm=o(Sc,"Seq2SeqTrainer"),Sc.forEach(a),Om=o(L,":"),L.forEach(a),tr=m(t),_(Dt.$$.fragment,t),ar=m(t),He=i(t,"P",{});var zr=l(He);wm=o(zr,"Check out more detailed "),At=i(zr,"A",{href:!0,rel:!0});var qc=l(At);$m=o(qc,"example scripts"),qc.forEach(a),xm=o(zr," in the optimum repository."),zr.forEach(a),nr=m(t),B=i(t,"DIV",{class:!0});var $a=l(B);_(Pt.$$.fragment,$a),Em=m($a),je=i($a,"DIV",{class:!0});var Cr=l(je);_(zt.$$.fragment,Cr),Rm=m(Cr),Bn=i(Cr,"P",{});var Dc=l(Bn);Nm=o(Dc,"Run evaluation with ONNX Runtime or PyTorch backend and returns metrics."),Dc.forEach(a),Cr.forEach(a),km=m($a),D=i($a,"DIV",{class:!0});var V=l(D);_(Ct.$$.fragment,V),Sm=m(V),Zn=i(V,"P",{});var Ac=l(Zn);qm=o(Ac,"Run prediction and returns predictions and potential metrics."),Ac.forEach(a),Dm=m(V),It=i(V,"P",{});var Ir=l(It);Am=o(Ir,`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Jn=i(Ir,"CODE",{});var Pc=l(Jn);Pm=o(Pc,"evaluate()"),Pc.forEach(a),zm=o(Ir,"."),Ir.forEach(a),Cm=m(V),_(We.$$.fragment,V),Im=m(V),Ft=i(V,"P",{});var Fr=l(Ft);Fm=o(Fr,"Returns: "),Kn=i(Fr,"EM",{});var zc=l(Kn);Lm=o(zc,"NamedTuple"),zc.forEach(a),Xm=o(Fr," A namedtuple with the following keys:"),Fr.forEach(a),Um=m(V),ge=i(V,"UL",{});var xa=l(ge);ve=i(xa,"LI",{});var Ea=l(ve);Mm=o(Ea,"predictions ("),Qn=i(Ea,"CODE",{});var Cc=l(Qn);Hm=o(Cc,"np.ndarray"),Cc.forEach(a),jm=o(Ea,"): The predictions on "),eo=i(Ea,"CODE",{});var Ic=l(eo);Wm=o(Ic,"test_dataset"),Ic.forEach(a),Gm=o(Ea,"."),Ea.forEach(a),Vm=m(xa),be=i(xa,"LI",{});var Ra=l(be);Ym=o(Ra,"label_ids ("),to=i(Ra,"CODE",{});var Fc=l(to);Bm=o(Fc,"np.ndarray"),Fc.forEach(a),Zm=o(Ra,", "),ao=i(Ra,"EM",{});var Lc=l(ao);Jm=o(Lc,"optional"),Lc.forEach(a),Km=o(Ra,"): The labels (if the dataset contained some)."),Ra.forEach(a),Qm=m(xa),ye=i(xa,"LI",{});var Na=l(ye);ed=o(Na,"metrics ("),no=i(Na,"CODE",{});var Xc=l(no);td=o(Xc,"Dict[str, float]"),Xc.forEach(a),ad=o(Na,", "),oo=i(Na,"EM",{});var Uc=l(oo);nd=o(Uc,"optional"),Uc.forEach(a),od=o(Na,`): The potential dictionary of metrics (if the dataset contained
labels).`),Na.forEach(a),xa.forEach(a),V.forEach(a),$a.forEach(a),or=m(t),Te=i(t,"H2",{class:!0});var Lr=l(Te);Ge=i(Lr,"A",{id:!0,class:!0,href:!0});var Mc=l(Ge);ro=i(Mc,"SPAN",{});var Hc=l(ro);_(Lt.$$.fragment,Hc),Hc.forEach(a),Mc.forEach(a),rd=m(Lr),io=i(Lr,"SPAN",{});var jc=l(io);id=o(jc,"ORTTrainingArguments"),jc.forEach(a),Lr.forEach(a),rr=m(t),z=i(t,"P",{});var ne=l(z);ld=o(ne,"The "),lo=i(ne,"CODE",{});var Wc=l(lo);sd=o(Wc,"ORTTrainingArguments"),Wc.forEach(a),md=o(ne," class inherits the "),Xt=i(ne,"A",{href:!0,rel:!0});var Gc=l(Xt);so=i(Gc,"CODE",{});var Vc=l(so);dd=o(Vc,"TrainingArguments"),Vc.forEach(a),Gc.forEach(a),pd=o(ne,`
class in Transformers. Besides the optimizers implemented in Transformers, it allows you to use the optimizers implemented in ONNX Runtime.
Replace `),mo=i(ne,"CODE",{});var Yc=l(mo);cd=o(Yc,"Seq2SeqTrainingArguments"),Yc.forEach(a),ud=o(ne," with "),po=i(ne,"CODE",{});var Bc=l(po);hd=o(Bc,"ORTSeq2SeqTrainingArguments"),Bc.forEach(a),fd=o(ne,":"),ne.forEach(a),ir=m(t),_(Ut.$$.fragment,t),lr=m(t),Mt=i(t,"DIV",{class:!0});var Zc=l(Mt);_(Ht.$$.fragment,Zc),Zc.forEach(a),sr=m(t),_(Ve.$$.fragment,t),mr=m(t),Oe=i(t,"H2",{class:!0});var Xr=l(Oe);Ye=i(Xr,"A",{id:!0,class:!0,href:!0});var Jc=l(Ye);co=i(Jc,"SPAN",{});var Kc=l(co);_(jt.$$.fragment,Kc),Kc.forEach(a),Jc.forEach(a),_d=m(Xr),uo=i(Xr,"SPAN",{});var Qc=l(uo);gd=o(Qc,"ORTSeq2SeqTrainingArguments"),Qc.forEach(a),Xr.forEach(a),dr=m(t),C=i(t,"P",{});var oe=l(C);vd=o(oe,"The "),ho=i(oe,"CODE",{});var eu=l(ho);bd=o(eu,"ORTSeq2SeqTrainingArguments"),eu.forEach(a),yd=o(oe," class inherits the "),Wt=i(oe,"A",{href:!0,rel:!0});var tu=l(Wt);fo=i(tu,"CODE",{});var au=l(fo);Td=o(au,"Seq2SeqTrainingArguments"),au.forEach(a),tu.forEach(a),Od=o(oe,`
class in Transformers. Besides the optimizers implemented in Transformers, it allows you to use the optimizers implemented in ONNX Runtime.
Replace `),_o=i(oe,"CODE",{});var nu=l(_o);wd=o(nu,"Seq2SeqTrainingArguments"),nu.forEach(a),$d=o(oe," with "),go=i(oe,"CODE",{});var ou=l(go);xd=o(ou,"ORTSeq2SeqTrainingArguments"),ou.forEach(a),Ed=o(oe,":"),oe.forEach(a),pr=m(t),_(Gt.$$.fragment,t),cr=m(t),Vt=i(t,"DIV",{class:!0});var ru=l(Vt);_(Yt.$$.fragment,ru),ru.forEach(a),ur=m(t),_(Be.$$.fragment,t),hr=m(t),we=i(t,"H2",{class:!0});var Ur=l(we);Ze=i(Ur,"A",{id:!0,class:!0,href:!0});var iu=l(Ze);vo=i(iu,"SPAN",{});var lu=l(vo);_(Bt.$$.fragment,lu),lu.forEach(a),iu.forEach(a),Rd=m(Ur),bo=i(Ur,"SPAN",{});var su=l(bo);Nd=o(su,"Other Resources"),su.forEach(a),Ur.forEach(a),fr=m(t),j=i(t,"UL",{});var Je=l(j);yo=i(Je,"LI",{});var mu=l(yo);Zt=i(mu,"A",{href:!0,rel:!0});var du=l(Zt);kd=o(du,"ONNX Runtime github"),du.forEach(a),mu.forEach(a),Sd=m(Je),To=i(Je,"LI",{});var pu=l(To);Jt=i(pu,"A",{href:!0,rel:!0});var cu=l(Jt);qd=o(cu,"Torch ORT github"),cu.forEach(a),pu.forEach(a),Dd=m(Je),Oo=i(Je,"LI",{});var uu=l(Oo);Kt=i(uu,"A",{href:!0,rel:!0});var hu=l(Kt);Ad=o(hu,"Download ONNX Runtime stable versions"),hu.forEach(a),uu.forEach(a),Pd=m(Je),pa=i(Je,"LI",{});var Vd=l(pa);zd=o(Vd,"Blog posts"),Qt=i(Vd,"UL",{});var Mr=l(Qt);wo=i(Mr,"LI",{});var fu=l(wo);ea=i(fu,"A",{href:!0,rel:!0});var _u=l(ea);Cd=o(_u,"Accelerate PyTorch transformer model training with ONNX Runtime \u2013 a deep dive"),_u.forEach(a),fu.forEach(a),Id=m(Mr),$o=i(Mr,"LI",{});var gu=l($o);ta=i(gu,"A",{href:!0,rel:!0});var vu=l(ta);Fd=o(vu,"ONNX Runtime Training Technical Deep Dive"),vu.forEach(a),gu.forEach(a),Mr.forEach(a),Vd.forEach(a),Je.forEach(a),_r=m(t),W=i(t,"P",{});var Ke=l(W);Ld=o(Ke,"If you have any problems or questions regarding "),xo=i(Ke,"CODE",{});var bu=l(xo);Xd=o(bu,"ORTTrainer"),bu.forEach(a),Ud=o(Ke,", please file an issue with "),aa=i(Ke,"A",{href:!0,rel:!0});var yu=l(aa);Md=o(yu,"Optimum Github"),yu.forEach(a),Hd=o(Ke,`
or discuss with us on `),na=i(Ke,"A",{href:!0,rel:!0});var Tu=l(na);jd=o(Tu,"HuggingFace\u2019s community forum"),Tu.forEach(a),Wd=o(Ke,", cheers \u{1F917} !"),Ke.forEach(a),this.h()},h(){p(u,"name","hf:doc:metadata"),p(u,"content",JSON.stringify(Du)),p(O,"id","trainer"),p(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(O,"href","#trainer"),p(h,"class","relative group"),p(Qe,"href","https://onnxruntime.ai/"),p(Qe,"rel","nofollow"),p(Ne,"id","prerequisite"),p(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ne,"href","#prerequisite"),p(re,"class","relative group"),p(ke,"id","install-onnx-runtime"),p(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ke,"href","#install-onnx-runtime"),p(ie,"class","relative group"),p(at,"href","https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training/docker"),p(at,"rel","nofollow"),p(nt,"href","https://github.com/huggingface/optimum/blob/main/examples/onnxruntime/training/docker/Dockerfile-ort1.12.0-cu113"),p(nt,"rel","nofollow"),p(qe,"id","install-optimum"),p(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(qe,"href","#install-optimum"),p(le,"class","relative group"),p(dt,"href","https://github.com/huggingface/optimum/issues"),p(dt,"rel","nofollow"),p(Ae,"id","optimum.onnxruntime.ORTTrainer"),p(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ae,"href","#optimum.onnxruntime.ORTTrainer"),p(se,"class","relative group"),p(ct,"href","https://huggingface.co/docs/transformers/main/en/main_classes/trainer#trainer"),p(ct,"rel","nofollow"),p(ht,"href","https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training"),p(ht,"rel","nofollow"),p(_t,"href","https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),p(_t,"rel","nofollow"),p(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Me,"id","optimum.onnxruntime.ORTSeq2SeqTrainer"),p(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Me,"href","#optimum.onnxruntime.ORTSeq2SeqTrainer"),p(_e,"class","relative group"),p(qt,"href","https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainer"),p(qt,"rel","nofollow"),p(At,"href","https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training"),p(At,"rel","nofollow"),p(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ge,"id","optimum.onnxruntime.ORTTrainingArguments"),p(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ge,"href","#optimum.onnxruntime.ORTTrainingArguments"),p(Te,"class","relative group"),p(Xt,"href","https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments"),p(Xt,"rel","nofollow"),p(Mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ye,"id","optimum.onnxruntime.ORTSeq2SeqTrainingArguments"),p(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ye,"href","#optimum.onnxruntime.ORTSeq2SeqTrainingArguments"),p(Oe,"class","relative group"),p(Wt,"href","https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments"),p(Wt,"rel","nofollow"),p(Vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ze,"id","other-resources"),p(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ze,"href","#other-resources"),p(we,"class","relative group"),p(Zt,"href","https://github.com/microsoft/onnxruntime"),p(Zt,"rel","nofollow"),p(Jt,"href","https://github.com/pytorch/ort"),p(Jt,"rel","nofollow"),p(Kt,"href","https://download.onnxruntime.ai/"),p(Kt,"rel","nofollow"),p(ea,"href","https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/accelerate-pytorch-transformer-model-training-with-onnx-runtime/ba-p/2540471"),p(ea,"rel","nofollow"),p(ta,"href","https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/onnx-runtime-training-technical-deep-dive/ba-p/1398310"),p(ta,"rel","nofollow"),p(aa,"href","https://github.com/huggingface/optimum"),p(aa,"rel","nofollow"),p(na,"href","https://discuss.huggingface.co/c/optimum/"),p(na,"rel","nofollow")},m(t,d){e(document.head,u),c(t,$,d),c(t,h,d),e(h,O),e(O,X),g(x,X,null),e(h,S),e(h,Y),e(Y,jr),c(t,So,d),c(t,E,d),e(E,Wr),e(E,ka),e(ka,Gr),e(E,Vr),e(E,Sa),e(Sa,Yr),e(E,Br),e(E,qa),e(qa,Qe),e(Qe,Zr),e(E,Jr),e(E,Da),e(Da,Kr),e(E,Qr),e(E,Aa),e(Aa,ei),e(E,ti),e(E,Pa),e(Pa,ai),e(E,ni),c(t,qo,d),c(t,re,d),e(re,Ne),e(Ne,za),g(et,za,null),e(re,oi),e(re,Ca),e(Ca,ri),c(t,Do,d),c(t,ra,d),e(ra,ii),c(t,Ao,d),c(t,J,d),e(J,li),e(J,Ia),e(Ia,si),e(J,mi),e(J,Fa),e(Fa,di),e(J,pi),c(t,Po,d),c(t,ie,d),e(ie,ke),e(ke,La),g(tt,La,null),e(ie,ci),e(ie,Xa),e(Xa,ui),c(t,zo,d),c(t,K,d),e(K,hi),e(K,Ua),e(Ua,fi),e(K,_i),e(K,at),e(at,gi),e(K,vi),c(t,Co,d),c(t,Se,d),e(Se,bi),e(Se,nt),e(nt,yi),e(Se,Ti),c(t,Io,d),g(ot,t,d),c(t,Fo,d),c(t,ia,d),e(ia,Oi),c(t,Lo,d),g(rt,t,d),c(t,Xo,d),c(t,la,d),e(la,wi),c(t,Uo,d),g(it,t,d),c(t,Mo,d),c(t,le,d),e(le,qe),e(qe,Ma),g(lt,Ma,null),e(le,$i),e(le,Ha),e(Ha,xi),c(t,Ho,d),c(t,sa,d),e(sa,Ei),c(t,jo,d),g(st,t,d),c(t,Wo,d),c(t,ma,d),e(ma,Ri),c(t,Go,d),g(mt,t,d),c(t,Vo,d),c(t,De,d),e(De,Ni),e(De,dt),e(dt,ki),e(De,Si),c(t,Yo,d),c(t,se,d),e(se,Ae),e(Ae,ja),g(pt,ja,null),e(se,qi),e(se,Wa),e(Wa,Di),c(t,Bo,d),c(t,R,d),e(R,Ai),e(R,Ga),e(Ga,Pi),e(R,zi),e(R,ct),e(ct,Va),e(Va,Ci),e(R,Ii),e(R,Ya),e(Ya,Fi),e(R,Li),e(R,Ba),e(Ba,Xi),e(R,Ui),e(R,Za),e(Za,Mi),e(R,Hi),e(R,Ja),e(Ja,ji),e(R,Wi),c(t,Zo,d),g(ut,t,d),c(t,Jo,d),c(t,Pe,d),e(Pe,Gi),e(Pe,ht),e(ht,Vi),e(Pe,Yi),c(t,Ko,d),c(t,T,d),g(ft,T,null),e(T,Bi),e(T,Ka),e(Ka,Zi),e(T,Ji),e(T,Qa),e(Qa,Ki),e(T,Qi),e(T,U),e(U,ze),e(ze,en),e(en,el),e(ze,tl),e(ze,_t),e(_t,al),e(ze,nl),e(U,ol),e(U,N),e(N,tn),e(tn,rl),e(N,il),e(N,an),e(an,ll),e(N,sl),e(N,nn),e(nn,ml),e(N,dl),e(N,on),e(on,pl),e(N,cl),e(N,rn),e(rn,ul),e(N,hl),e(N,ln),e(ln,fl),e(N,_l),e(N,sn),e(sn,gl),e(N,vl),e(U,bl),e(U,da),e(da,mn),e(mn,yl),e(da,Tl),e(U,Ol),e(U,M),e(M,dn),e(dn,wl),e(M,$l),e(M,pn),e(pn,xl),e(M,El),e(M,cn),e(cn,Rl),e(M,Nl),e(M,un),e(un,kl),e(M,Sl),e(U,ql),e(U,H),e(H,hn),e(hn,Dl),e(H,Al),e(H,fn),e(fn,Pl),e(H,zl),e(H,_n),e(_n,Cl),e(H,Il),e(H,gn),e(gn,Fl),e(H,Ll),e(T,Xl),e(T,Ce),g(gt,Ce,null),e(Ce,Ul),e(Ce,vn),e(vn,Ml),e(T,Hl),e(T,Q),g(vt,Q,null),e(Q,jl),e(Q,bn),e(bn,Wl),e(Q,Gl),e(Q,bt),e(bt,Vl),e(bt,yn),e(yn,Yl),e(bt,Bl),e(T,Zl),e(T,Ie),g(yt,Ie,null),e(Ie,Jl),e(Ie,Tn),e(Tn,Kl),e(T,Ql),e(T,ee),g(Tt,ee,null),e(ee,es),e(ee,me),e(me,ts),e(me,On),e(On,as),e(me,ns),e(me,wn),e(wn,os),e(me,rs),e(ee,is),e(ee,$n),e($n,ls),e(T,ss),e(T,Fe),g(Ot,Fe,null),e(Fe,ms),e(Fe,wt),e(wt,ds),e(wt,xn),e(xn,ps),e(wt,cs),e(T,us),e(T,q),g($t,q,null),e(q,hs),e(q,En),e(En,fs),e(q,_s),e(q,xt),e(xt,gs),e(xt,Rn),e(Rn,vs),e(xt,bs),e(q,ys),g(Le,q,null),e(q,Ts),e(q,Et),e(Et,Os),e(Et,Nn),e(Nn,ws),e(Et,$s),e(q,xs),e(q,de),e(de,pe),e(pe,Es),e(pe,kn),e(kn,Rs),e(pe,Ns),e(pe,Sn),e(Sn,ks),e(pe,Ss),e(de,qs),e(de,ce),e(ce,Ds),e(ce,qn),e(qn,As),e(ce,Ps),e(ce,Dn),e(Dn,zs),e(ce,Cs),e(de,Is),e(de,ue),e(ue,Fs),e(ue,An),e(An,Ls),e(ue,Xs),e(ue,Pn),e(Pn,Us),e(ue,Ms),e(T,Hs),e(T,te),g(Rt,te,null),e(te,js),e(te,he),e(he,Ws),e(he,zn),e(zn,Gs),e(he,Vs),e(he,Cn),e(Cn,Ys),e(he,Bs),e(te,Zs),e(te,In),e(In,Js),e(T,Ks),e(T,Xe),g(Nt,Xe,null),e(Xe,Qs),e(Xe,fe),e(fe,em),e(fe,Fn),e(Fn,tm),e(fe,am),e(fe,Ln),e(Ln,nm),e(fe,om),e(T,rm),e(T,Ue),g(kt,Ue,null),e(Ue,im),e(Ue,Xn),e(Xn,lm),c(t,Qo,d),c(t,_e,d),e(_e,Me),e(Me,Un),g(St,Un,null),e(_e,sm),e(_e,Mn),e(Mn,mm),c(t,er,d),c(t,k,d),e(k,dm),e(k,Hn),e(Hn,pm),e(k,cm),e(k,qt),e(qt,jn),e(jn,um),e(k,hm),e(k,Wn),e(Wn,fm),e(k,_m),e(k,Gn),e(Gn,gm),e(k,vm),e(k,Vn),e(Vn,bm),e(k,ym),e(k,Yn),e(Yn,Tm),e(k,Om),c(t,tr,d),g(Dt,t,d),c(t,ar,d),c(t,He,d),e(He,wm),e(He,At),e(At,$m),e(He,xm),c(t,nr,d),c(t,B,d),g(Pt,B,null),e(B,Em),e(B,je),g(zt,je,null),e(je,Rm),e(je,Bn),e(Bn,Nm),e(B,km),e(B,D),g(Ct,D,null),e(D,Sm),e(D,Zn),e(Zn,qm),e(D,Dm),e(D,It),e(It,Am),e(It,Jn),e(Jn,Pm),e(It,zm),e(D,Cm),g(We,D,null),e(D,Im),e(D,Ft),e(Ft,Fm),e(Ft,Kn),e(Kn,Lm),e(Ft,Xm),e(D,Um),e(D,ge),e(ge,ve),e(ve,Mm),e(ve,Qn),e(Qn,Hm),e(ve,jm),e(ve,eo),e(eo,Wm),e(ve,Gm),e(ge,Vm),e(ge,be),e(be,Ym),e(be,to),e(to,Bm),e(be,Zm),e(be,ao),e(ao,Jm),e(be,Km),e(ge,Qm),e(ge,ye),e(ye,ed),e(ye,no),e(no,td),e(ye,ad),e(ye,oo),e(oo,nd),e(ye,od),c(t,or,d),c(t,Te,d),e(Te,Ge),e(Ge,ro),g(Lt,ro,null),e(Te,rd),e(Te,io),e(io,id),c(t,rr,d),c(t,z,d),e(z,ld),e(z,lo),e(lo,sd),e(z,md),e(z,Xt),e(Xt,so),e(so,dd),e(z,pd),e(z,mo),e(mo,cd),e(z,ud),e(z,po),e(po,hd),e(z,fd),c(t,ir,d),g(Ut,t,d),c(t,lr,d),c(t,Mt,d),g(Ht,Mt,null),c(t,sr,d),g(Ve,t,d),c(t,mr,d),c(t,Oe,d),e(Oe,Ye),e(Ye,co),g(jt,co,null),e(Oe,_d),e(Oe,uo),e(uo,gd),c(t,dr,d),c(t,C,d),e(C,vd),e(C,ho),e(ho,bd),e(C,yd),e(C,Wt),e(Wt,fo),e(fo,Td),e(C,Od),e(C,_o),e(_o,wd),e(C,$d),e(C,go),e(go,xd),e(C,Ed),c(t,pr,d),g(Gt,t,d),c(t,cr,d),c(t,Vt,d),g(Yt,Vt,null),c(t,ur,d),g(Be,t,d),c(t,hr,d),c(t,we,d),e(we,Ze),e(Ze,vo),g(Bt,vo,null),e(we,Rd),e(we,bo),e(bo,Nd),c(t,fr,d),c(t,j,d),e(j,yo),e(yo,Zt),e(Zt,kd),e(j,Sd),e(j,To),e(To,Jt),e(Jt,qd),e(j,Dd),e(j,Oo),e(Oo,Kt),e(Kt,Ad),e(j,Pd),e(j,pa),e(pa,zd),e(pa,Qt),e(Qt,wo),e(wo,ea),e(ea,Cd),e(Qt,Id),e(Qt,$o),e($o,ta),e(ta,Fd),c(t,_r,d),c(t,W,d),e(W,Ld),e(W,xo),e(xo,Xd),e(W,Ud),e(W,aa),e(aa,Md),e(W,Hd),e(W,na),e(na,jd),e(W,Wd),gr=!0},p(t,[d]){const oa={};d&2&&(oa.$$scope={dirty:d,ctx:t}),Le.$set(oa);const Eo={};d&2&&(Eo.$$scope={dirty:d,ctx:t}),We.$set(Eo);const Ro={};d&2&&(Ro.$$scope={dirty:d,ctx:t}),Ve.$set(Ro);const No={};d&2&&(No.$$scope={dirty:d,ctx:t}),Be.$set(No)},i(t){gr||(v(x.$$.fragment,t),v(et.$$.fragment,t),v(tt.$$.fragment,t),v(ot.$$.fragment,t),v(rt.$$.fragment,t),v(it.$$.fragment,t),v(lt.$$.fragment,t),v(st.$$.fragment,t),v(mt.$$.fragment,t),v(pt.$$.fragment,t),v(ut.$$.fragment,t),v(ft.$$.fragment,t),v(gt.$$.fragment,t),v(vt.$$.fragment,t),v(yt.$$.fragment,t),v(Tt.$$.fragment,t),v(Ot.$$.fragment,t),v($t.$$.fragment,t),v(Le.$$.fragment,t),v(Rt.$$.fragment,t),v(Nt.$$.fragment,t),v(kt.$$.fragment,t),v(St.$$.fragment,t),v(Dt.$$.fragment,t),v(Pt.$$.fragment,t),v(zt.$$.fragment,t),v(Ct.$$.fragment,t),v(We.$$.fragment,t),v(Lt.$$.fragment,t),v(Ut.$$.fragment,t),v(Ht.$$.fragment,t),v(Ve.$$.fragment,t),v(jt.$$.fragment,t),v(Gt.$$.fragment,t),v(Yt.$$.fragment,t),v(Be.$$.fragment,t),v(Bt.$$.fragment,t),gr=!0)},o(t){b(x.$$.fragment,t),b(et.$$.fragment,t),b(tt.$$.fragment,t),b(ot.$$.fragment,t),b(rt.$$.fragment,t),b(it.$$.fragment,t),b(lt.$$.fragment,t),b(st.$$.fragment,t),b(mt.$$.fragment,t),b(pt.$$.fragment,t),b(ut.$$.fragment,t),b(ft.$$.fragment,t),b(gt.$$.fragment,t),b(vt.$$.fragment,t),b(yt.$$.fragment,t),b(Tt.$$.fragment,t),b(Ot.$$.fragment,t),b($t.$$.fragment,t),b(Le.$$.fragment,t),b(Rt.$$.fragment,t),b(Nt.$$.fragment,t),b(kt.$$.fragment,t),b(St.$$.fragment,t),b(Dt.$$.fragment,t),b(Pt.$$.fragment,t),b(zt.$$.fragment,t),b(Ct.$$.fragment,t),b(We.$$.fragment,t),b(Lt.$$.fragment,t),b(Ut.$$.fragment,t),b(Ht.$$.fragment,t),b(Ve.$$.fragment,t),b(jt.$$.fragment,t),b(Gt.$$.fragment,t),b(Yt.$$.fragment,t),b(Be.$$.fragment,t),b(Bt.$$.fragment,t),gr=!1},d(t){a(u),t&&a($),t&&a(h),y(x),t&&a(So),t&&a(E),t&&a(qo),t&&a(re),y(et),t&&a(Do),t&&a(ra),t&&a(Ao),t&&a(J),t&&a(Po),t&&a(ie),y(tt),t&&a(zo),t&&a(K),t&&a(Co),t&&a(Se),t&&a(Io),y(ot,t),t&&a(Fo),t&&a(ia),t&&a(Lo),y(rt,t),t&&a(Xo),t&&a(la),t&&a(Uo),y(it,t),t&&a(Mo),t&&a(le),y(lt),t&&a(Ho),t&&a(sa),t&&a(jo),y(st,t),t&&a(Wo),t&&a(ma),t&&a(Go),y(mt,t),t&&a(Vo),t&&a(De),t&&a(Yo),t&&a(se),y(pt),t&&a(Bo),t&&a(R),t&&a(Zo),y(ut,t),t&&a(Jo),t&&a(Pe),t&&a(Ko),t&&a(T),y(ft),y(gt),y(vt),y(yt),y(Tt),y(Ot),y($t),y(Le),y(Rt),y(Nt),y(kt),t&&a(Qo),t&&a(_e),y(St),t&&a(er),t&&a(k),t&&a(tr),y(Dt,t),t&&a(ar),t&&a(He),t&&a(nr),t&&a(B),y(Pt),y(zt),y(Ct),y(We),t&&a(or),t&&a(Te),y(Lt),t&&a(rr),t&&a(z),t&&a(ir),y(Ut,t),t&&a(lr),t&&a(Mt),y(Ht),t&&a(sr),y(Ve,t),t&&a(mr),t&&a(Oe),y(jt),t&&a(dr),t&&a(C),t&&a(pr),y(Gt,t),t&&a(cr),t&&a(Vt),y(Yt),t&&a(ur),y(Be,t),t&&a(hr),t&&a(we),y(Bt),t&&a(fr),t&&a(j),t&&a(_r),t&&a(W)}}}const Du={local:"trainer",sections:[{local:"prerequisite",sections:[{local:"install-onnx-runtime",title:"Install ONNX Runtime"},{local:"install-optimum",title:"Install Optimum"}],title:"Prerequisite"},{local:"optimum.onnxruntime.ORTTrainer",title:"ORTTrainer"},{local:"optimum.onnxruntime.ORTSeq2SeqTrainer",title:"ORTSeq2SeqTrainer"},{local:"optimum.onnxruntime.ORTTrainingArguments",title:"ORTTrainingArguments"},{local:"optimum.onnxruntime.ORTSeq2SeqTrainingArguments",title:"ORTSeq2SeqTrainingArguments"},{local:"other-resources",title:"Other Resources"}],title:"Trainer"};function Au(Z){return Eu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Lu extends Ou{constructor(u){super();wu(this,u,Au,qu,$u,{})}}export{Lu as default,Du as metadata};
