import{S as To,i as Co,s as Do,H as gt,e as c,c as u,a as f,d as t,b as w,g as p,F as a,P as qi,U as Em,I as bt,J as Et,K as $t,q as $,o as P,v as $m,Z as Eh,N as om,O as lm,f as Dm,L as rm,w as D,x as S,y as N,B as I,k as g,m as b,n as se,p as ce,_ as km,l as ue,t as n,h as i,j as Pm,$ as Nm,G as Am,a0 as Tm,M as Im}from"../chunks/vendor-1b69db54.js";import{T as Ce}from"../chunks/Tip-1ceee8ba.js";import{I as x}from"../chunks/IconCopyLink-b3ba9a38.js";import{C as H}from"../chunks/CodeBlock-40263a9f.js";function Sm(m){let o,h,r,l,_,d;const v=m[7].default,y=gt(v,m,m[6],null);return{c(){o=c("div"),h=c("ul"),y&&y.c(),this.h()},l(E){o=u(E,"DIV",{class:!0});var k=f(o);h=u(k,"UL",{class:!0});var U=f(h);y&&y.l(U),U.forEach(t),k.forEach(t),this.h()},h(){w(h,"class","min-w-full w-auto"),w(o,"class",r="absolute top-full mt-1 min-w-full w-auto bg-white rounded-xl overflow-hidden shadow-lg z-10 border border-gray-100 "+(m[2]==="right"?"right-0":"left-0")+" "+m[0])},m(E,k){p(E,o,k),a(o,h),y&&y.m(h,null),m[8](o),l=!0,_||(d=qi(o,"click",function(){Em(m[1])&&m[1].apply(this,arguments)}),_=!0)},p(E,[k]){m=E,y&&y.p&&(!l||k&64)&&bt(y,v,m,m[6],l?$t(v,m[6],k,null):Et(m[6]),null),(!l||k&5&&r!==(r="absolute top-full mt-1 min-w-full w-auto bg-white rounded-xl overflow-hidden shadow-lg z-10 border border-gray-100 "+(m[2]==="right"?"right-0":"left-0")+" "+m[0]))&&w(o,"class",r)},i(E){l||($(y,E),l=!0)},o(E){P(y,E),l=!1},d(E){E&&t(o),y&&y.d(E),m[8](null),_=!1,d()}}}function zm(m,o,h){let{$$slots:r={},$$scope:l}=o,{classNames:_=""}=o,{dropdownElement:d=void 0}=o,{forceAlignement:v=void 0}=o,{onClose:y}=o,E=v!=null?v:"left",k;$m(()=>{var z,C;if(document.addEventListener("click",U),!v){const A=document.documentElement.clientWidth,T=(k==null?void 0:k.getBoundingClientRect())||{},L=(z=T.left)!=null?z:0,G=(C=T.width)!=null?C:0;h(2,E=L+G>A?"right":"left")}return()=>{document.removeEventListener("click",U)}});function U(z){const C=z.target;C!==d&&!(d==null?void 0:d.contains(C))&&y()}function j(z){Eh[z?"unshift":"push"](()=>{k=z,h(3,k)})}return m.$$set=z=>{"classNames"in z&&h(0,_=z.classNames),"dropdownElement"in z&&h(4,d=z.dropdownElement),"forceAlignement"in z&&h(5,v=z.forceAlignement),"onClose"in z&&h(1,y=z.onClose),"$$scope"in z&&h(6,l=z.$$scope)},[_,y,E,k,d,v,l,r,j]}class Om extends To{constructor(o){super();Co(this,o,zm,Sm,Do,{classNames:0,dropdownElement:4,forceAlignement:5,onClose:1})}}function jm(m){let o,h;return{c(){o=om("svg"),h=om("path"),this.h()},l(r){o=lm(r,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0,style:!0});var l=f(o);h=lm(l,"path",{d:!0,fill:!0}),f(h).forEach(t),l.forEach(t),this.h()},h(){w(h,"d","M7 10l5 5l5-5z"),w(h,"fill","currentColor"),w(o,"class",m[0]),w(o,"xmlns","http://www.w3.org/2000/svg"),w(o,"xmlns:xlink","http://www.w3.org/1999/xlink"),w(o,"aria-hidden","true"),w(o,"focusable","false"),w(o,"role","img"),w(o,"width","1em"),w(o,"height","1em"),w(o,"preserveAspectRatio","xMidYMid meet"),w(o,"viewBox","0 0 24 24"),Dm(o,"transform","rotate(360deg)")},m(r,l){p(r,o,l),a(o,h)},p(r,[l]){l&1&&w(o,"class",r[0])},i:rm,o:rm,d(r){r&&t(o)}}}function Lm(m,o,h){let{classNames:r=""}=o;return m.$$set=l=>{"classNames"in l&&h(0,r=l.classNames)},[r]}class Um extends To{constructor(o){super();Co(this,o,Lm,jm,Do,{classNames:0})}}const Mm=m=>({}),nm=m=>({}),Gm=m=>({}),im=m=>({});function Hm(m){let o,h,r,l,_,d=m[2]&&sm(m),v=m[10]&&cm();return{c(){d&&d.c(),o=g(),h=n(m[4]),r=g(),v&&v.c(),l=ue()},l(y){d&&d.l(y),o=b(y),h=i(y,m[4]),r=b(y),v&&v.l(y),l=ue()},m(y,E){d&&d.m(y,E),p(y,o,E),p(y,h,E),p(y,r,E),v&&v.m(y,E),p(y,l,E),_=!0},p(y,E){y[2]?d?(d.p(y,E),E&4&&$(d,1)):(d=sm(y),d.c(),$(d,1),d.m(o.parentNode,o)):d&&(se(),P(d,1,1,()=>{d=null}),ce()),(!_||E&16)&&Pm(h,y[4]),y[10]?v?E&1024&&$(v,1):(v=cm(),v.c(),$(v,1),v.m(l.parentNode,l)):v&&(se(),P(v,1,1,()=>{v=null}),ce())},i(y){_||($(d),$(v),_=!0)},o(y){P(d),P(v),_=!1},d(y){d&&d.d(y),y&&t(o),y&&t(h),y&&t(r),v&&v.d(y),y&&t(l)}}}function Ym(m){let o;const h=m[14].button,r=gt(h,m,m[18],im);return{c(){r&&r.c()},l(l){r&&r.l(l)},m(l,_){r&&r.m(l,_),o=!0},p(l,_){r&&r.p&&(!o||_&262144)&&bt(r,h,l,l[18],o?$t(h,l[18],_,Gm):Et(l[18]),im)},i(l){o||($(r,l),o=!0)},o(l){P(r,l),o=!1},d(l){r&&r.d(l)}}}function sm(m){let o,h,r;var l=m[2];function _(d){return{props:{classNames:"mr-1.5 "+d[3]}}}return l&&(o=new l(_(m))),{c(){o&&D(o.$$.fragment),h=ue()},l(d){o&&S(o.$$.fragment,d),h=ue()},m(d,v){o&&N(o,d,v),p(d,h,v),r=!0},p(d,v){const y={};if(v&8&&(y.classNames="mr-1.5 "+d[3]),l!==(l=d[2])){if(o){se();const E=o;P(E.$$.fragment,1,0,()=>{I(E,1)}),ce()}l?(o=new l(_(d)),D(o.$$.fragment),$(o.$$.fragment,1),N(o,h.parentNode,h)):o=null}else l&&o.$set(y)},i(d){r||(o&&$(o.$$.fragment,d),r=!0)},o(d){o&&P(o.$$.fragment,d),r=!1},d(d){d&&t(h),o&&I(o,d)}}}function cm(m){let o,h;return o=new Um({props:{classNames:"-mr-1 text-gray-500"}}),{c(){D(o.$$.fragment)},l(r){S(o.$$.fragment,r)},m(r,l){N(o,r,l),h=!0},i(r){h||($(o.$$.fragment,r),h=!0)},o(r){P(o.$$.fragment,r),h=!1},d(r){I(o,r)}}}function um(m){let o,h;return o=new Om({props:{classNames:m[6]+" "+(m[9]?"v2-dropdown-menu hidden":""),dropdownElement:m[11],forceAlignement:m[5],onClose:m[16],$$slots:{default:[qm]},$$scope:{ctx:m}}}),{c(){D(o.$$.fragment)},l(r){S(o.$$.fragment,r)},m(r,l){N(o,r,l),h=!0},p(r,l){const _={};l&576&&(_.classNames=r[6]+" "+(r[9]?"v2-dropdown-menu hidden":"")),l&2048&&(_.dropdownElement=r[11]),l&32&&(_.forceAlignement=r[5]),l&4096&&(_.onClose=r[16]),l&262144&&(_.$$scope={dirty:l,ctx:r}),o.$set(_)},i(r){h||($(o.$$.fragment,r),h=!0)},o(r){P(o.$$.fragment,r),h=!1},d(r){I(o,r)}}}function qm(m){let o;const h=m[14].menu,r=gt(h,m,m[18],nm);return{c(){r&&r.c()},l(l){r&&r.l(l)},m(l,_){r&&r.m(l,_),o=!0},p(l,_){r&&r.p&&(!o||_&262144)&&bt(r,h,l,l[18],o?$t(h,l[18],_,Mm):Et(l[18]),nm)},i(l){o||($(r,l),o=!0)},o(l){P(r,l),o=!1},d(l){r&&r.d(l)}}}function Bm(m){let o,h,r,l,_,d,v,y,E,k,U;const j=[Ym,Hm],z=[];function C(T,L){return T[13].button?0:1}r=C(m),l=z[r]=j[r](m);let A=(m[12]||m[9])&&um(m);return{c(){o=c("div"),h=c("button"),l.c(),d=g(),A&&A.c(),this.h()},l(T){o=u(T,"DIV",{class:!0,"selected-value":!0});var L=f(o);h=u(L,"BUTTON",{class:!0,type:!0});var G=f(h);l.l(G),G.forEach(t),d=b(L),A&&A.l(L),L.forEach(t),this.h()},h(){w(h,"class",_=""+m[1]+" "+(m[7]?"":"cursor-pointer w-full btn text-sm")+" "+(m[9]?"v2-dropdown-button":"")),w(h,"type","button"),w(o,"class",v="relative "+m[0]+" "+(m[9]?"v2-dropdown":"")),w(o,"selected-value",y=m[8]||void 0)},m(T,L){p(T,o,L),a(o,h),z[r].m(h,null),a(o,d),A&&A.m(o,null),m[17](o),E=!0,k||(U=qi(h,"click",m[15]),k=!0)},p(T,[L]){let G=r;r=C(T),r===G?z[r].p(T,L):(se(),P(z[G],1,1,()=>{z[G]=null}),ce(),l=z[r],l?l.p(T,L):(l=z[r]=j[r](T),l.c()),$(l,1),l.m(h,null)),(!E||L&642&&_!==(_=""+T[1]+" "+(T[7]?"":"cursor-pointer w-full btn text-sm")+" "+(T[9]?"v2-dropdown-button":"")))&&w(h,"class",_),T[12]||T[9]?A?(A.p(T,L),L&4608&&$(A,1)):(A=um(T),A.c(),$(A,1),A.m(o,null)):A&&(se(),P(A,1,1,()=>{A=null}),ce()),(!E||L&513&&v!==(v="relative "+T[0]+" "+(T[9]?"v2-dropdown":"")))&&w(o,"class",v),(!E||L&256&&y!==(y=T[8]||void 0))&&w(o,"selected-value",y)},i(T){E||($(l),$(A),E=!0)},o(T){P(l),P(A),E=!1},d(T){T&&t(o),z[r].d(),A&&A.d(),m[17](null),k=!1,U()}}}function xm(m,o,h){let{$$slots:r={},$$scope:l}=o;const _=km(r);let{classNames:d=""}=o,{btnClassNames:v=""}=o,{btnIcon:y=void 0}=o,{btnIconClassNames:E=""}=o,{btnLabel:k=""}=o,{forceMenuAlignement:U=void 0}=o,{menuClassNames:j=""}=o,{noBtnClass:z=void 0}=o,{selectedValue:C=void 0}=o,{useDeprecatedJS:A=!0}=o,{withBtnCaret:T=!1}=o,L,G=!1;const O=()=>h(12,G=!G),J=()=>h(12,G=!1);function kt(M){Eh[M?"unshift":"push"](()=>{L=M,h(11,L)})}return m.$$set=M=>{"classNames"in M&&h(0,d=M.classNames),"btnClassNames"in M&&h(1,v=M.btnClassNames),"btnIcon"in M&&h(2,y=M.btnIcon),"btnIconClassNames"in M&&h(3,E=M.btnIconClassNames),"btnLabel"in M&&h(4,k=M.btnLabel),"forceMenuAlignement"in M&&h(5,U=M.forceMenuAlignement),"menuClassNames"in M&&h(6,j=M.menuClassNames),"noBtnClass"in M&&h(7,z=M.noBtnClass),"selectedValue"in M&&h(8,C=M.selectedValue),"useDeprecatedJS"in M&&h(9,A=M.useDeprecatedJS),"withBtnCaret"in M&&h(10,T=M.withBtnCaret),"$$scope"in M&&h(18,l=M.$$scope)},[d,v,y,E,k,U,j,z,C,A,T,L,G,_,r,O,J,kt,l]}class fm extends To{constructor(o){super();Co(this,o,xm,Bm,Do,{classNames:0,btnClassNames:1,btnIcon:2,btnIconClassNames:3,btnLabel:4,forceMenuAlignement:5,menuClassNames:6,noBtnClass:7,selectedValue:8,useDeprecatedJS:9,withBtnCaret:10})}}function Jm(m){let o,h,r,l=m[5]&&hm(m);return{c(){l&&l.c(),o=g(),h=n(m[7])},l(_){l&&l.l(_),o=b(_),h=i(_,m[7])},m(_,d){l&&l.m(_,d),p(_,o,d),p(_,h,d),r=!0},p(_,d){_[5]?l?(l.p(_,d),d&32&&$(l,1)):(l=hm(_),l.c(),$(l,1),l.m(o.parentNode,o)):l&&(se(),P(l,1,1,()=>{l=null}),ce()),(!r||d&128)&&Pm(h,_[7])},i(_){r||($(l),r=!0)},o(_){P(l),r=!1},d(_){l&&l.d(_),_&&t(o),_&&t(h)}}}function Xm(m){let o;const h=m[15].default,r=gt(h,m,m[14],null);return{c(){r&&r.c()},l(l){r&&r.l(l)},m(l,_){r&&r.m(l,_),o=!0},p(l,_){r&&r.p&&(!o||_&16384)&&bt(r,h,l,l[14],o?$t(h,l[14],_,null):Et(l[14]),null)},i(l){o||($(r,l),o=!0)},o(l){P(r,l),o=!1},d(l){r&&r.d(l)}}}function hm(m){let o,h,r;var l=m[5];function _(d){return{props:{classNames:"mr-1.5 "+d[6]}}}return l&&(o=new l(_(m))),{c(){o&&D(o.$$.fragment),h=ue()},l(d){o&&S(o.$$.fragment,d),h=ue()},m(d,v){o&&N(o,d,v),p(d,h,v),r=!0},p(d,v){const y={};if(v&64&&(y.classNames="mr-1.5 "+d[6]),l!==(l=d[5])){if(o){se();const E=o;P(E.$$.fragment,1,0,()=>{I(E,1)}),ce()}l?(o=new l(_(d)),D(o.$$.fragment),$(o.$$.fragment,1),N(o,h.parentNode,h)):o=null}else l&&o.$set(y)},i(d){r||(o&&$(o.$$.fragment,d),r=!0)},o(d){o&&P(o.$$.fragment,d),r=!1},d(d){d&&t(h),o&&I(o,d)}}}function Fm(m){let o,h,r,l,_,d,v,y,E,k;const U=[Xm,Jm],j=[];function z(C,A){return C[13].default?0:1}return r=z(m),l=j[r]=U[r](m),{c(){o=c("li"),h=c("a"),l.c(),this.h()},l(C){o=u(C,"LI",{});var A=f(o);h=u(A,"A",{class:!0,"data-label":!0,"data-url":!0,"data-value":!0,href:!0,rel:!0,target:!0});var T=f(h);l.l(T),T.forEach(t),A.forEach(t),this.h()},h(){w(h,"class",_="flex items-center hover:bg-gray-50 dark:hover:bg-gray-800 cursor-pointer px-3 py-1.5 whitespace-nowrap "+m[0]+" "+(m[9]?"hover:underline":"")+" "+(m[12]?"v2-dropdown-entry":"")),w(h,"data-label",m[1]),w(h,"data-url",m[2]),w(h,"data-value",m[3]),w(h,"href",m[4]),w(h,"rel",d=m[8]?"nofollow":void 0),w(h,"target",v=m[11]?"_blank":void 0)},m(C,A){p(C,o,A),a(o,h),j[r].m(h,null),y=!0,E||(k=qi(h,"click",function(){Em(m[10])&&m[10].apply(this,arguments)}),E=!0)},p(C,[A]){m=C;let T=r;r=z(m),r===T?j[r].p(m,A):(se(),P(j[T],1,1,()=>{j[T]=null}),ce(),l=j[r],l?l.p(m,A):(l=j[r]=U[r](m),l.c()),$(l,1),l.m(h,null)),(!y||A&4609&&_!==(_="flex items-center hover:bg-gray-50 dark:hover:bg-gray-800 cursor-pointer px-3 py-1.5 whitespace-nowrap "+m[0]+" "+(m[9]?"hover:underline":"")+" "+(m[12]?"v2-dropdown-entry":"")))&&w(h,"class",_),(!y||A&2)&&w(h,"data-label",m[1]),(!y||A&4)&&w(h,"data-url",m[2]),(!y||A&8)&&w(h,"data-value",m[3]),(!y||A&16)&&w(h,"href",m[4]),(!y||A&256&&d!==(d=m[8]?"nofollow":void 0))&&w(h,"rel",d),(!y||A&2048&&v!==(v=m[11]?"_blank":void 0))&&w(h,"target",v)},i(C){y||($(l),y=!0)},o(C){P(l),y=!1},d(C){C&&t(o),j[r].d(),E=!1,k()}}}function Rm(m,o,h){let{$$slots:r={},$$scope:l}=o;const _=km(r);let{classNames:d=""}=o,{dataLabel:v=void 0}=o,{dataUrl:y=void 0}=o,{dataValue:E=void 0}=o,{href:k=void 0}=o,{icon:U=void 0}=o,{iconClassNames:j=""}=o,{label:z=""}=o,{noFollow:C=!1}=o,{underline:A=!1}=o,{onClick:T=()=>{}}=o,{targetBlank:L=!1}=o,{useDeprecatedJS:G=!0}=o;return m.$$set=O=>{"classNames"in O&&h(0,d=O.classNames),"dataLabel"in O&&h(1,v=O.dataLabel),"dataUrl"in O&&h(2,y=O.dataUrl),"dataValue"in O&&h(3,E=O.dataValue),"href"in O&&h(4,k=O.href),"icon"in O&&h(5,U=O.icon),"iconClassNames"in O&&h(6,j=O.iconClassNames),"label"in O&&h(7,z=O.label),"noFollow"in O&&h(8,C=O.noFollow),"underline"in O&&h(9,A=O.underline),"onClick"in O&&h(10,T=O.onClick),"targetBlank"in O&&h(11,L=O.targetBlank),"useDeprecatedJS"in O&&h(12,G=O.useDeprecatedJS),"$$scope"in O&&h(14,l=O.$$scope)},[d,v,y,E,k,U,j,z,C,A,T,L,G,_,l,r]}class Cm extends To{constructor(o){super();Co(this,o,Rm,Fm,Do,{classNames:0,dataLabel:1,dataUrl:2,dataValue:3,href:4,icon:5,iconClassNames:6,label:7,noFollow:8,underline:9,onClick:10,targetBlank:11,useDeprecatedJS:12})}}const{window:Wm}=Nm,Vm=m=>({}),dm=m=>({slot:"button"});function pm(m,o,h){const r=m.slice();return r[11]=o[h].label,r[12]=o[h].value,r}const Qm=m=>({}),mm=m=>({slot:"menu"}),Km=m=>({}),_m=m=>({slot:"button"});function ym(m,o,h){const r=m.slice();return r[11]=o[h].label,r[12]=o[h].value,r}const Zm=m=>({}),vm=m=>({slot:"menu"});function e_(m){let o,h;return{c(){o=c("img"),this.h()},l(r){o=u(r,"IMG",{alt:!0,class:!0,src:!0}),this.h()},h(){w(o,"alt","Open In Colab"),w(o,"class","!m-0"),Tm(o.src,h="https://colab.research.google.com/assets/colab-badge.svg")||w(o,"src",h)},m(r,l){p(r,o,l)},d(r){r&&t(o)}}}function t_(m){let o;const h=m[6].default,r=gt(h,m,m[10],_m),l=r||e_();return{c(){l&&l.c()},l(_){l&&l.l(_)},m(_,d){l&&l.m(_,d),o=!0},p(_,d){r&&r.p&&(!o||d&1024)&&bt(r,h,_,_[10],o?$t(h,_[10],d,Km):Et(_[10]),_m)},i(_){o||($(l,_),o=!0)},o(_){P(l,_),o=!1},d(_){l&&l.d(_)}}}function wm(m){let o,h;function r(){return m[7](m[12])}return o=new Cm({props:{classNames:"text-sm !no-underline",iconClassNames:"text-gray-500",label:m[11],onClick:r,useDeprecatedJS:!1}}),{c(){D(o.$$.fragment)},l(l){S(o.$$.fragment,l)},m(l,_){N(o,l,_),h=!0},p(l,_){m=l},i(l){h||($(o.$$.fragment,l),h=!0)},o(l){P(o.$$.fragment,l),h=!1},d(l){I(o,l)}}}function a_(m){let o,h,r=m[2],l=[];for(let d=0;d<r.length;d+=1)l[d]=wm(ym(m,r,d));const _=d=>P(l[d],1,1,()=>{l[d]=null});return{c(){for(let d=0;d<l.length;d+=1)l[d].c();o=ue()},l(d){for(let v=0;v<l.length;v+=1)l[v].l(d);o=ue()},m(d,v){for(let y=0;y<l.length;y+=1)l[y].m(d,v);p(d,o,v),h=!0},p(d,v){if(v&4){r=d[2];let y;for(y=0;y<r.length;y+=1){const E=ym(d,r,y);l[y]?(l[y].p(E,v),$(l[y],1)):(l[y]=wm(E),l[y].c(),$(l[y],1),l[y].m(o.parentNode,o))}for(se(),y=r.length;y<l.length;y+=1)_(y);ce()}},i(d){if(!h){for(let v=0;v<r.length;v+=1)$(l[v]);h=!0}},o(d){l=l.filter(Boolean);for(let v=0;v<l.length;v+=1)P(l[v]);h=!1},d(d){Am(l,d),d&&t(o)}}}function o_(m){let o;const h=m[6].default,r=gt(h,m,m[10],vm),l=r||a_(m);return{c(){l&&l.c()},l(_){l&&l.l(_)},m(_,d){l&&l.m(_,d),o=!0},p(_,d){r&&r.p&&(!o||d&1024)&&bt(r,h,_,_[10],o?$t(h,_[10],d,Zm):Et(_[10]),vm)},i(_){o||($(l,_),o=!0)},o(_){P(l,_),o=!1},d(_){l&&l.d(_)}}}function l_(m){let o,h;return{c(){o=c("img"),this.h()},l(r){o=u(r,"IMG",{alt:!0,class:!0,src:!0}),this.h()},h(){w(o,"alt","Open In Studio Lab"),w(o,"class","!m-0"),Tm(o.src,h="https://studiolab.sagemaker.aws/studiolab.svg")||w(o,"src",h)},m(r,l){p(r,o,l)},d(r){r&&t(o)}}}function r_(m){let o;const h=m[6].default,r=gt(h,m,m[10],dm),l=r||l_();return{c(){l&&l.c()},l(_){l&&l.l(_)},m(_,d){l&&l.m(_,d),o=!0},p(_,d){r&&r.p&&(!o||d&1024)&&bt(r,h,_,_[10],o?$t(h,_[10],d,Vm):Et(_[10]),dm)},i(_){o||($(l,_),o=!0)},o(_){P(l,_),o=!1},d(_){l&&l.d(_)}}}function gm(m){let o,h;function r(){return m[8](m[12])}return o=new Cm({props:{classNames:"text-sm !no-underline",iconClassNames:"text-gray-500",label:m[11],onClick:r,useDeprecatedJS:!1}}),{c(){D(o.$$.fragment)},l(l){S(o.$$.fragment,l)},m(l,_){N(o,l,_),h=!0},p(l,_){m=l},i(l){h||($(o.$$.fragment,l),h=!0)},o(l){P(o.$$.fragment,l),h=!1},d(l){I(o,l)}}}function n_(m){let o,h,r=m[3],l=[];for(let d=0;d<r.length;d+=1)l[d]=gm(pm(m,r,d));const _=d=>P(l[d],1,1,()=>{l[d]=null});return{c(){for(let d=0;d<l.length;d+=1)l[d].c();o=ue()},l(d){for(let v=0;v<l.length;v+=1)l[v].l(d);o=ue()},m(d,v){for(let y=0;y<l.length;y+=1)l[y].m(d,v);p(d,o,v),h=!0},p(d,v){if(v&8){r=d[3];let y;for(y=0;y<r.length;y+=1){const E=pm(d,r,y);l[y]?(l[y].p(E,v),$(l[y],1)):(l[y]=gm(E),l[y].c(),$(l[y],1),l[y].m(o.parentNode,o))}for(se(),y=r.length;y<l.length;y+=1)_(y);ce()}},i(d){if(!h){for(let v=0;v<r.length;v+=1)$(l[v]);h=!0}},o(d){l=l.filter(Boolean);for(let v=0;v<l.length;v+=1)P(l[v]);h=!1},d(d){Am(l,d),d&&t(o)}}}function i_(m){let o;const h=m[6].default,r=gt(h,m,m[10],mm),l=r||n_(m);return{c(){l&&l.c()},l(_){l&&l.l(_)},m(_,d){l&&l.m(_,d),o=!0},p(_,d){r&&r.p&&(!o||d&1024)&&bt(r,h,_,_[10],o?$t(h,_[10],d,Qm):Et(_[10]),mm)},i(_){o||($(l,_),o=!0)},o(_){P(l,_),o=!1},d(_){l&&l.d(_)}}}function s_(m){let o,h,r,l,_,d,v,y;return h=new fm({props:{btnLabel:"",classNames:"colab-dropdown",noBtnClass:!0,useDeprecatedJS:!1,$$slots:{menu:[o_],button:[t_]},$$scope:{ctx:m}}}),l=new fm({props:{btnLabel:"",classNames:"colab-dropdown",noBtnClass:!0,useDeprecatedJS:!1,$$slots:{menu:[i_],button:[r_]},$$scope:{ctx:m}}}),{c(){o=c("div"),D(h.$$.fragment),r=g(),D(l.$$.fragment),this.h()},l(E){o=u(E,"DIV",{class:!0});var k=f(o);S(h.$$.fragment,k),r=b(k),S(l.$$.fragment,k),k.forEach(t),this.h()},h(){w(o,"class",_="flex space-x-1 "+m[0])},m(E,k){p(E,o,k),N(h,o,null),a(o,r),N(l,o,null),m[9](o),d=!0,v||(y=qi(Wm,"resize",m[4]),v=!0)},p(E,[k]){const U={};k&1024&&(U.$$scope={dirty:k,ctx:E}),h.$set(U);const j={};k&1024&&(j.$$scope={dirty:k,ctx:E}),l.$set(j),(!d||k&1&&_!==(_="flex space-x-1 "+E[0]))&&w(o,"class",_)},i(E){d||($(h.$$.fragment,E),$(l.$$.fragment,E),d=!0)},o(E){P(h.$$.fragment,E),P(l.$$.fragment,E),d=!1},d(E){E&&t(o),I(h),I(l),m[9](null),v=!1,y()}}}function bm(m){window.open(m)}function c_(m,o,h){let{$$slots:r={},$$scope:l}=o,{options:_=[]}=o,{classNames:d=""}=o,v;const y=_.filter(C=>C.value.includes("colab.research.google.com")),E=_.filter(C=>C.value.includes("studiolab.sagemaker.aws"));function k(){const C=document.querySelector(".prose-doc h1"),A=document.querySelector(".prose-doc h1 > span");if(C&&A){const{width:T}=C.getBoundingClientRect(),{width:L}=A.getBoundingClientRect();let G=0;for(let J=0;J<v.children.length;J++)G+=v.children.item(J).clientWidth;const O=20;T-L<G+O?v.classList.remove("absolute"):v.classList.add("absolute")}}$m(()=>{k()});const U=C=>bm(C),j=C=>bm(C);function z(C){Eh[C?"unshift":"push"](()=>{v=C,h(1,v)})}return m.$$set=C=>{"options"in C&&h(5,_=C.options),"classNames"in C&&h(0,d=C.classNames),"$$scope"in C&&h(10,l=C.$$scope)},[d,v,y,E,k,_,r,U,j,z,l]}class u_ extends To{constructor(o){super();Co(this,o,c_,s_,Do,{options:5,classNames:0})}}function f_(m){let o,h,r,l,_;return{c(){o=c("p"),h=n(`If you place your objects manually on the proper device, be careful to create your optimizer after putting your
model on `),r=c("code"),l=n("accelerator.device"),_=n(" or your training will fail on TPU.")},l(d){o=u(d,"P",{});var v=f(o);h=i(v,`If you place your objects manually on the proper device, be careful to create your optimizer after putting your
model on `),r=u(v,"CODE",{});var y=f(r);l=i(y,"accelerator.device"),y.forEach(t),_=i(v," or your training will fail on TPU."),v.forEach(t)},m(d,v){p(d,o,v),a(o,h),a(o,r),a(r,l),a(o,_)},d(d){d&&t(o)}}}function h_(m){let o,h;return{c(){o=c("p"),h=n(`The actual batch size for your training will be the number of devices used multiplied by the batch size you set in
your script: for instance training on 4 GPUs with a batch size of 16 set when creating the training dataloader will
train at an actual batch size of 64.`)},l(r){o=u(r,"P",{});var l=f(o);h=i(l,`The actual batch size for your training will be the number of devices used multiplied by the batch size you set in
your script: for instance training on 4 GPUs with a batch size of 16 set when creating the training dataloader will
train at an actual batch size of 64.`),l.forEach(t)},m(r,l){p(r,o,l),a(o,h)},d(r){r&&t(o)}}}function d_(m){let o,h,r,l,_;return{c(){o=c("p"),h=n(`Your training dataloader may change length when going through this method: if you run on X GPUs, it will have its
length divided by X (since your actual batch size will be multiplied by X), unless you set
`),r=c("code"),l=n("split_batches=True"),_=n(".")},l(d){o=u(d,"P",{});var v=f(o);h=i(v,`Your training dataloader may change length when going through this method: if you run on X GPUs, it will have its
length divided by X (since your actual batch size will be multiplied by X), unless you set
`),r=u(v,"CODE",{});var y=f(r);l=i(y,"split_batches=True"),y.forEach(t),_=i(v,"."),v.forEach(t)},m(d,v){p(d,o,v),a(o,h),a(o,r),a(r,l),a(o,_)},d(d){d&&t(o)}}}function p_(m){let o,h,r,l,_,d,v,y,E,k,U,j,z,C;return{c(){o=c("p"),h=n(`Like for the training dataloader, passing your validation dataloader through
`),r=c("a"),l=n("prepare()"),_=n(` may change its: if you run on X GPUs, it will have its length divided by X
(since your actual batch size will be multiplied by X), unless you set `),d=c("code"),v=n("split_batches=True"),y=n("."),E=g(),k=c("p"),U=n(`Any instruction using your training dataloader length (for instance if you need the number of total training steps
to create a learning rate scheduler) should go after the call to `),j=c("a"),z=n("prepare()"),C=n("."),this.h()},l(A){o=u(A,"P",{});var T=f(o);h=i(T,`Like for the training dataloader, passing your validation dataloader through
`),r=u(T,"A",{href:!0});var L=f(r);l=i(L,"prepare()"),L.forEach(t),_=i(T,` may change its: if you run on X GPUs, it will have its length divided by X
(since your actual batch size will be multiplied by X), unless you set `),d=u(T,"CODE",{});var G=f(d);v=i(G,"split_batches=True"),G.forEach(t),y=i(T,"."),T.forEach(t),E=b(A),k=u(A,"P",{});var O=f(k);U=i(O,`Any instruction using your training dataloader length (for instance if you need the number of total training steps
to create a learning rate scheduler) should go after the call to `),j=u(O,"A",{href:!0});var J=f(j);z=i(J,"prepare()"),J.forEach(t),C=i(O,"."),O.forEach(t),this.h()},h(){w(r,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(j,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare")},m(A,T){p(A,o,T),a(o,h),a(o,r),a(r,l),a(o,_),a(o,d),a(d,v),a(o,y),p(A,E,T),p(A,k,T),a(k,U),a(k,j),a(j,z),a(k,C)},d(A){A&&t(o),A&&t(E),A&&t(k)}}}function m_(m){let o,h,r,l,_,d,v,y;return{c(){o=c("p"),h=n("The "),r=c("a"),l=n("gather()"),_=n(` method requires the tensors to be all the same size on each process. If
you have tensors of different sizes on each process (for instance when dynamically padding to the maximum length in
a batch), you should use the `),d=c("a"),v=n("pad_across_processes()"),y=n(` method to pad you tensor to the
biggest size across processes.`),this.h()},l(E){o=u(E,"P",{});var k=f(o);h=i(k,"The "),r=u(k,"A",{href:!0});var U=f(r);l=i(U,"gather()"),U.forEach(t),_=i(k,` method requires the tensors to be all the same size on each process. If
you have tensors of different sizes on each process (for instance when dynamically padding to the maximum length in
a batch), you should use the `),d=u(k,"A",{href:!0});var j=f(d);v=i(j,"pad_across_processes()"),j.forEach(t),y=i(k,` method to pad you tensor to the
biggest size across processes.`),k.forEach(t),this.h()},h(){w(r,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.gather"),w(d,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.pad_across_processes")},m(E,k){p(E,o,k),a(o,h),a(o,r),a(r,l),a(o,_),a(o,d),a(d,v),a(o,y)},d(E){E&&t(o)}}}function __(m){let o,h,r,l,_;return{c(){o=c("p"),h=n("Your "),r=c("code"),l=n("Accelerator"),_=n(` object should only be defined inside the training function. This is because the
initialization should be done inside the launcher only.`)},l(d){o=u(d,"P",{});var v=f(o);h=i(v,"Your "),r=u(v,"CODE",{});var y=f(r);l=i(y,"Accelerator"),y.forEach(t),_=i(v,` object should only be defined inside the training function. This is because the
initialization should be done inside the launcher only.`),v.forEach(t)},m(d,v){p(d,o,v),a(o,h),a(o,r),a(r,l),a(o,_)},d(d){d&&t(o)}}}function y_(m){let o,h,r,l,_;return{c(){o=c("p"),h=n("The "),r=c("a"),l=n("notebook_launcher()"),_=n(" does not support the DeepSpeed integration yet."),this.h()},l(d){o=u(d,"P",{});var v=f(o);h=i(v,"The "),r=u(v,"A",{href:!0});var y=f(r);l=i(y,"notebook_launcher()"),y.forEach(t),_=i(v," does not support the DeepSpeed integration yet."),v.forEach(t),this.h()},h(){w(r,"href","/docs/accelerate/main/en/launcher#accelerate.notebook_launcher")},m(d,v){p(d,o,v),a(o,h),a(o,r),a(r,l),a(o,_)},d(d){d&&t(o)}}}function v_(m){let o,h;return{c(){o=c("p"),h=n(`Synchronization the main torch (or CUDA or XLA) random number generator will affect any other potential random
artifacts you could have in your dataset (like random data augmentation) in the sense all processes will get the
same random numbers from the torch random modules (so will apply the same random data augmentation if it\u2019s
controlled by torch).`)},l(r){o=u(r,"P",{});var l=f(o);h=i(l,`Synchronization the main torch (or CUDA or XLA) random number generator will affect any other potential random
artifacts you could have in your dataset (like random data augmentation) in the sense all processes will get the
same random numbers from the torch random modules (so will apply the same random data augmentation if it\u2019s
controlled by torch).`),l.forEach(t)},m(r,l){p(r,o,l),a(o,h)},d(r){r&&t(o)}}}function w_(m){let o,h,r,l,_,d,v,y;return{c(){o=c("p"),h=n(`The randomization part of your custom sampler, batch sampler or iterable dataset should be done using a local
`),r=c("code"),l=n("torch.Generator"),_=n(" object (in PyTorch >= 1.6), see the traditional "),d=c("code"),v=n("RandomSampler"),y=n(", as an example.")},l(E){o=u(E,"P",{});var k=f(o);h=i(k,`The randomization part of your custom sampler, batch sampler or iterable dataset should be done using a local
`),r=u(k,"CODE",{});var U=f(r);l=i(U,"torch.Generator"),U.forEach(t),_=i(k," object (in PyTorch >= 1.6), see the traditional "),d=u(k,"CODE",{});var j=f(d);v=i(j,"RandomSampler"),j.forEach(t),y=i(k,", as an example."),k.forEach(t)},m(E,k){p(E,o,k),a(o,h),a(o,r),a(r,l),a(o,_),a(o,d),a(d,v),a(o,y)},d(E){E&&t(o)}}}function g_(m){let o,h,r,l,_,d,v,y,E,k,U,j,z,C,A,T,L,G,O,J,kt,M,rr,da,Bi,nr,pa,fe,xi,ma,Ji,Xi,No,Fi,Ri,ir,Pt,sr,_a,Wi,cr,At,Y,Vi,Io,Qi,Ki,So,Zi,es,zo,ts,as,Oo,os,ls,jo,rs,ns,Lo,is,ss,ur,W,cs,Uo,us,fs,ya,hs,ds,fr,De,hr,Tt,Ct,ps,va,ms,_s,dr,Dt,pr,Ne,ys,Mo,vs,ws,mr,Ie,_r,V,gs,Go,bs,Es,wa,$s,ks,yr,ga,Ps,vr,Se,wr,ze,As,ba,Ts,Cs,gr,Q,Ds,Ea,Ns,Is,$a,Ss,zs,br,Oe,Os,ka,js,Ls,Er,Nt,he,Us,Ho,Ms,Gs,Yo,Hs,Ys,$r,Pa,qs,kr,de,je,qo,It,Bs,Bo,xs,Pr,K,Js,Aa,Xs,Fs,xo,Rs,Ws,Ar,Le,Vs,Ta,Qs,Ks,Tr,St,Cr,Ue,Zs,Ca,ec,tc,Dr,zt,Nr,Me,Ir,Ge,Sr,pe,He,Jo,Ot,ac,Xo,oc,zr,X,lc,Fo,rc,nc,Ro,ic,sc,Wo,cc,uc,Or,Da,fc,jr,jt,Lr,Ye,hc,Vo,dc,pc,Ur,Z,me,mc,Qo,_c,yc,Ko,vc,wc,gc,_e,bc,Zo,Ec,$c,el,kc,Pc,Ac,Na,Tc,tl,Cc,Mr,qe,Dc,al,Nc,Ic,Gr,Ia,Sc,Hr,Lt,Yr,Sa,zc,qr,za,Oc,Br,Ut,xr,Oa,jc,Jr,Mt,Xr,ja,Lc,Fr,Gt,Rr,La,Uc,Wr,ye,Be,ol,Ht,Mc,ll,Gc,Vr,xe,Hc,Ua,Yc,qc,Qr,Ma,Bc,Kr,Yt,Zr,Je,en,ve,Xe,rl,qt,xc,nl,Jc,tn,Ga,Xc,an,Ha,Fc,on,Fe,il,Rc,Wc,sl,Vc,ln,Ya,Qc,rn,ee,Kc,cl,Zc,eu,ul,tu,au,nn,Bt,sn,Re,ou,xt,lu,ru,cn,te,nu,qa,iu,su,Jt,cu,uu,un,we,We,fl,Xt,fu,hl,hu,fn,Ba,du,hn,ge,Ve,dl,Ft,pu,pl,mu,dn,xa,_u,pn,Rt,mn,Ja,yu,_n,Wt,yn,Qe,vu,ml,wu,gu,vn,Vt,wn,ae,bu,_l,Eu,$u,yl,ku,Pu,gn,be,Ke,vl,Qt,Au,wl,Tu,bn,Xa,Cu,En,Fa,Du,$n,Kt,kn,Ra,Nu,Pn,Ee,Ze,gl,Zt,Iu,bl,Su,An,et,zu,Wa,Ou,ju,Tn,tt,Lu,El,Uu,Mu,Cn,ea,Dn,at,Gu,Va,Hu,Yu,Nn,ta,In,ot,qu,$l,Bu,xu,Sn,$e,lt,kl,aa,Ju,Pl,Xu,zn,q,Fu,Al,Ru,Wu,Tl,Vu,Qu,Cl,Ku,Zu,Dl,ef,tf,On,ke,rt,Nl,oa,af,Il,of,jn,nt,lf,Sl,rf,nf,Ln,la,Un,Qa,sf,Mn,Ka,cf,Gn,ra,Hn,Pe,it,zl,na,uf,Ol,ff,Yn,Za,hf,qn,oe,df,jl,pf,mf,Ll,_f,yf,Bn,st,xn,Ae,ct,Ul,ia,vf,Ml,wf,Jn,ut,gf,Gl,bf,Ef,Xn,le,$f,eo,kf,Pf,to,Af,Tf,Fn,ft,Cf,ao,Df,Nf,Rn,re,Hl,If,Sf,sa,zf,oo,Of,jf,Lf,ca,Uf,lo,Mf,Gf,Wn,F,Hf,Yl,Yf,qf,ql,Bf,xf,Bl,Jf,Xf,Vn,ne,Ff,ro,Rf,Wf,xl,Vf,Qf,Qn,ht,Jl,Kf,Zf,ua,eh,Xl,th,ah,Kn,no,oh,Zn,dt,Te,lh,Fl,rh,nh,Rl,ih,sh,ch,Wl,uh,ei,R,fh,Vl,hh,dh,io,ph,mh,Ql,_h,yh,ti,pt,ai,mt,oi,_t,vh,so,wh,gh,li;return r=new u_({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/accelerate_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/accelerate_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/accelerate_doc/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/accelerate_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/accelerate_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/accelerate_doc/tensorflow/quicktour.ipynb"}]}}),y=new x({}),O=new x({}),Pt=new H({props:{code:`from accelerate import Accelerator

accelerator = Accelerator()`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> Accelerator

accelerator = Accelerator()`}}),De=new Ce({props:{warning:"&lcub;true}",$$slots:{default:[f_]},$$scope:{ctx:m}}}),Dt=new H({props:{code:"model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)",highlighted:"model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)"}}),Ie=new Ce({props:{$$slots:{default:[h_]},$$scope:{ctx:m}}}),Se=new Ce({props:{warning:"&lcub;true}",$$slots:{default:[d_]},$$scope:{ctx:m}}}),It=new x({}),St=new H({props:{code:"validation_dataloader = accelerator.prepare(validation_dataloader)",highlighted:"validation_dataloader = accelerator.prepare(validation_dataloader)"}}),zt=new H({props:{code:`for inputs, targets in validation_dataloader:
    predictions = model(inputs)
    # Gather all predictions and targets
    all_predictions = accelerator.gather(predictions)
    all_targets = accelerator.gather(targets)
    # Example of use with a *Datasets.Metric*
    metric.add_batch(all_predictions, all_targets)`,highlighted:`<span class="hljs-keyword">for</span> inputs, targets <span class="hljs-keyword">in</span> validation_dataloader:
    predictions = model(inputs)
    <span class="hljs-comment"># Gather all predictions and targets</span>
    all_predictions = accelerator.gather(predictions)
    all_targets = accelerator.gather(targets)
    <span class="hljs-comment"># Example of use with a *Datasets.Metric*</span>
    metric.add_batch(all_predictions, all_targets)`}}),Me=new Ce({props:{warning:"&lcub;true}",$$slots:{default:[p_]},$$scope:{ctx:m}}}),Ge=new Ce({props:{warning:"&lcub;true}",$$slots:{default:[m_]},$$scope:{ctx:m}}}),Ot=new x({}),jt=new H({props:{code:"accelerate config",highlighted:"accelerate config"}}),Lt=new H({props:{code:"accelerate test",highlighted:'accelerate <span class="hljs-built_in">test</span>'}}),Ut=new H({props:{code:"accelerate test --config_file path_to_config.yaml",highlighted:'accelerate <span class="hljs-built_in">test</span> --config_file path_to_config.yaml'}}),Mt=new H({props:{code:"accelerate launch path_to_script.py --args_for_the_script",highlighted:"accelerate launch path_to_script.py --args_for_the_script"}}),Gt=new H({props:{code:"accelerate launch --config_file path_to_config.yaml path_to_script.py --args_for_the_script",highlighted:"accelerate launch --config_file path_to_config.yaml path_to_script.py --args_for_the_script"}}),Ht=new x({}),Yt=new H({props:{code:`from accelerate import notebook_launcher

notebook_launcher(training_function)`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> notebook_launcher

notebook_launcher(training_function)`}}),Je=new Ce({props:{warning:"&lcub;true}",$$slots:{default:[__]},$$scope:{ctx:m}}}),qt=new x({}),Bt=new H({props:{code:`from accelerate import DistributedType

if accelerator.distributed_type == DistributedType.TPU:
    # do something of static shape
else:
    # go crazy and be dynamic`,highlighted:`<span class="hljs-keyword">from</span> accelerate <span class="hljs-keyword">import</span> DistributedType

<span class="hljs-keyword">if</span> accelerator.distributed_type == DistributedType.TPU:
    <span class="hljs-comment"># do something of static shape</span>
<span class="hljs-keyword">else</span>:
    <span class="hljs-comment"># go crazy and be dynamic</span>`}}),Xt=new x({}),Ft=new x({}),Rt=new H({props:{code:`if accelerator.is_local_main_process:
    # Is executed once per server`,highlighted:`<span class="hljs-keyword">if</span> accelerator.is_local_main_process:
    <span class="hljs-comment"># Is executed once per server</span>`}}),Wt=new H({props:{code:`from tqdm.auto import tqdm

progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)`,highlighted:`<span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm

progress_bar = tqdm(<span class="hljs-built_in">range</span>(args.max_train_steps), disable=<span class="hljs-keyword">not</span> accelerator.is_local_main_process)`}}),Vt=new H({props:{code:`if accelerator.is_main_process:
    # Is executed once only`,highlighted:`<span class="hljs-keyword">if</span> accelerator.is_main_process:
    <span class="hljs-comment"># Is executed once only</span>`}}),Qt=new x({}),Kt=new H({props:{code:"accelerator.wait_for_everyone()",highlighted:'accelerator.wait<span class="hljs-constructor">_for_everyone()</span>'}}),Zt=new x({}),ea=new H({props:{code:`accelerator.wait_for_everyone()
unwrapped_model = accelerator.unwrap_model(model)
accelerator.save(unwrapped_model.state_dict(), filename)`,highlighted:`accelerator.wait<span class="hljs-constructor">_for_everyone()</span>
unwrapped_model = accelerator.unwrap<span class="hljs-constructor">_model(<span class="hljs-params">model</span>)</span>
accelerator.save(unwrapped_model.state<span class="hljs-constructor">_dict()</span>, filename)`}}),ta=new H({props:{code:`unwrapped_model = accelerator.unwrap_model(model)
unwrapped_model.load_state_dict(torch.load(filename))`,highlighted:`unwrapped_model = accelerator.unwrap<span class="hljs-constructor">_model(<span class="hljs-params">model</span>)</span>
unwrapped_model.load<span class="hljs-constructor">_state_dict(<span class="hljs-params">torch</span>.<span class="hljs-params">load</span>(<span class="hljs-params">filename</span>)</span>)`}}),aa=new x({}),oa=new x({}),la=new H({props:{code:`with accelerator.autocast():
    loss = complex_loss_function(outputs, target):`,highlighted:`<span class="hljs-keyword">with</span> accelerator.autocast<span class="hljs-literal">()</span>:
    loss = complex<span class="hljs-constructor">_loss_function(<span class="hljs-params">outputs</span>, <span class="hljs-params">target</span>)</span>:`}}),ra=new H({props:{code:`if not accelerator.optimizer_step_was_skipped:
    lr_scheduler.step()`,highlighted:`<span class="hljs-keyword">if</span> not accelerator<span class="hljs-selector-class">.optimizer_step_was_skipped</span>:
    lr_scheduler<span class="hljs-selector-class">.step</span>()`}}),na=new x({}),st=new Ce({props:{warning:"&lcub;true}",$$slots:{default:[y_]},$$scope:{ctx:m}}}),ia=new x({}),pt=new Ce({props:{warning:"&lcub;true}",$$slots:{default:[v_]},$$scope:{ctx:m}}}),mt=new Ce({props:{$$slots:{default:[w_]},$$scope:{ctx:m}}}),{c(){o=c("meta"),h=g(),D(r.$$.fragment),l=g(),_=c("h1"),d=c("a"),v=c("span"),D(y.$$.fragment),E=g(),k=c("span"),U=n("Quick tour"),j=g(),z=c("p"),C=n("Let\u2019s have a look at a look at \u{1F917} Accelerate main features and traps to avoid."),A=g(),T=c("h2"),L=c("a"),G=c("span"),D(O.$$.fragment),J=g(),kt=c("span"),M=n("Main use"),rr=g(),da=c("p"),Bi=n("To use \u{1F917} Accelerate in your own script, you have to change four things:"),nr=g(),pa=c("ol"),fe=c("li"),xi=n("Import the "),ma=c("a"),Ji=n("Accelerator"),Xi=n(" main class instantiate one in an "),No=c("code"),Fi=n("accelerator"),Ri=n(" object:"),ir=g(),D(Pt.$$.fragment),sr=g(),_a=c("p"),Wi=n(`This should happen as early as possible in your training script as it will initialize everything necessary for
distributed training. You don\u2019t need to indicate the kind of environment you are in (just one machine with a GPU, one
match with several GPUs, several machines with multiple GPUs or a TPU), the library will detect this automatically.`),cr=g(),At=c("ol"),Y=c("li"),Vi=n("Remove the call "),Io=c("code"),Qi=n(".to(device)"),Ki=n(" or "),So=c("code"),Zi=n(".cuda()"),es=n(" for your model and input data. The "),zo=c("code"),ts=n("accelerator"),as=n(` object
will handle this for you and place all those objects on the right device for you. If you know what you\u2019re doing, you
can leave those `),Oo=c("code"),os=n(".to(device)"),ls=n(" calls but you should use the device provided by the "),jo=c("code"),rs=n("accelerator"),ns=n(` object:
`),Lo=c("code"),is=n("accelerator.device"),ss=n("."),ur=g(),W=c("p"),cs=n("To fully deactivate the automatic device placement, pass along "),Uo=c("code"),us=n("device_placement=False"),fs=n(` when initializing your
`),ya=c("a"),hs=n("Accelerator"),ds=n("."),fr=g(),D(De.$$.fragment),hr=g(),Tt=c("ol"),Ct=c("li"),ps=n(`Pass all objects relevant to training (optimizer, model, training dataloader) to the
`),va=c("a"),ms=n("prepare()"),_s=n(" method. This will make sure everything is ready for training."),dr=g(),D(Dt.$$.fragment),pr=g(),Ne=c("p"),ys=n(`In particular, your training dataloader will be sharded accross all GPUs/TPU cores available so that each one sees a
different portion of the training dataset. Also, the random states of all processes will be synchronized at the
beginning of each iteration through your dataloader, to make sure the data is shuffled the same way (if you decided to
use `),Mo=c("code"),vs=n("shuffle=True"),ws=n(" or any kind of random sampler)."),mr=g(),D(Ie.$$.fragment),_r=g(),V=c("p"),gs=n("Alternatively, you can use the option "),Go=c("code"),bs=n("split_batches=True"),Es=n(` when creating initializing your
`),wa=c("a"),$s=n("Accelerator"),ks=n(`, in which case the batch size will always stay the same, whether your run your
script on 1, 2, 4 or 64 GPUs.`),yr=g(),ga=c("p"),Ps=n(`You should execute this instruction as soon as all objects for training are created, before starting your actual
training loop.`),vr=g(),D(Se.$$.fragment),wr=g(),ze=c("p"),As=n(`Any instruction using your training dataloader length (for instance if you need the number of total training steps
to create a learning rate scheduler) should go after the call to `),ba=c("a"),Ts=n("prepare()"),Cs=n("."),gr=g(),Q=c("p"),Ds=n("You can perfectly send your dataloader to "),Ea=c("a"),Ns=n("prepare()"),Is=n(` on its own, but it\u2019s best to send the
model and optimizer to `),$a=c("a"),Ss=n("prepare()"),zs=n(" together."),br=g(),Oe=c("p"),Os=n("You may or may not want to send your validation dataloader to "),ka=c("a"),js=n("prepare()"),Ls=n(`, depending on
whether you want to run distributed evaluation or not (see below).`),Er=g(),Nt=c("ol"),he=c("li"),Us=n("Replace the line "),Ho=c("code"),Ms=n("loss.backward()"),Gs=n(" by "),Yo=c("code"),Hs=n("accelerator.backward(loss)"),Ys=n("."),$r=g(),Pa=c("p"),qs=n(`And you\u2019re all set! With all these changes, your script will run on your local machine as well as on multiple GPUs or a
TPU! You can either use your favorite tool to launch the distributed training, or you can use the \u{1F917} Accelerate
launcher.`),kr=g(),de=c("h2"),je=c("a"),qo=c("span"),D(It.$$.fragment),Bs=g(),Bo=c("span"),xs=n("Distributed evaluation"),Pr=g(),K=c("p"),Js=n(`You can perform regular evaluation in your training script, if you leave your validation dataloader out of the
`),Aa=c("a"),Xs=n("prepare()"),Fs=n(` method. In this case, you will need to put the input data on the
`),xo=c("code"),Rs=n("accelerator.device"),Ws=n(" manually."),Ar=g(),Le=c("p"),Vs=n("To perform distributed evaluation, send along your validation dataloader to the "),Ta=c("a"),Qs=n("prepare()"),Ks=n(`
method:`),Tr=g(),D(St.$$.fragment),Cr=g(),Ue=c("p"),Zs=n(`Like for your training dataloader, it will mean that (should you run your script on multiple devices) each device will
only see part of the evaluation data. This means you will need to group your predictions together. This is very easy to
do with the `),Ca=c("a"),ec=n("gather()"),tc=n(" method."),Dr=g(),D(zt.$$.fragment),Nr=g(),D(Me.$$.fragment),Ir=g(),D(Ge.$$.fragment),Sr=g(),pe=c("h2"),He=c("a"),Jo=c("span"),D(Ot.$$.fragment),ac=g(),Xo=c("span"),oc=n("Launching your distributed script"),zr=g(),X=c("p"),lc=n("You can use the regular commands to launch your distributed training (like "),Fo=c("code"),rc=n("torch.distributed.launch"),nc=n(` for
PyTorch), they are fully compatible with \u{1F917} Accelerate. The only caveat here is that \u{1F917} Accelerate uses the environment
to determine all useful information, so `),Ro=c("code"),ic=n("torch.distributed.launch"),sc=n(" should be used with the flag "),Wo=c("code"),cc=n("--use_env"),uc=n("."),Or=g(),Da=c("p"),fc=n(`\u{1F917} Accelerate also provides a CLI tool that unifies all launcher, so you only have to remember one command. To use it,
just run`),jr=g(),D(jt.$$.fragment),Lr=g(),Ye=c("p"),hc=n("on your machine and reply to the questions asked. This will save a "),Vo=c("em"),dc=n("default_config.yaml"),pc=n(` file in your cache folder for
\u{1F917} Accelerate. That cache folder is (with decreasing order of priority):`),Ur=g(),Z=c("ul"),me=c("li"),mc=n("The content of your environment variable "),Qo=c("code"),_c=n("HF_HOME"),yc=n(" suffixed with "),Ko=c("em"),vc=n("accelerate"),wc=n("."),gc=g(),_e=c("li"),bc=n("If it does not exist, the content of your environment variable "),Zo=c("code"),Ec=n("XDG_CACHE_HOME"),$c=n(` suffixed with
`),el=c("em"),kc=n("huggingface/accelerate"),Pc=n("."),Ac=g(),Na=c("li"),Tc=n("If this does not exist either, the folder "),tl=c("em"),Cc=n("~/.cache/huggingface/accelerate"),Mr=g(),qe=c("p"),Dc=n("You can also specify with the flag "),al=c("code"),Nc=n("--config_file"),Ic=n(" the location of the file you want to save."),Gr=g(),Ia=c("p"),Sc=n("Once this is done, you can test everything is going well on your setup by running"),Hr=g(),D(Lt.$$.fragment),Yr=g(),Sa=c("p"),zc=n(`This will launch a short script that will test the distributed environment. If it runs fine, you are ready for the next
step!`),qr=g(),za=c("p"),Oc=n("Note that if you specified a location for the config file in the previous step, you need to pass it here as well:"),Br=g(),D(Ut.$$.fragment),xr=g(),Oa=c("p"),jc=n("Now that this is done, you can run your script with the following command:"),Jr=g(),D(Mt.$$.fragment),Xr=g(),ja=c("p"),Lc=n("If you stored the config file in a non-default location, you can indicate it to the launcher like his:"),Fr=g(),D(Gt.$$.fragment),Rr=g(),La=c("p"),Uc=n("You can also override any of the arguments determined by your config file, see TODO: insert ref here."),Wr=g(),ye=c("h2"),Be=c("a"),ol=c("span"),D(Ht.$$.fragment),Mc=g(),ll=c("span"),Gc=n("Launching training from a notebook"),Vr=g(),xe=c("p"),Hc=n("In Accelerate 0.3.0, a new "),Ua=c("a"),Yc=n("notebook_launcher()"),qc=n(` has been introduced to help you launch your training
function from a notebook. This launcher supports launching a training with TPUs on Colab or Kaggle, as well as training
on several GPUs (if the machine on which you are running your notebook has them).`),Qr=g(),Ma=c("p"),Bc=n(`Just define a function responsible for your whole training and/or evaluation in a cell of the notebook, then execute a
cell with the following code:`),Kr=g(),D(Yt.$$.fragment),Zr=g(),D(Je.$$.fragment),en=g(),ve=c("h2"),Xe=c("a"),rl=c("span"),D(qt.$$.fragment),xc=g(),nl=c("span"),Jc=n("Training on TPU"),tn=g(),Ga=c("p"),Xc=n(`If you want to launch your script on TPUs, there are a few caveats you should be aware of. Behind the scenes, the TPUs
will create a graph of all the operations happening in your training step (forward pass, backward pass and optimizer
step). This is why your first step of training will always be very long as building and compiling this graph for
optimizations takes some time.`),an=g(),Ha=c("p"),Fc=n(`The good news is that this compilation will be cached so the second step and all the following will be much faster. The
bas news is that it only applies if all of your steps do exactly the same operations, which implies:`),on=g(),Fe=c("ul"),il=c("li"),Rc=n("having all tensors of the same length in all your lengths"),Wc=g(),sl=c("li"),Vc=n("having static code (i.e., not a for loop of length that could change from step to step)"),ln=g(),Ya=c("p"),Qc=n(`Having any of the things above change between two steps will trigger a new compilation which will, once again, take a
lot of time. In practice, that means you must take special care to have all your tensors in your inputs of the same
shape (so no dynamic padding for instance if you are in an NLP problem) and should not use layer with for loops that
have different lengths depending on the inputs (such as an LSTM) or the training will be excruciatingly slow.`),rn=g(),ee=c("p"),Kc=n("To introduce special behavior in your script for TPUs you can check the "),cl=c("code"),Zc=n("distributed_type"),eu=n(` of your
`),ul=c("code"),tu=n("accelerator"),au=n(":"),nn=g(),D(Bt.$$.fragment),sn=g(),Re=c("p"),ou=n("The "),xt=c("a"),lu=n("NLP example"),ru=n(` shows an example in
situation with dynamic padding.`),cn=g(),te=c("p"),nu=n(`One last thing to pay close attnetion to: if your model has tied weights (such as language models which tie the weights
of the embedding matrix with the weights of the decoder), moving this model to the TPU (either yourself or after you
passed your model to `),qa=c("a"),iu=n("prepare()"),su=n(`) will break the tying. You will need to retie the weights
after. You can find an example of this in the `),Jt=c("a"),cu=n("run_clm_no_trainer"),uu=n(` script in
the Transformers repository.`),un=g(),we=c("h2"),We=c("a"),fl=c("span"),D(Xt.$$.fragment),fu=g(),hl=c("span"),hu=n("Other caveats"),fn=g(),Ba=c("p"),du=n("We list here all smaller issues you could have in your script conversion and how to resolve them."),hn=g(),ge=c("h3"),Ve=c("a"),dl=c("span"),D(Ft.$$.fragment),pu=g(),pl=c("span"),mu=n("Execute a statement only on one processes"),dn=g(),xa=c("p"),_u=n(`Some of your instructions only need to run for one process on a given server: for instance a data download or a log
statement. To do this, wrap the statement in a test like this:`),pn=g(),D(Rt.$$.fragment),mn=g(),Ja=c("p"),yu=n(`Another example is progress bars: to avoid having multiple progress bars in your output, you should only display one on
the local main process:`),_n=g(),D(Wt.$$.fragment),yn=g(),Qe=c("p"),vu=n("The "),ml=c("em"),wu=n("local"),gu=n(` means per machine: if you are running your training on two servers with several GPUs, the instruction will
be executed once on each of those servers. If you need to execute something only once for all processes (and not per
machine) for instance, uploading the final model to the \u{1F917} model hub, wrap it in a test like this:`),vn=g(),D(Vt.$$.fragment),wn=g(),ae=c("p"),bu=n("For printing statements you only want executed once per machine, you can just replace the "),_l=c("code"),Eu=n("print"),$u=n(` function by
`),yl=c("code"),ku=n("accelerator.print"),Pu=n("."),gn=g(),be=c("h3"),Ke=c("a"),vl=c("span"),D(Qt.$$.fragment),Au=g(),wl=c("span"),Tu=n("Defer execution"),bn=g(),Xa=c("p"),Cu=n(`When you run your usual script, instructions are executed in order. Using \u{1F917} Accelerate to deploy your script on several
GPUs at the same time introduces a complication: while each process executes all instructions in order, some may be
faster than others.`),En=g(),Fa=c("p"),Du=n(`You might need to wait for all processes to have reached a certain point before executing a given instruction. For
instance, you shouldn\u2019t save a model before being sure every process is done with training. To do this, just write the
following line in your code:`),$n=g(),D(Kt.$$.fragment),kn=g(),Ra=c("p"),Nu=n(`This instruction will block all the processes that arrive them first until all the other processes have reached that
point (if you run your script on just one GPU or CPU, this wont\u2019 do anything).`),Pn=g(),Ee=c("h3"),Ze=c("a"),gl=c("span"),D(Zt.$$.fragment),Iu=g(),bl=c("span"),Su=n("Saving/loading a model"),An=g(),et=c("p"),zu=n(`Saving the model you trained might need a bit of adjustment: first you should wait for all processes to reach that
point in the script as shown above, and then, you should unwrap your model before saving it. This is because when going
through the `),Wa=c("a"),Ou=n("prepare()"),ju=n(` method, your model may have been placed inside a bigger model,
which deals with the distributed training. This in turn means that saving your model state dictionary without taking
any precaution will take that potential extra layer into account, and you will end up with weights you can\u2019t load back
in your base model.`),Tn=g(),tt=c("p"),Lu=n("This is why it\u2019s recommended to "),El=c("em"),Uu=n("unwrap"),Mu=n(" your model first. Here is an example:"),Cn=g(),D(ea.$$.fragment),Dn=g(),at=c("p"),Gu=n(`If your script contains a logic to load checkpoint, we also recommend you load your weights in the unwrapped model
(this is only useful if you use the load function after making your model go through
`),Va=c("a"),Hu=n("prepare()"),Yu=n("). Here is an example:"),Nn=g(),D(ta.$$.fragment),In=g(),ot=c("p"),qu=n("Note that since all the model parameters are references to tensors, this will load your weights inside "),$l=c("code"),Bu=n("model"),xu=n("."),Sn=g(),$e=c("h3"),lt=c("a"),kl=c("span"),D(aa.$$.fragment),Ju=g(),Pl=c("span"),Xu=n("Gradient clipping"),zn=g(),q=c("p"),Fu=n(`If you are using gradient clipping in your script, you should replace the calls to
`),Al=c("code"),Ru=n("torch.nn.utils.clip_grad_norm_"),Wu=n(" or "),Tl=c("code"),Vu=n("torch.nn.utils.clip_grad_value_"),Qu=n(" with "),Cl=c("code"),Ku=n("accelerator.clip_grad_norm_"),Zu=n(`
and `),Dl=c("code"),ef=n("accelerator.clip_grad_value_"),tf=n(" respectively."),On=g(),ke=c("h3"),rt=c("a"),Nl=c("span"),D(oa.$$.fragment),af=g(),Il=c("span"),of=n("Mixed Precision training"),jn=g(),nt=c("p"),lf=n(`If you are running your training in Mixed Precision with Accelerate, you will get the best result with your loss being
computed inside your model (like in Transformer models for instance). Every computation outside of the model will be
executed in full precision (which is generally what you want for loss computation, expecially if it involves a
softmax). However you might want to put your loss computation inside the `),Sl=c("em"),rf=n("accelerator.autocast"),nf=n(" context manager:"),Ln=g(),D(la.$$.fragment),Un=g(),Qa=c("p"),sf=n(`Another caveat with Mixed Precision training is that the gradient will skip a few updates at the beginning and
sometimes during training: because of the dynamic loss scaling strategy, there are points during training where the
gradients have overflown, and the loss scaling factor is reduced to avoid this happening again at the next step.`),Mn=g(),Ka=c("p"),cf=n(`This means that you may update your learning rate scheduler when there was no update, which is fine in general, but may
have an impact when you have very little training data, or if the first learning rate values of your scheduler are very
important. In this case, you can skip the learning rate scheduler updates when the optimizer step was not done like
this:`),Gn=g(),D(ra.$$.fragment),Hn=g(),Pe=c("h3"),it=c("a"),zl=c("span"),D(na.$$.fragment),uf=g(),Ol=c("span"),ff=n("DeepSpeed"),Yn=g(),Za=c("p"),hf=n(`DeepSpeed support is experimental, so the underlying API will evolve in the near future and may have some slight
breaking changes. In particular, \u{1F917} Accelerate does not support DeepSpeed config you have written yourself yet, this
will be added in a next version.`),qn=g(),oe=c("p"),df=n("One main caveat for the DeepSpeed integration is that the DeepSpeed launcher always passes a "),jl=c("code"),pf=n("local_rank"),mf=n(` variable to
the training script, so your training script should accept it (whether you launch training with the DeepSpeed launcher
or `),Ll=c("code"),_f=n("accelerate launch"),yf=n(")."),Bn=g(),D(st.$$.fragment),xn=g(),Ae=c("h2"),ct=c("a"),Ul=c("span"),D(ia.$$.fragment),vf=g(),Ml=c("span"),wf=n("Internal mechanism"),Jn=g(),ut=c("p"),gf=n(`Internally, the library works by first analyzing the environment in which the script is launched to determine which
kind of distributed setup is used, how many different processes there are and which one the current script is in. All
that information is stored in the `),Gl=c("code"),bf=n("~AcceleratorState"),Ef=n("."),Xn=g(),le=c("p"),$f=n("This class is initialized the first time you instantiate a "),eo=c("a"),kf=n("Accelerator"),Pf=n(` as well as performing any
specific initialization your distributed setup needs. Its state is then uniquely shared through all instances of
`),to=c("a"),Af=n("AcceleratorState"),Tf=n("."),Fn=g(),ft=c("p"),Cf=n("Then, when calling "),ao=c("a"),Df=n("prepare()"),Nf=n(", the library:"),Rn=g(),re=c("ul"),Hl=c("li"),If=n("wraps your model(s) in the container adapted for the distributed setup,"),Sf=g(),sa=c("li"),zf=n("wraps your optimizer(s) in a "),oo=c("a"),Of=n("AcceleratedOptimizer"),jf=n(","),Lf=g(),ca=c("li"),Uf=n("creates a new version of your dataloader(s) in a "),lo=c("a"),Mf=n("DataLoaderShard"),Gf=n("."),Wn=g(),F=c("p"),Hf=n(`While the model(s) and optimizer(s) are just put in simple wrappers, the dataloader(s) are re-created. This is mostly
because PyTorch does not let the user change the `),Yl=c("code"),Yf=n("batch_sampler"),qf=n(` of a dataloader once it\u2019s been created and the
library handles the sharding of your data between processes by changing that `),ql=c("code"),Bf=n("batch_sampler"),xf=n(` to yield every other
`),Bl=c("code"),Jf=n("num_processes"),Xf=n(" batches."),Vn=g(),ne=c("p"),Ff=n("The "),ro=c("a"),Rf=n("DataLoaderShard"),Wf=n(" subclasses "),xl=c("code"),Vf=n("DataLoader"),Qf=n(" to add the following functionality:"),Qn=g(),ht=c("ul"),Jl=c("li"),Kf=n(`it synchronizes the appropriate random number generator of all processes at each new iteration, to ensure any
randomization (like shuffling) is done the exact same way across processes.`),Zf=g(),ua=c("li"),eh=n(`it puts the batches on the proper device before yielding them (unless you have opted out of
`),Xl=c("code"),th=n("device_placement=True"),ah=n(")."),Kn=g(),no=c("p"),oh=n("The random number generator synchronization will by default synchronize:"),Zn=g(),dt=c("ul"),Te=c("li"),lh=n("the "),Fl=c("code"),rh=n("generator"),nh=n(" attribute of a given sampler (like the PyTorch "),Rl=c("code"),ih=n("RandomSampler"),sh=n(") for PyTorch >= 1.6"),ch=g(),Wl=c("li"),uh=n("the main random number generator in PyTorch <=1.5.1"),ei=g(),R=c("p"),fh=n("You can choose which random number generator(s) to synchronize with the "),Vl=c("code"),hh=n("rng_types"),dh=n(` argument of the main
`),io=c("a"),ph=n("Accelerator"),mh=n(". In PyTorch >= 1.6, it is recommended to rely on local "),Ql=c("code"),_h=n("generator"),yh=n(` to avoid
setting the same seed in the main random number generator in all processes.`),ti=g(),D(pt.$$.fragment),ai=g(),D(mt.$$.fragment),oi=g(),_t=c("p"),vh=n("See more details about the internal in the "),so=c("a"),wh=n("Internals page"),gh=n("."),this.h()},l(e){const s=Im('[data-svelte="svelte-1phssyn"]',document.head);o=u(s,"META",{name:!0,content:!0}),s.forEach(t),h=b(e),S(r.$$.fragment,e),l=b(e),_=u(e,"H1",{class:!0});var fa=f(_);d=u(fa,"A",{id:!0,class:!0,href:!0});var Kl=f(d);v=u(Kl,"SPAN",{});var Zl=f(v);S(y.$$.fragment,Zl),Zl.forEach(t),Kl.forEach(t),E=b(fa),k=u(fa,"SPAN",{});var er=f(k);U=i(er,"Quick tour"),er.forEach(t),fa.forEach(t),j=b(e),z=u(e,"P",{});var tr=f(z);C=i(tr,"Let\u2019s have a look at a look at \u{1F917} Accelerate main features and traps to avoid."),tr.forEach(t),A=b(e),T=u(e,"H2",{class:!0});var ha=f(T);L=u(ha,"A",{id:!0,class:!0,href:!0});var ar=f(L);G=u(ar,"SPAN",{});var or=f(G);S(O.$$.fragment,or),or.forEach(t),ar.forEach(t),J=b(ha),kt=u(ha,"SPAN",{});var lr=f(kt);M=i(lr,"Main use"),lr.forEach(t),ha.forEach(t),rr=b(e),da=u(e,"P",{});var $h=f(da);Bi=i($h,"To use \u{1F917} Accelerate in your own script, you have to change four things:"),$h.forEach(t),nr=b(e),pa=u(e,"OL",{});var kh=f(pa);fe=u(kh,"LI",{});var co=f(fe);xi=i(co,"Import the "),ma=u(co,"A",{href:!0});var Ph=f(ma);Ji=i(Ph,"Accelerator"),Ph.forEach(t),Xi=i(co," main class instantiate one in an "),No=u(co,"CODE",{});var Ah=f(No);Fi=i(Ah,"accelerator"),Ah.forEach(t),Ri=i(co," object:"),co.forEach(t),kh.forEach(t),ir=b(e),S(Pt.$$.fragment,e),sr=b(e),_a=u(e,"P",{});var Th=f(_a);Wi=i(Th,`This should happen as early as possible in your training script as it will initialize everything necessary for
distributed training. You don\u2019t need to indicate the kind of environment you are in (just one machine with a GPU, one
match with several GPUs, several machines with multiple GPUs or a TPU), the library will detect this automatically.`),Th.forEach(t),cr=b(e),At=u(e,"OL",{start:!0});var Ch=f(At);Y=u(Ch,"LI",{});var B=f(Y);Vi=i(B,"Remove the call "),Io=u(B,"CODE",{});var Dh=f(Io);Qi=i(Dh,".to(device)"),Dh.forEach(t),Ki=i(B," or "),So=u(B,"CODE",{});var Nh=f(So);Zi=i(Nh,".cuda()"),Nh.forEach(t),es=i(B," for your model and input data. The "),zo=u(B,"CODE",{});var Ih=f(zo);ts=i(Ih,"accelerator"),Ih.forEach(t),as=i(B,` object
will handle this for you and place all those objects on the right device for you. If you know what you\u2019re doing, you
can leave those `),Oo=u(B,"CODE",{});var Sh=f(Oo);os=i(Sh,".to(device)"),Sh.forEach(t),ls=i(B," calls but you should use the device provided by the "),jo=u(B,"CODE",{});var zh=f(jo);rs=i(zh,"accelerator"),zh.forEach(t),ns=i(B,` object:
`),Lo=u(B,"CODE",{});var Oh=f(Lo);is=i(Oh,"accelerator.device"),Oh.forEach(t),ss=i(B,"."),B.forEach(t),Ch.forEach(t),ur=b(e),W=u(e,"P",{});var uo=f(W);cs=i(uo,"To fully deactivate the automatic device placement, pass along "),Uo=u(uo,"CODE",{});var jh=f(Uo);us=i(jh,"device_placement=False"),jh.forEach(t),fs=i(uo,` when initializing your
`),ya=u(uo,"A",{href:!0});var Lh=f(ya);hs=i(Lh,"Accelerator"),Lh.forEach(t),ds=i(uo,"."),uo.forEach(t),fr=b(e),S(De.$$.fragment,e),hr=b(e),Tt=u(e,"OL",{start:!0});var Uh=f(Tt);Ct=u(Uh,"LI",{});var ri=f(Ct);ps=i(ri,`Pass all objects relevant to training (optimizer, model, training dataloader) to the
`),va=u(ri,"A",{href:!0});var Mh=f(va);ms=i(Mh,"prepare()"),Mh.forEach(t),_s=i(ri," method. This will make sure everything is ready for training."),ri.forEach(t),Uh.forEach(t),dr=b(e),S(Dt.$$.fragment,e),pr=b(e),Ne=u(e,"P",{});var ni=f(Ne);ys=i(ni,`In particular, your training dataloader will be sharded accross all GPUs/TPU cores available so that each one sees a
different portion of the training dataset. Also, the random states of all processes will be synchronized at the
beginning of each iteration through your dataloader, to make sure the data is shuffled the same way (if you decided to
use `),Mo=u(ni,"CODE",{});var Gh=f(Mo);vs=i(Gh,"shuffle=True"),Gh.forEach(t),ws=i(ni," or any kind of random sampler)."),ni.forEach(t),mr=b(e),S(Ie.$$.fragment,e),_r=b(e),V=u(e,"P",{});var fo=f(V);gs=i(fo,"Alternatively, you can use the option "),Go=u(fo,"CODE",{});var Hh=f(Go);bs=i(Hh,"split_batches=True"),Hh.forEach(t),Es=i(fo,` when creating initializing your
`),wa=u(fo,"A",{href:!0});var Yh=f(wa);$s=i(Yh,"Accelerator"),Yh.forEach(t),ks=i(fo,`, in which case the batch size will always stay the same, whether your run your
script on 1, 2, 4 or 64 GPUs.`),fo.forEach(t),yr=b(e),ga=u(e,"P",{});var qh=f(ga);Ps=i(qh,`You should execute this instruction as soon as all objects for training are created, before starting your actual
training loop.`),qh.forEach(t),vr=b(e),S(Se.$$.fragment,e),wr=b(e),ze=u(e,"P",{});var ii=f(ze);As=i(ii,`Any instruction using your training dataloader length (for instance if you need the number of total training steps
to create a learning rate scheduler) should go after the call to `),ba=u(ii,"A",{href:!0});var Bh=f(ba);Ts=i(Bh,"prepare()"),Bh.forEach(t),Cs=i(ii,"."),ii.forEach(t),gr=b(e),Q=u(e,"P",{});var ho=f(Q);Ds=i(ho,"You can perfectly send your dataloader to "),Ea=u(ho,"A",{href:!0});var xh=f(Ea);Ns=i(xh,"prepare()"),xh.forEach(t),Is=i(ho,` on its own, but it\u2019s best to send the
model and optimizer to `),$a=u(ho,"A",{href:!0});var Jh=f($a);Ss=i(Jh,"prepare()"),Jh.forEach(t),zs=i(ho," together."),ho.forEach(t),br=b(e),Oe=u(e,"P",{});var si=f(Oe);Os=i(si,"You may or may not want to send your validation dataloader to "),ka=u(si,"A",{href:!0});var Xh=f(ka);js=i(Xh,"prepare()"),Xh.forEach(t),Ls=i(si,`, depending on
whether you want to run distributed evaluation or not (see below).`),si.forEach(t),Er=b(e),Nt=u(e,"OL",{start:!0});var Fh=f(Nt);he=u(Fh,"LI",{});var po=f(he);Us=i(po,"Replace the line "),Ho=u(po,"CODE",{});var Rh=f(Ho);Ms=i(Rh,"loss.backward()"),Rh.forEach(t),Gs=i(po," by "),Yo=u(po,"CODE",{});var Wh=f(Yo);Hs=i(Wh,"accelerator.backward(loss)"),Wh.forEach(t),Ys=i(po,"."),po.forEach(t),Fh.forEach(t),$r=b(e),Pa=u(e,"P",{});var Vh=f(Pa);qs=i(Vh,`And you\u2019re all set! With all these changes, your script will run on your local machine as well as on multiple GPUs or a
TPU! You can either use your favorite tool to launch the distributed training, or you can use the \u{1F917} Accelerate
launcher.`),Vh.forEach(t),kr=b(e),de=u(e,"H2",{class:!0});var ci=f(de);je=u(ci,"A",{id:!0,class:!0,href:!0});var Qh=f(je);qo=u(Qh,"SPAN",{});var Kh=f(qo);S(It.$$.fragment,Kh),Kh.forEach(t),Qh.forEach(t),Bs=b(ci),Bo=u(ci,"SPAN",{});var Zh=f(Bo);xs=i(Zh,"Distributed evaluation"),Zh.forEach(t),ci.forEach(t),Pr=b(e),K=u(e,"P",{});var mo=f(K);Js=i(mo,`You can perform regular evaluation in your training script, if you leave your validation dataloader out of the
`),Aa=u(mo,"A",{href:!0});var ed=f(Aa);Xs=i(ed,"prepare()"),ed.forEach(t),Fs=i(mo,` method. In this case, you will need to put the input data on the
`),xo=u(mo,"CODE",{});var td=f(xo);Rs=i(td,"accelerator.device"),td.forEach(t),Ws=i(mo," manually."),mo.forEach(t),Ar=b(e),Le=u(e,"P",{});var ui=f(Le);Vs=i(ui,"To perform distributed evaluation, send along your validation dataloader to the "),Ta=u(ui,"A",{href:!0});var ad=f(Ta);Qs=i(ad,"prepare()"),ad.forEach(t),Ks=i(ui,`
method:`),ui.forEach(t),Tr=b(e),S(St.$$.fragment,e),Cr=b(e),Ue=u(e,"P",{});var fi=f(Ue);Zs=i(fi,`Like for your training dataloader, it will mean that (should you run your script on multiple devices) each device will
only see part of the evaluation data. This means you will need to group your predictions together. This is very easy to
do with the `),Ca=u(fi,"A",{href:!0});var od=f(Ca);ec=i(od,"gather()"),od.forEach(t),tc=i(fi," method."),fi.forEach(t),Dr=b(e),S(zt.$$.fragment,e),Nr=b(e),S(Me.$$.fragment,e),Ir=b(e),S(Ge.$$.fragment,e),Sr=b(e),pe=u(e,"H2",{class:!0});var hi=f(pe);He=u(hi,"A",{id:!0,class:!0,href:!0});var ld=f(He);Jo=u(ld,"SPAN",{});var rd=f(Jo);S(Ot.$$.fragment,rd),rd.forEach(t),ld.forEach(t),ac=b(hi),Xo=u(hi,"SPAN",{});var nd=f(Xo);oc=i(nd,"Launching your distributed script"),nd.forEach(t),hi.forEach(t),zr=b(e),X=u(e,"P",{});var yt=f(X);lc=i(yt,"You can use the regular commands to launch your distributed training (like "),Fo=u(yt,"CODE",{});var id=f(Fo);rc=i(id,"torch.distributed.launch"),id.forEach(t),nc=i(yt,` for
PyTorch), they are fully compatible with \u{1F917} Accelerate. The only caveat here is that \u{1F917} Accelerate uses the environment
to determine all useful information, so `),Ro=u(yt,"CODE",{});var sd=f(Ro);ic=i(sd,"torch.distributed.launch"),sd.forEach(t),sc=i(yt," should be used with the flag "),Wo=u(yt,"CODE",{});var cd=f(Wo);cc=i(cd,"--use_env"),cd.forEach(t),uc=i(yt,"."),yt.forEach(t),Or=b(e),Da=u(e,"P",{});var ud=f(Da);fc=i(ud,`\u{1F917} Accelerate also provides a CLI tool that unifies all launcher, so you only have to remember one command. To use it,
just run`),ud.forEach(t),jr=b(e),S(jt.$$.fragment,e),Lr=b(e),Ye=u(e,"P",{});var di=f(Ye);hc=i(di,"on your machine and reply to the questions asked. This will save a "),Vo=u(di,"EM",{});var fd=f(Vo);dc=i(fd,"default_config.yaml"),fd.forEach(t),pc=i(di,` file in your cache folder for
\u{1F917} Accelerate. That cache folder is (with decreasing order of priority):`),di.forEach(t),Ur=b(e),Z=u(e,"UL",{});var _o=f(Z);me=u(_o,"LI",{});var yo=f(me);mc=i(yo,"The content of your environment variable "),Qo=u(yo,"CODE",{});var hd=f(Qo);_c=i(hd,"HF_HOME"),hd.forEach(t),yc=i(yo," suffixed with "),Ko=u(yo,"EM",{});var dd=f(Ko);vc=i(dd,"accelerate"),dd.forEach(t),wc=i(yo,"."),yo.forEach(t),gc=b(_o),_e=u(_o,"LI",{});var vo=f(_e);bc=i(vo,"If it does not exist, the content of your environment variable "),Zo=u(vo,"CODE",{});var pd=f(Zo);Ec=i(pd,"XDG_CACHE_HOME"),pd.forEach(t),$c=i(vo,` suffixed with
`),el=u(vo,"EM",{});var md=f(el);kc=i(md,"huggingface/accelerate"),md.forEach(t),Pc=i(vo,"."),vo.forEach(t),Ac=b(_o),Na=u(_o,"LI",{});var bh=f(Na);Tc=i(bh,"If this does not exist either, the folder "),tl=u(bh,"EM",{});var _d=f(tl);Cc=i(_d,"~/.cache/huggingface/accelerate"),_d.forEach(t),bh.forEach(t),_o.forEach(t),Mr=b(e),qe=u(e,"P",{});var pi=f(qe);Dc=i(pi,"You can also specify with the flag "),al=u(pi,"CODE",{});var yd=f(al);Nc=i(yd,"--config_file"),yd.forEach(t),Ic=i(pi," the location of the file you want to save."),pi.forEach(t),Gr=b(e),Ia=u(e,"P",{});var vd=f(Ia);Sc=i(vd,"Once this is done, you can test everything is going well on your setup by running"),vd.forEach(t),Hr=b(e),S(Lt.$$.fragment,e),Yr=b(e),Sa=u(e,"P",{});var wd=f(Sa);zc=i(wd,`This will launch a short script that will test the distributed environment. If it runs fine, you are ready for the next
step!`),wd.forEach(t),qr=b(e),za=u(e,"P",{});var gd=f(za);Oc=i(gd,"Note that if you specified a location for the config file in the previous step, you need to pass it here as well:"),gd.forEach(t),Br=b(e),S(Ut.$$.fragment,e),xr=b(e),Oa=u(e,"P",{});var bd=f(Oa);jc=i(bd,"Now that this is done, you can run your script with the following command:"),bd.forEach(t),Jr=b(e),S(Mt.$$.fragment,e),Xr=b(e),ja=u(e,"P",{});var Ed=f(ja);Lc=i(Ed,"If you stored the config file in a non-default location, you can indicate it to the launcher like his:"),Ed.forEach(t),Fr=b(e),S(Gt.$$.fragment,e),Rr=b(e),La=u(e,"P",{});var $d=f(La);Uc=i($d,"You can also override any of the arguments determined by your config file, see TODO: insert ref here."),$d.forEach(t),Wr=b(e),ye=u(e,"H2",{class:!0});var mi=f(ye);Be=u(mi,"A",{id:!0,class:!0,href:!0});var kd=f(Be);ol=u(kd,"SPAN",{});var Pd=f(ol);S(Ht.$$.fragment,Pd),Pd.forEach(t),kd.forEach(t),Mc=b(mi),ll=u(mi,"SPAN",{});var Ad=f(ll);Gc=i(Ad,"Launching training from a notebook"),Ad.forEach(t),mi.forEach(t),Vr=b(e),xe=u(e,"P",{});var _i=f(xe);Hc=i(_i,"In Accelerate 0.3.0, a new "),Ua=u(_i,"A",{href:!0});var Td=f(Ua);Yc=i(Td,"notebook_launcher()"),Td.forEach(t),qc=i(_i,` has been introduced to help you launch your training
function from a notebook. This launcher supports launching a training with TPUs on Colab or Kaggle, as well as training
on several GPUs (if the machine on which you are running your notebook has them).`),_i.forEach(t),Qr=b(e),Ma=u(e,"P",{});var Cd=f(Ma);Bc=i(Cd,`Just define a function responsible for your whole training and/or evaluation in a cell of the notebook, then execute a
cell with the following code:`),Cd.forEach(t),Kr=b(e),S(Yt.$$.fragment,e),Zr=b(e),S(Je.$$.fragment,e),en=b(e),ve=u(e,"H2",{class:!0});var yi=f(ve);Xe=u(yi,"A",{id:!0,class:!0,href:!0});var Dd=f(Xe);rl=u(Dd,"SPAN",{});var Nd=f(rl);S(qt.$$.fragment,Nd),Nd.forEach(t),Dd.forEach(t),xc=b(yi),nl=u(yi,"SPAN",{});var Id=f(nl);Jc=i(Id,"Training on TPU"),Id.forEach(t),yi.forEach(t),tn=b(e),Ga=u(e,"P",{});var Sd=f(Ga);Xc=i(Sd,`If you want to launch your script on TPUs, there are a few caveats you should be aware of. Behind the scenes, the TPUs
will create a graph of all the operations happening in your training step (forward pass, backward pass and optimizer
step). This is why your first step of training will always be very long as building and compiling this graph for
optimizations takes some time.`),Sd.forEach(t),an=b(e),Ha=u(e,"P",{});var zd=f(Ha);Fc=i(zd,`The good news is that this compilation will be cached so the second step and all the following will be much faster. The
bas news is that it only applies if all of your steps do exactly the same operations, which implies:`),zd.forEach(t),on=b(e),Fe=u(e,"UL",{});var vi=f(Fe);il=u(vi,"LI",{});var Od=f(il);Rc=i(Od,"having all tensors of the same length in all your lengths"),Od.forEach(t),Wc=b(vi),sl=u(vi,"LI",{});var jd=f(sl);Vc=i(jd,"having static code (i.e., not a for loop of length that could change from step to step)"),jd.forEach(t),vi.forEach(t),ln=b(e),Ya=u(e,"P",{});var Ld=f(Ya);Qc=i(Ld,`Having any of the things above change between two steps will trigger a new compilation which will, once again, take a
lot of time. In practice, that means you must take special care to have all your tensors in your inputs of the same
shape (so no dynamic padding for instance if you are in an NLP problem) and should not use layer with for loops that
have different lengths depending on the inputs (such as an LSTM) or the training will be excruciatingly slow.`),Ld.forEach(t),rn=b(e),ee=u(e,"P",{});var wo=f(ee);Kc=i(wo,"To introduce special behavior in your script for TPUs you can check the "),cl=u(wo,"CODE",{});var Ud=f(cl);Zc=i(Ud,"distributed_type"),Ud.forEach(t),eu=i(wo,` of your
`),ul=u(wo,"CODE",{});var Md=f(ul);tu=i(Md,"accelerator"),Md.forEach(t),au=i(wo,":"),wo.forEach(t),nn=b(e),S(Bt.$$.fragment,e),sn=b(e),Re=u(e,"P",{});var wi=f(Re);ou=i(wi,"The "),xt=u(wi,"A",{href:!0,rel:!0});var Gd=f(xt);lu=i(Gd,"NLP example"),Gd.forEach(t),ru=i(wi,` shows an example in
situation with dynamic padding.`),wi.forEach(t),cn=b(e),te=u(e,"P",{});var go=f(te);nu=i(go,`One last thing to pay close attnetion to: if your model has tied weights (such as language models which tie the weights
of the embedding matrix with the weights of the decoder), moving this model to the TPU (either yourself or after you
passed your model to `),qa=u(go,"A",{href:!0});var Hd=f(qa);iu=i(Hd,"prepare()"),Hd.forEach(t),su=i(go,`) will break the tying. You will need to retie the weights
after. You can find an example of this in the `),Jt=u(go,"A",{href:!0,rel:!0});var Yd=f(Jt);cu=i(Yd,"run_clm_no_trainer"),Yd.forEach(t),uu=i(go,` script in
the Transformers repository.`),go.forEach(t),un=b(e),we=u(e,"H2",{class:!0});var gi=f(we);We=u(gi,"A",{id:!0,class:!0,href:!0});var qd=f(We);fl=u(qd,"SPAN",{});var Bd=f(fl);S(Xt.$$.fragment,Bd),Bd.forEach(t),qd.forEach(t),fu=b(gi),hl=u(gi,"SPAN",{});var xd=f(hl);hu=i(xd,"Other caveats"),xd.forEach(t),gi.forEach(t),fn=b(e),Ba=u(e,"P",{});var Jd=f(Ba);du=i(Jd,"We list here all smaller issues you could have in your script conversion and how to resolve them."),Jd.forEach(t),hn=b(e),ge=u(e,"H3",{class:!0});var bi=f(ge);Ve=u(bi,"A",{id:!0,class:!0,href:!0});var Xd=f(Ve);dl=u(Xd,"SPAN",{});var Fd=f(dl);S(Ft.$$.fragment,Fd),Fd.forEach(t),Xd.forEach(t),pu=b(bi),pl=u(bi,"SPAN",{});var Rd=f(pl);mu=i(Rd,"Execute a statement only on one processes"),Rd.forEach(t),bi.forEach(t),dn=b(e),xa=u(e,"P",{});var Wd=f(xa);_u=i(Wd,`Some of your instructions only need to run for one process on a given server: for instance a data download or a log
statement. To do this, wrap the statement in a test like this:`),Wd.forEach(t),pn=b(e),S(Rt.$$.fragment,e),mn=b(e),Ja=u(e,"P",{});var Vd=f(Ja);yu=i(Vd,`Another example is progress bars: to avoid having multiple progress bars in your output, you should only display one on
the local main process:`),Vd.forEach(t),_n=b(e),S(Wt.$$.fragment,e),yn=b(e),Qe=u(e,"P",{});var Ei=f(Qe);vu=i(Ei,"The "),ml=u(Ei,"EM",{});var Qd=f(ml);wu=i(Qd,"local"),Qd.forEach(t),gu=i(Ei,` means per machine: if you are running your training on two servers with several GPUs, the instruction will
be executed once on each of those servers. If you need to execute something only once for all processes (and not per
machine) for instance, uploading the final model to the \u{1F917} model hub, wrap it in a test like this:`),Ei.forEach(t),vn=b(e),S(Vt.$$.fragment,e),wn=b(e),ae=u(e,"P",{});var bo=f(ae);bu=i(bo,"For printing statements you only want executed once per machine, you can just replace the "),_l=u(bo,"CODE",{});var Kd=f(_l);Eu=i(Kd,"print"),Kd.forEach(t),$u=i(bo,` function by
`),yl=u(bo,"CODE",{});var Zd=f(yl);ku=i(Zd,"accelerator.print"),Zd.forEach(t),Pu=i(bo,"."),bo.forEach(t),gn=b(e),be=u(e,"H3",{class:!0});var $i=f(be);Ke=u($i,"A",{id:!0,class:!0,href:!0});var ep=f(Ke);vl=u(ep,"SPAN",{});var tp=f(vl);S(Qt.$$.fragment,tp),tp.forEach(t),ep.forEach(t),Au=b($i),wl=u($i,"SPAN",{});var ap=f(wl);Tu=i(ap,"Defer execution"),ap.forEach(t),$i.forEach(t),bn=b(e),Xa=u(e,"P",{});var op=f(Xa);Cu=i(op,`When you run your usual script, instructions are executed in order. Using \u{1F917} Accelerate to deploy your script on several
GPUs at the same time introduces a complication: while each process executes all instructions in order, some may be
faster than others.`),op.forEach(t),En=b(e),Fa=u(e,"P",{});var lp=f(Fa);Du=i(lp,`You might need to wait for all processes to have reached a certain point before executing a given instruction. For
instance, you shouldn\u2019t save a model before being sure every process is done with training. To do this, just write the
following line in your code:`),lp.forEach(t),$n=b(e),S(Kt.$$.fragment,e),kn=b(e),Ra=u(e,"P",{});var rp=f(Ra);Nu=i(rp,`This instruction will block all the processes that arrive them first until all the other processes have reached that
point (if you run your script on just one GPU or CPU, this wont\u2019 do anything).`),rp.forEach(t),Pn=b(e),Ee=u(e,"H3",{class:!0});var ki=f(Ee);Ze=u(ki,"A",{id:!0,class:!0,href:!0});var np=f(Ze);gl=u(np,"SPAN",{});var ip=f(gl);S(Zt.$$.fragment,ip),ip.forEach(t),np.forEach(t),Iu=b(ki),bl=u(ki,"SPAN",{});var sp=f(bl);Su=i(sp,"Saving/loading a model"),sp.forEach(t),ki.forEach(t),An=b(e),et=u(e,"P",{});var Pi=f(et);zu=i(Pi,`Saving the model you trained might need a bit of adjustment: first you should wait for all processes to reach that
point in the script as shown above, and then, you should unwrap your model before saving it. This is because when going
through the `),Wa=u(Pi,"A",{href:!0});var cp=f(Wa);Ou=i(cp,"prepare()"),cp.forEach(t),ju=i(Pi,` method, your model may have been placed inside a bigger model,
which deals with the distributed training. This in turn means that saving your model state dictionary without taking
any precaution will take that potential extra layer into account, and you will end up with weights you can\u2019t load back
in your base model.`),Pi.forEach(t),Tn=b(e),tt=u(e,"P",{});var Ai=f(tt);Lu=i(Ai,"This is why it\u2019s recommended to "),El=u(Ai,"EM",{});var up=f(El);Uu=i(up,"unwrap"),up.forEach(t),Mu=i(Ai," your model first. Here is an example:"),Ai.forEach(t),Cn=b(e),S(ea.$$.fragment,e),Dn=b(e),at=u(e,"P",{});var Ti=f(at);Gu=i(Ti,`If your script contains a logic to load checkpoint, we also recommend you load your weights in the unwrapped model
(this is only useful if you use the load function after making your model go through
`),Va=u(Ti,"A",{href:!0});var fp=f(Va);Hu=i(fp,"prepare()"),fp.forEach(t),Yu=i(Ti,"). Here is an example:"),Ti.forEach(t),Nn=b(e),S(ta.$$.fragment,e),In=b(e),ot=u(e,"P",{});var Ci=f(ot);qu=i(Ci,"Note that since all the model parameters are references to tensors, this will load your weights inside "),$l=u(Ci,"CODE",{});var hp=f($l);Bu=i(hp,"model"),hp.forEach(t),xu=i(Ci,"."),Ci.forEach(t),Sn=b(e),$e=u(e,"H3",{class:!0});var Di=f($e);lt=u(Di,"A",{id:!0,class:!0,href:!0});var dp=f(lt);kl=u(dp,"SPAN",{});var pp=f(kl);S(aa.$$.fragment,pp),pp.forEach(t),dp.forEach(t),Ju=b(Di),Pl=u(Di,"SPAN",{});var mp=f(Pl);Xu=i(mp,"Gradient clipping"),mp.forEach(t),Di.forEach(t),zn=b(e),q=u(e,"P",{});var ie=f(q);Fu=i(ie,`If you are using gradient clipping in your script, you should replace the calls to
`),Al=u(ie,"CODE",{});var _p=f(Al);Ru=i(_p,"torch.nn.utils.clip_grad_norm_"),_p.forEach(t),Wu=i(ie," or "),Tl=u(ie,"CODE",{});var yp=f(Tl);Vu=i(yp,"torch.nn.utils.clip_grad_value_"),yp.forEach(t),Qu=i(ie," with "),Cl=u(ie,"CODE",{});var vp=f(Cl);Ku=i(vp,"accelerator.clip_grad_norm_"),vp.forEach(t),Zu=i(ie,`
and `),Dl=u(ie,"CODE",{});var wp=f(Dl);ef=i(wp,"accelerator.clip_grad_value_"),wp.forEach(t),tf=i(ie," respectively."),ie.forEach(t),On=b(e),ke=u(e,"H3",{class:!0});var Ni=f(ke);rt=u(Ni,"A",{id:!0,class:!0,href:!0});var gp=f(rt);Nl=u(gp,"SPAN",{});var bp=f(Nl);S(oa.$$.fragment,bp),bp.forEach(t),gp.forEach(t),af=b(Ni),Il=u(Ni,"SPAN",{});var Ep=f(Il);of=i(Ep,"Mixed Precision training"),Ep.forEach(t),Ni.forEach(t),jn=b(e),nt=u(e,"P",{});var Ii=f(nt);lf=i(Ii,`If you are running your training in Mixed Precision with Accelerate, you will get the best result with your loss being
computed inside your model (like in Transformer models for instance). Every computation outside of the model will be
executed in full precision (which is generally what you want for loss computation, expecially if it involves a
softmax). However you might want to put your loss computation inside the `),Sl=u(Ii,"EM",{});var $p=f(Sl);rf=i($p,"accelerator.autocast"),$p.forEach(t),nf=i(Ii," context manager:"),Ii.forEach(t),Ln=b(e),S(la.$$.fragment,e),Un=b(e),Qa=u(e,"P",{});var kp=f(Qa);sf=i(kp,`Another caveat with Mixed Precision training is that the gradient will skip a few updates at the beginning and
sometimes during training: because of the dynamic loss scaling strategy, there are points during training where the
gradients have overflown, and the loss scaling factor is reduced to avoid this happening again at the next step.`),kp.forEach(t),Mn=b(e),Ka=u(e,"P",{});var Pp=f(Ka);cf=i(Pp,`This means that you may update your learning rate scheduler when there was no update, which is fine in general, but may
have an impact when you have very little training data, or if the first learning rate values of your scheduler are very
important. In this case, you can skip the learning rate scheduler updates when the optimizer step was not done like
this:`),Pp.forEach(t),Gn=b(e),S(ra.$$.fragment,e),Hn=b(e),Pe=u(e,"H3",{class:!0});var Si=f(Pe);it=u(Si,"A",{id:!0,class:!0,href:!0});var Ap=f(it);zl=u(Ap,"SPAN",{});var Tp=f(zl);S(na.$$.fragment,Tp),Tp.forEach(t),Ap.forEach(t),uf=b(Si),Ol=u(Si,"SPAN",{});var Cp=f(Ol);ff=i(Cp,"DeepSpeed"),Cp.forEach(t),Si.forEach(t),Yn=b(e),Za=u(e,"P",{});var Dp=f(Za);hf=i(Dp,`DeepSpeed support is experimental, so the underlying API will evolve in the near future and may have some slight
breaking changes. In particular, \u{1F917} Accelerate does not support DeepSpeed config you have written yourself yet, this
will be added in a next version.`),Dp.forEach(t),qn=b(e),oe=u(e,"P",{});var Eo=f(oe);df=i(Eo,"One main caveat for the DeepSpeed integration is that the DeepSpeed launcher always passes a "),jl=u(Eo,"CODE",{});var Np=f(jl);pf=i(Np,"local_rank"),Np.forEach(t),mf=i(Eo,` variable to
the training script, so your training script should accept it (whether you launch training with the DeepSpeed launcher
or `),Ll=u(Eo,"CODE",{});var Ip=f(Ll);_f=i(Ip,"accelerate launch"),Ip.forEach(t),yf=i(Eo,")."),Eo.forEach(t),Bn=b(e),S(st.$$.fragment,e),xn=b(e),Ae=u(e,"H2",{class:!0});var zi=f(Ae);ct=u(zi,"A",{id:!0,class:!0,href:!0});var Sp=f(ct);Ul=u(Sp,"SPAN",{});var zp=f(Ul);S(ia.$$.fragment,zp),zp.forEach(t),Sp.forEach(t),vf=b(zi),Ml=u(zi,"SPAN",{});var Op=f(Ml);wf=i(Op,"Internal mechanism"),Op.forEach(t),zi.forEach(t),Jn=b(e),ut=u(e,"P",{});var Oi=f(ut);gf=i(Oi,`Internally, the library works by first analyzing the environment in which the script is launched to determine which
kind of distributed setup is used, how many different processes there are and which one the current script is in. All
that information is stored in the `),Gl=u(Oi,"CODE",{});var jp=f(Gl);bf=i(jp,"~AcceleratorState"),jp.forEach(t),Ef=i(Oi,"."),Oi.forEach(t),Xn=b(e),le=u(e,"P",{});var $o=f(le);$f=i($o,"This class is initialized the first time you instantiate a "),eo=u($o,"A",{href:!0});var Lp=f(eo);kf=i(Lp,"Accelerator"),Lp.forEach(t),Pf=i($o,` as well as performing any
specific initialization your distributed setup needs. Its state is then uniquely shared through all instances of
`),to=u($o,"A",{href:!0});var Up=f(to);Af=i(Up,"AcceleratorState"),Up.forEach(t),Tf=i($o,"."),$o.forEach(t),Fn=b(e),ft=u(e,"P",{});var ji=f(ft);Cf=i(ji,"Then, when calling "),ao=u(ji,"A",{href:!0});var Mp=f(ao);Df=i(Mp,"prepare()"),Mp.forEach(t),Nf=i(ji,", the library:"),ji.forEach(t),Rn=b(e),re=u(e,"UL",{});var ko=f(re);Hl=u(ko,"LI",{});var Gp=f(Hl);If=i(Gp,"wraps your model(s) in the container adapted for the distributed setup,"),Gp.forEach(t),Sf=b(ko),sa=u(ko,"LI",{});var Li=f(sa);zf=i(Li,"wraps your optimizer(s) in a "),oo=u(Li,"A",{href:!0});var Hp=f(oo);Of=i(Hp,"AcceleratedOptimizer"),Hp.forEach(t),jf=i(Li,","),Li.forEach(t),Lf=b(ko),ca=u(ko,"LI",{});var Ui=f(ca);Uf=i(Ui,"creates a new version of your dataloader(s) in a "),lo=u(Ui,"A",{href:!0});var Yp=f(lo);Mf=i(Yp,"DataLoaderShard"),Yp.forEach(t),Gf=i(Ui,"."),Ui.forEach(t),ko.forEach(t),Wn=b(e),F=u(e,"P",{});var vt=f(F);Hf=i(vt,`While the model(s) and optimizer(s) are just put in simple wrappers, the dataloader(s) are re-created. This is mostly
because PyTorch does not let the user change the `),Yl=u(vt,"CODE",{});var qp=f(Yl);Yf=i(qp,"batch_sampler"),qp.forEach(t),qf=i(vt,` of a dataloader once it\u2019s been created and the
library handles the sharding of your data between processes by changing that `),ql=u(vt,"CODE",{});var Bp=f(ql);Bf=i(Bp,"batch_sampler"),Bp.forEach(t),xf=i(vt,` to yield every other
`),Bl=u(vt,"CODE",{});var xp=f(Bl);Jf=i(xp,"num_processes"),xp.forEach(t),Xf=i(vt," batches."),vt.forEach(t),Vn=b(e),ne=u(e,"P",{});var Po=f(ne);Ff=i(Po,"The "),ro=u(Po,"A",{href:!0});var Jp=f(ro);Rf=i(Jp,"DataLoaderShard"),Jp.forEach(t),Wf=i(Po," subclasses "),xl=u(Po,"CODE",{});var Xp=f(xl);Vf=i(Xp,"DataLoader"),Xp.forEach(t),Qf=i(Po," to add the following functionality:"),Po.forEach(t),Qn=b(e),ht=u(e,"UL",{});var Mi=f(ht);Jl=u(Mi,"LI",{});var Fp=f(Jl);Kf=i(Fp,`it synchronizes the appropriate random number generator of all processes at each new iteration, to ensure any
randomization (like shuffling) is done the exact same way across processes.`),Fp.forEach(t),Zf=b(Mi),ua=u(Mi,"LI",{});var Gi=f(ua);eh=i(Gi,`it puts the batches on the proper device before yielding them (unless you have opted out of
`),Xl=u(Gi,"CODE",{});var Rp=f(Xl);th=i(Rp,"device_placement=True"),Rp.forEach(t),ah=i(Gi,")."),Gi.forEach(t),Mi.forEach(t),Kn=b(e),no=u(e,"P",{});var Wp=f(no);oh=i(Wp,"The random number generator synchronization will by default synchronize:"),Wp.forEach(t),Zn=b(e),dt=u(e,"UL",{});var Hi=f(dt);Te=u(Hi,"LI",{});var Ao=f(Te);lh=i(Ao,"the "),Fl=u(Ao,"CODE",{});var Vp=f(Fl);rh=i(Vp,"generator"),Vp.forEach(t),nh=i(Ao," attribute of a given sampler (like the PyTorch "),Rl=u(Ao,"CODE",{});var Qp=f(Rl);ih=i(Qp,"RandomSampler"),Qp.forEach(t),sh=i(Ao,") for PyTorch >= 1.6"),Ao.forEach(t),ch=b(Hi),Wl=u(Hi,"LI",{});var Kp=f(Wl);uh=i(Kp,"the main random number generator in PyTorch <=1.5.1"),Kp.forEach(t),Hi.forEach(t),ei=b(e),R=u(e,"P",{});var wt=f(R);fh=i(wt,"You can choose which random number generator(s) to synchronize with the "),Vl=u(wt,"CODE",{});var Zp=f(Vl);hh=i(Zp,"rng_types"),Zp.forEach(t),dh=i(wt,` argument of the main
`),io=u(wt,"A",{href:!0});var em=f(io);ph=i(em,"Accelerator"),em.forEach(t),mh=i(wt,". In PyTorch >= 1.6, it is recommended to rely on local "),Ql=u(wt,"CODE",{});var tm=f(Ql);_h=i(tm,"generator"),tm.forEach(t),yh=i(wt,` to avoid
setting the same seed in the main random number generator in all processes.`),wt.forEach(t),ti=b(e),S(pt.$$.fragment,e),ai=b(e),S(mt.$$.fragment,e),oi=b(e),_t=u(e,"P",{});var Yi=f(_t);vh=i(Yi,"See more details about the internal in the "),so=u(Yi,"A",{href:!0});var am=f(so);wh=i(am,"Internals page"),am.forEach(t),gh=i(Yi,"."),Yi.forEach(t),this.h()},h(){w(o,"name","hf:doc:metadata"),w(o,"content",JSON.stringify(b_)),w(d,"id","quick-tour"),w(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(d,"href","#quick-tour"),w(_,"class","relative group"),w(L,"id","main-use"),w(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(L,"href","#main-use"),w(T,"class","relative group"),w(ma,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator"),w(At,"start","2"),w(ya,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator"),w(va,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(Tt,"start","3"),w(wa,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator"),w(ba,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(Ea,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w($a,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(ka,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(Nt,"start","4"),w(je,"id","distributed-evaluation"),w(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(je,"href","#distributed-evaluation"),w(de,"class","relative group"),w(Aa,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(Ta,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(Ca,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.gather"),w(He,"id","launching-your-distributed-script"),w(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(He,"href","#launching-your-distributed-script"),w(pe,"class","relative group"),w(Be,"id","launching-training-from-a-notebook"),w(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Be,"href","#launching-training-from-a-notebook"),w(ye,"class","relative group"),w(Ua,"href","/docs/accelerate/main/en/launcher#accelerate.notebook_launcher"),w(Xe,"id","training-on-tpu"),w(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Xe,"href","#training-on-tpu"),w(ve,"class","relative group"),w(xt,"href","https://github.com/huggingface/accelerate/blob/main/examples/nlp_example.py"),w(xt,"rel","nofollow"),w(qa,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(Jt,"href","https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm.py"),w(Jt,"rel","nofollow"),w(We,"id","other-caveats"),w(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(We,"href","#other-caveats"),w(we,"class","relative group"),w(Ve,"id","execute-a-statement-only-on-one-processes"),w(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Ve,"href","#execute-a-statement-only-on-one-processes"),w(ge,"class","relative group"),w(Ke,"id","defer-execution"),w(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Ke,"href","#defer-execution"),w(be,"class","relative group"),w(Ze,"id","savingloading-a-model"),w(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(Ze,"href","#savingloading-a-model"),w(Ee,"class","relative group"),w(Wa,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(Va,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(lt,"id","gradient-clipping"),w(lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(lt,"href","#gradient-clipping"),w($e,"class","relative group"),w(rt,"id","mixed-precision-training"),w(rt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(rt,"href","#mixed-precision-training"),w(ke,"class","relative group"),w(it,"id","deepspeed"),w(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(it,"href","#deepspeed"),w(Pe,"class","relative group"),w(ct,"id","internal-mechanism"),w(ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),w(ct,"href","#internal-mechanism"),w(Ae,"class","relative group"),w(eo,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator"),w(to,"href","/docs/accelerate/main/en/internal#accelerate.state.AcceleratorState"),w(ao,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator.prepare"),w(oo,"href","/docs/accelerate/main/en/internal#accelerate.optimizer.AcceleratedOptimizer"),w(lo,"href","/docs/accelerate/main/en/internal#accelerate.data_loader.DataLoaderShard"),w(ro,"href","/docs/accelerate/main/en/internal#accelerate.data_loader.DataLoaderShard"),w(io,"href","/docs/accelerate/main/en/accelerator#accelerate.Accelerator"),w(so,"href","internal")},m(e,s){a(document.head,o),p(e,h,s),N(r,e,s),p(e,l,s),p(e,_,s),a(_,d),a(d,v),N(y,v,null),a(_,E),a(_,k),a(k,U),p(e,j,s),p(e,z,s),a(z,C),p(e,A,s),p(e,T,s),a(T,L),a(L,G),N(O,G,null),a(T,J),a(T,kt),a(kt,M),p(e,rr,s),p(e,da,s),a(da,Bi),p(e,nr,s),p(e,pa,s),a(pa,fe),a(fe,xi),a(fe,ma),a(ma,Ji),a(fe,Xi),a(fe,No),a(No,Fi),a(fe,Ri),p(e,ir,s),N(Pt,e,s),p(e,sr,s),p(e,_a,s),a(_a,Wi),p(e,cr,s),p(e,At,s),a(At,Y),a(Y,Vi),a(Y,Io),a(Io,Qi),a(Y,Ki),a(Y,So),a(So,Zi),a(Y,es),a(Y,zo),a(zo,ts),a(Y,as),a(Y,Oo),a(Oo,os),a(Y,ls),a(Y,jo),a(jo,rs),a(Y,ns),a(Y,Lo),a(Lo,is),a(Y,ss),p(e,ur,s),p(e,W,s),a(W,cs),a(W,Uo),a(Uo,us),a(W,fs),a(W,ya),a(ya,hs),a(W,ds),p(e,fr,s),N(De,e,s),p(e,hr,s),p(e,Tt,s),a(Tt,Ct),a(Ct,ps),a(Ct,va),a(va,ms),a(Ct,_s),p(e,dr,s),N(Dt,e,s),p(e,pr,s),p(e,Ne,s),a(Ne,ys),a(Ne,Mo),a(Mo,vs),a(Ne,ws),p(e,mr,s),N(Ie,e,s),p(e,_r,s),p(e,V,s),a(V,gs),a(V,Go),a(Go,bs),a(V,Es),a(V,wa),a(wa,$s),a(V,ks),p(e,yr,s),p(e,ga,s),a(ga,Ps),p(e,vr,s),N(Se,e,s),p(e,wr,s),p(e,ze,s),a(ze,As),a(ze,ba),a(ba,Ts),a(ze,Cs),p(e,gr,s),p(e,Q,s),a(Q,Ds),a(Q,Ea),a(Ea,Ns),a(Q,Is),a(Q,$a),a($a,Ss),a(Q,zs),p(e,br,s),p(e,Oe,s),a(Oe,Os),a(Oe,ka),a(ka,js),a(Oe,Ls),p(e,Er,s),p(e,Nt,s),a(Nt,he),a(he,Us),a(he,Ho),a(Ho,Ms),a(he,Gs),a(he,Yo),a(Yo,Hs),a(he,Ys),p(e,$r,s),p(e,Pa,s),a(Pa,qs),p(e,kr,s),p(e,de,s),a(de,je),a(je,qo),N(It,qo,null),a(de,Bs),a(de,Bo),a(Bo,xs),p(e,Pr,s),p(e,K,s),a(K,Js),a(K,Aa),a(Aa,Xs),a(K,Fs),a(K,xo),a(xo,Rs),a(K,Ws),p(e,Ar,s),p(e,Le,s),a(Le,Vs),a(Le,Ta),a(Ta,Qs),a(Le,Ks),p(e,Tr,s),N(St,e,s),p(e,Cr,s),p(e,Ue,s),a(Ue,Zs),a(Ue,Ca),a(Ca,ec),a(Ue,tc),p(e,Dr,s),N(zt,e,s),p(e,Nr,s),N(Me,e,s),p(e,Ir,s),N(Ge,e,s),p(e,Sr,s),p(e,pe,s),a(pe,He),a(He,Jo),N(Ot,Jo,null),a(pe,ac),a(pe,Xo),a(Xo,oc),p(e,zr,s),p(e,X,s),a(X,lc),a(X,Fo),a(Fo,rc),a(X,nc),a(X,Ro),a(Ro,ic),a(X,sc),a(X,Wo),a(Wo,cc),a(X,uc),p(e,Or,s),p(e,Da,s),a(Da,fc),p(e,jr,s),N(jt,e,s),p(e,Lr,s),p(e,Ye,s),a(Ye,hc),a(Ye,Vo),a(Vo,dc),a(Ye,pc),p(e,Ur,s),p(e,Z,s),a(Z,me),a(me,mc),a(me,Qo),a(Qo,_c),a(me,yc),a(me,Ko),a(Ko,vc),a(me,wc),a(Z,gc),a(Z,_e),a(_e,bc),a(_e,Zo),a(Zo,Ec),a(_e,$c),a(_e,el),a(el,kc),a(_e,Pc),a(Z,Ac),a(Z,Na),a(Na,Tc),a(Na,tl),a(tl,Cc),p(e,Mr,s),p(e,qe,s),a(qe,Dc),a(qe,al),a(al,Nc),a(qe,Ic),p(e,Gr,s),p(e,Ia,s),a(Ia,Sc),p(e,Hr,s),N(Lt,e,s),p(e,Yr,s),p(e,Sa,s),a(Sa,zc),p(e,qr,s),p(e,za,s),a(za,Oc),p(e,Br,s),N(Ut,e,s),p(e,xr,s),p(e,Oa,s),a(Oa,jc),p(e,Jr,s),N(Mt,e,s),p(e,Xr,s),p(e,ja,s),a(ja,Lc),p(e,Fr,s),N(Gt,e,s),p(e,Rr,s),p(e,La,s),a(La,Uc),p(e,Wr,s),p(e,ye,s),a(ye,Be),a(Be,ol),N(Ht,ol,null),a(ye,Mc),a(ye,ll),a(ll,Gc),p(e,Vr,s),p(e,xe,s),a(xe,Hc),a(xe,Ua),a(Ua,Yc),a(xe,qc),p(e,Qr,s),p(e,Ma,s),a(Ma,Bc),p(e,Kr,s),N(Yt,e,s),p(e,Zr,s),N(Je,e,s),p(e,en,s),p(e,ve,s),a(ve,Xe),a(Xe,rl),N(qt,rl,null),a(ve,xc),a(ve,nl),a(nl,Jc),p(e,tn,s),p(e,Ga,s),a(Ga,Xc),p(e,an,s),p(e,Ha,s),a(Ha,Fc),p(e,on,s),p(e,Fe,s),a(Fe,il),a(il,Rc),a(Fe,Wc),a(Fe,sl),a(sl,Vc),p(e,ln,s),p(e,Ya,s),a(Ya,Qc),p(e,rn,s),p(e,ee,s),a(ee,Kc),a(ee,cl),a(cl,Zc),a(ee,eu),a(ee,ul),a(ul,tu),a(ee,au),p(e,nn,s),N(Bt,e,s),p(e,sn,s),p(e,Re,s),a(Re,ou),a(Re,xt),a(xt,lu),a(Re,ru),p(e,cn,s),p(e,te,s),a(te,nu),a(te,qa),a(qa,iu),a(te,su),a(te,Jt),a(Jt,cu),a(te,uu),p(e,un,s),p(e,we,s),a(we,We),a(We,fl),N(Xt,fl,null),a(we,fu),a(we,hl),a(hl,hu),p(e,fn,s),p(e,Ba,s),a(Ba,du),p(e,hn,s),p(e,ge,s),a(ge,Ve),a(Ve,dl),N(Ft,dl,null),a(ge,pu),a(ge,pl),a(pl,mu),p(e,dn,s),p(e,xa,s),a(xa,_u),p(e,pn,s),N(Rt,e,s),p(e,mn,s),p(e,Ja,s),a(Ja,yu),p(e,_n,s),N(Wt,e,s),p(e,yn,s),p(e,Qe,s),a(Qe,vu),a(Qe,ml),a(ml,wu),a(Qe,gu),p(e,vn,s),N(Vt,e,s),p(e,wn,s),p(e,ae,s),a(ae,bu),a(ae,_l),a(_l,Eu),a(ae,$u),a(ae,yl),a(yl,ku),a(ae,Pu),p(e,gn,s),p(e,be,s),a(be,Ke),a(Ke,vl),N(Qt,vl,null),a(be,Au),a(be,wl),a(wl,Tu),p(e,bn,s),p(e,Xa,s),a(Xa,Cu),p(e,En,s),p(e,Fa,s),a(Fa,Du),p(e,$n,s),N(Kt,e,s),p(e,kn,s),p(e,Ra,s),a(Ra,Nu),p(e,Pn,s),p(e,Ee,s),a(Ee,Ze),a(Ze,gl),N(Zt,gl,null),a(Ee,Iu),a(Ee,bl),a(bl,Su),p(e,An,s),p(e,et,s),a(et,zu),a(et,Wa),a(Wa,Ou),a(et,ju),p(e,Tn,s),p(e,tt,s),a(tt,Lu),a(tt,El),a(El,Uu),a(tt,Mu),p(e,Cn,s),N(ea,e,s),p(e,Dn,s),p(e,at,s),a(at,Gu),a(at,Va),a(Va,Hu),a(at,Yu),p(e,Nn,s),N(ta,e,s),p(e,In,s),p(e,ot,s),a(ot,qu),a(ot,$l),a($l,Bu),a(ot,xu),p(e,Sn,s),p(e,$e,s),a($e,lt),a(lt,kl),N(aa,kl,null),a($e,Ju),a($e,Pl),a(Pl,Xu),p(e,zn,s),p(e,q,s),a(q,Fu),a(q,Al),a(Al,Ru),a(q,Wu),a(q,Tl),a(Tl,Vu),a(q,Qu),a(q,Cl),a(Cl,Ku),a(q,Zu),a(q,Dl),a(Dl,ef),a(q,tf),p(e,On,s),p(e,ke,s),a(ke,rt),a(rt,Nl),N(oa,Nl,null),a(ke,af),a(ke,Il),a(Il,of),p(e,jn,s),p(e,nt,s),a(nt,lf),a(nt,Sl),a(Sl,rf),a(nt,nf),p(e,Ln,s),N(la,e,s),p(e,Un,s),p(e,Qa,s),a(Qa,sf),p(e,Mn,s),p(e,Ka,s),a(Ka,cf),p(e,Gn,s),N(ra,e,s),p(e,Hn,s),p(e,Pe,s),a(Pe,it),a(it,zl),N(na,zl,null),a(Pe,uf),a(Pe,Ol),a(Ol,ff),p(e,Yn,s),p(e,Za,s),a(Za,hf),p(e,qn,s),p(e,oe,s),a(oe,df),a(oe,jl),a(jl,pf),a(oe,mf),a(oe,Ll),a(Ll,_f),a(oe,yf),p(e,Bn,s),N(st,e,s),p(e,xn,s),p(e,Ae,s),a(Ae,ct),a(ct,Ul),N(ia,Ul,null),a(Ae,vf),a(Ae,Ml),a(Ml,wf),p(e,Jn,s),p(e,ut,s),a(ut,gf),a(ut,Gl),a(Gl,bf),a(ut,Ef),p(e,Xn,s),p(e,le,s),a(le,$f),a(le,eo),a(eo,kf),a(le,Pf),a(le,to),a(to,Af),a(le,Tf),p(e,Fn,s),p(e,ft,s),a(ft,Cf),a(ft,ao),a(ao,Df),a(ft,Nf),p(e,Rn,s),p(e,re,s),a(re,Hl),a(Hl,If),a(re,Sf),a(re,sa),a(sa,zf),a(sa,oo),a(oo,Of),a(sa,jf),a(re,Lf),a(re,ca),a(ca,Uf),a(ca,lo),a(lo,Mf),a(ca,Gf),p(e,Wn,s),p(e,F,s),a(F,Hf),a(F,Yl),a(Yl,Yf),a(F,qf),a(F,ql),a(ql,Bf),a(F,xf),a(F,Bl),a(Bl,Jf),a(F,Xf),p(e,Vn,s),p(e,ne,s),a(ne,Ff),a(ne,ro),a(ro,Rf),a(ne,Wf),a(ne,xl),a(xl,Vf),a(ne,Qf),p(e,Qn,s),p(e,ht,s),a(ht,Jl),a(Jl,Kf),a(ht,Zf),a(ht,ua),a(ua,eh),a(ua,Xl),a(Xl,th),a(ua,ah),p(e,Kn,s),p(e,no,s),a(no,oh),p(e,Zn,s),p(e,dt,s),a(dt,Te),a(Te,lh),a(Te,Fl),a(Fl,rh),a(Te,nh),a(Te,Rl),a(Rl,ih),a(Te,sh),a(dt,ch),a(dt,Wl),a(Wl,uh),p(e,ei,s),p(e,R,s),a(R,fh),a(R,Vl),a(Vl,hh),a(R,dh),a(R,io),a(io,ph),a(R,mh),a(R,Ql),a(Ql,_h),a(R,yh),p(e,ti,s),N(pt,e,s),p(e,ai,s),N(mt,e,s),p(e,oi,s),p(e,_t,s),a(_t,vh),a(_t,so),a(so,wh),a(_t,gh),li=!0},p(e,[s]){const fa={};s&2&&(fa.$$scope={dirty:s,ctx:e}),De.$set(fa);const Kl={};s&2&&(Kl.$$scope={dirty:s,ctx:e}),Ie.$set(Kl);const Zl={};s&2&&(Zl.$$scope={dirty:s,ctx:e}),Se.$set(Zl);const er={};s&2&&(er.$$scope={dirty:s,ctx:e}),Me.$set(er);const tr={};s&2&&(tr.$$scope={dirty:s,ctx:e}),Ge.$set(tr);const ha={};s&2&&(ha.$$scope={dirty:s,ctx:e}),Je.$set(ha);const ar={};s&2&&(ar.$$scope={dirty:s,ctx:e}),st.$set(ar);const or={};s&2&&(or.$$scope={dirty:s,ctx:e}),pt.$set(or);const lr={};s&2&&(lr.$$scope={dirty:s,ctx:e}),mt.$set(lr)},i(e){li||($(r.$$.fragment,e),$(y.$$.fragment,e),$(O.$$.fragment,e),$(Pt.$$.fragment,e),$(De.$$.fragment,e),$(Dt.$$.fragment,e),$(Ie.$$.fragment,e),$(Se.$$.fragment,e),$(It.$$.fragment,e),$(St.$$.fragment,e),$(zt.$$.fragment,e),$(Me.$$.fragment,e),$(Ge.$$.fragment,e),$(Ot.$$.fragment,e),$(jt.$$.fragment,e),$(Lt.$$.fragment,e),$(Ut.$$.fragment,e),$(Mt.$$.fragment,e),$(Gt.$$.fragment,e),$(Ht.$$.fragment,e),$(Yt.$$.fragment,e),$(Je.$$.fragment,e),$(qt.$$.fragment,e),$(Bt.$$.fragment,e),$(Xt.$$.fragment,e),$(Ft.$$.fragment,e),$(Rt.$$.fragment,e),$(Wt.$$.fragment,e),$(Vt.$$.fragment,e),$(Qt.$$.fragment,e),$(Kt.$$.fragment,e),$(Zt.$$.fragment,e),$(ea.$$.fragment,e),$(ta.$$.fragment,e),$(aa.$$.fragment,e),$(oa.$$.fragment,e),$(la.$$.fragment,e),$(ra.$$.fragment,e),$(na.$$.fragment,e),$(st.$$.fragment,e),$(ia.$$.fragment,e),$(pt.$$.fragment,e),$(mt.$$.fragment,e),li=!0)},o(e){P(r.$$.fragment,e),P(y.$$.fragment,e),P(O.$$.fragment,e),P(Pt.$$.fragment,e),P(De.$$.fragment,e),P(Dt.$$.fragment,e),P(Ie.$$.fragment,e),P(Se.$$.fragment,e),P(It.$$.fragment,e),P(St.$$.fragment,e),P(zt.$$.fragment,e),P(Me.$$.fragment,e),P(Ge.$$.fragment,e),P(Ot.$$.fragment,e),P(jt.$$.fragment,e),P(Lt.$$.fragment,e),P(Ut.$$.fragment,e),P(Mt.$$.fragment,e),P(Gt.$$.fragment,e),P(Ht.$$.fragment,e),P(Yt.$$.fragment,e),P(Je.$$.fragment,e),P(qt.$$.fragment,e),P(Bt.$$.fragment,e),P(Xt.$$.fragment,e),P(Ft.$$.fragment,e),P(Rt.$$.fragment,e),P(Wt.$$.fragment,e),P(Vt.$$.fragment,e),P(Qt.$$.fragment,e),P(Kt.$$.fragment,e),P(Zt.$$.fragment,e),P(ea.$$.fragment,e),P(ta.$$.fragment,e),P(aa.$$.fragment,e),P(oa.$$.fragment,e),P(la.$$.fragment,e),P(ra.$$.fragment,e),P(na.$$.fragment,e),P(st.$$.fragment,e),P(ia.$$.fragment,e),P(pt.$$.fragment,e),P(mt.$$.fragment,e),li=!1},d(e){t(o),e&&t(h),I(r,e),e&&t(l),e&&t(_),I(y),e&&t(j),e&&t(z),e&&t(A),e&&t(T),I(O),e&&t(rr),e&&t(da),e&&t(nr),e&&t(pa),e&&t(ir),I(Pt,e),e&&t(sr),e&&t(_a),e&&t(cr),e&&t(At),e&&t(ur),e&&t(W),e&&t(fr),I(De,e),e&&t(hr),e&&t(Tt),e&&t(dr),I(Dt,e),e&&t(pr),e&&t(Ne),e&&t(mr),I(Ie,e),e&&t(_r),e&&t(V),e&&t(yr),e&&t(ga),e&&t(vr),I(Se,e),e&&t(wr),e&&t(ze),e&&t(gr),e&&t(Q),e&&t(br),e&&t(Oe),e&&t(Er),e&&t(Nt),e&&t($r),e&&t(Pa),e&&t(kr),e&&t(de),I(It),e&&t(Pr),e&&t(K),e&&t(Ar),e&&t(Le),e&&t(Tr),I(St,e),e&&t(Cr),e&&t(Ue),e&&t(Dr),I(zt,e),e&&t(Nr),I(Me,e),e&&t(Ir),I(Ge,e),e&&t(Sr),e&&t(pe),I(Ot),e&&t(zr),e&&t(X),e&&t(Or),e&&t(Da),e&&t(jr),I(jt,e),e&&t(Lr),e&&t(Ye),e&&t(Ur),e&&t(Z),e&&t(Mr),e&&t(qe),e&&t(Gr),e&&t(Ia),e&&t(Hr),I(Lt,e),e&&t(Yr),e&&t(Sa),e&&t(qr),e&&t(za),e&&t(Br),I(Ut,e),e&&t(xr),e&&t(Oa),e&&t(Jr),I(Mt,e),e&&t(Xr),e&&t(ja),e&&t(Fr),I(Gt,e),e&&t(Rr),e&&t(La),e&&t(Wr),e&&t(ye),I(Ht),e&&t(Vr),e&&t(xe),e&&t(Qr),e&&t(Ma),e&&t(Kr),I(Yt,e),e&&t(Zr),I(Je,e),e&&t(en),e&&t(ve),I(qt),e&&t(tn),e&&t(Ga),e&&t(an),e&&t(Ha),e&&t(on),e&&t(Fe),e&&t(ln),e&&t(Ya),e&&t(rn),e&&t(ee),e&&t(nn),I(Bt,e),e&&t(sn),e&&t(Re),e&&t(cn),e&&t(te),e&&t(un),e&&t(we),I(Xt),e&&t(fn),e&&t(Ba),e&&t(hn),e&&t(ge),I(Ft),e&&t(dn),e&&t(xa),e&&t(pn),I(Rt,e),e&&t(mn),e&&t(Ja),e&&t(_n),I(Wt,e),e&&t(yn),e&&t(Qe),e&&t(vn),I(Vt,e),e&&t(wn),e&&t(ae),e&&t(gn),e&&t(be),I(Qt),e&&t(bn),e&&t(Xa),e&&t(En),e&&t(Fa),e&&t($n),I(Kt,e),e&&t(kn),e&&t(Ra),e&&t(Pn),e&&t(Ee),I(Zt),e&&t(An),e&&t(et),e&&t(Tn),e&&t(tt),e&&t(Cn),I(ea,e),e&&t(Dn),e&&t(at),e&&t(Nn),I(ta,e),e&&t(In),e&&t(ot),e&&t(Sn),e&&t($e),I(aa),e&&t(zn),e&&t(q),e&&t(On),e&&t(ke),I(oa),e&&t(jn),e&&t(nt),e&&t(Ln),I(la,e),e&&t(Un),e&&t(Qa),e&&t(Mn),e&&t(Ka),e&&t(Gn),I(ra,e),e&&t(Hn),e&&t(Pe),I(na),e&&t(Yn),e&&t(Za),e&&t(qn),e&&t(oe),e&&t(Bn),I(st,e),e&&t(xn),e&&t(Ae),I(ia),e&&t(Jn),e&&t(ut),e&&t(Xn),e&&t(le),e&&t(Fn),e&&t(ft),e&&t(Rn),e&&t(re),e&&t(Wn),e&&t(F),e&&t(Vn),e&&t(ne),e&&t(Qn),e&&t(ht),e&&t(Kn),e&&t(no),e&&t(Zn),e&&t(dt),e&&t(ei),e&&t(R),e&&t(ti),I(pt,e),e&&t(ai),I(mt,e),e&&t(oi),e&&t(_t)}}}const b_={local:"quick-tour",sections:[{local:"main-use",title:"Main use"},{local:"distributed-evaluation",title:"Distributed evaluation"},{local:"launching-your-distributed-script",title:"Launching your distributed script"},{local:"launching-training-from-a-notebook",title:"Launching training from a notebook"},{local:"training-on-tpu",title:"Training on TPU"},{local:"other-caveats",sections:[{local:"execute-a-statement-only-on-one-processes",title:"Execute a statement only on one processes"},{local:"defer-execution",title:"Defer execution"},{local:"savingloading-a-model",title:"Saving/loading a model"},{local:"gradient-clipping",title:"Gradient clipping"},{local:"mixed-precision-training",title:"Mixed Precision training"},{local:"deepspeed",title:"DeepSpeed"}],title:"Other caveats"},{local:"internal-mechanism",title:"Internal mechanism"}],title:"Quick tour"};function E_(m,o,h){let{fw:r}=o;return m.$$set=l=>{"fw"in l&&h(0,r=l.fw)},[r]}class T_ extends To{constructor(o){super();Co(this,o,E_,g_,Do,{fw:0})}}export{T_ as default,b_ as metadata};
