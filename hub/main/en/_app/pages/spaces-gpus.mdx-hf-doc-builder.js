import{S as $s,i as Ps,s as bs,e as o,k as p,w as g,t as n,N as xs,c as l,d as t,m as u,a as r,x as y,h as c,b as f,P as st,G as s,g as i,y as w,q as _,o as $,B as P,v as ks}from"../chunks/vendor-hf-doc-builder.js";import{T as Es}from"../chunks/Tip-hf-doc-builder.js";import{I as at}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as oe}from"../chunks/CodeBlock-hf-doc-builder.js";function js(xe){let h;return{c(){h=n("Longer-term, we would also like to expose non-GPU hardware, like HPU, IPU or TPU. If you have a specific AI hardware you'd like to run on, please let us know (website at huggingface.co).")},l(b){h=c(b,"Longer-term, we would also like to expose non-GPU hardware, like HPU, IPU or TPU. If you have a specific AI hardware you'd like to run on, please let us know (website at huggingface.co).")},m(b,v){i(b,h,v)},d(b){b&&t(h)}}}function Us(xe){let h,b,v,S,le,M,ot,re,lt,ke,T,rt,ie,it,nt,Ee,k,Q,zt,ct,W,Bt,je,D,Ue,Z,pt,Ae,E,ee,Vt,ut,te,Rt,Se,se,ft,Te,j,G,ne,O,ht,ce,dt,De,C,mt,pe,vt,gt,Ge,Y,Ce,q,yt,ue,wt,_t,qe,H,Ie,m,$t,fe,Pt,bt,he,xt,kt,de,Et,jt,Je,L,Ne,U,I,me,z,Ut,ve,At,Xe,J,St,ge,Tt,Dt,Me,B,Oe,ae,Gt,Ye,V,He,A,N,ye,R,Ct,we,qt,Le,d,It,_e,Jt,Nt,$e,Xt,Mt,Pe,Ot,Yt,be,Ht,Lt,ze,F,Be;return M=new at({}),D=new Es({props:{$$slots:{default:[js]},$$scope:{ctx:xe}}}),O=new at({}),Y=new oe({props:{code:`--extra-index-url https://download.pytorch.org/whl/cu113
torch`,highlighted:`--extra-index-url https:<span class="hljs-regexp">//</span>download.pytorch.org<span class="hljs-regexp">/whl/</span>cu113
torch`}}),H=new oe({props:{code:`import torch
print(f"Is CUDA available: {torch.cuda.is_available()}")
# True
print(f"CUDA device: {torch.cuda.get_device_name(torch.cuda.current_device())}")
# Tesla T4`,highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Is CUDA available: <span class="hljs-subst">{torch.cuda.is_available()}</span>&quot;</span>)
<span class="hljs-comment"># True</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;CUDA device: <span class="hljs-subst">{torch.cuda.get_device_name(torch.cuda.current_device())}</span>&quot;</span>)
<span class="hljs-comment"># Tesla T4</span>`}}),L=new oe({props:{code:`model = load_pytorch_model()
model = model.to("cuda")`,highlighted:`model = load_pytorch_model()
model = model.to(<span class="hljs-string">&quot;cuda&quot;</span>)`}}),z=new at({}),B=new oe({props:{code:`-f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
jax[cuda11_cudnn805]`,highlighted:`-f https:<span class="hljs-regexp">//</span>storage.googleapis.com<span class="hljs-regexp">/jax-releases/</span>jax_cuda_releases.html
jax[cuda11_cudnn805]`}}),V=new oe({props:{code:`import jax

print(f"JAX devices: {jax.devices()}")
# JAX devices: [StreamExecutorGpuDevice(id=0, process_index=0)]
print(f"JAX device type: {jax.devices()[0].device_kind}")
# JAX device type: Tesla T4`,highlighted:`<span class="hljs-keyword">import</span> jax

<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;JAX devices: <span class="hljs-subst">{jax.devices()}</span>&quot;</span>)
<span class="hljs-comment"># JAX devices: [StreamExecutorGpuDevice(id=0, process_index=0)]</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;JAX device type: <span class="hljs-subst">{jax.devices()[<span class="hljs-number">0</span>].device_kind}</span>&quot;</span>)
<span class="hljs-comment"># JAX device type: Tesla T4</span>`}}),R=new at({}),F=new oe({props:{code:`import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))
# [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-built_in">print</span>(tf.config.list_physical_devices(<span class="hljs-string">&#x27;GPU&#x27;</span>))
<span class="hljs-comment"># [PhysicalDevice(name=&#x27;/physical_device:GPU:0&#x27;, device_type=&#x27;GPU&#x27;)]</span>`}}),{c(){h=o("meta"),b=p(),v=o("h1"),S=o("a"),le=o("span"),g(M.$$.fragment),ot=p(),re=o("span"),lt=n("Using GPU Spaces"),ke=p(),T=o("p"),rt=n("You can upgrade your Space to use a GPU accelerator using the "),ie=o("em"),it=n("Settings"),nt=n(" button in the top navigation bar of the Space. You can even request a free upgrade if you are building a cool demo for a side project!"),Ee=p(),k=o("div"),Q=o("img"),ct=p(),W=o("img"),je=p(),g(D.$$.fragment),Ue=p(),Z=o("p"),pt=n("As soon as your Space is running on GPU you can see which hardware it\u2019s running on directly from this badge:"),Ae=p(),E=o("div"),ee=o("img"),ut=p(),te=o("img"),Se=p(),se=o("p"),ft=n("Most Spaces should run out of the box after a GPU upgrade, but sometimes you\u2019ll need to install CUDA versions of the machine learning frameworks you use. Please, follow this guide to ensure your Space takes advantage of the improved hardware."),Te=p(),j=o("h2"),G=o("a"),ne=o("span"),g(O.$$.fragment),ht=p(),ce=o("span"),dt=n("PyTorch installation"),De=p(),C=o("p"),mt=n("You\u2019ll need to install a version of PyTorch compatible with the built-in CUDA drivers. Adding the following two lines to your "),pe=o("code"),vt=n("requirements.txt"),gt=n(" file should work:"),Ge=p(),g(Y.$$.fragment),Ce=p(),q=o("p"),yt=n("You can verify whether the installation was successful by running the following code in your "),ue=o("code"),wt=n("app.py"),_t=n(" and checking the output in your Space logs:"),qe=p(),g(H.$$.fragment),Ie=p(),m=o("p"),$t=n("Many frameworks automatically use the GPU if one is available. This is the case for the Pipelines in \u{1F917} "),fe=o("code"),Pt=n("transformers"),bt=n(", "),he=o("code"),xt=n("fastai"),kt=n(" and many others. In other cases, or if you use PyTorch directly, you may need to move your models and data to the GPU to ensure computation is done on the accelerator and not on the CPU. You can use PyTorch\u2019s "),de=o("code"),Et=n(".to()"),jt=n(" syntax, for example:"),Je=p(),g(L.$$.fragment),Ne=p(),U=o("h2"),I=o("a"),me=o("span"),g(z.$$.fragment),Ut=p(),ve=o("span"),At=n("JAX"),Xe=p(),J=o("p"),St=n("If you use JAX, you need to specify the binary that is compatible with the CUDA and cuDNN versions installed in your GPU Space. Please, add the following two lines to your "),ge=o("code"),Tt=n("requirements.txt"),Dt=n(" file:"),Me=p(),g(B.$$.fragment),Oe=p(),ae=o("p"),Gt=n("After that, you can verify the installation by printing the output from the following code and checking it in your Space logs."),Ye=p(),g(V.$$.fragment),He=p(),A=o("h2"),N=o("a"),ye=o("span"),g(R.$$.fragment),Ct=p(),we=o("span"),qt=n("Tensorflow"),Le=p(),d=o("p"),It=n("The default "),_e=o("code"),Jt=n("tensorflow"),Nt=n(" installation should recognize the CUDA device. Just add "),$e=o("code"),Xt=n("tensorflow"),Mt=n(" to your "),Pe=o("code"),Ot=n("requirements.txt"),Yt=n(" file and use the following code in your "),be=o("code"),Ht=n("app.py"),Lt=n(" to verify in your Space logs."),ze=p(),g(F.$$.fragment),this.h()},l(e){const a=xs('[data-svelte="svelte-1phssyn"]',document.head);h=l(a,"META",{name:!0,content:!0}),a.forEach(t),b=u(e),v=l(e,"H1",{class:!0});var K=r(v);S=l(K,"A",{id:!0,class:!0,href:!0});var Ft=r(S);le=l(Ft,"SPAN",{});var Kt=r(le);y(M.$$.fragment,Kt),Kt.forEach(t),Ft.forEach(t),ot=u(K),re=l(K,"SPAN",{});var Qt=r(re);lt=c(Qt,"Using GPU Spaces"),Qt.forEach(t),K.forEach(t),ke=u(e),T=l(e,"P",{});var Ve=r(T);rt=c(Ve,"You can upgrade your Space to use a GPU accelerator using the "),ie=l(Ve,"EM",{});var Wt=r(ie);it=c(Wt,"Settings"),Wt.forEach(t),nt=c(Ve," button in the top navigation bar of the Space. You can even request a free upgrade if you are building a cool demo for a side project!"),Ve.forEach(t),Ee=u(e),k=l(e,"DIV",{class:!0});var Re=r(k);Q=l(Re,"IMG",{class:!0,src:!0}),ct=u(Re),W=l(Re,"IMG",{class:!0,src:!0}),Re.forEach(t),je=u(e),y(D.$$.fragment,e),Ue=u(e),Z=l(e,"P",{});var Zt=r(Z);pt=c(Zt,"As soon as your Space is running on GPU you can see which hardware it\u2019s running on directly from this badge:"),Zt.forEach(t),Ae=u(e),E=l(e,"DIV",{class:!0});var Fe=r(E);ee=l(Fe,"IMG",{class:!0,src:!0}),ut=u(Fe),te=l(Fe,"IMG",{class:!0,src:!0}),Fe.forEach(t),Se=u(e),se=l(e,"P",{});var es=r(se);ft=c(es,"Most Spaces should run out of the box after a GPU upgrade, but sometimes you\u2019ll need to install CUDA versions of the machine learning frameworks you use. Please, follow this guide to ensure your Space takes advantage of the improved hardware."),es.forEach(t),Te=u(e),j=l(e,"H2",{class:!0});var Ke=r(j);G=l(Ke,"A",{id:!0,class:!0,href:!0});var ts=r(G);ne=l(ts,"SPAN",{});var ss=r(ne);y(O.$$.fragment,ss),ss.forEach(t),ts.forEach(t),ht=u(Ke),ce=l(Ke,"SPAN",{});var as=r(ce);dt=c(as,"PyTorch installation"),as.forEach(t),Ke.forEach(t),De=u(e),C=l(e,"P",{});var Qe=r(C);mt=c(Qe,"You\u2019ll need to install a version of PyTorch compatible with the built-in CUDA drivers. Adding the following two lines to your "),pe=l(Qe,"CODE",{});var os=r(pe);vt=c(os,"requirements.txt"),os.forEach(t),gt=c(Qe," file should work:"),Qe.forEach(t),Ge=u(e),y(Y.$$.fragment,e),Ce=u(e),q=l(e,"P",{});var We=r(q);yt=c(We,"You can verify whether the installation was successful by running the following code in your "),ue=l(We,"CODE",{});var ls=r(ue);wt=c(ls,"app.py"),ls.forEach(t),_t=c(We," and checking the output in your Space logs:"),We.forEach(t),qe=u(e),y(H.$$.fragment,e),Ie=u(e),m=l(e,"P",{});var X=r(m);$t=c(X,"Many frameworks automatically use the GPU if one is available. This is the case for the Pipelines in \u{1F917} "),fe=l(X,"CODE",{});var rs=r(fe);Pt=c(rs,"transformers"),rs.forEach(t),bt=c(X,", "),he=l(X,"CODE",{});var is=r(he);xt=c(is,"fastai"),is.forEach(t),kt=c(X," and many others. In other cases, or if you use PyTorch directly, you may need to move your models and data to the GPU to ensure computation is done on the accelerator and not on the CPU. You can use PyTorch\u2019s "),de=l(X,"CODE",{});var ns=r(de);Et=c(ns,".to()"),ns.forEach(t),jt=c(X," syntax, for example:"),X.forEach(t),Je=u(e),y(L.$$.fragment,e),Ne=u(e),U=l(e,"H2",{class:!0});var Ze=r(U);I=l(Ze,"A",{id:!0,class:!0,href:!0});var cs=r(I);me=l(cs,"SPAN",{});var ps=r(me);y(z.$$.fragment,ps),ps.forEach(t),cs.forEach(t),Ut=u(Ze),ve=l(Ze,"SPAN",{});var us=r(ve);At=c(us,"JAX"),us.forEach(t),Ze.forEach(t),Xe=u(e),J=l(e,"P",{});var et=r(J);St=c(et,"If you use JAX, you need to specify the binary that is compatible with the CUDA and cuDNN versions installed in your GPU Space. Please, add the following two lines to your "),ge=l(et,"CODE",{});var fs=r(ge);Tt=c(fs,"requirements.txt"),fs.forEach(t),Dt=c(et," file:"),et.forEach(t),Me=u(e),y(B.$$.fragment,e),Oe=u(e),ae=l(e,"P",{});var hs=r(ae);Gt=c(hs,"After that, you can verify the installation by printing the output from the following code and checking it in your Space logs."),hs.forEach(t),Ye=u(e),y(V.$$.fragment,e),He=u(e),A=l(e,"H2",{class:!0});var tt=r(A);N=l(tt,"A",{id:!0,class:!0,href:!0});var ds=r(N);ye=l(ds,"SPAN",{});var ms=r(ye);y(R.$$.fragment,ms),ms.forEach(t),ds.forEach(t),Ct=u(tt),we=l(tt,"SPAN",{});var vs=r(we);qt=c(vs,"Tensorflow"),vs.forEach(t),tt.forEach(t),Le=u(e),d=l(e,"P",{});var x=r(d);It=c(x,"The default "),_e=l(x,"CODE",{});var gs=r(_e);Jt=c(gs,"tensorflow"),gs.forEach(t),Nt=c(x," installation should recognize the CUDA device. Just add "),$e=l(x,"CODE",{});var ys=r($e);Xt=c(ys,"tensorflow"),ys.forEach(t),Mt=c(x," to your "),Pe=l(x,"CODE",{});var ws=r(Pe);Ot=c(ws,"requirements.txt"),ws.forEach(t),Yt=c(x," file and use the following code in your "),be=l(x,"CODE",{});var _s=r(be);Ht=c(_s,"app.py"),_s.forEach(t),Lt=c(x," to verify in your Space logs."),x.forEach(t),ze=u(e),y(F.$$.fragment,e),this.h()},h(){f(h,"name","hf:doc:metadata"),f(h,"content",JSON.stringify(As)),f(S,"id","using-gpu-spaces"),f(S,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(S,"href","#using-gpu-spaces"),f(v,"class","relative group"),f(Q,"class","block dark:hidden"),st(Q.src,zt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-gpu-settings.png")||f(Q,"src",zt),f(W,"class","hidden dark:block"),st(W.src,Bt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-gpu-settings-dark.png")||f(W,"src",Bt),f(k,"class","flex justify-center"),f(ee,"class","block dark:hidden"),st(ee.src,Vt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-running-badge.png")||f(ee,"src",Vt),f(te,"class","hidden dark:block"),st(te.src,Rt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-running-badge-dark.png")||f(te,"src",Rt),f(E,"class","flex justify-center"),f(G,"id","pytorch-installation"),f(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(G,"href","#pytorch-installation"),f(j,"class","relative group"),f(I,"id","jax"),f(I,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(I,"href","#jax"),f(U,"class","relative group"),f(N,"id","tensorflow"),f(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),f(N,"href","#tensorflow"),f(A,"class","relative group")},m(e,a){s(document.head,h),i(e,b,a),i(e,v,a),s(v,S),s(S,le),w(M,le,null),s(v,ot),s(v,re),s(re,lt),i(e,ke,a),i(e,T,a),s(T,rt),s(T,ie),s(ie,it),s(T,nt),i(e,Ee,a),i(e,k,a),s(k,Q),s(k,ct),s(k,W),i(e,je,a),w(D,e,a),i(e,Ue,a),i(e,Z,a),s(Z,pt),i(e,Ae,a),i(e,E,a),s(E,ee),s(E,ut),s(E,te),i(e,Se,a),i(e,se,a),s(se,ft),i(e,Te,a),i(e,j,a),s(j,G),s(G,ne),w(O,ne,null),s(j,ht),s(j,ce),s(ce,dt),i(e,De,a),i(e,C,a),s(C,mt),s(C,pe),s(pe,vt),s(C,gt),i(e,Ge,a),w(Y,e,a),i(e,Ce,a),i(e,q,a),s(q,yt),s(q,ue),s(ue,wt),s(q,_t),i(e,qe,a),w(H,e,a),i(e,Ie,a),i(e,m,a),s(m,$t),s(m,fe),s(fe,Pt),s(m,bt),s(m,he),s(he,xt),s(m,kt),s(m,de),s(de,Et),s(m,jt),i(e,Je,a),w(L,e,a),i(e,Ne,a),i(e,U,a),s(U,I),s(I,me),w(z,me,null),s(U,Ut),s(U,ve),s(ve,At),i(e,Xe,a),i(e,J,a),s(J,St),s(J,ge),s(ge,Tt),s(J,Dt),i(e,Me,a),w(B,e,a),i(e,Oe,a),i(e,ae,a),s(ae,Gt),i(e,Ye,a),w(V,e,a),i(e,He,a),i(e,A,a),s(A,N),s(N,ye),w(R,ye,null),s(A,Ct),s(A,we),s(we,qt),i(e,Le,a),i(e,d,a),s(d,It),s(d,_e),s(_e,Jt),s(d,Nt),s(d,$e),s($e,Xt),s(d,Mt),s(d,Pe),s(Pe,Ot),s(d,Yt),s(d,be),s(be,Ht),s(d,Lt),i(e,ze,a),w(F,e,a),Be=!0},p(e,[a]){const K={};a&2&&(K.$$scope={dirty:a,ctx:e}),D.$set(K)},i(e){Be||(_(M.$$.fragment,e),_(D.$$.fragment,e),_(O.$$.fragment,e),_(Y.$$.fragment,e),_(H.$$.fragment,e),_(L.$$.fragment,e),_(z.$$.fragment,e),_(B.$$.fragment,e),_(V.$$.fragment,e),_(R.$$.fragment,e),_(F.$$.fragment,e),Be=!0)},o(e){$(M.$$.fragment,e),$(D.$$.fragment,e),$(O.$$.fragment,e),$(Y.$$.fragment,e),$(H.$$.fragment,e),$(L.$$.fragment,e),$(z.$$.fragment,e),$(B.$$.fragment,e),$(V.$$.fragment,e),$(R.$$.fragment,e),$(F.$$.fragment,e),Be=!1},d(e){t(h),e&&t(b),e&&t(v),P(M),e&&t(ke),e&&t(T),e&&t(Ee),e&&t(k),e&&t(je),P(D,e),e&&t(Ue),e&&t(Z),e&&t(Ae),e&&t(E),e&&t(Se),e&&t(se),e&&t(Te),e&&t(j),P(O),e&&t(De),e&&t(C),e&&t(Ge),P(Y,e),e&&t(Ce),e&&t(q),e&&t(qe),P(H,e),e&&t(Ie),e&&t(m),e&&t(Je),P(L,e),e&&t(Ne),e&&t(U),P(z),e&&t(Xe),e&&t(J),e&&t(Me),P(B,e),e&&t(Oe),e&&t(ae),e&&t(Ye),P(V,e),e&&t(He),e&&t(A),P(R),e&&t(Le),e&&t(d),e&&t(ze),P(F,e)}}}const As={local:"using-gpu-spaces",sections:[{local:"pytorch-installation",title:"PyTorch installation"},{local:"jax",title:"JAX"},{local:"tensorflow",title:"Tensorflow"}],title:"Using GPU Spaces"};function Ss(xe){return ks(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class qs extends $s{constructor(h){super();Ps(this,h,Ss,Us,bs,{})}}export{qs as default,As as metadata};
