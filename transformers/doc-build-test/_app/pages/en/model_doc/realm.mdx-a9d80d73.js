import{S as Cm,i as Im,s as Dm,e as n,k as d,w as f,t as a,L as Nm,c as r,d as t,m as l,a as s,x as u,h as i,b as c,J as e,g as p,y as g,q as _,o as k,B as v}from"../../../chunks/vendor-9e2b328e.js";import{T as _r}from"../../../chunks/Tip-76f97a76.js";import{D as q}from"../../../chunks/Docstring-50fd6873.js";import{C as Ve}from"../../../chunks/CodeBlock-b9ff96e9.js";import{I as J}from"../../../chunks/IconCopyLink-fd0e58fd.js";import"../../../chunks/CopyButton-4b97cbf7.js";function Om(N){let h,y,b,T,$;return{c(){h=n("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),b=n("code"),T=a("Module"),$=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var w=s(h);y=i(w,"Although the recipe for forward pass needs to be defined within this function, one should call the "),b=r(w,"CODE",{});var z=s(b);T=i(z,"Module"),z.forEach(t),$=i(w,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),w.forEach(t)},m(R,w){p(R,h,w),e(h,y),e(h,b),e(b,T),e(h,$)},d(R){R&&t(h)}}}function Wm(N){let h,y,b,T,$;return{c(){h=n("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),b=n("code"),T=a("Module"),$=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var w=s(h);y=i(w,"Although the recipe for forward pass needs to be defined within this function, one should call the "),b=r(w,"CODE",{});var z=s(b);T=i(z,"Module"),z.forEach(t),$=i(w,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),w.forEach(t)},m(R,w){p(R,h,w),e(h,y),e(h,b),e(b,T),e(h,$)},d(R){R&&t(h)}}}function Km(N){let h,y,b,T,$;return{c(){h=n("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),b=n("code"),T=a("Module"),$=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var w=s(h);y=i(w,"Although the recipe for forward pass needs to be defined within this function, one should call the "),b=r(w,"CODE",{});var z=s(b);T=i(z,"Module"),z.forEach(t),$=i(w,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),w.forEach(t)},m(R,w){p(R,h,w),e(h,y),e(h,b),e(b,T),e(h,$)},d(R){R&&t(h)}}}function Bm(N){let h,y,b,T,$;return{c(){h=n("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),b=n("code"),T=a("Module"),$=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var w=s(h);y=i(w,"Although the recipe for forward pass needs to be defined within this function, one should call the "),b=r(w,"CODE",{});var z=s(b);T=i(z,"Module"),z.forEach(t),$=i(w,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),w.forEach(t)},m(R,w){p(R,h,w),e(h,y),e(h,b),e(b,T),e(h,$)},d(R){R&&t(h)}}}function Qm(N){let h,y,b,T,$;return{c(){h=n("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),b=n("code"),T=a("Module"),$=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var w=s(h);y=i(w,"Although the recipe for forward pass needs to be defined within this function, one should call the "),b=r(w,"CODE",{});var z=s(b);T=i(z,"Module"),z.forEach(t),$=i(w,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),w.forEach(t)},m(R,w){p(R,h,w),e(h,y),e(h,b),e(b,T),e(h,$)},d(R){R&&t(h)}}}function Hm(N){let h,y,b,T,$,R,w,z,ks,kr,ae,$e,Yo,Ue,vs,en,bs,vr,Ee,ws,Xe,Rs,Ts,br,io,ys,wr,lo,tn,$s,Rr,Z,Es,Ge,zs,qs,Je,xs,As,Tr,ie,ze,on,Ze,Ps,nn,js,yr,x,Ye,Ls,rn,Ms,Fs,L,sn,co,Ss,Cs,an,mo,Is,Ds,dn,ho,Ns,Os,ln,po,Ws,Ks,cn,fo,Bs,Qs,mn,uo,Hs,Vs,et,Us,tt,Xs,Gs,Js,de,Zs,go,Ys,ea,_o,ta,oa,na,hn,ra,sa,ot,$r,le,qe,pn,nt,aa,fn,ia,Er,E,rt,da,un,la,ca,xe,ko,ma,ha,vo,pa,fa,ua,st,ga,bo,_a,ka,va,Y,at,ba,gn,wa,Ra,it,wo,Ta,_n,ya,$a,Ro,Ea,kn,za,qa,Ae,dt,xa,lt,Aa,vn,Pa,ja,La,O,ct,Ma,bn,Fa,Sa,mt,Ca,ce,Ia,wn,Da,Na,Rn,Oa,Wa,Ka,Tn,Ba,P,ht,Qa,pt,Ha,yn,Va,Ua,Xa,me,$n,Ga,Ja,ft,Za,En,Ya,ei,ti,ut,oi,zn,ni,ri,si,gt,To,ai,qn,ii,di,yo,li,xn,ci,mi,An,hi,pi,_t,zr,he,Pe,Pn,kt,fi,jn,ui,qr,M,vt,gi,bt,_i,Ln,ki,vi,bi,je,$o,wi,Ri,Eo,Ti,yi,$i,wt,Ei,zo,zi,qi,xi,j,Rt,Ai,Tt,Pi,Mn,ji,Li,Mi,pe,Fn,Fi,Si,yt,Ci,Sn,Ii,Di,Ni,$t,Oi,Cn,Wi,Ki,Bi,Et,qo,Qi,In,Hi,Vi,xo,Ui,Dn,Xi,Gi,Nn,Ji,Zi,zt,xr,fe,Le,On,qt,Yi,Wn,ed,Ar,Q,xt,td,Kn,od,nd,Me,At,rd,Bn,sd,Pr,ue,Fe,Qn,Pt,ad,Hn,id,jr,H,jt,dd,Lt,ld,Mt,cd,md,hd,F,Ft,pd,ge,fd,Ao,ud,gd,Vn,_d,kd,vd,Se,bd,Un,wd,Rd,St,Lr,_e,Ce,Xn,Ct,Td,Gn,yd,Mr,V,It,$d,Dt,Ed,Nt,zd,qd,xd,S,Ot,Ad,ke,Pd,Po,jd,Ld,Jn,Md,Fd,Sd,Ie,Cd,Zn,Id,Dd,Wt,Fr,ve,De,Yn,Kt,Nd,er,Od,Sr,U,Bt,Wd,Qt,Kd,Ht,Bd,Qd,Hd,C,Vt,Vd,be,Ud,jo,Xd,Gd,tr,Jd,Zd,Yd,Ne,el,or,tl,ol,Ut,Cr,we,Oe,nr,Xt,nl,rr,rl,Ir,X,Gt,sl,Jt,al,Zt,il,dl,ll,ee,Yt,cl,Re,ml,Lo,hl,pl,sr,fl,ul,gl,We,Dr,Te,Ke,ar,eo,_l,ir,kl,Nr,G,to,vl,Be,dr,bl,wl,oo,Rl,Tl,yl,I,no,$l,ye,El,Mo,zl,ql,lr,xl,Al,Pl,Qe,jl,cr,Ll,Ml,ro,Or;return R=new J({}),Ue=new J({}),Ze=new J({}),Ye=new q({props:{name:"class transformers.RealmConfig",anchor:"transformers.RealmConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"retriever_proj_size",val:" = 128"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_candidates",val:" = 8"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"span_hidden_size",val:" = 256"},{name:"max_span_width",val:" = 10"},{name:"reader_layer_norm_eps",val:" = 0.001"},{name:"reader_beam_size",val:" = 5"},{name:"reader_seq_len",val:" = 320"},{name:"num_block_records",val:" = 13353718"},{name:"searcher_beam_size",val:" = 5000"},{name:"searcher_seq_len",val:" = 64"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/configuration_realm.py#L36",parametersDescription:[{anchor:"transformers.RealmConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the REALM model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>, <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmScorer">RealmScorer</a>, <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder">RealmKnowledgeAugEncoder</a>, or
<a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmReader">RealmReader</a>.`,name:"vocab_size"},{anchor:"transformers.RealmConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimension of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.RealmConfig.retriever_proj_size",description:`<strong>retriever_proj_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Dimension of the retriever(embedder) projection.`,name:"retriever_proj_size"},{anchor:"transformers.RealmConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.RealmConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.RealmConfig.num_candidates",description:`<strong>num_candidates</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of candidates inputted to the RealmScorer or RealmKnowledgeAugEncoder.`,name:"num_candidates"},{anchor:"transformers.RealmConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.RealmConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.RealmConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.RealmConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.RealmConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.RealmConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>, <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmScorer">RealmScorer</a>,
<a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder">RealmKnowledgeAugEncoder</a>, or <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmReader">RealmReader</a>.`,name:"type_vocab_size"},{anchor:"transformers.RealmConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.RealmConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.RealmConfig.span_hidden_size",description:`<strong>span_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimension of the reader&#x2019;s spans.`,name:"span_hidden_size"},{anchor:"transformers.RealmConfig.max_span_width",description:`<strong>max_span_width</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Max span width of the reader.`,name:"max_span_width"},{anchor:"transformers.RealmConfig.reader_layer_norm_eps",description:`<strong>reader_layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The epsilon used by the reader&#x2019;s layer normalization layers.`,name:"reader_layer_norm_eps"},{anchor:"transformers.RealmConfig.reader_beam_size",description:`<strong>reader_beam_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Beam size of the reader.`,name:"reader_beam_size"},{anchor:"transformers.RealmConfig.reader_seq_len",description:`<strong>reader_seq_len</strong> (<code>int</code>, <em>optional</em>, defaults to 288+32) &#x2014;
Maximum sequence length of the reader.`,name:"reader_seq_len"},{anchor:"transformers.RealmConfig.num_block_records",description:`<strong>num_block_records</strong> (<code>int</code>, <em>optional</em>, defaults to 13353718) &#x2014;
Number of block records.`,name:"num_block_records"},{anchor:"transformers.RealmConfig.searcher_beam_size",description:`<strong>searcher_beam_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5000) &#x2014;
Beam size of the searcher. Note that when eval mode is enabled, <em>searcher_beam_size</em> will be the same as
<em>reader_beam_size</em>.`,name:"searcher_beam_size"},{anchor:"transformers.RealmConfig.searcher_seq_len",description:`<strong>searcher_seq_len</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Maximum sequence length of the searcher.`,name:"searcher_seq_len"}]}}),ot=new Ve({props:{code:`from transformers import RealmEmbedder, RealmConfig

# Initializing a REALM realm-cc-news-pretrained-* style configuration
configuration = RealmConfig()

# Initializing a model from the google/realm-cc-news-pretrained-embedder style configuration
model = RealmEmbedder(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmEmbedder, RealmConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a REALM realm-cc-news-pretrained-* style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = RealmConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the google/realm-cc-news-pretrained-embedder style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmEmbedder(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),nt=new J({}),rt=new q({props:{name:"class transformers.RealmTokenizer",anchor:"transformers.RealmTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/tokenization_realm.py#L88",parametersDescription:[{anchor:"transformers.RealmTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RealmTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RealmTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.RealmTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.RealmTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RealmTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RealmTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RealmTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RealmTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RealmTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).
strip_accents &#x2014; (<code>bool</code>, <em>optional</em>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"tokenize_chinese_chars"}]}}),at=new q({props:{name:"build\\_inputs\\_with\\_special\\_tokens",anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/tokenization_realm.py#L295",parametersDescription:[{anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),dt=new q({props:{name:"get\\_special\\_tokens\\_mask",anchor:"transformers.RealmTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/tokenization_realm.py#L320",parametersDescription:[{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new q({props:{name:"create\\_token\\_type\\_ids\\_from\\_sequences",anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/tokenization_realm.py#L348",parametersDescription:[{anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new Ve({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),ht=new q({props:{name:"batch\\_encode\\_candidates",anchor:"transformers.RealmTokenizer.batch_encode_candidates",parameters:[{name:"text",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/tokenization_realm.py#L222",parametersDescription:[{anchor:"transformers.RealmTokenizer.batch_encode_candidates.text",description:`<strong>text</strong> (<code>List[List[str]]</code>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).`,name:"text"},{anchor:"transformers.RealmTokenizer.batch_encode_candidates.text_pair",description:`<strong>text_pair</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).
**kwargs &#x2014;
Keyword arguments of the <strong>call</strong> method.`,name:"text_pair"}],returnDescription:`
<p>Encoded text or text pair.</p>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),_t=new Ve({props:{code:`from transformers import RealmTokenizer

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_text = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`}}),kt=new J({}),vt=new q({props:{name:"class transformers.RealmTokenizerFast",anchor:"transformers.RealmTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/tokenization_realm_fast.py#L79",parametersDescription:[{anchor:"transformers.RealmTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RealmTokenizerFast.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RealmTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RealmTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RealmTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RealmTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RealmTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RealmTokenizerFast.clean_text",description:`<strong>clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean the text before tokenization by removing any control characters and replacing all
whitespaces by the classic one.`,name:"clean_text"},{anchor:"transformers.RealmTokenizerFast.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see <a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">this
issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RealmTokenizerFast.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"},{anchor:"transformers.RealmTokenizerFast.wordpieces_prefix",description:`<strong>wordpieces_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;##&quot;</code>) &#x2014;
The prefix for subwords.`,name:"wordpieces_prefix"}]}}),Rt=new q({props:{name:"batch\\_encode\\_candidates",anchor:"transformers.RealmTokenizerFast.batch_encode_candidates",parameters:[{name:"text",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/tokenization_realm_fast.py#L170",parametersDescription:[{anchor:"transformers.RealmTokenizerFast.batch_encode_candidates.text",description:`<strong>text</strong> (<code>List[List[str]]</code>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).`,name:"text"},{anchor:"transformers.RealmTokenizerFast.batch_encode_candidates.text_pair",description:`<strong>text_pair</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).
**kwargs &#x2014;
Keyword arguments of the <strong>call</strong> method.`,name:"text_pair"}],returnDescription:`
<p>Encoded text or text pair.</p>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),zt=new Ve({props:{code:`from transformers import RealmTokenizerFast

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizerFast.from_pretrained("google/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizerFast.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_text = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`}}),qt=new J({}),xt=new q({props:{name:"class transformers.RealmRetriever",anchor:"transformers.RealmRetriever",parameters:[{name:"block_records",val:""},{name:"tokenizer",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/retrieval_realm.py#L73",parametersDescription:[{anchor:"transformers.RealmRetriever.block_records",description:`<strong>block_records</strong> (<code>np.ndarray</code>) &#x2014;
A numpy array which cantains evidence texts.`,name:"block_records"},{anchor:"transformers.RealmRetriever.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>) &#x2014;
The tokenizer to encode retrieved texts.`,name:"tokenizer"}]}}),At=new q({props:{name:"block\\_has\\_answer",anchor:"transformers.RealmRetriever.block_has_answer",parameters:[{name:"concat_inputs",val:""},{name:"answer_ids",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/retrieval_realm.py#L128"}}),Pt=new J({}),jt=new q({props:{name:"class transformers.RealmEmbedder",anchor:"transformers.RealmEmbedder",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/modeling_realm.py#L1146",parametersDescription:[{anchor:"transformers.RealmEmbedder.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ft=new q({props:{name:"forward",anchor:"transformers.RealmEmbedder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/modeling_realm.py#L1160",parametersDescription:[{anchor:"transformers.RealmEmbedder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmEmbedder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmEmbedder.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmEmbedder.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmEmbedder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmEmbedder.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmEmbedder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmEmbedder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmEmbedder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmEmbedderOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>projected_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.retriever_proj_size)</code>) \u2014 Projected score.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmEmbedderOutput</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),Se=new _r({props:{$$slots:{default:[Om]},$$scope:{ctx:N}}}),St=new Ve({props:{code:`from transformers import RealmTokenizer, RealmEmbedder
import torch

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-embedder")
model = RealmEmbedder.from_pretrained("google/realm-cc-news-pretrained-embedder")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

projected_score = outputs.projected_score,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmEmbedder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-embedder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmEmbedder.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-embedder&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>projected_score = outputs.projected_score`}}),Ct=new J({}),It=new q({props:{name:"class transformers.RealmScorer",anchor:"transformers.RealmScorer",parameters:[{name:"config",val:""},{name:"query_embedder",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/modeling_realm.py#L1226",parametersDescription:[{anchor:"transformers.RealmScorer.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.RealmScorer.query_embedder",description:`<strong>query_embedder</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>) &#x2014;
Embedder for input sequences. If not specified, it will use the same embedder as candidate sequences.`,name:"query_embedder"}]}}),Ot=new q({props:{name:"forward",anchor:"transformers.RealmScorer.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"candidate_input_ids",val:" = None"},{name:"candidate_attention_mask",val:" = None"},{name:"candidate_token_type_ids",val:" = None"},{name:"candidate_inputs_embeds",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/modeling_realm.py#L1242",parametersDescription:[{anchor:"transformers.RealmScorer.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmScorer.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmScorer.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmScorer.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmScorer.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmScorer.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmScorer.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmScorer.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmScorer.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmScorer.forward.candidate_input_ids",description:`<strong>candidate_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>) &#x2014;
Indices of candidate input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"candidate_input_ids"},{anchor:"transformers.RealmScorer.forward.candidate_attention_mask",description:`<strong>candidate_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"candidate_attention_mask"},{anchor:"transformers.RealmScorer.forward.candidate_token_type_ids",description:`<strong>candidate_token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"candidate_token_type_ids"},{anchor:"transformers.RealmScorer.forward.candidate_inputs_embeds",description:`<strong>candidate_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size * num_candidates, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>candidate_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <em>candidate_input_ids</em> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"candidate_inputs_embeds"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmScorerOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li><strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_candidates)</code>) \u2014 The relevance score of document candidates (before softmax).</li>
<li><strong>query_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.retriever_proj_size)</code>) \u2014 Query score derived from the query embedder.</li>
<li><strong>candidate_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_candidates, config.retriever_proj_size)</code>) \u2014 Candidate score derived from the embedder.</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmScorerOutput</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ie=new _r({props:{$$slots:{default:[Wm]},$$scope:{ctx:N}}}),Wt=new Ve({props:{code:`import torch
from transformers import RealmTokenizer, RealmScorer

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-scorer")
model = RealmScorer.from_pretrained("google/realm-cc-news-pretrained-scorer", num_candidates=2)

# batch_size = 2, num_candidates = 2
input_texts = ["How are you?", "What is the item in the picture?"]
candidates_texts = [["Hello world!", "Nice to meet you!"], ["A cute cat.", "An adorable dog."]]

inputs = tokenizer(input_texts, return_tensors="pt")
candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=10, return_tensors="pt")

outputs = model(
    **inputs,
    candidate_input_ids=candidates_inputs.input_ids,
    candidate_attention_mask=candidates_inputs.attention_mask,
    candidate_token_type_ids=candidates_inputs.token_type_ids,
)
relevance_score = outputs.relevance_score,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmScorer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-scorer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmScorer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-scorer&quot;</span>, num_candidates=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_texts = [<span class="hljs-string">&quot;How are you?&quot;</span>, <span class="hljs-string">&quot;What is the item in the picture?&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>candidates_texts = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;A cute cat.&quot;</span>, <span class="hljs-string">&quot;An adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(input_texts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(
<span class="hljs-meta">... </span>    **inputs,
<span class="hljs-meta">... </span>    candidate_input_ids=candidates_inputs.input_ids,
<span class="hljs-meta">... </span>    candidate_attention_mask=candidates_inputs.attention_mask,
<span class="hljs-meta">... </span>    candidate_token_type_ids=candidates_inputs.token_type_ids,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_score = outputs.relevance_score`}}),Kt=new J({}),Bt=new q({props:{name:"class transformers.RealmKnowledgeAugEncoder",anchor:"transformers.RealmKnowledgeAugEncoder",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/modeling_realm.py#L1373",parametersDescription:[{anchor:"transformers.RealmKnowledgeAugEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Vt=new q({props:{name:"forward",anchor:"transformers.RealmKnowledgeAugEncoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"relevance_score",val:" = None"},{name:"labels",val:" = None"},{name:"mlm_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/modeling_realm.py#L1392",parametersDescription:[{anchor:"transformers.RealmKnowledgeAugEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.relevance_score",description:`<strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates)</code>, <em>optional</em>) &#x2014;
Relevance score derived from RealmScorer, must be specified if you want to compute the masked language
modeling loss.`,name:"relevance_score"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.mlm_mask",description:`<strong>mlm_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid calculating joint loss on certain positions. If not specified, the loss will not be masked.
Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"mlm_mask"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ne=new _r({props:{$$slots:{default:[Km]},$$scope:{ctx:N}}}),Ut=new Ve({props:{code:`import torch
from transformers import RealmTokenizer, RealmKnowledgeAugEncoder

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
model = RealmKnowledgeAugEncoder.from_pretrained(
    "google/realm-cc-news-pretrained-encoder", num_candidates=2
)

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

inputs = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmKnowledgeAugEncoder

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmKnowledgeAugEncoder.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>, num_candidates=<span class="hljs-number">2</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Xt=new J({}),Gt=new q({props:{name:"class transformers.RealmReader",anchor:"transformers.RealmReader",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/modeling_realm.py#L1521",parametersDescription:[{anchor:"transformers.RealmReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Yt=new q({props:{name:"forward",anchor:"transformers.RealmReader.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"relevance_score",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"has_answers",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/modeling_realm.py#L1535",parametersDescription:[{anchor:"transformers.RealmReader.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmReader.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmReader.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmReader.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(reader_beam_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmReader.forward.relevance_score",description:`<strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Relevance score, which must be specified if you want to compute the marginal log loss.`,name:"relevance_score"},{anchor:"transformers.RealmReader.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.RealmReader.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"},{anchor:"transformers.RealmReader.forward.has_answers",description:`<strong>has_answers</strong> (<code>torch.BoolTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Whether or not the evidence block has answer(s).`,name:"has_answers"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmReaderOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Total loss.</p>
</li>
<li>
<p><strong>retriever_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Retriever loss.</p>
</li>
<li>
<p><strong>reader_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Reader loss.</p>
</li>
<li>
<p><strong>retriever_correct</strong> (<code>torch.BoolTensor</code> of shape <code>(config.searcher_beam_size,)</code>, <em>optional</em>) \u2014 Whether or not an evidence block contains answer.</p>
</li>
<li>
<p><strong>reader_correct</strong> (<code>torch.BoolTensor</code> of shape <code>(config.reader_beam_size, num_candidates)</code>, <em>optional</em>) \u2014 Whether or not a span candidate contains answer.</p>
</li>
<li>
<p><strong>block_idx</strong> (<code>torch.LongTensor</code> of shape <code>()</code>) \u2014 The index of the retrieved evidence block in which the predicted answer is most likely.</p>
</li>
<li>
<p><strong>candidate</strong> (<code>torch.LongTensor</code> of shape <code>()</code>) \u2014 The index of the retrieved span candidates in which the predicted answer is most likely.</p>
</li>
<li>
<p><strong>start_pos</strong> (<code>torch.IntTensor</code> of shape <code>()</code>) \u2014 Predicted answer starting position in <em>RealmReader</em>\u2019s inputs.</p>
</li>
<li>
<p><strong>end_pos:</strong> (<code>torch.IntTensor</code> of shape <code>()</code>) \u2014 Predicted answer ending position in <em>RealmReader</em>\u2019s inputs.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmReaderOutput</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),We=new _r({props:{$$slots:{default:[Bm]},$$scope:{ctx:N}}}),eo=new J({}),to=new q({props:{name:"class transformers.RealmForOpenQA",anchor:"transformers.RealmForOpenQA",parameters:[{name:"config",val:""},{name:"retriever",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/modeling_realm.py#L1722",parametersDescription:[{anchor:"transformers.RealmForOpenQA.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),no=new q({props:{name:"forward",anchor:"transformers.RealmForOpenQA.forward",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"answer_ids",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/realm/modeling_realm.py#L1745",parametersDescription:[{anchor:"transformers.RealmForOpenQA.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(1, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmForOpenQA.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(1, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmForOpenQA.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(1, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token (should not be used in this model by design).</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmForOpenQA.forward.answer_ids",description:`<strong>answer_ids</strong> (<code>list</code> of shape <code>(num_answers, answer_length)</code>, <em>optional</em>) &#x2014;
Answer ids for computing the marginal log-likelihood loss. Indices should be in <code>[-1, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-1</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"answer_ids"},{anchor:"transformers.RealmForOpenQA.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmForOpenQAOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li><strong>reader_output</strong> (<code>dict</code>) \u2014 Reader output.</li>
<li><strong>predicted_answer_ids</strong> (<code>torch.LongTensor</code> of shape <code>(answer_sequence_length)</code>) \u2014 Predicted answer ids.</li>
</ul>
`,returnType:`
<p><code>transformers.models.realm.modeling_realm.RealmForOpenQAOutput</code>or <code>tuple(torch.FloatTensor)</code></p>
`}}),Qe=new _r({props:{$$slots:{default:[Qm]},$$scope:{ctx:N}}}),ro=new Ve({props:{code:`import torch
from transformers import RealmForOpenQA, RealmRetriever, RealmTokenizer

retriever = RealmRetriever.from_pretrained("google/realm-orqa-nq-openqa")
tokenizer = RealmTokenizer.from_pretrained("google/realm-orqa-nq-openqa")
model = RealmForOpenQA.from_pretrained("google/realm-orqa-nq-openqa", retriever=retriever)

question = "Who is the pioneer in modern computer science?"
question_ids = tokenizer([question], return_tensors="pt")
answer_ids = tokenizer(
    ["alan mathison turing"],
    add_special_tokens=False,
    return_token_type_ids=False,
    return_attention_mask=False,
).input_ids

reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=False)
predicted_answer = tokenizer.decode(predicted_answer_ids)
loss = reader_output.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmForOpenQA, RealmRetriever, RealmTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>retriever = RealmRetriever.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmForOpenQA.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>, retriever=retriever)

<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Who is the pioneer in modern computer science?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>question_ids = tokenizer([question], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_ids = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;alan mathison turing&quot;</span>],
<span class="hljs-meta">... </span>    add_special_tokens=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    return_token_type_ids=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    return_attention_mask=<span class="hljs-literal">False</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_answer = tokenizer.decode(predicted_answer_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = reader_output.loss`}}),{c(){h=n("meta"),y=d(),b=n("h1"),T=n("a"),$=n("span"),f(R.$$.fragment),w=d(),z=n("span"),ks=a("REALM"),kr=d(),ae=n("h2"),$e=n("a"),Yo=n("span"),f(Ue.$$.fragment),vs=d(),en=n("span"),bs=a("Overview"),vr=d(),Ee=n("p"),ws=a("The REALM model was proposed in "),Xe=n("a"),Rs=a("REALM: Retrieval-Augmented Language Model Pre-Training"),Ts=a(` by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It\u2019s a
retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then
utilizes retrieved documents to process question answering tasks.`),br=d(),io=n("p"),ys=a("The abstract from the paper is the following:"),wr=d(),lo=n("p"),tn=n("em"),$s=a(`Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks
such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we
augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend
over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the
first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language
modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We
demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the
challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both
explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous
methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as
interpretability and modularity.`),Rr=d(),Z=n("p"),Es=a("This model was contributed by "),Ge=n("a"),zs=a("qqaatw"),qs=a(`. The original code can be found
`),Je=n("a"),xs=a("here"),As=a("."),Tr=d(),ie=n("h2"),ze=n("a"),on=n("span"),f(Ze.$$.fragment),Ps=d(),nn=n("span"),js=a("RealmConfig"),yr=d(),x=n("div"),f(Ye.$$.fragment),Ls=d(),rn=n("p"),Ms=a("This is the configuration class to store the configuration of"),Fs=d(),L=n("ol"),sn=n("li"),co=n("a"),Ss=a("RealmEmbedder"),Cs=d(),an=n("li"),mo=n("a"),Is=a("RealmScorer"),Ds=d(),dn=n("li"),ho=n("a"),Ns=a("RealmKnowledgeAugEncoder"),Os=d(),ln=n("li"),po=n("a"),Ws=a("RealmRetriever"),Ks=d(),cn=n("li"),fo=n("a"),Bs=a("RealmReader"),Qs=d(),mn=n("li"),uo=n("a"),Hs=a("RealmForOpenQA"),Vs=d(),et=n("p"),Us=a(`It is used to instantiate an REALM model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the REALM
`),tt=n("a"),Xs=a("realm-cc-news-pretrained"),Gs=a(" architecture."),Js=d(),de=n("p"),Zs=a("Configuration objects inherit from "),go=n("a"),Ys=a("PretrainedConfig"),ea=a(` and can be used to control the model outputs. Read the
documentation from `),_o=n("a"),ta=a("PretrainedConfig"),oa=a(" for more information."),na=d(),hn=n("p"),ra=a("Example:"),sa=d(),f(ot.$$.fragment),$r=d(),le=n("h2"),qe=n("a"),pn=n("span"),f(nt.$$.fragment),aa=d(),fn=n("span"),ia=a("RealmTokenizer"),Er=d(),E=n("div"),f(rt.$$.fragment),da=d(),un=n("p"),la=a("Construct a REALM tokenizer."),ca=d(),xe=n("p"),ko=n("a"),ma=a("RealmTokenizer"),ha=a(" is identical to "),vo=n("a"),pa=a("BertTokenizer"),fa=a(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),ua=d(),st=n("p"),ga=a("This tokenizer inherits from "),bo=n("a"),_a=a("PreTrainedTokenizer"),ka=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),va=d(),Y=n("div"),f(at.$$.fragment),ba=d(),gn=n("p"),wa=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A REALM sequence has the following format:`),Ra=d(),it=n("ul"),wo=n("li"),Ta=a("single sequence: "),_n=n("code"),ya=a("[CLS] X [SEP]"),$a=d(),Ro=n("li"),Ea=a("pair of sequences: "),kn=n("code"),za=a("[CLS] A [SEP] B [SEP]"),qa=d(),Ae=n("div"),f(dt.$$.fragment),xa=d(),lt=n("p"),Aa=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),vn=n("code"),Pa=a("prepare_for_model"),ja=a(" method."),La=d(),O=n("div"),f(ct.$$.fragment),Ma=d(),bn=n("p"),Fa=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence
pair mask has the following format:`),Sa=d(),f(mt.$$.fragment),Ca=d(),ce=n("p"),Ia=a("If "),wn=n("code"),Da=a("token_ids_1"),Na=a(" is "),Rn=n("code"),Oa=a("None"),Wa=a(", this method only returns the first portion of the mask (0s)."),Ka=d(),Tn=n("div"),Ba=d(),P=n("div"),f(ht.$$.fragment),Qa=d(),pt=n("p"),Ha=a("Encode a batch of text or text pair. This method is similar to regular "),yn=n("strong"),Va=a("call"),Ua=a(` method but has the following
differences:`),Xa=d(),me=n("ol"),$n=n("li"),Ga=a("Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Ja=d(),ft=n("li"),Za=a("Always pad the sequences to "),En=n("em"),Ya=a("max_length"),ei=a("."),ti=d(),ut=n("li"),oi=a("Must specify "),zn=n("em"),ni=a("max_length"),ri=a(" in order to stack packs of candidates into a batch."),si=d(),gt=n("ul"),To=n("li"),ai=a("single sequence: "),qn=n("code"),ii=a("[CLS] X [SEP]"),di=d(),yo=n("li"),li=a("pair of sequences: "),xn=n("code"),ci=a("[CLS] A [SEP] B [SEP]"),mi=d(),An=n("p"),hi=a("Example:"),pi=d(),f(_t.$$.fragment),zr=d(),he=n("h2"),Pe=n("a"),Pn=n("span"),f(kt.$$.fragment),fi=d(),jn=n("span"),ui=a("RealmTokenizerFast"),qr=d(),M=n("div"),f(vt.$$.fragment),gi=d(),bt=n("p"),_i=a("Construct a \u201Cfast\u201D REALM tokenizer (backed by HuggingFace\u2019s "),Ln=n("em"),ki=a("tokenizers"),vi=a(" library). Based on WordPiece."),bi=d(),je=n("p"),$o=n("a"),wi=a("RealmTokenizerFast"),Ri=a(" is identical to "),Eo=n("a"),Ti=a("BertTokenizerFast"),yi=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),$i=d(),wt=n("p"),Ei=a("This tokenizer inherits from "),zo=n("a"),zi=a("PreTrainedTokenizerFast"),qi=a(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),xi=d(),j=n("div"),f(Rt.$$.fragment),Ai=d(),Tt=n("p"),Pi=a("Encode a batch of text or text pair. This method is similar to regular "),Mn=n("strong"),ji=a("call"),Li=a(` method but has the following
differences:`),Mi=d(),pe=n("ol"),Fn=n("li"),Fi=a("Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Si=d(),yt=n("li"),Ci=a("Always pad the sequences to "),Sn=n("em"),Ii=a("max_length"),Di=a("."),Ni=d(),$t=n("li"),Oi=a("Must specify "),Cn=n("em"),Wi=a("max_length"),Ki=a(" in order to stack packs of candidates into a batch."),Bi=d(),Et=n("ul"),qo=n("li"),Qi=a("single sequence: "),In=n("code"),Hi=a("[CLS] X [SEP]"),Vi=d(),xo=n("li"),Ui=a("pair of sequences: "),Dn=n("code"),Xi=a("[CLS] A [SEP] B [SEP]"),Gi=d(),Nn=n("p"),Ji=a("Example:"),Zi=d(),f(zt.$$.fragment),xr=d(),fe=n("h2"),Le=n("a"),On=n("span"),f(qt.$$.fragment),Yi=d(),Wn=n("span"),ed=a("RealmRetriever"),Ar=d(),Q=n("div"),f(xt.$$.fragment),td=d(),Kn=n("p"),od=a(`The retriever of REALM outputting the retrieved evidence block and whether the block has answers as well as answer
positions.\u201D`),nd=d(),Me=n("div"),f(At.$$.fragment),rd=d(),Bn=n("p"),sd=a("check if retrieved_blocks has answers."),Pr=d(),ue=n("h2"),Fe=n("a"),Qn=n("span"),f(Pt.$$.fragment),ad=d(),Hn=n("span"),id=a("RealmEmbedder"),jr=d(),H=n("div"),f(jt.$$.fragment),dd=d(),Lt=n("p"),ld=a(`The embedder of REALM outputting projected score that will be used to calculate relevance score.
This model is a PyTorch `),Mt=n("a"),cd=a("torch.nn.Module"),md=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),hd=d(),F=n("div"),f(Ft.$$.fragment),pd=d(),ge=n("p"),fd=a("The "),Ao=n("a"),ud=a("RealmEmbedder"),gd=a(" forward method, overrides the "),Vn=n("code"),_d=a("__call__"),kd=a(" special method."),vd=d(),f(Se.$$.fragment),bd=d(),Un=n("p"),wd=a("Example:"),Rd=d(),f(St.$$.fragment),Lr=d(),_e=n("h2"),Ce=n("a"),Xn=n("span"),f(Ct.$$.fragment),Td=d(),Gn=n("span"),yd=a("RealmScorer"),Mr=d(),V=n("div"),f(It.$$.fragment),$d=d(),Dt=n("p"),Ed=a(`The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).
This model is a PyTorch `),Nt=n("a"),zd=a("torch.nn.Module"),qd=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),xd=d(),S=n("div"),f(Ot.$$.fragment),Ad=d(),ke=n("p"),Pd=a("The "),Po=n("a"),jd=a("RealmScorer"),Ld=a(" forward method, overrides the "),Jn=n("code"),Md=a("__call__"),Fd=a(" special method."),Sd=d(),f(Ie.$$.fragment),Cd=d(),Zn=n("p"),Id=a("Example:"),Dd=d(),f(Wt.$$.fragment),Fr=d(),ve=n("h2"),De=n("a"),Yn=n("span"),f(Kt.$$.fragment),Nd=d(),er=n("span"),Od=a("RealmKnowledgeAugEncoder"),Sr=d(),U=n("div"),f(Bt.$$.fragment),Wd=d(),Qt=n("p"),Kd=a(`The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood loss.
This model is a PyTorch `),Ht=n("a"),Bd=a("torch.nn.Module"),Qd=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Hd=d(),C=n("div"),f(Vt.$$.fragment),Vd=d(),be=n("p"),Ud=a("The "),jo=n("a"),Xd=a("RealmKnowledgeAugEncoder"),Gd=a(" forward method, overrides the "),tr=n("code"),Jd=a("__call__"),Zd=a(" special method."),Yd=d(),f(Ne.$$.fragment),el=d(),or=n("p"),tl=a("Example:"),ol=d(),f(Ut.$$.fragment),Cr=d(),we=n("h2"),Oe=n("a"),nr=n("span"),f(Xt.$$.fragment),nl=d(),rr=n("span"),rl=a("RealmReader"),Ir=d(),X=n("div"),f(Gt.$$.fragment),sl=d(),Jt=n("p"),al=a(`The reader of REALM.
This model is a PyTorch `),Zt=n("a"),il=a("torch.nn.Module"),dl=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ll=d(),ee=n("div"),f(Yt.$$.fragment),cl=d(),Re=n("p"),ml=a("The "),Lo=n("a"),hl=a("RealmReader"),pl=a(" forward method, overrides the "),sr=n("code"),fl=a("__call__"),ul=a(" special method."),gl=d(),f(We.$$.fragment),Dr=d(),Te=n("h2"),Ke=n("a"),ar=n("span"),f(eo.$$.fragment),_l=d(),ir=n("span"),kl=a("RealmForOpenQA"),Nr=d(),G=n("div"),f(to.$$.fragment),vl=d(),Be=n("p"),dr=n("code"),bl=a("RealmForOpenQA"),wl=a(` for end-to-end open domain question answering.
This model is a PyTorch `),oo=n("a"),Rl=a("torch.nn.Module"),Tl=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),yl=d(),I=n("div"),f(no.$$.fragment),$l=d(),ye=n("p"),El=a("The "),Mo=n("a"),zl=a("RealmForOpenQA"),ql=a(" forward method, overrides the "),lr=n("code"),xl=a("__call__"),Al=a(" special method."),Pl=d(),f(Qe.$$.fragment),jl=d(),cr=n("p"),Ll=a("Example:"),Ml=d(),f(ro.$$.fragment),this.h()},l(o){const m=Nm('[data-svelte="svelte-1phssyn"]',document.head);h=r(m,"META",{name:!0,content:!0}),m.forEach(t),y=l(o),b=r(o,"H1",{class:!0});var so=s(b);T=r(so,"A",{id:!0,class:!0,href:!0});var mr=s(T);$=r(mr,"SPAN",{});var hr=s($);u(R.$$.fragment,hr),hr.forEach(t),mr.forEach(t),w=l(so),z=r(so,"SPAN",{});var pr=s(z);ks=i(pr,"REALM"),pr.forEach(t),so.forEach(t),kr=l(o),ae=r(o,"H2",{class:!0});var ao=s(ae);$e=r(ao,"A",{id:!0,class:!0,href:!0});var Ol=s($e);Yo=r(Ol,"SPAN",{});var Wl=s(Yo);u(Ue.$$.fragment,Wl),Wl.forEach(t),Ol.forEach(t),vs=l(ao),en=r(ao,"SPAN",{});var Kl=s(en);bs=i(Kl,"Overview"),Kl.forEach(t),ao.forEach(t),vr=l(o),Ee=r(o,"P",{});var Wr=s(Ee);ws=i(Wr,"The REALM model was proposed in "),Xe=r(Wr,"A",{href:!0,rel:!0});var Bl=s(Xe);Rs=i(Bl,"REALM: Retrieval-Augmented Language Model Pre-Training"),Bl.forEach(t),Ts=i(Wr,` by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It\u2019s a
retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then
utilizes retrieved documents to process question answering tasks.`),Wr.forEach(t),br=l(o),io=r(o,"P",{});var Ql=s(io);ys=i(Ql,"The abstract from the paper is the following:"),Ql.forEach(t),wr=l(o),lo=r(o,"P",{});var Hl=s(lo);tn=r(Hl,"EM",{});var Vl=s(tn);$s=i(Vl,`Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks
such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we
augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend
over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the
first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language
modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We
demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the
challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both
explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous
methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as
interpretability and modularity.`),Vl.forEach(t),Hl.forEach(t),Rr=l(o),Z=r(o,"P",{});var Fo=s(Z);Es=i(Fo,"This model was contributed by "),Ge=r(Fo,"A",{href:!0,rel:!0});var Ul=s(Ge);zs=i(Ul,"qqaatw"),Ul.forEach(t),qs=i(Fo,`. The original code can be found
`),Je=r(Fo,"A",{href:!0,rel:!0});var Xl=s(Je);xs=i(Xl,"here"),Xl.forEach(t),As=i(Fo,"."),Fo.forEach(t),Tr=l(o),ie=r(o,"H2",{class:!0});var Kr=s(ie);ze=r(Kr,"A",{id:!0,class:!0,href:!0});var Gl=s(ze);on=r(Gl,"SPAN",{});var Jl=s(on);u(Ze.$$.fragment,Jl),Jl.forEach(t),Gl.forEach(t),Ps=l(Kr),nn=r(Kr,"SPAN",{});var Zl=s(nn);js=i(Zl,"RealmConfig"),Zl.forEach(t),Kr.forEach(t),yr=l(o),x=r(o,"DIV",{class:!0});var D=s(x);u(Ye.$$.fragment,D),Ls=l(D),rn=r(D,"P",{});var Yl=s(rn);Ms=i(Yl,"This is the configuration class to store the configuration of"),Yl.forEach(t),Fs=l(D),L=r(D,"OL",{});var W=s(L);sn=r(W,"LI",{});var ec=s(sn);co=r(ec,"A",{href:!0});var tc=s(co);Ss=i(tc,"RealmEmbedder"),tc.forEach(t),ec.forEach(t),Cs=l(W),an=r(W,"LI",{});var oc=s(an);mo=r(oc,"A",{href:!0});var nc=s(mo);Is=i(nc,"RealmScorer"),nc.forEach(t),oc.forEach(t),Ds=l(W),dn=r(W,"LI",{});var rc=s(dn);ho=r(rc,"A",{href:!0});var sc=s(ho);Ns=i(sc,"RealmKnowledgeAugEncoder"),sc.forEach(t),rc.forEach(t),Os=l(W),ln=r(W,"LI",{});var ac=s(ln);po=r(ac,"A",{href:!0});var ic=s(po);Ws=i(ic,"RealmRetriever"),ic.forEach(t),ac.forEach(t),Ks=l(W),cn=r(W,"LI",{});var dc=s(cn);fo=r(dc,"A",{href:!0});var lc=s(fo);Bs=i(lc,"RealmReader"),lc.forEach(t),dc.forEach(t),Qs=l(W),mn=r(W,"LI",{});var cc=s(mn);uo=r(cc,"A",{href:!0});var mc=s(uo);Hs=i(mc,"RealmForOpenQA"),mc.forEach(t),cc.forEach(t),W.forEach(t),Vs=l(D),et=r(D,"P",{});var Br=s(et);Us=i(Br,`It is used to instantiate an REALM model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the REALM
`),tt=r(Br,"A",{href:!0,rel:!0});var hc=s(tt);Xs=i(hc,"realm-cc-news-pretrained"),hc.forEach(t),Gs=i(Br," architecture."),Br.forEach(t),Js=l(D),de=r(D,"P",{});var So=s(de);Zs=i(So,"Configuration objects inherit from "),go=r(So,"A",{href:!0});var pc=s(go);Ys=i(pc,"PretrainedConfig"),pc.forEach(t),ea=i(So,` and can be used to control the model outputs. Read the
documentation from `),_o=r(So,"A",{href:!0});var fc=s(_o);ta=i(fc,"PretrainedConfig"),fc.forEach(t),oa=i(So," for more information."),So.forEach(t),na=l(D),hn=r(D,"P",{});var uc=s(hn);ra=i(uc,"Example:"),uc.forEach(t),sa=l(D),u(ot.$$.fragment,D),D.forEach(t),$r=l(o),le=r(o,"H2",{class:!0});var Qr=s(le);qe=r(Qr,"A",{id:!0,class:!0,href:!0});var gc=s(qe);pn=r(gc,"SPAN",{});var _c=s(pn);u(nt.$$.fragment,_c),_c.forEach(t),gc.forEach(t),aa=l(Qr),fn=r(Qr,"SPAN",{});var kc=s(fn);ia=i(kc,"RealmTokenizer"),kc.forEach(t),Qr.forEach(t),Er=l(o),E=r(o,"DIV",{class:!0});var A=s(E);u(rt.$$.fragment,A),da=l(A),un=r(A,"P",{});var vc=s(un);la=i(vc,"Construct a REALM tokenizer."),vc.forEach(t),ca=l(A),xe=r(A,"P",{});var fr=s(xe);ko=r(fr,"A",{href:!0});var bc=s(ko);ma=i(bc,"RealmTokenizer"),bc.forEach(t),ha=i(fr," is identical to "),vo=r(fr,"A",{href:!0});var wc=s(vo);pa=i(wc,"BertTokenizer"),wc.forEach(t),fa=i(fr,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),fr.forEach(t),ua=l(A),st=r(A,"P",{});var Hr=s(st);ga=i(Hr,"This tokenizer inherits from "),bo=r(Hr,"A",{href:!0});var Rc=s(bo);_a=i(Rc,"PreTrainedTokenizer"),Rc.forEach(t),ka=i(Hr,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Hr.forEach(t),va=l(A),Y=r(A,"DIV",{class:!0});var Co=s(Y);u(at.$$.fragment,Co),ba=l(Co),gn=r(Co,"P",{});var Tc=s(gn);wa=i(Tc,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A REALM sequence has the following format:`),Tc.forEach(t),Ra=l(Co),it=r(Co,"UL",{});var Vr=s(it);wo=r(Vr,"LI",{});var Fl=s(wo);Ta=i(Fl,"single sequence: "),_n=r(Fl,"CODE",{});var yc=s(_n);ya=i(yc,"[CLS] X [SEP]"),yc.forEach(t),Fl.forEach(t),$a=l(Vr),Ro=r(Vr,"LI",{});var Sl=s(Ro);Ea=i(Sl,"pair of sequences: "),kn=r(Sl,"CODE",{});var $c=s(kn);za=i($c,"[CLS] A [SEP] B [SEP]"),$c.forEach(t),Sl.forEach(t),Vr.forEach(t),Co.forEach(t),qa=l(A),Ae=r(A,"DIV",{class:!0});var Ur=s(Ae);u(dt.$$.fragment,Ur),xa=l(Ur),lt=r(Ur,"P",{});var Xr=s(lt);Aa=i(Xr,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),vn=r(Xr,"CODE",{});var Ec=s(vn);Pa=i(Ec,"prepare_for_model"),Ec.forEach(t),ja=i(Xr," method."),Xr.forEach(t),Ur.forEach(t),La=l(A),O=r(A,"DIV",{class:!0});var He=s(O);u(ct.$$.fragment,He),Ma=l(He),bn=r(He,"P",{});var zc=s(bn);Fa=i(zc,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence
pair mask has the following format:`),zc.forEach(t),Sa=l(He),u(mt.$$.fragment,He),Ca=l(He),ce=r(He,"P",{});var Io=s(ce);Ia=i(Io,"If "),wn=r(Io,"CODE",{});var qc=s(wn);Da=i(qc,"token_ids_1"),qc.forEach(t),Na=i(Io," is "),Rn=r(Io,"CODE",{});var xc=s(Rn);Oa=i(xc,"None"),xc.forEach(t),Wa=i(Io,", this method only returns the first portion of the mask (0s)."),Io.forEach(t),He.forEach(t),Ka=l(A),Tn=r(A,"DIV",{class:!0}),s(Tn).forEach(t),Ba=l(A),P=r(A,"DIV",{class:!0});var K=s(P);u(ht.$$.fragment,K),Qa=l(K),pt=r(K,"P",{});var Gr=s(pt);Ha=i(Gr,"Encode a batch of text or text pair. This method is similar to regular "),yn=r(Gr,"STRONG",{});var Ac=s(yn);Va=i(Ac,"call"),Ac.forEach(t),Ua=i(Gr,` method but has the following
differences:`),Gr.forEach(t),Xa=l(K),me=r(K,"OL",{});var Do=s(me);$n=r(Do,"LI",{});var Pc=s($n);Ga=i(Pc,"Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Pc.forEach(t),Ja=l(Do),ft=r(Do,"LI",{});var Jr=s(ft);Za=i(Jr,"Always pad the sequences to "),En=r(Jr,"EM",{});var jc=s(En);Ya=i(jc,"max_length"),jc.forEach(t),ei=i(Jr,"."),Jr.forEach(t),ti=l(Do),ut=r(Do,"LI",{});var Zr=s(ut);oi=i(Zr,"Must specify "),zn=r(Zr,"EM",{});var Lc=s(zn);ni=i(Lc,"max_length"),Lc.forEach(t),ri=i(Zr," in order to stack packs of candidates into a batch."),Zr.forEach(t),Do.forEach(t),si=l(K),gt=r(K,"UL",{});var Yr=s(gt);To=r(Yr,"LI",{});var Cl=s(To);ai=i(Cl,"single sequence: "),qn=r(Cl,"CODE",{});var Mc=s(qn);ii=i(Mc,"[CLS] X [SEP]"),Mc.forEach(t),Cl.forEach(t),di=l(Yr),yo=r(Yr,"LI",{});var Il=s(yo);li=i(Il,"pair of sequences: "),xn=r(Il,"CODE",{});var Fc=s(xn);ci=i(Fc,"[CLS] A [SEP] B [SEP]"),Fc.forEach(t),Il.forEach(t),Yr.forEach(t),mi=l(K),An=r(K,"P",{});var Sc=s(An);hi=i(Sc,"Example:"),Sc.forEach(t),pi=l(K),u(_t.$$.fragment,K),K.forEach(t),A.forEach(t),zr=l(o),he=r(o,"H2",{class:!0});var es=s(he);Pe=r(es,"A",{id:!0,class:!0,href:!0});var Cc=s(Pe);Pn=r(Cc,"SPAN",{});var Ic=s(Pn);u(kt.$$.fragment,Ic),Ic.forEach(t),Cc.forEach(t),fi=l(es),jn=r(es,"SPAN",{});var Dc=s(jn);ui=i(Dc,"RealmTokenizerFast"),Dc.forEach(t),es.forEach(t),qr=l(o),M=r(o,"DIV",{class:!0});var te=s(M);u(vt.$$.fragment,te),gi=l(te),bt=r(te,"P",{});var ts=s(bt);_i=i(ts,"Construct a \u201Cfast\u201D REALM tokenizer (backed by HuggingFace\u2019s "),Ln=r(ts,"EM",{});var Nc=s(Ln);ki=i(Nc,"tokenizers"),Nc.forEach(t),vi=i(ts," library). Based on WordPiece."),ts.forEach(t),bi=l(te),je=r(te,"P",{});var ur=s(je);$o=r(ur,"A",{href:!0});var Oc=s($o);wi=i(Oc,"RealmTokenizerFast"),Oc.forEach(t),Ri=i(ur," is identical to "),Eo=r(ur,"A",{href:!0});var Wc=s(Eo);Ti=i(Wc,"BertTokenizerFast"),Wc.forEach(t),yi=i(ur,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ur.forEach(t),$i=l(te),wt=r(te,"P",{});var os=s(wt);Ei=i(os,"This tokenizer inherits from "),zo=r(os,"A",{href:!0});var Kc=s(zo);zi=i(Kc,"PreTrainedTokenizerFast"),Kc.forEach(t),qi=i(os,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),os.forEach(t),xi=l(te),j=r(te,"DIV",{class:!0});var B=s(j);u(Rt.$$.fragment,B),Ai=l(B),Tt=r(B,"P",{});var ns=s(Tt);Pi=i(ns,"Encode a batch of text or text pair. This method is similar to regular "),Mn=r(ns,"STRONG",{});var Bc=s(Mn);ji=i(Bc,"call"),Bc.forEach(t),Li=i(ns,` method but has the following
differences:`),ns.forEach(t),Mi=l(B),pe=r(B,"OL",{});var No=s(pe);Fn=r(No,"LI",{});var Qc=s(Fn);Fi=i(Qc,"Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Qc.forEach(t),Si=l(No),yt=r(No,"LI",{});var rs=s(yt);Ci=i(rs,"Always pad the sequences to "),Sn=r(rs,"EM",{});var Hc=s(Sn);Ii=i(Hc,"max_length"),Hc.forEach(t),Di=i(rs,"."),rs.forEach(t),Ni=l(No),$t=r(No,"LI",{});var ss=s($t);Oi=i(ss,"Must specify "),Cn=r(ss,"EM",{});var Vc=s(Cn);Wi=i(Vc,"max_length"),Vc.forEach(t),Ki=i(ss," in order to stack packs of candidates into a batch."),ss.forEach(t),No.forEach(t),Bi=l(B),Et=r(B,"UL",{});var as=s(Et);qo=r(as,"LI",{});var Dl=s(qo);Qi=i(Dl,"single sequence: "),In=r(Dl,"CODE",{});var Uc=s(In);Hi=i(Uc,"[CLS] X [SEP]"),Uc.forEach(t),Dl.forEach(t),Vi=l(as),xo=r(as,"LI",{});var Nl=s(xo);Ui=i(Nl,"pair of sequences: "),Dn=r(Nl,"CODE",{});var Xc=s(Dn);Xi=i(Xc,"[CLS] A [SEP] B [SEP]"),Xc.forEach(t),Nl.forEach(t),as.forEach(t),Gi=l(B),Nn=r(B,"P",{});var Gc=s(Nn);Ji=i(Gc,"Example:"),Gc.forEach(t),Zi=l(B),u(zt.$$.fragment,B),B.forEach(t),te.forEach(t),xr=l(o),fe=r(o,"H2",{class:!0});var is=s(fe);Le=r(is,"A",{id:!0,class:!0,href:!0});var Jc=s(Le);On=r(Jc,"SPAN",{});var Zc=s(On);u(qt.$$.fragment,Zc),Zc.forEach(t),Jc.forEach(t),Yi=l(is),Wn=r(is,"SPAN",{});var Yc=s(Wn);ed=i(Yc,"RealmRetriever"),Yc.forEach(t),is.forEach(t),Ar=l(o),Q=r(o,"DIV",{class:!0});var Oo=s(Q);u(xt.$$.fragment,Oo),td=l(Oo),Kn=r(Oo,"P",{});var em=s(Kn);od=i(em,`The retriever of REALM outputting the retrieved evidence block and whether the block has answers as well as answer
positions.\u201D`),em.forEach(t),nd=l(Oo),Me=r(Oo,"DIV",{class:!0});var ds=s(Me);u(At.$$.fragment,ds),rd=l(ds),Bn=r(ds,"P",{});var tm=s(Bn);sd=i(tm,"check if retrieved_blocks has answers."),tm.forEach(t),ds.forEach(t),Oo.forEach(t),Pr=l(o),ue=r(o,"H2",{class:!0});var ls=s(ue);Fe=r(ls,"A",{id:!0,class:!0,href:!0});var om=s(Fe);Qn=r(om,"SPAN",{});var nm=s(Qn);u(Pt.$$.fragment,nm),nm.forEach(t),om.forEach(t),ad=l(ls),Hn=r(ls,"SPAN",{});var rm=s(Hn);id=i(rm,"RealmEmbedder"),rm.forEach(t),ls.forEach(t),jr=l(o),H=r(o,"DIV",{class:!0});var Wo=s(H);u(jt.$$.fragment,Wo),dd=l(Wo),Lt=r(Wo,"P",{});var cs=s(Lt);ld=i(cs,`The embedder of REALM outputting projected score that will be used to calculate relevance score.
This model is a PyTorch `),Mt=r(cs,"A",{href:!0,rel:!0});var sm=s(Mt);cd=i(sm,"torch.nn.Module"),sm.forEach(t),md=i(cs,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),cs.forEach(t),hd=l(Wo),F=r(Wo,"DIV",{class:!0});var oe=s(F);u(Ft.$$.fragment,oe),pd=l(oe),ge=r(oe,"P",{});var Ko=s(ge);fd=i(Ko,"The "),Ao=r(Ko,"A",{href:!0});var am=s(Ao);ud=i(am,"RealmEmbedder"),am.forEach(t),gd=i(Ko," forward method, overrides the "),Vn=r(Ko,"CODE",{});var im=s(Vn);_d=i(im,"__call__"),im.forEach(t),kd=i(Ko," special method."),Ko.forEach(t),vd=l(oe),u(Se.$$.fragment,oe),bd=l(oe),Un=r(oe,"P",{});var dm=s(Un);wd=i(dm,"Example:"),dm.forEach(t),Rd=l(oe),u(St.$$.fragment,oe),oe.forEach(t),Wo.forEach(t),Lr=l(o),_e=r(o,"H2",{class:!0});var ms=s(_e);Ce=r(ms,"A",{id:!0,class:!0,href:!0});var lm=s(Ce);Xn=r(lm,"SPAN",{});var cm=s(Xn);u(Ct.$$.fragment,cm),cm.forEach(t),lm.forEach(t),Td=l(ms),Gn=r(ms,"SPAN",{});var mm=s(Gn);yd=i(mm,"RealmScorer"),mm.forEach(t),ms.forEach(t),Mr=l(o),V=r(o,"DIV",{class:!0});var Bo=s(V);u(It.$$.fragment,Bo),$d=l(Bo),Dt=r(Bo,"P",{});var hs=s(Dt);Ed=i(hs,`The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).
This model is a PyTorch `),Nt=r(hs,"A",{href:!0,rel:!0});var hm=s(Nt);zd=i(hm,"torch.nn.Module"),hm.forEach(t),qd=i(hs,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),hs.forEach(t),xd=l(Bo),S=r(Bo,"DIV",{class:!0});var ne=s(S);u(Ot.$$.fragment,ne),Ad=l(ne),ke=r(ne,"P",{});var Qo=s(ke);Pd=i(Qo,"The "),Po=r(Qo,"A",{href:!0});var pm=s(Po);jd=i(pm,"RealmScorer"),pm.forEach(t),Ld=i(Qo," forward method, overrides the "),Jn=r(Qo,"CODE",{});var fm=s(Jn);Md=i(fm,"__call__"),fm.forEach(t),Fd=i(Qo," special method."),Qo.forEach(t),Sd=l(ne),u(Ie.$$.fragment,ne),Cd=l(ne),Zn=r(ne,"P",{});var um=s(Zn);Id=i(um,"Example:"),um.forEach(t),Dd=l(ne),u(Wt.$$.fragment,ne),ne.forEach(t),Bo.forEach(t),Fr=l(o),ve=r(o,"H2",{class:!0});var ps=s(ve);De=r(ps,"A",{id:!0,class:!0,href:!0});var gm=s(De);Yn=r(gm,"SPAN",{});var _m=s(Yn);u(Kt.$$.fragment,_m),_m.forEach(t),gm.forEach(t),Nd=l(ps),er=r(ps,"SPAN",{});var km=s(er);Od=i(km,"RealmKnowledgeAugEncoder"),km.forEach(t),ps.forEach(t),Sr=l(o),U=r(o,"DIV",{class:!0});var Ho=s(U);u(Bt.$$.fragment,Ho),Wd=l(Ho),Qt=r(Ho,"P",{});var fs=s(Qt);Kd=i(fs,`The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood loss.
This model is a PyTorch `),Ht=r(fs,"A",{href:!0,rel:!0});var vm=s(Ht);Bd=i(vm,"torch.nn.Module"),vm.forEach(t),Qd=i(fs,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),fs.forEach(t),Hd=l(Ho),C=r(Ho,"DIV",{class:!0});var re=s(C);u(Vt.$$.fragment,re),Vd=l(re),be=r(re,"P",{});var Vo=s(be);Ud=i(Vo,"The "),jo=r(Vo,"A",{href:!0});var bm=s(jo);Xd=i(bm,"RealmKnowledgeAugEncoder"),bm.forEach(t),Gd=i(Vo," forward method, overrides the "),tr=r(Vo,"CODE",{});var wm=s(tr);Jd=i(wm,"__call__"),wm.forEach(t),Zd=i(Vo," special method."),Vo.forEach(t),Yd=l(re),u(Ne.$$.fragment,re),el=l(re),or=r(re,"P",{});var Rm=s(or);tl=i(Rm,"Example:"),Rm.forEach(t),ol=l(re),u(Ut.$$.fragment,re),re.forEach(t),Ho.forEach(t),Cr=l(o),we=r(o,"H2",{class:!0});var us=s(we);Oe=r(us,"A",{id:!0,class:!0,href:!0});var Tm=s(Oe);nr=r(Tm,"SPAN",{});var ym=s(nr);u(Xt.$$.fragment,ym),ym.forEach(t),Tm.forEach(t),nl=l(us),rr=r(us,"SPAN",{});var $m=s(rr);rl=i($m,"RealmReader"),$m.forEach(t),us.forEach(t),Ir=l(o),X=r(o,"DIV",{class:!0});var Uo=s(X);u(Gt.$$.fragment,Uo),sl=l(Uo),Jt=r(Uo,"P",{});var gs=s(Jt);al=i(gs,`The reader of REALM.
This model is a PyTorch `),Zt=r(gs,"A",{href:!0,rel:!0});var Em=s(Zt);il=i(Em,"torch.nn.Module"),Em.forEach(t),dl=i(gs,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),gs.forEach(t),ll=l(Uo),ee=r(Uo,"DIV",{class:!0});var Xo=s(ee);u(Yt.$$.fragment,Xo),cl=l(Xo),Re=r(Xo,"P",{});var Go=s(Re);ml=i(Go,"The "),Lo=r(Go,"A",{href:!0});var zm=s(Lo);hl=i(zm,"RealmReader"),zm.forEach(t),pl=i(Go," forward method, overrides the "),sr=r(Go,"CODE",{});var qm=s(sr);fl=i(qm,"__call__"),qm.forEach(t),ul=i(Go," special method."),Go.forEach(t),gl=l(Xo),u(We.$$.fragment,Xo),Xo.forEach(t),Uo.forEach(t),Dr=l(o),Te=r(o,"H2",{class:!0});var _s=s(Te);Ke=r(_s,"A",{id:!0,class:!0,href:!0});var xm=s(Ke);ar=r(xm,"SPAN",{});var Am=s(ar);u(eo.$$.fragment,Am),Am.forEach(t),xm.forEach(t),_l=l(_s),ir=r(_s,"SPAN",{});var Pm=s(ir);kl=i(Pm,"RealmForOpenQA"),Pm.forEach(t),_s.forEach(t),Nr=l(o),G=r(o,"DIV",{class:!0});var Jo=s(G);u(to.$$.fragment,Jo),vl=l(Jo),Be=r(Jo,"P",{});var gr=s(Be);dr=r(gr,"CODE",{});var jm=s(dr);bl=i(jm,"RealmForOpenQA"),jm.forEach(t),wl=i(gr,` for end-to-end open domain question answering.
This model is a PyTorch `),oo=r(gr,"A",{href:!0,rel:!0});var Lm=s(oo);Rl=i(Lm,"torch.nn.Module"),Lm.forEach(t),Tl=i(gr,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),gr.forEach(t),yl=l(Jo),I=r(Jo,"DIV",{class:!0});var se=s(I);u(no.$$.fragment,se),$l=l(se),ye=r(se,"P",{});var Zo=s(ye);El=i(Zo,"The "),Mo=r(Zo,"A",{href:!0});var Mm=s(Mo);zl=i(Mm,"RealmForOpenQA"),Mm.forEach(t),ql=i(Zo," forward method, overrides the "),lr=r(Zo,"CODE",{});var Fm=s(lr);xl=i(Fm,"__call__"),Fm.forEach(t),Al=i(Zo," special method."),Zo.forEach(t),Pl=l(se),u(Qe.$$.fragment,se),jl=l(se),cr=r(se,"P",{});var Sm=s(cr);Ll=i(Sm,"Example:"),Sm.forEach(t),Ml=l(se),u(ro.$$.fragment,se),se.forEach(t),Jo.forEach(t),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(Vm)),c(T,"id","realm"),c(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T,"href","#realm"),c(b,"class","relative group"),c($e,"id","overview"),c($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($e,"href","#overview"),c(ae,"class","relative group"),c(Xe,"href","https://arxiv.org/abs/2002.08909"),c(Xe,"rel","nofollow"),c(Ge,"href","https://huggingface.co/qqaatw"),c(Ge,"rel","nofollow"),c(Je,"href","https://github.com/google-research/language/tree/master/language/realm"),c(Je,"rel","nofollow"),c(ze,"id","transformers.RealmConfig"),c(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ze,"href","#transformers.RealmConfig"),c(ie,"class","relative group"),c(co,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmEmbedder"),c(mo,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmScorer"),c(ho,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder"),c(po,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmRetriever"),c(fo,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmReader"),c(uo,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmForOpenQA"),c(tt,"href","https://huggingface.co/google/realm-cc-news-pretrained-embedder"),c(tt,"rel","nofollow"),c(go,"href","/docs/transformers/doc-build-test/en/main_classes/configuration#transformers.PretrainedConfig"),c(_o,"href","/docs/transformers/doc-build-test/en/main_classes/configuration#transformers.PretrainedConfig"),c(x,"class","docstring"),c(qe,"id","transformers.RealmTokenizer"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#transformers.RealmTokenizer"),c(le,"class","relative group"),c(ko,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmTokenizer"),c(vo,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer"),c(bo,"href","/docs/transformers/doc-build-test/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(Y,"class","docstring"),c(Ae,"class","docstring"),c(O,"class","docstring"),c(Tn,"class","docstring"),c(P,"class","docstring"),c(E,"class","docstring"),c(Pe,"id","transformers.RealmTokenizerFast"),c(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pe,"href","#transformers.RealmTokenizerFast"),c(he,"class","relative group"),c($o,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmTokenizerFast"),c(Eo,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizerFast"),c(zo,"href","/docs/transformers/doc-build-test/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),c(j,"class","docstring"),c(M,"class","docstring"),c(Le,"id","transformers.RealmRetriever"),c(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Le,"href","#transformers.RealmRetriever"),c(fe,"class","relative group"),c(Me,"class","docstring"),c(Q,"class","docstring"),c(Fe,"id","transformers.RealmEmbedder"),c(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fe,"href","#transformers.RealmEmbedder"),c(ue,"class","relative group"),c(Mt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Mt,"rel","nofollow"),c(Ao,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmEmbedder"),c(F,"class","docstring"),c(H,"class","docstring"),c(Ce,"id","transformers.RealmScorer"),c(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ce,"href","#transformers.RealmScorer"),c(_e,"class","relative group"),c(Nt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Nt,"rel","nofollow"),c(Po,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmScorer"),c(S,"class","docstring"),c(V,"class","docstring"),c(De,"id","transformers.RealmKnowledgeAugEncoder"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#transformers.RealmKnowledgeAugEncoder"),c(ve,"class","relative group"),c(Ht,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ht,"rel","nofollow"),c(jo,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder"),c(C,"class","docstring"),c(U,"class","docstring"),c(Oe,"id","transformers.RealmReader"),c(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Oe,"href","#transformers.RealmReader"),c(we,"class","relative group"),c(Zt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Zt,"rel","nofollow"),c(Lo,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmReader"),c(ee,"class","docstring"),c(X,"class","docstring"),c(Ke,"id","transformers.RealmForOpenQA"),c(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ke,"href","#transformers.RealmForOpenQA"),c(Te,"class","relative group"),c(oo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(oo,"rel","nofollow"),c(Mo,"href","/docs/transformers/doc-build-test/en/model_doc/realm#transformers.RealmForOpenQA"),c(I,"class","docstring"),c(G,"class","docstring")},m(o,m){e(document.head,h),p(o,y,m),p(o,b,m),e(b,T),e(T,$),g(R,$,null),e(b,w),e(b,z),e(z,ks),p(o,kr,m),p(o,ae,m),e(ae,$e),e($e,Yo),g(Ue,Yo,null),e(ae,vs),e(ae,en),e(en,bs),p(o,vr,m),p(o,Ee,m),e(Ee,ws),e(Ee,Xe),e(Xe,Rs),e(Ee,Ts),p(o,br,m),p(o,io,m),e(io,ys),p(o,wr,m),p(o,lo,m),e(lo,tn),e(tn,$s),p(o,Rr,m),p(o,Z,m),e(Z,Es),e(Z,Ge),e(Ge,zs),e(Z,qs),e(Z,Je),e(Je,xs),e(Z,As),p(o,Tr,m),p(o,ie,m),e(ie,ze),e(ze,on),g(Ze,on,null),e(ie,Ps),e(ie,nn),e(nn,js),p(o,yr,m),p(o,x,m),g(Ye,x,null),e(x,Ls),e(x,rn),e(rn,Ms),e(x,Fs),e(x,L),e(L,sn),e(sn,co),e(co,Ss),e(L,Cs),e(L,an),e(an,mo),e(mo,Is),e(L,Ds),e(L,dn),e(dn,ho),e(ho,Ns),e(L,Os),e(L,ln),e(ln,po),e(po,Ws),e(L,Ks),e(L,cn),e(cn,fo),e(fo,Bs),e(L,Qs),e(L,mn),e(mn,uo),e(uo,Hs),e(x,Vs),e(x,et),e(et,Us),e(et,tt),e(tt,Xs),e(et,Gs),e(x,Js),e(x,de),e(de,Zs),e(de,go),e(go,Ys),e(de,ea),e(de,_o),e(_o,ta),e(de,oa),e(x,na),e(x,hn),e(hn,ra),e(x,sa),g(ot,x,null),p(o,$r,m),p(o,le,m),e(le,qe),e(qe,pn),g(nt,pn,null),e(le,aa),e(le,fn),e(fn,ia),p(o,Er,m),p(o,E,m),g(rt,E,null),e(E,da),e(E,un),e(un,la),e(E,ca),e(E,xe),e(xe,ko),e(ko,ma),e(xe,ha),e(xe,vo),e(vo,pa),e(xe,fa),e(E,ua),e(E,st),e(st,ga),e(st,bo),e(bo,_a),e(st,ka),e(E,va),e(E,Y),g(at,Y,null),e(Y,ba),e(Y,gn),e(gn,wa),e(Y,Ra),e(Y,it),e(it,wo),e(wo,Ta),e(wo,_n),e(_n,ya),e(it,$a),e(it,Ro),e(Ro,Ea),e(Ro,kn),e(kn,za),e(E,qa),e(E,Ae),g(dt,Ae,null),e(Ae,xa),e(Ae,lt),e(lt,Aa),e(lt,vn),e(vn,Pa),e(lt,ja),e(E,La),e(E,O),g(ct,O,null),e(O,Ma),e(O,bn),e(bn,Fa),e(O,Sa),g(mt,O,null),e(O,Ca),e(O,ce),e(ce,Ia),e(ce,wn),e(wn,Da),e(ce,Na),e(ce,Rn),e(Rn,Oa),e(ce,Wa),e(E,Ka),e(E,Tn),e(E,Ba),e(E,P),g(ht,P,null),e(P,Qa),e(P,pt),e(pt,Ha),e(pt,yn),e(yn,Va),e(pt,Ua),e(P,Xa),e(P,me),e(me,$n),e($n,Ga),e(me,Ja),e(me,ft),e(ft,Za),e(ft,En),e(En,Ya),e(ft,ei),e(me,ti),e(me,ut),e(ut,oi),e(ut,zn),e(zn,ni),e(ut,ri),e(P,si),e(P,gt),e(gt,To),e(To,ai),e(To,qn),e(qn,ii),e(gt,di),e(gt,yo),e(yo,li),e(yo,xn),e(xn,ci),e(P,mi),e(P,An),e(An,hi),e(P,pi),g(_t,P,null),p(o,zr,m),p(o,he,m),e(he,Pe),e(Pe,Pn),g(kt,Pn,null),e(he,fi),e(he,jn),e(jn,ui),p(o,qr,m),p(o,M,m),g(vt,M,null),e(M,gi),e(M,bt),e(bt,_i),e(bt,Ln),e(Ln,ki),e(bt,vi),e(M,bi),e(M,je),e(je,$o),e($o,wi),e(je,Ri),e(je,Eo),e(Eo,Ti),e(je,yi),e(M,$i),e(M,wt),e(wt,Ei),e(wt,zo),e(zo,zi),e(wt,qi),e(M,xi),e(M,j),g(Rt,j,null),e(j,Ai),e(j,Tt),e(Tt,Pi),e(Tt,Mn),e(Mn,ji),e(Tt,Li),e(j,Mi),e(j,pe),e(pe,Fn),e(Fn,Fi),e(pe,Si),e(pe,yt),e(yt,Ci),e(yt,Sn),e(Sn,Ii),e(yt,Di),e(pe,Ni),e(pe,$t),e($t,Oi),e($t,Cn),e(Cn,Wi),e($t,Ki),e(j,Bi),e(j,Et),e(Et,qo),e(qo,Qi),e(qo,In),e(In,Hi),e(Et,Vi),e(Et,xo),e(xo,Ui),e(xo,Dn),e(Dn,Xi),e(j,Gi),e(j,Nn),e(Nn,Ji),e(j,Zi),g(zt,j,null),p(o,xr,m),p(o,fe,m),e(fe,Le),e(Le,On),g(qt,On,null),e(fe,Yi),e(fe,Wn),e(Wn,ed),p(o,Ar,m),p(o,Q,m),g(xt,Q,null),e(Q,td),e(Q,Kn),e(Kn,od),e(Q,nd),e(Q,Me),g(At,Me,null),e(Me,rd),e(Me,Bn),e(Bn,sd),p(o,Pr,m),p(o,ue,m),e(ue,Fe),e(Fe,Qn),g(Pt,Qn,null),e(ue,ad),e(ue,Hn),e(Hn,id),p(o,jr,m),p(o,H,m),g(jt,H,null),e(H,dd),e(H,Lt),e(Lt,ld),e(Lt,Mt),e(Mt,cd),e(Lt,md),e(H,hd),e(H,F),g(Ft,F,null),e(F,pd),e(F,ge),e(ge,fd),e(ge,Ao),e(Ao,ud),e(ge,gd),e(ge,Vn),e(Vn,_d),e(ge,kd),e(F,vd),g(Se,F,null),e(F,bd),e(F,Un),e(Un,wd),e(F,Rd),g(St,F,null),p(o,Lr,m),p(o,_e,m),e(_e,Ce),e(Ce,Xn),g(Ct,Xn,null),e(_e,Td),e(_e,Gn),e(Gn,yd),p(o,Mr,m),p(o,V,m),g(It,V,null),e(V,$d),e(V,Dt),e(Dt,Ed),e(Dt,Nt),e(Nt,zd),e(Dt,qd),e(V,xd),e(V,S),g(Ot,S,null),e(S,Ad),e(S,ke),e(ke,Pd),e(ke,Po),e(Po,jd),e(ke,Ld),e(ke,Jn),e(Jn,Md),e(ke,Fd),e(S,Sd),g(Ie,S,null),e(S,Cd),e(S,Zn),e(Zn,Id),e(S,Dd),g(Wt,S,null),p(o,Fr,m),p(o,ve,m),e(ve,De),e(De,Yn),g(Kt,Yn,null),e(ve,Nd),e(ve,er),e(er,Od),p(o,Sr,m),p(o,U,m),g(Bt,U,null),e(U,Wd),e(U,Qt),e(Qt,Kd),e(Qt,Ht),e(Ht,Bd),e(Qt,Qd),e(U,Hd),e(U,C),g(Vt,C,null),e(C,Vd),e(C,be),e(be,Ud),e(be,jo),e(jo,Xd),e(be,Gd),e(be,tr),e(tr,Jd),e(be,Zd),e(C,Yd),g(Ne,C,null),e(C,el),e(C,or),e(or,tl),e(C,ol),g(Ut,C,null),p(o,Cr,m),p(o,we,m),e(we,Oe),e(Oe,nr),g(Xt,nr,null),e(we,nl),e(we,rr),e(rr,rl),p(o,Ir,m),p(o,X,m),g(Gt,X,null),e(X,sl),e(X,Jt),e(Jt,al),e(Jt,Zt),e(Zt,il),e(Jt,dl),e(X,ll),e(X,ee),g(Yt,ee,null),e(ee,cl),e(ee,Re),e(Re,ml),e(Re,Lo),e(Lo,hl),e(Re,pl),e(Re,sr),e(sr,fl),e(Re,ul),e(ee,gl),g(We,ee,null),p(o,Dr,m),p(o,Te,m),e(Te,Ke),e(Ke,ar),g(eo,ar,null),e(Te,_l),e(Te,ir),e(ir,kl),p(o,Nr,m),p(o,G,m),g(to,G,null),e(G,vl),e(G,Be),e(Be,dr),e(dr,bl),e(Be,wl),e(Be,oo),e(oo,Rl),e(Be,Tl),e(G,yl),e(G,I),g(no,I,null),e(I,$l),e(I,ye),e(ye,El),e(ye,Mo),e(Mo,zl),e(ye,ql),e(ye,lr),e(lr,xl),e(ye,Al),e(I,Pl),g(Qe,I,null),e(I,jl),e(I,cr),e(cr,Ll),e(I,Ml),g(ro,I,null),Or=!0},p(o,[m]){const so={};m&2&&(so.$$scope={dirty:m,ctx:o}),Se.$set(so);const mr={};m&2&&(mr.$$scope={dirty:m,ctx:o}),Ie.$set(mr);const hr={};m&2&&(hr.$$scope={dirty:m,ctx:o}),Ne.$set(hr);const pr={};m&2&&(pr.$$scope={dirty:m,ctx:o}),We.$set(pr);const ao={};m&2&&(ao.$$scope={dirty:m,ctx:o}),Qe.$set(ao)},i(o){Or||(_(R.$$.fragment,o),_(Ue.$$.fragment,o),_(Ze.$$.fragment,o),_(Ye.$$.fragment,o),_(ot.$$.fragment,o),_(nt.$$.fragment,o),_(rt.$$.fragment,o),_(at.$$.fragment,o),_(dt.$$.fragment,o),_(ct.$$.fragment,o),_(mt.$$.fragment,o),_(ht.$$.fragment,o),_(_t.$$.fragment,o),_(kt.$$.fragment,o),_(vt.$$.fragment,o),_(Rt.$$.fragment,o),_(zt.$$.fragment,o),_(qt.$$.fragment,o),_(xt.$$.fragment,o),_(At.$$.fragment,o),_(Pt.$$.fragment,o),_(jt.$$.fragment,o),_(Ft.$$.fragment,o),_(Se.$$.fragment,o),_(St.$$.fragment,o),_(Ct.$$.fragment,o),_(It.$$.fragment,o),_(Ot.$$.fragment,o),_(Ie.$$.fragment,o),_(Wt.$$.fragment,o),_(Kt.$$.fragment,o),_(Bt.$$.fragment,o),_(Vt.$$.fragment,o),_(Ne.$$.fragment,o),_(Ut.$$.fragment,o),_(Xt.$$.fragment,o),_(Gt.$$.fragment,o),_(Yt.$$.fragment,o),_(We.$$.fragment,o),_(eo.$$.fragment,o),_(to.$$.fragment,o),_(no.$$.fragment,o),_(Qe.$$.fragment,o),_(ro.$$.fragment,o),Or=!0)},o(o){k(R.$$.fragment,o),k(Ue.$$.fragment,o),k(Ze.$$.fragment,o),k(Ye.$$.fragment,o),k(ot.$$.fragment,o),k(nt.$$.fragment,o),k(rt.$$.fragment,o),k(at.$$.fragment,o),k(dt.$$.fragment,o),k(ct.$$.fragment,o),k(mt.$$.fragment,o),k(ht.$$.fragment,o),k(_t.$$.fragment,o),k(kt.$$.fragment,o),k(vt.$$.fragment,o),k(Rt.$$.fragment,o),k(zt.$$.fragment,o),k(qt.$$.fragment,o),k(xt.$$.fragment,o),k(At.$$.fragment,o),k(Pt.$$.fragment,o),k(jt.$$.fragment,o),k(Ft.$$.fragment,o),k(Se.$$.fragment,o),k(St.$$.fragment,o),k(Ct.$$.fragment,o),k(It.$$.fragment,o),k(Ot.$$.fragment,o),k(Ie.$$.fragment,o),k(Wt.$$.fragment,o),k(Kt.$$.fragment,o),k(Bt.$$.fragment,o),k(Vt.$$.fragment,o),k(Ne.$$.fragment,o),k(Ut.$$.fragment,o),k(Xt.$$.fragment,o),k(Gt.$$.fragment,o),k(Yt.$$.fragment,o),k(We.$$.fragment,o),k(eo.$$.fragment,o),k(to.$$.fragment,o),k(no.$$.fragment,o),k(Qe.$$.fragment,o),k(ro.$$.fragment,o),Or=!1},d(o){t(h),o&&t(y),o&&t(b),v(R),o&&t(kr),o&&t(ae),v(Ue),o&&t(vr),o&&t(Ee),o&&t(br),o&&t(io),o&&t(wr),o&&t(lo),o&&t(Rr),o&&t(Z),o&&t(Tr),o&&t(ie),v(Ze),o&&t(yr),o&&t(x),v(Ye),v(ot),o&&t($r),o&&t(le),v(nt),o&&t(Er),o&&t(E),v(rt),v(at),v(dt),v(ct),v(mt),v(ht),v(_t),o&&t(zr),o&&t(he),v(kt),o&&t(qr),o&&t(M),v(vt),v(Rt),v(zt),o&&t(xr),o&&t(fe),v(qt),o&&t(Ar),o&&t(Q),v(xt),v(At),o&&t(Pr),o&&t(ue),v(Pt),o&&t(jr),o&&t(H),v(jt),v(Ft),v(Se),v(St),o&&t(Lr),o&&t(_e),v(Ct),o&&t(Mr),o&&t(V),v(It),v(Ot),v(Ie),v(Wt),o&&t(Fr),o&&t(ve),v(Kt),o&&t(Sr),o&&t(U),v(Bt),v(Vt),v(Ne),v(Ut),o&&t(Cr),o&&t(we),v(Xt),o&&t(Ir),o&&t(X),v(Gt),v(Yt),v(We),o&&t(Dr),o&&t(Te),v(eo),o&&t(Nr),o&&t(G),v(to),v(no),v(Qe),v(ro)}}}const Vm={local:"realm",sections:[{local:"overview",title:"Overview"},{local:"transformers.RealmConfig",title:"RealmConfig"},{local:"transformers.RealmTokenizer",title:"RealmTokenizer"},{local:"transformers.RealmTokenizerFast",title:"RealmTokenizerFast"},{local:"transformers.RealmRetriever",title:"RealmRetriever"},{local:"transformers.RealmEmbedder",title:"RealmEmbedder"},{local:"transformers.RealmScorer",title:"RealmScorer"},{local:"transformers.RealmKnowledgeAugEncoder",title:"RealmKnowledgeAugEncoder"},{local:"transformers.RealmReader",title:"RealmReader"},{local:"transformers.RealmForOpenQA",title:"RealmForOpenQA"}],title:"REALM"};function Um(N,h,y){let{fw:b}=h;return N.$$set=T=>{"fw"in T&&y(0,b=T.fw)},[b]}class th extends Cm{constructor(h){super();Im(this,h,Um,Hm,Dm,{fw:0})}}export{th as default,Vm as metadata};
