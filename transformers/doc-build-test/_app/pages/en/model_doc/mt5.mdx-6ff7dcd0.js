import{S as Xd,i as Kd,s as Jd,e as n,k as d,w as f,t as a,L as Qd,c as r,d as s,m as p,a as o,x as h,h as i,b as l,J as e,g as m,y as u,K as Yd,q as g,o as _,B as k}from"../../../chunks/vendor-9e2b328e.js";import{D as T}from"../../../chunks/Docstring-50fd6873.js";import{C as Ce}from"../../../chunks/CodeBlock-b9ff96e9.js";import{I as C}from"../../../chunks/IconCopyLink-fd0e58fd.js";import"../../../chunks/CopyButton-4b97cbf7.js";function Zd(Lr){let N,Ut,P,A,Ts,Pe,Nr,bs,Dr,$n,U,ne,$s,Ae,Ir,ws,Gr,wn,re,Or,Se,Vr,Ur,yn,Wt,Wr,zn,Bt,ys,Br,Mn,oe,Hr,Le,Rr,Xr,En,Ht,Kr,xn,$,zs,Ms,Ne,Jr,Qr,Es,xs,De,Yr,Zr,qs,js,Ie,eo,to,Fs,Cs,Ge,so,no,Ps,Rt,Oe,ro,oo,qn,D,ao,Ve,io,lo,Ue,po,co,jn,W,ae,As,We,mo,Ss,fo,Fn,S,Be,ho,L,uo,Xt,go,_o,Kt,ko,vo,He,To,bo,$o,B,wo,Jt,yo,zo,Qt,Mo,Eo,Cn,H,ie,Ls,Re,xo,Ns,qo,Pn,v,Xe,jo,Ke,Fo,Je,Co,Po,Ao,Qe,So,Yt,Lo,No,Do,I,Ye,Io,Ds,Go,Oo,Ze,Zt,Vo,Is,Uo,Wo,es,Bo,Gs,Ho,Ro,le,et,Xo,Os,Ko,Jo,de,tt,Qo,Vs,Yo,Zo,pe,st,ea,nt,ta,Us,sa,na,An,ce,ra,ts,oa,aa,Sn,R,me,Ws,rt,ia,Bs,la,Ln,b,ot,da,X,pa,Hs,ca,ma,at,fa,ha,ua,it,ga,ss,_a,ka,va,G,lt,Ta,Rs,ba,$a,dt,ns,wa,Xs,ya,za,rs,Ma,Ks,Ea,xa,fe,pt,qa,Js,ja,Nn,he,Fa,os,Ca,Pa,Dn,K,ue,Qs,ct,Aa,Ys,Sa,In,y,mt,La,ft,Na,as,Da,Ia,Ga,Zs,Oa,Va,ht,Gn,J,ge,en,ut,Ua,tn,Wa,On,z,gt,Ba,_t,Ha,is,Ra,Xa,Ka,sn,Ja,Qa,kt,Vn,Q,_e,nn,vt,Ya,rn,Za,Un,M,Tt,ei,bt,ti,ls,si,ni,ri,on,oi,ai,$t,Wn,Y,ke,an,wt,ii,ln,li,Bn,E,yt,di,zt,pi,ds,ci,mi,fi,dn,hi,ui,Mt,Hn,Z,ve,pn,Et,gi,cn,_i,Rn,x,xt,ki,qt,vi,ps,Ti,bi,$i,mn,wi,yi,jt,Xn,ee,Te,fn,Ft,zi,hn,Mi,Kn,q,Ct,Ei,Pt,xi,cs,qi,ji,Fi,un,Ci,Pi,At,Jn,te,be,gn,St,Ai,_n,Si,Qn,j,Lt,Li,Nt,Ni,ms,Di,Ii,Gi,kn,Oi,Vi,Dt,Yn,se,$e,vn,It,Ui,Tn,Wi,Zn,F,Gt,Bi,Ot,Hi,fs,Ri,Xi,Ki,bn,Ji,Qi,Vt,er;return Pe=new C({}),Ae=new C({}),We=new C({}),Be=new T({props:{name:"class transformers.MT5Config",anchor:"transformers.MT5Config",parameters:[{name:"vocab_size",val:" = 250112"},{name:"d_model",val:" = 512"},{name:"d_kv",val:" = 64"},{name:"d_ff",val:" = 1024"},{name:"num_layers",val:" = 8"},{name:"num_decoder_layers",val:" = None"},{name:"num_heads",val:" = 6"},{name:"relative_attention_num_buckets",val:" = 32"},{name:"dropout_rate",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-06"},{name:"initializer_factor",val:" = 1.0"},{name:"feed_forward_proj",val:" = 'gated-gelu'"},{name:"is_encoder_decoder",val:" = True"},{name:"use_cache",val:" = True"},{name:"tokenizer_class",val:" = 'T5Tokenizer'"},{name:"tie_word_embeddings",val:" = False"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"decoder_start_token_id",val:" = 0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/mt5/configuration_mt5.py#L24",parametersDescription:[{anchor:"transformers.MT5Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250112) &#x2014;
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/doc-build-test/en/model_doc/t5#transformers.T5Model">T5Model</a> or <a href="/docs/transformers/doc-build-test/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a>.`,name:"vocab_size"},{anchor:"transformers.MT5Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.MT5Config.d_kv",description:`<strong>d_kv</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Size of the key, query, value projections per attention head. <code>d_kv</code> has to be equal to <code>d_model // num_heads</code>.`,name:"d_kv"},{anchor:"transformers.MT5Config.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the intermediate feed forward layer in each <code>T5Block</code>.`,name:"d_ff"},{anchor:"transformers.MT5Config.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.MT5Config.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of hidden layers in the Transformer decoder. Will use the same value as <code>num_layers</code> if not set.`,name:"num_decoder_layers"},{anchor:"transformers.MT5Config.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.MT5Config.relative_attention_num_buckets",description:`<strong>relative_attention_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer.`,name:"relative_attention_num_buckets"},{anchor:"transformers.MT5Config.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.MT5Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MT5Config.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.MT5Config.feed_forward_proj",description:`<strong>feed_forward_proj</strong> (<code>string</code>, <em>optional</em>, defaults to <code>&quot;gated-gelu&quot;</code>) &#x2014;
Type of feed forward layer to be used. Should be one of <code>&quot;relu&quot;</code> or <code>&quot;gated-gelu&quot;</code>.`,name:"feed_forward_proj"},{anchor:"transformers.MT5Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}]}}),Re=new C({}),Xe=new T({props:{name:"class transformers.T5Tokenizer",anchor:"transformers.T5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/t5/tokenization_t5.py#L53",parametersDescription:[{anchor:"transformers.T5Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.T5Tokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.T5Tokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}]}}),Ye=new T({props:{name:"build\\_inputs\\_with\\_special\\_tokens",anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/t5/tokenization_t5.py#L223",parametersDescription:[{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),et=new T({props:{name:"convert\\_tokens\\_to\\_string",anchor:"transformers.T5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/t5/tokenization_t5.py#L284"}}),tt=new T({props:{name:"create\\_token\\_type\\_ids\\_from\\_sequences",anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/t5/tokenization_t5.py#L201",parametersDescription:[{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),st=new T({props:{name:"get\\_special\\_tokens\\_mask",anchor:"transformers.T5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/t5/tokenization_t5.py#L163",parametersDescription:[{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),rt=new C({}),ot=new T({props:{name:"class transformers.T5TokenizerFast",anchor:"transformers.T5TokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/t5/tokenization_t5_fast.py#L63",parametersDescription:[{anchor:"transformers.T5TokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5TokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5TokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5TokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5TokenizerFast.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5TokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}]}}),lt=new T({props:{name:"build\\_inputs\\_with\\_special\\_tokens",anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/t5/tokenization_t5_fast.py#L166",parametersDescription:[{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pt=new T({props:{name:"create\\_token\\_type\\_ids\\_from\\_sequences",anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/t5/tokenization_t5_fast.py#L192",parametersDescription:[{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new C({}),mt=new T({props:{name:"class transformers.MT5Model",anchor:"transformers.MT5Model",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/mt5/modeling_mt5.py#L28"}}),ht=new Ce({props:{code:`from transformers import MT5Model, T5Tokenizer

model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),ut=new C({}),gt=new T({props:{name:"class transformers.MT5ForConditionalGeneration",anchor:"transformers.MT5ForConditionalGeneration",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/mt5/modeling_mt5.py#L62"}}),kt=new Ce({props:{code:`from transformers import MT5ForConditionalGeneration, T5Tokenizer

model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(**inputs, labels=labels["input_ids"])
loss = outputs.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),vt=new C({}),Tt=new T({props:{name:"class transformers.MT5EncoderModel",anchor:"transformers.MT5EncoderModel",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/mt5/modeling_mt5.py#L94"}}),$t=new Ce({props:{code:`from transformers import MT5EncoderModel, T5Tokenizer

model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),wt=new C({}),yt=new T({props:{name:"class transformers.TFMT5Model",anchor:"transformers.TFMT5Model",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/mt5/modeling_tf_mt5.py#L28"}}),Mt=new Ce({props:{code:`from transformers import TFMT5Model, T5Tokenizer

model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),Et=new C({}),xt=new T({props:{name:"class transformers.TFMT5ForConditionalGeneration",anchor:"transformers.TFMT5ForConditionalGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/mt5/modeling_tf_mt5.py#L53"}}),jt=new Ce({props:{code:`from transformers import TFMT5ForConditionalGeneration, T5Tokenizer

model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(**inputs, labels=labels["input_ids"])
loss = outputs.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),Ft=new C({}),Ct=new T({props:{name:"class transformers.TFMT5EncoderModel",anchor:"transformers.TFMT5EncoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/mt5/modeling_tf_mt5.py#L79"}}),At=new Ce({props:{code:`from transformers import TFMT5EncoderModel, T5Tokenizer

model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),St=new C({}),Lt=new T({props:{name:"class transformers.FlaxMT5Model",anchor:"transformers.FlaxMT5Model",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/mt5/modeling_flax_mt5.py#L28"}}),Dt=new Ce({props:{code:`from transformers import FlaxMT5Model, T5Tokenizer

model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),It=new C({}),Gt=new T({props:{name:"class transformers.FlaxMT5ForConditionalGeneration",anchor:"transformers.FlaxMT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/mt5/modeling_flax_mt5.py#L55"}}),Vt=new Ce({props:{code:`from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer

model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){N=n("meta"),Ut=d(),P=n("h1"),A=n("a"),Ts=n("span"),f(Pe.$$.fragment),Nr=d(),bs=n("span"),Dr=a("mT5"),$n=d(),U=n("h2"),ne=n("a"),$s=n("span"),f(Ae.$$.fragment),Ir=d(),ws=n("span"),Gr=a("Overview"),wn=d(),re=n("p"),Or=a("The mT5 model was presented in "),Se=n("a"),Vr=a("mT5: A massively multilingual pre-trained text-to-text transformer"),Ur=a(` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),yn=d(),Wt=n("p"),Wr=a("The abstract from the paper is the following:"),zn=d(),Bt=n("p"),ys=n("em"),Br=a(`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),Mn=d(),oe=n("p"),Hr=a("Note: mT5 was only pre-trained on "),Le=n("a"),Rr=a("mC4"),Xr=a(` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),En=d(),Ht=n("p"),Kr=a("Google has released the following variants:"),xn=d(),$=n("ul"),zs=n("li"),Ms=n("p"),Ne=n("a"),Jr=a("google/mt5-small"),Qr=d(),Es=n("li"),xs=n("p"),De=n("a"),Yr=a("google/mt5-base"),Zr=d(),qs=n("li"),js=n("p"),Ie=n("a"),eo=a("google/mt5-large"),to=d(),Fs=n("li"),Cs=n("p"),Ge=n("a"),so=a("google/mt5-xl"),no=d(),Ps=n("li"),Rt=n("p"),Oe=n("a"),ro=a("google/mt5-xxl"),oo=a("."),qn=d(),D=n("p"),ao=a("This model was contributed by "),Ve=n("a"),io=a("patrickvonplaten"),lo=a(`. The original code can be
found `),Ue=n("a"),po=a("here"),co=a("."),jn=d(),W=n("h2"),ae=n("a"),As=n("span"),f(We.$$.fragment),mo=d(),Ss=n("span"),fo=a("MT5Config"),Fn=d(),S=n("div"),f(Be.$$.fragment),ho=d(),L=n("p"),uo=a("This is the configuration class to store the configuration of a "),Xt=n("a"),go=a("MT5Model"),_o=a(" or a "),Kt=n("a"),ko=a("TFMT5Model"),vo=a(`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),He=n("a"),To=a("google/mt5-small"),bo=a(" architecture."),$o=d(),B=n("p"),wo=a("Configuration objects inherit from "),Jt=n("a"),yo=a("PretrainedConfig"),zo=a(` and can be used to control the model outputs. Read the
documentation from `),Qt=n("a"),Mo=a("PretrainedConfig"),Eo=a(" for more information."),Cn=d(),H=n("h2"),ie=n("a"),Ls=n("span"),f(Re.$$.fragment),xo=d(),Ns=n("span"),qo=a("MT5Tokenizer"),Pn=d(),v=n("div"),f(Xe.$$.fragment),jo=d(),Ke=n("p"),Fo=a("Construct a T5 tokenizer. Based on "),Je=n("a"),Co=a("SentencePiece"),Po=a("."),Ao=d(),Qe=n("p"),So=a("This tokenizer inherits from "),Yt=n("a"),Lo=a("PreTrainedTokenizer"),No=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Do=d(),I=n("div"),f(Ye.$$.fragment),Io=d(),Ds=n("p"),Go=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Oo=d(),Ze=n("ul"),Zt=n("li"),Vo=a("single sequence: "),Is=n("code"),Uo=a("X </s>"),Wo=d(),es=n("li"),Bo=a("pair of sequences: "),Gs=n("code"),Ho=a("A </s> B </s>"),Ro=d(),le=n("div"),f(et.$$.fragment),Xo=d(),Os=n("p"),Ko=a("Converts a sequence of tokens (string) in a single string."),Jo=d(),de=n("div"),f(tt.$$.fragment),Qo=d(),Vs=n("p"),Yo=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Zo=d(),pe=n("div"),f(st.$$.fragment),ea=d(),nt=n("p"),ta=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Us=n("code"),sa=a("prepare_for_model"),na=a(" method."),An=d(),ce=n("p"),ra=a("See "),ts=n("a"),oa=a("T5Tokenizer"),aa=a(" for all details."),Sn=d(),R=n("h2"),me=n("a"),Ws=n("span"),f(rt.$$.fragment),ia=d(),Bs=n("span"),la=a("MT5TokenizerFast"),Ln=d(),b=n("div"),f(ot.$$.fragment),da=d(),X=n("p"),pa=a("Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),Hs=n("em"),ca=a("tokenizers"),ma=a(` library). Based on
`),at=n("a"),fa=a("Unigram"),ha=a("."),ua=d(),it=n("p"),ga=a("This tokenizer inherits from "),ss=n("a"),_a=a("PreTrainedTokenizerFast"),ka=a(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),va=d(),G=n("div"),f(lt.$$.fragment),Ta=d(),Rs=n("p"),ba=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),$a=d(),dt=n("ul"),ns=n("li"),wa=a("single sequence: "),Xs=n("code"),ya=a("X </s>"),za=d(),rs=n("li"),Ma=a("pair of sequences: "),Ks=n("code"),Ea=a("A </s> B </s>"),xa=d(),fe=n("div"),f(pt.$$.fragment),qa=d(),Js=n("p"),ja=a(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Nn=d(),he=n("p"),Fa=a("See "),os=n("a"),Ca=a("T5TokenizerFast"),Pa=a(" for all details."),Dn=d(),K=n("h2"),ue=n("a"),Qs=n("span"),f(ct.$$.fragment),Aa=d(),Ys=n("span"),Sa=a("MT5Model"),In=d(),y=n("div"),f(mt.$$.fragment),La=d(),ft=n("p"),Na=a("This class overrides "),as=n("a"),Da=a("T5Model"),Ia=a(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ga=d(),Zs=n("p"),Oa=a("Examples:"),Va=d(),f(ht.$$.fragment),Gn=d(),J=n("h2"),ge=n("a"),en=n("span"),f(ut.$$.fragment),Ua=d(),tn=n("span"),Wa=a("MT5ForConditionalGeneration"),On=d(),z=n("div"),f(gt.$$.fragment),Ba=d(),_t=n("p"),Ha=a("This class overrides "),is=n("a"),Ra=a("T5ForConditionalGeneration"),Xa=a(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Ka=d(),sn=n("p"),Ja=a("Examples:"),Qa=d(),f(kt.$$.fragment),Vn=d(),Q=n("h2"),_e=n("a"),nn=n("span"),f(vt.$$.fragment),Ya=d(),rn=n("span"),Za=a("MT5EncoderModel"),Un=d(),M=n("div"),f(Tt.$$.fragment),ei=d(),bt=n("p"),ti=a("This class overrides "),ls=n("a"),si=a("T5EncoderModel"),ni=a(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),ri=d(),on=n("p"),oi=a("Examples:"),ai=d(),f($t.$$.fragment),Wn=d(),Y=n("h2"),ke=n("a"),an=n("span"),f(wt.$$.fragment),ii=d(),ln=n("span"),li=a("TFMT5Model"),Bn=d(),E=n("div"),f(yt.$$.fragment),di=d(),zt=n("p"),pi=a("This class overrides "),ds=n("a"),ci=a("TFT5Model"),mi=a(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),fi=d(),dn=n("p"),hi=a("Examples:"),ui=d(),f(Mt.$$.fragment),Hn=d(),Z=n("h2"),ve=n("a"),pn=n("span"),f(Et.$$.fragment),gi=d(),cn=n("span"),_i=a("TFMT5ForConditionalGeneration"),Rn=d(),x=n("div"),f(xt.$$.fragment),ki=d(),qt=n("p"),vi=a("This class overrides "),ps=n("a"),Ti=a("TFT5ForConditionalGeneration"),bi=a(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),$i=d(),mn=n("p"),wi=a("Examples:"),yi=d(),f(jt.$$.fragment),Xn=d(),ee=n("h2"),Te=n("a"),fn=n("span"),f(Ft.$$.fragment),zi=d(),hn=n("span"),Mi=a("TFMT5EncoderModel"),Kn=d(),q=n("div"),f(Ct.$$.fragment),Ei=d(),Pt=n("p"),xi=a("This class overrides "),cs=n("a"),qi=a("TFT5EncoderModel"),ji=a(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Fi=d(),un=n("p"),Ci=a("Examples:"),Pi=d(),f(At.$$.fragment),Jn=d(),te=n("h2"),be=n("a"),gn=n("span"),f(St.$$.fragment),Ai=d(),_n=n("span"),Si=a("FlaxMT5Model"),Qn=d(),j=n("div"),f(Lt.$$.fragment),Li=d(),Nt=n("p"),Ni=a("This class overrides "),ms=n("a"),Di=a("FlaxT5Model"),Ii=a(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Gi=d(),kn=n("p"),Oi=a("Examples:"),Vi=d(),f(Dt.$$.fragment),Yn=d(),se=n("h2"),$e=n("a"),vn=n("span"),f(It.$$.fragment),Ui=d(),Tn=n("span"),Wi=a("FlaxMT5ForConditionalGeneration"),Zn=d(),F=n("div"),f(Gt.$$.fragment),Bi=d(),Ot=n("p"),Hi=a("This class overrides "),fs=n("a"),Ri=a("FlaxT5ForConditionalGeneration"),Xi=a(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Ki=d(),bn=n("p"),Ji=a("Examples:"),Qi=d(),f(Vt.$$.fragment),this.h()},l(t){const c=Qd('[data-svelte="svelte-1phssyn"]',document.head);N=r(c,"META",{name:!0,content:!0}),c.forEach(s),Ut=p(t),P=r(t,"H1",{class:!0});var tr=o(P);A=r(tr,"A",{id:!0,class:!0,href:!0});var nl=o(A);Ts=r(nl,"SPAN",{});var rl=o(Ts);h(Pe.$$.fragment,rl),rl.forEach(s),nl.forEach(s),Nr=p(tr),bs=r(tr,"SPAN",{});var ol=o(bs);Dr=i(ol,"mT5"),ol.forEach(s),tr.forEach(s),$n=p(t),U=r(t,"H2",{class:!0});var sr=o(U);ne=r(sr,"A",{id:!0,class:!0,href:!0});var al=o(ne);$s=r(al,"SPAN",{});var il=o($s);h(Ae.$$.fragment,il),il.forEach(s),al.forEach(s),Ir=p(sr),ws=r(sr,"SPAN",{});var ll=o(ws);Gr=i(ll,"Overview"),ll.forEach(s),sr.forEach(s),wn=p(t),re=r(t,"P",{});var nr=o(re);Or=i(nr,"The mT5 model was presented in "),Se=r(nr,"A",{href:!0,rel:!0});var dl=o(Se);Vr=i(dl,"mT5: A massively multilingual pre-trained text-to-text transformer"),dl.forEach(s),Ur=i(nr,` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),nr.forEach(s),yn=p(t),Wt=r(t,"P",{});var pl=o(Wt);Wr=i(pl,"The abstract from the paper is the following:"),pl.forEach(s),zn=p(t),Bt=r(t,"P",{});var cl=o(Bt);ys=r(cl,"EM",{});var ml=o(ys);Br=i(ml,`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),ml.forEach(s),cl.forEach(s),Mn=p(t),oe=r(t,"P",{});var rr=o(oe);Hr=i(rr,"Note: mT5 was only pre-trained on "),Le=r(rr,"A",{href:!0,rel:!0});var fl=o(Le);Rr=i(fl,"mC4"),fl.forEach(s),Xr=i(rr,` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),rr.forEach(s),En=p(t),Ht=r(t,"P",{});var hl=o(Ht);Kr=i(hl,"Google has released the following variants:"),hl.forEach(s),xn=p(t),$=r(t,"UL",{});var O=o($);zs=r(O,"LI",{});var ul=o(zs);Ms=r(ul,"P",{});var gl=o(Ms);Ne=r(gl,"A",{href:!0,rel:!0});var _l=o(Ne);Jr=i(_l,"google/mt5-small"),_l.forEach(s),gl.forEach(s),ul.forEach(s),Qr=p(O),Es=r(O,"LI",{});var kl=o(Es);xs=r(kl,"P",{});var vl=o(xs);De=r(vl,"A",{href:!0,rel:!0});var Tl=o(De);Yr=i(Tl,"google/mt5-base"),Tl.forEach(s),vl.forEach(s),kl.forEach(s),Zr=p(O),qs=r(O,"LI",{});var bl=o(qs);js=r(bl,"P",{});var $l=o(js);Ie=r($l,"A",{href:!0,rel:!0});var wl=o(Ie);eo=i(wl,"google/mt5-large"),wl.forEach(s),$l.forEach(s),bl.forEach(s),to=p(O),Fs=r(O,"LI",{});var yl=o(Fs);Cs=r(yl,"P",{});var zl=o(Cs);Ge=r(zl,"A",{href:!0,rel:!0});var Ml=o(Ge);so=i(Ml,"google/mt5-xl"),Ml.forEach(s),zl.forEach(s),yl.forEach(s),no=p(O),Ps=r(O,"LI",{});var El=o(Ps);Rt=r(El,"P",{});var Yi=o(Rt);Oe=r(Yi,"A",{href:!0,rel:!0});var xl=o(Oe);ro=i(xl,"google/mt5-xxl"),xl.forEach(s),oo=i(Yi,"."),Yi.forEach(s),El.forEach(s),O.forEach(s),qn=p(t),D=r(t,"P",{});var hs=o(D);ao=i(hs,"This model was contributed by "),Ve=r(hs,"A",{href:!0,rel:!0});var ql=o(Ve);io=i(ql,"patrickvonplaten"),ql.forEach(s),lo=i(hs,`. The original code can be
found `),Ue=r(hs,"A",{href:!0,rel:!0});var jl=o(Ue);po=i(jl,"here"),jl.forEach(s),co=i(hs,"."),hs.forEach(s),jn=p(t),W=r(t,"H2",{class:!0});var or=o(W);ae=r(or,"A",{id:!0,class:!0,href:!0});var Fl=o(ae);As=r(Fl,"SPAN",{});var Cl=o(As);h(We.$$.fragment,Cl),Cl.forEach(s),Fl.forEach(s),mo=p(or),Ss=r(or,"SPAN",{});var Pl=o(Ss);fo=i(Pl,"MT5Config"),Pl.forEach(s),or.forEach(s),Fn=p(t),S=r(t,"DIV",{class:!0});var us=o(S);h(Be.$$.fragment,us),ho=p(us),L=r(us,"P",{});var we=o(L);uo=i(we,"This is the configuration class to store the configuration of a "),Xt=r(we,"A",{href:!0});var Al=o(Xt);go=i(Al,"MT5Model"),Al.forEach(s),_o=i(we," or a "),Kt=r(we,"A",{href:!0});var Sl=o(Kt);ko=i(Sl,"TFMT5Model"),Sl.forEach(s),vo=i(we,`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),He=r(we,"A",{href:!0,rel:!0});var Ll=o(He);To=i(Ll,"google/mt5-small"),Ll.forEach(s),bo=i(we," architecture."),we.forEach(s),$o=p(us),B=r(us,"P",{});var gs=o(B);wo=i(gs,"Configuration objects inherit from "),Jt=r(gs,"A",{href:!0});var Nl=o(Jt);yo=i(Nl,"PretrainedConfig"),Nl.forEach(s),zo=i(gs,` and can be used to control the model outputs. Read the
documentation from `),Qt=r(gs,"A",{href:!0});var Dl=o(Qt);Mo=i(Dl,"PretrainedConfig"),Dl.forEach(s),Eo=i(gs," for more information."),gs.forEach(s),us.forEach(s),Cn=p(t),H=r(t,"H2",{class:!0});var ar=o(H);ie=r(ar,"A",{id:!0,class:!0,href:!0});var Il=o(ie);Ls=r(Il,"SPAN",{});var Gl=o(Ls);h(Re.$$.fragment,Gl),Gl.forEach(s),Il.forEach(s),xo=p(ar),Ns=r(ar,"SPAN",{});var Ol=o(Ns);qo=i(Ol,"MT5Tokenizer"),Ol.forEach(s),ar.forEach(s),Pn=p(t),v=r(t,"DIV",{class:!0});var w=o(v);h(Xe.$$.fragment,w),jo=p(w),Ke=r(w,"P",{});var ir=o(Ke);Fo=i(ir,"Construct a T5 tokenizer. Based on "),Je=r(ir,"A",{href:!0,rel:!0});var Vl=o(Je);Co=i(Vl,"SentencePiece"),Vl.forEach(s),Po=i(ir,"."),ir.forEach(s),Ao=p(w),Qe=r(w,"P",{});var lr=o(Qe);So=i(lr,"This tokenizer inherits from "),Yt=r(lr,"A",{href:!0});var Ul=o(Yt);Lo=i(Ul,"PreTrainedTokenizer"),Ul.forEach(s),No=i(lr,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),lr.forEach(s),Do=p(w),I=r(w,"DIV",{class:!0});var _s=o(I);h(Ye.$$.fragment,_s),Io=p(_s),Ds=r(_s,"P",{});var Wl=o(Ds);Go=i(Wl,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Wl.forEach(s),Oo=p(_s),Ze=r(_s,"UL",{});var dr=o(Ze);Zt=r(dr,"LI",{});var Zi=o(Zt);Vo=i(Zi,"single sequence: "),Is=r(Zi,"CODE",{});var Bl=o(Is);Uo=i(Bl,"X </s>"),Bl.forEach(s),Zi.forEach(s),Wo=p(dr),es=r(dr,"LI",{});var el=o(es);Bo=i(el,"pair of sequences: "),Gs=r(el,"CODE",{});var Hl=o(Gs);Ho=i(Hl,"A </s> B </s>"),Hl.forEach(s),el.forEach(s),dr.forEach(s),_s.forEach(s),Ro=p(w),le=r(w,"DIV",{class:!0});var pr=o(le);h(et.$$.fragment,pr),Xo=p(pr),Os=r(pr,"P",{});var Rl=o(Os);Ko=i(Rl,"Converts a sequence of tokens (string) in a single string."),Rl.forEach(s),pr.forEach(s),Jo=p(w),de=r(w,"DIV",{class:!0});var cr=o(de);h(tt.$$.fragment,cr),Qo=p(cr),Vs=r(cr,"P",{});var Xl=o(Vs);Yo=i(Xl,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Xl.forEach(s),cr.forEach(s),Zo=p(w),pe=r(w,"DIV",{class:!0});var mr=o(pe);h(st.$$.fragment,mr),ea=p(mr),nt=r(mr,"P",{});var fr=o(nt);ta=i(fr,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Us=r(fr,"CODE",{});var Kl=o(Us);sa=i(Kl,"prepare_for_model"),Kl.forEach(s),na=i(fr," method."),fr.forEach(s),mr.forEach(s),w.forEach(s),An=p(t),ce=r(t,"P",{});var hr=o(ce);ra=i(hr,"See "),ts=r(hr,"A",{href:!0});var Jl=o(ts);oa=i(Jl,"T5Tokenizer"),Jl.forEach(s),aa=i(hr," for all details."),hr.forEach(s),Sn=p(t),R=r(t,"H2",{class:!0});var ur=o(R);me=r(ur,"A",{id:!0,class:!0,href:!0});var Ql=o(me);Ws=r(Ql,"SPAN",{});var Yl=o(Ws);h(rt.$$.fragment,Yl),Yl.forEach(s),Ql.forEach(s),ia=p(ur),Bs=r(ur,"SPAN",{});var Zl=o(Bs);la=i(Zl,"MT5TokenizerFast"),Zl.forEach(s),ur.forEach(s),Ln=p(t),b=r(t,"DIV",{class:!0});var V=o(b);h(ot.$$.fragment,V),da=p(V),X=r(V,"P",{});var ks=o(X);pa=i(ks,"Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),Hs=r(ks,"EM",{});var ed=o(Hs);ca=i(ed,"tokenizers"),ed.forEach(s),ma=i(ks,` library). Based on
`),at=r(ks,"A",{href:!0,rel:!0});var td=o(at);fa=i(td,"Unigram"),td.forEach(s),ha=i(ks,"."),ks.forEach(s),ua=p(V),it=r(V,"P",{});var gr=o(it);ga=i(gr,"This tokenizer inherits from "),ss=r(gr,"A",{href:!0});var sd=o(ss);_a=i(sd,"PreTrainedTokenizerFast"),sd.forEach(s),ka=i(gr,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),gr.forEach(s),va=p(V),G=r(V,"DIV",{class:!0});var vs=o(G);h(lt.$$.fragment,vs),Ta=p(vs),Rs=r(vs,"P",{});var nd=o(Rs);ba=i(nd,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),nd.forEach(s),$a=p(vs),dt=r(vs,"UL",{});var _r=o(dt);ns=r(_r,"LI",{});var tl=o(ns);wa=i(tl,"single sequence: "),Xs=r(tl,"CODE",{});var rd=o(Xs);ya=i(rd,"X </s>"),rd.forEach(s),tl.forEach(s),za=p(_r),rs=r(_r,"LI",{});var sl=o(rs);Ma=i(sl,"pair of sequences: "),Ks=r(sl,"CODE",{});var od=o(Ks);Ea=i(od,"A </s> B </s>"),od.forEach(s),sl.forEach(s),_r.forEach(s),vs.forEach(s),xa=p(V),fe=r(V,"DIV",{class:!0});var kr=o(fe);h(pt.$$.fragment,kr),qa=p(kr),Js=r(kr,"P",{});var ad=o(Js);ja=i(ad,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),ad.forEach(s),kr.forEach(s),V.forEach(s),Nn=p(t),he=r(t,"P",{});var vr=o(he);Fa=i(vr,"See "),os=r(vr,"A",{href:!0});var id=o(os);Ca=i(id,"T5TokenizerFast"),id.forEach(s),Pa=i(vr," for all details."),vr.forEach(s),Dn=p(t),K=r(t,"H2",{class:!0});var Tr=o(K);ue=r(Tr,"A",{id:!0,class:!0,href:!0});var ld=o(ue);Qs=r(ld,"SPAN",{});var dd=o(Qs);h(ct.$$.fragment,dd),dd.forEach(s),ld.forEach(s),Aa=p(Tr),Ys=r(Tr,"SPAN",{});var pd=o(Ys);Sa=i(pd,"MT5Model"),pd.forEach(s),Tr.forEach(s),In=p(t),y=r(t,"DIV",{class:!0});var ye=o(y);h(mt.$$.fragment,ye),La=p(ye),ft=r(ye,"P",{});var br=o(ft);Na=i(br,"This class overrides "),as=r(br,"A",{href:!0});var cd=o(as);Da=i(cd,"T5Model"),cd.forEach(s),Ia=i(br,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),br.forEach(s),Ga=p(ye),Zs=r(ye,"P",{});var md=o(Zs);Oa=i(md,"Examples:"),md.forEach(s),Va=p(ye),h(ht.$$.fragment,ye),ye.forEach(s),Gn=p(t),J=r(t,"H2",{class:!0});var $r=o(J);ge=r($r,"A",{id:!0,class:!0,href:!0});var fd=o(ge);en=r(fd,"SPAN",{});var hd=o(en);h(ut.$$.fragment,hd),hd.forEach(s),fd.forEach(s),Ua=p($r),tn=r($r,"SPAN",{});var ud=o(tn);Wa=i(ud,"MT5ForConditionalGeneration"),ud.forEach(s),$r.forEach(s),On=p(t),z=r(t,"DIV",{class:!0});var ze=o(z);h(gt.$$.fragment,ze),Ba=p(ze),_t=r(ze,"P",{});var wr=o(_t);Ha=i(wr,"This class overrides "),is=r(wr,"A",{href:!0});var gd=o(is);Ra=i(gd,"T5ForConditionalGeneration"),gd.forEach(s),Xa=i(wr,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),wr.forEach(s),Ka=p(ze),sn=r(ze,"P",{});var _d=o(sn);Ja=i(_d,"Examples:"),_d.forEach(s),Qa=p(ze),h(kt.$$.fragment,ze),ze.forEach(s),Vn=p(t),Q=r(t,"H2",{class:!0});var yr=o(Q);_e=r(yr,"A",{id:!0,class:!0,href:!0});var kd=o(_e);nn=r(kd,"SPAN",{});var vd=o(nn);h(vt.$$.fragment,vd),vd.forEach(s),kd.forEach(s),Ya=p(yr),rn=r(yr,"SPAN",{});var Td=o(rn);Za=i(Td,"MT5EncoderModel"),Td.forEach(s),yr.forEach(s),Un=p(t),M=r(t,"DIV",{class:!0});var Me=o(M);h(Tt.$$.fragment,Me),ei=p(Me),bt=r(Me,"P",{});var zr=o(bt);ti=i(zr,"This class overrides "),ls=r(zr,"A",{href:!0});var bd=o(ls);si=i(bd,"T5EncoderModel"),bd.forEach(s),ni=i(zr,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),zr.forEach(s),ri=p(Me),on=r(Me,"P",{});var $d=o(on);oi=i($d,"Examples:"),$d.forEach(s),ai=p(Me),h($t.$$.fragment,Me),Me.forEach(s),Wn=p(t),Y=r(t,"H2",{class:!0});var Mr=o(Y);ke=r(Mr,"A",{id:!0,class:!0,href:!0});var wd=o(ke);an=r(wd,"SPAN",{});var yd=o(an);h(wt.$$.fragment,yd),yd.forEach(s),wd.forEach(s),ii=p(Mr),ln=r(Mr,"SPAN",{});var zd=o(ln);li=i(zd,"TFMT5Model"),zd.forEach(s),Mr.forEach(s),Bn=p(t),E=r(t,"DIV",{class:!0});var Ee=o(E);h(yt.$$.fragment,Ee),di=p(Ee),zt=r(Ee,"P",{});var Er=o(zt);pi=i(Er,"This class overrides "),ds=r(Er,"A",{href:!0});var Md=o(ds);ci=i(Md,"TFT5Model"),Md.forEach(s),mi=i(Er,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Er.forEach(s),fi=p(Ee),dn=r(Ee,"P",{});var Ed=o(dn);hi=i(Ed,"Examples:"),Ed.forEach(s),ui=p(Ee),h(Mt.$$.fragment,Ee),Ee.forEach(s),Hn=p(t),Z=r(t,"H2",{class:!0});var xr=o(Z);ve=r(xr,"A",{id:!0,class:!0,href:!0});var xd=o(ve);pn=r(xd,"SPAN",{});var qd=o(pn);h(Et.$$.fragment,qd),qd.forEach(s),xd.forEach(s),gi=p(xr),cn=r(xr,"SPAN",{});var jd=o(cn);_i=i(jd,"TFMT5ForConditionalGeneration"),jd.forEach(s),xr.forEach(s),Rn=p(t),x=r(t,"DIV",{class:!0});var xe=o(x);h(xt.$$.fragment,xe),ki=p(xe),qt=r(xe,"P",{});var qr=o(qt);vi=i(qr,"This class overrides "),ps=r(qr,"A",{href:!0});var Fd=o(ps);Ti=i(Fd,"TFT5ForConditionalGeneration"),Fd.forEach(s),bi=i(qr,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),qr.forEach(s),$i=p(xe),mn=r(xe,"P",{});var Cd=o(mn);wi=i(Cd,"Examples:"),Cd.forEach(s),yi=p(xe),h(jt.$$.fragment,xe),xe.forEach(s),Xn=p(t),ee=r(t,"H2",{class:!0});var jr=o(ee);Te=r(jr,"A",{id:!0,class:!0,href:!0});var Pd=o(Te);fn=r(Pd,"SPAN",{});var Ad=o(fn);h(Ft.$$.fragment,Ad),Ad.forEach(s),Pd.forEach(s),zi=p(jr),hn=r(jr,"SPAN",{});var Sd=o(hn);Mi=i(Sd,"TFMT5EncoderModel"),Sd.forEach(s),jr.forEach(s),Kn=p(t),q=r(t,"DIV",{class:!0});var qe=o(q);h(Ct.$$.fragment,qe),Ei=p(qe),Pt=r(qe,"P",{});var Fr=o(Pt);xi=i(Fr,"This class overrides "),cs=r(Fr,"A",{href:!0});var Ld=o(cs);qi=i(Ld,"TFT5EncoderModel"),Ld.forEach(s),ji=i(Fr,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Fr.forEach(s),Fi=p(qe),un=r(qe,"P",{});var Nd=o(un);Ci=i(Nd,"Examples:"),Nd.forEach(s),Pi=p(qe),h(At.$$.fragment,qe),qe.forEach(s),Jn=p(t),te=r(t,"H2",{class:!0});var Cr=o(te);be=r(Cr,"A",{id:!0,class:!0,href:!0});var Dd=o(be);gn=r(Dd,"SPAN",{});var Id=o(gn);h(St.$$.fragment,Id),Id.forEach(s),Dd.forEach(s),Ai=p(Cr),_n=r(Cr,"SPAN",{});var Gd=o(_n);Si=i(Gd,"FlaxMT5Model"),Gd.forEach(s),Cr.forEach(s),Qn=p(t),j=r(t,"DIV",{class:!0});var je=o(j);h(Lt.$$.fragment,je),Li=p(je),Nt=r(je,"P",{});var Pr=o(Nt);Ni=i(Pr,"This class overrides "),ms=r(Pr,"A",{href:!0});var Od=o(ms);Di=i(Od,"FlaxT5Model"),Od.forEach(s),Ii=i(Pr,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Pr.forEach(s),Gi=p(je),kn=r(je,"P",{});var Vd=o(kn);Oi=i(Vd,"Examples:"),Vd.forEach(s),Vi=p(je),h(Dt.$$.fragment,je),je.forEach(s),Yn=p(t),se=r(t,"H2",{class:!0});var Ar=o(se);$e=r(Ar,"A",{id:!0,class:!0,href:!0});var Ud=o($e);vn=r(Ud,"SPAN",{});var Wd=o(vn);h(It.$$.fragment,Wd),Wd.forEach(s),Ud.forEach(s),Ui=p(Ar),Tn=r(Ar,"SPAN",{});var Bd=o(Tn);Wi=i(Bd,"FlaxMT5ForConditionalGeneration"),Bd.forEach(s),Ar.forEach(s),Zn=p(t),F=r(t,"DIV",{class:!0});var Fe=o(F);h(Gt.$$.fragment,Fe),Bi=p(Fe),Ot=r(Fe,"P",{});var Sr=o(Ot);Hi=i(Sr,"This class overrides "),fs=r(Sr,"A",{href:!0});var Hd=o(fs);Ri=i(Hd,"FlaxT5ForConditionalGeneration"),Hd.forEach(s),Xi=i(Sr,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Sr.forEach(s),Ki=p(Fe),bn=r(Fe,"P",{});var Rd=o(bn);Ji=i(Rd,"Examples:"),Rd.forEach(s),Qi=p(Fe),h(Vt.$$.fragment,Fe),Fe.forEach(s),this.h()},h(){l(N,"name","hf:doc:metadata"),l(N,"content",JSON.stringify(ep)),l(A,"id","mt5"),l(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(A,"href","#mt5"),l(P,"class","relative group"),l(ne,"id","overview"),l(ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ne,"href","#overview"),l(U,"class","relative group"),l(Se,"href","https://arxiv.org/abs/2010.11934"),l(Se,"rel","nofollow"),l(Le,"href","https://huggingface.co/datasets/mc4"),l(Le,"rel","nofollow"),l(Ne,"href","https://huggingface.co/google/mt5-small"),l(Ne,"rel","nofollow"),l(De,"href","https://huggingface.co/google/mt5-base"),l(De,"rel","nofollow"),l(Ie,"href","https://huggingface.co/google/mt5-large"),l(Ie,"rel","nofollow"),l(Ge,"href","https://huggingface.co/google/mt5-xl"),l(Ge,"rel","nofollow"),l(Oe,"href","https://huggingface.co/google/mt5-xxl"),l(Oe,"rel","nofollow"),l(Ve,"href","https://huggingface.co/patrickvonplaten"),l(Ve,"rel","nofollow"),l(Ue,"href","https://github.com/google-research/multilingual-t5"),l(Ue,"rel","nofollow"),l(ae,"id","transformers.MT5Config"),l(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ae,"href","#transformers.MT5Config"),l(W,"class","relative group"),l(Xt,"href","/docs/transformers/doc-build-test/en/model_doc/mt5#transformers.MT5Model"),l(Kt,"href","/docs/transformers/doc-build-test/en/model_doc/mt5#transformers.TFMT5Model"),l(He,"href","https://huggingface.co/google/mt5-small"),l(He,"rel","nofollow"),l(Jt,"href","/docs/transformers/doc-build-test/en/main_classes/configuration#transformers.PretrainedConfig"),l(Qt,"href","/docs/transformers/doc-build-test/en/main_classes/configuration#transformers.PretrainedConfig"),l(S,"class","docstring"),l(ie,"id","transformers.T5Tokenizer"),l(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ie,"href","#transformers.T5Tokenizer"),l(H,"class","relative group"),l(Je,"href","https://github.com/google/sentencepiece"),l(Je,"rel","nofollow"),l(Yt,"href","/docs/transformers/doc-build-test/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),l(I,"class","docstring"),l(le,"class","docstring"),l(de,"class","docstring"),l(pe,"class","docstring"),l(v,"class","docstring"),l(ts,"href","/docs/transformers/doc-build-test/en/model_doc/mt5#transformers.T5Tokenizer"),l(me,"id","transformers.T5TokenizerFast"),l(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(me,"href","#transformers.T5TokenizerFast"),l(R,"class","relative group"),l(at,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models"),l(at,"rel","nofollow"),l(ss,"href","/docs/transformers/doc-build-test/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),l(G,"class","docstring"),l(fe,"class","docstring"),l(b,"class","docstring"),l(os,"href","/docs/transformers/doc-build-test/en/model_doc/mt5#transformers.T5TokenizerFast"),l(ue,"id","transformers.MT5Model"),l(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ue,"href","#transformers.MT5Model"),l(K,"class","relative group"),l(as,"href","/docs/transformers/doc-build-test/en/model_doc/t5#transformers.T5Model"),l(y,"class","docstring"),l(ge,"id","transformers.MT5ForConditionalGeneration"),l(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ge,"href","#transformers.MT5ForConditionalGeneration"),l(J,"class","relative group"),l(is,"href","/docs/transformers/doc-build-test/en/model_doc/t5#transformers.T5ForConditionalGeneration"),l(z,"class","docstring"),l(_e,"id","transformers.MT5EncoderModel"),l(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_e,"href","#transformers.MT5EncoderModel"),l(Q,"class","relative group"),l(ls,"href","/docs/transformers/doc-build-test/en/model_doc/t5#transformers.T5EncoderModel"),l(M,"class","docstring"),l(ke,"id","transformers.TFMT5Model"),l(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ke,"href","#transformers.TFMT5Model"),l(Y,"class","relative group"),l(ds,"href","/docs/transformers/doc-build-test/en/model_doc/t5#transformers.TFT5Model"),l(E,"class","docstring"),l(ve,"id","transformers.TFMT5ForConditionalGeneration"),l(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ve,"href","#transformers.TFMT5ForConditionalGeneration"),l(Z,"class","relative group"),l(ps,"href","/docs/transformers/doc-build-test/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),l(x,"class","docstring"),l(Te,"id","transformers.TFMT5EncoderModel"),l(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Te,"href","#transformers.TFMT5EncoderModel"),l(ee,"class","relative group"),l(cs,"href","/docs/transformers/doc-build-test/en/model_doc/t5#transformers.TFT5EncoderModel"),l(q,"class","docstring"),l(be,"id","transformers.FlaxMT5Model"),l(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(be,"href","#transformers.FlaxMT5Model"),l(te,"class","relative group"),l(ms,"href","/docs/transformers/doc-build-test/en/model_doc/t5#transformers.FlaxT5Model"),l(j,"class","docstring"),l($e,"id","transformers.FlaxMT5ForConditionalGeneration"),l($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($e,"href","#transformers.FlaxMT5ForConditionalGeneration"),l(se,"class","relative group"),l(fs,"href","/docs/transformers/doc-build-test/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),l(F,"class","docstring")},m(t,c){e(document.head,N),m(t,Ut,c),m(t,P,c),e(P,A),e(A,Ts),u(Pe,Ts,null),e(P,Nr),e(P,bs),e(bs,Dr),m(t,$n,c),m(t,U,c),e(U,ne),e(ne,$s),u(Ae,$s,null),e(U,Ir),e(U,ws),e(ws,Gr),m(t,wn,c),m(t,re,c),e(re,Or),e(re,Se),e(Se,Vr),e(re,Ur),m(t,yn,c),m(t,Wt,c),e(Wt,Wr),m(t,zn,c),m(t,Bt,c),e(Bt,ys),e(ys,Br),m(t,Mn,c),m(t,oe,c),e(oe,Hr),e(oe,Le),e(Le,Rr),e(oe,Xr),m(t,En,c),m(t,Ht,c),e(Ht,Kr),m(t,xn,c),m(t,$,c),e($,zs),e(zs,Ms),e(Ms,Ne),e(Ne,Jr),e($,Qr),e($,Es),e(Es,xs),e(xs,De),e(De,Yr),e($,Zr),e($,qs),e(qs,js),e(js,Ie),e(Ie,eo),e($,to),e($,Fs),e(Fs,Cs),e(Cs,Ge),e(Ge,so),e($,no),e($,Ps),e(Ps,Rt),e(Rt,Oe),e(Oe,ro),e(Rt,oo),m(t,qn,c),m(t,D,c),e(D,ao),e(D,Ve),e(Ve,io),e(D,lo),e(D,Ue),e(Ue,po),e(D,co),m(t,jn,c),m(t,W,c),e(W,ae),e(ae,As),u(We,As,null),e(W,mo),e(W,Ss),e(Ss,fo),m(t,Fn,c),m(t,S,c),u(Be,S,null),e(S,ho),e(S,L),e(L,uo),e(L,Xt),e(Xt,go),e(L,_o),e(L,Kt),e(Kt,ko),e(L,vo),e(L,He),e(He,To),e(L,bo),e(S,$o),e(S,B),e(B,wo),e(B,Jt),e(Jt,yo),e(B,zo),e(B,Qt),e(Qt,Mo),e(B,Eo),m(t,Cn,c),m(t,H,c),e(H,ie),e(ie,Ls),u(Re,Ls,null),e(H,xo),e(H,Ns),e(Ns,qo),m(t,Pn,c),m(t,v,c),u(Xe,v,null),e(v,jo),e(v,Ke),e(Ke,Fo),e(Ke,Je),e(Je,Co),e(Ke,Po),e(v,Ao),e(v,Qe),e(Qe,So),e(Qe,Yt),e(Yt,Lo),e(Qe,No),e(v,Do),e(v,I),u(Ye,I,null),e(I,Io),e(I,Ds),e(Ds,Go),e(I,Oo),e(I,Ze),e(Ze,Zt),e(Zt,Vo),e(Zt,Is),e(Is,Uo),e(Ze,Wo),e(Ze,es),e(es,Bo),e(es,Gs),e(Gs,Ho),e(v,Ro),e(v,le),u(et,le,null),e(le,Xo),e(le,Os),e(Os,Ko),e(v,Jo),e(v,de),u(tt,de,null),e(de,Qo),e(de,Vs),e(Vs,Yo),e(v,Zo),e(v,pe),u(st,pe,null),e(pe,ea),e(pe,nt),e(nt,ta),e(nt,Us),e(Us,sa),e(nt,na),m(t,An,c),m(t,ce,c),e(ce,ra),e(ce,ts),e(ts,oa),e(ce,aa),m(t,Sn,c),m(t,R,c),e(R,me),e(me,Ws),u(rt,Ws,null),e(R,ia),e(R,Bs),e(Bs,la),m(t,Ln,c),m(t,b,c),u(ot,b,null),e(b,da),e(b,X),e(X,pa),e(X,Hs),e(Hs,ca),e(X,ma),e(X,at),e(at,fa),e(X,ha),e(b,ua),e(b,it),e(it,ga),e(it,ss),e(ss,_a),e(it,ka),e(b,va),e(b,G),u(lt,G,null),e(G,Ta),e(G,Rs),e(Rs,ba),e(G,$a),e(G,dt),e(dt,ns),e(ns,wa),e(ns,Xs),e(Xs,ya),e(dt,za),e(dt,rs),e(rs,Ma),e(rs,Ks),e(Ks,Ea),e(b,xa),e(b,fe),u(pt,fe,null),e(fe,qa),e(fe,Js),e(Js,ja),m(t,Nn,c),m(t,he,c),e(he,Fa),e(he,os),e(os,Ca),e(he,Pa),m(t,Dn,c),m(t,K,c),e(K,ue),e(ue,Qs),u(ct,Qs,null),e(K,Aa),e(K,Ys),e(Ys,Sa),m(t,In,c),m(t,y,c),u(mt,y,null),e(y,La),e(y,ft),e(ft,Na),e(ft,as),e(as,Da),e(ft,Ia),e(y,Ga),e(y,Zs),e(Zs,Oa),e(y,Va),u(ht,y,null),m(t,Gn,c),m(t,J,c),e(J,ge),e(ge,en),u(ut,en,null),e(J,Ua),e(J,tn),e(tn,Wa),m(t,On,c),m(t,z,c),u(gt,z,null),e(z,Ba),e(z,_t),e(_t,Ha),e(_t,is),e(is,Ra),e(_t,Xa),e(z,Ka),e(z,sn),e(sn,Ja),e(z,Qa),u(kt,z,null),m(t,Vn,c),m(t,Q,c),e(Q,_e),e(_e,nn),u(vt,nn,null),e(Q,Ya),e(Q,rn),e(rn,Za),m(t,Un,c),m(t,M,c),u(Tt,M,null),e(M,ei),e(M,bt),e(bt,ti),e(bt,ls),e(ls,si),e(bt,ni),e(M,ri),e(M,on),e(on,oi),e(M,ai),u($t,M,null),m(t,Wn,c),m(t,Y,c),e(Y,ke),e(ke,an),u(wt,an,null),e(Y,ii),e(Y,ln),e(ln,li),m(t,Bn,c),m(t,E,c),u(yt,E,null),e(E,di),e(E,zt),e(zt,pi),e(zt,ds),e(ds,ci),e(zt,mi),e(E,fi),e(E,dn),e(dn,hi),e(E,ui),u(Mt,E,null),m(t,Hn,c),m(t,Z,c),e(Z,ve),e(ve,pn),u(Et,pn,null),e(Z,gi),e(Z,cn),e(cn,_i),m(t,Rn,c),m(t,x,c),u(xt,x,null),e(x,ki),e(x,qt),e(qt,vi),e(qt,ps),e(ps,Ti),e(qt,bi),e(x,$i),e(x,mn),e(mn,wi),e(x,yi),u(jt,x,null),m(t,Xn,c),m(t,ee,c),e(ee,Te),e(Te,fn),u(Ft,fn,null),e(ee,zi),e(ee,hn),e(hn,Mi),m(t,Kn,c),m(t,q,c),u(Ct,q,null),e(q,Ei),e(q,Pt),e(Pt,xi),e(Pt,cs),e(cs,qi),e(Pt,ji),e(q,Fi),e(q,un),e(un,Ci),e(q,Pi),u(At,q,null),m(t,Jn,c),m(t,te,c),e(te,be),e(be,gn),u(St,gn,null),e(te,Ai),e(te,_n),e(_n,Si),m(t,Qn,c),m(t,j,c),u(Lt,j,null),e(j,Li),e(j,Nt),e(Nt,Ni),e(Nt,ms),e(ms,Di),e(Nt,Ii),e(j,Gi),e(j,kn),e(kn,Oi),e(j,Vi),u(Dt,j,null),m(t,Yn,c),m(t,se,c),e(se,$e),e($e,vn),u(It,vn,null),e(se,Ui),e(se,Tn),e(Tn,Wi),m(t,Zn,c),m(t,F,c),u(Gt,F,null),e(F,Bi),e(F,Ot),e(Ot,Hi),e(Ot,fs),e(fs,Ri),e(Ot,Xi),e(F,Ki),e(F,bn),e(bn,Ji),e(F,Qi),u(Vt,F,null),er=!0},p:Yd,i(t){er||(g(Pe.$$.fragment,t),g(Ae.$$.fragment,t),g(We.$$.fragment,t),g(Be.$$.fragment,t),g(Re.$$.fragment,t),g(Xe.$$.fragment,t),g(Ye.$$.fragment,t),g(et.$$.fragment,t),g(tt.$$.fragment,t),g(st.$$.fragment,t),g(rt.$$.fragment,t),g(ot.$$.fragment,t),g(lt.$$.fragment,t),g(pt.$$.fragment,t),g(ct.$$.fragment,t),g(mt.$$.fragment,t),g(ht.$$.fragment,t),g(ut.$$.fragment,t),g(gt.$$.fragment,t),g(kt.$$.fragment,t),g(vt.$$.fragment,t),g(Tt.$$.fragment,t),g($t.$$.fragment,t),g(wt.$$.fragment,t),g(yt.$$.fragment,t),g(Mt.$$.fragment,t),g(Et.$$.fragment,t),g(xt.$$.fragment,t),g(jt.$$.fragment,t),g(Ft.$$.fragment,t),g(Ct.$$.fragment,t),g(At.$$.fragment,t),g(St.$$.fragment,t),g(Lt.$$.fragment,t),g(Dt.$$.fragment,t),g(It.$$.fragment,t),g(Gt.$$.fragment,t),g(Vt.$$.fragment,t),er=!0)},o(t){_(Pe.$$.fragment,t),_(Ae.$$.fragment,t),_(We.$$.fragment,t),_(Be.$$.fragment,t),_(Re.$$.fragment,t),_(Xe.$$.fragment,t),_(Ye.$$.fragment,t),_(et.$$.fragment,t),_(tt.$$.fragment,t),_(st.$$.fragment,t),_(rt.$$.fragment,t),_(ot.$$.fragment,t),_(lt.$$.fragment,t),_(pt.$$.fragment,t),_(ct.$$.fragment,t),_(mt.$$.fragment,t),_(ht.$$.fragment,t),_(ut.$$.fragment,t),_(gt.$$.fragment,t),_(kt.$$.fragment,t),_(vt.$$.fragment,t),_(Tt.$$.fragment,t),_($t.$$.fragment,t),_(wt.$$.fragment,t),_(yt.$$.fragment,t),_(Mt.$$.fragment,t),_(Et.$$.fragment,t),_(xt.$$.fragment,t),_(jt.$$.fragment,t),_(Ft.$$.fragment,t),_(Ct.$$.fragment,t),_(At.$$.fragment,t),_(St.$$.fragment,t),_(Lt.$$.fragment,t),_(Dt.$$.fragment,t),_(It.$$.fragment,t),_(Gt.$$.fragment,t),_(Vt.$$.fragment,t),er=!1},d(t){s(N),t&&s(Ut),t&&s(P),k(Pe),t&&s($n),t&&s(U),k(Ae),t&&s(wn),t&&s(re),t&&s(yn),t&&s(Wt),t&&s(zn),t&&s(Bt),t&&s(Mn),t&&s(oe),t&&s(En),t&&s(Ht),t&&s(xn),t&&s($),t&&s(qn),t&&s(D),t&&s(jn),t&&s(W),k(We),t&&s(Fn),t&&s(S),k(Be),t&&s(Cn),t&&s(H),k(Re),t&&s(Pn),t&&s(v),k(Xe),k(Ye),k(et),k(tt),k(st),t&&s(An),t&&s(ce),t&&s(Sn),t&&s(R),k(rt),t&&s(Ln),t&&s(b),k(ot),k(lt),k(pt),t&&s(Nn),t&&s(he),t&&s(Dn),t&&s(K),k(ct),t&&s(In),t&&s(y),k(mt),k(ht),t&&s(Gn),t&&s(J),k(ut),t&&s(On),t&&s(z),k(gt),k(kt),t&&s(Vn),t&&s(Q),k(vt),t&&s(Un),t&&s(M),k(Tt),k($t),t&&s(Wn),t&&s(Y),k(wt),t&&s(Bn),t&&s(E),k(yt),k(Mt),t&&s(Hn),t&&s(Z),k(Et),t&&s(Rn),t&&s(x),k(xt),k(jt),t&&s(Xn),t&&s(ee),k(Ft),t&&s(Kn),t&&s(q),k(Ct),k(At),t&&s(Jn),t&&s(te),k(St),t&&s(Qn),t&&s(j),k(Lt),k(Dt),t&&s(Yn),t&&s(se),k(It),t&&s(Zn),t&&s(F),k(Gt),k(Vt)}}}const ep={local:"mt5",sections:[{local:"overview",title:"Overview"},{local:"transformers.MT5Config",title:"MT5Config"},{local:"transformers.T5Tokenizer",title:"MT5Tokenizer"},{local:"transformers.T5TokenizerFast",title:"MT5TokenizerFast"},{local:"transformers.MT5Model",title:"MT5Model"},{local:"transformers.MT5ForConditionalGeneration",title:"MT5ForConditionalGeneration"},{local:"transformers.MT5EncoderModel",title:"MT5EncoderModel"},{local:"transformers.TFMT5Model",title:"TFMT5Model"},{local:"transformers.TFMT5ForConditionalGeneration",title:"TFMT5ForConditionalGeneration"},{local:"transformers.TFMT5EncoderModel",title:"TFMT5EncoderModel"},{local:"transformers.FlaxMT5Model",title:"FlaxMT5Model"},{local:"transformers.FlaxMT5ForConditionalGeneration",title:"FlaxMT5ForConditionalGeneration"}],title:"mT5"};function tp(Lr,N,Ut){let{fw:P}=N;return Lr.$$set=A=>{"fw"in A&&Ut(0,P=A.fw)},[P]}class ip extends Xd{constructor(N){super();Kd(this,N,tp,Zd,Jd,{fw:0})}}export{ip as default,ep as metadata};
