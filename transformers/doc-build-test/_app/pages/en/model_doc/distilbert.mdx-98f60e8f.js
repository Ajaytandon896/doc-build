import{S as ry,i as ay,s as iy,e as s,k as l,w as T,t as a,L as ly,c as n,d as t,m as d,a as r,x as k,h as i,b as c,J as e,g as u,y as w,q as $,o as D,B as F}from"../../../chunks/vendor-9e2b328e.js";import{T as me}from"../../../chunks/Tip-76f97a76.js";import{D as U}from"../../../chunks/Docstring-50fd6873.js";import{C as ye}from"../../../chunks/CodeBlock-b9ff96e9.js";import{I as be}from"../../../chunks/IconCopyLink-fd0e58fd.js";import"../../../chunks/CopyButton-4b97cbf7.js";function dy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function cy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function hy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function py(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function uy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function fy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function my(C){let h,y,g,b,v,_,f,B,de,J,E,G,N,X,ce,O,he,re,L,q,Y,V,x,z,pe,W,oe,ue,R,ae,ee,A,ie,S,se,fe,P,te,H,le;return{c(){h=s("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=s("ul"),v=s("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=s("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),J=l(),E=s("p"),G=a("This second option is useful when using "),N=s("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),he=a("model(inputs)"),re=a("."),L=l(),q=s("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=s("ul"),z=s("li"),pe=a("a single Tensor with "),W=s("code"),oe=a("input_ids"),ue=a(" only and nothing else: "),R=s("code"),ae=a("model(inputs_ids)"),ee=l(),A=s("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=s("code"),se=a("model([input_ids, attention_mask])"),fe=l(),P=s("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),le=a('model({"input_ids": input_ids})')},l(p){h=n(p,"P",{});var M=r(h);y=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(p),b=n(p,"UL",{});var K=r(b);v=n(K,"LI",{});var ge=r(v);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var ve=r(B);de=i(ve,"having all inputs as a list, tuple or dict in the first positional arguments."),ve.forEach(t),K.forEach(t),J=d(p),E=n(p,"P",{});var I=r(E);G=i(I,"This second option is useful when using "),N=n(I,"CODE",{});var _e=r(N);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);he=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),L=d(p),q=n(p,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),V=d(p),x=n(p,"UL",{});var j=r(x);z=n(j,"LI",{});var Q=r(z);pe=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);oe=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(j),A=n(j,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=n(Z,"CODE",{});var De=r(S);se=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(j),P=n(j,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),j.forEach(t)},m(p,M){u(p,h,M),e(h,y),u(p,g,M),u(p,b,M),e(b,v),e(v,_),e(b,f),e(b,B),e(B,de),u(p,J,M),u(p,E,M),e(E,G),e(E,N),e(N,X),e(E,ce),e(E,O),e(O,he),e(E,re),u(p,L,M),u(p,q,M),e(q,Y),u(p,V,M),u(p,x,M),e(x,z),e(z,pe),e(z,W),e(W,oe),e(z,ue),e(z,R),e(R,ae),e(x,ee),e(x,A),e(A,ie),e(A,S),e(S,se),e(x,fe),e(x,P),e(P,te),e(P,H),e(H,le)},d(p){p&&t(h),p&&t(g),p&&t(b),p&&t(J),p&&t(E),p&&t(L),p&&t(q),p&&t(V),p&&t(x)}}}function gy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function _y(C){let h,y,g,b,v,_,f,B,de,J,E,G,N,X,ce,O,he,re,L,q,Y,V,x,z,pe,W,oe,ue,R,ae,ee,A,ie,S,se,fe,P,te,H,le;return{c(){h=s("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=s("ul"),v=s("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=s("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),J=l(),E=s("p"),G=a("This second option is useful when using "),N=s("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),he=a("model(inputs)"),re=a("."),L=l(),q=s("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=s("ul"),z=s("li"),pe=a("a single Tensor with "),W=s("code"),oe=a("input_ids"),ue=a(" only and nothing else: "),R=s("code"),ae=a("model(inputs_ids)"),ee=l(),A=s("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=s("code"),se=a("model([input_ids, attention_mask])"),fe=l(),P=s("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),le=a('model({"input_ids": input_ids})')},l(p){h=n(p,"P",{});var M=r(h);y=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(p),b=n(p,"UL",{});var K=r(b);v=n(K,"LI",{});var ge=r(v);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var ve=r(B);de=i(ve,"having all inputs as a list, tuple or dict in the first positional arguments."),ve.forEach(t),K.forEach(t),J=d(p),E=n(p,"P",{});var I=r(E);G=i(I,"This second option is useful when using "),N=n(I,"CODE",{});var _e=r(N);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);he=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),L=d(p),q=n(p,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),V=d(p),x=n(p,"UL",{});var j=r(x);z=n(j,"LI",{});var Q=r(z);pe=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);oe=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(j),A=n(j,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=n(Z,"CODE",{});var De=r(S);se=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(j),P=n(j,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),j.forEach(t)},m(p,M){u(p,h,M),e(h,y),u(p,g,M),u(p,b,M),e(b,v),e(v,_),e(b,f),e(b,B),e(B,de),u(p,J,M),u(p,E,M),e(E,G),e(E,N),e(N,X),e(E,ce),e(E,O),e(O,he),e(E,re),u(p,L,M),u(p,q,M),e(q,Y),u(p,V,M),u(p,x,M),e(x,z),e(z,pe),e(z,W),e(W,oe),e(z,ue),e(z,R),e(R,ae),e(x,ee),e(x,A),e(A,ie),e(A,S),e(S,se),e(x,fe),e(x,P),e(P,te),e(P,H),e(H,le)},d(p){p&&t(h),p&&t(g),p&&t(b),p&&t(J),p&&t(E),p&&t(L),p&&t(q),p&&t(V),p&&t(x)}}}function by(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function vy(C){let h,y,g,b,v,_,f,B,de,J,E,G,N,X,ce,O,he,re,L,q,Y,V,x,z,pe,W,oe,ue,R,ae,ee,A,ie,S,se,fe,P,te,H,le;return{c(){h=s("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=s("ul"),v=s("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=s("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),J=l(),E=s("p"),G=a("This second option is useful when using "),N=s("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),he=a("model(inputs)"),re=a("."),L=l(),q=s("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=s("ul"),z=s("li"),pe=a("a single Tensor with "),W=s("code"),oe=a("input_ids"),ue=a(" only and nothing else: "),R=s("code"),ae=a("model(inputs_ids)"),ee=l(),A=s("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=s("code"),se=a("model([input_ids, attention_mask])"),fe=l(),P=s("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),le=a('model({"input_ids": input_ids})')},l(p){h=n(p,"P",{});var M=r(h);y=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(p),b=n(p,"UL",{});var K=r(b);v=n(K,"LI",{});var ge=r(v);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var ve=r(B);de=i(ve,"having all inputs as a list, tuple or dict in the first positional arguments."),ve.forEach(t),K.forEach(t),J=d(p),E=n(p,"P",{});var I=r(E);G=i(I,"This second option is useful when using "),N=n(I,"CODE",{});var _e=r(N);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);he=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),L=d(p),q=n(p,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),V=d(p),x=n(p,"UL",{});var j=r(x);z=n(j,"LI",{});var Q=r(z);pe=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);oe=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(j),A=n(j,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=n(Z,"CODE",{});var De=r(S);se=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(j),P=n(j,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),j.forEach(t)},m(p,M){u(p,h,M),e(h,y),u(p,g,M),u(p,b,M),e(b,v),e(v,_),e(b,f),e(b,B),e(B,de),u(p,J,M),u(p,E,M),e(E,G),e(E,N),e(N,X),e(E,ce),e(E,O),e(O,he),e(E,re),u(p,L,M),u(p,q,M),e(q,Y),u(p,V,M),u(p,x,M),e(x,z),e(z,pe),e(z,W),e(W,oe),e(z,ue),e(z,R),e(R,ae),e(x,ee),e(x,A),e(A,ie),e(A,S),e(S,se),e(x,fe),e(x,P),e(P,te),e(P,H),e(H,le)},d(p){p&&t(h),p&&t(g),p&&t(b),p&&t(J),p&&t(E),p&&t(L),p&&t(q),p&&t(V),p&&t(x)}}}function Ty(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function ky(C){let h,y,g,b,v,_,f,B,de,J,E,G,N,X,ce,O,he,re,L,q,Y,V,x,z,pe,W,oe,ue,R,ae,ee,A,ie,S,se,fe,P,te,H,le;return{c(){h=s("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=s("ul"),v=s("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=s("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),J=l(),E=s("p"),G=a("This second option is useful when using "),N=s("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),he=a("model(inputs)"),re=a("."),L=l(),q=s("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=s("ul"),z=s("li"),pe=a("a single Tensor with "),W=s("code"),oe=a("input_ids"),ue=a(" only and nothing else: "),R=s("code"),ae=a("model(inputs_ids)"),ee=l(),A=s("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=s("code"),se=a("model([input_ids, attention_mask])"),fe=l(),P=s("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),le=a('model({"input_ids": input_ids})')},l(p){h=n(p,"P",{});var M=r(h);y=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(p),b=n(p,"UL",{});var K=r(b);v=n(K,"LI",{});var ge=r(v);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var ve=r(B);de=i(ve,"having all inputs as a list, tuple or dict in the first positional arguments."),ve.forEach(t),K.forEach(t),J=d(p),E=n(p,"P",{});var I=r(E);G=i(I,"This second option is useful when using "),N=n(I,"CODE",{});var _e=r(N);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);he=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),L=d(p),q=n(p,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),V=d(p),x=n(p,"UL",{});var j=r(x);z=n(j,"LI",{});var Q=r(z);pe=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);oe=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(j),A=n(j,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=n(Z,"CODE",{});var De=r(S);se=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(j),P=n(j,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),j.forEach(t)},m(p,M){u(p,h,M),e(h,y),u(p,g,M),u(p,b,M),e(b,v),e(v,_),e(b,f),e(b,B),e(B,de),u(p,J,M),u(p,E,M),e(E,G),e(E,N),e(N,X),e(E,ce),e(E,O),e(O,he),e(E,re),u(p,L,M),u(p,q,M),e(q,Y),u(p,V,M),u(p,x,M),e(x,z),e(z,pe),e(z,W),e(W,oe),e(z,ue),e(z,R),e(R,ae),e(x,ee),e(x,A),e(A,ie),e(A,S),e(S,se),e(x,fe),e(x,P),e(P,te),e(P,H),e(H,le)},d(p){p&&t(h),p&&t(g),p&&t(b),p&&t(J),p&&t(E),p&&t(L),p&&t(q),p&&t(V),p&&t(x)}}}function wy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function $y(C){let h,y,g,b,v,_,f,B,de,J,E,G,N,X,ce,O,he,re,L,q,Y,V,x,z,pe,W,oe,ue,R,ae,ee,A,ie,S,se,fe,P,te,H,le;return{c(){h=s("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=s("ul"),v=s("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=s("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),J=l(),E=s("p"),G=a("This second option is useful when using "),N=s("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),he=a("model(inputs)"),re=a("."),L=l(),q=s("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=s("ul"),z=s("li"),pe=a("a single Tensor with "),W=s("code"),oe=a("input_ids"),ue=a(" only and nothing else: "),R=s("code"),ae=a("model(inputs_ids)"),ee=l(),A=s("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=s("code"),se=a("model([input_ids, attention_mask])"),fe=l(),P=s("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),le=a('model({"input_ids": input_ids})')},l(p){h=n(p,"P",{});var M=r(h);y=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(p),b=n(p,"UL",{});var K=r(b);v=n(K,"LI",{});var ge=r(v);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var ve=r(B);de=i(ve,"having all inputs as a list, tuple or dict in the first positional arguments."),ve.forEach(t),K.forEach(t),J=d(p),E=n(p,"P",{});var I=r(E);G=i(I,"This second option is useful when using "),N=n(I,"CODE",{});var _e=r(N);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);he=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),L=d(p),q=n(p,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),V=d(p),x=n(p,"UL",{});var j=r(x);z=n(j,"LI",{});var Q=r(z);pe=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);oe=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(j),A=n(j,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=n(Z,"CODE",{});var De=r(S);se=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(j),P=n(j,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),j.forEach(t)},m(p,M){u(p,h,M),e(h,y),u(p,g,M),u(p,b,M),e(b,v),e(v,_),e(b,f),e(b,B),e(B,de),u(p,J,M),u(p,E,M),e(E,G),e(E,N),e(N,X),e(E,ce),e(E,O),e(O,he),e(E,re),u(p,L,M),u(p,q,M),e(q,Y),u(p,V,M),u(p,x,M),e(x,z),e(z,pe),e(z,W),e(W,oe),e(z,ue),e(z,R),e(R,ae),e(x,ee),e(x,A),e(A,ie),e(A,S),e(S,se),e(x,fe),e(x,P),e(P,te),e(P,H),e(H,le)},d(p){p&&t(h),p&&t(g),p&&t(b),p&&t(J),p&&t(E),p&&t(L),p&&t(q),p&&t(V),p&&t(x)}}}function Dy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function Fy(C){let h,y,g,b,v,_,f,B,de,J,E,G,N,X,ce,O,he,re,L,q,Y,V,x,z,pe,W,oe,ue,R,ae,ee,A,ie,S,se,fe,P,te,H,le;return{c(){h=s("p"),y=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=s("ul"),v=s("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),f=l(),B=s("li"),de=a("having all inputs as a list, tuple or dict in the first positional arguments."),J=l(),E=s("p"),G=a("This second option is useful when using "),N=s("code"),X=a("tf.keras.Model.fit"),ce=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),he=a("model(inputs)"),re=a("."),L=l(),q=s("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=s("ul"),z=s("li"),pe=a("a single Tensor with "),W=s("code"),oe=a("input_ids"),ue=a(" only and nothing else: "),R=s("code"),ae=a("model(inputs_ids)"),ee=l(),A=s("li"),ie=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=s("code"),se=a("model([input_ids, attention_mask])"),fe=l(),P=s("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),le=a('model({"input_ids": input_ids})')},l(p){h=n(p,"P",{});var M=r(h);y=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(p),b=n(p,"UL",{});var K=r(b);v=n(K,"LI",{});var ge=r(v);_=i(ge,"having all inputs as keyword arguments (like PyTorch models), or"),ge.forEach(t),f=d(K),B=n(K,"LI",{});var ve=r(B);de=i(ve,"having all inputs as a list, tuple or dict in the first positional arguments."),ve.forEach(t),K.forEach(t),J=d(p),E=n(p,"P",{});var I=r(E);G=i(I,"This second option is useful when using "),N=n(I,"CODE",{});var _e=r(N);X=i(_e,"tf.keras.Model.fit"),_e.forEach(t),ce=i(I,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=n(I,"CODE",{});var Te=r(O);he=i(Te,"model(inputs)"),Te.forEach(t),re=i(I,"."),I.forEach(t),L=d(p),q=n(p,"P",{});var ke=r(q);Y=i(ke,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ke.forEach(t),V=d(p),x=n(p,"UL",{});var j=r(x);z=n(j,"LI",{});var Q=r(z);pe=i(Q,"a single Tensor with "),W=n(Q,"CODE",{});var we=r(W);oe=i(we,"input_ids"),we.forEach(t),ue=i(Q," only and nothing else: "),R=n(Q,"CODE",{});var $e=r(R);ae=i($e,"model(inputs_ids)"),$e.forEach(t),Q.forEach(t),ee=d(j),A=n(j,"LI",{});var Z=r(A);ie=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),S=n(Z,"CODE",{});var De=r(S);se=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(j),P=n(j,"LI",{});var ne=r(P);te=i(ne,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=n(ne,"CODE",{});var Fe=r(H);le=i(Fe,'model({"input_ids": input_ids})'),Fe.forEach(t),ne.forEach(t),j.forEach(t)},m(p,M){u(p,h,M),e(h,y),u(p,g,M),u(p,b,M),e(b,v),e(v,_),e(b,f),e(b,B),e(B,de),u(p,J,M),u(p,E,M),e(E,G),e(E,N),e(N,X),e(E,ce),e(E,O),e(O,he),e(E,re),u(p,L,M),u(p,q,M),e(q,Y),u(p,V,M),u(p,x,M),e(x,z),e(z,pe),e(z,W),e(W,oe),e(z,ue),e(z,R),e(R,ae),e(x,ee),e(x,A),e(A,ie),e(A,S),e(S,se),e(x,fe),e(x,P),e(P,te),e(P,H),e(H,le)},d(p){p&&t(h),p&&t(g),p&&t(b),p&&t(J),p&&t(E),p&&t(L),p&&t(q),p&&t(V),p&&t(x)}}}function yy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function By(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function My(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function Ey(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function xy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function zy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function Cy(C){let h,y,g,b,v;return{c(){h=s("p"),y=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s("code"),b=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){h=n(_,"P",{});var f=r(h);y=i(f,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n(f,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),v=i(f,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),f.forEach(t)},m(_,f){u(_,h,f),e(h,y),e(h,g),e(g,b),e(h,v)},d(_){_&&t(h)}}}function jy(C){let h,y,g,b,v,_,f,B,de,J,E,G,N,X,ce,O,he,re,L,q,Y,V,x,z,pe,W,oe,ue,R,ae,ee,A,ie,S,se,fe,P,te,H,le,p,M,K,ge,ve,I,_e,Te,ke,j,Q,we,$e,Z,De,ne,Fe,pu,uh,vt,uu,Ys,fu,mu,Zs,gu,_u,en,bu,vu,fh,ao,Go,_l,tn,Tu,bl,ku,mh,Ne,on,wu,yt,$u,di,Du,Fu,ci,yu,Bu,sn,Mu,Eu,xu,io,zu,hi,Cu,ju,pi,Pu,qu,Au,vl,Iu,Lu,nn,gh,lo,Xo,Tl,rn,Su,kl,Nu,_h,_t,an,Ou,wl,Wu,Ru,Yo,ui,Hu,Qu,fi,Uu,Ju,Vu,ln,Ku,mi,Gu,Xu,bh,co,Zo,$l,dn,Yu,Dl,Zu,vh,bt,cn,ef,hn,tf,Fl,of,sf,nf,es,gi,rf,af,_i,lf,df,cf,pn,hf,bi,pf,uf,Th,ho,ts,yl,un,ff,Bl,mf,kh,Oe,fn,gf,Ml,_f,bf,mn,vf,vi,Tf,kf,wf,gn,$f,_n,Df,Ff,yf,Je,bn,Bf,po,Mf,Ti,Ef,xf,El,zf,Cf,jf,os,Pf,xl,qf,Af,vn,wh,uo,ss,zl,Tn,If,Cl,Lf,$h,We,kn,Sf,wn,Nf,jl,Of,Wf,Rf,$n,Hf,ki,Qf,Uf,Jf,Dn,Vf,Fn,Kf,Gf,Xf,Ve,yn,Yf,fo,Zf,wi,em,tm,Pl,om,sm,nm,ns,rm,ql,am,im,Bn,Dh,mo,rs,Al,Mn,lm,Il,dm,Fh,Re,En,cm,Ll,hm,pm,xn,um,$i,fm,mm,gm,zn,_m,Cn,bm,vm,Tm,je,jn,km,go,wm,Di,$m,Dm,Sl,Fm,ym,Bm,as,Mm,Nl,Em,xm,Pn,zm,Ol,Cm,jm,qn,yh,_o,is,Wl,An,Pm,Rl,qm,Bh,He,In,Am,Hl,Im,Lm,Ln,Sm,Fi,Nm,Om,Wm,Sn,Rm,Nn,Hm,Qm,Um,Ke,On,Jm,bo,Vm,yi,Km,Gm,Ql,Xm,Ym,Zm,ls,eg,Ul,tg,og,Wn,Mh,vo,ds,Jl,Rn,sg,Vl,ng,Eh,Qe,Hn,rg,Kl,ag,ig,Qn,lg,Bi,dg,cg,hg,Un,pg,Jn,ug,fg,mg,Ge,Vn,gg,To,_g,Mi,bg,vg,Gl,Tg,kg,wg,cs,$g,Xl,Dg,Fg,Kn,xh,ko,hs,Yl,Gn,yg,Zl,Bg,zh,Ue,Xn,Mg,wo,Eg,ed,xg,zg,td,Cg,jg,Pg,Yn,qg,Ei,Ag,Ig,Lg,Zn,Sg,er,Ng,Og,Wg,Xe,tr,Rg,$o,Hg,xi,Qg,Ug,od,Jg,Vg,Kg,ps,Gg,sd,Xg,Yg,or,Ch,Do,us,nd,sr,Zg,rd,e_,jh,Pe,nr,t_,ad,o_,s_,rr,n_,zi,r_,a_,i_,ar,l_,ir,d_,c_,h_,fs,p_,Ye,lr,u_,Fo,f_,Ci,m_,g_,id,__,b_,v_,ms,T_,ld,k_,w_,dr,Ph,yo,gs,dd,cr,$_,cd,D_,qh,qe,hr,F_,pr,y_,hd,B_,M_,E_,ur,x_,ji,z_,C_,j_,fr,P_,mr,q_,A_,I_,_s,L_,Ze,gr,S_,Bo,N_,Pi,O_,W_,pd,R_,H_,Q_,bs,U_,ud,J_,V_,_r,Ah,Mo,vs,fd,br,K_,md,G_,Ih,Ae,vr,X_,gd,Y_,Z_,Tr,eb,qi,tb,ob,sb,kr,nb,wr,rb,ab,ib,Ts,lb,et,$r,db,Eo,cb,Ai,hb,pb,_d,ub,fb,mb,ks,gb,bd,_b,bb,Dr,Lh,xo,ws,vd,Fr,vb,Td,Tb,Sh,Ie,yr,kb,kd,wb,$b,Br,Db,Ii,Fb,yb,Bb,Mr,Mb,Er,Eb,xb,zb,$s,Cb,tt,xr,jb,zo,Pb,Li,qb,Ab,wd,Ib,Lb,Sb,Ds,Nb,$d,Ob,Wb,zr,Nh,Co,Fs,Dd,Cr,Rb,Fd,Hb,Oh,Le,jr,Qb,yd,Ub,Jb,Pr,Vb,Si,Kb,Gb,Xb,qr,Yb,Ar,Zb,ev,tv,ys,ov,ot,Ir,sv,jo,nv,Ni,rv,av,Bd,iv,lv,dv,Bs,cv,Md,hv,pv,Lr,Wh,Po,Ms,Ed,Sr,uv,xd,fv,Rh,Se,Nr,mv,qo,gv,zd,_v,bv,Cd,vv,Tv,kv,Or,wv,Oi,$v,Dv,Fv,Wr,yv,Rr,Bv,Mv,Ev,Es,xv,st,Hr,zv,Ao,Cv,Wi,jv,Pv,jd,qv,Av,Iv,xs,Lv,Pd,Sv,Nv,Qr,Hh,Io,zs,qd,Ur,Ov,Ad,Wv,Qh,Be,Jr,Rv,Id,Hv,Qv,Vr,Uv,Ri,Jv,Vv,Kv,Kr,Gv,Gr,Xv,Yv,Zv,Ld,eT,tT,Bt,Sd,Xr,oT,sT,Nd,Yr,nT,rT,Od,Zr,aT,iT,Wd,ea,lT,dT,nt,ta,cT,Lo,hT,Rd,pT,uT,Hd,fT,mT,gT,Cs,_T,Qd,bT,vT,oa,Uh,So,js,Ud,sa,TT,Jd,kT,Jh,Me,na,wT,ra,$T,Vd,DT,FT,yT,aa,BT,Hi,MT,ET,xT,ia,zT,la,CT,jT,PT,Kd,qT,AT,Mt,Gd,da,IT,LT,Xd,ca,ST,NT,Yd,ha,OT,WT,Zd,pa,RT,HT,rt,ua,QT,No,UT,ec,JT,VT,tc,KT,GT,XT,Ps,YT,oc,ZT,ek,fa,Vh,Oo,qs,sc,ma,tk,nc,ok,Kh,Ee,ga,sk,rc,nk,rk,_a,ak,Qi,ik,lk,dk,ba,ck,va,hk,pk,uk,ac,fk,mk,Et,ic,Ta,gk,_k,lc,ka,bk,vk,dc,wa,Tk,kk,cc,$a,wk,$k,at,Da,Dk,Wo,Fk,hc,yk,Bk,pc,Mk,Ek,xk,As,zk,uc,Ck,jk,Fa,Gh,Ro,Is,fc,ya,Pk,mc,qk,Xh,xe,Ba,Ak,gc,Ik,Lk,Ma,Sk,Ui,Nk,Ok,Wk,Ea,Rk,xa,Hk,Qk,Uk,_c,Jk,Vk,xt,bc,za,Kk,Gk,vc,Ca,Xk,Yk,Tc,ja,Zk,ew,kc,Pa,tw,ow,it,qa,sw,Ho,nw,wc,rw,aw,$c,iw,lw,dw,Ls,cw,Dc,hw,pw,Aa,Yh,Qo,Ss,Fc,Ia,uw,yc,fw,Zh,ze,La,mw,Bc,gw,_w,Sa,bw,Ji,vw,Tw,kw,Na,ww,Oa,$w,Dw,Fw,Mc,yw,Bw,zt,Ec,Wa,Mw,Ew,xc,Ra,xw,zw,zc,Ha,Cw,jw,Cc,Qa,Pw,qw,lt,Ua,Aw,Uo,Iw,jc,Lw,Sw,Pc,Nw,Ow,Ww,Ns,Rw,qc,Hw,Qw,Ja,ep,Jo,Os,Ac,Va,Uw,Ic,Jw,tp,Ce,Ka,Vw,Vo,Kw,Lc,Gw,Xw,Sc,Yw,Zw,e1,Ga,t1,Vi,o1,s1,n1,Xa,r1,Ya,a1,i1,l1,Nc,d1,c1,Ct,Oc,Za,h1,p1,Wc,ei,u1,f1,Rc,ti,m1,g1,Hc,oi,_1,b1,dt,si,v1,Ko,T1,Qc,k1,w1,Uc,$1,D1,F1,Ws,y1,Jc,B1,M1,ni,op;return _=new be({}),X=new be({}),tn=new be({}),on=new U({props:{name:"class transformers.DistilBertConfig",anchor:"transformers.DistilBertConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"max_position_embeddings",val:" = 512"},{name:"sinusoidal_pos_embds",val:" = False"},{name:"n_layers",val:" = 6"},{name:"n_heads",val:" = 12"},{name:"dim",val:" = 768"},{name:"hidden_dim",val:" = 3072"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"qa_dropout",val:" = 0.1"},{name:"seq_classif_dropout",val:" = 0.2"},{name:"pad_token_id",val:" = 0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/configuration_distilbert.py#L37",parametersDescription:[{anchor:"transformers.DistilBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DistilBERT model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> or <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DistilBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DistilBertConfig.sinusoidal_pos_embds",description:`<strong>sinusoidal_pos_embds</strong> (<code>boolean</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use sinusoidal positional embeddings.`,name:"sinusoidal_pos_embds"},{anchor:"transformers.DistilBertConfig.n_layers",description:`<strong>n_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"n_layers"},{anchor:"transformers.DistilBertConfig.n_heads",description:`<strong>n_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_heads"},{anchor:"transformers.DistilBertConfig.dim",description:`<strong>dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"dim"},{anchor:"transformers.DistilBertConfig.hidden_dim",description:`<strong>hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
The size of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"hidden_dim"},{anchor:"transformers.DistilBertConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.DistilBertConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.DistilBertConfig.activation",description:`<strong>activation</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation"},{anchor:"transformers.DistilBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DistilBertConfig.qa_dropout",description:`<strong>qa_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilities used in the question answering model <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a>.`,name:"qa_dropout"},{anchor:"transformers.DistilBertConfig.seq_classif_dropout",description:`<strong>seq_classif_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The dropout probabilities used in the sequence classification and the multiple choice model
<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a>.`,name:"seq_classif_dropout"}]}}),nn=new ye({props:{code:`from transformers import DistilBertModel, DistilBertConfig

# Initializing a DistilBERT configuration
configuration = DistilBertConfig()

# Initializing a model from the configuration
model = DistilBertModel(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertModel, DistilBertConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DistilBERT configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DistilBertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),rn=new be({}),an=new U({props:{name:"class transformers.DistilBertTokenizer",anchor:"transformers.DistilBertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/tokenization_distilbert.py#L56"}}),dn=new be({}),cn=new U({props:{name:"class transformers.DistilBertTokenizerFast",anchor:"transformers.DistilBertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/tokenization_distilbert_fast.py#L65"}}),un=new be({}),fn=new U({props:{name:"class transformers.DistilBertModel",anchor:"transformers.DistilBertModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L435",parametersDescription:[{anchor:"transformers.DistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),bn=new U({props:{name:"forward",anchor:"transformers.DistilBertModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L507",parametersDescription:[{anchor:"transformers.DistilBertModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),os=new me({props:{$$slots:{default:[dy]},$$scope:{ctx:C}}}),vn=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertModel
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Tn=new be({}),kn=new U({props:{name:"class transformers.DistilBertForMaskedLM",anchor:"transformers.DistilBertForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L563",parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),yn=new U({props:{name:"forward",anchor:"transformers.DistilBertForMaskedLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L603",parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ns=new me({props:{$$slots:{default:[cy]},$$scope:{ctx:C}}}),Bn=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMaskedLM
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Mn=new be({}),En=new U({props:{name:"class transformers.DistilBertForSequenceClassification",anchor:"transformers.DistilBertForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L667",parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),jn=new U({props:{name:"forward",anchor:"transformers.DistilBertForSequenceClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L701",parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),as=new me({props:{$$slots:{default:[hy]},$$scope:{ctx:C}}}),Pn=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),qn=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),An=new be({}),In=new U({props:{name:"class transformers.DistilBertForMultipleChoice",anchor:"transformers.DistilBertForMultipleChoice",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L997",parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),On=new U({props:{name:"forward",anchor:"transformers.DistilBertForMultipleChoice.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L1029",parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ls=new me({props:{$$slots:{default:[py]},$$scope:{ctx:C}}}),Wn=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMultipleChoice
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-cased")
model = DistilBertForMultipleChoice.from_pretrained("distilbert-base-cased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Rn=new be({}),Hn=new U({props:{name:"class transformers.DistilBertForTokenClassification",anchor:"transformers.DistilBertForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L902",parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Vn=new U({props:{name:"forward",anchor:"transformers.DistilBertForTokenClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L934",parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),cs=new me({props:{$$slots:{default:[uy]},$$scope:{ctx:C}}}),Kn=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertForTokenClassification
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>] * inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].size(<span class="hljs-number">1</span>)).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Gn=new be({}),Xn=new U({props:{name:"class transformers.DistilBertForQuestionAnswering",anchor:"transformers.DistilBertForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L785",parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),tr=new U({props:{name:"forward",anchor:"transformers.DistilBertForQuestionAnswering.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_distilbert.py#L817",parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ps=new me({props:{$$slots:{default:[fy]},$$scope:{ctx:C}}}),or=new ye({props:{code:`from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="pt")
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_positions = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>end_positions = torch.tensor([<span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),sr=new be({}),nr=new U({props:{name:"class transformers.TFDistilBertModel",anchor:"transformers.TFDistilBertModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L536",parametersDescription:[{anchor:"transformers.TFDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),fs=new me({props:{$$slots:{default:[my]},$$scope:{ctx:C}}}),lr=new U({props:{name:"call",anchor:"transformers.TFDistilBertModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L541",parametersDescription:[{anchor:"transformers.TFDistilBertModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertModel.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ms=new me({props:{$$slots:{default:[gy]},$$scope:{ctx:C}}}),dr=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),cr=new be({}),hr=new U({props:{name:"class transformers.TFDistilBertForMaskedLM",anchor:"transformers.TFDistilBertForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L636",parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_s=new me({props:{$$slots:{default:[_y]},$$scope:{ctx:C}}}),gr=new U({props:{name:"call",anchor:"transformers.TFDistilBertForMaskedLM.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L656",parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),bs=new me({props:{$$slots:{default:[by]},$$scope:{ctx:C}}}),_r=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMaskedLM
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),br=new be({}),vr=new U({props:{name:"class transformers.TFDistilBertForSequenceClassification",anchor:"transformers.TFDistilBertForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L740",parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ts=new me({props:{$$slots:{default:[vy]},$$scope:{ctx:C}}}),$r=new U({props:{name:"call",anchor:"transformers.TFDistilBertForSequenceClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L757",parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ks=new me({props:{$$slots:{default:[Ty]},$$scope:{ctx:C}}}),Dr=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Fr=new be({}),yr=new U({props:{name:"class transformers.TFDistilBertForMultipleChoice",anchor:"transformers.TFDistilBertForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L931",parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),$s=new me({props:{$$slots:{default:[ky]},$$scope:{ctx:C}}}),xr=new U({props:{name:"call",anchor:"transformers.TFDistilBertForMultipleChoice.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L957",parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Ds=new me({props:{$$slots:{default:[wy]},$$scope:{ctx:C}}}),zr=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMultipleChoice
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Cr=new be({}),jr=new U({props:{name:"class transformers.TFDistilBertForTokenClassification",anchor:"transformers.TFDistilBertForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L841",parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ys=new me({props:{$$slots:{default:[$y]},$$scope:{ctx:C}}}),Ir=new U({props:{name:"call",anchor:"transformers.TFDistilBertForTokenClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L852",parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Bs=new me({props:{$$slots:{default:[Dy]},$$scope:{ctx:C}}}),Lr=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForTokenClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Sr=new be({}),Nr=new U({props:{name:"class transformers.TFDistilBertForQuestionAnswering",anchor:"transformers.TFDistilBertForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L1073",parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Es=new me({props:{$$slots:{default:[Fy]},$$scope:{ctx:C}}}),Hr=new U({props:{name:"call",anchor:"transformers.TFDistilBertForQuestionAnswering.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_tf_distilbert.py#L1084",parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),xs=new me({props:{$$slots:{default:[yy]},$$scope:{ctx:C}}}),Qr=new ye({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),Ur=new be({}),Jr=new U({props:{name:"class transformers.FlaxDistilBertModel",anchor:"transformers.FlaxDistilBertModel",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L527",parametersDescription:[{anchor:"transformers.FlaxDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ta=new U({props:{name:"\\_\\_call\\_\\_",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}]}}),Cs=new me({props:{$$slots:{default:[By]},$$scope:{ctx:C}}}),oa=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),sa=new be({}),na=new U({props:{name:"class transformers.FlaxDistilBertForMaskedLM",anchor:"transformers.FlaxDistilBertForMaskedLM",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L600",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ua=new U({props:{name:"\\_\\_call\\_\\_",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ps=new me({props:{$$slots:{default:[My]},$$scope:{ctx:C}}}),fa=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMaskedLM

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ma=new be({}),ga=new U({props:{name:"class transformers.FlaxDistilBertForSequenceClassification",anchor:"transformers.FlaxDistilBertForSequenceClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L669",parametersDescription:[{anchor:"transformers.FlaxDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Da=new U({props:{name:"\\_\\_call\\_\\_",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),As=new me({props:{$$slots:{default:[Ey]},$$scope:{ctx:C}}}),Fa=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ya=new be({}),Ba=new U({props:{name:"class transformers.FlaxDistilBertForMultipleChoice",anchor:"transformers.FlaxDistilBertForMultipleChoice",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L749",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),qa=new U({props:{name:"\\_\\_call\\_\\_",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ls=new me({props:{$$slots:{default:[xy]},$$scope:{ctx:C}}}),Aa=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMultipleChoice

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="jax", padding=True)
outputs = model(**{k: v[None, :] for k, v in encoding.items()})

logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;jax&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v[<span class="hljs-literal">None</span>, :] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()})

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ia=new be({}),La=new U({props:{name:"class transformers.FlaxDistilBertForTokenClassification",anchor:"transformers.FlaxDistilBertForTokenClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L815",parametersDescription:[{anchor:"transformers.FlaxDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ua=new U({props:{name:"\\_\\_call\\_\\_",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ns=new me({props:{$$slots:{default:[zy]},$$scope:{ctx:C}}}),Ja=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForTokenClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Va=new be({}),Ka=new U({props:{name:"class transformers.FlaxDistilBertForQuestionAnswering",anchor:"transformers.FlaxDistilBertForQuestionAnswering",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L885",parametersDescription:[{anchor:"transformers.FlaxDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),si=new U({props:{name:"\\_\\_call\\_\\_",anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/distilbert/modeling_flax_distilbert.py#L450",parametersDescription:[{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ws=new me({props:{$$slots:{default:[Cy]},$$scope:{ctx:C}}}),ni=new ye({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="jax")

outputs = model(**inputs)
start_scores = outputs.start_logits
end_scores = outputs.end_logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),{c(){h=s("meta"),y=l(),g=s("h1"),b=s("a"),v=s("span"),T(_.$$.fragment),f=l(),B=s("span"),de=a("DistilBERT"),J=l(),E=s("h2"),G=s("a"),N=s("span"),T(X.$$.fragment),ce=l(),O=s("span"),he=a("Overview"),re=l(),L=s("p"),q=a("The DistilBERT model was proposed in the blog post "),Y=s("a"),V=a(`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),x=a(", and the paper "),z=s("a"),pe=a(`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),W=a(`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),oe=s("em"),ue=a("bert-base-uncased"),R=a(`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),ae=l(),ee=s("p"),A=a("The abstract from the paper is the following:"),ie=l(),S=s("p"),se=s("em"),fe=a(`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),P=l(),te=s("p"),H=a("Tips:"),le=l(),p=s("ul"),M=s("li"),K=a("DistilBERT doesn\u2019t have "),ge=s("code"),ve=a("token_type_ids"),I=a(`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),_e=s("code"),Te=a("tokenizer.sep_token"),ke=a(" (or "),j=s("code"),Q=a("[SEP]"),we=a(")."),$e=l(),Z=s("li"),De=a("DistilBERT doesn\u2019t have options to select the input positions ("),ne=s("code"),Fe=a("position_ids"),pu=a(` input). This could be added if
necessary though, just let us know if you need this option.`),uh=l(),vt=s("p"),uu=a("This model was contributed by "),Ys=s("a"),fu=a("victorsanh"),mu=a(`. This model jax version was
contributed by `),Zs=s("a"),gu=a("kamalkraj"),_u=a(". The original code can be found "),en=s("a"),bu=a("here"),vu=a("."),fh=l(),ao=s("h2"),Go=s("a"),_l=s("span"),T(tn.$$.fragment),Tu=l(),bl=s("span"),ku=a("DistilBertConfig"),mh=l(),Ne=s("div"),T(on.$$.fragment),wu=l(),yt=s("p"),$u=a("This is the configuration class to store the configuration of a "),di=s("a"),Du=a("DistilBertModel"),Fu=a(" or a "),ci=s("a"),yu=a("TFDistilBertModel"),Bu=a(`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),sn=s("a"),Mu=a("distilbert-base-uncased"),Eu=a(" architecture."),xu=l(),io=s("p"),zu=a("Configuration objects inherit from "),hi=s("a"),Cu=a("PretrainedConfig"),ju=a(` and can be used to control the model outputs. Read the
documentation from `),pi=s("a"),Pu=a("PretrainedConfig"),qu=a(" for more information."),Au=l(),vl=s("p"),Iu=a("Examples:"),Lu=l(),T(nn.$$.fragment),gh=l(),lo=s("h2"),Xo=s("a"),Tl=s("span"),T(rn.$$.fragment),Su=l(),kl=s("span"),Nu=a("DistilBertTokenizer"),_h=l(),_t=s("div"),T(an.$$.fragment),Ou=l(),wl=s("p"),Wu=a("Construct a DistilBERT tokenizer."),Ru=l(),Yo=s("p"),ui=s("a"),Hu=a("DistilBertTokenizer"),Qu=a(" is identical to "),fi=s("a"),Uu=a("BertTokenizer"),Ju=a(` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),Vu=l(),ln=s("p"),Ku=a("Refer to superclass "),mi=s("a"),Gu=a("BertTokenizer"),Xu=a(" for usage examples and documentation concerning parameters."),bh=l(),co=s("h2"),Zo=s("a"),$l=s("span"),T(dn.$$.fragment),Yu=l(),Dl=s("span"),Zu=a("DistilBertTokenizerFast"),vh=l(),bt=s("div"),T(cn.$$.fragment),ef=l(),hn=s("p"),tf=a("Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),Fl=s("em"),of=a("tokenizers"),sf=a(" library)."),nf=l(),es=s("p"),gi=s("a"),rf=a("DistilBertTokenizerFast"),af=a(" is identical to "),_i=s("a"),lf=a("BertTokenizerFast"),df=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),cf=l(),pn=s("p"),hf=a("Refer to superclass "),bi=s("a"),pf=a("BertTokenizerFast"),uf=a(" for usage examples and documentation concerning parameters."),Th=l(),ho=s("h2"),ts=s("a"),yl=s("span"),T(un.$$.fragment),ff=l(),Bl=s("span"),mf=a("DistilBertModel"),kh=l(),Oe=s("div"),T(fn.$$.fragment),gf=l(),Ml=s("p"),_f=a("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),bf=l(),mn=s("p"),vf=a("This model inherits from "),vi=s("a"),Tf=a("PreTrainedModel"),kf=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),wf=l(),gn=s("p"),$f=a("This model is also a PyTorch "),_n=s("a"),Df=a("torch.nn.Module"),Ff=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),yf=l(),Je=s("div"),T(bn.$$.fragment),Bf=l(),po=s("p"),Mf=a("The "),Ti=s("a"),Ef=a("DistilBertModel"),xf=a(" forward method, overrides the "),El=s("code"),zf=a("__call__"),Cf=a(" special method."),jf=l(),T(os.$$.fragment),Pf=l(),xl=s("p"),qf=a("Example:"),Af=l(),T(vn.$$.fragment),wh=l(),uo=s("h2"),ss=s("a"),zl=s("span"),T(Tn.$$.fragment),If=l(),Cl=s("span"),Lf=a("DistilBertForMaskedLM"),$h=l(),We=s("div"),T(kn.$$.fragment),Sf=l(),wn=s("p"),Nf=a("DistilBert Model with a "),jl=s("code"),Of=a("masked language modeling"),Wf=a(" head on top."),Rf=l(),$n=s("p"),Hf=a("This model inherits from "),ki=s("a"),Qf=a("PreTrainedModel"),Uf=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Jf=l(),Dn=s("p"),Vf=a("This model is also a PyTorch "),Fn=s("a"),Kf=a("torch.nn.Module"),Gf=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xf=l(),Ve=s("div"),T(yn.$$.fragment),Yf=l(),fo=s("p"),Zf=a("The "),wi=s("a"),em=a("DistilBertForMaskedLM"),tm=a(" forward method, overrides the "),Pl=s("code"),om=a("__call__"),sm=a(" special method."),nm=l(),T(ns.$$.fragment),rm=l(),ql=s("p"),am=a("Example:"),im=l(),T(Bn.$$.fragment),Dh=l(),mo=s("h2"),rs=s("a"),Al=s("span"),T(Mn.$$.fragment),lm=l(),Il=s("span"),dm=a("DistilBertForSequenceClassification"),Fh=l(),Re=s("div"),T(En.$$.fragment),cm=l(),Ll=s("p"),hm=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),pm=l(),xn=s("p"),um=a("This model inherits from "),$i=s("a"),fm=a("PreTrainedModel"),mm=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),gm=l(),zn=s("p"),_m=a("This model is also a PyTorch "),Cn=s("a"),bm=a("torch.nn.Module"),vm=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Tm=l(),je=s("div"),T(jn.$$.fragment),km=l(),go=s("p"),wm=a("The "),Di=s("a"),$m=a("DistilBertForSequenceClassification"),Dm=a(" forward method, overrides the "),Sl=s("code"),Fm=a("__call__"),ym=a(" special method."),Bm=l(),T(as.$$.fragment),Mm=l(),Nl=s("p"),Em=a("Example of single-label classification:"),xm=l(),T(Pn.$$.fragment),zm=l(),Ol=s("p"),Cm=a("Example of multi-label classification:"),jm=l(),T(qn.$$.fragment),yh=l(),_o=s("h2"),is=s("a"),Wl=s("span"),T(An.$$.fragment),Pm=l(),Rl=s("span"),qm=a("DistilBertForMultipleChoice"),Bh=l(),He=s("div"),T(In.$$.fragment),Am=l(),Hl=s("p"),Im=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Lm=l(),Ln=s("p"),Sm=a("This model inherits from "),Fi=s("a"),Nm=a("PreTrainedModel"),Om=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wm=l(),Sn=s("p"),Rm=a("This model is also a PyTorch "),Nn=s("a"),Hm=a("torch.nn.Module"),Qm=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Um=l(),Ke=s("div"),T(On.$$.fragment),Jm=l(),bo=s("p"),Vm=a("The "),yi=s("a"),Km=a("DistilBertForMultipleChoice"),Gm=a(" forward method, overrides the "),Ql=s("code"),Xm=a("__call__"),Ym=a(" special method."),Zm=l(),T(ls.$$.fragment),eg=l(),Ul=s("p"),tg=a("Examples:"),og=l(),T(Wn.$$.fragment),Mh=l(),vo=s("h2"),ds=s("a"),Jl=s("span"),T(Rn.$$.fragment),sg=l(),Vl=s("span"),ng=a("DistilBertForTokenClassification"),Eh=l(),Qe=s("div"),T(Hn.$$.fragment),rg=l(),Kl=s("p"),ag=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),ig=l(),Qn=s("p"),lg=a("This model inherits from "),Bi=s("a"),dg=a("PreTrainedModel"),cg=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hg=l(),Un=s("p"),pg=a("This model is also a PyTorch "),Jn=s("a"),ug=a("torch.nn.Module"),fg=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),mg=l(),Ge=s("div"),T(Vn.$$.fragment),gg=l(),To=s("p"),_g=a("The "),Mi=s("a"),bg=a("DistilBertForTokenClassification"),vg=a(" forward method, overrides the "),Gl=s("code"),Tg=a("__call__"),kg=a(" special method."),wg=l(),T(cs.$$.fragment),$g=l(),Xl=s("p"),Dg=a("Example:"),Fg=l(),T(Kn.$$.fragment),xh=l(),ko=s("h2"),hs=s("a"),Yl=s("span"),T(Gn.$$.fragment),yg=l(),Zl=s("span"),Bg=a("DistilBertForQuestionAnswering"),zh=l(),Ue=s("div"),T(Xn.$$.fragment),Mg=l(),wo=s("p"),Eg=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),ed=s("code"),xg=a("span start logits"),zg=a(" and "),td=s("code"),Cg=a("span end logits"),jg=a(")."),Pg=l(),Yn=s("p"),qg=a("This model inherits from "),Ei=s("a"),Ag=a("PreTrainedModel"),Ig=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Lg=l(),Zn=s("p"),Sg=a("This model is also a PyTorch "),er=s("a"),Ng=a("torch.nn.Module"),Og=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Wg=l(),Xe=s("div"),T(tr.$$.fragment),Rg=l(),$o=s("p"),Hg=a("The "),xi=s("a"),Qg=a("DistilBertForQuestionAnswering"),Ug=a(" forward method, overrides the "),od=s("code"),Jg=a("__call__"),Vg=a(" special method."),Kg=l(),T(ps.$$.fragment),Gg=l(),sd=s("p"),Xg=a("Example:"),Yg=l(),T(or.$$.fragment),Ch=l(),Do=s("h2"),us=s("a"),nd=s("span"),T(sr.$$.fragment),Zg=l(),rd=s("span"),e_=a("TFDistilBertModel"),jh=l(),Pe=s("div"),T(nr.$$.fragment),t_=l(),ad=s("p"),o_=a("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),s_=l(),rr=s("p"),n_=a("This model inherits from "),zi=s("a"),r_=a("TFPreTrainedModel"),a_=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),i_=l(),ar=s("p"),l_=a("This model is also a "),ir=s("a"),d_=a("tf.keras.Model"),c_=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),h_=l(),T(fs.$$.fragment),p_=l(),Ye=s("div"),T(lr.$$.fragment),u_=l(),Fo=s("p"),f_=a("The "),Ci=s("a"),m_=a("TFDistilBertModel"),g_=a(" forward method, overrides the "),id=s("code"),__=a("__call__"),b_=a(" special method."),v_=l(),T(ms.$$.fragment),T_=l(),ld=s("p"),k_=a("Example:"),w_=l(),T(dr.$$.fragment),Ph=l(),yo=s("h2"),gs=s("a"),dd=s("span"),T(cr.$$.fragment),$_=l(),cd=s("span"),D_=a("TFDistilBertForMaskedLM"),qh=l(),qe=s("div"),T(hr.$$.fragment),F_=l(),pr=s("p"),y_=a("DistilBert Model with a "),hd=s("code"),B_=a("masked language modeling"),M_=a(" head on top."),E_=l(),ur=s("p"),x_=a("This model inherits from "),ji=s("a"),z_=a("TFPreTrainedModel"),C_=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),j_=l(),fr=s("p"),P_=a("This model is also a "),mr=s("a"),q_=a("tf.keras.Model"),A_=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),I_=l(),T(_s.$$.fragment),L_=l(),Ze=s("div"),T(gr.$$.fragment),S_=l(),Bo=s("p"),N_=a("The "),Pi=s("a"),O_=a("TFDistilBertForMaskedLM"),W_=a(" forward method, overrides the "),pd=s("code"),R_=a("__call__"),H_=a(" special method."),Q_=l(),T(bs.$$.fragment),U_=l(),ud=s("p"),J_=a("Example:"),V_=l(),T(_r.$$.fragment),Ah=l(),Mo=s("h2"),vs=s("a"),fd=s("span"),T(br.$$.fragment),K_=l(),md=s("span"),G_=a("TFDistilBertForSequenceClassification"),Ih=l(),Ae=s("div"),T(vr.$$.fragment),X_=l(),gd=s("p"),Y_=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),Z_=l(),Tr=s("p"),eb=a("This model inherits from "),qi=s("a"),tb=a("TFPreTrainedModel"),ob=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),sb=l(),kr=s("p"),nb=a("This model is also a "),wr=s("a"),rb=a("tf.keras.Model"),ab=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ib=l(),T(Ts.$$.fragment),lb=l(),et=s("div"),T($r.$$.fragment),db=l(),Eo=s("p"),cb=a("The "),Ai=s("a"),hb=a("TFDistilBertForSequenceClassification"),pb=a(" forward method, overrides the "),_d=s("code"),ub=a("__call__"),fb=a(" special method."),mb=l(),T(ks.$$.fragment),gb=l(),bd=s("p"),_b=a("Example:"),bb=l(),T(Dr.$$.fragment),Lh=l(),xo=s("h2"),ws=s("a"),vd=s("span"),T(Fr.$$.fragment),vb=l(),Td=s("span"),Tb=a("TFDistilBertForMultipleChoice"),Sh=l(),Ie=s("div"),T(yr.$$.fragment),kb=l(),kd=s("p"),wb=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),$b=l(),Br=s("p"),Db=a("This model inherits from "),Ii=s("a"),Fb=a("TFPreTrainedModel"),yb=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bb=l(),Mr=s("p"),Mb=a("This model is also a "),Er=s("a"),Eb=a("tf.keras.Model"),xb=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),zb=l(),T($s.$$.fragment),Cb=l(),tt=s("div"),T(xr.$$.fragment),jb=l(),zo=s("p"),Pb=a("The "),Li=s("a"),qb=a("TFDistilBertForMultipleChoice"),Ab=a(" forward method, overrides the "),wd=s("code"),Ib=a("__call__"),Lb=a(" special method."),Sb=l(),T(Ds.$$.fragment),Nb=l(),$d=s("p"),Ob=a("Example:"),Wb=l(),T(zr.$$.fragment),Nh=l(),Co=s("h2"),Fs=s("a"),Dd=s("span"),T(Cr.$$.fragment),Rb=l(),Fd=s("span"),Hb=a("TFDistilBertForTokenClassification"),Oh=l(),Le=s("div"),T(jr.$$.fragment),Qb=l(),yd=s("p"),Ub=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Jb=l(),Pr=s("p"),Vb=a("This model inherits from "),Si=s("a"),Kb=a("TFPreTrainedModel"),Gb=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xb=l(),qr=s("p"),Yb=a("This model is also a "),Ar=s("a"),Zb=a("tf.keras.Model"),ev=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),tv=l(),T(ys.$$.fragment),ov=l(),ot=s("div"),T(Ir.$$.fragment),sv=l(),jo=s("p"),nv=a("The "),Ni=s("a"),rv=a("TFDistilBertForTokenClassification"),av=a(" forward method, overrides the "),Bd=s("code"),iv=a("__call__"),lv=a(" special method."),dv=l(),T(Bs.$$.fragment),cv=l(),Md=s("p"),hv=a("Example:"),pv=l(),T(Lr.$$.fragment),Wh=l(),Po=s("h2"),Ms=s("a"),Ed=s("span"),T(Sr.$$.fragment),uv=l(),xd=s("span"),fv=a("TFDistilBertForQuestionAnswering"),Rh=l(),Se=s("div"),T(Nr.$$.fragment),mv=l(),qo=s("p"),gv=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),zd=s("code"),_v=a("span start logits"),bv=a(" and "),Cd=s("code"),vv=a("span end logits"),Tv=a(")."),kv=l(),Or=s("p"),wv=a("This model inherits from "),Oi=s("a"),$v=a("TFPreTrainedModel"),Dv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Fv=l(),Wr=s("p"),yv=a("This model is also a "),Rr=s("a"),Bv=a("tf.keras.Model"),Mv=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ev=l(),T(Es.$$.fragment),xv=l(),st=s("div"),T(Hr.$$.fragment),zv=l(),Ao=s("p"),Cv=a("The "),Wi=s("a"),jv=a("TFDistilBertForQuestionAnswering"),Pv=a(" forward method, overrides the "),jd=s("code"),qv=a("__call__"),Av=a(" special method."),Iv=l(),T(xs.$$.fragment),Lv=l(),Pd=s("p"),Sv=a("Example:"),Nv=l(),T(Qr.$$.fragment),Hh=l(),Io=s("h2"),zs=s("a"),qd=s("span"),T(Ur.$$.fragment),Ov=l(),Ad=s("span"),Wv=a("FlaxDistilBertModel"),Qh=l(),Be=s("div"),T(Jr.$$.fragment),Rv=l(),Id=s("p"),Hv=a("The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),Qv=l(),Vr=s("p"),Uv=a("This model inherits from "),Ri=s("a"),Jv=a("FlaxPreTrainedModel"),Vv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Kv=l(),Kr=s("p"),Gv=a("This model is also a Flax Linen "),Gr=s("a"),Xv=a("flax.linen.Module"),Yv=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Zv=l(),Ld=s("p"),eT=a("Finally, this model supports inherent JAX features such as:"),tT=l(),Bt=s("ul"),Sd=s("li"),Xr=s("a"),oT=a("Just-In-Time (JIT) compilation"),sT=l(),Nd=s("li"),Yr=s("a"),nT=a("Automatic Differentiation"),rT=l(),Od=s("li"),Zr=s("a"),aT=a("Vectorization"),iT=l(),Wd=s("li"),ea=s("a"),lT=a("Parallelization"),dT=l(),nt=s("div"),T(ta.$$.fragment),cT=l(),Lo=s("p"),hT=a("The "),Rd=s("code"),pT=a("FlaxDistilBertPreTrainedModel"),uT=a("forward method, overrides the "),Hd=s("code"),fT=a("__call__"),mT=a(" special method."),gT=l(),T(Cs.$$.fragment),_T=l(),Qd=s("p"),bT=a("Example:"),vT=l(),T(oa.$$.fragment),Uh=l(),So=s("h2"),js=s("a"),Ud=s("span"),T(sa.$$.fragment),TT=l(),Jd=s("span"),kT=a("FlaxDistilBertForMaskedLM"),Jh=l(),Me=s("div"),T(na.$$.fragment),wT=l(),ra=s("p"),$T=a("DistilBert Model with a "),Vd=s("code"),DT=a("language modeling"),FT=a(" head on top."),yT=l(),aa=s("p"),BT=a("This model inherits from "),Hi=s("a"),MT=a("FlaxPreTrainedModel"),ET=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),xT=l(),ia=s("p"),zT=a("This model is also a Flax Linen "),la=s("a"),CT=a("flax.linen.Module"),jT=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),PT=l(),Kd=s("p"),qT=a("Finally, this model supports inherent JAX features such as:"),AT=l(),Mt=s("ul"),Gd=s("li"),da=s("a"),IT=a("Just-In-Time (JIT) compilation"),LT=l(),Xd=s("li"),ca=s("a"),ST=a("Automatic Differentiation"),NT=l(),Yd=s("li"),ha=s("a"),OT=a("Vectorization"),WT=l(),Zd=s("li"),pa=s("a"),RT=a("Parallelization"),HT=l(),rt=s("div"),T(ua.$$.fragment),QT=l(),No=s("p"),UT=a("The "),ec=s("code"),JT=a("FlaxDistilBertPreTrainedModel"),VT=a("forward method, overrides the "),tc=s("code"),KT=a("__call__"),GT=a(" special method."),XT=l(),T(Ps.$$.fragment),YT=l(),oc=s("p"),ZT=a("Example:"),ek=l(),T(fa.$$.fragment),Vh=l(),Oo=s("h2"),qs=s("a"),sc=s("span"),T(ma.$$.fragment),tk=l(),nc=s("span"),ok=a("FlaxDistilBertForSequenceClassification"),Kh=l(),Ee=s("div"),T(ga.$$.fragment),sk=l(),rc=s("p"),nk=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),rk=l(),_a=s("p"),ak=a("This model inherits from "),Qi=s("a"),ik=a("FlaxPreTrainedModel"),lk=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),dk=l(),ba=s("p"),ck=a("This model is also a Flax Linen "),va=s("a"),hk=a("flax.linen.Module"),pk=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),uk=l(),ac=s("p"),fk=a("Finally, this model supports inherent JAX features such as:"),mk=l(),Et=s("ul"),ic=s("li"),Ta=s("a"),gk=a("Just-In-Time (JIT) compilation"),_k=l(),lc=s("li"),ka=s("a"),bk=a("Automatic Differentiation"),vk=l(),dc=s("li"),wa=s("a"),Tk=a("Vectorization"),kk=l(),cc=s("li"),$a=s("a"),wk=a("Parallelization"),$k=l(),at=s("div"),T(Da.$$.fragment),Dk=l(),Wo=s("p"),Fk=a("The "),hc=s("code"),yk=a("FlaxDistilBertPreTrainedModel"),Bk=a("forward method, overrides the "),pc=s("code"),Mk=a("__call__"),Ek=a(" special method."),xk=l(),T(As.$$.fragment),zk=l(),uc=s("p"),Ck=a("Example:"),jk=l(),T(Fa.$$.fragment),Gh=l(),Ro=s("h2"),Is=s("a"),fc=s("span"),T(ya.$$.fragment),Pk=l(),mc=s("span"),qk=a("FlaxDistilBertForMultipleChoice"),Xh=l(),xe=s("div"),T(Ba.$$.fragment),Ak=l(),gc=s("p"),Ik=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Lk=l(),Ma=s("p"),Sk=a("This model inherits from "),Ui=s("a"),Nk=a("FlaxPreTrainedModel"),Ok=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Wk=l(),Ea=s("p"),Rk=a("This model is also a Flax Linen "),xa=s("a"),Hk=a("flax.linen.Module"),Qk=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Uk=l(),_c=s("p"),Jk=a("Finally, this model supports inherent JAX features such as:"),Vk=l(),xt=s("ul"),bc=s("li"),za=s("a"),Kk=a("Just-In-Time (JIT) compilation"),Gk=l(),vc=s("li"),Ca=s("a"),Xk=a("Automatic Differentiation"),Yk=l(),Tc=s("li"),ja=s("a"),Zk=a("Vectorization"),ew=l(),kc=s("li"),Pa=s("a"),tw=a("Parallelization"),ow=l(),it=s("div"),T(qa.$$.fragment),sw=l(),Ho=s("p"),nw=a("The "),wc=s("code"),rw=a("FlaxDistilBertPreTrainedModel"),aw=a("forward method, overrides the "),$c=s("code"),iw=a("__call__"),lw=a(" special method."),dw=l(),T(Ls.$$.fragment),cw=l(),Dc=s("p"),hw=a("Example:"),pw=l(),T(Aa.$$.fragment),Yh=l(),Qo=s("h2"),Ss=s("a"),Fc=s("span"),T(Ia.$$.fragment),uw=l(),yc=s("span"),fw=a("FlaxDistilBertForTokenClassification"),Zh=l(),ze=s("div"),T(La.$$.fragment),mw=l(),Bc=s("p"),gw=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),_w=l(),Sa=s("p"),bw=a("This model inherits from "),Ji=s("a"),vw=a("FlaxPreTrainedModel"),Tw=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),kw=l(),Na=s("p"),ww=a("This model is also a Flax Linen "),Oa=s("a"),$w=a("flax.linen.Module"),Dw=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Fw=l(),Mc=s("p"),yw=a("Finally, this model supports inherent JAX features such as:"),Bw=l(),zt=s("ul"),Ec=s("li"),Wa=s("a"),Mw=a("Just-In-Time (JIT) compilation"),Ew=l(),xc=s("li"),Ra=s("a"),xw=a("Automatic Differentiation"),zw=l(),zc=s("li"),Ha=s("a"),Cw=a("Vectorization"),jw=l(),Cc=s("li"),Qa=s("a"),Pw=a("Parallelization"),qw=l(),lt=s("div"),T(Ua.$$.fragment),Aw=l(),Uo=s("p"),Iw=a("The "),jc=s("code"),Lw=a("FlaxDistilBertPreTrainedModel"),Sw=a("forward method, overrides the "),Pc=s("code"),Nw=a("__call__"),Ow=a(" special method."),Ww=l(),T(Ns.$$.fragment),Rw=l(),qc=s("p"),Hw=a("Example:"),Qw=l(),T(Ja.$$.fragment),ep=l(),Jo=s("h2"),Os=s("a"),Ac=s("span"),T(Va.$$.fragment),Uw=l(),Ic=s("span"),Jw=a("FlaxDistilBertForQuestionAnswering"),tp=l(),Ce=s("div"),T(Ka.$$.fragment),Vw=l(),Vo=s("p"),Kw=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Lc=s("code"),Gw=a("span start logits"),Xw=a(" and "),Sc=s("code"),Yw=a("span end logits"),Zw=a(")."),e1=l(),Ga=s("p"),t1=a("This model inherits from "),Vi=s("a"),o1=a("FlaxPreTrainedModel"),s1=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),n1=l(),Xa=s("p"),r1=a("This model is also a Flax Linen "),Ya=s("a"),a1=a("flax.linen.Module"),i1=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),l1=l(),Nc=s("p"),d1=a("Finally, this model supports inherent JAX features such as:"),c1=l(),Ct=s("ul"),Oc=s("li"),Za=s("a"),h1=a("Just-In-Time (JIT) compilation"),p1=l(),Wc=s("li"),ei=s("a"),u1=a("Automatic Differentiation"),f1=l(),Rc=s("li"),ti=s("a"),m1=a("Vectorization"),g1=l(),Hc=s("li"),oi=s("a"),_1=a("Parallelization"),b1=l(),dt=s("div"),T(si.$$.fragment),v1=l(),Ko=s("p"),T1=a("The "),Qc=s("code"),k1=a("FlaxDistilBertPreTrainedModel"),w1=a("forward method, overrides the "),Uc=s("code"),$1=a("__call__"),D1=a(" special method."),F1=l(),T(Ws.$$.fragment),y1=l(),Jc=s("p"),B1=a("Example:"),M1=l(),T(ni.$$.fragment),this.h()},l(o){const m=ly('[data-svelte="svelte-1phssyn"]',document.head);h=n(m,"META",{name:!0,content:!0}),m.forEach(t),y=d(o),g=n(o,"H1",{class:!0});var ri=r(g);b=n(ri,"A",{id:!0,class:!0,href:!0});var Vc=r(b);v=n(Vc,"SPAN",{});var Kc=r(v);k(_.$$.fragment,Kc),Kc.forEach(t),Vc.forEach(t),f=d(ri),B=n(ri,"SPAN",{});var Gc=r(B);de=i(Gc,"DistilBERT"),Gc.forEach(t),ri.forEach(t),J=d(o),E=n(o,"H2",{class:!0});var ai=r(E);G=n(ai,"A",{id:!0,class:!0,href:!0});var Xc=r(G);N=n(Xc,"SPAN",{});var Yc=r(N);k(X.$$.fragment,Yc),Yc.forEach(t),Xc.forEach(t),ce=d(ai),O=n(ai,"SPAN",{});var Zc=r(O);he=i(Zc,"Overview"),Zc.forEach(t),ai.forEach(t),re=d(o),L=n(o,"P",{});var jt=r(L);q=i(jt,"The DistilBERT model was proposed in the blog post "),Y=n(jt,"A",{href:!0,rel:!0});var eh=r(Y);V=i(eh,`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),eh.forEach(t),x=i(jt,", and the paper "),z=n(jt,"A",{href:!0,rel:!0});var th=r(z);pe=i(th,`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),th.forEach(t),W=i(jt,`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),oe=n(jt,"EM",{});var oh=r(oe);ue=i(oh,"bert-base-uncased"),oh.forEach(t),R=i(jt,`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),jt.forEach(t),ae=d(o),ee=n(o,"P",{});var sh=r(ee);A=i(sh,"The abstract from the paper is the following:"),sh.forEach(t),ie=d(o),S=n(o,"P",{});var nh=r(S);se=n(nh,"EM",{});var rh=r(se);fe=i(rh,`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),rh.forEach(t),nh.forEach(t),P=d(o),te=n(o,"P",{});var ah=r(te);H=i(ah,"Tips:"),ah.forEach(t),le=d(o),p=n(o,"UL",{});var ii=r(p);M=n(ii,"LI",{});var Pt=r(M);K=i(Pt,"DistilBERT doesn\u2019t have "),ge=n(Pt,"CODE",{});var ih=r(ge);ve=i(ih,"token_type_ids"),ih.forEach(t),I=i(Pt,`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),_e=n(Pt,"CODE",{});var lh=r(_e);Te=i(lh,"tokenizer.sep_token"),lh.forEach(t),ke=i(Pt," (or "),j=n(Pt,"CODE",{});var dh=r(j);Q=i(dh,"[SEP]"),dh.forEach(t),we=i(Pt,")."),Pt.forEach(t),$e=d(ii),Z=n(ii,"LI",{});var li=r(Z);De=i(li,"DistilBERT doesn\u2019t have options to select the input positions ("),ne=n(li,"CODE",{});var ch=r(ne);Fe=i(ch,"position_ids"),ch.forEach(t),pu=i(li,` input). This could be added if
necessary though, just let us know if you need this option.`),li.forEach(t),ii.forEach(t),uh=d(o),vt=n(o,"P",{});var qt=r(vt);uu=i(qt,"This model was contributed by "),Ys=n(qt,"A",{href:!0,rel:!0});var E1=r(Ys);fu=i(E1,"victorsanh"),E1.forEach(t),mu=i(qt,`. This model jax version was
contributed by `),Zs=n(qt,"A",{href:!0,rel:!0});var x1=r(Zs);gu=i(x1,"kamalkraj"),x1.forEach(t),_u=i(qt,". The original code can be found "),en=n(qt,"A",{href:!0,rel:!0});var z1=r(en);bu=i(z1,"here"),z1.forEach(t),vu=i(qt,"."),qt.forEach(t),fh=d(o),ao=n(o,"H2",{class:!0});var sp=r(ao);Go=n(sp,"A",{id:!0,class:!0,href:!0});var C1=r(Go);_l=n(C1,"SPAN",{});var j1=r(_l);k(tn.$$.fragment,j1),j1.forEach(t),C1.forEach(t),Tu=d(sp),bl=n(sp,"SPAN",{});var P1=r(bl);ku=i(P1,"DistilBertConfig"),P1.forEach(t),sp.forEach(t),mh=d(o),Ne=n(o,"DIV",{class:!0});var At=r(Ne);k(on.$$.fragment,At),wu=d(At),yt=n(At,"P",{});var Rs=r(yt);$u=i(Rs,"This is the configuration class to store the configuration of a "),di=n(Rs,"A",{href:!0});var q1=r(di);Du=i(q1,"DistilBertModel"),q1.forEach(t),Fu=i(Rs," or a "),ci=n(Rs,"A",{href:!0});var A1=r(ci);yu=i(A1,"TFDistilBertModel"),A1.forEach(t),Bu=i(Rs,`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),sn=n(Rs,"A",{href:!0,rel:!0});var I1=r(sn);Mu=i(I1,"distilbert-base-uncased"),I1.forEach(t),Eu=i(Rs," architecture."),Rs.forEach(t),xu=d(At),io=n(At,"P",{});var Ki=r(io);zu=i(Ki,"Configuration objects inherit from "),hi=n(Ki,"A",{href:!0});var L1=r(hi);Cu=i(L1,"PretrainedConfig"),L1.forEach(t),ju=i(Ki,` and can be used to control the model outputs. Read the
documentation from `),pi=n(Ki,"A",{href:!0});var S1=r(pi);Pu=i(S1,"PretrainedConfig"),S1.forEach(t),qu=i(Ki," for more information."),Ki.forEach(t),Au=d(At),vl=n(At,"P",{});var N1=r(vl);Iu=i(N1,"Examples:"),N1.forEach(t),Lu=d(At),k(nn.$$.fragment,At),At.forEach(t),gh=d(o),lo=n(o,"H2",{class:!0});var np=r(lo);Xo=n(np,"A",{id:!0,class:!0,href:!0});var O1=r(Xo);Tl=n(O1,"SPAN",{});var W1=r(Tl);k(rn.$$.fragment,W1),W1.forEach(t),O1.forEach(t),Su=d(np),kl=n(np,"SPAN",{});var R1=r(kl);Nu=i(R1,"DistilBertTokenizer"),R1.forEach(t),np.forEach(t),_h=d(o),_t=n(o,"DIV",{class:!0});var Hs=r(_t);k(an.$$.fragment,Hs),Ou=d(Hs),wl=n(Hs,"P",{});var H1=r(wl);Wu=i(H1,"Construct a DistilBERT tokenizer."),H1.forEach(t),Ru=d(Hs),Yo=n(Hs,"P",{});var hh=r(Yo);ui=n(hh,"A",{href:!0});var Q1=r(ui);Hu=i(Q1,"DistilBertTokenizer"),Q1.forEach(t),Qu=i(hh," is identical to "),fi=n(hh,"A",{href:!0});var U1=r(fi);Uu=i(U1,"BertTokenizer"),U1.forEach(t),Ju=i(hh,` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),hh.forEach(t),Vu=d(Hs),ln=n(Hs,"P",{});var rp=r(ln);Ku=i(rp,"Refer to superclass "),mi=n(rp,"A",{href:!0});var J1=r(mi);Gu=i(J1,"BertTokenizer"),J1.forEach(t),Xu=i(rp," for usage examples and documentation concerning parameters."),rp.forEach(t),Hs.forEach(t),bh=d(o),co=n(o,"H2",{class:!0});var ap=r(co);Zo=n(ap,"A",{id:!0,class:!0,href:!0});var V1=r(Zo);$l=n(V1,"SPAN",{});var K1=r($l);k(dn.$$.fragment,K1),K1.forEach(t),V1.forEach(t),Yu=d(ap),Dl=n(ap,"SPAN",{});var G1=r(Dl);Zu=i(G1,"DistilBertTokenizerFast"),G1.forEach(t),ap.forEach(t),vh=d(o),bt=n(o,"DIV",{class:!0});var Qs=r(bt);k(cn.$$.fragment,Qs),ef=d(Qs),hn=n(Qs,"P",{});var ip=r(hn);tf=i(ip,"Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),Fl=n(ip,"EM",{});var X1=r(Fl);of=i(X1,"tokenizers"),X1.forEach(t),sf=i(ip," library)."),ip.forEach(t),nf=d(Qs),es=n(Qs,"P",{});var ph=r(es);gi=n(ph,"A",{href:!0});var Y1=r(gi);rf=i(Y1,"DistilBertTokenizerFast"),Y1.forEach(t),af=i(ph," is identical to "),_i=n(ph,"A",{href:!0});var Z1=r(_i);lf=i(Z1,"BertTokenizerFast"),Z1.forEach(t),df=i(ph,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ph.forEach(t),cf=d(Qs),pn=n(Qs,"P",{});var lp=r(pn);hf=i(lp,"Refer to superclass "),bi=n(lp,"A",{href:!0});var e$=r(bi);pf=i(e$,"BertTokenizerFast"),e$.forEach(t),uf=i(lp," for usage examples and documentation concerning parameters."),lp.forEach(t),Qs.forEach(t),Th=d(o),ho=n(o,"H2",{class:!0});var dp=r(ho);ts=n(dp,"A",{id:!0,class:!0,href:!0});var t$=r(ts);yl=n(t$,"SPAN",{});var o$=r(yl);k(un.$$.fragment,o$),o$.forEach(t),t$.forEach(t),ff=d(dp),Bl=n(dp,"SPAN",{});var s$=r(Bl);mf=i(s$,"DistilBertModel"),s$.forEach(t),dp.forEach(t),kh=d(o),Oe=n(o,"DIV",{class:!0});var It=r(Oe);k(fn.$$.fragment,It),gf=d(It),Ml=n(It,"P",{});var n$=r(Ml);_f=i(n$,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),n$.forEach(t),bf=d(It),mn=n(It,"P",{});var cp=r(mn);vf=i(cp,"This model inherits from "),vi=n(cp,"A",{href:!0});var r$=r(vi);Tf=i(r$,"PreTrainedModel"),r$.forEach(t),kf=i(cp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),cp.forEach(t),wf=d(It),gn=n(It,"P",{});var hp=r(gn);$f=i(hp,"This model is also a PyTorch "),_n=n(hp,"A",{href:!0,rel:!0});var a$=r(_n);Df=i(a$,"torch.nn.Module"),a$.forEach(t),Ff=i(hp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),hp.forEach(t),yf=d(It),Je=n(It,"DIV",{class:!0});var Lt=r(Je);k(bn.$$.fragment,Lt),Bf=d(Lt),po=n(Lt,"P",{});var Gi=r(po);Mf=i(Gi,"The "),Ti=n(Gi,"A",{href:!0});var i$=r(Ti);Ef=i(i$,"DistilBertModel"),i$.forEach(t),xf=i(Gi," forward method, overrides the "),El=n(Gi,"CODE",{});var l$=r(El);zf=i(l$,"__call__"),l$.forEach(t),Cf=i(Gi," special method."),Gi.forEach(t),jf=d(Lt),k(os.$$.fragment,Lt),Pf=d(Lt),xl=n(Lt,"P",{});var d$=r(xl);qf=i(d$,"Example:"),d$.forEach(t),Af=d(Lt),k(vn.$$.fragment,Lt),Lt.forEach(t),It.forEach(t),wh=d(o),uo=n(o,"H2",{class:!0});var pp=r(uo);ss=n(pp,"A",{id:!0,class:!0,href:!0});var c$=r(ss);zl=n(c$,"SPAN",{});var h$=r(zl);k(Tn.$$.fragment,h$),h$.forEach(t),c$.forEach(t),If=d(pp),Cl=n(pp,"SPAN",{});var p$=r(Cl);Lf=i(p$,"DistilBertForMaskedLM"),p$.forEach(t),pp.forEach(t),$h=d(o),We=n(o,"DIV",{class:!0});var St=r(We);k(kn.$$.fragment,St),Sf=d(St),wn=n(St,"P",{});var up=r(wn);Nf=i(up,"DistilBert Model with a "),jl=n(up,"CODE",{});var u$=r(jl);Of=i(u$,"masked language modeling"),u$.forEach(t),Wf=i(up," head on top."),up.forEach(t),Rf=d(St),$n=n(St,"P",{});var fp=r($n);Hf=i(fp,"This model inherits from "),ki=n(fp,"A",{href:!0});var f$=r(ki);Qf=i(f$,"PreTrainedModel"),f$.forEach(t),Uf=i(fp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),fp.forEach(t),Jf=d(St),Dn=n(St,"P",{});var mp=r(Dn);Vf=i(mp,"This model is also a PyTorch "),Fn=n(mp,"A",{href:!0,rel:!0});var m$=r(Fn);Kf=i(m$,"torch.nn.Module"),m$.forEach(t),Gf=i(mp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),mp.forEach(t),Xf=d(St),Ve=n(St,"DIV",{class:!0});var Nt=r(Ve);k(yn.$$.fragment,Nt),Yf=d(Nt),fo=n(Nt,"P",{});var Xi=r(fo);Zf=i(Xi,"The "),wi=n(Xi,"A",{href:!0});var g$=r(wi);em=i(g$,"DistilBertForMaskedLM"),g$.forEach(t),tm=i(Xi," forward method, overrides the "),Pl=n(Xi,"CODE",{});var _$=r(Pl);om=i(_$,"__call__"),_$.forEach(t),sm=i(Xi," special method."),Xi.forEach(t),nm=d(Nt),k(ns.$$.fragment,Nt),rm=d(Nt),ql=n(Nt,"P",{});var b$=r(ql);am=i(b$,"Example:"),b$.forEach(t),im=d(Nt),k(Bn.$$.fragment,Nt),Nt.forEach(t),St.forEach(t),Dh=d(o),mo=n(o,"H2",{class:!0});var gp=r(mo);rs=n(gp,"A",{id:!0,class:!0,href:!0});var v$=r(rs);Al=n(v$,"SPAN",{});var T$=r(Al);k(Mn.$$.fragment,T$),T$.forEach(t),v$.forEach(t),lm=d(gp),Il=n(gp,"SPAN",{});var k$=r(Il);dm=i(k$,"DistilBertForSequenceClassification"),k$.forEach(t),gp.forEach(t),Fh=d(o),Re=n(o,"DIV",{class:!0});var Ot=r(Re);k(En.$$.fragment,Ot),cm=d(Ot),Ll=n(Ot,"P",{});var w$=r(Ll);hm=i(w$,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),w$.forEach(t),pm=d(Ot),xn=n(Ot,"P",{});var _p=r(xn);um=i(_p,"This model inherits from "),$i=n(_p,"A",{href:!0});var $$=r($i);fm=i($$,"PreTrainedModel"),$$.forEach(t),mm=i(_p,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_p.forEach(t),gm=d(Ot),zn=n(Ot,"P",{});var bp=r(zn);_m=i(bp,"This model is also a PyTorch "),Cn=n(bp,"A",{href:!0,rel:!0});var D$=r(Cn);bm=i(D$,"torch.nn.Module"),D$.forEach(t),vm=i(bp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),bp.forEach(t),Tm=d(Ot),je=n(Ot,"DIV",{class:!0});var ct=r(je);k(jn.$$.fragment,ct),km=d(ct),go=n(ct,"P",{});var Yi=r(go);wm=i(Yi,"The "),Di=n(Yi,"A",{href:!0});var F$=r(Di);$m=i(F$,"DistilBertForSequenceClassification"),F$.forEach(t),Dm=i(Yi," forward method, overrides the "),Sl=n(Yi,"CODE",{});var y$=r(Sl);Fm=i(y$,"__call__"),y$.forEach(t),ym=i(Yi," special method."),Yi.forEach(t),Bm=d(ct),k(as.$$.fragment,ct),Mm=d(ct),Nl=n(ct,"P",{});var B$=r(Nl);Em=i(B$,"Example of single-label classification:"),B$.forEach(t),xm=d(ct),k(Pn.$$.fragment,ct),zm=d(ct),Ol=n(ct,"P",{});var M$=r(Ol);Cm=i(M$,"Example of multi-label classification:"),M$.forEach(t),jm=d(ct),k(qn.$$.fragment,ct),ct.forEach(t),Ot.forEach(t),yh=d(o),_o=n(o,"H2",{class:!0});var vp=r(_o);is=n(vp,"A",{id:!0,class:!0,href:!0});var E$=r(is);Wl=n(E$,"SPAN",{});var x$=r(Wl);k(An.$$.fragment,x$),x$.forEach(t),E$.forEach(t),Pm=d(vp),Rl=n(vp,"SPAN",{});var z$=r(Rl);qm=i(z$,"DistilBertForMultipleChoice"),z$.forEach(t),vp.forEach(t),Bh=d(o),He=n(o,"DIV",{class:!0});var Wt=r(He);k(In.$$.fragment,Wt),Am=d(Wt),Hl=n(Wt,"P",{});var C$=r(Hl);Im=i(C$,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),C$.forEach(t),Lm=d(Wt),Ln=n(Wt,"P",{});var Tp=r(Ln);Sm=i(Tp,"This model inherits from "),Fi=n(Tp,"A",{href:!0});var j$=r(Fi);Nm=i(j$,"PreTrainedModel"),j$.forEach(t),Om=i(Tp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Tp.forEach(t),Wm=d(Wt),Sn=n(Wt,"P",{});var kp=r(Sn);Rm=i(kp,"This model is also a PyTorch "),Nn=n(kp,"A",{href:!0,rel:!0});var P$=r(Nn);Hm=i(P$,"torch.nn.Module"),P$.forEach(t),Qm=i(kp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),kp.forEach(t),Um=d(Wt),Ke=n(Wt,"DIV",{class:!0});var Rt=r(Ke);k(On.$$.fragment,Rt),Jm=d(Rt),bo=n(Rt,"P",{});var Zi=r(bo);Vm=i(Zi,"The "),yi=n(Zi,"A",{href:!0});var q$=r(yi);Km=i(q$,"DistilBertForMultipleChoice"),q$.forEach(t),Gm=i(Zi," forward method, overrides the "),Ql=n(Zi,"CODE",{});var A$=r(Ql);Xm=i(A$,"__call__"),A$.forEach(t),Ym=i(Zi," special method."),Zi.forEach(t),Zm=d(Rt),k(ls.$$.fragment,Rt),eg=d(Rt),Ul=n(Rt,"P",{});var I$=r(Ul);tg=i(I$,"Examples:"),I$.forEach(t),og=d(Rt),k(Wn.$$.fragment,Rt),Rt.forEach(t),Wt.forEach(t),Mh=d(o),vo=n(o,"H2",{class:!0});var wp=r(vo);ds=n(wp,"A",{id:!0,class:!0,href:!0});var L$=r(ds);Jl=n(L$,"SPAN",{});var S$=r(Jl);k(Rn.$$.fragment,S$),S$.forEach(t),L$.forEach(t),sg=d(wp),Vl=n(wp,"SPAN",{});var N$=r(Vl);ng=i(N$,"DistilBertForTokenClassification"),N$.forEach(t),wp.forEach(t),Eh=d(o),Qe=n(o,"DIV",{class:!0});var Ht=r(Qe);k(Hn.$$.fragment,Ht),rg=d(Ht),Kl=n(Ht,"P",{});var O$=r(Kl);ag=i(O$,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),O$.forEach(t),ig=d(Ht),Qn=n(Ht,"P",{});var $p=r(Qn);lg=i($p,"This model inherits from "),Bi=n($p,"A",{href:!0});var W$=r(Bi);dg=i(W$,"PreTrainedModel"),W$.forEach(t),cg=i($p,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),$p.forEach(t),hg=d(Ht),Un=n(Ht,"P",{});var Dp=r(Un);pg=i(Dp,"This model is also a PyTorch "),Jn=n(Dp,"A",{href:!0,rel:!0});var R$=r(Jn);ug=i(R$,"torch.nn.Module"),R$.forEach(t),fg=i(Dp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Dp.forEach(t),mg=d(Ht),Ge=n(Ht,"DIV",{class:!0});var Qt=r(Ge);k(Vn.$$.fragment,Qt),gg=d(Qt),To=n(Qt,"P",{});var el=r(To);_g=i(el,"The "),Mi=n(el,"A",{href:!0});var H$=r(Mi);bg=i(H$,"DistilBertForTokenClassification"),H$.forEach(t),vg=i(el," forward method, overrides the "),Gl=n(el,"CODE",{});var Q$=r(Gl);Tg=i(Q$,"__call__"),Q$.forEach(t),kg=i(el," special method."),el.forEach(t),wg=d(Qt),k(cs.$$.fragment,Qt),$g=d(Qt),Xl=n(Qt,"P",{});var U$=r(Xl);Dg=i(U$,"Example:"),U$.forEach(t),Fg=d(Qt),k(Kn.$$.fragment,Qt),Qt.forEach(t),Ht.forEach(t),xh=d(o),ko=n(o,"H2",{class:!0});var Fp=r(ko);hs=n(Fp,"A",{id:!0,class:!0,href:!0});var J$=r(hs);Yl=n(J$,"SPAN",{});var V$=r(Yl);k(Gn.$$.fragment,V$),V$.forEach(t),J$.forEach(t),yg=d(Fp),Zl=n(Fp,"SPAN",{});var K$=r(Zl);Bg=i(K$,"DistilBertForQuestionAnswering"),K$.forEach(t),Fp.forEach(t),zh=d(o),Ue=n(o,"DIV",{class:!0});var Ut=r(Ue);k(Xn.$$.fragment,Ut),Mg=d(Ut),wo=n(Ut,"P",{});var tl=r(wo);Eg=i(tl,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),ed=n(tl,"CODE",{});var G$=r(ed);xg=i(G$,"span start logits"),G$.forEach(t),zg=i(tl," and "),td=n(tl,"CODE",{});var X$=r(td);Cg=i(X$,"span end logits"),X$.forEach(t),jg=i(tl,")."),tl.forEach(t),Pg=d(Ut),Yn=n(Ut,"P",{});var yp=r(Yn);qg=i(yp,"This model inherits from "),Ei=n(yp,"A",{href:!0});var Y$=r(Ei);Ag=i(Y$,"PreTrainedModel"),Y$.forEach(t),Ig=i(yp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),yp.forEach(t),Lg=d(Ut),Zn=n(Ut,"P",{});var Bp=r(Zn);Sg=i(Bp,"This model is also a PyTorch "),er=n(Bp,"A",{href:!0,rel:!0});var Z$=r(er);Ng=i(Z$,"torch.nn.Module"),Z$.forEach(t),Og=i(Bp,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bp.forEach(t),Wg=d(Ut),Xe=n(Ut,"DIV",{class:!0});var Jt=r(Xe);k(tr.$$.fragment,Jt),Rg=d(Jt),$o=n(Jt,"P",{});var ol=r($o);Hg=i(ol,"The "),xi=n(ol,"A",{href:!0});var eD=r(xi);Qg=i(eD,"DistilBertForQuestionAnswering"),eD.forEach(t),Ug=i(ol," forward method, overrides the "),od=n(ol,"CODE",{});var tD=r(od);Jg=i(tD,"__call__"),tD.forEach(t),Vg=i(ol," special method."),ol.forEach(t),Kg=d(Jt),k(ps.$$.fragment,Jt),Gg=d(Jt),sd=n(Jt,"P",{});var oD=r(sd);Xg=i(oD,"Example:"),oD.forEach(t),Yg=d(Jt),k(or.$$.fragment,Jt),Jt.forEach(t),Ut.forEach(t),Ch=d(o),Do=n(o,"H2",{class:!0});var Mp=r(Do);us=n(Mp,"A",{id:!0,class:!0,href:!0});var sD=r(us);nd=n(sD,"SPAN",{});var nD=r(nd);k(sr.$$.fragment,nD),nD.forEach(t),sD.forEach(t),Zg=d(Mp),rd=n(Mp,"SPAN",{});var rD=r(rd);e_=i(rD,"TFDistilBertModel"),rD.forEach(t),Mp.forEach(t),jh=d(o),Pe=n(o,"DIV",{class:!0});var Tt=r(Pe);k(nr.$$.fragment,Tt),t_=d(Tt),ad=n(Tt,"P",{});var aD=r(ad);o_=i(aD,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),aD.forEach(t),s_=d(Tt),rr=n(Tt,"P",{});var Ep=r(rr);n_=i(Ep,"This model inherits from "),zi=n(Ep,"A",{href:!0});var iD=r(zi);r_=i(iD,"TFPreTrainedModel"),iD.forEach(t),a_=i(Ep,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ep.forEach(t),i_=d(Tt),ar=n(Tt,"P",{});var xp=r(ar);l_=i(xp,"This model is also a "),ir=n(xp,"A",{href:!0,rel:!0});var lD=r(ir);d_=i(lD,"tf.keras.Model"),lD.forEach(t),c_=i(xp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),xp.forEach(t),h_=d(Tt),k(fs.$$.fragment,Tt),p_=d(Tt),Ye=n(Tt,"DIV",{class:!0});var Vt=r(Ye);k(lr.$$.fragment,Vt),u_=d(Vt),Fo=n(Vt,"P",{});var sl=r(Fo);f_=i(sl,"The "),Ci=n(sl,"A",{href:!0});var dD=r(Ci);m_=i(dD,"TFDistilBertModel"),dD.forEach(t),g_=i(sl," forward method, overrides the "),id=n(sl,"CODE",{});var cD=r(id);__=i(cD,"__call__"),cD.forEach(t),b_=i(sl," special method."),sl.forEach(t),v_=d(Vt),k(ms.$$.fragment,Vt),T_=d(Vt),ld=n(Vt,"P",{});var hD=r(ld);k_=i(hD,"Example:"),hD.forEach(t),w_=d(Vt),k(dr.$$.fragment,Vt),Vt.forEach(t),Tt.forEach(t),Ph=d(o),yo=n(o,"H2",{class:!0});var zp=r(yo);gs=n(zp,"A",{id:!0,class:!0,href:!0});var pD=r(gs);dd=n(pD,"SPAN",{});var uD=r(dd);k(cr.$$.fragment,uD),uD.forEach(t),pD.forEach(t),$_=d(zp),cd=n(zp,"SPAN",{});var fD=r(cd);D_=i(fD,"TFDistilBertForMaskedLM"),fD.forEach(t),zp.forEach(t),qh=d(o),qe=n(o,"DIV",{class:!0});var kt=r(qe);k(hr.$$.fragment,kt),F_=d(kt),pr=n(kt,"P",{});var Cp=r(pr);y_=i(Cp,"DistilBert Model with a "),hd=n(Cp,"CODE",{});var mD=r(hd);B_=i(mD,"masked language modeling"),mD.forEach(t),M_=i(Cp," head on top."),Cp.forEach(t),E_=d(kt),ur=n(kt,"P",{});var jp=r(ur);x_=i(jp,"This model inherits from "),ji=n(jp,"A",{href:!0});var gD=r(ji);z_=i(gD,"TFPreTrainedModel"),gD.forEach(t),C_=i(jp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jp.forEach(t),j_=d(kt),fr=n(kt,"P",{});var Pp=r(fr);P_=i(Pp,"This model is also a "),mr=n(Pp,"A",{href:!0,rel:!0});var _D=r(mr);q_=i(_D,"tf.keras.Model"),_D.forEach(t),A_=i(Pp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Pp.forEach(t),I_=d(kt),k(_s.$$.fragment,kt),L_=d(kt),Ze=n(kt,"DIV",{class:!0});var Kt=r(Ze);k(gr.$$.fragment,Kt),S_=d(Kt),Bo=n(Kt,"P",{});var nl=r(Bo);N_=i(nl,"The "),Pi=n(nl,"A",{href:!0});var bD=r(Pi);O_=i(bD,"TFDistilBertForMaskedLM"),bD.forEach(t),W_=i(nl," forward method, overrides the "),pd=n(nl,"CODE",{});var vD=r(pd);R_=i(vD,"__call__"),vD.forEach(t),H_=i(nl," special method."),nl.forEach(t),Q_=d(Kt),k(bs.$$.fragment,Kt),U_=d(Kt),ud=n(Kt,"P",{});var TD=r(ud);J_=i(TD,"Example:"),TD.forEach(t),V_=d(Kt),k(_r.$$.fragment,Kt),Kt.forEach(t),kt.forEach(t),Ah=d(o),Mo=n(o,"H2",{class:!0});var qp=r(Mo);vs=n(qp,"A",{id:!0,class:!0,href:!0});var kD=r(vs);fd=n(kD,"SPAN",{});var wD=r(fd);k(br.$$.fragment,wD),wD.forEach(t),kD.forEach(t),K_=d(qp),md=n(qp,"SPAN",{});var $D=r(md);G_=i($D,"TFDistilBertForSequenceClassification"),$D.forEach(t),qp.forEach(t),Ih=d(o),Ae=n(o,"DIV",{class:!0});var wt=r(Ae);k(vr.$$.fragment,wt),X_=d(wt),gd=n(wt,"P",{});var DD=r(gd);Y_=i(DD,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),DD.forEach(t),Z_=d(wt),Tr=n(wt,"P",{});var Ap=r(Tr);eb=i(Ap,"This model inherits from "),qi=n(Ap,"A",{href:!0});var FD=r(qi);tb=i(FD,"TFPreTrainedModel"),FD.forEach(t),ob=i(Ap,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ap.forEach(t),sb=d(wt),kr=n(wt,"P",{});var Ip=r(kr);nb=i(Ip,"This model is also a "),wr=n(Ip,"A",{href:!0,rel:!0});var yD=r(wr);rb=i(yD,"tf.keras.Model"),yD.forEach(t),ab=i(Ip,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ip.forEach(t),ib=d(wt),k(Ts.$$.fragment,wt),lb=d(wt),et=n(wt,"DIV",{class:!0});var Gt=r(et);k($r.$$.fragment,Gt),db=d(Gt),Eo=n(Gt,"P",{});var rl=r(Eo);cb=i(rl,"The "),Ai=n(rl,"A",{href:!0});var BD=r(Ai);hb=i(BD,"TFDistilBertForSequenceClassification"),BD.forEach(t),pb=i(rl," forward method, overrides the "),_d=n(rl,"CODE",{});var MD=r(_d);ub=i(MD,"__call__"),MD.forEach(t),fb=i(rl," special method."),rl.forEach(t),mb=d(Gt),k(ks.$$.fragment,Gt),gb=d(Gt),bd=n(Gt,"P",{});var ED=r(bd);_b=i(ED,"Example:"),ED.forEach(t),bb=d(Gt),k(Dr.$$.fragment,Gt),Gt.forEach(t),wt.forEach(t),Lh=d(o),xo=n(o,"H2",{class:!0});var Lp=r(xo);ws=n(Lp,"A",{id:!0,class:!0,href:!0});var xD=r(ws);vd=n(xD,"SPAN",{});var zD=r(vd);k(Fr.$$.fragment,zD),zD.forEach(t),xD.forEach(t),vb=d(Lp),Td=n(Lp,"SPAN",{});var CD=r(Td);Tb=i(CD,"TFDistilBertForMultipleChoice"),CD.forEach(t),Lp.forEach(t),Sh=d(o),Ie=n(o,"DIV",{class:!0});var $t=r(Ie);k(yr.$$.fragment,$t),kb=d($t),kd=n($t,"P",{});var jD=r(kd);wb=i(jD,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),jD.forEach(t),$b=d($t),Br=n($t,"P",{});var Sp=r(Br);Db=i(Sp,"This model inherits from "),Ii=n(Sp,"A",{href:!0});var PD=r(Ii);Fb=i(PD,"TFPreTrainedModel"),PD.forEach(t),yb=i(Sp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Sp.forEach(t),Bb=d($t),Mr=n($t,"P",{});var Np=r(Mr);Mb=i(Np,"This model is also a "),Er=n(Np,"A",{href:!0,rel:!0});var qD=r(Er);Eb=i(qD,"tf.keras.Model"),qD.forEach(t),xb=i(Np,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Np.forEach(t),zb=d($t),k($s.$$.fragment,$t),Cb=d($t),tt=n($t,"DIV",{class:!0});var Xt=r(tt);k(xr.$$.fragment,Xt),jb=d(Xt),zo=n(Xt,"P",{});var al=r(zo);Pb=i(al,"The "),Li=n(al,"A",{href:!0});var AD=r(Li);qb=i(AD,"TFDistilBertForMultipleChoice"),AD.forEach(t),Ab=i(al," forward method, overrides the "),wd=n(al,"CODE",{});var ID=r(wd);Ib=i(ID,"__call__"),ID.forEach(t),Lb=i(al," special method."),al.forEach(t),Sb=d(Xt),k(Ds.$$.fragment,Xt),Nb=d(Xt),$d=n(Xt,"P",{});var LD=r($d);Ob=i(LD,"Example:"),LD.forEach(t),Wb=d(Xt),k(zr.$$.fragment,Xt),Xt.forEach(t),$t.forEach(t),Nh=d(o),Co=n(o,"H2",{class:!0});var Op=r(Co);Fs=n(Op,"A",{id:!0,class:!0,href:!0});var SD=r(Fs);Dd=n(SD,"SPAN",{});var ND=r(Dd);k(Cr.$$.fragment,ND),ND.forEach(t),SD.forEach(t),Rb=d(Op),Fd=n(Op,"SPAN",{});var OD=r(Fd);Hb=i(OD,"TFDistilBertForTokenClassification"),OD.forEach(t),Op.forEach(t),Oh=d(o),Le=n(o,"DIV",{class:!0});var Dt=r(Le);k(jr.$$.fragment,Dt),Qb=d(Dt),yd=n(Dt,"P",{});var WD=r(yd);Ub=i(WD,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),WD.forEach(t),Jb=d(Dt),Pr=n(Dt,"P",{});var Wp=r(Pr);Vb=i(Wp,"This model inherits from "),Si=n(Wp,"A",{href:!0});var RD=r(Si);Kb=i(RD,"TFPreTrainedModel"),RD.forEach(t),Gb=i(Wp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wp.forEach(t),Xb=d(Dt),qr=n(Dt,"P",{});var Rp=r(qr);Yb=i(Rp,"This model is also a "),Ar=n(Rp,"A",{href:!0,rel:!0});var HD=r(Ar);Zb=i(HD,"tf.keras.Model"),HD.forEach(t),ev=i(Rp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Rp.forEach(t),tv=d(Dt),k(ys.$$.fragment,Dt),ov=d(Dt),ot=n(Dt,"DIV",{class:!0});var Yt=r(ot);k(Ir.$$.fragment,Yt),sv=d(Yt),jo=n(Yt,"P",{});var il=r(jo);nv=i(il,"The "),Ni=n(il,"A",{href:!0});var QD=r(Ni);rv=i(QD,"TFDistilBertForTokenClassification"),QD.forEach(t),av=i(il," forward method, overrides the "),Bd=n(il,"CODE",{});var UD=r(Bd);iv=i(UD,"__call__"),UD.forEach(t),lv=i(il," special method."),il.forEach(t),dv=d(Yt),k(Bs.$$.fragment,Yt),cv=d(Yt),Md=n(Yt,"P",{});var JD=r(Md);hv=i(JD,"Example:"),JD.forEach(t),pv=d(Yt),k(Lr.$$.fragment,Yt),Yt.forEach(t),Dt.forEach(t),Wh=d(o),Po=n(o,"H2",{class:!0});var Hp=r(Po);Ms=n(Hp,"A",{id:!0,class:!0,href:!0});var VD=r(Ms);Ed=n(VD,"SPAN",{});var KD=r(Ed);k(Sr.$$.fragment,KD),KD.forEach(t),VD.forEach(t),uv=d(Hp),xd=n(Hp,"SPAN",{});var GD=r(xd);fv=i(GD,"TFDistilBertForQuestionAnswering"),GD.forEach(t),Hp.forEach(t),Rh=d(o),Se=n(o,"DIV",{class:!0});var Ft=r(Se);k(Nr.$$.fragment,Ft),mv=d(Ft),qo=n(Ft,"P",{});var ll=r(qo);gv=i(ll,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),zd=n(ll,"CODE",{});var XD=r(zd);_v=i(XD,"span start logits"),XD.forEach(t),bv=i(ll," and "),Cd=n(ll,"CODE",{});var YD=r(Cd);vv=i(YD,"span end logits"),YD.forEach(t),Tv=i(ll,")."),ll.forEach(t),kv=d(Ft),Or=n(Ft,"P",{});var Qp=r(Or);wv=i(Qp,"This model inherits from "),Oi=n(Qp,"A",{href:!0});var ZD=r(Oi);$v=i(ZD,"TFPreTrainedModel"),ZD.forEach(t),Dv=i(Qp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qp.forEach(t),Fv=d(Ft),Wr=n(Ft,"P",{});var Up=r(Wr);yv=i(Up,"This model is also a "),Rr=n(Up,"A",{href:!0,rel:!0});var eF=r(Rr);Bv=i(eF,"tf.keras.Model"),eF.forEach(t),Mv=i(Up,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Up.forEach(t),Ev=d(Ft),k(Es.$$.fragment,Ft),xv=d(Ft),st=n(Ft,"DIV",{class:!0});var Zt=r(st);k(Hr.$$.fragment,Zt),zv=d(Zt),Ao=n(Zt,"P",{});var dl=r(Ao);Cv=i(dl,"The "),Wi=n(dl,"A",{href:!0});var tF=r(Wi);jv=i(tF,"TFDistilBertForQuestionAnswering"),tF.forEach(t),Pv=i(dl," forward method, overrides the "),jd=n(dl,"CODE",{});var oF=r(jd);qv=i(oF,"__call__"),oF.forEach(t),Av=i(dl," special method."),dl.forEach(t),Iv=d(Zt),k(xs.$$.fragment,Zt),Lv=d(Zt),Pd=n(Zt,"P",{});var sF=r(Pd);Sv=i(sF,"Example:"),sF.forEach(t),Nv=d(Zt),k(Qr.$$.fragment,Zt),Zt.forEach(t),Ft.forEach(t),Hh=d(o),Io=n(o,"H2",{class:!0});var Jp=r(Io);zs=n(Jp,"A",{id:!0,class:!0,href:!0});var nF=r(zs);qd=n(nF,"SPAN",{});var rF=r(qd);k(Ur.$$.fragment,rF),rF.forEach(t),nF.forEach(t),Ov=d(Jp),Ad=n(Jp,"SPAN",{});var aF=r(Ad);Wv=i(aF,"FlaxDistilBertModel"),aF.forEach(t),Jp.forEach(t),Qh=d(o),Be=n(o,"DIV",{class:!0});var ht=r(Be);k(Jr.$$.fragment,ht),Rv=d(ht),Id=n(ht,"P",{});var iF=r(Id);Hv=i(iF,"The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),iF.forEach(t),Qv=d(ht),Vr=n(ht,"P",{});var Vp=r(Vr);Uv=i(Vp,"This model inherits from "),Ri=n(Vp,"A",{href:!0});var lF=r(Ri);Jv=i(lF,"FlaxPreTrainedModel"),lF.forEach(t),Vv=i(Vp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Vp.forEach(t),Kv=d(ht),Kr=n(ht,"P",{});var Kp=r(Kr);Gv=i(Kp,"This model is also a Flax Linen "),Gr=n(Kp,"A",{href:!0,rel:!0});var dF=r(Gr);Xv=i(dF,"flax.linen.Module"),dF.forEach(t),Yv=i(Kp,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Kp.forEach(t),Zv=d(ht),Ld=n(ht,"P",{});var cF=r(Ld);eT=i(cF,"Finally, this model supports inherent JAX features such as:"),cF.forEach(t),tT=d(ht),Bt=n(ht,"UL",{});var Us=r(Bt);Sd=n(Us,"LI",{});var hF=r(Sd);Xr=n(hF,"A",{href:!0,rel:!0});var pF=r(Xr);oT=i(pF,"Just-In-Time (JIT) compilation"),pF.forEach(t),hF.forEach(t),sT=d(Us),Nd=n(Us,"LI",{});var uF=r(Nd);Yr=n(uF,"A",{href:!0,rel:!0});var fF=r(Yr);nT=i(fF,"Automatic Differentiation"),fF.forEach(t),uF.forEach(t),rT=d(Us),Od=n(Us,"LI",{});var mF=r(Od);Zr=n(mF,"A",{href:!0,rel:!0});var gF=r(Zr);aT=i(gF,"Vectorization"),gF.forEach(t),mF.forEach(t),iT=d(Us),Wd=n(Us,"LI",{});var _F=r(Wd);ea=n(_F,"A",{href:!0,rel:!0});var bF=r(ea);lT=i(bF,"Parallelization"),bF.forEach(t),_F.forEach(t),Us.forEach(t),dT=d(ht),nt=n(ht,"DIV",{class:!0});var eo=r(nt);k(ta.$$.fragment,eo),cT=d(eo),Lo=n(eo,"P",{});var cl=r(Lo);hT=i(cl,"The "),Rd=n(cl,"CODE",{});var vF=r(Rd);pT=i(vF,"FlaxDistilBertPreTrainedModel"),vF.forEach(t),uT=i(cl,"forward method, overrides the "),Hd=n(cl,"CODE",{});var TF=r(Hd);fT=i(TF,"__call__"),TF.forEach(t),mT=i(cl," special method."),cl.forEach(t),gT=d(eo),k(Cs.$$.fragment,eo),_T=d(eo),Qd=n(eo,"P",{});var kF=r(Qd);bT=i(kF,"Example:"),kF.forEach(t),vT=d(eo),k(oa.$$.fragment,eo),eo.forEach(t),ht.forEach(t),Uh=d(o),So=n(o,"H2",{class:!0});var Gp=r(So);js=n(Gp,"A",{id:!0,class:!0,href:!0});var wF=r(js);Ud=n(wF,"SPAN",{});var $F=r(Ud);k(sa.$$.fragment,$F),$F.forEach(t),wF.forEach(t),TT=d(Gp),Jd=n(Gp,"SPAN",{});var DF=r(Jd);kT=i(DF,"FlaxDistilBertForMaskedLM"),DF.forEach(t),Gp.forEach(t),Jh=d(o),Me=n(o,"DIV",{class:!0});var pt=r(Me);k(na.$$.fragment,pt),wT=d(pt),ra=n(pt,"P",{});var Xp=r(ra);$T=i(Xp,"DistilBert Model with a "),Vd=n(Xp,"CODE",{});var FF=r(Vd);DT=i(FF,"language modeling"),FF.forEach(t),FT=i(Xp," head on top."),Xp.forEach(t),yT=d(pt),aa=n(pt,"P",{});var Yp=r(aa);BT=i(Yp,"This model inherits from "),Hi=n(Yp,"A",{href:!0});var yF=r(Hi);MT=i(yF,"FlaxPreTrainedModel"),yF.forEach(t),ET=i(Yp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Yp.forEach(t),xT=d(pt),ia=n(pt,"P",{});var Zp=r(ia);zT=i(Zp,"This model is also a Flax Linen "),la=n(Zp,"A",{href:!0,rel:!0});var BF=r(la);CT=i(BF,"flax.linen.Module"),BF.forEach(t),jT=i(Zp,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Zp.forEach(t),PT=d(pt),Kd=n(pt,"P",{});var MF=r(Kd);qT=i(MF,"Finally, this model supports inherent JAX features such as:"),MF.forEach(t),AT=d(pt),Mt=n(pt,"UL",{});var Js=r(Mt);Gd=n(Js,"LI",{});var EF=r(Gd);da=n(EF,"A",{href:!0,rel:!0});var xF=r(da);IT=i(xF,"Just-In-Time (JIT) compilation"),xF.forEach(t),EF.forEach(t),LT=d(Js),Xd=n(Js,"LI",{});var zF=r(Xd);ca=n(zF,"A",{href:!0,rel:!0});var CF=r(ca);ST=i(CF,"Automatic Differentiation"),CF.forEach(t),zF.forEach(t),NT=d(Js),Yd=n(Js,"LI",{});var jF=r(Yd);ha=n(jF,"A",{href:!0,rel:!0});var PF=r(ha);OT=i(PF,"Vectorization"),PF.forEach(t),jF.forEach(t),WT=d(Js),Zd=n(Js,"LI",{});var qF=r(Zd);pa=n(qF,"A",{href:!0,rel:!0});var AF=r(pa);RT=i(AF,"Parallelization"),AF.forEach(t),qF.forEach(t),Js.forEach(t),HT=d(pt),rt=n(pt,"DIV",{class:!0});var to=r(rt);k(ua.$$.fragment,to),QT=d(to),No=n(to,"P",{});var hl=r(No);UT=i(hl,"The "),ec=n(hl,"CODE",{});var IF=r(ec);JT=i(IF,"FlaxDistilBertPreTrainedModel"),IF.forEach(t),VT=i(hl,"forward method, overrides the "),tc=n(hl,"CODE",{});var LF=r(tc);KT=i(LF,"__call__"),LF.forEach(t),GT=i(hl," special method."),hl.forEach(t),XT=d(to),k(Ps.$$.fragment,to),YT=d(to),oc=n(to,"P",{});var SF=r(oc);ZT=i(SF,"Example:"),SF.forEach(t),ek=d(to),k(fa.$$.fragment,to),to.forEach(t),pt.forEach(t),Vh=d(o),Oo=n(o,"H2",{class:!0});var eu=r(Oo);qs=n(eu,"A",{id:!0,class:!0,href:!0});var NF=r(qs);sc=n(NF,"SPAN",{});var OF=r(sc);k(ma.$$.fragment,OF),OF.forEach(t),NF.forEach(t),tk=d(eu),nc=n(eu,"SPAN",{});var WF=r(nc);ok=i(WF,"FlaxDistilBertForSequenceClassification"),WF.forEach(t),eu.forEach(t),Kh=d(o),Ee=n(o,"DIV",{class:!0});var ut=r(Ee);k(ga.$$.fragment,ut),sk=d(ut),rc=n(ut,"P",{});var RF=r(rc);nk=i(RF,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),RF.forEach(t),rk=d(ut),_a=n(ut,"P",{});var tu=r(_a);ak=i(tu,"This model inherits from "),Qi=n(tu,"A",{href:!0});var HF=r(Qi);ik=i(HF,"FlaxPreTrainedModel"),HF.forEach(t),lk=i(tu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),tu.forEach(t),dk=d(ut),ba=n(ut,"P",{});var ou=r(ba);ck=i(ou,"This model is also a Flax Linen "),va=n(ou,"A",{href:!0,rel:!0});var QF=r(va);hk=i(QF,"flax.linen.Module"),QF.forEach(t),pk=i(ou,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),ou.forEach(t),uk=d(ut),ac=n(ut,"P",{});var UF=r(ac);fk=i(UF,"Finally, this model supports inherent JAX features such as:"),UF.forEach(t),mk=d(ut),Et=n(ut,"UL",{});var Vs=r(Et);ic=n(Vs,"LI",{});var JF=r(ic);Ta=n(JF,"A",{href:!0,rel:!0});var VF=r(Ta);gk=i(VF,"Just-In-Time (JIT) compilation"),VF.forEach(t),JF.forEach(t),_k=d(Vs),lc=n(Vs,"LI",{});var KF=r(lc);ka=n(KF,"A",{href:!0,rel:!0});var GF=r(ka);bk=i(GF,"Automatic Differentiation"),GF.forEach(t),KF.forEach(t),vk=d(Vs),dc=n(Vs,"LI",{});var XF=r(dc);wa=n(XF,"A",{href:!0,rel:!0});var YF=r(wa);Tk=i(YF,"Vectorization"),YF.forEach(t),XF.forEach(t),kk=d(Vs),cc=n(Vs,"LI",{});var ZF=r(cc);$a=n(ZF,"A",{href:!0,rel:!0});var e2=r($a);wk=i(e2,"Parallelization"),e2.forEach(t),ZF.forEach(t),Vs.forEach(t),$k=d(ut),at=n(ut,"DIV",{class:!0});var oo=r(at);k(Da.$$.fragment,oo),Dk=d(oo),Wo=n(oo,"P",{});var pl=r(Wo);Fk=i(pl,"The "),hc=n(pl,"CODE",{});var t2=r(hc);yk=i(t2,"FlaxDistilBertPreTrainedModel"),t2.forEach(t),Bk=i(pl,"forward method, overrides the "),pc=n(pl,"CODE",{});var o2=r(pc);Mk=i(o2,"__call__"),o2.forEach(t),Ek=i(pl," special method."),pl.forEach(t),xk=d(oo),k(As.$$.fragment,oo),zk=d(oo),uc=n(oo,"P",{});var s2=r(uc);Ck=i(s2,"Example:"),s2.forEach(t),jk=d(oo),k(Fa.$$.fragment,oo),oo.forEach(t),ut.forEach(t),Gh=d(o),Ro=n(o,"H2",{class:!0});var su=r(Ro);Is=n(su,"A",{id:!0,class:!0,href:!0});var n2=r(Is);fc=n(n2,"SPAN",{});var r2=r(fc);k(ya.$$.fragment,r2),r2.forEach(t),n2.forEach(t),Pk=d(su),mc=n(su,"SPAN",{});var a2=r(mc);qk=i(a2,"FlaxDistilBertForMultipleChoice"),a2.forEach(t),su.forEach(t),Xh=d(o),xe=n(o,"DIV",{class:!0});var ft=r(xe);k(Ba.$$.fragment,ft),Ak=d(ft),gc=n(ft,"P",{});var i2=r(gc);Ik=i(i2,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),i2.forEach(t),Lk=d(ft),Ma=n(ft,"P",{});var nu=r(Ma);Sk=i(nu,"This model inherits from "),Ui=n(nu,"A",{href:!0});var l2=r(Ui);Nk=i(l2,"FlaxPreTrainedModel"),l2.forEach(t),Ok=i(nu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),nu.forEach(t),Wk=d(ft),Ea=n(ft,"P",{});var ru=r(Ea);Rk=i(ru,"This model is also a Flax Linen "),xa=n(ru,"A",{href:!0,rel:!0});var d2=r(xa);Hk=i(d2,"flax.linen.Module"),d2.forEach(t),Qk=i(ru,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),ru.forEach(t),Uk=d(ft),_c=n(ft,"P",{});var c2=r(_c);Jk=i(c2,"Finally, this model supports inherent JAX features such as:"),c2.forEach(t),Vk=d(ft),xt=n(ft,"UL",{});var Ks=r(xt);bc=n(Ks,"LI",{});var h2=r(bc);za=n(h2,"A",{href:!0,rel:!0});var p2=r(za);Kk=i(p2,"Just-In-Time (JIT) compilation"),p2.forEach(t),h2.forEach(t),Gk=d(Ks),vc=n(Ks,"LI",{});var u2=r(vc);Ca=n(u2,"A",{href:!0,rel:!0});var f2=r(Ca);Xk=i(f2,"Automatic Differentiation"),f2.forEach(t),u2.forEach(t),Yk=d(Ks),Tc=n(Ks,"LI",{});var m2=r(Tc);ja=n(m2,"A",{href:!0,rel:!0});var g2=r(ja);Zk=i(g2,"Vectorization"),g2.forEach(t),m2.forEach(t),ew=d(Ks),kc=n(Ks,"LI",{});var _2=r(kc);Pa=n(_2,"A",{href:!0,rel:!0});var b2=r(Pa);tw=i(b2,"Parallelization"),b2.forEach(t),_2.forEach(t),Ks.forEach(t),ow=d(ft),it=n(ft,"DIV",{class:!0});var so=r(it);k(qa.$$.fragment,so),sw=d(so),Ho=n(so,"P",{});var ul=r(Ho);nw=i(ul,"The "),wc=n(ul,"CODE",{});var v2=r(wc);rw=i(v2,"FlaxDistilBertPreTrainedModel"),v2.forEach(t),aw=i(ul,"forward method, overrides the "),$c=n(ul,"CODE",{});var T2=r($c);iw=i(T2,"__call__"),T2.forEach(t),lw=i(ul," special method."),ul.forEach(t),dw=d(so),k(Ls.$$.fragment,so),cw=d(so),Dc=n(so,"P",{});var k2=r(Dc);hw=i(k2,"Example:"),k2.forEach(t),pw=d(so),k(Aa.$$.fragment,so),so.forEach(t),ft.forEach(t),Yh=d(o),Qo=n(o,"H2",{class:!0});var au=r(Qo);Ss=n(au,"A",{id:!0,class:!0,href:!0});var w2=r(Ss);Fc=n(w2,"SPAN",{});var $2=r(Fc);k(Ia.$$.fragment,$2),$2.forEach(t),w2.forEach(t),uw=d(au),yc=n(au,"SPAN",{});var D2=r(yc);fw=i(D2,"FlaxDistilBertForTokenClassification"),D2.forEach(t),au.forEach(t),Zh=d(o),ze=n(o,"DIV",{class:!0});var mt=r(ze);k(La.$$.fragment,mt),mw=d(mt),Bc=n(mt,"P",{});var F2=r(Bc);gw=i(F2,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),F2.forEach(t),_w=d(mt),Sa=n(mt,"P",{});var iu=r(Sa);bw=i(iu,"This model inherits from "),Ji=n(iu,"A",{href:!0});var y2=r(Ji);vw=i(y2,"FlaxPreTrainedModel"),y2.forEach(t),Tw=i(iu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),iu.forEach(t),kw=d(mt),Na=n(mt,"P",{});var lu=r(Na);ww=i(lu,"This model is also a Flax Linen "),Oa=n(lu,"A",{href:!0,rel:!0});var B2=r(Oa);$w=i(B2,"flax.linen.Module"),B2.forEach(t),Dw=i(lu,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),lu.forEach(t),Fw=d(mt),Mc=n(mt,"P",{});var M2=r(Mc);yw=i(M2,"Finally, this model supports inherent JAX features such as:"),M2.forEach(t),Bw=d(mt),zt=n(mt,"UL",{});var Gs=r(zt);Ec=n(Gs,"LI",{});var E2=r(Ec);Wa=n(E2,"A",{href:!0,rel:!0});var x2=r(Wa);Mw=i(x2,"Just-In-Time (JIT) compilation"),x2.forEach(t),E2.forEach(t),Ew=d(Gs),xc=n(Gs,"LI",{});var z2=r(xc);Ra=n(z2,"A",{href:!0,rel:!0});var C2=r(Ra);xw=i(C2,"Automatic Differentiation"),C2.forEach(t),z2.forEach(t),zw=d(Gs),zc=n(Gs,"LI",{});var j2=r(zc);Ha=n(j2,"A",{href:!0,rel:!0});var P2=r(Ha);Cw=i(P2,"Vectorization"),P2.forEach(t),j2.forEach(t),jw=d(Gs),Cc=n(Gs,"LI",{});var q2=r(Cc);Qa=n(q2,"A",{href:!0,rel:!0});var A2=r(Qa);Pw=i(A2,"Parallelization"),A2.forEach(t),q2.forEach(t),Gs.forEach(t),qw=d(mt),lt=n(mt,"DIV",{class:!0});var no=r(lt);k(Ua.$$.fragment,no),Aw=d(no),Uo=n(no,"P",{});var fl=r(Uo);Iw=i(fl,"The "),jc=n(fl,"CODE",{});var I2=r(jc);Lw=i(I2,"FlaxDistilBertPreTrainedModel"),I2.forEach(t),Sw=i(fl,"forward method, overrides the "),Pc=n(fl,"CODE",{});var L2=r(Pc);Nw=i(L2,"__call__"),L2.forEach(t),Ow=i(fl," special method."),fl.forEach(t),Ww=d(no),k(Ns.$$.fragment,no),Rw=d(no),qc=n(no,"P",{});var S2=r(qc);Hw=i(S2,"Example:"),S2.forEach(t),Qw=d(no),k(Ja.$$.fragment,no),no.forEach(t),mt.forEach(t),ep=d(o),Jo=n(o,"H2",{class:!0});var du=r(Jo);Os=n(du,"A",{id:!0,class:!0,href:!0});var N2=r(Os);Ac=n(N2,"SPAN",{});var O2=r(Ac);k(Va.$$.fragment,O2),O2.forEach(t),N2.forEach(t),Uw=d(du),Ic=n(du,"SPAN",{});var W2=r(Ic);Jw=i(W2,"FlaxDistilBertForQuestionAnswering"),W2.forEach(t),du.forEach(t),tp=d(o),Ce=n(o,"DIV",{class:!0});var gt=r(Ce);k(Ka.$$.fragment,gt),Vw=d(gt),Vo=n(gt,"P",{});var ml=r(Vo);Kw=i(ml,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Lc=n(ml,"CODE",{});var R2=r(Lc);Gw=i(R2,"span start logits"),R2.forEach(t),Xw=i(ml," and "),Sc=n(ml,"CODE",{});var H2=r(Sc);Yw=i(H2,"span end logits"),H2.forEach(t),Zw=i(ml,")."),ml.forEach(t),e1=d(gt),Ga=n(gt,"P",{});var cu=r(Ga);t1=i(cu,"This model inherits from "),Vi=n(cu,"A",{href:!0});var Q2=r(Vi);o1=i(Q2,"FlaxPreTrainedModel"),Q2.forEach(t),s1=i(cu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),cu.forEach(t),n1=d(gt),Xa=n(gt,"P",{});var hu=r(Xa);r1=i(hu,"This model is also a Flax Linen "),Ya=n(hu,"A",{href:!0,rel:!0});var U2=r(Ya);a1=i(U2,"flax.linen.Module"),U2.forEach(t),i1=i(hu,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),hu.forEach(t),l1=d(gt),Nc=n(gt,"P",{});var J2=r(Nc);d1=i(J2,"Finally, this model supports inherent JAX features such as:"),J2.forEach(t),c1=d(gt),Ct=n(gt,"UL",{});var Xs=r(Ct);Oc=n(Xs,"LI",{});var V2=r(Oc);Za=n(V2,"A",{href:!0,rel:!0});var K2=r(Za);h1=i(K2,"Just-In-Time (JIT) compilation"),K2.forEach(t),V2.forEach(t),p1=d(Xs),Wc=n(Xs,"LI",{});var G2=r(Wc);ei=n(G2,"A",{href:!0,rel:!0});var X2=r(ei);u1=i(X2,"Automatic Differentiation"),X2.forEach(t),G2.forEach(t),f1=d(Xs),Rc=n(Xs,"LI",{});var Y2=r(Rc);ti=n(Y2,"A",{href:!0,rel:!0});var Z2=r(ti);m1=i(Z2,"Vectorization"),Z2.forEach(t),Y2.forEach(t),g1=d(Xs),Hc=n(Xs,"LI",{});var ey=r(Hc);oi=n(ey,"A",{href:!0,rel:!0});var ty=r(oi);_1=i(ty,"Parallelization"),ty.forEach(t),ey.forEach(t),Xs.forEach(t),b1=d(gt),dt=n(gt,"DIV",{class:!0});var ro=r(dt);k(si.$$.fragment,ro),v1=d(ro),Ko=n(ro,"P",{});var gl=r(Ko);T1=i(gl,"The "),Qc=n(gl,"CODE",{});var oy=r(Qc);k1=i(oy,"FlaxDistilBertPreTrainedModel"),oy.forEach(t),w1=i(gl,"forward method, overrides the "),Uc=n(gl,"CODE",{});var sy=r(Uc);$1=i(sy,"__call__"),sy.forEach(t),D1=i(gl," special method."),gl.forEach(t),F1=d(ro),k(Ws.$$.fragment,ro),y1=d(ro),Jc=n(ro,"P",{});var ny=r(Jc);B1=i(ny,"Example:"),ny.forEach(t),M1=d(ro),k(ni.$$.fragment,ro),ro.forEach(t),gt.forEach(t),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(Py)),c(b,"id","distilbert"),c(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b,"href","#distilbert"),c(g,"class","relative group"),c(G,"id","overview"),c(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G,"href","#overview"),c(E,"class","relative group"),c(Y,"href","https://medium.com/huggingface/distilbert-8cf3380435b5"),c(Y,"rel","nofollow"),c(z,"href","https://arxiv.org/abs/1910.01108"),c(z,"rel","nofollow"),c(Ys,"href","https://huggingface.co/victorsanh"),c(Ys,"rel","nofollow"),c(Zs,"href","https://huggingface.co/kamalkraj"),c(Zs,"rel","nofollow"),c(en,"href","https://github.com/huggingface/transformers/tree/master/examples/research_projects/distillation"),c(en,"rel","nofollow"),c(Go,"id","transformers.DistilBertConfig"),c(Go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Go,"href","#transformers.DistilBertConfig"),c(ao,"class","relative group"),c(di,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertModel"),c(ci,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(sn,"href","https://huggingface.co/distilbert-base-uncased"),c(sn,"rel","nofollow"),c(hi,"href","/docs/transformers/doc-build-test/en/main_classes/configuration#transformers.PretrainedConfig"),c(pi,"href","/docs/transformers/doc-build-test/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ne,"class","docstring"),c(Xo,"id","transformers.DistilBertTokenizer"),c(Xo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xo,"href","#transformers.DistilBertTokenizer"),c(lo,"class","relative group"),c(ui,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(fi,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer"),c(mi,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer"),c(_t,"class","docstring"),c(Zo,"id","transformers.DistilBertTokenizerFast"),c(Zo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Zo,"href","#transformers.DistilBertTokenizerFast"),c(co,"class","relative group"),c(gi,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(_i,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizerFast"),c(bi,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizerFast"),c(bt,"class","docstring"),c(ts,"id","transformers.DistilBertModel"),c(ts,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ts,"href","#transformers.DistilBertModel"),c(ho,"class","relative group"),c(vi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(_n,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(_n,"rel","nofollow"),c(Ti,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertModel"),c(Je,"class","docstring"),c(Oe,"class","docstring"),c(ss,"id","transformers.DistilBertForMaskedLM"),c(ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ss,"href","#transformers.DistilBertForMaskedLM"),c(uo,"class","relative group"),c(ki,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(Fn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Fn,"rel","nofollow"),c(wi,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(Ve,"class","docstring"),c(We,"class","docstring"),c(rs,"id","transformers.DistilBertForSequenceClassification"),c(rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(rs,"href","#transformers.DistilBertForSequenceClassification"),c(mo,"class","relative group"),c($i,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(Cn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Cn,"rel","nofollow"),c(Di,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(je,"class","docstring"),c(Re,"class","docstring"),c(is,"id","transformers.DistilBertForMultipleChoice"),c(is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(is,"href","#transformers.DistilBertForMultipleChoice"),c(_o,"class","relative group"),c(Fi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(Nn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Nn,"rel","nofollow"),c(yi,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(Ke,"class","docstring"),c(He,"class","docstring"),c(ds,"id","transformers.DistilBertForTokenClassification"),c(ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ds,"href","#transformers.DistilBertForTokenClassification"),c(vo,"class","relative group"),c(Bi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(Jn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Jn,"rel","nofollow"),c(Mi,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(Ge,"class","docstring"),c(Qe,"class","docstring"),c(hs,"id","transformers.DistilBertForQuestionAnswering"),c(hs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(hs,"href","#transformers.DistilBertForQuestionAnswering"),c(ko,"class","relative group"),c(Ei,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(er,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(er,"rel","nofollow"),c(xi,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(Xe,"class","docstring"),c(Ue,"class","docstring"),c(us,"id","transformers.TFDistilBertModel"),c(us,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(us,"href","#transformers.TFDistilBertModel"),c(Do,"class","relative group"),c(zi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(ir,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ir,"rel","nofollow"),c(Ci,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(Ye,"class","docstring"),c(Pe,"class","docstring"),c(gs,"id","transformers.TFDistilBertForMaskedLM"),c(gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gs,"href","#transformers.TFDistilBertForMaskedLM"),c(yo,"class","relative group"),c(ji,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(mr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(mr,"rel","nofollow"),c(Pi,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(Ze,"class","docstring"),c(qe,"class","docstring"),c(vs,"id","transformers.TFDistilBertForSequenceClassification"),c(vs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vs,"href","#transformers.TFDistilBertForSequenceClassification"),c(Mo,"class","relative group"),c(qi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(wr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(wr,"rel","nofollow"),c(Ai,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(et,"class","docstring"),c(Ae,"class","docstring"),c(ws,"id","transformers.TFDistilBertForMultipleChoice"),c(ws,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ws,"href","#transformers.TFDistilBertForMultipleChoice"),c(xo,"class","relative group"),c(Ii,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(Er,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Er,"rel","nofollow"),c(Li,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(tt,"class","docstring"),c(Ie,"class","docstring"),c(Fs,"id","transformers.TFDistilBertForTokenClassification"),c(Fs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fs,"href","#transformers.TFDistilBertForTokenClassification"),c(Co,"class","relative group"),c(Si,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ar,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ar,"rel","nofollow"),c(Ni,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(ot,"class","docstring"),c(Le,"class","docstring"),c(Ms,"id","transformers.TFDistilBertForQuestionAnswering"),c(Ms,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ms,"href","#transformers.TFDistilBertForQuestionAnswering"),c(Po,"class","relative group"),c(Oi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(Rr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Rr,"rel","nofollow"),c(Wi,"href","/docs/transformers/doc-build-test/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(st,"class","docstring"),c(Se,"class","docstring"),c(zs,"id","transformers.FlaxDistilBertModel"),c(zs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zs,"href","#transformers.FlaxDistilBertModel"),c(Io,"class","relative group"),c(Ri,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Gr,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Gr,"rel","nofollow"),c(Xr,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Xr,"rel","nofollow"),c(Yr,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Yr,"rel","nofollow"),c(Zr,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Zr,"rel","nofollow"),c(ea,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(ea,"rel","nofollow"),c(nt,"class","docstring"),c(Be,"class","docstring"),c(js,"id","transformers.FlaxDistilBertForMaskedLM"),c(js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(js,"href","#transformers.FlaxDistilBertForMaskedLM"),c(So,"class","relative group"),c(Hi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(la,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(la,"rel","nofollow"),c(da,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(da,"rel","nofollow"),c(ca,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(ca,"rel","nofollow"),c(ha,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(ha,"rel","nofollow"),c(pa,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(pa,"rel","nofollow"),c(rt,"class","docstring"),c(Me,"class","docstring"),c(qs,"id","transformers.FlaxDistilBertForSequenceClassification"),c(qs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qs,"href","#transformers.FlaxDistilBertForSequenceClassification"),c(Oo,"class","relative group"),c(Qi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(va,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(va,"rel","nofollow"),c(Ta,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Ta,"rel","nofollow"),c(ka,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(ka,"rel","nofollow"),c(wa,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(wa,"rel","nofollow"),c($a,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c($a,"rel","nofollow"),c(at,"class","docstring"),c(Ee,"class","docstring"),c(Is,"id","transformers.FlaxDistilBertForMultipleChoice"),c(Is,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Is,"href","#transformers.FlaxDistilBertForMultipleChoice"),c(Ro,"class","relative group"),c(Ui,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(xa,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(xa,"rel","nofollow"),c(za,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(za,"rel","nofollow"),c(Ca,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Ca,"rel","nofollow"),c(ja,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(ja,"rel","nofollow"),c(Pa,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Pa,"rel","nofollow"),c(it,"class","docstring"),c(xe,"class","docstring"),c(Ss,"id","transformers.FlaxDistilBertForTokenClassification"),c(Ss,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ss,"href","#transformers.FlaxDistilBertForTokenClassification"),c(Qo,"class","relative group"),c(Ji,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Oa,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Oa,"rel","nofollow"),c(Wa,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Wa,"rel","nofollow"),c(Ra,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Ra,"rel","nofollow"),c(Ha,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Ha,"rel","nofollow"),c(Qa,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Qa,"rel","nofollow"),c(lt,"class","docstring"),c(ze,"class","docstring"),c(Os,"id","transformers.FlaxDistilBertForQuestionAnswering"),c(Os,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Os,"href","#transformers.FlaxDistilBertForQuestionAnswering"),c(Jo,"class","relative group"),c(Vi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ya,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Ya,"rel","nofollow"),c(Za,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Za,"rel","nofollow"),c(ei,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(ei,"rel","nofollow"),c(ti,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(ti,"rel","nofollow"),c(oi,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(oi,"rel","nofollow"),c(dt,"class","docstring"),c(Ce,"class","docstring")},m(o,m){e(document.head,h),u(o,y,m),u(o,g,m),e(g,b),e(b,v),w(_,v,null),e(g,f),e(g,B),e(B,de),u(o,J,m),u(o,E,m),e(E,G),e(G,N),w(X,N,null),e(E,ce),e(E,O),e(O,he),u(o,re,m),u(o,L,m),e(L,q),e(L,Y),e(Y,V),e(L,x),e(L,z),e(z,pe),e(L,W),e(L,oe),e(oe,ue),e(L,R),u(o,ae,m),u(o,ee,m),e(ee,A),u(o,ie,m),u(o,S,m),e(S,se),e(se,fe),u(o,P,m),u(o,te,m),e(te,H),u(o,le,m),u(o,p,m),e(p,M),e(M,K),e(M,ge),e(ge,ve),e(M,I),e(M,_e),e(_e,Te),e(M,ke),e(M,j),e(j,Q),e(M,we),e(p,$e),e(p,Z),e(Z,De),e(Z,ne),e(ne,Fe),e(Z,pu),u(o,uh,m),u(o,vt,m),e(vt,uu),e(vt,Ys),e(Ys,fu),e(vt,mu),e(vt,Zs),e(Zs,gu),e(vt,_u),e(vt,en),e(en,bu),e(vt,vu),u(o,fh,m),u(o,ao,m),e(ao,Go),e(Go,_l),w(tn,_l,null),e(ao,Tu),e(ao,bl),e(bl,ku),u(o,mh,m),u(o,Ne,m),w(on,Ne,null),e(Ne,wu),e(Ne,yt),e(yt,$u),e(yt,di),e(di,Du),e(yt,Fu),e(yt,ci),e(ci,yu),e(yt,Bu),e(yt,sn),e(sn,Mu),e(yt,Eu),e(Ne,xu),e(Ne,io),e(io,zu),e(io,hi),e(hi,Cu),e(io,ju),e(io,pi),e(pi,Pu),e(io,qu),e(Ne,Au),e(Ne,vl),e(vl,Iu),e(Ne,Lu),w(nn,Ne,null),u(o,gh,m),u(o,lo,m),e(lo,Xo),e(Xo,Tl),w(rn,Tl,null),e(lo,Su),e(lo,kl),e(kl,Nu),u(o,_h,m),u(o,_t,m),w(an,_t,null),e(_t,Ou),e(_t,wl),e(wl,Wu),e(_t,Ru),e(_t,Yo),e(Yo,ui),e(ui,Hu),e(Yo,Qu),e(Yo,fi),e(fi,Uu),e(Yo,Ju),e(_t,Vu),e(_t,ln),e(ln,Ku),e(ln,mi),e(mi,Gu),e(ln,Xu),u(o,bh,m),u(o,co,m),e(co,Zo),e(Zo,$l),w(dn,$l,null),e(co,Yu),e(co,Dl),e(Dl,Zu),u(o,vh,m),u(o,bt,m),w(cn,bt,null),e(bt,ef),e(bt,hn),e(hn,tf),e(hn,Fl),e(Fl,of),e(hn,sf),e(bt,nf),e(bt,es),e(es,gi),e(gi,rf),e(es,af),e(es,_i),e(_i,lf),e(es,df),e(bt,cf),e(bt,pn),e(pn,hf),e(pn,bi),e(bi,pf),e(pn,uf),u(o,Th,m),u(o,ho,m),e(ho,ts),e(ts,yl),w(un,yl,null),e(ho,ff),e(ho,Bl),e(Bl,mf),u(o,kh,m),u(o,Oe,m),w(fn,Oe,null),e(Oe,gf),e(Oe,Ml),e(Ml,_f),e(Oe,bf),e(Oe,mn),e(mn,vf),e(mn,vi),e(vi,Tf),e(mn,kf),e(Oe,wf),e(Oe,gn),e(gn,$f),e(gn,_n),e(_n,Df),e(gn,Ff),e(Oe,yf),e(Oe,Je),w(bn,Je,null),e(Je,Bf),e(Je,po),e(po,Mf),e(po,Ti),e(Ti,Ef),e(po,xf),e(po,El),e(El,zf),e(po,Cf),e(Je,jf),w(os,Je,null),e(Je,Pf),e(Je,xl),e(xl,qf),e(Je,Af),w(vn,Je,null),u(o,wh,m),u(o,uo,m),e(uo,ss),e(ss,zl),w(Tn,zl,null),e(uo,If),e(uo,Cl),e(Cl,Lf),u(o,$h,m),u(o,We,m),w(kn,We,null),e(We,Sf),e(We,wn),e(wn,Nf),e(wn,jl),e(jl,Of),e(wn,Wf),e(We,Rf),e(We,$n),e($n,Hf),e($n,ki),e(ki,Qf),e($n,Uf),e(We,Jf),e(We,Dn),e(Dn,Vf),e(Dn,Fn),e(Fn,Kf),e(Dn,Gf),e(We,Xf),e(We,Ve),w(yn,Ve,null),e(Ve,Yf),e(Ve,fo),e(fo,Zf),e(fo,wi),e(wi,em),e(fo,tm),e(fo,Pl),e(Pl,om),e(fo,sm),e(Ve,nm),w(ns,Ve,null),e(Ve,rm),e(Ve,ql),e(ql,am),e(Ve,im),w(Bn,Ve,null),u(o,Dh,m),u(o,mo,m),e(mo,rs),e(rs,Al),w(Mn,Al,null),e(mo,lm),e(mo,Il),e(Il,dm),u(o,Fh,m),u(o,Re,m),w(En,Re,null),e(Re,cm),e(Re,Ll),e(Ll,hm),e(Re,pm),e(Re,xn),e(xn,um),e(xn,$i),e($i,fm),e(xn,mm),e(Re,gm),e(Re,zn),e(zn,_m),e(zn,Cn),e(Cn,bm),e(zn,vm),e(Re,Tm),e(Re,je),w(jn,je,null),e(je,km),e(je,go),e(go,wm),e(go,Di),e(Di,$m),e(go,Dm),e(go,Sl),e(Sl,Fm),e(go,ym),e(je,Bm),w(as,je,null),e(je,Mm),e(je,Nl),e(Nl,Em),e(je,xm),w(Pn,je,null),e(je,zm),e(je,Ol),e(Ol,Cm),e(je,jm),w(qn,je,null),u(o,yh,m),u(o,_o,m),e(_o,is),e(is,Wl),w(An,Wl,null),e(_o,Pm),e(_o,Rl),e(Rl,qm),u(o,Bh,m),u(o,He,m),w(In,He,null),e(He,Am),e(He,Hl),e(Hl,Im),e(He,Lm),e(He,Ln),e(Ln,Sm),e(Ln,Fi),e(Fi,Nm),e(Ln,Om),e(He,Wm),e(He,Sn),e(Sn,Rm),e(Sn,Nn),e(Nn,Hm),e(Sn,Qm),e(He,Um),e(He,Ke),w(On,Ke,null),e(Ke,Jm),e(Ke,bo),e(bo,Vm),e(bo,yi),e(yi,Km),e(bo,Gm),e(bo,Ql),e(Ql,Xm),e(bo,Ym),e(Ke,Zm),w(ls,Ke,null),e(Ke,eg),e(Ke,Ul),e(Ul,tg),e(Ke,og),w(Wn,Ke,null),u(o,Mh,m),u(o,vo,m),e(vo,ds),e(ds,Jl),w(Rn,Jl,null),e(vo,sg),e(vo,Vl),e(Vl,ng),u(o,Eh,m),u(o,Qe,m),w(Hn,Qe,null),e(Qe,rg),e(Qe,Kl),e(Kl,ag),e(Qe,ig),e(Qe,Qn),e(Qn,lg),e(Qn,Bi),e(Bi,dg),e(Qn,cg),e(Qe,hg),e(Qe,Un),e(Un,pg),e(Un,Jn),e(Jn,ug),e(Un,fg),e(Qe,mg),e(Qe,Ge),w(Vn,Ge,null),e(Ge,gg),e(Ge,To),e(To,_g),e(To,Mi),e(Mi,bg),e(To,vg),e(To,Gl),e(Gl,Tg),e(To,kg),e(Ge,wg),w(cs,Ge,null),e(Ge,$g),e(Ge,Xl),e(Xl,Dg),e(Ge,Fg),w(Kn,Ge,null),u(o,xh,m),u(o,ko,m),e(ko,hs),e(hs,Yl),w(Gn,Yl,null),e(ko,yg),e(ko,Zl),e(Zl,Bg),u(o,zh,m),u(o,Ue,m),w(Xn,Ue,null),e(Ue,Mg),e(Ue,wo),e(wo,Eg),e(wo,ed),e(ed,xg),e(wo,zg),e(wo,td),e(td,Cg),e(wo,jg),e(Ue,Pg),e(Ue,Yn),e(Yn,qg),e(Yn,Ei),e(Ei,Ag),e(Yn,Ig),e(Ue,Lg),e(Ue,Zn),e(Zn,Sg),e(Zn,er),e(er,Ng),e(Zn,Og),e(Ue,Wg),e(Ue,Xe),w(tr,Xe,null),e(Xe,Rg),e(Xe,$o),e($o,Hg),e($o,xi),e(xi,Qg),e($o,Ug),e($o,od),e(od,Jg),e($o,Vg),e(Xe,Kg),w(ps,Xe,null),e(Xe,Gg),e(Xe,sd),e(sd,Xg),e(Xe,Yg),w(or,Xe,null),u(o,Ch,m),u(o,Do,m),e(Do,us),e(us,nd),w(sr,nd,null),e(Do,Zg),e(Do,rd),e(rd,e_),u(o,jh,m),u(o,Pe,m),w(nr,Pe,null),e(Pe,t_),e(Pe,ad),e(ad,o_),e(Pe,s_),e(Pe,rr),e(rr,n_),e(rr,zi),e(zi,r_),e(rr,a_),e(Pe,i_),e(Pe,ar),e(ar,l_),e(ar,ir),e(ir,d_),e(ar,c_),e(Pe,h_),w(fs,Pe,null),e(Pe,p_),e(Pe,Ye),w(lr,Ye,null),e(Ye,u_),e(Ye,Fo),e(Fo,f_),e(Fo,Ci),e(Ci,m_),e(Fo,g_),e(Fo,id),e(id,__),e(Fo,b_),e(Ye,v_),w(ms,Ye,null),e(Ye,T_),e(Ye,ld),e(ld,k_),e(Ye,w_),w(dr,Ye,null),u(o,Ph,m),u(o,yo,m),e(yo,gs),e(gs,dd),w(cr,dd,null),e(yo,$_),e(yo,cd),e(cd,D_),u(o,qh,m),u(o,qe,m),w(hr,qe,null),e(qe,F_),e(qe,pr),e(pr,y_),e(pr,hd),e(hd,B_),e(pr,M_),e(qe,E_),e(qe,ur),e(ur,x_),e(ur,ji),e(ji,z_),e(ur,C_),e(qe,j_),e(qe,fr),e(fr,P_),e(fr,mr),e(mr,q_),e(fr,A_),e(qe,I_),w(_s,qe,null),e(qe,L_),e(qe,Ze),w(gr,Ze,null),e(Ze,S_),e(Ze,Bo),e(Bo,N_),e(Bo,Pi),e(Pi,O_),e(Bo,W_),e(Bo,pd),e(pd,R_),e(Bo,H_),e(Ze,Q_),w(bs,Ze,null),e(Ze,U_),e(Ze,ud),e(ud,J_),e(Ze,V_),w(_r,Ze,null),u(o,Ah,m),u(o,Mo,m),e(Mo,vs),e(vs,fd),w(br,fd,null),e(Mo,K_),e(Mo,md),e(md,G_),u(o,Ih,m),u(o,Ae,m),w(vr,Ae,null),e(Ae,X_),e(Ae,gd),e(gd,Y_),e(Ae,Z_),e(Ae,Tr),e(Tr,eb),e(Tr,qi),e(qi,tb),e(Tr,ob),e(Ae,sb),e(Ae,kr),e(kr,nb),e(kr,wr),e(wr,rb),e(kr,ab),e(Ae,ib),w(Ts,Ae,null),e(Ae,lb),e(Ae,et),w($r,et,null),e(et,db),e(et,Eo),e(Eo,cb),e(Eo,Ai),e(Ai,hb),e(Eo,pb),e(Eo,_d),e(_d,ub),e(Eo,fb),e(et,mb),w(ks,et,null),e(et,gb),e(et,bd),e(bd,_b),e(et,bb),w(Dr,et,null),u(o,Lh,m),u(o,xo,m),e(xo,ws),e(ws,vd),w(Fr,vd,null),e(xo,vb),e(xo,Td),e(Td,Tb),u(o,Sh,m),u(o,Ie,m),w(yr,Ie,null),e(Ie,kb),e(Ie,kd),e(kd,wb),e(Ie,$b),e(Ie,Br),e(Br,Db),e(Br,Ii),e(Ii,Fb),e(Br,yb),e(Ie,Bb),e(Ie,Mr),e(Mr,Mb),e(Mr,Er),e(Er,Eb),e(Mr,xb),e(Ie,zb),w($s,Ie,null),e(Ie,Cb),e(Ie,tt),w(xr,tt,null),e(tt,jb),e(tt,zo),e(zo,Pb),e(zo,Li),e(Li,qb),e(zo,Ab),e(zo,wd),e(wd,Ib),e(zo,Lb),e(tt,Sb),w(Ds,tt,null),e(tt,Nb),e(tt,$d),e($d,Ob),e(tt,Wb),w(zr,tt,null),u(o,Nh,m),u(o,Co,m),e(Co,Fs),e(Fs,Dd),w(Cr,Dd,null),e(Co,Rb),e(Co,Fd),e(Fd,Hb),u(o,Oh,m),u(o,Le,m),w(jr,Le,null),e(Le,Qb),e(Le,yd),e(yd,Ub),e(Le,Jb),e(Le,Pr),e(Pr,Vb),e(Pr,Si),e(Si,Kb),e(Pr,Gb),e(Le,Xb),e(Le,qr),e(qr,Yb),e(qr,Ar),e(Ar,Zb),e(qr,ev),e(Le,tv),w(ys,Le,null),e(Le,ov),e(Le,ot),w(Ir,ot,null),e(ot,sv),e(ot,jo),e(jo,nv),e(jo,Ni),e(Ni,rv),e(jo,av),e(jo,Bd),e(Bd,iv),e(jo,lv),e(ot,dv),w(Bs,ot,null),e(ot,cv),e(ot,Md),e(Md,hv),e(ot,pv),w(Lr,ot,null),u(o,Wh,m),u(o,Po,m),e(Po,Ms),e(Ms,Ed),w(Sr,Ed,null),e(Po,uv),e(Po,xd),e(xd,fv),u(o,Rh,m),u(o,Se,m),w(Nr,Se,null),e(Se,mv),e(Se,qo),e(qo,gv),e(qo,zd),e(zd,_v),e(qo,bv),e(qo,Cd),e(Cd,vv),e(qo,Tv),e(Se,kv),e(Se,Or),e(Or,wv),e(Or,Oi),e(Oi,$v),e(Or,Dv),e(Se,Fv),e(Se,Wr),e(Wr,yv),e(Wr,Rr),e(Rr,Bv),e(Wr,Mv),e(Se,Ev),w(Es,Se,null),e(Se,xv),e(Se,st),w(Hr,st,null),e(st,zv),e(st,Ao),e(Ao,Cv),e(Ao,Wi),e(Wi,jv),e(Ao,Pv),e(Ao,jd),e(jd,qv),e(Ao,Av),e(st,Iv),w(xs,st,null),e(st,Lv),e(st,Pd),e(Pd,Sv),e(st,Nv),w(Qr,st,null),u(o,Hh,m),u(o,Io,m),e(Io,zs),e(zs,qd),w(Ur,qd,null),e(Io,Ov),e(Io,Ad),e(Ad,Wv),u(o,Qh,m),u(o,Be,m),w(Jr,Be,null),e(Be,Rv),e(Be,Id),e(Id,Hv),e(Be,Qv),e(Be,Vr),e(Vr,Uv),e(Vr,Ri),e(Ri,Jv),e(Vr,Vv),e(Be,Kv),e(Be,Kr),e(Kr,Gv),e(Kr,Gr),e(Gr,Xv),e(Kr,Yv),e(Be,Zv),e(Be,Ld),e(Ld,eT),e(Be,tT),e(Be,Bt),e(Bt,Sd),e(Sd,Xr),e(Xr,oT),e(Bt,sT),e(Bt,Nd),e(Nd,Yr),e(Yr,nT),e(Bt,rT),e(Bt,Od),e(Od,Zr),e(Zr,aT),e(Bt,iT),e(Bt,Wd),e(Wd,ea),e(ea,lT),e(Be,dT),e(Be,nt),w(ta,nt,null),e(nt,cT),e(nt,Lo),e(Lo,hT),e(Lo,Rd),e(Rd,pT),e(Lo,uT),e(Lo,Hd),e(Hd,fT),e(Lo,mT),e(nt,gT),w(Cs,nt,null),e(nt,_T),e(nt,Qd),e(Qd,bT),e(nt,vT),w(oa,nt,null),u(o,Uh,m),u(o,So,m),e(So,js),e(js,Ud),w(sa,Ud,null),e(So,TT),e(So,Jd),e(Jd,kT),u(o,Jh,m),u(o,Me,m),w(na,Me,null),e(Me,wT),e(Me,ra),e(ra,$T),e(ra,Vd),e(Vd,DT),e(ra,FT),e(Me,yT),e(Me,aa),e(aa,BT),e(aa,Hi),e(Hi,MT),e(aa,ET),e(Me,xT),e(Me,ia),e(ia,zT),e(ia,la),e(la,CT),e(ia,jT),e(Me,PT),e(Me,Kd),e(Kd,qT),e(Me,AT),e(Me,Mt),e(Mt,Gd),e(Gd,da),e(da,IT),e(Mt,LT),e(Mt,Xd),e(Xd,ca),e(ca,ST),e(Mt,NT),e(Mt,Yd),e(Yd,ha),e(ha,OT),e(Mt,WT),e(Mt,Zd),e(Zd,pa),e(pa,RT),e(Me,HT),e(Me,rt),w(ua,rt,null),e(rt,QT),e(rt,No),e(No,UT),e(No,ec),e(ec,JT),e(No,VT),e(No,tc),e(tc,KT),e(No,GT),e(rt,XT),w(Ps,rt,null),e(rt,YT),e(rt,oc),e(oc,ZT),e(rt,ek),w(fa,rt,null),u(o,Vh,m),u(o,Oo,m),e(Oo,qs),e(qs,sc),w(ma,sc,null),e(Oo,tk),e(Oo,nc),e(nc,ok),u(o,Kh,m),u(o,Ee,m),w(ga,Ee,null),e(Ee,sk),e(Ee,rc),e(rc,nk),e(Ee,rk),e(Ee,_a),e(_a,ak),e(_a,Qi),e(Qi,ik),e(_a,lk),e(Ee,dk),e(Ee,ba),e(ba,ck),e(ba,va),e(va,hk),e(ba,pk),e(Ee,uk),e(Ee,ac),e(ac,fk),e(Ee,mk),e(Ee,Et),e(Et,ic),e(ic,Ta),e(Ta,gk),e(Et,_k),e(Et,lc),e(lc,ka),e(ka,bk),e(Et,vk),e(Et,dc),e(dc,wa),e(wa,Tk),e(Et,kk),e(Et,cc),e(cc,$a),e($a,wk),e(Ee,$k),e(Ee,at),w(Da,at,null),e(at,Dk),e(at,Wo),e(Wo,Fk),e(Wo,hc),e(hc,yk),e(Wo,Bk),e(Wo,pc),e(pc,Mk),e(Wo,Ek),e(at,xk),w(As,at,null),e(at,zk),e(at,uc),e(uc,Ck),e(at,jk),w(Fa,at,null),u(o,Gh,m),u(o,Ro,m),e(Ro,Is),e(Is,fc),w(ya,fc,null),e(Ro,Pk),e(Ro,mc),e(mc,qk),u(o,Xh,m),u(o,xe,m),w(Ba,xe,null),e(xe,Ak),e(xe,gc),e(gc,Ik),e(xe,Lk),e(xe,Ma),e(Ma,Sk),e(Ma,Ui),e(Ui,Nk),e(Ma,Ok),e(xe,Wk),e(xe,Ea),e(Ea,Rk),e(Ea,xa),e(xa,Hk),e(Ea,Qk),e(xe,Uk),e(xe,_c),e(_c,Jk),e(xe,Vk),e(xe,xt),e(xt,bc),e(bc,za),e(za,Kk),e(xt,Gk),e(xt,vc),e(vc,Ca),e(Ca,Xk),e(xt,Yk),e(xt,Tc),e(Tc,ja),e(ja,Zk),e(xt,ew),e(xt,kc),e(kc,Pa),e(Pa,tw),e(xe,ow),e(xe,it),w(qa,it,null),e(it,sw),e(it,Ho),e(Ho,nw),e(Ho,wc),e(wc,rw),e(Ho,aw),e(Ho,$c),e($c,iw),e(Ho,lw),e(it,dw),w(Ls,it,null),e(it,cw),e(it,Dc),e(Dc,hw),e(it,pw),w(Aa,it,null),u(o,Yh,m),u(o,Qo,m),e(Qo,Ss),e(Ss,Fc),w(Ia,Fc,null),e(Qo,uw),e(Qo,yc),e(yc,fw),u(o,Zh,m),u(o,ze,m),w(La,ze,null),e(ze,mw),e(ze,Bc),e(Bc,gw),e(ze,_w),e(ze,Sa),e(Sa,bw),e(Sa,Ji),e(Ji,vw),e(Sa,Tw),e(ze,kw),e(ze,Na),e(Na,ww),e(Na,Oa),e(Oa,$w),e(Na,Dw),e(ze,Fw),e(ze,Mc),e(Mc,yw),e(ze,Bw),e(ze,zt),e(zt,Ec),e(Ec,Wa),e(Wa,Mw),e(zt,Ew),e(zt,xc),e(xc,Ra),e(Ra,xw),e(zt,zw),e(zt,zc),e(zc,Ha),e(Ha,Cw),e(zt,jw),e(zt,Cc),e(Cc,Qa),e(Qa,Pw),e(ze,qw),e(ze,lt),w(Ua,lt,null),e(lt,Aw),e(lt,Uo),e(Uo,Iw),e(Uo,jc),e(jc,Lw),e(Uo,Sw),e(Uo,Pc),e(Pc,Nw),e(Uo,Ow),e(lt,Ww),w(Ns,lt,null),e(lt,Rw),e(lt,qc),e(qc,Hw),e(lt,Qw),w(Ja,lt,null),u(o,ep,m),u(o,Jo,m),e(Jo,Os),e(Os,Ac),w(Va,Ac,null),e(Jo,Uw),e(Jo,Ic),e(Ic,Jw),u(o,tp,m),u(o,Ce,m),w(Ka,Ce,null),e(Ce,Vw),e(Ce,Vo),e(Vo,Kw),e(Vo,Lc),e(Lc,Gw),e(Vo,Xw),e(Vo,Sc),e(Sc,Yw),e(Vo,Zw),e(Ce,e1),e(Ce,Ga),e(Ga,t1),e(Ga,Vi),e(Vi,o1),e(Ga,s1),e(Ce,n1),e(Ce,Xa),e(Xa,r1),e(Xa,Ya),e(Ya,a1),e(Xa,i1),e(Ce,l1),e(Ce,Nc),e(Nc,d1),e(Ce,c1),e(Ce,Ct),e(Ct,Oc),e(Oc,Za),e(Za,h1),e(Ct,p1),e(Ct,Wc),e(Wc,ei),e(ei,u1),e(Ct,f1),e(Ct,Rc),e(Rc,ti),e(ti,m1),e(Ct,g1),e(Ct,Hc),e(Hc,oi),e(oi,_1),e(Ce,b1),e(Ce,dt),w(si,dt,null),e(dt,v1),e(dt,Ko),e(Ko,T1),e(Ko,Qc),e(Qc,k1),e(Ko,w1),e(Ko,Uc),e(Uc,$1),e(Ko,D1),e(dt,F1),w(Ws,dt,null),e(dt,y1),e(dt,Jc),e(Jc,B1),e(dt,M1),w(ni,dt,null),op=!0},p(o,[m]){const ri={};m&2&&(ri.$$scope={dirty:m,ctx:o}),os.$set(ri);const Vc={};m&2&&(Vc.$$scope={dirty:m,ctx:o}),ns.$set(Vc);const Kc={};m&2&&(Kc.$$scope={dirty:m,ctx:o}),as.$set(Kc);const Gc={};m&2&&(Gc.$$scope={dirty:m,ctx:o}),ls.$set(Gc);const ai={};m&2&&(ai.$$scope={dirty:m,ctx:o}),cs.$set(ai);const Xc={};m&2&&(Xc.$$scope={dirty:m,ctx:o}),ps.$set(Xc);const Yc={};m&2&&(Yc.$$scope={dirty:m,ctx:o}),fs.$set(Yc);const Zc={};m&2&&(Zc.$$scope={dirty:m,ctx:o}),ms.$set(Zc);const jt={};m&2&&(jt.$$scope={dirty:m,ctx:o}),_s.$set(jt);const eh={};m&2&&(eh.$$scope={dirty:m,ctx:o}),bs.$set(eh);const th={};m&2&&(th.$$scope={dirty:m,ctx:o}),Ts.$set(th);const oh={};m&2&&(oh.$$scope={dirty:m,ctx:o}),ks.$set(oh);const sh={};m&2&&(sh.$$scope={dirty:m,ctx:o}),$s.$set(sh);const nh={};m&2&&(nh.$$scope={dirty:m,ctx:o}),Ds.$set(nh);const rh={};m&2&&(rh.$$scope={dirty:m,ctx:o}),ys.$set(rh);const ah={};m&2&&(ah.$$scope={dirty:m,ctx:o}),Bs.$set(ah);const ii={};m&2&&(ii.$$scope={dirty:m,ctx:o}),Es.$set(ii);const Pt={};m&2&&(Pt.$$scope={dirty:m,ctx:o}),xs.$set(Pt);const ih={};m&2&&(ih.$$scope={dirty:m,ctx:o}),Cs.$set(ih);const lh={};m&2&&(lh.$$scope={dirty:m,ctx:o}),Ps.$set(lh);const dh={};m&2&&(dh.$$scope={dirty:m,ctx:o}),As.$set(dh);const li={};m&2&&(li.$$scope={dirty:m,ctx:o}),Ls.$set(li);const ch={};m&2&&(ch.$$scope={dirty:m,ctx:o}),Ns.$set(ch);const qt={};m&2&&(qt.$$scope={dirty:m,ctx:o}),Ws.$set(qt)},i(o){op||($(_.$$.fragment,o),$(X.$$.fragment,o),$(tn.$$.fragment,o),$(on.$$.fragment,o),$(nn.$$.fragment,o),$(rn.$$.fragment,o),$(an.$$.fragment,o),$(dn.$$.fragment,o),$(cn.$$.fragment,o),$(un.$$.fragment,o),$(fn.$$.fragment,o),$(bn.$$.fragment,o),$(os.$$.fragment,o),$(vn.$$.fragment,o),$(Tn.$$.fragment,o),$(kn.$$.fragment,o),$(yn.$$.fragment,o),$(ns.$$.fragment,o),$(Bn.$$.fragment,o),$(Mn.$$.fragment,o),$(En.$$.fragment,o),$(jn.$$.fragment,o),$(as.$$.fragment,o),$(Pn.$$.fragment,o),$(qn.$$.fragment,o),$(An.$$.fragment,o),$(In.$$.fragment,o),$(On.$$.fragment,o),$(ls.$$.fragment,o),$(Wn.$$.fragment,o),$(Rn.$$.fragment,o),$(Hn.$$.fragment,o),$(Vn.$$.fragment,o),$(cs.$$.fragment,o),$(Kn.$$.fragment,o),$(Gn.$$.fragment,o),$(Xn.$$.fragment,o),$(tr.$$.fragment,o),$(ps.$$.fragment,o),$(or.$$.fragment,o),$(sr.$$.fragment,o),$(nr.$$.fragment,o),$(fs.$$.fragment,o),$(lr.$$.fragment,o),$(ms.$$.fragment,o),$(dr.$$.fragment,o),$(cr.$$.fragment,o),$(hr.$$.fragment,o),$(_s.$$.fragment,o),$(gr.$$.fragment,o),$(bs.$$.fragment,o),$(_r.$$.fragment,o),$(br.$$.fragment,o),$(vr.$$.fragment,o),$(Ts.$$.fragment,o),$($r.$$.fragment,o),$(ks.$$.fragment,o),$(Dr.$$.fragment,o),$(Fr.$$.fragment,o),$(yr.$$.fragment,o),$($s.$$.fragment,o),$(xr.$$.fragment,o),$(Ds.$$.fragment,o),$(zr.$$.fragment,o),$(Cr.$$.fragment,o),$(jr.$$.fragment,o),$(ys.$$.fragment,o),$(Ir.$$.fragment,o),$(Bs.$$.fragment,o),$(Lr.$$.fragment,o),$(Sr.$$.fragment,o),$(Nr.$$.fragment,o),$(Es.$$.fragment,o),$(Hr.$$.fragment,o),$(xs.$$.fragment,o),$(Qr.$$.fragment,o),$(Ur.$$.fragment,o),$(Jr.$$.fragment,o),$(ta.$$.fragment,o),$(Cs.$$.fragment,o),$(oa.$$.fragment,o),$(sa.$$.fragment,o),$(na.$$.fragment,o),$(ua.$$.fragment,o),$(Ps.$$.fragment,o),$(fa.$$.fragment,o),$(ma.$$.fragment,o),$(ga.$$.fragment,o),$(Da.$$.fragment,o),$(As.$$.fragment,o),$(Fa.$$.fragment,o),$(ya.$$.fragment,o),$(Ba.$$.fragment,o),$(qa.$$.fragment,o),$(Ls.$$.fragment,o),$(Aa.$$.fragment,o),$(Ia.$$.fragment,o),$(La.$$.fragment,o),$(Ua.$$.fragment,o),$(Ns.$$.fragment,o),$(Ja.$$.fragment,o),$(Va.$$.fragment,o),$(Ka.$$.fragment,o),$(si.$$.fragment,o),$(Ws.$$.fragment,o),$(ni.$$.fragment,o),op=!0)},o(o){D(_.$$.fragment,o),D(X.$$.fragment,o),D(tn.$$.fragment,o),D(on.$$.fragment,o),D(nn.$$.fragment,o),D(rn.$$.fragment,o),D(an.$$.fragment,o),D(dn.$$.fragment,o),D(cn.$$.fragment,o),D(un.$$.fragment,o),D(fn.$$.fragment,o),D(bn.$$.fragment,o),D(os.$$.fragment,o),D(vn.$$.fragment,o),D(Tn.$$.fragment,o),D(kn.$$.fragment,o),D(yn.$$.fragment,o),D(ns.$$.fragment,o),D(Bn.$$.fragment,o),D(Mn.$$.fragment,o),D(En.$$.fragment,o),D(jn.$$.fragment,o),D(as.$$.fragment,o),D(Pn.$$.fragment,o),D(qn.$$.fragment,o),D(An.$$.fragment,o),D(In.$$.fragment,o),D(On.$$.fragment,o),D(ls.$$.fragment,o),D(Wn.$$.fragment,o),D(Rn.$$.fragment,o),D(Hn.$$.fragment,o),D(Vn.$$.fragment,o),D(cs.$$.fragment,o),D(Kn.$$.fragment,o),D(Gn.$$.fragment,o),D(Xn.$$.fragment,o),D(tr.$$.fragment,o),D(ps.$$.fragment,o),D(or.$$.fragment,o),D(sr.$$.fragment,o),D(nr.$$.fragment,o),D(fs.$$.fragment,o),D(lr.$$.fragment,o),D(ms.$$.fragment,o),D(dr.$$.fragment,o),D(cr.$$.fragment,o),D(hr.$$.fragment,o),D(_s.$$.fragment,o),D(gr.$$.fragment,o),D(bs.$$.fragment,o),D(_r.$$.fragment,o),D(br.$$.fragment,o),D(vr.$$.fragment,o),D(Ts.$$.fragment,o),D($r.$$.fragment,o),D(ks.$$.fragment,o),D(Dr.$$.fragment,o),D(Fr.$$.fragment,o),D(yr.$$.fragment,o),D($s.$$.fragment,o),D(xr.$$.fragment,o),D(Ds.$$.fragment,o),D(zr.$$.fragment,o),D(Cr.$$.fragment,o),D(jr.$$.fragment,o),D(ys.$$.fragment,o),D(Ir.$$.fragment,o),D(Bs.$$.fragment,o),D(Lr.$$.fragment,o),D(Sr.$$.fragment,o),D(Nr.$$.fragment,o),D(Es.$$.fragment,o),D(Hr.$$.fragment,o),D(xs.$$.fragment,o),D(Qr.$$.fragment,o),D(Ur.$$.fragment,o),D(Jr.$$.fragment,o),D(ta.$$.fragment,o),D(Cs.$$.fragment,o),D(oa.$$.fragment,o),D(sa.$$.fragment,o),D(na.$$.fragment,o),D(ua.$$.fragment,o),D(Ps.$$.fragment,o),D(fa.$$.fragment,o),D(ma.$$.fragment,o),D(ga.$$.fragment,o),D(Da.$$.fragment,o),D(As.$$.fragment,o),D(Fa.$$.fragment,o),D(ya.$$.fragment,o),D(Ba.$$.fragment,o),D(qa.$$.fragment,o),D(Ls.$$.fragment,o),D(Aa.$$.fragment,o),D(Ia.$$.fragment,o),D(La.$$.fragment,o),D(Ua.$$.fragment,o),D(Ns.$$.fragment,o),D(Ja.$$.fragment,o),D(Va.$$.fragment,o),D(Ka.$$.fragment,o),D(si.$$.fragment,o),D(Ws.$$.fragment,o),D(ni.$$.fragment,o),op=!1},d(o){t(h),o&&t(y),o&&t(g),F(_),o&&t(J),o&&t(E),F(X),o&&t(re),o&&t(L),o&&t(ae),o&&t(ee),o&&t(ie),o&&t(S),o&&t(P),o&&t(te),o&&t(le),o&&t(p),o&&t(uh),o&&t(vt),o&&t(fh),o&&t(ao),F(tn),o&&t(mh),o&&t(Ne),F(on),F(nn),o&&t(gh),o&&t(lo),F(rn),o&&t(_h),o&&t(_t),F(an),o&&t(bh),o&&t(co),F(dn),o&&t(vh),o&&t(bt),F(cn),o&&t(Th),o&&t(ho),F(un),o&&t(kh),o&&t(Oe),F(fn),F(bn),F(os),F(vn),o&&t(wh),o&&t(uo),F(Tn),o&&t($h),o&&t(We),F(kn),F(yn),F(ns),F(Bn),o&&t(Dh),o&&t(mo),F(Mn),o&&t(Fh),o&&t(Re),F(En),F(jn),F(as),F(Pn),F(qn),o&&t(yh),o&&t(_o),F(An),o&&t(Bh),o&&t(He),F(In),F(On),F(ls),F(Wn),o&&t(Mh),o&&t(vo),F(Rn),o&&t(Eh),o&&t(Qe),F(Hn),F(Vn),F(cs),F(Kn),o&&t(xh),o&&t(ko),F(Gn),o&&t(zh),o&&t(Ue),F(Xn),F(tr),F(ps),F(or),o&&t(Ch),o&&t(Do),F(sr),o&&t(jh),o&&t(Pe),F(nr),F(fs),F(lr),F(ms),F(dr),o&&t(Ph),o&&t(yo),F(cr),o&&t(qh),o&&t(qe),F(hr),F(_s),F(gr),F(bs),F(_r),o&&t(Ah),o&&t(Mo),F(br),o&&t(Ih),o&&t(Ae),F(vr),F(Ts),F($r),F(ks),F(Dr),o&&t(Lh),o&&t(xo),F(Fr),o&&t(Sh),o&&t(Ie),F(yr),F($s),F(xr),F(Ds),F(zr),o&&t(Nh),o&&t(Co),F(Cr),o&&t(Oh),o&&t(Le),F(jr),F(ys),F(Ir),F(Bs),F(Lr),o&&t(Wh),o&&t(Po),F(Sr),o&&t(Rh),o&&t(Se),F(Nr),F(Es),F(Hr),F(xs),F(Qr),o&&t(Hh),o&&t(Io),F(Ur),o&&t(Qh),o&&t(Be),F(Jr),F(ta),F(Cs),F(oa),o&&t(Uh),o&&t(So),F(sa),o&&t(Jh),o&&t(Me),F(na),F(ua),F(Ps),F(fa),o&&t(Vh),o&&t(Oo),F(ma),o&&t(Kh),o&&t(Ee),F(ga),F(Da),F(As),F(Fa),o&&t(Gh),o&&t(Ro),F(ya),o&&t(Xh),o&&t(xe),F(Ba),F(qa),F(Ls),F(Aa),o&&t(Yh),o&&t(Qo),F(Ia),o&&t(Zh),o&&t(ze),F(La),F(Ua),F(Ns),F(Ja),o&&t(ep),o&&t(Jo),F(Va),o&&t(tp),o&&t(Ce),F(Ka),F(si),F(Ws),F(ni)}}}const Py={local:"distilbert",sections:[{local:"overview",title:"Overview"},{local:"transformers.DistilBertConfig",title:"DistilBertConfig"},{local:"transformers.DistilBertTokenizer",title:"DistilBertTokenizer"},{local:"transformers.DistilBertTokenizerFast",title:"DistilBertTokenizerFast"},{local:"transformers.DistilBertModel",title:"DistilBertModel"},{local:"transformers.DistilBertForMaskedLM",title:"DistilBertForMaskedLM"},{local:"transformers.DistilBertForSequenceClassification",title:"DistilBertForSequenceClassification"},{local:"transformers.DistilBertForMultipleChoice",title:"DistilBertForMultipleChoice"},{local:"transformers.DistilBertForTokenClassification",title:"DistilBertForTokenClassification"},{local:"transformers.DistilBertForQuestionAnswering",title:"DistilBertForQuestionAnswering"},{local:"transformers.TFDistilBertModel",title:"TFDistilBertModel"},{local:"transformers.TFDistilBertForMaskedLM",title:"TFDistilBertForMaskedLM"},{local:"transformers.TFDistilBertForSequenceClassification",title:"TFDistilBertForSequenceClassification"},{local:"transformers.TFDistilBertForMultipleChoice",title:"TFDistilBertForMultipleChoice"},{local:"transformers.TFDistilBertForTokenClassification",title:"TFDistilBertForTokenClassification"},{local:"transformers.TFDistilBertForQuestionAnswering",title:"TFDistilBertForQuestionAnswering"},{local:"transformers.FlaxDistilBertModel",title:"FlaxDistilBertModel"},{local:"transformers.FlaxDistilBertForMaskedLM",title:"FlaxDistilBertForMaskedLM"},{local:"transformers.FlaxDistilBertForSequenceClassification",title:"FlaxDistilBertForSequenceClassification"},{local:"transformers.FlaxDistilBertForMultipleChoice",title:"FlaxDistilBertForMultipleChoice"},{local:"transformers.FlaxDistilBertForTokenClassification",title:"FlaxDistilBertForTokenClassification"},{local:"transformers.FlaxDistilBertForQuestionAnswering",title:"FlaxDistilBertForQuestionAnswering"}],title:"DistilBERT"};function qy(C,h,y){let{fw:g}=h;return C.$$set=b=>{"fw"in b&&y(0,g=b.fw)},[g]}class Wy extends ry{constructor(h){super();ay(this,h,qy,jy,iy,{fw:0})}}export{Wy as default,Py as metadata};
