import{S as F$,i as v$,s as k$,e as s,k as l,w as k,t as n,L as b$,c as r,d as t,m as d,a,x as b,h as o,b as c,J as e,g as h,y as w,q as y,o as $,B as E}from"../../../chunks/vendor-9e2b328e.js";import{T as ze}from"../../../chunks/Tip-76f97a76.js";import{D as ee}from"../../../chunks/Docstring-50fd6873.js";import{C as Ne}from"../../../chunks/CodeBlock-b9ff96e9.js";import{I as qe}from"../../../chunks/IconCopyLink-fd0e58fd.js";import"../../../chunks/CopyButton-4b97cbf7.js";function w$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function y$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function $$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function E$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function M$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function z$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function q$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function P$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function C$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,te,ue,O,pe,ie,U,L,ne,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=n("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=n("This second option is useful when using "),I=s("code"),te=n("tf.keras.Model.fit"),ue=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=n("model(inputs)"),ie=n("."),U=l(),L=s("p"),ne=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=n("a single Tensor with "),Q=s("code"),le=n("input_ids"),se=n(" only and nothing else: "),S=s("code"),he=n("model(inputs_ids)"),de=l(),C=s("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=n("model([input_ids, attention_mask])"),ae=n(" or "),R=s("code"),me=n("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(t),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(t),_=d(K),z=r(K,"LI",{});var be=a(z);ce=o(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);te=o(Fe,"tf.keras.Model.fit"),Fe.forEach(t),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var we=a(O);pe=o(we,"model(inputs)"),we.forEach(t),ie=o(D,"."),D.forEach(t),U=d(u),L=r(u,"P",{});var ye=a(L);ne=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(t),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(t),V.forEach(t),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(t),Y.forEach(t),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(t),_e.forEach(t),j.forEach(t)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,te),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,ne),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&t(p),u&&t(m),u&&t(g),u&&t(G),u&&t(q),u&&t(U),u&&t(L),u&&t(Z),u&&t(P)}}}function x$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function j$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,te,ue,O,pe,ie,U,L,ne,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=n("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=n("This second option is useful when using "),I=s("code"),te=n("tf.keras.Model.fit"),ue=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=n("model(inputs)"),ie=n("."),U=l(),L=s("p"),ne=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=n("a single Tensor with "),Q=s("code"),le=n("input_ids"),se=n(" only and nothing else: "),S=s("code"),he=n("model(inputs_ids)"),de=l(),C=s("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=n("model([input_ids, attention_mask])"),ae=n(" or "),R=s("code"),me=n("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(t),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(t),_=d(K),z=r(K,"LI",{});var be=a(z);ce=o(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);te=o(Fe,"tf.keras.Model.fit"),Fe.forEach(t),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var we=a(O);pe=o(we,"model(inputs)"),we.forEach(t),ie=o(D,"."),D.forEach(t),U=d(u),L=r(u,"P",{});var ye=a(L);ne=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(t),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(t),V.forEach(t),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(t),Y.forEach(t),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(t),_e.forEach(t),j.forEach(t)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,te),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,ne),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&t(p),u&&t(m),u&&t(g),u&&t(G),u&&t(q),u&&t(U),u&&t(L),u&&t(Z),u&&t(P)}}}function L$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function A$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,te,ue,O,pe,ie,U,L,ne,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=n("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=n("This second option is useful when using "),I=s("code"),te=n("tf.keras.Model.fit"),ue=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=n("model(inputs)"),ie=n("."),U=l(),L=s("p"),ne=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=n("a single Tensor with "),Q=s("code"),le=n("input_ids"),se=n(" only and nothing else: "),S=s("code"),he=n("model(inputs_ids)"),de=l(),C=s("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=n("model([input_ids, attention_mask])"),ae=n(" or "),R=s("code"),me=n("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(t),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(t),_=d(K),z=r(K,"LI",{});var be=a(z);ce=o(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);te=o(Fe,"tf.keras.Model.fit"),Fe.forEach(t),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var we=a(O);pe=o(we,"model(inputs)"),we.forEach(t),ie=o(D,"."),D.forEach(t),U=d(u),L=r(u,"P",{});var ye=a(L);ne=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(t),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(t),V.forEach(t),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(t),Y.forEach(t),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(t),_e.forEach(t),j.forEach(t)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,te),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,ne),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&t(p),u&&t(m),u&&t(g),u&&t(G),u&&t(q),u&&t(U),u&&t(L),u&&t(Z),u&&t(P)}}}function D$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function I$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,te,ue,O,pe,ie,U,L,ne,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=n("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=n("This second option is useful when using "),I=s("code"),te=n("tf.keras.Model.fit"),ue=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=n("model(inputs)"),ie=n("."),U=l(),L=s("p"),ne=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=n("a single Tensor with "),Q=s("code"),le=n("input_ids"),se=n(" only and nothing else: "),S=s("code"),he=n("model(inputs_ids)"),de=l(),C=s("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=n("model([input_ids, attention_mask])"),ae=n(" or "),R=s("code"),me=n("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(t),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(t),_=d(K),z=r(K,"LI",{});var be=a(z);ce=o(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);te=o(Fe,"tf.keras.Model.fit"),Fe.forEach(t),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var we=a(O);pe=o(we,"model(inputs)"),we.forEach(t),ie=o(D,"."),D.forEach(t),U=d(u),L=r(u,"P",{});var ye=a(L);ne=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(t),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(t),V.forEach(t),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(t),Y.forEach(t),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(t),_e.forEach(t),j.forEach(t)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,te),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,ne),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&t(p),u&&t(m),u&&t(g),u&&t(G),u&&t(q),u&&t(U),u&&t(L),u&&t(Z),u&&t(P)}}}function O$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function S$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,te,ue,O,pe,ie,U,L,ne,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=n("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=n("This second option is useful when using "),I=s("code"),te=n("tf.keras.Model.fit"),ue=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=n("model(inputs)"),ie=n("."),U=l(),L=s("p"),ne=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=n("a single Tensor with "),Q=s("code"),le=n("input_ids"),se=n(" only and nothing else: "),S=s("code"),he=n("model(inputs_ids)"),de=l(),C=s("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=n("model([input_ids, attention_mask])"),ae=n(" or "),R=s("code"),me=n("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(t),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(t),_=d(K),z=r(K,"LI",{});var be=a(z);ce=o(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);te=o(Fe,"tf.keras.Model.fit"),Fe.forEach(t),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var we=a(O);pe=o(we,"model(inputs)"),we.forEach(t),ie=o(D,"."),D.forEach(t),U=d(u),L=r(u,"P",{});var ye=a(L);ne=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(t),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(t),V.forEach(t),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(t),Y.forEach(t),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(t),_e.forEach(t),j.forEach(t)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,te),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,ne),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&t(p),u&&t(m),u&&t(g),u&&t(G),u&&t(q),u&&t(U),u&&t(L),u&&t(Z),u&&t(P)}}}function N$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function B$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,te,ue,O,pe,ie,U,L,ne,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=n("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=n("This second option is useful when using "),I=s("code"),te=n("tf.keras.Model.fit"),ue=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=n("model(inputs)"),ie=n("."),U=l(),L=s("p"),ne=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=n("a single Tensor with "),Q=s("code"),le=n("input_ids"),se=n(" only and nothing else: "),S=s("code"),he=n("model(inputs_ids)"),de=l(),C=s("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=n("model([input_ids, attention_mask])"),ae=n(" or "),R=s("code"),me=n("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(t),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(t),_=d(K),z=r(K,"LI",{});var be=a(z);ce=o(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);te=o(Fe,"tf.keras.Model.fit"),Fe.forEach(t),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var we=a(O);pe=o(we,"model(inputs)"),we.forEach(t),ie=o(D,"."),D.forEach(t),U=d(u),L=r(u,"P",{});var ye=a(L);ne=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(t),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(t),V.forEach(t),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(t),Y.forEach(t),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(t),_e.forEach(t),j.forEach(t)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,te),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,ne),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&t(p),u&&t(m),u&&t(g),u&&t(G),u&&t(q),u&&t(U),u&&t(L),u&&t(Z),u&&t(P)}}}function W$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function Q$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,te,ue,O,pe,ie,U,L,ne,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=n("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=n("This second option is useful when using "),I=s("code"),te=n("tf.keras.Model.fit"),ue=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=n("model(inputs)"),ie=n("."),U=l(),L=s("p"),ne=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=n("a single Tensor with "),Q=s("code"),le=n("input_ids"),se=n(" only and nothing else: "),S=s("code"),he=n("model(inputs_ids)"),de=l(),C=s("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=n("model([input_ids, attention_mask])"),ae=n(" or "),R=s("code"),me=n("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(t),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(t),_=d(K),z=r(K,"LI",{});var be=a(z);ce=o(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);te=o(Fe,"tf.keras.Model.fit"),Fe.forEach(t),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var we=a(O);pe=o(we,"model(inputs)"),we.forEach(t),ie=o(D,"."),D.forEach(t),U=d(u),L=r(u,"P",{});var ye=a(L);ne=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(t),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(t),V.forEach(t),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(t),Y.forEach(t),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(t),_e.forEach(t),j.forEach(t)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,te),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,ne),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&t(p),u&&t(m),u&&t(g),u&&t(G),u&&t(q),u&&t(U),u&&t(L),u&&t(Z),u&&t(P)}}}function R$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function H$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,te,ue,O,pe,ie,U,L,ne,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge;return{c(){p=s("p"),M=n("TF 2.0 models accepts two formats as inputs:"),m=l(),g=s("ul"),F=s("li"),T=n("having all inputs as keyword arguments (like PyTorch models), or"),_=l(),z=s("li"),ce=n("having all inputs as a list, tuple or dict in the first positional arguments."),G=l(),q=s("p"),X=n("This second option is useful when using "),I=s("code"),te=n("tf.keras.Model.fit"),ue=n(` method which currently requires having all the
tensors in the first argument of the model call function: `),O=s("code"),pe=n("model(inputs)"),ie=n("."),U=l(),L=s("p"),ne=n(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),Z=l(),P=s("ul"),x=s("li"),oe=n("a single Tensor with "),Q=s("code"),le=n("input_ids"),se=n(" only and nothing else: "),S=s("code"),he=n("model(inputs_ids)"),de=l(),C=s("li"),fe=n(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=s("code"),J=n("model([input_ids, attention_mask])"),ae=n(" or "),R=s("code"),me=n("model([input_ids, attention_mask, token_type_ids])"),N=l(),A=s("li"),re=n(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=s("code"),ge=n('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(u){p=r(u,"P",{});var v=a(p);M=o(v,"TF 2.0 models accepts two formats as inputs:"),v.forEach(t),m=d(u),g=r(u,"UL",{});var K=a(g);F=r(K,"LI",{});var Te=a(F);T=o(Te,"having all inputs as keyword arguments (like PyTorch models), or"),Te.forEach(t),_=d(K),z=r(K,"LI",{});var be=a(z);ce=o(be,"having all inputs as a list, tuple or dict in the first positional arguments."),be.forEach(t),K.forEach(t),G=d(u),q=r(u,"P",{});var D=a(q);X=o(D,"This second option is useful when using "),I=r(D,"CODE",{});var Fe=a(I);te=o(Fe,"tf.keras.Model.fit"),Fe.forEach(t),ue=o(D,` method which currently requires having all the
tensors in the first argument of the model call function: `),O=r(D,"CODE",{});var we=a(O);pe=o(we,"model(inputs)"),we.forEach(t),ie=o(D,"."),D.forEach(t),U=d(u),L=r(u,"P",{});var ye=a(L);ne=o(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),Z=d(u),P=r(u,"UL",{});var j=a(P);x=r(j,"LI",{});var V=a(x);oe=o(V,"a single Tensor with "),Q=r(V,"CODE",{});var $e=a(Q);le=o($e,"input_ids"),$e.forEach(t),se=o(V," only and nothing else: "),S=r(V,"CODE",{});var ve=a(S);he=o(ve,"model(inputs_ids)"),ve.forEach(t),V.forEach(t),de=d(j),C=r(j,"LI",{});var Y=a(C);fe=o(Y,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),B=r(Y,"CODE",{});var Ee=a(B);J=o(Ee,"model([input_ids, attention_mask])"),Ee.forEach(t),ae=o(Y," or "),R=r(Y,"CODE",{});var ke=a(R);me=o(ke,"model([input_ids, attention_mask, token_type_ids])"),ke.forEach(t),Y.forEach(t),N=d(j),A=r(j,"LI",{});var _e=a(A);re=o(_e,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),H=r(_e,"CODE",{});var Me=a(H);ge=o(Me,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Me.forEach(t),_e.forEach(t),j.forEach(t)},m(u,v){h(u,p,v),e(p,M),h(u,m,v),h(u,g,v),e(g,F),e(F,T),e(g,_),e(g,z),e(z,ce),h(u,G,v),h(u,q,v),e(q,X),e(q,I),e(I,te),e(q,ue),e(q,O),e(O,pe),e(q,ie),h(u,U,v),h(u,L,v),e(L,ne),h(u,Z,v),h(u,P,v),e(P,x),e(x,oe),e(x,Q),e(Q,le),e(x,se),e(x,S),e(S,he),e(P,de),e(P,C),e(C,fe),e(C,B),e(B,J),e(C,ae),e(C,R),e(R,me),e(P,N),e(P,A),e(A,re),e(A,H),e(H,ge)},d(u){u&&t(p),u&&t(m),u&&t(g),u&&t(G),u&&t(q),u&&t(U),u&&t(L),u&&t(Z),u&&t(P)}}}function V$(W){let p,M,m,g,F;return{c(){p=s("p"),M=n("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s("code"),g=n("Module"),F=n(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(T){p=r(T,"P",{});var _=a(p);M=o(_,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=r(_,"CODE",{});var z=a(m);g=o(z,"Module"),z.forEach(t),F=o(_,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),_.forEach(t)},m(T,_){h(T,p,_),e(p,M),e(p,m),e(m,g),e(p,F)},d(T){T&&t(p)}}}function Y$(W){let p,M,m,g,F,T,_,z,ce,G,q,X,I,te,ue,O,pe,ie,U,L,ne,Z,P,x,oe,Q,le,se,S,he,de,C,fe,B,J,ae,R,me,N,A,re,H,ge,u,v,K,Te,be,D,Fe,we,ye,j,V,$e,ve,Y,Ee,ke,_e,Me,Va,Dp,Ip,yc,xt,Op,Io,Sp,Np,Oo,Bp,Wp,$c,Zt,Nn,ll,So,Qp,dl,Rp,Ec,Pt,No,Hp,Ct,Vp,Ya,Yp,Up,Ua,Gp,Zp,Bo,Kp,Xp,Jp,Kt,eh,Ga,th,nh,Za,oh,sh,Mc,Xt,Bn,cl,Wo,rh,ul,ah,zc,Pe,Qo,ih,pl,lh,dh,Wn,Ka,ch,uh,Xa,ph,hh,fh,Ro,mh,Ja,gh,_h,Th,jt,Ho,Fh,hl,vh,kh,Vo,ei,bh,fl,wh,yh,ti,$h,ml,Eh,Mh,Qn,Yo,zh,Uo,qh,gl,Ph,Ch,xh,kt,Go,jh,_l,Lh,Ah,Zo,Dh,Jt,Ih,Tl,Oh,Sh,Fl,Nh,Bh,Wh,vl,qc,en,Rn,kl,Ko,Qh,bl,Rh,Pc,Ze,Xo,Hh,Jo,Vh,wl,Yh,Uh,Gh,Hn,ni,Zh,Kh,oi,Xh,Jh,ef,es,tf,si,nf,of,sf,bt,ts,rf,yl,af,lf,ns,df,tn,cf,$l,uf,pf,El,hf,ff,Cc,nn,Vn,Ml,os,mf,zl,gf,xc,on,ss,_f,rs,Tf,ri,Ff,vf,jc,sn,as,kf,is,bf,ai,wf,yf,Lc,rn,Yn,ql,ls,$f,Pl,Ef,Ac,We,ds,Mf,Cl,zf,qf,cs,Pf,us,Cf,xf,jf,ps,Lf,ii,Af,Df,If,hs,Of,fs,Sf,Nf,Bf,Ke,ms,Wf,an,Qf,li,Rf,Hf,xl,Vf,Yf,Uf,Un,Gf,jl,Zf,Kf,gs,Dc,ln,Gn,Ll,_s,Xf,Al,Jf,Ic,Qe,Ts,em,Dl,tm,nm,Fs,om,vs,sm,rm,am,ks,im,di,lm,dm,cm,bs,um,ws,pm,hm,fm,Xe,ys,mm,dn,gm,ci,_m,Tm,Il,Fm,vm,km,Zn,bm,Ol,wm,ym,$s,Oc,cn,Kn,Sl,Es,$m,Nl,Em,Sc,Ms,Je,zs,Mm,un,zm,ui,qm,Pm,Bl,Cm,xm,jm,Xn,Lm,Wl,Am,Dm,qs,Nc,pn,Jn,Ql,Ps,Im,Rl,Om,Bc,Re,Cs,Sm,xs,Nm,Hl,Bm,Wm,Qm,js,Rm,Ls,Hm,Vm,Ym,As,Um,pi,Gm,Zm,Km,Ds,Xm,Is,Jm,eg,tg,et,Os,ng,hn,og,hi,sg,rg,Vl,ag,ig,lg,eo,dg,Yl,cg,ug,Ss,Wc,fn,to,Ul,Ns,pg,Gl,hg,Qc,He,Bs,fg,Zl,mg,gg,Ws,_g,Qs,Tg,Fg,vg,Rs,kg,fi,bg,wg,yg,Hs,$g,Vs,Eg,Mg,zg,Be,Ys,qg,mn,Pg,mi,Cg,xg,Kl,jg,Lg,Ag,no,Dg,Xl,Ig,Og,Us,Sg,Jl,Ng,Bg,Gs,Rc,gn,oo,ed,Zs,Wg,td,Qg,Hc,Ve,Ks,Rg,nd,Hg,Vg,Xs,Yg,Js,Ug,Gg,Zg,er,Kg,gi,Xg,Jg,e_,tr,t_,nr,n_,o_,s_,tt,or,r_,_n,a_,_i,i_,l_,od,d_,c_,u_,so,p_,sd,h_,f_,sr,Vc,Tn,ro,rd,rr,m_,ad,g_,Yc,Ye,ar,__,id,T_,F_,ir,v_,lr,k_,b_,w_,dr,y_,Ti,$_,E_,M_,cr,z_,ur,q_,P_,C_,nt,pr,x_,Fn,j_,Fi,L_,A_,ld,D_,I_,O_,ao,S_,dd,N_,B_,hr,Uc,vn,io,cd,fr,W_,ud,Q_,Gc,Ue,mr,R_,kn,H_,pd,V_,Y_,hd,U_,G_,Z_,gr,K_,_r,X_,J_,eT,Tr,tT,vi,nT,oT,sT,Fr,rT,vr,aT,iT,lT,ot,kr,dT,bn,cT,ki,uT,pT,fd,hT,fT,mT,lo,gT,md,_T,TT,br,Zc,wn,co,gd,wr,FT,_d,vT,Kc,xe,yr,kT,Td,bT,wT,$r,yT,Er,$T,ET,MT,Mr,zT,bi,qT,PT,CT,zr,xT,qr,jT,LT,AT,uo,DT,st,Pr,IT,yn,OT,wi,ST,NT,Fd,BT,WT,QT,po,RT,vd,HT,VT,Cr,Xc,$n,ho,kd,xr,YT,bd,UT,Jc,je,jr,GT,wd,ZT,KT,Lr,XT,Ar,JT,eF,tF,Dr,nF,yi,oF,sF,rF,Ir,aF,Or,iF,lF,dF,fo,cF,rt,Sr,uF,En,pF,$i,hF,fF,yd,mF,gF,_F,mo,TF,$d,FF,vF,Nr,eu,Mn,go,Ed,Br,kF,Md,bF,tu,Le,Wr,wF,zd,yF,$F,Qr,EF,Rr,MF,zF,qF,Hr,PF,Ei,CF,xF,jF,Vr,LF,Yr,AF,DF,IF,_o,OF,at,Ur,SF,zn,NF,Mi,BF,WF,qd,QF,RF,HF,To,VF,Pd,YF,UF,Gr,nu,qn,Fo,Cd,Zr,GF,xd,ZF,ou,Ae,Kr,KF,Xr,XF,jd,JF,ev,tv,Jr,nv,ea,ov,sv,rv,ta,av,zi,iv,lv,dv,na,cv,oa,uv,pv,hv,vo,fv,it,sa,mv,Pn,gv,qi,_v,Tv,Ld,Fv,vv,kv,ko,bv,Ad,wv,yv,ra,su,Cn,bo,Dd,aa,$v,Id,Ev,ru,De,ia,Mv,Od,zv,qv,la,Pv,da,Cv,xv,jv,ca,Lv,Pi,Av,Dv,Iv,ua,Ov,pa,Sv,Nv,Bv,wo,Wv,lt,ha,Qv,xn,Rv,Ci,Hv,Vv,Sd,Yv,Uv,Gv,yo,Zv,Nd,Kv,Xv,fa,au,jn,$o,Bd,ma,Jv,Wd,ek,iu,Ie,ga,tk,Qd,nk,ok,_a,sk,Ta,rk,ak,ik,Fa,lk,xi,dk,ck,uk,va,pk,ka,hk,fk,mk,Eo,gk,dt,ba,_k,Ln,Tk,ji,Fk,vk,Rd,kk,bk,wk,Mo,yk,Hd,$k,Ek,wa,lu,An,zo,Vd,ya,Mk,Yd,zk,du,Oe,$a,qk,Ud,Pk,Ck,Ea,xk,Ma,jk,Lk,Ak,za,Dk,Li,Ik,Ok,Sk,qa,Nk,Pa,Bk,Wk,Qk,qo,Rk,ct,Ca,Hk,Dn,Vk,Ai,Yk,Uk,Gd,Gk,Zk,Kk,Po,Xk,Zd,Jk,eb,xa,cu,In,Co,Kd,ja,tb,Xd,nb,uu,Se,La,ob,On,sb,Jd,rb,ab,ec,ib,lb,db,Aa,cb,Da,ub,pb,hb,Ia,fb,Di,mb,gb,_b,Oa,Tb,Sa,Fb,vb,kb,xo,bb,ut,Na,wb,Sn,yb,Ii,$b,Eb,tc,Mb,zb,qb,jo,Pb,nc,Cb,xb,Ba,pu;return T=new qe({}),te=new qe({}),So=new qe({}),No=new ee({props:{name:"class transformers.FunnelConfig",anchor:"transformers.FunnelConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"block_sizes",val:" = [4, 4, 4]"},{name:"block_repeats",val:" = None"},{name:"num_decoder_layers",val:" = 2"},{name:"d_model",val:" = 768"},{name:"n_head",val:" = 12"},{name:"d_head",val:" = 64"},{name:"d_inner",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 3"},{name:"initializer_range",val:" = 0.1"},{name:"initializer_std",val:" = None"},{name:"layer_norm_eps",val:" = 1e-09"},{name:"pooling_type",val:" = 'mean'"},{name:"attention_type",val:" = 'relative_shift'"},{name:"separate_cls",val:" = True"},{name:"truncate_seq",val:" = True"},{name:"pool_q_only",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/configuration_funnel.py#L37",parametersDescription:[{anchor:"transformers.FunnelConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Funnel transformer. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"vocab_size"},{anchor:"transformers.FunnelConfig.block_sizes",description:`<strong>block_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 4, 4]</code>) &#x2014;
The sizes of the blocks used in the model.`,name:"block_sizes"},{anchor:"transformers.FunnelConfig.block_repeats",description:`<strong>block_repeats</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If passed along, each layer of each block is repeated the number of times indicated.`,name:"block_repeats"},{anchor:"transformers.FunnelConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The number of layers in the decoder (when not using the base model).`,name:"num_decoder_layers"},{anchor:"transformers.FunnelConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the model&#x2019;s hidden states.`,name:"d_model"},{anchor:"transformers.FunnelConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.FunnelConfig.d_head",description:`<strong>d_head</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of the model&#x2019;s heads.`,name:"d_head"},{anchor:"transformers.FunnelConfig.d_inner",description:`<strong>d_inner</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Inner dimension in the feed-forward blocks.`,name:"d_inner"},{anchor:"transformers.FunnelConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>callable</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FunnelConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.FunnelConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FunnelConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability used between the two layers of the feed-forward blocks.`,name:"activation_dropout"},{anchor:"transformers.FunnelConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FunnelConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.FunnelConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The standard deviation of the <em>uniform initializer</em> for initializing all weight matrices in attention
layers.`,name:"initializer_range"},{anchor:"transformers.FunnelConfig.initializer_std",description:`<strong>initializer_std</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The standard deviation of the <em>normal initializer</em> for initializing the embedding matrix and the weight of
linear layers. Will default to 1 for the embedding matrix and the value given by Xavier initialization for
linear layers.`,name:"initializer_std"},{anchor:"transformers.FunnelConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-9) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FunnelConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Possible values are <code>&quot;mean&quot;</code> or <code>&quot;max&quot;</code>. The way pooling is performed at the beginning of each block.`,name:"pooling_type"},{anchor:"transformers.FunnelConfig.attention_type",description:`<strong>attention_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relative_shift&quot;</code>) &#x2014;
Possible values are <code>&quot;relative_shift&quot;</code> or <code>&quot;factorized&quot;</code>. The former is faster on CPU/GPU while the latter
is faster on TPU.`,name:"attention_type"},{anchor:"transformers.FunnelConfig.separate_cls",description:`<strong>separate_cls</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to separate the cls token when applying pooling.`,name:"separate_cls"},{anchor:"transformers.FunnelConfig.truncate_seq",description:`<strong>truncate_seq</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
When using <code>separate_cls</code>, whether or not to truncate the last token when pooling, to avoid getting a
sequence length that is not a multiple of 2.`,name:"truncate_seq"},{anchor:"transformers.FunnelConfig.pool_q_only",description:`<strong>pool_q_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply the pooling only to the query or to query, key and values for the attention layers.`,name:"pool_q_only"}]}}),Wo=new qe({}),Qo=new ee({props:{name:"class transformers.FunnelTokenizer",anchor:"transformers.FunnelTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/tokenization_funnel.py#L58"}}),Ho=new ee({props:{name:"build\\_inputs\\_with\\_special\\_tokens",anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/bert/tokenization_bert.py#L248",parametersDescription:[{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Yo=new ee({props:{name:"get\\_special\\_tokens\\_mask",anchor:"transformers.BertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/bert/tokenization_bert.py#L273",parametersDescription:[{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Go=new ee({props:{name:"create\\_token\\_type\\_ids\\_from\\_sequences",anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/tokenization_funnel.py#L108",parametersDescription:[{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Zo=new Ne({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),Ko=new qe({}),Xo=new ee({props:{name:"class transformers.FunnelTokenizerFast",anchor:"transformers.FunnelTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '<unk>'"},{name:"sep_token",val:" = '<sep>'"},{name:"pad_token",val:" = '<pad>'"},{name:"cls_token",val:" = '<cls>'"},{name:"mask_token",val:" = '<mask>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"clean_text",val:" = True"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"wordpieces_prefix",val:" = '##'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/tokenization_funnel_fast.py#L71"}}),ts=new ee({props:{name:"create\\_token\\_type\\_ids\\_from\\_sequences",anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/tokenization_funnel_fast.py#L124",parametersDescription:[{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FunnelTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ns=new Ne({props:{code:`2 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`2<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),os=new qe({}),ss=new ee({props:{name:"class transformers.models.funnel.modeling\\_funnel.FunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L801",parametersDescription:[{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss of the ELECTRA-style objective.`,name:"loss"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),as=new ee({props:{name:"class transformers.models.funnel.modeling\\_tf\\_funnel.TFFunnelForPreTrainingOutput",anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput",parameters:[{name:"logits",val:": Tensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1005",parametersDescription:[{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.logits",description:`<strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Prediction scores of the head (scores for each token before SoftMax).`,name:"logits"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),ls=new qe({}),ds=new ee({props:{name:"class transformers.FunnelBaseModel",anchor:"transformers.FunnelBaseModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L894",parametersDescription:[{anchor:"transformers.FunnelBaseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ms=new ee({props:{name:"forward",anchor:"transformers.FunnelBaseModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L910",parametersDescription:[{anchor:"transformers.FunnelBaseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelBaseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelBaseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelBaseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelBaseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelBaseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelBaseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Un=new ze({props:{$$slots:{default:[w$]},$$scope:{ctx:W}}}),gs=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelBaseModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),_s=new qe({}),Ts=new ee({props:{name:"class transformers.FunnelModel",anchor:"transformers.FunnelModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L971",parametersDescription:[{anchor:"transformers.FunnelModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ys=new ee({props:{name:"forward",anchor:"transformers.FunnelModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L988",parametersDescription:[{anchor:"transformers.FunnelModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Zn=new ze({props:{$$slots:{default:[y$]},$$scope:{ctx:W}}}),$s=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelModel
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Es=new qe({}),zs=new ee({props:{name:"forward",anchor:"transformers.FunnelForPreTraining.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1088",parametersDescription:[{anchor:"transformers.FunnelForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForPreTraining.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForPreTraining.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the ELECTRA-style loss. Input should be a sequence of tokens (see <code>input_ids</code>
docstring) Indices should be in <code>[0, 1]</code>:</p>
<ul>
<li>0 indicates the token is an original token,</li>
<li>1 indicates the token was replaced.</li>
</ul>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss of the ELECTRA-style objective.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Xn=new ze({props:{$$slots:{default:[$$]},$$scope:{ctx:W}}}),qs=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
logits = model(**inputs).logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`}}),Ps=new qe({}),Cs=new ee({props:{name:"class transformers.FunnelForMaskedLM",anchor:"transformers.FunnelForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1162",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Os=new ee({props:{name:"forward",anchor:"transformers.FunnelForMaskedLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1178",parametersDescription:[{anchor:"transformers.FunnelForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),eo=new ze({props:{$$slots:{default:[E$]},$$scope:{ctx:W}}}),Ss=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForMaskedLM
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is <mask>.", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is &lt;mask&gt;.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ns=new qe({}),Bs=new ee({props:{name:"class transformers.FunnelForSequenceClassification",anchor:"transformers.FunnelForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1242",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ys=new ee({props:{name:"forward",anchor:"transformers.FunnelForSequenceClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1253",parametersDescription:[{anchor:"transformers.FunnelForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),no=new ze({props:{$$slots:{default:[M$]},$$scope:{ctx:W}}}),Us=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForSequenceClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Gs=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForSequenceClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float)  # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>)  <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Zs=new qe({}),Ks=new ee({props:{name:"class transformers.FunnelForMultipleChoice",anchor:"transformers.FunnelForMultipleChoice",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1335",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),or=new ee({props:{name:"forward",anchor:"transformers.FunnelForMultipleChoice.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1344",parametersDescription:[{anchor:"transformers.FunnelForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForMultipleChoice.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),so=new ze({props:{$$slots:{default:[z$]},$$scope:{ctx:W}}}),sr=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForMultipleChoice
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = FunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),rr=new qe({}),ar=new ee({props:{name:"class transformers.FunnelForTokenClassification",anchor:"transformers.FunnelForTokenClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1419",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),pr=new ee({props:{name:"forward",anchor:"transformers.FunnelForTokenClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1431",parametersDescription:[{anchor:"transformers.FunnelForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForTokenClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ao=new ze({props:{$$slots:{default:[q$]},$$scope:{ctx:W}}}),hr=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForTokenClassification
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1] * inputs["input_ids"].size(1)).unsqueeze(0)  # Batch size 1

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>] * inputs[<span class="hljs-string">&quot;input_ids&quot;</span>].size(<span class="hljs-number">1</span>)).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),fr=new qe({}),mr=new ee({props:{name:"class transformers.FunnelForQuestionAnswering",anchor:"transformers.FunnelForQuestionAnswering",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1493",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),kr=new ee({props:{name:"forward",anchor:"transformers.FunnelForQuestionAnswering.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_funnel.py#L1504",parametersDescription:[{anchor:"transformers.FunnelForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FunnelForQuestionAnswering.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.FunnelForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FunnelForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FunnelForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.FunnelForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),lo=new ze({props:{$$slots:{default:[P$]},$$scope:{ctx:W}}}),br=new Ne({props:{code:`from transformers import FunnelTokenizer, FunnelForQuestionAnswering
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = FunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="pt")
start_positions = torch.tensor([1])
end_positions = torch.tensor([3])

outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
loss = outputs.loss
start_scores = outputs.start_logits
end_scores = outputs.end_logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, FunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_positions = torch.tensor([<span class="hljs-number">1</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>end_positions = torch.tensor([<span class="hljs-number">3</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=start_positions, end_positions=end_positions)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),wr=new qe({}),yr=new ee({props:{name:"class transformers.TFFunnelBaseModel",anchor:"transformers.TFFunnelBaseModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1122",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),uo=new ze({props:{$$slots:{default:[C$]},$$scope:{ctx:W}}}),Pr=new ee({props:{name:"call",anchor:"transformers.TFFunnelBaseModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1127",parametersDescription:[{anchor:"transformers.TFFunnelBaseModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelBaseModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelBaseModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelBaseModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelBaseModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelBaseModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelBaseModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelBaseModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),po=new ze({props:{$$slots:{default:[x$]},$$scope:{ctx:W}}}),Cr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelBaseModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelBaseModel.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelBaseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelBaseModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),xr=new qe({}),jr=new ee({props:{name:"class transformers.TFFunnelModel",anchor:"transformers.TFFunnelModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1183",parametersDescription:[{anchor:"transformers.TFFunnelModel.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),fo=new ze({props:{$$slots:{default:[j$]},$$scope:{ctx:W}}}),Sr=new ee({props:{name:"call",anchor:"transformers.TFFunnelModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1188",parametersDescription:[{anchor:"transformers.TFFunnelModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelModel.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),mo=new ze({props:{$$slots:{default:[L$]},$$scope:{ctx:W}}}),Nr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelModel
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelModel.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelModel.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Br=new qe({}),Wr=new ee({props:{name:"class transformers.TFFunnelForPreTraining",anchor:"transformers.TFFunnelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1246",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_o=new ze({props:{$$slots:{default:[A$]},$$scope:{ctx:W}}}),Ur=new ee({props:{name:"call",anchor:"transformers.TFFunnelForPreTraining.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1253",parametersDescription:[{anchor:"transformers.TFFunnelForPreTraining.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForPreTraining.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForPreTraining.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForPreTraining.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForPreTraining.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForPreTraining.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForPreTraining.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Prediction scores of the head (scores for each token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput"
>transformers.models.funnel.modeling_tf_funnel.TFFunnelForPreTrainingOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),To=new ze({props:{$$slots:{default:[D$]},$$scope:{ctx:W}}}),Gr=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForPreTraining
import torch

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForPreTraining.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
logits = model(inputs).logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForPreTraining.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(inputs).logits`}}),Zr=new qe({}),Kr=new ee({props:{name:"class transformers.TFFunnelForMaskedLM",anchor:"transformers.TFFunnelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1325",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),vo=new ze({props:{$$slots:{default:[I$]},$$scope:{ctx:W}}}),sa=new ee({props:{name:"call",anchor:"transformers.TFFunnelForMaskedLM.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1339",parametersDescription:[{anchor:"transformers.TFFunnelForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMaskedLM.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ko=new ze({props:{$$slots:{default:[O$]},$$scope:{ctx:W}}}),ra=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMaskedLM
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForMaskedLM.from_pretrained("funnel-transformer/small")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
inputs["labels"] = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),aa=new qe({}),ia=new ee({props:{name:"class transformers.TFFunnelForSequenceClassification",anchor:"transformers.TFFunnelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1420",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),wo=new ze({props:{$$slots:{default:[S$]},$$scope:{ctx:W}}}),ha=new ee({props:{name:"call",anchor:"transformers.TFFunnelForSequenceClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1428",parametersDescription:[{anchor:"transformers.TFFunnelForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForSequenceClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),yo=new ze({props:{$$slots:{default:[N$]},$$scope:{ctx:W}}}),fa=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForSequenceClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForSequenceClassification.from_pretrained("funnel-transformer/small-base")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
inputs["labels"] = tf.reshape(tf.constant(1), (-1, 1))  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(tf.constant(<span class="hljs-number">1</span>), (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ma=new qe({}),ga=new ee({props:{name:"class transformers.TFFunnelForMultipleChoice",anchor:"transformers.TFFunnelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1510",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Eo=new ze({props:{$$slots:{default:[B$]},$$scope:{ctx:W}}}),ba=new ee({props:{name:"call",anchor:"transformers.TFFunnelForMultipleChoice.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1527",parametersDescription:[{anchor:"transformers.TFFunnelForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForMultipleChoice.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Mo=new ze({props:{$$slots:{default:[W$]},$$scope:{ctx:W}}}),wa=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForMultipleChoice
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small-base")
model = TFFunnelForMultipleChoice.from_pretrained("funnel-transformer/small-base")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ya=new qe({}),$a=new ee({props:{name:"class transformers.TFFunnelForTokenClassification",anchor:"transformers.TFFunnelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1645",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),qo=new ze({props:{$$slots:{default:[Q$]},$$scope:{ctx:W}}}),Ca=new ee({props:{name:"call",anchor:"transformers.TFFunnelForTokenClassification.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1656",parametersDescription:[{anchor:"transformers.TFFunnelForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForTokenClassification.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Po=new ze({props:{$$slots:{default:[R$]},$$scope:{ctx:W}}}),xa=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForTokenClassification
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForTokenClassification.from_pretrained("funnel-transformer/small")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
input_ids = inputs["input_ids"]
inputs["labels"] = tf.reshape(
    tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))
)  # Batch size 1

outputs = model(inputs)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = tf.reshape(
<span class="hljs-meta">... </span>    tf.constant([<span class="hljs-number">1</span>] * tf.size(input_ids).numpy()), (-<span class="hljs-number">1</span>, tf.size(input_ids))
<span class="hljs-meta">&gt;&gt;&gt; </span>)  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ja=new qe({}),La=new ee({props:{name:"class transformers.TFFunnelForQuestionAnswering",anchor:"transformers.TFFunnelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1737",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.config",description:`<strong>config</strong> (<code>XxxConfig</code>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),xo=new ze({props:{$$slots:{default:[H$]},$$scope:{ctx:W}}}),Na=new ee({props:{name:"call",anchor:"transformers.TFFunnelForQuestionAnswering.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/doc-build-test/src/transformers/models/funnel/modeling_tf_funnel.py#L1747",parametersDescription:[{anchor:"transformers.TFFunnelForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a>. See <a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/doc-build-test/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.token_type_ids",description:`<strong>token_type_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used
in eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFFunnelForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelConfig"
>FunnelConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/doc-build-test/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),jo=new ze({props:{$$slots:{default:[V$]},$$scope:{ctx:W}}}),Ba=new Ne({props:{code:`from transformers import FunnelTokenizer, TFFunnelForQuestionAnswering
import tensorflow as tf

tokenizer = FunnelTokenizer.from_pretrained("funnel-transformer/small")
model = TFFunnelForQuestionAnswering.from_pretrained("funnel-transformer/small")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
input_dict = tokenizer(question, text, return_tensors="tf")
outputs = model(input_dict)
start_logits = outputs.start_logits
end_logits = outputs.end_logits

all_tokens = tokenizer.convert_ids_to_tokens(input_dict["input_ids"].numpy()[0])
answer = " ".join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0] + 1]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FunnelTokenizer, TFFunnelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FunnelTokenizer.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFFunnelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;funnel-transformer/small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_dict = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_dict)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits

<span class="hljs-meta">&gt;&gt;&gt; </span>all_tokens = tokenizer.convert_ids_to_tokens(input_dict[<span class="hljs-string">&quot;input_ids&quot;</span>].numpy()[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer = <span class="hljs-string">&quot; &quot;</span>.join(all_tokens[tf.math.argmax(start_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] : tf.math.argmax(end_logits, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>] + <span class="hljs-number">1</span>])`}}),{c(){p=s("meta"),M=l(),m=s("h1"),g=s("a"),F=s("span"),k(T.$$.fragment),_=l(),z=s("span"),ce=n("Funnel Transformer"),G=l(),q=s("h2"),X=s("a"),I=s("span"),k(te.$$.fragment),ue=l(),O=s("span"),pe=n("Overview"),ie=l(),U=s("p"),L=n("The Funnel Transformer model was proposed in the paper "),ne=s("a"),Z=n(`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),P=n(`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),x=l(),oe=s("p"),Q=n("The abstract from the paper is the following:"),le=l(),se=s("p"),S=s("em"),he=n(`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),de=l(),C=s("p"),fe=n("Tips:"),B=l(),J=s("ul"),ae=s("li"),R=n(`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),me=l(),N=s("li"),A=n(`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=s("a"),H=n("FunnelModel"),ge=n(", "),u=s("a"),v=n("FunnelForPreTraining"),K=n(`,
`),Te=s("a"),be=n("FunnelForMaskedLM"),D=n(", "),Fe=s("a"),we=n("FunnelForTokenClassification"),ye=n(` and
class:`),j=s("em"),V=n("~transformers.FunnelForQuestionAnswering"),$e=n(`. The second ones should be used for
`),ve=s("a"),Y=n("FunnelBaseModel"),Ee=n(", "),ke=s("a"),_e=n("FunnelForSequenceClassification"),Me=n(` and
`),Va=s("a"),Dp=n("FunnelForMultipleChoice"),Ip=n("."),yc=l(),xt=s("p"),Op=n("This model was contributed by "),Io=s("a"),Sp=n("sgugger"),Np=n(". The original code can be found "),Oo=s("a"),Bp=n("here"),Wp=n("."),$c=l(),Zt=s("h2"),Nn=s("a"),ll=s("span"),k(So.$$.fragment),Qp=l(),dl=s("span"),Rp=n("FunnelConfig"),Ec=l(),Pt=s("div"),k(No.$$.fragment),Hp=l(),Ct=s("p"),Vp=n("This is the configuration class to store the configuration of a "),Ya=s("a"),Yp=n("FunnelModel"),Up=n(" or a "),Ua=s("a"),Gp=n("TFBertModel"),Zp=n(`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Bo=s("a"),Kp=n("funnel-transformer/small"),Xp=n(" architecture."),Jp=l(),Kt=s("p"),eh=n("Configuration objects inherit from "),Ga=s("a"),th=n("PretrainedConfig"),nh=n(` and can be used to control the model outputs. Read the
documentation from `),Za=s("a"),oh=n("PretrainedConfig"),sh=n(" for more information."),Mc=l(),Xt=s("h2"),Bn=s("a"),cl=s("span"),k(Wo.$$.fragment),rh=l(),ul=s("span"),ah=n("FunnelTokenizer"),zc=l(),Pe=s("div"),k(Qo.$$.fragment),ih=l(),pl=s("p"),lh=n("Construct a Funnel Transformer tokenizer."),dh=l(),Wn=s("p"),Ka=s("a"),ch=n("FunnelTokenizer"),uh=n(" is identical to "),Xa=s("a"),ph=n("BertTokenizer"),hh=n(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),fh=l(),Ro=s("p"),mh=n("Refer to superclass "),Ja=s("a"),gh=n("BertTokenizer"),_h=n(" for usage examples and documentation concerning parameters."),Th=l(),jt=s("div"),k(Ho.$$.fragment),Fh=l(),hl=s("p"),vh=n(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),kh=l(),Vo=s("ul"),ei=s("li"),bh=n("single sequence: "),fl=s("code"),wh=n("[CLS] X [SEP]"),yh=l(),ti=s("li"),$h=n("pair of sequences: "),ml=s("code"),Eh=n("[CLS] A [SEP] B [SEP]"),Mh=l(),Qn=s("div"),k(Yo.$$.fragment),zh=l(),Uo=s("p"),qh=n(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),gl=s("code"),Ph=n("prepare_for_model"),Ch=n(" method."),xh=l(),kt=s("div"),k(Go.$$.fragment),jh=l(),_l=s("p"),Lh=n(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),Ah=l(),k(Zo.$$.fragment),Dh=l(),Jt=s("p"),Ih=n("If "),Tl=s("code"),Oh=n("token_ids_1"),Sh=n(" is "),Fl=s("code"),Nh=n("None"),Bh=n(", this method only returns the first portion of the mask (0s)."),Wh=l(),vl=s("div"),qc=l(),en=s("h2"),Rn=s("a"),kl=s("span"),k(Ko.$$.fragment),Qh=l(),bl=s("span"),Rh=n("FunnelTokenizerFast"),Pc=l(),Ze=s("div"),k(Xo.$$.fragment),Hh=l(),Jo=s("p"),Vh=n("Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),wl=s("em"),Yh=n("tokenizers"),Uh=n(" library)."),Gh=l(),Hn=s("p"),ni=s("a"),Zh=n("FunnelTokenizerFast"),Kh=n(" is identical to "),oi=s("a"),Xh=n("BertTokenizerFast"),Jh=n(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ef=l(),es=s("p"),tf=n("Refer to superclass "),si=s("a"),nf=n("BertTokenizerFast"),of=n(" for usage examples and documentation concerning parameters."),sf=l(),bt=s("div"),k(ts.$$.fragment),rf=l(),yl=s("p"),af=n(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),lf=l(),k(ns.$$.fragment),df=l(),tn=s("p"),cf=n("If "),$l=s("code"),uf=n("token_ids_1"),pf=n(" is "),El=s("code"),hf=n("None"),ff=n(", this method only returns the first portion of the mask (0s)."),Cc=l(),nn=s("h2"),Vn=s("a"),Ml=s("span"),k(os.$$.fragment),mf=l(),zl=s("span"),gf=n("Funnel specific outputs"),xc=l(),on=s("div"),k(ss.$$.fragment),_f=l(),rs=s("p"),Tf=n("Output type of "),ri=s("a"),Ff=n("FunnelForPreTraining"),vf=n("."),jc=l(),sn=s("div"),k(as.$$.fragment),kf=l(),is=s("p"),bf=n("Output type of "),ai=s("a"),wf=n("FunnelForPreTraining"),yf=n("."),Lc=l(),rn=s("h2"),Yn=s("a"),ql=s("span"),k(ls.$$.fragment),$f=l(),Pl=s("span"),Ef=n("FunnelBaseModel"),Ac=l(),We=s("div"),k(ds.$$.fragment),Mf=l(),Cl=s("p"),zf=n(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),qf=l(),cs=s("p"),Pf=n("The Funnel Transformer model was proposed in "),us=s("a"),Cf=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xf=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jf=l(),ps=s("p"),Lf=n("This model inherits from "),ii=s("a"),Af=n("PreTrainedModel"),Df=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),If=l(),hs=s("p"),Of=n("This model is also a PyTorch "),fs=s("a"),Sf=n("torch.nn.Module"),Nf=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bf=l(),Ke=s("div"),k(ms.$$.fragment),Wf=l(),an=s("p"),Qf=n("The "),li=s("a"),Rf=n("FunnelBaseModel"),Hf=n(" forward method, overrides the "),xl=s("code"),Vf=n("__call__"),Yf=n(" special method."),Uf=l(),k(Un.$$.fragment),Gf=l(),jl=s("p"),Zf=n("Example:"),Kf=l(),k(gs.$$.fragment),Dc=l(),ln=s("h2"),Gn=s("a"),Ll=s("span"),k(_s.$$.fragment),Xf=l(),Al=s("span"),Jf=n("FunnelModel"),Ic=l(),Qe=s("div"),k(Ts.$$.fragment),em=l(),Dl=s("p"),tm=n("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),nm=l(),Fs=s("p"),om=n("The Funnel Transformer model was proposed in "),vs=s("a"),sm=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),rm=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),am=l(),ks=s("p"),im=n("This model inherits from "),di=s("a"),lm=n("PreTrainedModel"),dm=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),cm=l(),bs=s("p"),um=n("This model is also a PyTorch "),ws=s("a"),pm=n("torch.nn.Module"),hm=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),fm=l(),Xe=s("div"),k(ys.$$.fragment),mm=l(),dn=s("p"),gm=n("The "),ci=s("a"),_m=n("FunnelModel"),Tm=n(" forward method, overrides the "),Il=s("code"),Fm=n("__call__"),vm=n(" special method."),km=l(),k(Zn.$$.fragment),bm=l(),Ol=s("p"),wm=n("Example:"),ym=l(),k($s.$$.fragment),Oc=l(),cn=s("h2"),Kn=s("a"),Sl=s("span"),k(Es.$$.fragment),$m=l(),Nl=s("span"),Em=n("FunnelModelForPreTraining"),Sc=l(),Ms=s("div"),Je=s("div"),k(zs.$$.fragment),Mm=l(),un=s("p"),zm=n("The "),ui=s("a"),qm=n("FunnelForPreTraining"),Pm=n(" forward method, overrides the "),Bl=s("code"),Cm=n("__call__"),xm=n(" special method."),jm=l(),k(Xn.$$.fragment),Lm=l(),Wl=s("p"),Am=n("Examples:"),Dm=l(),k(qs.$$.fragment),Nc=l(),pn=s("h2"),Jn=s("a"),Ql=s("span"),k(Ps.$$.fragment),Im=l(),Rl=s("span"),Om=n("FunnelForMaskedLM"),Bc=l(),Re=s("div"),k(Cs.$$.fragment),Sm=l(),xs=s("p"),Nm=n("Funnel Transformer Model with a "),Hl=s("code"),Bm=n("language modeling"),Wm=n(" head on top."),Qm=l(),js=s("p"),Rm=n("The Funnel Transformer model was proposed in "),Ls=s("a"),Hm=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Vm=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ym=l(),As=s("p"),Um=n("This model inherits from "),pi=s("a"),Gm=n("PreTrainedModel"),Zm=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Km=l(),Ds=s("p"),Xm=n("This model is also a PyTorch "),Is=s("a"),Jm=n("torch.nn.Module"),eg=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),tg=l(),et=s("div"),k(Os.$$.fragment),ng=l(),hn=s("p"),og=n("The "),hi=s("a"),sg=n("FunnelForMaskedLM"),rg=n(" forward method, overrides the "),Vl=s("code"),ag=n("__call__"),ig=n(" special method."),lg=l(),k(eo.$$.fragment),dg=l(),Yl=s("p"),cg=n("Example:"),ug=l(),k(Ss.$$.fragment),Wc=l(),fn=s("h2"),to=s("a"),Ul=s("span"),k(Ns.$$.fragment),pg=l(),Gl=s("span"),hg=n("FunnelForSequenceClassification"),Qc=l(),He=s("div"),k(Bs.$$.fragment),fg=l(),Zl=s("p"),mg=n(`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),gg=l(),Ws=s("p"),_g=n("The Funnel Transformer model was proposed in "),Qs=s("a"),Tg=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Fg=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),vg=l(),Rs=s("p"),kg=n("This model inherits from "),fi=s("a"),bg=n("PreTrainedModel"),wg=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),yg=l(),Hs=s("p"),$g=n("This model is also a PyTorch "),Vs=s("a"),Eg=n("torch.nn.Module"),Mg=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),zg=l(),Be=s("div"),k(Ys.$$.fragment),qg=l(),mn=s("p"),Pg=n("The "),mi=s("a"),Cg=n("FunnelForSequenceClassification"),xg=n(" forward method, overrides the "),Kl=s("code"),jg=n("__call__"),Lg=n(" special method."),Ag=l(),k(no.$$.fragment),Dg=l(),Xl=s("p"),Ig=n("Example of single-label classification:"),Og=l(),k(Us.$$.fragment),Sg=l(),Jl=s("p"),Ng=n("Example of multi-label classification:"),Bg=l(),k(Gs.$$.fragment),Rc=l(),gn=s("h2"),oo=s("a"),ed=s("span"),k(Zs.$$.fragment),Wg=l(),td=s("span"),Qg=n("FunnelForMultipleChoice"),Hc=l(),Ve=s("div"),k(Ks.$$.fragment),Rg=l(),nd=s("p"),Hg=n(`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),Vg=l(),Xs=s("p"),Yg=n("The Funnel Transformer model was proposed in "),Js=s("a"),Ug=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Gg=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Zg=l(),er=s("p"),Kg=n("This model inherits from "),gi=s("a"),Xg=n("PreTrainedModel"),Jg=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),e_=l(),tr=s("p"),t_=n("This model is also a PyTorch "),nr=s("a"),n_=n("torch.nn.Module"),o_=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),s_=l(),tt=s("div"),k(or.$$.fragment),r_=l(),_n=s("p"),a_=n("The "),_i=s("a"),i_=n("FunnelForMultipleChoice"),l_=n(" forward method, overrides the "),od=s("code"),d_=n("__call__"),c_=n(" special method."),u_=l(),k(so.$$.fragment),p_=l(),sd=s("p"),h_=n("Example:"),f_=l(),k(sr.$$.fragment),Vc=l(),Tn=s("h2"),ro=s("a"),rd=s("span"),k(rr.$$.fragment),m_=l(),ad=s("span"),g_=n("FunnelForTokenClassification"),Yc=l(),Ye=s("div"),k(ar.$$.fragment),__=l(),id=s("p"),T_=n(`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),F_=l(),ir=s("p"),v_=n("The Funnel Transformer model was proposed in "),lr=s("a"),k_=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),b_=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),w_=l(),dr=s("p"),y_=n("This model inherits from "),Ti=s("a"),$_=n("PreTrainedModel"),E_=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),M_=l(),cr=s("p"),z_=n("This model is also a PyTorch "),ur=s("a"),q_=n("torch.nn.Module"),P_=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),C_=l(),nt=s("div"),k(pr.$$.fragment),x_=l(),Fn=s("p"),j_=n("The "),Fi=s("a"),L_=n("FunnelForTokenClassification"),A_=n(" forward method, overrides the "),ld=s("code"),D_=n("__call__"),I_=n(" special method."),O_=l(),k(ao.$$.fragment),S_=l(),dd=s("p"),N_=n("Example:"),B_=l(),k(hr.$$.fragment),Uc=l(),vn=s("h2"),io=s("a"),cd=s("span"),k(fr.$$.fragment),W_=l(),ud=s("span"),Q_=n("FunnelForQuestionAnswering"),Gc=l(),Ue=s("div"),k(mr.$$.fragment),R_=l(),kn=s("p"),H_=n(`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),pd=s("code"),V_=n("span start logits"),Y_=n(" and "),hd=s("code"),U_=n("span end logits"),G_=n(")."),Z_=l(),gr=s("p"),K_=n("The Funnel Transformer model was proposed in "),_r=s("a"),X_=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),J_=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),eT=l(),Tr=s("p"),tT=n("This model inherits from "),vi=s("a"),nT=n("PreTrainedModel"),oT=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),sT=l(),Fr=s("p"),rT=n("This model is also a PyTorch "),vr=s("a"),aT=n("torch.nn.Module"),iT=n(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),lT=l(),ot=s("div"),k(kr.$$.fragment),dT=l(),bn=s("p"),cT=n("The "),ki=s("a"),uT=n("FunnelForQuestionAnswering"),pT=n(" forward method, overrides the "),fd=s("code"),hT=n("__call__"),fT=n(" special method."),mT=l(),k(lo.$$.fragment),gT=l(),md=s("p"),_T=n("Example:"),TT=l(),k(br.$$.fragment),Zc=l(),wn=s("h2"),co=s("a"),gd=s("span"),k(wr.$$.fragment),FT=l(),_d=s("span"),vT=n("TFFunnelBaseModel"),Kc=l(),xe=s("div"),k(yr.$$.fragment),kT=l(),Td=s("p"),bT=n(`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),wT=l(),$r=s("p"),yT=n("The Funnel Transformer model was proposed in "),Er=s("a"),$T=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ET=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),MT=l(),Mr=s("p"),zT=n("This model inherits from "),bi=s("a"),qT=n("TFPreTrainedModel"),PT=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),CT=l(),zr=s("p"),xT=n("This model is also a "),qr=s("a"),jT=n("tf.keras.Model"),LT=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),AT=l(),k(uo.$$.fragment),DT=l(),st=s("div"),k(Pr.$$.fragment),IT=l(),yn=s("p"),OT=n("The "),wi=s("a"),ST=n("TFFunnelBaseModel"),NT=n(" forward method, overrides the "),Fd=s("code"),BT=n("__call__"),WT=n(" special method."),QT=l(),k(po.$$.fragment),RT=l(),vd=s("p"),HT=n("Example:"),VT=l(),k(Cr.$$.fragment),Xc=l(),$n=s("h2"),ho=s("a"),kd=s("span"),k(xr.$$.fragment),YT=l(),bd=s("span"),UT=n("TFFunnelModel"),Jc=l(),je=s("div"),k(jr.$$.fragment),GT=l(),wd=s("p"),ZT=n("The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),KT=l(),Lr=s("p"),XT=n("The Funnel Transformer model was proposed in "),Ar=s("a"),JT=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),eF=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),tF=l(),Dr=s("p"),nF=n("This model inherits from "),yi=s("a"),oF=n("TFPreTrainedModel"),sF=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),rF=l(),Ir=s("p"),aF=n("This model is also a "),Or=s("a"),iF=n("tf.keras.Model"),lF=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),dF=l(),k(fo.$$.fragment),cF=l(),rt=s("div"),k(Sr.$$.fragment),uF=l(),En=s("p"),pF=n("The "),$i=s("a"),hF=n("TFFunnelModel"),fF=n(" forward method, overrides the "),yd=s("code"),mF=n("__call__"),gF=n(" special method."),_F=l(),k(mo.$$.fragment),TF=l(),$d=s("p"),FF=n("Example:"),vF=l(),k(Nr.$$.fragment),eu=l(),Mn=s("h2"),go=s("a"),Ed=s("span"),k(Br.$$.fragment),kF=l(),Md=s("span"),bF=n("TFFunnelModelForPreTraining"),tu=l(),Le=s("div"),k(Wr.$$.fragment),wF=l(),zd=s("p"),yF=n("Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),$F=l(),Qr=s("p"),EF=n("The Funnel Transformer model was proposed in "),Rr=s("a"),MF=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),zF=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),qF=l(),Hr=s("p"),PF=n("This model inherits from "),Ei=s("a"),CF=n("TFPreTrainedModel"),xF=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jF=l(),Vr=s("p"),LF=n("This model is also a "),Yr=s("a"),AF=n("tf.keras.Model"),DF=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),IF=l(),k(_o.$$.fragment),OF=l(),at=s("div"),k(Ur.$$.fragment),SF=l(),zn=s("p"),NF=n("The "),Mi=s("a"),BF=n("TFFunnelForPreTraining"),WF=n(" forward method, overrides the "),qd=s("code"),QF=n("__call__"),RF=n(" special method."),HF=l(),k(To.$$.fragment),VF=l(),Pd=s("p"),YF=n("Examples:"),UF=l(),k(Gr.$$.fragment),nu=l(),qn=s("h2"),Fo=s("a"),Cd=s("span"),k(Zr.$$.fragment),GF=l(),xd=s("span"),ZF=n("TFFunnelForMaskedLM"),ou=l(),Ae=s("div"),k(Kr.$$.fragment),KF=l(),Xr=s("p"),XF=n("Funnel Model with a "),jd=s("code"),JF=n("language modeling"),ev=n(" head on top."),tv=l(),Jr=s("p"),nv=n("The Funnel Transformer model was proposed in "),ea=s("a"),ov=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),sv=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),rv=l(),ta=s("p"),av=n("This model inherits from "),zi=s("a"),iv=n("TFPreTrainedModel"),lv=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dv=l(),na=s("p"),cv=n("This model is also a "),oa=s("a"),uv=n("tf.keras.Model"),pv=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),hv=l(),k(vo.$$.fragment),fv=l(),it=s("div"),k(sa.$$.fragment),mv=l(),Pn=s("p"),gv=n("The "),qi=s("a"),_v=n("TFFunnelForMaskedLM"),Tv=n(" forward method, overrides the "),Ld=s("code"),Fv=n("__call__"),vv=n(" special method."),kv=l(),k(ko.$$.fragment),bv=l(),Ad=s("p"),wv=n("Example:"),yv=l(),k(ra.$$.fragment),su=l(),Cn=s("h2"),bo=s("a"),Dd=s("span"),k(aa.$$.fragment),$v=l(),Id=s("span"),Ev=n("TFFunnelForSequenceClassification"),ru=l(),De=s("div"),k(ia.$$.fragment),Mv=l(),Od=s("p"),zv=n(`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),qv=l(),la=s("p"),Pv=n("The Funnel Transformer model was proposed in "),da=s("a"),Cv=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),xv=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jv=l(),ca=s("p"),Lv=n("This model inherits from "),Pi=s("a"),Av=n("TFPreTrainedModel"),Dv=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Iv=l(),ua=s("p"),Ov=n("This model is also a "),pa=s("a"),Sv=n("tf.keras.Model"),Nv=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Bv=l(),k(wo.$$.fragment),Wv=l(),lt=s("div"),k(ha.$$.fragment),Qv=l(),xn=s("p"),Rv=n("The "),Ci=s("a"),Hv=n("TFFunnelForSequenceClassification"),Vv=n(" forward method, overrides the "),Sd=s("code"),Yv=n("__call__"),Uv=n(" special method."),Gv=l(),k(yo.$$.fragment),Zv=l(),Nd=s("p"),Kv=n("Example:"),Xv=l(),k(fa.$$.fragment),au=l(),jn=s("h2"),$o=s("a"),Bd=s("span"),k(ma.$$.fragment),Jv=l(),Wd=s("span"),ek=n("TFFunnelForMultipleChoice"),iu=l(),Ie=s("div"),k(ga.$$.fragment),tk=l(),Qd=s("p"),nk=n(`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),ok=l(),_a=s("p"),sk=n("The Funnel Transformer model was proposed in "),Ta=s("a"),rk=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ak=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ik=l(),Fa=s("p"),lk=n("This model inherits from "),xi=s("a"),dk=n("TFPreTrainedModel"),ck=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),uk=l(),va=s("p"),pk=n("This model is also a "),ka=s("a"),hk=n("tf.keras.Model"),fk=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),mk=l(),k(Eo.$$.fragment),gk=l(),dt=s("div"),k(ba.$$.fragment),_k=l(),Ln=s("p"),Tk=n("The "),ji=s("a"),Fk=n("TFFunnelForMultipleChoice"),vk=n(" forward method, overrides the "),Rd=s("code"),kk=n("__call__"),bk=n(" special method."),wk=l(),k(Mo.$$.fragment),yk=l(),Hd=s("p"),$k=n("Example:"),Ek=l(),k(wa.$$.fragment),lu=l(),An=s("h2"),zo=s("a"),Vd=s("span"),k(ya.$$.fragment),Mk=l(),Yd=s("span"),zk=n("TFFunnelForTokenClassification"),du=l(),Oe=s("div"),k($a.$$.fragment),qk=l(),Ud=s("p"),Pk=n(`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),Ck=l(),Ea=s("p"),xk=n("The Funnel Transformer model was proposed in "),Ma=s("a"),jk=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Lk=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ak=l(),za=s("p"),Dk=n("This model inherits from "),Li=s("a"),Ik=n("TFPreTrainedModel"),Ok=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Sk=l(),qa=s("p"),Nk=n("This model is also a "),Pa=s("a"),Bk=n("tf.keras.Model"),Wk=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Qk=l(),k(qo.$$.fragment),Rk=l(),ct=s("div"),k(Ca.$$.fragment),Hk=l(),Dn=s("p"),Vk=n("The "),Ai=s("a"),Yk=n("TFFunnelForTokenClassification"),Uk=n(" forward method, overrides the "),Gd=s("code"),Gk=n("__call__"),Zk=n(" special method."),Kk=l(),k(Po.$$.fragment),Xk=l(),Zd=s("p"),Jk=n("Example:"),eb=l(),k(xa.$$.fragment),cu=l(),In=s("h2"),Co=s("a"),Kd=s("span"),k(ja.$$.fragment),tb=l(),Xd=s("span"),nb=n("TFFunnelForQuestionAnswering"),uu=l(),Se=s("div"),k(La.$$.fragment),ob=l(),On=s("p"),sb=n(`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Jd=s("code"),rb=n("span start logits"),ab=n(" and "),ec=s("code"),ib=n("span end logits"),lb=n(")."),db=l(),Aa=s("p"),cb=n("The Funnel Transformer model was proposed in "),Da=s("a"),ub=n(`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),pb=n(" by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),hb=l(),Ia=s("p"),fb=n("This model inherits from "),Di=s("a"),mb=n("TFPreTrainedModel"),gb=n(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_b=l(),Oa=s("p"),Tb=n("This model is also a "),Sa=s("a"),Fb=n("tf.keras.Model"),vb=n(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),kb=l(),k(xo.$$.fragment),bb=l(),ut=s("div"),k(Na.$$.fragment),wb=l(),Sn=s("p"),yb=n("The "),Ii=s("a"),$b=n("TFFunnelForQuestionAnswering"),Eb=n(" forward method, overrides the "),tc=s("code"),Mb=n("__call__"),zb=n(" special method."),qb=l(),k(jo.$$.fragment),Pb=l(),nc=s("p"),Cb=n("Example:"),xb=l(),k(Ba.$$.fragment),this.h()},l(i){const f=b$('[data-svelte="svelte-1phssyn"]',document.head);p=r(f,"META",{name:!0,content:!0}),f.forEach(t),M=d(i),m=r(i,"H1",{class:!0});var Wa=a(m);g=r(Wa,"A",{id:!0,class:!0,href:!0});var oc=a(g);F=r(oc,"SPAN",{});var sc=a(F);b(T.$$.fragment,sc),sc.forEach(t),oc.forEach(t),_=d(Wa),z=r(Wa,"SPAN",{});var rc=a(z);ce=o(rc,"Funnel Transformer"),rc.forEach(t),Wa.forEach(t),G=d(i),q=r(i,"H2",{class:!0});var Qa=a(q);X=r(Qa,"A",{id:!0,class:!0,href:!0});var ac=a(X);I=r(ac,"SPAN",{});var ic=a(I);b(te.$$.fragment,ic),ic.forEach(t),ac.forEach(t),ue=d(Qa),O=r(Qa,"SPAN",{});var lc=a(O);pe=o(lc,"Overview"),lc.forEach(t),Qa.forEach(t),ie=d(i),U=r(i,"P",{});var Ra=a(U);L=o(Ra,"The Funnel Transformer model was proposed in the paper "),ne=r(Ra,"A",{href:!0,rel:!0});var dc=a(ne);Z=o(dc,`Funnel-Transformer: Filtering out Sequential Redundancy for
Efficient Language Processing`),dc.forEach(t),P=o(Ra,`. It is a bidirectional transformer model, like
BERT, but with a pooling operation after each block of layers, a bit like in traditional convolutional neural networks
(CNN) in computer vision.`),Ra.forEach(t),x=d(i),oe=r(i,"P",{});var cc=a(oe);Q=o(cc,"The abstract from the paper is the following:"),cc.forEach(t),le=d(i),se=r(i,"P",{});var uc=a(se);S=r(uc,"EM",{});var pc=a(S);he=o(pc,`With the success of language pretraining, it is highly desirable to develop more efficient architectures of good
scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the
much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only
require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which
gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More
importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further
improve the model capacity. In addition, to perform token-level predictions as required by common pretraining
objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence
via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on
a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading
comprehension.`),pc.forEach(t),uc.forEach(t),de=d(i),C=r(i,"P",{});var hc=a(C);fe=o(hc,"Tips:"),hc.forEach(t),B=d(i),J=r(i,"UL",{});var Ha=a(J);ae=r(Ha,"LI",{});var fc=a(ae);R=o(fc,`Since Funnel Transformer uses pooling, the sequence length of the hidden states changes after each block of layers.
The base model therefore has a final sequence length that is a quarter of the original one. This model can be used
directly for tasks that just require a sentence summary (like sequence classification or multiple choice). For other
tasks, the full model is used; this full model has a decoder that upsamples the final hidden states to the same
sequence length as the input.`),fc.forEach(t),me=d(Ha),N=r(Ha,"LI",{});var Ce=a(N);A=o(Ce,`The Funnel Transformer checkpoints are all available with a full version and a base version. The first ones should be
used for `),re=r(Ce,"A",{href:!0});var mc=a(re);H=o(mc,"FunnelModel"),mc.forEach(t),ge=o(Ce,", "),u=r(Ce,"A",{href:!0});var gc=a(u);v=o(gc,"FunnelForPreTraining"),gc.forEach(t),K=o(Ce,`,
`),Te=r(Ce,"A",{href:!0});var _c=a(Te);be=o(_c,"FunnelForMaskedLM"),_c.forEach(t),D=o(Ce,", "),Fe=r(Ce,"A",{href:!0});var Tc=a(Fe);we=o(Tc,"FunnelForTokenClassification"),Tc.forEach(t),ye=o(Ce,` and
class:`),j=r(Ce,"EM",{});var Fc=a(j);V=o(Fc,"~transformers.FunnelForQuestionAnswering"),Fc.forEach(t),$e=o(Ce,`. The second ones should be used for
`),ve=r(Ce,"A",{href:!0});var vc=a(ve);Y=o(vc,"FunnelBaseModel"),vc.forEach(t),Ee=o(Ce,", "),ke=r(Ce,"A",{href:!0});var kc=a(ke);_e=o(kc,"FunnelForSequenceClassification"),kc.forEach(t),Me=o(Ce,` and
`),Va=r(Ce,"A",{href:!0});var Ab=a(Va);Dp=o(Ab,"FunnelForMultipleChoice"),Ab.forEach(t),Ip=o(Ce,"."),Ce.forEach(t),Ha.forEach(t),yc=d(i),xt=r(i,"P",{});var Oi=a(xt);Op=o(Oi,"This model was contributed by "),Io=r(Oi,"A",{href:!0,rel:!0});var Db=a(Io);Sp=o(Db,"sgugger"),Db.forEach(t),Np=o(Oi,". The original code can be found "),Oo=r(Oi,"A",{href:!0,rel:!0});var Ib=a(Oo);Bp=o(Ib,"here"),Ib.forEach(t),Wp=o(Oi,"."),Oi.forEach(t),$c=d(i),Zt=r(i,"H2",{class:!0});var hu=a(Zt);Nn=r(hu,"A",{id:!0,class:!0,href:!0});var Ob=a(Nn);ll=r(Ob,"SPAN",{});var Sb=a(ll);b(So.$$.fragment,Sb),Sb.forEach(t),Ob.forEach(t),Qp=d(hu),dl=r(hu,"SPAN",{});var Nb=a(dl);Rp=o(Nb,"FunnelConfig"),Nb.forEach(t),hu.forEach(t),Ec=d(i),Pt=r(i,"DIV",{class:!0});var Si=a(Pt);b(No.$$.fragment,Si),Hp=d(Si),Ct=r(Si,"P",{});var Lo=a(Ct);Vp=o(Lo,"This is the configuration class to store the configuration of a "),Ya=r(Lo,"A",{href:!0});var Bb=a(Ya);Yp=o(Bb,"FunnelModel"),Bb.forEach(t),Up=o(Lo," or a "),Ua=r(Lo,"A",{href:!0});var Wb=a(Ua);Gp=o(Wb,"TFBertModel"),Wb.forEach(t),Zp=o(Lo,`. It is used to
instantiate a Funnel Transformer model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Funnel
Transformer `),Bo=r(Lo,"A",{href:!0,rel:!0});var Qb=a(Bo);Kp=o(Qb,"funnel-transformer/small"),Qb.forEach(t),Xp=o(Lo," architecture."),Lo.forEach(t),Jp=d(Si),Kt=r(Si,"P",{});var Ni=a(Kt);eh=o(Ni,"Configuration objects inherit from "),Ga=r(Ni,"A",{href:!0});var Rb=a(Ga);th=o(Rb,"PretrainedConfig"),Rb.forEach(t),nh=o(Ni,` and can be used to control the model outputs. Read the
documentation from `),Za=r(Ni,"A",{href:!0});var Hb=a(Za);oh=o(Hb,"PretrainedConfig"),Hb.forEach(t),sh=o(Ni," for more information."),Ni.forEach(t),Si.forEach(t),Mc=d(i),Xt=r(i,"H2",{class:!0});var fu=a(Xt);Bn=r(fu,"A",{id:!0,class:!0,href:!0});var Vb=a(Bn);cl=r(Vb,"SPAN",{});var Yb=a(cl);b(Wo.$$.fragment,Yb),Yb.forEach(t),Vb.forEach(t),rh=d(fu),ul=r(fu,"SPAN",{});var Ub=a(ul);ah=o(Ub,"FunnelTokenizer"),Ub.forEach(t),fu.forEach(t),zc=d(i),Pe=r(i,"DIV",{class:!0});var Ge=a(Pe);b(Qo.$$.fragment,Ge),ih=d(Ge),pl=r(Ge,"P",{});var Gb=a(pl);lh=o(Gb,"Construct a Funnel Transformer tokenizer."),Gb.forEach(t),dh=d(Ge),Wn=r(Ge,"P",{});var bc=a(Wn);Ka=r(bc,"A",{href:!0});var Zb=a(Ka);ch=o(Zb,"FunnelTokenizer"),Zb.forEach(t),uh=o(bc," is identical to "),Xa=r(bc,"A",{href:!0});var Kb=a(Xa);ph=o(Kb,"BertTokenizer"),Kb.forEach(t),hh=o(bc,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),bc.forEach(t),fh=d(Ge),Ro=r(Ge,"P",{});var mu=a(Ro);mh=o(mu,"Refer to superclass "),Ja=r(mu,"A",{href:!0});var Xb=a(Ja);gh=o(Xb,"BertTokenizer"),Xb.forEach(t),_h=o(mu," for usage examples and documentation concerning parameters."),mu.forEach(t),Th=d(Ge),jt=r(Ge,"DIV",{class:!0});var Bi=a(jt);b(Ho.$$.fragment,Bi),Fh=d(Bi),hl=r(Bi,"P",{});var Jb=a(hl);vh=o(Jb,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Jb.forEach(t),kh=d(Bi),Vo=r(Bi,"UL",{});var gu=a(Vo);ei=r(gu,"LI",{});var jb=a(ei);bh=o(jb,"single sequence: "),fl=r(jb,"CODE",{});var ew=a(fl);wh=o(ew,"[CLS] X [SEP]"),ew.forEach(t),jb.forEach(t),yh=d(gu),ti=r(gu,"LI",{});var Lb=a(ti);$h=o(Lb,"pair of sequences: "),ml=r(Lb,"CODE",{});var tw=a(ml);Eh=o(tw,"[CLS] A [SEP] B [SEP]"),tw.forEach(t),Lb.forEach(t),gu.forEach(t),Bi.forEach(t),Mh=d(Ge),Qn=r(Ge,"DIV",{class:!0});var _u=a(Qn);b(Yo.$$.fragment,_u),zh=d(_u),Uo=r(_u,"P",{});var Tu=a(Uo);qh=o(Tu,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),gl=r(Tu,"CODE",{});var nw=a(gl);Ph=o(nw,"prepare_for_model"),nw.forEach(t),Ch=o(Tu," method."),Tu.forEach(t),_u.forEach(t),xh=d(Ge),kt=r(Ge,"DIV",{class:!0});var Ao=a(kt);b(Go.$$.fragment,Ao),jh=d(Ao),_l=r(Ao,"P",{});var ow=a(_l);Lh=o(ow,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),ow.forEach(t),Ah=d(Ao),b(Zo.$$.fragment,Ao),Dh=d(Ao),Jt=r(Ao,"P",{});var Wi=a(Jt);Ih=o(Wi,"If "),Tl=r(Wi,"CODE",{});var sw=a(Tl);Oh=o(sw,"token_ids_1"),sw.forEach(t),Sh=o(Wi," is "),Fl=r(Wi,"CODE",{});var rw=a(Fl);Nh=o(rw,"None"),rw.forEach(t),Bh=o(Wi,", this method only returns the first portion of the mask (0s)."),Wi.forEach(t),Ao.forEach(t),Wh=d(Ge),vl=r(Ge,"DIV",{class:!0}),a(vl).forEach(t),Ge.forEach(t),qc=d(i),en=r(i,"H2",{class:!0});var Fu=a(en);Rn=r(Fu,"A",{id:!0,class:!0,href:!0});var aw=a(Rn);kl=r(aw,"SPAN",{});var iw=a(kl);b(Ko.$$.fragment,iw),iw.forEach(t),aw.forEach(t),Qh=d(Fu),bl=r(Fu,"SPAN",{});var lw=a(bl);Rh=o(lw,"FunnelTokenizerFast"),lw.forEach(t),Fu.forEach(t),Pc=d(i),Ze=r(i,"DIV",{class:!0});var Lt=a(Ze);b(Xo.$$.fragment,Lt),Hh=d(Lt),Jo=r(Lt,"P",{});var vu=a(Jo);Vh=o(vu,"Construct a \u201Cfast\u201D Funnel Transformer tokenizer (backed by HuggingFace\u2019s "),wl=r(vu,"EM",{});var dw=a(wl);Yh=o(dw,"tokenizers"),dw.forEach(t),Uh=o(vu," library)."),vu.forEach(t),Gh=d(Lt),Hn=r(Lt,"P",{});var wc=a(Hn);ni=r(wc,"A",{href:!0});var cw=a(ni);Zh=o(cw,"FunnelTokenizerFast"),cw.forEach(t),Kh=o(wc," is identical to "),oi=r(wc,"A",{href:!0});var uw=a(oi);Xh=o(uw,"BertTokenizerFast"),uw.forEach(t),Jh=o(wc,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),wc.forEach(t),ef=d(Lt),es=r(Lt,"P",{});var ku=a(es);tf=o(ku,"Refer to superclass "),si=r(ku,"A",{href:!0});var pw=a(si);nf=o(pw,"BertTokenizerFast"),pw.forEach(t),of=o(ku," for usage examples and documentation concerning parameters."),ku.forEach(t),sf=d(Lt),bt=r(Lt,"DIV",{class:!0});var Do=a(bt);b(ts.$$.fragment,Do),rf=d(Do),yl=r(Do,"P",{});var hw=a(yl);af=o(hw,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A Funnel
Transformer sequence pair mask has the following format:`),hw.forEach(t),lf=d(Do),b(ns.$$.fragment,Do),df=d(Do),tn=r(Do,"P",{});var Qi=a(tn);cf=o(Qi,"If "),$l=r(Qi,"CODE",{});var fw=a($l);uf=o(fw,"token_ids_1"),fw.forEach(t),pf=o(Qi," is "),El=r(Qi,"CODE",{});var mw=a(El);hf=o(mw,"None"),mw.forEach(t),ff=o(Qi,", this method only returns the first portion of the mask (0s)."),Qi.forEach(t),Do.forEach(t),Lt.forEach(t),Cc=d(i),nn=r(i,"H2",{class:!0});var bu=a(nn);Vn=r(bu,"A",{id:!0,class:!0,href:!0});var gw=a(Vn);Ml=r(gw,"SPAN",{});var _w=a(Ml);b(os.$$.fragment,_w),_w.forEach(t),gw.forEach(t),mf=d(bu),zl=r(bu,"SPAN",{});var Tw=a(zl);gf=o(Tw,"Funnel specific outputs"),Tw.forEach(t),bu.forEach(t),xc=d(i),on=r(i,"DIV",{class:!0});var wu=a(on);b(ss.$$.fragment,wu),_f=d(wu),rs=r(wu,"P",{});var yu=a(rs);Tf=o(yu,"Output type of "),ri=r(yu,"A",{href:!0});var Fw=a(ri);Ff=o(Fw,"FunnelForPreTraining"),Fw.forEach(t),vf=o(yu,"."),yu.forEach(t),wu.forEach(t),jc=d(i),sn=r(i,"DIV",{class:!0});var $u=a(sn);b(as.$$.fragment,$u),kf=d($u),is=r($u,"P",{});var Eu=a(is);bf=o(Eu,"Output type of "),ai=r(Eu,"A",{href:!0});var vw=a(ai);wf=o(vw,"FunnelForPreTraining"),vw.forEach(t),yf=o(Eu,"."),Eu.forEach(t),$u.forEach(t),Lc=d(i),rn=r(i,"H2",{class:!0});var Mu=a(rn);Yn=r(Mu,"A",{id:!0,class:!0,href:!0});var kw=a(Yn);ql=r(kw,"SPAN",{});var bw=a(ql);b(ls.$$.fragment,bw),bw.forEach(t),kw.forEach(t),$f=d(Mu),Pl=r(Mu,"SPAN",{});var ww=a(Pl);Ef=o(ww,"FunnelBaseModel"),ww.forEach(t),Mu.forEach(t),Ac=d(i),We=r(i,"DIV",{class:!0});var wt=a(We);b(ds.$$.fragment,wt),Mf=d(wt),Cl=r(wt,"P",{});var yw=a(Cl);zf=o(yw,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),yw.forEach(t),qf=d(wt),cs=r(wt,"P",{});var zu=a(cs);Pf=o(zu,"The Funnel Transformer model was proposed in "),us=r(zu,"A",{href:!0,rel:!0});var $w=a(us);Cf=o($w,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),$w.forEach(t),xf=o(zu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),zu.forEach(t),jf=d(wt),ps=r(wt,"P",{});var qu=a(ps);Lf=o(qu,"This model inherits from "),ii=r(qu,"A",{href:!0});var Ew=a(ii);Af=o(Ew,"PreTrainedModel"),Ew.forEach(t),Df=o(qu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),qu.forEach(t),If=d(wt),hs=r(wt,"P",{});var Pu=a(hs);Of=o(Pu,"This model is also a PyTorch "),fs=r(Pu,"A",{href:!0,rel:!0});var Mw=a(fs);Sf=o(Mw,"torch.nn.Module"),Mw.forEach(t),Nf=o(Pu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Pu.forEach(t),Bf=d(wt),Ke=r(wt,"DIV",{class:!0});var At=a(Ke);b(ms.$$.fragment,At),Wf=d(At),an=r(At,"P",{});var Ri=a(an);Qf=o(Ri,"The "),li=r(Ri,"A",{href:!0});var zw=a(li);Rf=o(zw,"FunnelBaseModel"),zw.forEach(t),Hf=o(Ri," forward method, overrides the "),xl=r(Ri,"CODE",{});var qw=a(xl);Vf=o(qw,"__call__"),qw.forEach(t),Yf=o(Ri," special method."),Ri.forEach(t),Uf=d(At),b(Un.$$.fragment,At),Gf=d(At),jl=r(At,"P",{});var Pw=a(jl);Zf=o(Pw,"Example:"),Pw.forEach(t),Kf=d(At),b(gs.$$.fragment,At),At.forEach(t),wt.forEach(t),Dc=d(i),ln=r(i,"H2",{class:!0});var Cu=a(ln);Gn=r(Cu,"A",{id:!0,class:!0,href:!0});var Cw=a(Gn);Ll=r(Cw,"SPAN",{});var xw=a(Ll);b(_s.$$.fragment,xw),xw.forEach(t),Cw.forEach(t),Xf=d(Cu),Al=r(Cu,"SPAN",{});var jw=a(Al);Jf=o(jw,"FunnelModel"),jw.forEach(t),Cu.forEach(t),Ic=d(i),Qe=r(i,"DIV",{class:!0});var yt=a(Qe);b(Ts.$$.fragment,yt),em=d(yt),Dl=r(yt,"P",{});var Lw=a(Dl);tm=o(Lw,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),Lw.forEach(t),nm=d(yt),Fs=r(yt,"P",{});var xu=a(Fs);om=o(xu,"The Funnel Transformer model was proposed in "),vs=r(xu,"A",{href:!0,rel:!0});var Aw=a(vs);sm=o(Aw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Aw.forEach(t),rm=o(xu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),xu.forEach(t),am=d(yt),ks=r(yt,"P",{});var ju=a(ks);im=o(ju,"This model inherits from "),di=r(ju,"A",{href:!0});var Dw=a(di);lm=o(Dw,"PreTrainedModel"),Dw.forEach(t),dm=o(ju,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ju.forEach(t),cm=d(yt),bs=r(yt,"P",{});var Lu=a(bs);um=o(Lu,"This model is also a PyTorch "),ws=r(Lu,"A",{href:!0,rel:!0});var Iw=a(ws);pm=o(Iw,"torch.nn.Module"),Iw.forEach(t),hm=o(Lu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Lu.forEach(t),fm=d(yt),Xe=r(yt,"DIV",{class:!0});var Dt=a(Xe);b(ys.$$.fragment,Dt),mm=d(Dt),dn=r(Dt,"P",{});var Hi=a(dn);gm=o(Hi,"The "),ci=r(Hi,"A",{href:!0});var Ow=a(ci);_m=o(Ow,"FunnelModel"),Ow.forEach(t),Tm=o(Hi," forward method, overrides the "),Il=r(Hi,"CODE",{});var Sw=a(Il);Fm=o(Sw,"__call__"),Sw.forEach(t),vm=o(Hi," special method."),Hi.forEach(t),km=d(Dt),b(Zn.$$.fragment,Dt),bm=d(Dt),Ol=r(Dt,"P",{});var Nw=a(Ol);wm=o(Nw,"Example:"),Nw.forEach(t),ym=d(Dt),b($s.$$.fragment,Dt),Dt.forEach(t),yt.forEach(t),Oc=d(i),cn=r(i,"H2",{class:!0});var Au=a(cn);Kn=r(Au,"A",{id:!0,class:!0,href:!0});var Bw=a(Kn);Sl=r(Bw,"SPAN",{});var Ww=a(Sl);b(Es.$$.fragment,Ww),Ww.forEach(t),Bw.forEach(t),$m=d(Au),Nl=r(Au,"SPAN",{});var Qw=a(Nl);Em=o(Qw,"FunnelModelForPreTraining"),Qw.forEach(t),Au.forEach(t),Sc=d(i),Ms=r(i,"DIV",{class:!0});var Rw=a(Ms);Je=r(Rw,"DIV",{class:!0});var It=a(Je);b(zs.$$.fragment,It),Mm=d(It),un=r(It,"P",{});var Vi=a(un);zm=o(Vi,"The "),ui=r(Vi,"A",{href:!0});var Hw=a(ui);qm=o(Hw,"FunnelForPreTraining"),Hw.forEach(t),Pm=o(Vi," forward method, overrides the "),Bl=r(Vi,"CODE",{});var Vw=a(Bl);Cm=o(Vw,"__call__"),Vw.forEach(t),xm=o(Vi," special method."),Vi.forEach(t),jm=d(It),b(Xn.$$.fragment,It),Lm=d(It),Wl=r(It,"P",{});var Yw=a(Wl);Am=o(Yw,"Examples:"),Yw.forEach(t),Dm=d(It),b(qs.$$.fragment,It),It.forEach(t),Rw.forEach(t),Nc=d(i),pn=r(i,"H2",{class:!0});var Du=a(pn);Jn=r(Du,"A",{id:!0,class:!0,href:!0});var Uw=a(Jn);Ql=r(Uw,"SPAN",{});var Gw=a(Ql);b(Ps.$$.fragment,Gw),Gw.forEach(t),Uw.forEach(t),Im=d(Du),Rl=r(Du,"SPAN",{});var Zw=a(Rl);Om=o(Zw,"FunnelForMaskedLM"),Zw.forEach(t),Du.forEach(t),Bc=d(i),Re=r(i,"DIV",{class:!0});var $t=a(Re);b(Cs.$$.fragment,$t),Sm=d($t),xs=r($t,"P",{});var Iu=a(xs);Nm=o(Iu,"Funnel Transformer Model with a "),Hl=r(Iu,"CODE",{});var Kw=a(Hl);Bm=o(Kw,"language modeling"),Kw.forEach(t),Wm=o(Iu," head on top."),Iu.forEach(t),Qm=d($t),js=r($t,"P",{});var Ou=a(js);Rm=o(Ou,"The Funnel Transformer model was proposed in "),Ls=r(Ou,"A",{href:!0,rel:!0});var Xw=a(Ls);Hm=o(Xw,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Xw.forEach(t),Vm=o(Ou," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Ou.forEach(t),Ym=d($t),As=r($t,"P",{});var Su=a(As);Um=o(Su,"This model inherits from "),pi=r(Su,"A",{href:!0});var Jw=a(pi);Gm=o(Jw,"PreTrainedModel"),Jw.forEach(t),Zm=o(Su,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Su.forEach(t),Km=d($t),Ds=r($t,"P",{});var Nu=a(Ds);Xm=o(Nu,"This model is also a PyTorch "),Is=r(Nu,"A",{href:!0,rel:!0});var e1=a(Is);Jm=o(e1,"torch.nn.Module"),e1.forEach(t),eg=o(Nu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Nu.forEach(t),tg=d($t),et=r($t,"DIV",{class:!0});var Ot=a(et);b(Os.$$.fragment,Ot),ng=d(Ot),hn=r(Ot,"P",{});var Yi=a(hn);og=o(Yi,"The "),hi=r(Yi,"A",{href:!0});var t1=a(hi);sg=o(t1,"FunnelForMaskedLM"),t1.forEach(t),rg=o(Yi," forward method, overrides the "),Vl=r(Yi,"CODE",{});var n1=a(Vl);ag=o(n1,"__call__"),n1.forEach(t),ig=o(Yi," special method."),Yi.forEach(t),lg=d(Ot),b(eo.$$.fragment,Ot),dg=d(Ot),Yl=r(Ot,"P",{});var o1=a(Yl);cg=o(o1,"Example:"),o1.forEach(t),ug=d(Ot),b(Ss.$$.fragment,Ot),Ot.forEach(t),$t.forEach(t),Wc=d(i),fn=r(i,"H2",{class:!0});var Bu=a(fn);to=r(Bu,"A",{id:!0,class:!0,href:!0});var s1=a(to);Ul=r(s1,"SPAN",{});var r1=a(Ul);b(Ns.$$.fragment,r1),r1.forEach(t),s1.forEach(t),pg=d(Bu),Gl=r(Bu,"SPAN",{});var a1=a(Gl);hg=o(a1,"FunnelForSequenceClassification"),a1.forEach(t),Bu.forEach(t),Qc=d(i),He=r(i,"DIV",{class:!0});var Et=a(He);b(Bs.$$.fragment,Et),fg=d(Et),Zl=r(Et,"P",{});var i1=a(Zl);mg=o(i1,`Funnel Transformer Model with a sequence classification/regression head on top (two linear layer on top of the
first timestep of the last hidden state) e.g. for GLUE tasks.`),i1.forEach(t),gg=d(Et),Ws=r(Et,"P",{});var Wu=a(Ws);_g=o(Wu,"The Funnel Transformer model was proposed in "),Qs=r(Wu,"A",{href:!0,rel:!0});var l1=a(Qs);Tg=o(l1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),l1.forEach(t),Fg=o(Wu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Wu.forEach(t),vg=d(Et),Rs=r(Et,"P",{});var Qu=a(Rs);kg=o(Qu,"This model inherits from "),fi=r(Qu,"A",{href:!0});var d1=a(fi);bg=o(d1,"PreTrainedModel"),d1.forEach(t),wg=o(Qu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qu.forEach(t),yg=d(Et),Hs=r(Et,"P",{});var Ru=a(Hs);$g=o(Ru,"This model is also a PyTorch "),Vs=r(Ru,"A",{href:!0,rel:!0});var c1=a(Vs);Eg=o(c1,"torch.nn.Module"),c1.forEach(t),Mg=o(Ru,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ru.forEach(t),zg=d(Et),Be=r(Et,"DIV",{class:!0});var pt=a(Be);b(Ys.$$.fragment,pt),qg=d(pt),mn=r(pt,"P",{});var Ui=a(mn);Pg=o(Ui,"The "),mi=r(Ui,"A",{href:!0});var u1=a(mi);Cg=o(u1,"FunnelForSequenceClassification"),u1.forEach(t),xg=o(Ui," forward method, overrides the "),Kl=r(Ui,"CODE",{});var p1=a(Kl);jg=o(p1,"__call__"),p1.forEach(t),Lg=o(Ui," special method."),Ui.forEach(t),Ag=d(pt),b(no.$$.fragment,pt),Dg=d(pt),Xl=r(pt,"P",{});var h1=a(Xl);Ig=o(h1,"Example of single-label classification:"),h1.forEach(t),Og=d(pt),b(Us.$$.fragment,pt),Sg=d(pt),Jl=r(pt,"P",{});var f1=a(Jl);Ng=o(f1,"Example of multi-label classification:"),f1.forEach(t),Bg=d(pt),b(Gs.$$.fragment,pt),pt.forEach(t),Et.forEach(t),Rc=d(i),gn=r(i,"H2",{class:!0});var Hu=a(gn);oo=r(Hu,"A",{id:!0,class:!0,href:!0});var m1=a(oo);ed=r(m1,"SPAN",{});var g1=a(ed);b(Zs.$$.fragment,g1),g1.forEach(t),m1.forEach(t),Wg=d(Hu),td=r(Hu,"SPAN",{});var _1=a(td);Qg=o(_1,"FunnelForMultipleChoice"),_1.forEach(t),Hu.forEach(t),Hc=d(i),Ve=r(i,"DIV",{class:!0});var Mt=a(Ve);b(Ks.$$.fragment,Mt),Rg=d(Mt),nd=r(Mt,"P",{});var T1=a(nd);Hg=o(T1,`Funnel Transformer Model with a multiple choice classification head on top (two linear layer on top of the first
timestep of the last hidden state, and a softmax) e.g. for RocStories/SWAG tasks.`),T1.forEach(t),Vg=d(Mt),Xs=r(Mt,"P",{});var Vu=a(Xs);Yg=o(Vu,"The Funnel Transformer model was proposed in "),Js=r(Vu,"A",{href:!0,rel:!0});var F1=a(Js);Ug=o(F1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),F1.forEach(t),Gg=o(Vu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Vu.forEach(t),Zg=d(Mt),er=r(Mt,"P",{});var Yu=a(er);Kg=o(Yu,"This model inherits from "),gi=r(Yu,"A",{href:!0});var v1=a(gi);Xg=o(v1,"PreTrainedModel"),v1.forEach(t),Jg=o(Yu,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Yu.forEach(t),e_=d(Mt),tr=r(Mt,"P",{});var Uu=a(tr);t_=o(Uu,"This model is also a PyTorch "),nr=r(Uu,"A",{href:!0,rel:!0});var k1=a(nr);n_=o(k1,"torch.nn.Module"),k1.forEach(t),o_=o(Uu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Uu.forEach(t),s_=d(Mt),tt=r(Mt,"DIV",{class:!0});var St=a(tt);b(or.$$.fragment,St),r_=d(St),_n=r(St,"P",{});var Gi=a(_n);a_=o(Gi,"The "),_i=r(Gi,"A",{href:!0});var b1=a(_i);i_=o(b1,"FunnelForMultipleChoice"),b1.forEach(t),l_=o(Gi," forward method, overrides the "),od=r(Gi,"CODE",{});var w1=a(od);d_=o(w1,"__call__"),w1.forEach(t),c_=o(Gi," special method."),Gi.forEach(t),u_=d(St),b(so.$$.fragment,St),p_=d(St),sd=r(St,"P",{});var y1=a(sd);h_=o(y1,"Example:"),y1.forEach(t),f_=d(St),b(sr.$$.fragment,St),St.forEach(t),Mt.forEach(t),Vc=d(i),Tn=r(i,"H2",{class:!0});var Gu=a(Tn);ro=r(Gu,"A",{id:!0,class:!0,href:!0});var $1=a(ro);rd=r($1,"SPAN",{});var E1=a(rd);b(rr.$$.fragment,E1),E1.forEach(t),$1.forEach(t),m_=d(Gu),ad=r(Gu,"SPAN",{});var M1=a(ad);g_=o(M1,"FunnelForTokenClassification"),M1.forEach(t),Gu.forEach(t),Yc=d(i),Ye=r(i,"DIV",{class:!0});var zt=a(Ye);b(ar.$$.fragment,zt),__=d(zt),id=r(zt,"P",{});var z1=a(id);T_=o(z1,`Funnel Transformer Model with a token classification head on top (a linear layer on top of the hidden-states
output) e.g. for Named-Entity-Recognition (NER) tasks.`),z1.forEach(t),F_=d(zt),ir=r(zt,"P",{});var Zu=a(ir);v_=o(Zu,"The Funnel Transformer model was proposed in "),lr=r(Zu,"A",{href:!0,rel:!0});var q1=a(lr);k_=o(q1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),q1.forEach(t),b_=o(Zu," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),Zu.forEach(t),w_=d(zt),dr=r(zt,"P",{});var Ku=a(dr);y_=o(Ku,"This model inherits from "),Ti=r(Ku,"A",{href:!0});var P1=a(Ti);$_=o(P1,"PreTrainedModel"),P1.forEach(t),E_=o(Ku,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ku.forEach(t),M_=d(zt),cr=r(zt,"P",{});var Xu=a(cr);z_=o(Xu,"This model is also a PyTorch "),ur=r(Xu,"A",{href:!0,rel:!0});var C1=a(ur);q_=o(C1,"torch.nn.Module"),C1.forEach(t),P_=o(Xu,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Xu.forEach(t),C_=d(zt),nt=r(zt,"DIV",{class:!0});var Nt=a(nt);b(pr.$$.fragment,Nt),x_=d(Nt),Fn=r(Nt,"P",{});var Zi=a(Fn);j_=o(Zi,"The "),Fi=r(Zi,"A",{href:!0});var x1=a(Fi);L_=o(x1,"FunnelForTokenClassification"),x1.forEach(t),A_=o(Zi," forward method, overrides the "),ld=r(Zi,"CODE",{});var j1=a(ld);D_=o(j1,"__call__"),j1.forEach(t),I_=o(Zi," special method."),Zi.forEach(t),O_=d(Nt),b(ao.$$.fragment,Nt),S_=d(Nt),dd=r(Nt,"P",{});var L1=a(dd);N_=o(L1,"Example:"),L1.forEach(t),B_=d(Nt),b(hr.$$.fragment,Nt),Nt.forEach(t),zt.forEach(t),Uc=d(i),vn=r(i,"H2",{class:!0});var Ju=a(vn);io=r(Ju,"A",{id:!0,class:!0,href:!0});var A1=a(io);cd=r(A1,"SPAN",{});var D1=a(cd);b(fr.$$.fragment,D1),D1.forEach(t),A1.forEach(t),W_=d(Ju),ud=r(Ju,"SPAN",{});var I1=a(ud);Q_=o(I1,"FunnelForQuestionAnswering"),I1.forEach(t),Ju.forEach(t),Gc=d(i),Ue=r(i,"DIV",{class:!0});var qt=a(Ue);b(mr.$$.fragment,qt),R_=d(qt),kn=r(qt,"P",{});var Ki=a(kn);H_=o(Ki,`Funnel Transformer Model with a span classification head on top for extractive question-answering tasks like SQuAD
(a linear layer on top of the hidden-states output to compute `),pd=r(Ki,"CODE",{});var O1=a(pd);V_=o(O1,"span start logits"),O1.forEach(t),Y_=o(Ki," and "),hd=r(Ki,"CODE",{});var S1=a(hd);U_=o(S1,"span end logits"),S1.forEach(t),G_=o(Ki,")."),Ki.forEach(t),Z_=d(qt),gr=r(qt,"P",{});var ep=a(gr);K_=o(ep,"The Funnel Transformer model was proposed in "),_r=r(ep,"A",{href:!0,rel:!0});var N1=a(_r);X_=o(N1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),N1.forEach(t),J_=o(ep," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),ep.forEach(t),eT=d(qt),Tr=r(qt,"P",{});var tp=a(Tr);tT=o(tp,"This model inherits from "),vi=r(tp,"A",{href:!0});var B1=a(vi);nT=o(B1,"PreTrainedModel"),B1.forEach(t),oT=o(tp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),tp.forEach(t),sT=d(qt),Fr=r(qt,"P",{});var np=a(Fr);rT=o(np,"This model is also a PyTorch "),vr=r(np,"A",{href:!0,rel:!0});var W1=a(vr);aT=o(W1,"torch.nn.Module"),W1.forEach(t),iT=o(np,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),np.forEach(t),lT=d(qt),ot=r(qt,"DIV",{class:!0});var Bt=a(ot);b(kr.$$.fragment,Bt),dT=d(Bt),bn=r(Bt,"P",{});var Xi=a(bn);cT=o(Xi,"The "),ki=r(Xi,"A",{href:!0});var Q1=a(ki);uT=o(Q1,"FunnelForQuestionAnswering"),Q1.forEach(t),pT=o(Xi," forward method, overrides the "),fd=r(Xi,"CODE",{});var R1=a(fd);hT=o(R1,"__call__"),R1.forEach(t),fT=o(Xi," special method."),Xi.forEach(t),mT=d(Bt),b(lo.$$.fragment,Bt),gT=d(Bt),md=r(Bt,"P",{});var H1=a(md);_T=o(H1,"Example:"),H1.forEach(t),TT=d(Bt),b(br.$$.fragment,Bt),Bt.forEach(t),qt.forEach(t),Zc=d(i),wn=r(i,"H2",{class:!0});var op=a(wn);co=r(op,"A",{id:!0,class:!0,href:!0});var V1=a(co);gd=r(V1,"SPAN",{});var Y1=a(gd);b(wr.$$.fragment,Y1),Y1.forEach(t),V1.forEach(t),FT=d(op),_d=r(op,"SPAN",{});var U1=a(_d);vT=o(U1,"TFFunnelBaseModel"),U1.forEach(t),op.forEach(t),Kc=d(i),xe=r(i,"DIV",{class:!0});var ht=a(xe);b(yr.$$.fragment,ht),kT=d(ht),Td=r(ht,"P",{});var G1=a(Td);bT=o(G1,`The base Funnel Transformer Model transformer outputting raw hidden-states without upsampling head (also called
decoder) or any task-specific head on top.`),G1.forEach(t),wT=d(ht),$r=r(ht,"P",{});var sp=a($r);yT=o(sp,"The Funnel Transformer model was proposed in "),Er=r(sp,"A",{href:!0,rel:!0});var Z1=a(Er);$T=o(Z1,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Z1.forEach(t),ET=o(sp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),sp.forEach(t),MT=d(ht),Mr=r(ht,"P",{});var rp=a(Mr);zT=o(rp,"This model inherits from "),bi=r(rp,"A",{href:!0});var K1=a(bi);qT=o(K1,"TFPreTrainedModel"),K1.forEach(t),PT=o(rp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),rp.forEach(t),CT=d(ht),zr=r(ht,"P",{});var ap=a(zr);xT=o(ap,"This model is also a "),qr=r(ap,"A",{href:!0,rel:!0});var X1=a(qr);jT=o(X1,"tf.keras.Model"),X1.forEach(t),LT=o(ap,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),ap.forEach(t),AT=d(ht),b(uo.$$.fragment,ht),DT=d(ht),st=r(ht,"DIV",{class:!0});var Wt=a(st);b(Pr.$$.fragment,Wt),IT=d(Wt),yn=r(Wt,"P",{});var Ji=a(yn);OT=o(Ji,"The "),wi=r(Ji,"A",{href:!0});var J1=a(wi);ST=o(J1,"TFFunnelBaseModel"),J1.forEach(t),NT=o(Ji," forward method, overrides the "),Fd=r(Ji,"CODE",{});var ey=a(Fd);BT=o(ey,"__call__"),ey.forEach(t),WT=o(Ji," special method."),Ji.forEach(t),QT=d(Wt),b(po.$$.fragment,Wt),RT=d(Wt),vd=r(Wt,"P",{});var ty=a(vd);HT=o(ty,"Example:"),ty.forEach(t),VT=d(Wt),b(Cr.$$.fragment,Wt),Wt.forEach(t),ht.forEach(t),Xc=d(i),$n=r(i,"H2",{class:!0});var ip=a($n);ho=r(ip,"A",{id:!0,class:!0,href:!0});var ny=a(ho);kd=r(ny,"SPAN",{});var oy=a(kd);b(xr.$$.fragment,oy),oy.forEach(t),ny.forEach(t),YT=d(ip),bd=r(ip,"SPAN",{});var sy=a(bd);UT=o(sy,"TFFunnelModel"),sy.forEach(t),ip.forEach(t),Jc=d(i),je=r(i,"DIV",{class:!0});var ft=a(je);b(jr.$$.fragment,ft),GT=d(ft),wd=r(ft,"P",{});var ry=a(wd);ZT=o(ry,"The bare Funnel Transformer Model transformer outputting raw hidden-states without any specific head on top."),ry.forEach(t),KT=d(ft),Lr=r(ft,"P",{});var lp=a(Lr);XT=o(lp,"The Funnel Transformer model was proposed in "),Ar=r(lp,"A",{href:!0,rel:!0});var ay=a(Ar);JT=o(ay,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),ay.forEach(t),eF=o(lp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),lp.forEach(t),tF=d(ft),Dr=r(ft,"P",{});var dp=a(Dr);nF=o(dp,"This model inherits from "),yi=r(dp,"A",{href:!0});var iy=a(yi);oF=o(iy,"TFPreTrainedModel"),iy.forEach(t),sF=o(dp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dp.forEach(t),rF=d(ft),Ir=r(ft,"P",{});var cp=a(Ir);aF=o(cp,"This model is also a "),Or=r(cp,"A",{href:!0,rel:!0});var ly=a(Or);iF=o(ly,"tf.keras.Model"),ly.forEach(t),lF=o(cp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),cp.forEach(t),dF=d(ft),b(fo.$$.fragment,ft),cF=d(ft),rt=r(ft,"DIV",{class:!0});var Qt=a(rt);b(Sr.$$.fragment,Qt),uF=d(Qt),En=r(Qt,"P",{});var el=a(En);pF=o(el,"The "),$i=r(el,"A",{href:!0});var dy=a($i);hF=o(dy,"TFFunnelModel"),dy.forEach(t),fF=o(el," forward method, overrides the "),yd=r(el,"CODE",{});var cy=a(yd);mF=o(cy,"__call__"),cy.forEach(t),gF=o(el," special method."),el.forEach(t),_F=d(Qt),b(mo.$$.fragment,Qt),TF=d(Qt),$d=r(Qt,"P",{});var uy=a($d);FF=o(uy,"Example:"),uy.forEach(t),vF=d(Qt),b(Nr.$$.fragment,Qt),Qt.forEach(t),ft.forEach(t),eu=d(i),Mn=r(i,"H2",{class:!0});var up=a(Mn);go=r(up,"A",{id:!0,class:!0,href:!0});var py=a(go);Ed=r(py,"SPAN",{});var hy=a(Ed);b(Br.$$.fragment,hy),hy.forEach(t),py.forEach(t),kF=d(up),Md=r(up,"SPAN",{});var fy=a(Md);bF=o(fy,"TFFunnelModelForPreTraining"),fy.forEach(t),up.forEach(t),tu=d(i),Le=r(i,"DIV",{class:!0});var mt=a(Le);b(Wr.$$.fragment,mt),wF=d(mt),zd=r(mt,"P",{});var my=a(zd);yF=o(my,"Funnel model with a binary classification head on top as used during pretraining for identifying generated tokens."),my.forEach(t),$F=d(mt),Qr=r(mt,"P",{});var pp=a(Qr);EF=o(pp,"The Funnel Transformer model was proposed in "),Rr=r(pp,"A",{href:!0,rel:!0});var gy=a(Rr);MF=o(gy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),gy.forEach(t),zF=o(pp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),pp.forEach(t),qF=d(mt),Hr=r(mt,"P",{});var hp=a(Hr);PF=o(hp,"This model inherits from "),Ei=r(hp,"A",{href:!0});var _y=a(Ei);CF=o(_y,"TFPreTrainedModel"),_y.forEach(t),xF=o(hp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),hp.forEach(t),jF=d(mt),Vr=r(mt,"P",{});var fp=a(Vr);LF=o(fp,"This model is also a "),Yr=r(fp,"A",{href:!0,rel:!0});var Ty=a(Yr);AF=o(Ty,"tf.keras.Model"),Ty.forEach(t),DF=o(fp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),fp.forEach(t),IF=d(mt),b(_o.$$.fragment,mt),OF=d(mt),at=r(mt,"DIV",{class:!0});var Rt=a(at);b(Ur.$$.fragment,Rt),SF=d(Rt),zn=r(Rt,"P",{});var tl=a(zn);NF=o(tl,"The "),Mi=r(tl,"A",{href:!0});var Fy=a(Mi);BF=o(Fy,"TFFunnelForPreTraining"),Fy.forEach(t),WF=o(tl," forward method, overrides the "),qd=r(tl,"CODE",{});var vy=a(qd);QF=o(vy,"__call__"),vy.forEach(t),RF=o(tl," special method."),tl.forEach(t),HF=d(Rt),b(To.$$.fragment,Rt),VF=d(Rt),Pd=r(Rt,"P",{});var ky=a(Pd);YF=o(ky,"Examples:"),ky.forEach(t),UF=d(Rt),b(Gr.$$.fragment,Rt),Rt.forEach(t),mt.forEach(t),nu=d(i),qn=r(i,"H2",{class:!0});var mp=a(qn);Fo=r(mp,"A",{id:!0,class:!0,href:!0});var by=a(Fo);Cd=r(by,"SPAN",{});var wy=a(Cd);b(Zr.$$.fragment,wy),wy.forEach(t),by.forEach(t),GF=d(mp),xd=r(mp,"SPAN",{});var yy=a(xd);ZF=o(yy,"TFFunnelForMaskedLM"),yy.forEach(t),mp.forEach(t),ou=d(i),Ae=r(i,"DIV",{class:!0});var gt=a(Ae);b(Kr.$$.fragment,gt),KF=d(gt),Xr=r(gt,"P",{});var gp=a(Xr);XF=o(gp,"Funnel Model with a "),jd=r(gp,"CODE",{});var $y=a(jd);JF=o($y,"language modeling"),$y.forEach(t),ev=o(gp," head on top."),gp.forEach(t),tv=d(gt),Jr=r(gt,"P",{});var _p=a(Jr);nv=o(_p,"The Funnel Transformer model was proposed in "),ea=r(_p,"A",{href:!0,rel:!0});var Ey=a(ea);ov=o(Ey,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Ey.forEach(t),sv=o(_p," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),_p.forEach(t),rv=d(gt),ta=r(gt,"P",{});var Tp=a(ta);av=o(Tp,"This model inherits from "),zi=r(Tp,"A",{href:!0});var My=a(zi);iv=o(My,"TFPreTrainedModel"),My.forEach(t),lv=o(Tp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Tp.forEach(t),dv=d(gt),na=r(gt,"P",{});var Fp=a(na);cv=o(Fp,"This model is also a "),oa=r(Fp,"A",{href:!0,rel:!0});var zy=a(oa);uv=o(zy,"tf.keras.Model"),zy.forEach(t),pv=o(Fp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Fp.forEach(t),hv=d(gt),b(vo.$$.fragment,gt),fv=d(gt),it=r(gt,"DIV",{class:!0});var Ht=a(it);b(sa.$$.fragment,Ht),mv=d(Ht),Pn=r(Ht,"P",{});var nl=a(Pn);gv=o(nl,"The "),qi=r(nl,"A",{href:!0});var qy=a(qi);_v=o(qy,"TFFunnelForMaskedLM"),qy.forEach(t),Tv=o(nl," forward method, overrides the "),Ld=r(nl,"CODE",{});var Py=a(Ld);Fv=o(Py,"__call__"),Py.forEach(t),vv=o(nl," special method."),nl.forEach(t),kv=d(Ht),b(ko.$$.fragment,Ht),bv=d(Ht),Ad=r(Ht,"P",{});var Cy=a(Ad);wv=o(Cy,"Example:"),Cy.forEach(t),yv=d(Ht),b(ra.$$.fragment,Ht),Ht.forEach(t),gt.forEach(t),su=d(i),Cn=r(i,"H2",{class:!0});var vp=a(Cn);bo=r(vp,"A",{id:!0,class:!0,href:!0});var xy=a(bo);Dd=r(xy,"SPAN",{});var jy=a(Dd);b(aa.$$.fragment,jy),jy.forEach(t),xy.forEach(t),$v=d(vp),Id=r(vp,"SPAN",{});var Ly=a(Id);Ev=o(Ly,"TFFunnelForSequenceClassification"),Ly.forEach(t),vp.forEach(t),ru=d(i),De=r(i,"DIV",{class:!0});var _t=a(De);b(ia.$$.fragment,_t),Mv=d(_t),Od=r(_t,"P",{});var Ay=a(Od);zv=o(Ay,`Funnel Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
output) e.g. for GLUE tasks.`),Ay.forEach(t),qv=d(_t),la=r(_t,"P",{});var kp=a(la);Pv=o(kp,"The Funnel Transformer model was proposed in "),da=r(kp,"A",{href:!0,rel:!0});var Dy=a(da);Cv=o(Dy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Dy.forEach(t),xv=o(kp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),kp.forEach(t),jv=d(_t),ca=r(_t,"P",{});var bp=a(ca);Lv=o(bp,"This model inherits from "),Pi=r(bp,"A",{href:!0});var Iy=a(Pi);Av=o(Iy,"TFPreTrainedModel"),Iy.forEach(t),Dv=o(bp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),bp.forEach(t),Iv=d(_t),ua=r(_t,"P",{});var wp=a(ua);Ov=o(wp,"This model is also a "),pa=r(wp,"A",{href:!0,rel:!0});var Oy=a(pa);Sv=o(Oy,"tf.keras.Model"),Oy.forEach(t),Nv=o(wp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),wp.forEach(t),Bv=d(_t),b(wo.$$.fragment,_t),Wv=d(_t),lt=r(_t,"DIV",{class:!0});var Vt=a(lt);b(ha.$$.fragment,Vt),Qv=d(Vt),xn=r(Vt,"P",{});var ol=a(xn);Rv=o(ol,"The "),Ci=r(ol,"A",{href:!0});var Sy=a(Ci);Hv=o(Sy,"TFFunnelForSequenceClassification"),Sy.forEach(t),Vv=o(ol," forward method, overrides the "),Sd=r(ol,"CODE",{});var Ny=a(Sd);Yv=o(Ny,"__call__"),Ny.forEach(t),Uv=o(ol," special method."),ol.forEach(t),Gv=d(Vt),b(yo.$$.fragment,Vt),Zv=d(Vt),Nd=r(Vt,"P",{});var By=a(Nd);Kv=o(By,"Example:"),By.forEach(t),Xv=d(Vt),b(fa.$$.fragment,Vt),Vt.forEach(t),_t.forEach(t),au=d(i),jn=r(i,"H2",{class:!0});var yp=a(jn);$o=r(yp,"A",{id:!0,class:!0,href:!0});var Wy=a($o);Bd=r(Wy,"SPAN",{});var Qy=a(Bd);b(ma.$$.fragment,Qy),Qy.forEach(t),Wy.forEach(t),Jv=d(yp),Wd=r(yp,"SPAN",{});var Ry=a(Wd);ek=o(Ry,"TFFunnelForMultipleChoice"),Ry.forEach(t),yp.forEach(t),iu=d(i),Ie=r(i,"DIV",{class:!0});var Tt=a(Ie);b(ga.$$.fragment,Tt),tk=d(Tt),Qd=r(Tt,"P",{});var Hy=a(Qd);nk=o(Hy,`Funnel Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
softmax) e.g. for RocStories/SWAG tasks.`),Hy.forEach(t),ok=d(Tt),_a=r(Tt,"P",{});var $p=a(_a);sk=o($p,"The Funnel Transformer model was proposed in "),Ta=r($p,"A",{href:!0,rel:!0});var Vy=a(Ta);rk=o(Vy,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),Vy.forEach(t),ak=o($p," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),$p.forEach(t),ik=d(Tt),Fa=r(Tt,"P",{});var Ep=a(Fa);lk=o(Ep,"This model inherits from "),xi=r(Ep,"A",{href:!0});var Yy=a(xi);dk=o(Yy,"TFPreTrainedModel"),Yy.forEach(t),ck=o(Ep,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ep.forEach(t),uk=d(Tt),va=r(Tt,"P",{});var Mp=a(va);pk=o(Mp,"This model is also a "),ka=r(Mp,"A",{href:!0,rel:!0});var Uy=a(ka);hk=o(Uy,"tf.keras.Model"),Uy.forEach(t),fk=o(Mp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Mp.forEach(t),mk=d(Tt),b(Eo.$$.fragment,Tt),gk=d(Tt),dt=r(Tt,"DIV",{class:!0});var Yt=a(dt);b(ba.$$.fragment,Yt),_k=d(Yt),Ln=r(Yt,"P",{});var sl=a(Ln);Tk=o(sl,"The "),ji=r(sl,"A",{href:!0});var Gy=a(ji);Fk=o(Gy,"TFFunnelForMultipleChoice"),Gy.forEach(t),vk=o(sl," forward method, overrides the "),Rd=r(sl,"CODE",{});var Zy=a(Rd);kk=o(Zy,"__call__"),Zy.forEach(t),bk=o(sl," special method."),sl.forEach(t),wk=d(Yt),b(Mo.$$.fragment,Yt),yk=d(Yt),Hd=r(Yt,"P",{});var Ky=a(Hd);$k=o(Ky,"Example:"),Ky.forEach(t),Ek=d(Yt),b(wa.$$.fragment,Yt),Yt.forEach(t),Tt.forEach(t),lu=d(i),An=r(i,"H2",{class:!0});var zp=a(An);zo=r(zp,"A",{id:!0,class:!0,href:!0});var Xy=a(zo);Vd=r(Xy,"SPAN",{});var Jy=a(Vd);b(ya.$$.fragment,Jy),Jy.forEach(t),Xy.forEach(t),Mk=d(zp),Yd=r(zp,"SPAN",{});var e$=a(Yd);zk=o(e$,"TFFunnelForTokenClassification"),e$.forEach(t),zp.forEach(t),du=d(i),Oe=r(i,"DIV",{class:!0});var Ft=a(Oe);b($a.$$.fragment,Ft),qk=d(Ft),Ud=r(Ft,"P",{});var t$=a(Ud);Pk=o(t$,`Funnel Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.`),t$.forEach(t),Ck=d(Ft),Ea=r(Ft,"P",{});var qp=a(Ea);xk=o(qp,"The Funnel Transformer model was proposed in "),Ma=r(qp,"A",{href:!0,rel:!0});var n$=a(Ma);jk=o(n$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),n$.forEach(t),Lk=o(qp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),qp.forEach(t),Ak=d(Ft),za=r(Ft,"P",{});var Pp=a(za);Dk=o(Pp,"This model inherits from "),Li=r(Pp,"A",{href:!0});var o$=a(Li);Ik=o(o$,"TFPreTrainedModel"),o$.forEach(t),Ok=o(Pp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Pp.forEach(t),Sk=d(Ft),qa=r(Ft,"P",{});var Cp=a(qa);Nk=o(Cp,"This model is also a "),Pa=r(Cp,"A",{href:!0,rel:!0});var s$=a(Pa);Bk=o(s$,"tf.keras.Model"),s$.forEach(t),Wk=o(Cp,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Cp.forEach(t),Qk=d(Ft),b(qo.$$.fragment,Ft),Rk=d(Ft),ct=r(Ft,"DIV",{class:!0});var Ut=a(ct);b(Ca.$$.fragment,Ut),Hk=d(Ut),Dn=r(Ut,"P",{});var rl=a(Dn);Vk=o(rl,"The "),Ai=r(rl,"A",{href:!0});var r$=a(Ai);Yk=o(r$,"TFFunnelForTokenClassification"),r$.forEach(t),Uk=o(rl," forward method, overrides the "),Gd=r(rl,"CODE",{});var a$=a(Gd);Gk=o(a$,"__call__"),a$.forEach(t),Zk=o(rl," special method."),rl.forEach(t),Kk=d(Ut),b(Po.$$.fragment,Ut),Xk=d(Ut),Zd=r(Ut,"P",{});var i$=a(Zd);Jk=o(i$,"Example:"),i$.forEach(t),eb=d(Ut),b(xa.$$.fragment,Ut),Ut.forEach(t),Ft.forEach(t),cu=d(i),In=r(i,"H2",{class:!0});var xp=a(In);Co=r(xp,"A",{id:!0,class:!0,href:!0});var l$=a(Co);Kd=r(l$,"SPAN",{});var d$=a(Kd);b(ja.$$.fragment,d$),d$.forEach(t),l$.forEach(t),tb=d(xp),Xd=r(xp,"SPAN",{});var c$=a(Xd);nb=o(c$,"TFFunnelForQuestionAnswering"),c$.forEach(t),xp.forEach(t),uu=d(i),Se=r(i,"DIV",{class:!0});var vt=a(Se);b(La.$$.fragment,vt),ob=d(vt),On=r(vt,"P",{});var al=a(On);sb=o(al,`Funnel Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
layers on top of the hidden-states output to compute `),Jd=r(al,"CODE",{});var u$=a(Jd);rb=o(u$,"span start logits"),u$.forEach(t),ab=o(al," and "),ec=r(al,"CODE",{});var p$=a(ec);ib=o(p$,"span end logits"),p$.forEach(t),lb=o(al,")."),al.forEach(t),db=d(vt),Aa=r(vt,"P",{});var jp=a(Aa);cb=o(jp,"The Funnel Transformer model was proposed in "),Da=r(jp,"A",{href:!0,rel:!0});var h$=a(Da);ub=o(h$,`Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
Language Processing`),h$.forEach(t),pb=o(jp," by Zihang Dai, Guokun Lai, Yiming Yang, Quoc V. Le."),jp.forEach(t),hb=d(vt),Ia=r(vt,"P",{});var Lp=a(Ia);fb=o(Lp,"This model inherits from "),Di=r(Lp,"A",{href:!0});var f$=a(Di);mb=o(f$,"TFPreTrainedModel"),f$.forEach(t),gb=o(Lp,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Lp.forEach(t),_b=d(vt),Oa=r(vt,"P",{});var Ap=a(Oa);Tb=o(Ap,"This model is also a "),Sa=r(Ap,"A",{href:!0,rel:!0});var m$=a(Sa);Fb=o(m$,"tf.keras.Model"),m$.forEach(t),vb=o(Ap,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Ap.forEach(t),kb=d(vt),b(xo.$$.fragment,vt),bb=d(vt),ut=r(vt,"DIV",{class:!0});var Gt=a(ut);b(Na.$$.fragment,Gt),wb=d(Gt),Sn=r(Gt,"P",{});var il=a(Sn);yb=o(il,"The "),Ii=r(il,"A",{href:!0});var g$=a(Ii);$b=o(g$,"TFFunnelForQuestionAnswering"),g$.forEach(t),Eb=o(il," forward method, overrides the "),tc=r(il,"CODE",{});var _$=a(tc);Mb=o(_$,"__call__"),_$.forEach(t),zb=o(il," special method."),il.forEach(t),qb=d(Gt),b(jo.$$.fragment,Gt),Pb=d(Gt),nc=r(Gt,"P",{});var T$=a(nc);Cb=o(T$,"Example:"),T$.forEach(t),xb=d(Gt),b(Ba.$$.fragment,Gt),Gt.forEach(t),vt.forEach(t),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(U$)),c(g,"id","funnel-transformer"),c(g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g,"href","#funnel-transformer"),c(m,"class","relative group"),c(X,"id","overview"),c(X,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X,"href","#overview"),c(q,"class","relative group"),c(ne,"href","https://arxiv.org/abs/2006.03236"),c(ne,"rel","nofollow"),c(re,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelModel"),c(u,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Te,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(Fe,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(ve,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelBaseModel"),c(ke,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Va,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(Io,"href","https://huggingface.co/sgugger"),c(Io,"rel","nofollow"),c(Oo,"href","https://github.com/laiguokun/Funnel-Transformer"),c(Oo,"rel","nofollow"),c(Nn,"id","transformers.FunnelConfig"),c(Nn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Nn,"href","#transformers.FunnelConfig"),c(Zt,"class","relative group"),c(Ya,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelModel"),c(Ua,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.TFBertModel"),c(Bo,"href","https://huggingface.co/funnel-transformer/small"),c(Bo,"rel","nofollow"),c(Ga,"href","/docs/transformers/doc-build-test/en/main_classes/configuration#transformers.PretrainedConfig"),c(Za,"href","/docs/transformers/doc-build-test/en/main_classes/configuration#transformers.PretrainedConfig"),c(Pt,"class","docstring"),c(Bn,"id","transformers.FunnelTokenizer"),c(Bn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Bn,"href","#transformers.FunnelTokenizer"),c(Xt,"class","relative group"),c(Ka,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelTokenizer"),c(Xa,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer"),c(Ja,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizer"),c(jt,"class","docstring"),c(Qn,"class","docstring"),c(kt,"class","docstring"),c(vl,"class","docstring"),c(Pe,"class","docstring"),c(Rn,"id","transformers.FunnelTokenizerFast"),c(Rn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rn,"href","#transformers.FunnelTokenizerFast"),c(en,"class","relative group"),c(ni,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(oi,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizerFast"),c(si,"href","/docs/transformers/doc-build-test/en/model_doc/bert#transformers.BertTokenizerFast"),c(bt,"class","docstring"),c(Ze,"class","docstring"),c(Vn,"id","transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(Vn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vn,"href","#transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput"),c(nn,"class","relative group"),c(ri,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(on,"class","docstring"),c(ai,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(sn,"class","docstring"),c(Yn,"id","transformers.FunnelBaseModel"),c(Yn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Yn,"href","#transformers.FunnelBaseModel"),c(rn,"class","relative group"),c(us,"href","https://arxiv.org/abs/2006.03236"),c(us,"rel","nofollow"),c(ii,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(fs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(fs,"rel","nofollow"),c(li,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelBaseModel"),c(Ke,"class","docstring"),c(We,"class","docstring"),c(Gn,"id","transformers.FunnelModel"),c(Gn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gn,"href","#transformers.FunnelModel"),c(ln,"class","relative group"),c(vs,"href","https://arxiv.org/abs/2006.03236"),c(vs,"rel","nofollow"),c(di,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(ws,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ws,"rel","nofollow"),c(ci,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelModel"),c(Xe,"class","docstring"),c(Qe,"class","docstring"),c(Kn,"id","transformers.FunnelForPreTraining"),c(Kn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Kn,"href","#transformers.FunnelForPreTraining"),c(cn,"class","relative group"),c(ui,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(Je,"class","docstring"),c(Ms,"class","docstring"),c(Jn,"id","transformers.FunnelForMaskedLM"),c(Jn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Jn,"href","#transformers.FunnelForMaskedLM"),c(pn,"class","relative group"),c(Ls,"href","https://arxiv.org/abs/2006.03236"),c(Ls,"rel","nofollow"),c(pi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(Is,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Is,"rel","nofollow"),c(hi,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(et,"class","docstring"),c(Re,"class","docstring"),c(to,"id","transformers.FunnelForSequenceClassification"),c(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(to,"href","#transformers.FunnelForSequenceClassification"),c(fn,"class","relative group"),c(Qs,"href","https://arxiv.org/abs/2006.03236"),c(Qs,"rel","nofollow"),c(fi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(Vs,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Vs,"rel","nofollow"),c(mi,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(Be,"class","docstring"),c(He,"class","docstring"),c(oo,"id","transformers.FunnelForMultipleChoice"),c(oo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(oo,"href","#transformers.FunnelForMultipleChoice"),c(gn,"class","relative group"),c(Js,"href","https://arxiv.org/abs/2006.03236"),c(Js,"rel","nofollow"),c(gi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(nr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(nr,"rel","nofollow"),c(_i,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(tt,"class","docstring"),c(Ve,"class","docstring"),c(ro,"id","transformers.FunnelForTokenClassification"),c(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ro,"href","#transformers.FunnelForTokenClassification"),c(Tn,"class","relative group"),c(lr,"href","https://arxiv.org/abs/2006.03236"),c(lr,"rel","nofollow"),c(Ti,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(ur,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(ur,"rel","nofollow"),c(Fi,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(nt,"class","docstring"),c(Ye,"class","docstring"),c(io,"id","transformers.FunnelForQuestionAnswering"),c(io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(io,"href","#transformers.FunnelForQuestionAnswering"),c(vn,"class","relative group"),c(_r,"href","https://arxiv.org/abs/2006.03236"),c(_r,"rel","nofollow"),c(vi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel"),c(vr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(vr,"rel","nofollow"),c(ki,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(ot,"class","docstring"),c(Ue,"class","docstring"),c(co,"id","transformers.TFFunnelBaseModel"),c(co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(co,"href","#transformers.TFFunnelBaseModel"),c(wn,"class","relative group"),c(Er,"href","https://arxiv.org/abs/2006.03236"),c(Er,"rel","nofollow"),c(bi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(qr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(qr,"rel","nofollow"),c(wi,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(st,"class","docstring"),c(xe,"class","docstring"),c(ho,"id","transformers.TFFunnelModel"),c(ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ho,"href","#transformers.TFFunnelModel"),c($n,"class","relative group"),c(Ar,"href","https://arxiv.org/abs/2006.03236"),c(Ar,"rel","nofollow"),c(yi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(Or,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Or,"rel","nofollow"),c($i,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.TFFunnelModel"),c(rt,"class","docstring"),c(je,"class","docstring"),c(go,"id","transformers.TFFunnelForPreTraining"),c(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(go,"href","#transformers.TFFunnelForPreTraining"),c(Mn,"class","relative group"),c(Rr,"href","https://arxiv.org/abs/2006.03236"),c(Rr,"rel","nofollow"),c(Ei,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(Yr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Yr,"rel","nofollow"),c(Mi,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(at,"class","docstring"),c(Le,"class","docstring"),c(Fo,"id","transformers.TFFunnelForMaskedLM"),c(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fo,"href","#transformers.TFFunnelForMaskedLM"),c(qn,"class","relative group"),c(ea,"href","https://arxiv.org/abs/2006.03236"),c(ea,"rel","nofollow"),c(zi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(oa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(oa,"rel","nofollow"),c(qi,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(it,"class","docstring"),c(Ae,"class","docstring"),c(bo,"id","transformers.TFFunnelForSequenceClassification"),c(bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bo,"href","#transformers.TFFunnelForSequenceClassification"),c(Cn,"class","relative group"),c(da,"href","https://arxiv.org/abs/2006.03236"),c(da,"rel","nofollow"),c(Pi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(pa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(pa,"rel","nofollow"),c(Ci,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(lt,"class","docstring"),c(De,"class","docstring"),c($o,"id","transformers.TFFunnelForMultipleChoice"),c($o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($o,"href","#transformers.TFFunnelForMultipleChoice"),c(jn,"class","relative group"),c(Ta,"href","https://arxiv.org/abs/2006.03236"),c(Ta,"rel","nofollow"),c(xi,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(ka,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(ka,"rel","nofollow"),c(ji,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(dt,"class","docstring"),c(Ie,"class","docstring"),c(zo,"id","transformers.TFFunnelForTokenClassification"),c(zo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zo,"href","#transformers.TFFunnelForTokenClassification"),c(An,"class","relative group"),c(Ma,"href","https://arxiv.org/abs/2006.03236"),c(Ma,"rel","nofollow"),c(Li,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(Pa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Pa,"rel","nofollow"),c(Ai,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(ct,"class","docstring"),c(Oe,"class","docstring"),c(Co,"id","transformers.TFFunnelForQuestionAnswering"),c(Co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Co,"href","#transformers.TFFunnelForQuestionAnswering"),c(In,"class","relative group"),c(Da,"href","https://arxiv.org/abs/2006.03236"),c(Da,"rel","nofollow"),c(Di,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.TFPreTrainedModel"),c(Sa,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Sa,"rel","nofollow"),c(Ii,"href","/docs/transformers/doc-build-test/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(ut,"class","docstring"),c(Se,"class","docstring")},m(i,f){e(document.head,p),h(i,M,f),h(i,m,f),e(m,g),e(g,F),w(T,F,null),e(m,_),e(m,z),e(z,ce),h(i,G,f),h(i,q,f),e(q,X),e(X,I),w(te,I,null),e(q,ue),e(q,O),e(O,pe),h(i,ie,f),h(i,U,f),e(U,L),e(U,ne),e(ne,Z),e(U,P),h(i,x,f),h(i,oe,f),e(oe,Q),h(i,le,f),h(i,se,f),e(se,S),e(S,he),h(i,de,f),h(i,C,f),e(C,fe),h(i,B,f),h(i,J,f),e(J,ae),e(ae,R),e(J,me),e(J,N),e(N,A),e(N,re),e(re,H),e(N,ge),e(N,u),e(u,v),e(N,K),e(N,Te),e(Te,be),e(N,D),e(N,Fe),e(Fe,we),e(N,ye),e(N,j),e(j,V),e(N,$e),e(N,ve),e(ve,Y),e(N,Ee),e(N,ke),e(ke,_e),e(N,Me),e(N,Va),e(Va,Dp),e(N,Ip),h(i,yc,f),h(i,xt,f),e(xt,Op),e(xt,Io),e(Io,Sp),e(xt,Np),e(xt,Oo),e(Oo,Bp),e(xt,Wp),h(i,$c,f),h(i,Zt,f),e(Zt,Nn),e(Nn,ll),w(So,ll,null),e(Zt,Qp),e(Zt,dl),e(dl,Rp),h(i,Ec,f),h(i,Pt,f),w(No,Pt,null),e(Pt,Hp),e(Pt,Ct),e(Ct,Vp),e(Ct,Ya),e(Ya,Yp),e(Ct,Up),e(Ct,Ua),e(Ua,Gp),e(Ct,Zp),e(Ct,Bo),e(Bo,Kp),e(Ct,Xp),e(Pt,Jp),e(Pt,Kt),e(Kt,eh),e(Kt,Ga),e(Ga,th),e(Kt,nh),e(Kt,Za),e(Za,oh),e(Kt,sh),h(i,Mc,f),h(i,Xt,f),e(Xt,Bn),e(Bn,cl),w(Wo,cl,null),e(Xt,rh),e(Xt,ul),e(ul,ah),h(i,zc,f),h(i,Pe,f),w(Qo,Pe,null),e(Pe,ih),e(Pe,pl),e(pl,lh),e(Pe,dh),e(Pe,Wn),e(Wn,Ka),e(Ka,ch),e(Wn,uh),e(Wn,Xa),e(Xa,ph),e(Wn,hh),e(Pe,fh),e(Pe,Ro),e(Ro,mh),e(Ro,Ja),e(Ja,gh),e(Ro,_h),e(Pe,Th),e(Pe,jt),w(Ho,jt,null),e(jt,Fh),e(jt,hl),e(hl,vh),e(jt,kh),e(jt,Vo),e(Vo,ei),e(ei,bh),e(ei,fl),e(fl,wh),e(Vo,yh),e(Vo,ti),e(ti,$h),e(ti,ml),e(ml,Eh),e(Pe,Mh),e(Pe,Qn),w(Yo,Qn,null),e(Qn,zh),e(Qn,Uo),e(Uo,qh),e(Uo,gl),e(gl,Ph),e(Uo,Ch),e(Pe,xh),e(Pe,kt),w(Go,kt,null),e(kt,jh),e(kt,_l),e(_l,Lh),e(kt,Ah),w(Zo,kt,null),e(kt,Dh),e(kt,Jt),e(Jt,Ih),e(Jt,Tl),e(Tl,Oh),e(Jt,Sh),e(Jt,Fl),e(Fl,Nh),e(Jt,Bh),e(Pe,Wh),e(Pe,vl),h(i,qc,f),h(i,en,f),e(en,Rn),e(Rn,kl),w(Ko,kl,null),e(en,Qh),e(en,bl),e(bl,Rh),h(i,Pc,f),h(i,Ze,f),w(Xo,Ze,null),e(Ze,Hh),e(Ze,Jo),e(Jo,Vh),e(Jo,wl),e(wl,Yh),e(Jo,Uh),e(Ze,Gh),e(Ze,Hn),e(Hn,ni),e(ni,Zh),e(Hn,Kh),e(Hn,oi),e(oi,Xh),e(Hn,Jh),e(Ze,ef),e(Ze,es),e(es,tf),e(es,si),e(si,nf),e(es,of),e(Ze,sf),e(Ze,bt),w(ts,bt,null),e(bt,rf),e(bt,yl),e(yl,af),e(bt,lf),w(ns,bt,null),e(bt,df),e(bt,tn),e(tn,cf),e(tn,$l),e($l,uf),e(tn,pf),e(tn,El),e(El,hf),e(tn,ff),h(i,Cc,f),h(i,nn,f),e(nn,Vn),e(Vn,Ml),w(os,Ml,null),e(nn,mf),e(nn,zl),e(zl,gf),h(i,xc,f),h(i,on,f),w(ss,on,null),e(on,_f),e(on,rs),e(rs,Tf),e(rs,ri),e(ri,Ff),e(rs,vf),h(i,jc,f),h(i,sn,f),w(as,sn,null),e(sn,kf),e(sn,is),e(is,bf),e(is,ai),e(ai,wf),e(is,yf),h(i,Lc,f),h(i,rn,f),e(rn,Yn),e(Yn,ql),w(ls,ql,null),e(rn,$f),e(rn,Pl),e(Pl,Ef),h(i,Ac,f),h(i,We,f),w(ds,We,null),e(We,Mf),e(We,Cl),e(Cl,zf),e(We,qf),e(We,cs),e(cs,Pf),e(cs,us),e(us,Cf),e(cs,xf),e(We,jf),e(We,ps),e(ps,Lf),e(ps,ii),e(ii,Af),e(ps,Df),e(We,If),e(We,hs),e(hs,Of),e(hs,fs),e(fs,Sf),e(hs,Nf),e(We,Bf),e(We,Ke),w(ms,Ke,null),e(Ke,Wf),e(Ke,an),e(an,Qf),e(an,li),e(li,Rf),e(an,Hf),e(an,xl),e(xl,Vf),e(an,Yf),e(Ke,Uf),w(Un,Ke,null),e(Ke,Gf),e(Ke,jl),e(jl,Zf),e(Ke,Kf),w(gs,Ke,null),h(i,Dc,f),h(i,ln,f),e(ln,Gn),e(Gn,Ll),w(_s,Ll,null),e(ln,Xf),e(ln,Al),e(Al,Jf),h(i,Ic,f),h(i,Qe,f),w(Ts,Qe,null),e(Qe,em),e(Qe,Dl),e(Dl,tm),e(Qe,nm),e(Qe,Fs),e(Fs,om),e(Fs,vs),e(vs,sm),e(Fs,rm),e(Qe,am),e(Qe,ks),e(ks,im),e(ks,di),e(di,lm),e(ks,dm),e(Qe,cm),e(Qe,bs),e(bs,um),e(bs,ws),e(ws,pm),e(bs,hm),e(Qe,fm),e(Qe,Xe),w(ys,Xe,null),e(Xe,mm),e(Xe,dn),e(dn,gm),e(dn,ci),e(ci,_m),e(dn,Tm),e(dn,Il),e(Il,Fm),e(dn,vm),e(Xe,km),w(Zn,Xe,null),e(Xe,bm),e(Xe,Ol),e(Ol,wm),e(Xe,ym),w($s,Xe,null),h(i,Oc,f),h(i,cn,f),e(cn,Kn),e(Kn,Sl),w(Es,Sl,null),e(cn,$m),e(cn,Nl),e(Nl,Em),h(i,Sc,f),h(i,Ms,f),e(Ms,Je),w(zs,Je,null),e(Je,Mm),e(Je,un),e(un,zm),e(un,ui),e(ui,qm),e(un,Pm),e(un,Bl),e(Bl,Cm),e(un,xm),e(Je,jm),w(Xn,Je,null),e(Je,Lm),e(Je,Wl),e(Wl,Am),e(Je,Dm),w(qs,Je,null),h(i,Nc,f),h(i,pn,f),e(pn,Jn),e(Jn,Ql),w(Ps,Ql,null),e(pn,Im),e(pn,Rl),e(Rl,Om),h(i,Bc,f),h(i,Re,f),w(Cs,Re,null),e(Re,Sm),e(Re,xs),e(xs,Nm),e(xs,Hl),e(Hl,Bm),e(xs,Wm),e(Re,Qm),e(Re,js),e(js,Rm),e(js,Ls),e(Ls,Hm),e(js,Vm),e(Re,Ym),e(Re,As),e(As,Um),e(As,pi),e(pi,Gm),e(As,Zm),e(Re,Km),e(Re,Ds),e(Ds,Xm),e(Ds,Is),e(Is,Jm),e(Ds,eg),e(Re,tg),e(Re,et),w(Os,et,null),e(et,ng),e(et,hn),e(hn,og),e(hn,hi),e(hi,sg),e(hn,rg),e(hn,Vl),e(Vl,ag),e(hn,ig),e(et,lg),w(eo,et,null),e(et,dg),e(et,Yl),e(Yl,cg),e(et,ug),w(Ss,et,null),h(i,Wc,f),h(i,fn,f),e(fn,to),e(to,Ul),w(Ns,Ul,null),e(fn,pg),e(fn,Gl),e(Gl,hg),h(i,Qc,f),h(i,He,f),w(Bs,He,null),e(He,fg),e(He,Zl),e(Zl,mg),e(He,gg),e(He,Ws),e(Ws,_g),e(Ws,Qs),e(Qs,Tg),e(Ws,Fg),e(He,vg),e(He,Rs),e(Rs,kg),e(Rs,fi),e(fi,bg),e(Rs,wg),e(He,yg),e(He,Hs),e(Hs,$g),e(Hs,Vs),e(Vs,Eg),e(Hs,Mg),e(He,zg),e(He,Be),w(Ys,Be,null),e(Be,qg),e(Be,mn),e(mn,Pg),e(mn,mi),e(mi,Cg),e(mn,xg),e(mn,Kl),e(Kl,jg),e(mn,Lg),e(Be,Ag),w(no,Be,null),e(Be,Dg),e(Be,Xl),e(Xl,Ig),e(Be,Og),w(Us,Be,null),e(Be,Sg),e(Be,Jl),e(Jl,Ng),e(Be,Bg),w(Gs,Be,null),h(i,Rc,f),h(i,gn,f),e(gn,oo),e(oo,ed),w(Zs,ed,null),e(gn,Wg),e(gn,td),e(td,Qg),h(i,Hc,f),h(i,Ve,f),w(Ks,Ve,null),e(Ve,Rg),e(Ve,nd),e(nd,Hg),e(Ve,Vg),e(Ve,Xs),e(Xs,Yg),e(Xs,Js),e(Js,Ug),e(Xs,Gg),e(Ve,Zg),e(Ve,er),e(er,Kg),e(er,gi),e(gi,Xg),e(er,Jg),e(Ve,e_),e(Ve,tr),e(tr,t_),e(tr,nr),e(nr,n_),e(tr,o_),e(Ve,s_),e(Ve,tt),w(or,tt,null),e(tt,r_),e(tt,_n),e(_n,a_),e(_n,_i),e(_i,i_),e(_n,l_),e(_n,od),e(od,d_),e(_n,c_),e(tt,u_),w(so,tt,null),e(tt,p_),e(tt,sd),e(sd,h_),e(tt,f_),w(sr,tt,null),h(i,Vc,f),h(i,Tn,f),e(Tn,ro),e(ro,rd),w(rr,rd,null),e(Tn,m_),e(Tn,ad),e(ad,g_),h(i,Yc,f),h(i,Ye,f),w(ar,Ye,null),e(Ye,__),e(Ye,id),e(id,T_),e(Ye,F_),e(Ye,ir),e(ir,v_),e(ir,lr),e(lr,k_),e(ir,b_),e(Ye,w_),e(Ye,dr),e(dr,y_),e(dr,Ti),e(Ti,$_),e(dr,E_),e(Ye,M_),e(Ye,cr),e(cr,z_),e(cr,ur),e(ur,q_),e(cr,P_),e(Ye,C_),e(Ye,nt),w(pr,nt,null),e(nt,x_),e(nt,Fn),e(Fn,j_),e(Fn,Fi),e(Fi,L_),e(Fn,A_),e(Fn,ld),e(ld,D_),e(Fn,I_),e(nt,O_),w(ao,nt,null),e(nt,S_),e(nt,dd),e(dd,N_),e(nt,B_),w(hr,nt,null),h(i,Uc,f),h(i,vn,f),e(vn,io),e(io,cd),w(fr,cd,null),e(vn,W_),e(vn,ud),e(ud,Q_),h(i,Gc,f),h(i,Ue,f),w(mr,Ue,null),e(Ue,R_),e(Ue,kn),e(kn,H_),e(kn,pd),e(pd,V_),e(kn,Y_),e(kn,hd),e(hd,U_),e(kn,G_),e(Ue,Z_),e(Ue,gr),e(gr,K_),e(gr,_r),e(_r,X_),e(gr,J_),e(Ue,eT),e(Ue,Tr),e(Tr,tT),e(Tr,vi),e(vi,nT),e(Tr,oT),e(Ue,sT),e(Ue,Fr),e(Fr,rT),e(Fr,vr),e(vr,aT),e(Fr,iT),e(Ue,lT),e(Ue,ot),w(kr,ot,null),e(ot,dT),e(ot,bn),e(bn,cT),e(bn,ki),e(ki,uT),e(bn,pT),e(bn,fd),e(fd,hT),e(bn,fT),e(ot,mT),w(lo,ot,null),e(ot,gT),e(ot,md),e(md,_T),e(ot,TT),w(br,ot,null),h(i,Zc,f),h(i,wn,f),e(wn,co),e(co,gd),w(wr,gd,null),e(wn,FT),e(wn,_d),e(_d,vT),h(i,Kc,f),h(i,xe,f),w(yr,xe,null),e(xe,kT),e(xe,Td),e(Td,bT),e(xe,wT),e(xe,$r),e($r,yT),e($r,Er),e(Er,$T),e($r,ET),e(xe,MT),e(xe,Mr),e(Mr,zT),e(Mr,bi),e(bi,qT),e(Mr,PT),e(xe,CT),e(xe,zr),e(zr,xT),e(zr,qr),e(qr,jT),e(zr,LT),e(xe,AT),w(uo,xe,null),e(xe,DT),e(xe,st),w(Pr,st,null),e(st,IT),e(st,yn),e(yn,OT),e(yn,wi),e(wi,ST),e(yn,NT),e(yn,Fd),e(Fd,BT),e(yn,WT),e(st,QT),w(po,st,null),e(st,RT),e(st,vd),e(vd,HT),e(st,VT),w(Cr,st,null),h(i,Xc,f),h(i,$n,f),e($n,ho),e(ho,kd),w(xr,kd,null),e($n,YT),e($n,bd),e(bd,UT),h(i,Jc,f),h(i,je,f),w(jr,je,null),e(je,GT),e(je,wd),e(wd,ZT),e(je,KT),e(je,Lr),e(Lr,XT),e(Lr,Ar),e(Ar,JT),e(Lr,eF),e(je,tF),e(je,Dr),e(Dr,nF),e(Dr,yi),e(yi,oF),e(Dr,sF),e(je,rF),e(je,Ir),e(Ir,aF),e(Ir,Or),e(Or,iF),e(Ir,lF),e(je,dF),w(fo,je,null),e(je,cF),e(je,rt),w(Sr,rt,null),e(rt,uF),e(rt,En),e(En,pF),e(En,$i),e($i,hF),e(En,fF),e(En,yd),e(yd,mF),e(En,gF),e(rt,_F),w(mo,rt,null),e(rt,TF),e(rt,$d),e($d,FF),e(rt,vF),w(Nr,rt,null),h(i,eu,f),h(i,Mn,f),e(Mn,go),e(go,Ed),w(Br,Ed,null),e(Mn,kF),e(Mn,Md),e(Md,bF),h(i,tu,f),h(i,Le,f),w(Wr,Le,null),e(Le,wF),e(Le,zd),e(zd,yF),e(Le,$F),e(Le,Qr),e(Qr,EF),e(Qr,Rr),e(Rr,MF),e(Qr,zF),e(Le,qF),e(Le,Hr),e(Hr,PF),e(Hr,Ei),e(Ei,CF),e(Hr,xF),e(Le,jF),e(Le,Vr),e(Vr,LF),e(Vr,Yr),e(Yr,AF),e(Vr,DF),e(Le,IF),w(_o,Le,null),e(Le,OF),e(Le,at),w(Ur,at,null),e(at,SF),e(at,zn),e(zn,NF),e(zn,Mi),e(Mi,BF),e(zn,WF),e(zn,qd),e(qd,QF),e(zn,RF),e(at,HF),w(To,at,null),e(at,VF),e(at,Pd),e(Pd,YF),e(at,UF),w(Gr,at,null),h(i,nu,f),h(i,qn,f),e(qn,Fo),e(Fo,Cd),w(Zr,Cd,null),e(qn,GF),e(qn,xd),e(xd,ZF),h(i,ou,f),h(i,Ae,f),w(Kr,Ae,null),e(Ae,KF),e(Ae,Xr),e(Xr,XF),e(Xr,jd),e(jd,JF),e(Xr,ev),e(Ae,tv),e(Ae,Jr),e(Jr,nv),e(Jr,ea),e(ea,ov),e(Jr,sv),e(Ae,rv),e(Ae,ta),e(ta,av),e(ta,zi),e(zi,iv),e(ta,lv),e(Ae,dv),e(Ae,na),e(na,cv),e(na,oa),e(oa,uv),e(na,pv),e(Ae,hv),w(vo,Ae,null),e(Ae,fv),e(Ae,it),w(sa,it,null),e(it,mv),e(it,Pn),e(Pn,gv),e(Pn,qi),e(qi,_v),e(Pn,Tv),e(Pn,Ld),e(Ld,Fv),e(Pn,vv),e(it,kv),w(ko,it,null),e(it,bv),e(it,Ad),e(Ad,wv),e(it,yv),w(ra,it,null),h(i,su,f),h(i,Cn,f),e(Cn,bo),e(bo,Dd),w(aa,Dd,null),e(Cn,$v),e(Cn,Id),e(Id,Ev),h(i,ru,f),h(i,De,f),w(ia,De,null),e(De,Mv),e(De,Od),e(Od,zv),e(De,qv),e(De,la),e(la,Pv),e(la,da),e(da,Cv),e(la,xv),e(De,jv),e(De,ca),e(ca,Lv),e(ca,Pi),e(Pi,Av),e(ca,Dv),e(De,Iv),e(De,ua),e(ua,Ov),e(ua,pa),e(pa,Sv),e(ua,Nv),e(De,Bv),w(wo,De,null),e(De,Wv),e(De,lt),w(ha,lt,null),e(lt,Qv),e(lt,xn),e(xn,Rv),e(xn,Ci),e(Ci,Hv),e(xn,Vv),e(xn,Sd),e(Sd,Yv),e(xn,Uv),e(lt,Gv),w(yo,lt,null),e(lt,Zv),e(lt,Nd),e(Nd,Kv),e(lt,Xv),w(fa,lt,null),h(i,au,f),h(i,jn,f),e(jn,$o),e($o,Bd),w(ma,Bd,null),e(jn,Jv),e(jn,Wd),e(Wd,ek),h(i,iu,f),h(i,Ie,f),w(ga,Ie,null),e(Ie,tk),e(Ie,Qd),e(Qd,nk),e(Ie,ok),e(Ie,_a),e(_a,sk),e(_a,Ta),e(Ta,rk),e(_a,ak),e(Ie,ik),e(Ie,Fa),e(Fa,lk),e(Fa,xi),e(xi,dk),e(Fa,ck),e(Ie,uk),e(Ie,va),e(va,pk),e(va,ka),e(ka,hk),e(va,fk),e(Ie,mk),w(Eo,Ie,null),e(Ie,gk),e(Ie,dt),w(ba,dt,null),e(dt,_k),e(dt,Ln),e(Ln,Tk),e(Ln,ji),e(ji,Fk),e(Ln,vk),e(Ln,Rd),e(Rd,kk),e(Ln,bk),e(dt,wk),w(Mo,dt,null),e(dt,yk),e(dt,Hd),e(Hd,$k),e(dt,Ek),w(wa,dt,null),h(i,lu,f),h(i,An,f),e(An,zo),e(zo,Vd),w(ya,Vd,null),e(An,Mk),e(An,Yd),e(Yd,zk),h(i,du,f),h(i,Oe,f),w($a,Oe,null),e(Oe,qk),e(Oe,Ud),e(Ud,Pk),e(Oe,Ck),e(Oe,Ea),e(Ea,xk),e(Ea,Ma),e(Ma,jk),e(Ea,Lk),e(Oe,Ak),e(Oe,za),e(za,Dk),e(za,Li),e(Li,Ik),e(za,Ok),e(Oe,Sk),e(Oe,qa),e(qa,Nk),e(qa,Pa),e(Pa,Bk),e(qa,Wk),e(Oe,Qk),w(qo,Oe,null),e(Oe,Rk),e(Oe,ct),w(Ca,ct,null),e(ct,Hk),e(ct,Dn),e(Dn,Vk),e(Dn,Ai),e(Ai,Yk),e(Dn,Uk),e(Dn,Gd),e(Gd,Gk),e(Dn,Zk),e(ct,Kk),w(Po,ct,null),e(ct,Xk),e(ct,Zd),e(Zd,Jk),e(ct,eb),w(xa,ct,null),h(i,cu,f),h(i,In,f),e(In,Co),e(Co,Kd),w(ja,Kd,null),e(In,tb),e(In,Xd),e(Xd,nb),h(i,uu,f),h(i,Se,f),w(La,Se,null),e(Se,ob),e(Se,On),e(On,sb),e(On,Jd),e(Jd,rb),e(On,ab),e(On,ec),e(ec,ib),e(On,lb),e(Se,db),e(Se,Aa),e(Aa,cb),e(Aa,Da),e(Da,ub),e(Aa,pb),e(Se,hb),e(Se,Ia),e(Ia,fb),e(Ia,Di),e(Di,mb),e(Ia,gb),e(Se,_b),e(Se,Oa),e(Oa,Tb),e(Oa,Sa),e(Sa,Fb),e(Oa,vb),e(Se,kb),w(xo,Se,null),e(Se,bb),e(Se,ut),w(Na,ut,null),e(ut,wb),e(ut,Sn),e(Sn,yb),e(Sn,Ii),e(Ii,$b),e(Sn,Eb),e(Sn,tc),e(tc,Mb),e(Sn,zb),e(ut,qb),w(jo,ut,null),e(ut,Pb),e(ut,nc),e(nc,Cb),e(ut,xb),w(Ba,ut,null),pu=!0},p(i,[f]){const Wa={};f&2&&(Wa.$$scope={dirty:f,ctx:i}),Un.$set(Wa);const oc={};f&2&&(oc.$$scope={dirty:f,ctx:i}),Zn.$set(oc);const sc={};f&2&&(sc.$$scope={dirty:f,ctx:i}),Xn.$set(sc);const rc={};f&2&&(rc.$$scope={dirty:f,ctx:i}),eo.$set(rc);const Qa={};f&2&&(Qa.$$scope={dirty:f,ctx:i}),no.$set(Qa);const ac={};f&2&&(ac.$$scope={dirty:f,ctx:i}),so.$set(ac);const ic={};f&2&&(ic.$$scope={dirty:f,ctx:i}),ao.$set(ic);const lc={};f&2&&(lc.$$scope={dirty:f,ctx:i}),lo.$set(lc);const Ra={};f&2&&(Ra.$$scope={dirty:f,ctx:i}),uo.$set(Ra);const dc={};f&2&&(dc.$$scope={dirty:f,ctx:i}),po.$set(dc);const cc={};f&2&&(cc.$$scope={dirty:f,ctx:i}),fo.$set(cc);const uc={};f&2&&(uc.$$scope={dirty:f,ctx:i}),mo.$set(uc);const pc={};f&2&&(pc.$$scope={dirty:f,ctx:i}),_o.$set(pc);const hc={};f&2&&(hc.$$scope={dirty:f,ctx:i}),To.$set(hc);const Ha={};f&2&&(Ha.$$scope={dirty:f,ctx:i}),vo.$set(Ha);const fc={};f&2&&(fc.$$scope={dirty:f,ctx:i}),ko.$set(fc);const Ce={};f&2&&(Ce.$$scope={dirty:f,ctx:i}),wo.$set(Ce);const mc={};f&2&&(mc.$$scope={dirty:f,ctx:i}),yo.$set(mc);const gc={};f&2&&(gc.$$scope={dirty:f,ctx:i}),Eo.$set(gc);const _c={};f&2&&(_c.$$scope={dirty:f,ctx:i}),Mo.$set(_c);const Tc={};f&2&&(Tc.$$scope={dirty:f,ctx:i}),qo.$set(Tc);const Fc={};f&2&&(Fc.$$scope={dirty:f,ctx:i}),Po.$set(Fc);const vc={};f&2&&(vc.$$scope={dirty:f,ctx:i}),xo.$set(vc);const kc={};f&2&&(kc.$$scope={dirty:f,ctx:i}),jo.$set(kc)},i(i){pu||(y(T.$$.fragment,i),y(te.$$.fragment,i),y(So.$$.fragment,i),y(No.$$.fragment,i),y(Wo.$$.fragment,i),y(Qo.$$.fragment,i),y(Ho.$$.fragment,i),y(Yo.$$.fragment,i),y(Go.$$.fragment,i),y(Zo.$$.fragment,i),y(Ko.$$.fragment,i),y(Xo.$$.fragment,i),y(ts.$$.fragment,i),y(ns.$$.fragment,i),y(os.$$.fragment,i),y(ss.$$.fragment,i),y(as.$$.fragment,i),y(ls.$$.fragment,i),y(ds.$$.fragment,i),y(ms.$$.fragment,i),y(Un.$$.fragment,i),y(gs.$$.fragment,i),y(_s.$$.fragment,i),y(Ts.$$.fragment,i),y(ys.$$.fragment,i),y(Zn.$$.fragment,i),y($s.$$.fragment,i),y(Es.$$.fragment,i),y(zs.$$.fragment,i),y(Xn.$$.fragment,i),y(qs.$$.fragment,i),y(Ps.$$.fragment,i),y(Cs.$$.fragment,i),y(Os.$$.fragment,i),y(eo.$$.fragment,i),y(Ss.$$.fragment,i),y(Ns.$$.fragment,i),y(Bs.$$.fragment,i),y(Ys.$$.fragment,i),y(no.$$.fragment,i),y(Us.$$.fragment,i),y(Gs.$$.fragment,i),y(Zs.$$.fragment,i),y(Ks.$$.fragment,i),y(or.$$.fragment,i),y(so.$$.fragment,i),y(sr.$$.fragment,i),y(rr.$$.fragment,i),y(ar.$$.fragment,i),y(pr.$$.fragment,i),y(ao.$$.fragment,i),y(hr.$$.fragment,i),y(fr.$$.fragment,i),y(mr.$$.fragment,i),y(kr.$$.fragment,i),y(lo.$$.fragment,i),y(br.$$.fragment,i),y(wr.$$.fragment,i),y(yr.$$.fragment,i),y(uo.$$.fragment,i),y(Pr.$$.fragment,i),y(po.$$.fragment,i),y(Cr.$$.fragment,i),y(xr.$$.fragment,i),y(jr.$$.fragment,i),y(fo.$$.fragment,i),y(Sr.$$.fragment,i),y(mo.$$.fragment,i),y(Nr.$$.fragment,i),y(Br.$$.fragment,i),y(Wr.$$.fragment,i),y(_o.$$.fragment,i),y(Ur.$$.fragment,i),y(To.$$.fragment,i),y(Gr.$$.fragment,i),y(Zr.$$.fragment,i),y(Kr.$$.fragment,i),y(vo.$$.fragment,i),y(sa.$$.fragment,i),y(ko.$$.fragment,i),y(ra.$$.fragment,i),y(aa.$$.fragment,i),y(ia.$$.fragment,i),y(wo.$$.fragment,i),y(ha.$$.fragment,i),y(yo.$$.fragment,i),y(fa.$$.fragment,i),y(ma.$$.fragment,i),y(ga.$$.fragment,i),y(Eo.$$.fragment,i),y(ba.$$.fragment,i),y(Mo.$$.fragment,i),y(wa.$$.fragment,i),y(ya.$$.fragment,i),y($a.$$.fragment,i),y(qo.$$.fragment,i),y(Ca.$$.fragment,i),y(Po.$$.fragment,i),y(xa.$$.fragment,i),y(ja.$$.fragment,i),y(La.$$.fragment,i),y(xo.$$.fragment,i),y(Na.$$.fragment,i),y(jo.$$.fragment,i),y(Ba.$$.fragment,i),pu=!0)},o(i){$(T.$$.fragment,i),$(te.$$.fragment,i),$(So.$$.fragment,i),$(No.$$.fragment,i),$(Wo.$$.fragment,i),$(Qo.$$.fragment,i),$(Ho.$$.fragment,i),$(Yo.$$.fragment,i),$(Go.$$.fragment,i),$(Zo.$$.fragment,i),$(Ko.$$.fragment,i),$(Xo.$$.fragment,i),$(ts.$$.fragment,i),$(ns.$$.fragment,i),$(os.$$.fragment,i),$(ss.$$.fragment,i),$(as.$$.fragment,i),$(ls.$$.fragment,i),$(ds.$$.fragment,i),$(ms.$$.fragment,i),$(Un.$$.fragment,i),$(gs.$$.fragment,i),$(_s.$$.fragment,i),$(Ts.$$.fragment,i),$(ys.$$.fragment,i),$(Zn.$$.fragment,i),$($s.$$.fragment,i),$(Es.$$.fragment,i),$(zs.$$.fragment,i),$(Xn.$$.fragment,i),$(qs.$$.fragment,i),$(Ps.$$.fragment,i),$(Cs.$$.fragment,i),$(Os.$$.fragment,i),$(eo.$$.fragment,i),$(Ss.$$.fragment,i),$(Ns.$$.fragment,i),$(Bs.$$.fragment,i),$(Ys.$$.fragment,i),$(no.$$.fragment,i),$(Us.$$.fragment,i),$(Gs.$$.fragment,i),$(Zs.$$.fragment,i),$(Ks.$$.fragment,i),$(or.$$.fragment,i),$(so.$$.fragment,i),$(sr.$$.fragment,i),$(rr.$$.fragment,i),$(ar.$$.fragment,i),$(pr.$$.fragment,i),$(ao.$$.fragment,i),$(hr.$$.fragment,i),$(fr.$$.fragment,i),$(mr.$$.fragment,i),$(kr.$$.fragment,i),$(lo.$$.fragment,i),$(br.$$.fragment,i),$(wr.$$.fragment,i),$(yr.$$.fragment,i),$(uo.$$.fragment,i),$(Pr.$$.fragment,i),$(po.$$.fragment,i),$(Cr.$$.fragment,i),$(xr.$$.fragment,i),$(jr.$$.fragment,i),$(fo.$$.fragment,i),$(Sr.$$.fragment,i),$(mo.$$.fragment,i),$(Nr.$$.fragment,i),$(Br.$$.fragment,i),$(Wr.$$.fragment,i),$(_o.$$.fragment,i),$(Ur.$$.fragment,i),$(To.$$.fragment,i),$(Gr.$$.fragment,i),$(Zr.$$.fragment,i),$(Kr.$$.fragment,i),$(vo.$$.fragment,i),$(sa.$$.fragment,i),$(ko.$$.fragment,i),$(ra.$$.fragment,i),$(aa.$$.fragment,i),$(ia.$$.fragment,i),$(wo.$$.fragment,i),$(ha.$$.fragment,i),$(yo.$$.fragment,i),$(fa.$$.fragment,i),$(ma.$$.fragment,i),$(ga.$$.fragment,i),$(Eo.$$.fragment,i),$(ba.$$.fragment,i),$(Mo.$$.fragment,i),$(wa.$$.fragment,i),$(ya.$$.fragment,i),$($a.$$.fragment,i),$(qo.$$.fragment,i),$(Ca.$$.fragment,i),$(Po.$$.fragment,i),$(xa.$$.fragment,i),$(ja.$$.fragment,i),$(La.$$.fragment,i),$(xo.$$.fragment,i),$(Na.$$.fragment,i),$(jo.$$.fragment,i),$(Ba.$$.fragment,i),pu=!1},d(i){t(p),i&&t(M),i&&t(m),E(T),i&&t(G),i&&t(q),E(te),i&&t(ie),i&&t(U),i&&t(x),i&&t(oe),i&&t(le),i&&t(se),i&&t(de),i&&t(C),i&&t(B),i&&t(J),i&&t(yc),i&&t(xt),i&&t($c),i&&t(Zt),E(So),i&&t(Ec),i&&t(Pt),E(No),i&&t(Mc),i&&t(Xt),E(Wo),i&&t(zc),i&&t(Pe),E(Qo),E(Ho),E(Yo),E(Go),E(Zo),i&&t(qc),i&&t(en),E(Ko),i&&t(Pc),i&&t(Ze),E(Xo),E(ts),E(ns),i&&t(Cc),i&&t(nn),E(os),i&&t(xc),i&&t(on),E(ss),i&&t(jc),i&&t(sn),E(as),i&&t(Lc),i&&t(rn),E(ls),i&&t(Ac),i&&t(We),E(ds),E(ms),E(Un),E(gs),i&&t(Dc),i&&t(ln),E(_s),i&&t(Ic),i&&t(Qe),E(Ts),E(ys),E(Zn),E($s),i&&t(Oc),i&&t(cn),E(Es),i&&t(Sc),i&&t(Ms),E(zs),E(Xn),E(qs),i&&t(Nc),i&&t(pn),E(Ps),i&&t(Bc),i&&t(Re),E(Cs),E(Os),E(eo),E(Ss),i&&t(Wc),i&&t(fn),E(Ns),i&&t(Qc),i&&t(He),E(Bs),E(Ys),E(no),E(Us),E(Gs),i&&t(Rc),i&&t(gn),E(Zs),i&&t(Hc),i&&t(Ve),E(Ks),E(or),E(so),E(sr),i&&t(Vc),i&&t(Tn),E(rr),i&&t(Yc),i&&t(Ye),E(ar),E(pr),E(ao),E(hr),i&&t(Uc),i&&t(vn),E(fr),i&&t(Gc),i&&t(Ue),E(mr),E(kr),E(lo),E(br),i&&t(Zc),i&&t(wn),E(wr),i&&t(Kc),i&&t(xe),E(yr),E(uo),E(Pr),E(po),E(Cr),i&&t(Xc),i&&t($n),E(xr),i&&t(Jc),i&&t(je),E(jr),E(fo),E(Sr),E(mo),E(Nr),i&&t(eu),i&&t(Mn),E(Br),i&&t(tu),i&&t(Le),E(Wr),E(_o),E(Ur),E(To),E(Gr),i&&t(nu),i&&t(qn),E(Zr),i&&t(ou),i&&t(Ae),E(Kr),E(vo),E(sa),E(ko),E(ra),i&&t(su),i&&t(Cn),E(aa),i&&t(ru),i&&t(De),E(ia),E(wo),E(ha),E(yo),E(fa),i&&t(au),i&&t(jn),E(ma),i&&t(iu),i&&t(Ie),E(ga),E(Eo),E(ba),E(Mo),E(wa),i&&t(lu),i&&t(An),E(ya),i&&t(du),i&&t(Oe),E($a),E(qo),E(Ca),E(Po),E(xa),i&&t(cu),i&&t(In),E(ja),i&&t(uu),i&&t(Se),E(La),E(xo),E(Na),E(jo),E(Ba)}}}const U$={local:"funnel-transformer",sections:[{local:"overview",title:"Overview"},{local:"transformers.FunnelConfig",title:"FunnelConfig"},{local:"transformers.FunnelTokenizer",title:"FunnelTokenizer"},{local:"transformers.FunnelTokenizerFast",title:"FunnelTokenizerFast"},{local:"transformers.models.funnel.modeling_funnel.FunnelForPreTrainingOutput",title:"Funnel specific outputs"},{local:"transformers.FunnelBaseModel",title:"FunnelBaseModel"},{local:"transformers.FunnelModel",title:"FunnelModel"},{local:"transformers.FunnelForPreTraining",title:"FunnelModelForPreTraining"},{local:"transformers.FunnelForMaskedLM",title:"FunnelForMaskedLM"},{local:"transformers.FunnelForSequenceClassification",title:"FunnelForSequenceClassification"},{local:"transformers.FunnelForMultipleChoice",title:"FunnelForMultipleChoice"},{local:"transformers.FunnelForTokenClassification",title:"FunnelForTokenClassification"},{local:"transformers.FunnelForQuestionAnswering",title:"FunnelForQuestionAnswering"},{local:"transformers.TFFunnelBaseModel",title:"TFFunnelBaseModel"},{local:"transformers.TFFunnelModel",title:"TFFunnelModel"},{local:"transformers.TFFunnelForPreTraining",title:"TFFunnelModelForPreTraining"},{local:"transformers.TFFunnelForMaskedLM",title:"TFFunnelForMaskedLM"},{local:"transformers.TFFunnelForSequenceClassification",title:"TFFunnelForSequenceClassification"},{local:"transformers.TFFunnelForMultipleChoice",title:"TFFunnelForMultipleChoice"},{local:"transformers.TFFunnelForTokenClassification",title:"TFFunnelForTokenClassification"},{local:"transformers.TFFunnelForQuestionAnswering",title:"TFFunnelForQuestionAnswering"}],title:"Funnel Transformer"};function G$(W,p,M){let{fw:m}=p;return W.$$set=g=>{"fw"in g&&M(0,m=g.fw)},[m]}class n2 extends F${constructor(p){super();v$(this,p,G$,Y$,k$,{fw:0})}}export{n2 as default,U$ as metadata};
