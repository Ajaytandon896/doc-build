import{S as Yf,i as Vf,s as Zf,e as l,k as f,w as d,t as a,L as Xf,c as n,d as s,m as u,a as i,x as _,h as o,b as h,J as t,g as p,y as g,q as v,o as y,B as $}from"../../chunks/vendor-9e2b328e.js";import{T as ka}from"../../chunks/Tip-76f97a76.js";import{Y as Jf}from"../../chunks/Youtube-cb4469d8.js";import{I as De}from"../../chunks/IconCopyLink-fd0e58fd.js";import{C as R}from"../../chunks/CodeBlock-b9ff96e9.js";import{C as fe}from"../../chunks/CodeBlockFw-0c398fb3.js";import{D as eu}from"../../chunks/DocNotebookDropdown-112a30e3.js";import"../../chunks/CopyButton-4b97cbf7.js";function tu(I){let m,k;return{c(){m=l("p"),k=a(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(c){m=n(c,"P",{});var w=i(m);k=o(w,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),w.forEach(s)},m(c,w){p(c,m,w),t(m,k)},d(c){c&&s(m)}}}function su(I){let m,k,c,w,A,b,E,q;return{c(){m=l("p"),k=a("For more details about the "),c=l("a"),w=a("pipeline()"),A=a(" and associated tasks, refer to the documentation "),b=l("a"),E=a("here"),q=a("."),this.h()},l(S){m=n(S,"P",{});var j=i(m);k=o(j,"For more details about the "),c=n(j,"A",{href:!0});var D=i(c);w=o(D,"pipeline()"),D.forEach(s),A=o(j," and associated tasks, refer to the documentation "),b=n(j,"A",{href:!0});var U=i(b);E=o(U,"here"),U.forEach(s),q=o(j,"."),j.forEach(s),this.h()},h(){h(c,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(b,"href","./main_classes/pipelines")},m(S,j){p(S,m,j),t(m,k),t(m,c),t(c,w),t(m,A),t(m,b),t(b,E),t(m,q)},d(S){S&&s(m)}}}function au(I){let m,k,c,w,A,b,E,q;return{c(){m=l("p"),k=a("See the "),c=l("a"),w=a("task summary"),A=a(" for which "),b=l("a"),E=a("AutoModel"),q=a(" class to use for which task."),this.h()},l(S){m=n(S,"P",{});var j=i(m);k=o(j,"See the "),c=n(j,"A",{href:!0});var D=i(c);w=o(D,"task summary"),D.forEach(s),A=o(j," for which "),b=n(j,"A",{href:!0});var U=i(b);E=o(U,"AutoModel"),U.forEach(s),q=o(j," class to use for which task."),j.forEach(s),this.h()},h(){h(c,"href","./task_summary"),h(b,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.AutoModel")},m(S,j){p(S,m,j),t(m,k),t(m,c),t(c,w),t(m,A),t(m,b),t(b,E),t(m,q)},d(S){S&&s(m)}}}function ou(I){let m,k,c,w,A;return{c(){m=l("p"),k=a("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=l("em"),w=a("before"),A=a(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(b){m=n(b,"P",{});var E=i(m);k=o(E,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=n(E,"EM",{});var q=i(c);w=o(q,"before"),q.forEach(s),A=o(E,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),E.forEach(s)},m(b,E){p(b,m,E),t(m,k),t(m,c),t(c,w),t(m,A)},d(b){b&&s(m)}}}function ru(I){let m,k,c,w,A;return{c(){m=l("p"),k=a(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=l("code"),w=a("None"),A=a(" are ignored.")},l(b){m=n(b,"P",{});var E=i(m);k=o(E,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=n(E,"CODE",{});var q=i(c);w=o(q,"None"),q.forEach(s),A=o(E," are ignored."),E.forEach(s)},m(b,E){p(b,m,E),t(m,k),t(m,c),t(c,w),t(m,A)},d(b){b&&s(m)}}}function lu(I){let m,k,c,w,A,b,E,q,S,j,D,U,W,ur,qt,hr,mr,zt,cr,dr,Ea,ue,ja,se,he,As,Oe,_r,Ts,gr,Aa,Le,xt,vr,yr,Ta,Re,qa,me,$r,Ft,br,wr,za,Ue,qs,kr,Er,xa,T,zs,jr,Ar,xs,Tr,qr,Fs,zr,xr,Ps,Fr,Pr,Ss,Sr,Cr,Cs,Mr,Nr,Ms,Ir,Dr,Ns,Or,Fa,We,Is,Lr,Rr,Pa,H,Ds,Ur,Wr,Os,Hr,Br,Ls,Kr,Sa,He,Rs,Gr,Qr,Ca,ce,Us,Jr,Yr,Ws,Vr,Ma,de,Na,ae,_e,Hs,Be,Zr,Bs,Xr,Ia,ge,el,Pt,tl,sl,Da,St,al,Oa,Ke,La,ve,ol,Ct,rl,ll,Ra,Ge,Ua,B,nl,Qe,il,pl,Ks,fl,ul,Wa,Je,Ha,ye,hl,Mt,ml,cl,Ba,Ye,Ka,K,dl,Nt,_l,gl,Ve,vl,yl,Ga,Ze,Qa,O,$l,It,bl,wl,Gs,kl,El,Qs,jl,Al,Ja,Xe,Ya,G,Tl,et,ql,zl,tt,xl,Fl,Va,st,Za,$e,Pl,Js,Sl,Cl,Xa,at,eo,oe,be,Ys,ot,Ml,Vs,Nl,to,C,Il,Dt,Dl,Ol,rt,Ll,Rl,Ot,Ul,Wl,lt,Hl,Bl,so,nt,ao,Q,Kl,Lt,Gl,Ql,Zs,Jl,Yl,oo,it,ro,J,Vl,Rt,Zl,Xl,Xs,en,tn,lo,pt,no,Y,sn,Ut,an,on,Wt,rn,ln,io,re,we,ea,ft,nn,ta,pn,po,ut,fo,z,fn,Ht,un,hn,Bt,mn,cn,Kt,dn,_n,Gt,gn,vn,sa,yn,$n,Qt,bn,wn,uo,V,kn,aa,En,jn,Jt,An,Tn,ho,le,ke,oa,ht,qn,ra,zn,mo,Z,xn,la,Fn,Pn,Yt,Sn,Cn,co,Ee,Mn,Vt,Nn,In,_o,mt,go,je,Dn,na,On,Ln,vo,Zt,Rn,yo,ct,$o,Xt,Un,bo,Ae,es,ts,Wn,Hn,Bn,ss,as,Kn,Gn,wo,Te,Qn,os,Jn,Yn,ko,dt,Eo,qe,Vn,rs,Zn,Xn,jo,ne,ze,ia,_t,ei,pa,ti,Ao,F,si,ls,ai,oi,ns,ri,li,is,ni,ii,ps,pi,fi,fs,ui,hi,To,gt,qo,xe,zo,Fe,mi,fa,ci,di,xo,vt,Fo,X,_i,ua,gi,vi,ha,yi,$i,Po,yt,So,Pe,Co,x,bi,$t,ma,wi,ki,bt,ca,Ei,ji,us,Ai,Ti,da,qi,zi,wt,xi,Fi,hs,Pi,Si,Mo,Se,No,ie,Ce,_a,kt,Ci,ga,Mi,Io,Me,Ni,ms,Ii,Di,Do,Et,Oo,Ne,Oi,cs,Li,Ri,Lo,jt,Ro,ee,Ui,va,Wi,Hi,ya,Bi,Ki,Uo,At,Wo;return b=new De({}),D=new eu({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"}]}}),ue=new ka({props:{$$slots:{default:[tu]},$$scope:{ctx:I}}}),Oe=new De({}),Re=new Jf({props:{id:"tiZFewofSLM"}}),de=new ka({props:{$$slots:{default:[su]},$$scope:{ctx:I}}}),Be=new De({}),Ke=new fe({props:{pt:{code:"pip install torch",highlighted:"pip install torch"},tf:{code:"pip install tensorflow",highlighted:"pip install tensorflow"}}}),Ge=new R({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),Je=new R({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library."),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.9998</span>}]`}}),Ye=new R({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),Ze=new R({props:{code:"pip install datasets ,",highlighted:"pip install datasets "}}),Xe=new R({props:{code:`from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, device=<span class="hljs-number">0</span>)`}}),st=new R({props:{code:`import datasets

dataset = datasets.load_dataset("superb", name="asr", split="test"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = datasets.load_dataset(<span class="hljs-string">&quot;superb&quot;</span>, name=<span class="hljs-string">&quot;asr&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)`}}),at=new R({props:{code:`from transformers.pipelines.pt_utils import KeyDataset
from tqdm.auto import tqdm

for out in tqdm(speech_recognizer(KeyDataset(dataset, "file"))):
    print(out),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.pipelines.pt_utils <span class="hljs-keyword">import</span> KeyDataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> out <span class="hljs-keyword">in</span> tqdm(speech_recognizer(KeyDataset(dataset, <span class="hljs-string">&quot;file&quot;</span>))):
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(out)
{<span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FAT AND SAUCE&quot;</span>}`}}),ot=new De({}),nt=new R({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment",',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),it=new fe({props:{pt:{code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`},tf:{code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}}),pt=new R({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers."),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-string">&quot;5 stars&quot;</span>, <span class="hljs-string">&quot;score&quot;</span>: <span class="hljs-number">0.7272651791572571</span>}]`}}),ft=new De({}),ut=new Jf({props:{id:"AhChOFRegn4"}}),ht=new De({}),mt=new R({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),ct=new R({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&quot;input_ids&quot;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">2024</span>, <span class="hljs-number">2200</span>, <span class="hljs-number">3407</span>, <span class="hljs-number">2000</span>, <span class="hljs-number">2265</span>, <span class="hljs-number">2017</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">100</span>, <span class="hljs-number">19081</span>, <span class="hljs-number">3075</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&quot;attention_mask&quot;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),dt=new fe({props:{pt:{code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`},tf:{code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}}),_t=new De({}),gt=new fe({props:{pt:{code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`},tf:{code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}}),xe=new ka({props:{$$slots:{default:[au]},$$scope:{ctx:I}}}),vt=new fe({props:{pt:{code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'},tf:{code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}}),yt=new fe({props:{pt:{code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">2.2043e-04</span>, <span class="hljs-number">9.9978e-01</span>],
        [<span class="hljs-number">5.3086e-01</span>, <span class="hljs-number">4.6914e-01</span>]], grad_fn=&lt;SoftmaxBackward&gt;)`},tf:{code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
print(tf_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf_predictions)
tf.Tensor(
[[<span class="hljs-number">2.2043e-04</span> <span class="hljs-number">9.9978e-01</span>]
 [<span class="hljs-number">5.3086e-01</span> <span class="hljs-number">4.6914e-01</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), dtype=float32)`}}}),Pe=new ka({props:{$$slots:{default:[ou]},$$scope:{ctx:I}}}),Se=new ka({props:{$$slots:{default:[ru]},$$scope:{ctx:I}}}),kt=new De({}),Et=new fe({props:{pt:{code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`},tf:{code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}}),jt=new fe({props:{pt:{code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'},tf:{code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}}),At=new fe({props:{pt:{code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`},tf:{code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}}),{c(){m=l("meta"),k=f(),c=l("h1"),w=l("a"),A=l("span"),d(b.$$.fragment),E=f(),q=l("span"),S=a("Quick tour"),j=f(),d(D.$$.fragment),U=f(),W=l("p"),ur=a("Get up and running with \u{1F917} Transformers! Start using the "),qt=l("a"),hr=a("pipeline()"),mr=a(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),zt=l("a"),cr=a("AutoClass"),dr=a(" to solve your text, vision or audio task."),Ea=f(),d(ue.$$.fragment),ja=f(),se=l("h2"),he=l("a"),As=l("span"),d(Oe.$$.fragment),_r=f(),Ts=l("span"),gr=a("Pipeline"),Aa=f(),Le=l("p"),xt=l("a"),vr=a("pipeline()"),yr=a(" is the easiest way to use a pretrained model for a given task."),Ta=f(),d(Re.$$.fragment),qa=f(),me=l("p"),$r=a("The "),Ft=l("a"),br=a("pipeline()"),wr=a(" supports many common tasks out-of-the-box:"),za=f(),Ue=l("p"),qs=l("strong"),kr=a("Text"),Er=a(":"),xa=f(),T=l("ul"),zs=l("li"),jr=a("Sentiment analysis: classify the polarity of a given text."),Ar=f(),xs=l("li"),Tr=a("Text generation (in English): generate text from a given input."),qr=f(),Fs=l("li"),zr=a("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),xr=f(),Ps=l("li"),Fr=a("Question answering: extract the answer from the context, given some context and a question."),Pr=f(),Ss=l("li"),Sr=a("Fill-mask: fill in the blank given a text with masked words."),Cr=f(),Cs=l("li"),Mr=a("Summarization: generate a summary of a long sequence of text or document."),Nr=f(),Ms=l("li"),Ir=a("Translation: translate text into another language."),Dr=f(),Ns=l("li"),Or=a("Feature extraction: create a tensor representation of the text."),Fa=f(),We=l("p"),Is=l("strong"),Lr=a("Image"),Rr=a(":"),Pa=f(),H=l("ul"),Ds=l("li"),Ur=a("Image classification: classify an image."),Wr=f(),Os=l("li"),Hr=a("Image segmentation: classify every pixel in an image."),Br=f(),Ls=l("li"),Kr=a("Object detection: detect objects within an image."),Sa=f(),He=l("p"),Rs=l("strong"),Gr=a("Audio"),Qr=a(":"),Ca=f(),ce=l("ul"),Us=l("li"),Jr=a("Audio classification: assign a label to a given segment of audio."),Yr=f(),Ws=l("li"),Vr=a("Automatic speech recognition (ASR): transcribe audio data into text."),Ma=f(),d(de.$$.fragment),Na=f(),ae=l("h3"),_e=l("a"),Hs=l("span"),d(Be.$$.fragment),Zr=f(),Bs=l("span"),Xr=a("Pipeline usage"),Ia=f(),ge=l("p"),el=a("In the following example, you will use the "),Pt=l("a"),tl=a("pipeline()"),sl=a(" for sentiment analysis."),Da=f(),St=l("p"),al=a("Install the following dependencies if you haven\u2019t already:"),Oa=f(),d(Ke.$$.fragment),La=f(),ve=l("p"),ol=a("Import "),Ct=l("a"),rl=a("pipeline()"),ll=a(" and specify the task you want to complete:"),Ra=f(),d(Ge.$$.fragment),Ua=f(),B=l("p"),nl=a("The pipeline downloads and caches a default "),Qe=l("a"),il=a("pretrained model"),pl=a(" and tokenizer for sentiment analysis. Now you can use the "),Ks=l("code"),fl=a("classifier"),ul=a(" on your target text:"),Wa=f(),d(Je.$$.fragment),Ha=f(),ye=l("p"),hl=a("For more than one sentence, pass a list of sentences to the "),Mt=l("a"),ml=a("pipeline()"),cl=a(" which returns a list of dictionaries:"),Ba=f(),d(Ye.$$.fragment),Ka=f(),K=l("p"),dl=a("The "),Nt=l("a"),_l=a("pipeline()"),gl=a(" can also iterate over an entire dataset. Start by installing the "),Ve=l("a"),vl=a("\u{1F917} Datasets"),yl=a(" library:"),Ga=f(),d(Ze.$$.fragment),Qa=f(),O=l("p"),$l=a("Create a "),It=l("a"),bl=a("pipeline()"),wl=a(" with the task you want to solve for and the model you want to use. Set the "),Gs=l("code"),kl=a("device"),El=a(" parameter to "),Qs=l("code"),jl=a("0"),Al=a(" to place the tensors on a CUDA device:"),Ja=f(),d(Xe.$$.fragment),Ya=f(),G=l("p"),Tl=a("Next, load a dataset (see the \u{1F917} Datasets "),et=l("a"),ql=a("Quick Start"),zl=a(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),tt=l("a"),xl=a("SUPERB"),Fl=a(" dataset:"),Va=f(),d(st.$$.fragment),Za=f(),$e=l("p"),Pl=a("Now you can iterate over the dataset with the pipeline. "),Js=l("code"),Sl=a("KeyDataset"),Cl=a(" retrieves the item in the dictionary returned by the dataset:"),Xa=f(),d(at.$$.fragment),eo=f(),oe=l("h3"),be=l("a"),Ys=l("span"),d(ot.$$.fragment),Ml=f(),Vs=l("span"),Nl=a("Use another model and tokenizer in the pipeline"),to=f(),C=l("p"),Il=a("The "),Dt=l("a"),Dl=a("pipeline()"),Ol=a(" can accommodate any model from the "),rt=l("a"),Ll=a("Model Hub"),Rl=a(", making it easy to adapt the "),Ot=l("a"),Ul=a("pipeline()"),Wl=a(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),lt=l("a"),Hl=a("BERT model"),Bl=a(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),so=f(),d(nt.$$.fragment),ao=f(),Q=l("p"),Kl=a("Use the "),Lt=l("a"),Gl=a("AutoModelForSequenceClassification"),Ql=a(" and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Zs=l("code"),Jl=a("AutoClass"),Yl=a(" below):"),oo=f(),d(it.$$.fragment),ro=f(),J=l("p"),Vl=a("Then you can specify the model and tokenizer in the "),Rt=l("a"),Zl=a("pipeline()"),Xl=a(", and apply the "),Xs=l("code"),en=a("classifier"),tn=a(" on your target text:"),lo=f(),d(pt.$$.fragment),no=f(),Y=l("p"),sn=a("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Ut=l("a"),an=a("fine-tuning tutorial"),on=a(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Wt=l("a"),rn=a("here"),ln=a(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),io=f(),re=l("h2"),we=l("a"),ea=l("span"),d(ft.$$.fragment),nn=f(),ta=l("span"),pn=a("AutoClass"),po=f(),d(ut.$$.fragment),fo=f(),z=l("p"),fn=a("Under the hood, the "),Ht=l("a"),un=a("AutoModelForSequenceClassification"),hn=a(" and "),Bt=l("a"),mn=a("AutoTokenizer"),cn=a(" classes work together to power the "),Kt=l("a"),dn=a("pipeline()"),_n=a(". An "),Gt=l("a"),gn=a("AutoClass"),vn=a(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),sa=l("code"),yn=a("AutoClass"),$n=a(" for your task and it\u2019s associated tokenizer with "),Qt=l("a"),bn=a("AutoTokenizer"),wn=a("."),uo=f(),V=l("p"),kn=a("Let\u2019s return to our example and see how you can use the "),aa=l("code"),En=a("AutoClass"),jn=a(" to replicate the results of the "),Jt=l("a"),An=a("pipeline()"),Tn=a("."),ho=f(),le=l("h3"),ke=l("a"),oa=l("span"),d(ht.$$.fragment),qn=f(),ra=l("span"),zn=a("AutoTokenizer"),mo=f(),Z=l("p"),xn=a("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=l("em"),Fn=a("tokens"),Pn=a(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Yt=l("a"),Sn=a("here"),Cn=a("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),co=f(),Ee=l("p"),Mn=a("Load a tokenizer with "),Vt=l("a"),Nn=a("AutoTokenizer"),In=a(":"),_o=f(),d(mt.$$.fragment),go=f(),je=l("p"),Dn=a("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),na=l("em"),On=a("vocabulary"),Ln=a("."),vo=f(),Zt=l("p"),Rn=a("Pass your text to the tokenizer:"),yo=f(),d(ct.$$.fragment),$o=f(),Xt=l("p"),Un=a("The tokenizer will return a dictionary containing:"),bo=f(),Ae=l("ul"),es=l("li"),ts=l("a"),Wn=a("input_ids"),Hn=a(": numerical representions of your tokens."),Bn=f(),ss=l("li"),as=l("a"),Kn=a("atttention_mask"),Gn=a(": indicates which tokens should be attended to."),wo=f(),Te=l("p"),Qn=a("Just like the "),os=l("a"),Jn=a("pipeline()"),Yn=a(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),ko=f(),d(dt.$$.fragment),Eo=f(),qe=l("p"),Vn=a("Read the "),rs=l("a"),Zn=a("preprocessing"),Xn=a(" tutorial for more details about tokenization."),jo=f(),ne=l("h3"),ze=l("a"),ia=l("span"),d(_t.$$.fragment),ei=f(),pa=l("span"),ti=a("AutoModel"),Ao=f(),F=l("p"),si=a("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),ls=l("a"),ai=a("AutoModel"),oi=a(" like you would load an "),ns=l("a"),ri=a("AutoTokenizer"),li=a(". The only difference is selecting the correct "),is=l("a"),ni=a("AutoModel"),ii=a(" for the task. Since you are doing text - or sequence - classification, load "),ps=l("a"),pi=a("AutoModelForSequenceClassification"),fi=a(". The TensorFlow equivalent is simply "),fs=l("a"),ui=a("TFAutoModelForSequenceClassification"),hi=a(":"),To=f(),d(gt.$$.fragment),qo=f(),d(xe.$$.fragment),zo=f(),Fe=l("p"),mi=a("Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),fa=l("code"),ci=a("**"),di=a(". For TensorFlow models, pass the dictionary keys directly to the tensors:"),xo=f(),d(vt.$$.fragment),Fo=f(),X=l("p"),_i=a("The model outputs the final activations in the "),ua=l("code"),gi=a("logits"),vi=a(" attribute. Apply the softmax function to the "),ha=l("code"),yi=a("logits"),$i=a(" to retrieve the probabilities:"),Po=f(),d(yt.$$.fragment),So=f(),d(Pe.$$.fragment),Co=f(),x=l("p"),bi=a("Models are a standard "),$t=l("a"),ma=l("code"),wi=a("torch.nn.Module"),ki=a(" or a "),bt=l("a"),ca=l("code"),Ei=a("tf.keras.Model"),ji=a(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),us=l("a"),Ai=a("Trainer"),Ti=a(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),da=l("code"),qi=a("fit"),zi=a(" method from "),wt=l("a"),xi=a("Keras"),Fi=a(". Refer to the "),hs=l("a"),Pi=a("training tutorial"),Si=a(" for more details."),Mo=f(),d(Se.$$.fragment),No=f(),ie=l("h3"),Ce=l("a"),_a=l("span"),d(kt.$$.fragment),Ci=f(),ga=l("span"),Mi=a("Save a model"),Io=f(),Me=l("p"),Ni=a("Once your model is fine-tuned, you can save it with its tokenizer using "),ms=l("a"),Ii=a("PreTrainedModel.save_pretrained()"),Di=a(":"),Do=f(),d(Et.$$.fragment),Oo=f(),Ne=l("p"),Oi=a("When you are ready to use the model again, reload it with "),cs=l("a"),Li=a("PreTrainedModel.from_pretrained()"),Ri=a(":"),Lo=f(),d(jt.$$.fragment),Ro=f(),ee=l("p"),Ui=a("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),va=l("code"),Wi=a("from_pt"),Hi=a(" or "),ya=l("code"),Bi=a("from_tf"),Ki=a(" parameter can convert the model from one framework to the other:"),Uo=f(),d(At.$$.fragment),this.h()},l(e){const r=Xf('[data-svelte="svelte-1phssyn"]',document.head);m=n(r,"META",{name:!0,content:!0}),r.forEach(s),k=u(e),c=n(e,"H1",{class:!0});var Tt=i(c);w=n(Tt,"A",{id:!0,class:!0,href:!0});var $a=i(w);A=n($a,"SPAN",{});var ba=i(A);_(b.$$.fragment,ba),ba.forEach(s),$a.forEach(s),E=u(Tt),q=n(Tt,"SPAN",{});var wa=i(q);S=o(wa,"Quick tour"),wa.forEach(s),Tt.forEach(s),j=u(e),_(D.$$.fragment,e),U=u(e),W=n(e,"P",{});var pe=i(W);ur=o(pe,"Get up and running with \u{1F917} Transformers! Start using the "),qt=n(pe,"A",{href:!0});var Xi=i(qt);hr=o(Xi,"pipeline()"),Xi.forEach(s),mr=o(pe," for rapid inference, and quickly load a pretrained model and tokenizer with an "),zt=n(pe,"A",{href:!0});var ep=i(zt);cr=o(ep,"AutoClass"),ep.forEach(s),dr=o(pe," to solve your text, vision or audio task."),pe.forEach(s),Ea=u(e),_(ue.$$.fragment,e),ja=u(e),se=n(e,"H2",{class:!0});var Ho=i(se);he=n(Ho,"A",{id:!0,class:!0,href:!0});var tp=i(he);As=n(tp,"SPAN",{});var sp=i(As);_(Oe.$$.fragment,sp),sp.forEach(s),tp.forEach(s),_r=u(Ho),Ts=n(Ho,"SPAN",{});var ap=i(Ts);gr=o(ap,"Pipeline"),ap.forEach(s),Ho.forEach(s),Aa=u(e),Le=n(e,"P",{});var Gi=i(Le);xt=n(Gi,"A",{href:!0});var op=i(xt);vr=o(op,"pipeline()"),op.forEach(s),yr=o(Gi," is the easiest way to use a pretrained model for a given task."),Gi.forEach(s),Ta=u(e),_(Re.$$.fragment,e),qa=u(e),me=n(e,"P",{});var Bo=i(me);$r=o(Bo,"The "),Ft=n(Bo,"A",{href:!0});var rp=i(Ft);br=o(rp,"pipeline()"),rp.forEach(s),wr=o(Bo," supports many common tasks out-of-the-box:"),Bo.forEach(s),za=u(e),Ue=n(e,"P",{});var Qi=i(Ue);qs=n(Qi,"STRONG",{});var lp=i(qs);kr=o(lp,"Text"),lp.forEach(s),Er=o(Qi,":"),Qi.forEach(s),xa=u(e),T=n(e,"UL",{});var P=i(T);zs=n(P,"LI",{});var np=i(zs);jr=o(np,"Sentiment analysis: classify the polarity of a given text."),np.forEach(s),Ar=u(P),xs=n(P,"LI",{});var ip=i(xs);Tr=o(ip,"Text generation (in English): generate text from a given input."),ip.forEach(s),qr=u(P),Fs=n(P,"LI",{});var pp=i(Fs);zr=o(pp,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),pp.forEach(s),xr=u(P),Ps=n(P,"LI",{});var fp=i(Ps);Fr=o(fp,"Question answering: extract the answer from the context, given some context and a question."),fp.forEach(s),Pr=u(P),Ss=n(P,"LI",{});var up=i(Ss);Sr=o(up,"Fill-mask: fill in the blank given a text with masked words."),up.forEach(s),Cr=u(P),Cs=n(P,"LI",{});var hp=i(Cs);Mr=o(hp,"Summarization: generate a summary of a long sequence of text or document."),hp.forEach(s),Nr=u(P),Ms=n(P,"LI",{});var mp=i(Ms);Ir=o(mp,"Translation: translate text into another language."),mp.forEach(s),Dr=u(P),Ns=n(P,"LI",{});var cp=i(Ns);Or=o(cp,"Feature extraction: create a tensor representation of the text."),cp.forEach(s),P.forEach(s),Fa=u(e),We=n(e,"P",{});var Ji=i(We);Is=n(Ji,"STRONG",{});var dp=i(Is);Lr=o(dp,"Image"),dp.forEach(s),Rr=o(Ji,":"),Ji.forEach(s),Pa=u(e),H=n(e,"UL",{});var ds=i(H);Ds=n(ds,"LI",{});var _p=i(Ds);Ur=o(_p,"Image classification: classify an image."),_p.forEach(s),Wr=u(ds),Os=n(ds,"LI",{});var gp=i(Os);Hr=o(gp,"Image segmentation: classify every pixel in an image."),gp.forEach(s),Br=u(ds),Ls=n(ds,"LI",{});var vp=i(Ls);Kr=o(vp,"Object detection: detect objects within an image."),vp.forEach(s),ds.forEach(s),Sa=u(e),He=n(e,"P",{});var Yi=i(He);Rs=n(Yi,"STRONG",{});var yp=i(Rs);Gr=o(yp,"Audio"),yp.forEach(s),Qr=o(Yi,":"),Yi.forEach(s),Ca=u(e),ce=n(e,"UL",{});var Ko=i(ce);Us=n(Ko,"LI",{});var $p=i(Us);Jr=o($p,"Audio classification: assign a label to a given segment of audio."),$p.forEach(s),Yr=u(Ko),Ws=n(Ko,"LI",{});var bp=i(Ws);Vr=o(bp,"Automatic speech recognition (ASR): transcribe audio data into text."),bp.forEach(s),Ko.forEach(s),Ma=u(e),_(de.$$.fragment,e),Na=u(e),ae=n(e,"H3",{class:!0});var Go=i(ae);_e=n(Go,"A",{id:!0,class:!0,href:!0});var wp=i(_e);Hs=n(wp,"SPAN",{});var kp=i(Hs);_(Be.$$.fragment,kp),kp.forEach(s),wp.forEach(s),Zr=u(Go),Bs=n(Go,"SPAN",{});var Ep=i(Bs);Xr=o(Ep,"Pipeline usage"),Ep.forEach(s),Go.forEach(s),Ia=u(e),ge=n(e,"P",{});var Qo=i(ge);el=o(Qo,"In the following example, you will use the "),Pt=n(Qo,"A",{href:!0});var jp=i(Pt);tl=o(jp,"pipeline()"),jp.forEach(s),sl=o(Qo," for sentiment analysis."),Qo.forEach(s),Da=u(e),St=n(e,"P",{});var Ap=i(St);al=o(Ap,"Install the following dependencies if you haven\u2019t already:"),Ap.forEach(s),Oa=u(e),_(Ke.$$.fragment,e),La=u(e),ve=n(e,"P",{});var Jo=i(ve);ol=o(Jo,"Import "),Ct=n(Jo,"A",{href:!0});var Tp=i(Ct);rl=o(Tp,"pipeline()"),Tp.forEach(s),ll=o(Jo," and specify the task you want to complete:"),Jo.forEach(s),Ra=u(e),_(Ge.$$.fragment,e),Ua=u(e),B=n(e,"P",{});var _s=i(B);nl=o(_s,"The pipeline downloads and caches a default "),Qe=n(_s,"A",{href:!0,rel:!0});var qp=i(Qe);il=o(qp,"pretrained model"),qp.forEach(s),pl=o(_s," and tokenizer for sentiment analysis. Now you can use the "),Ks=n(_s,"CODE",{});var zp=i(Ks);fl=o(zp,"classifier"),zp.forEach(s),ul=o(_s," on your target text:"),_s.forEach(s),Wa=u(e),_(Je.$$.fragment,e),Ha=u(e),ye=n(e,"P",{});var Yo=i(ye);hl=o(Yo,"For more than one sentence, pass a list of sentences to the "),Mt=n(Yo,"A",{href:!0});var xp=i(Mt);ml=o(xp,"pipeline()"),xp.forEach(s),cl=o(Yo," which returns a list of dictionaries:"),Yo.forEach(s),Ba=u(e),_(Ye.$$.fragment,e),Ka=u(e),K=n(e,"P",{});var gs=i(K);dl=o(gs,"The "),Nt=n(gs,"A",{href:!0});var Fp=i(Nt);_l=o(Fp,"pipeline()"),Fp.forEach(s),gl=o(gs," can also iterate over an entire dataset. Start by installing the "),Ve=n(gs,"A",{href:!0,rel:!0});var Pp=i(Ve);vl=o(Pp,"\u{1F917} Datasets"),Pp.forEach(s),yl=o(gs," library:"),gs.forEach(s),Ga=u(e),_(Ze.$$.fragment,e),Qa=u(e),O=n(e,"P",{});var Ie=i(O);$l=o(Ie,"Create a "),It=n(Ie,"A",{href:!0});var Sp=i(It);bl=o(Sp,"pipeline()"),Sp.forEach(s),wl=o(Ie," with the task you want to solve for and the model you want to use. Set the "),Gs=n(Ie,"CODE",{});var Cp=i(Gs);kl=o(Cp,"device"),Cp.forEach(s),El=o(Ie," parameter to "),Qs=n(Ie,"CODE",{});var Mp=i(Qs);jl=o(Mp,"0"),Mp.forEach(s),Al=o(Ie," to place the tensors on a CUDA device:"),Ie.forEach(s),Ja=u(e),_(Xe.$$.fragment,e),Ya=u(e),G=n(e,"P",{});var vs=i(G);Tl=o(vs,"Next, load a dataset (see the \u{1F917} Datasets "),et=n(vs,"A",{href:!0,rel:!0});var Np=i(et);ql=o(Np,"Quick Start"),Np.forEach(s),zl=o(vs," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),tt=n(vs,"A",{href:!0,rel:!0});var Ip=i(tt);xl=o(Ip,"SUPERB"),Ip.forEach(s),Fl=o(vs," dataset:"),vs.forEach(s),Va=u(e),_(st.$$.fragment,e),Za=u(e),$e=n(e,"P",{});var Vo=i($e);Pl=o(Vo,"Now you can iterate over the dataset with the pipeline. "),Js=n(Vo,"CODE",{});var Dp=i(Js);Sl=o(Dp,"KeyDataset"),Dp.forEach(s),Cl=o(Vo," retrieves the item in the dictionary returned by the dataset:"),Vo.forEach(s),Xa=u(e),_(at.$$.fragment,e),eo=u(e),oe=n(e,"H3",{class:!0});var Zo=i(oe);be=n(Zo,"A",{id:!0,class:!0,href:!0});var Op=i(be);Ys=n(Op,"SPAN",{});var Lp=i(Ys);_(ot.$$.fragment,Lp),Lp.forEach(s),Op.forEach(s),Ml=u(Zo),Vs=n(Zo,"SPAN",{});var Rp=i(Vs);Nl=o(Rp,"Use another model and tokenizer in the pipeline"),Rp.forEach(s),Zo.forEach(s),to=u(e),C=n(e,"P",{});var te=i(C);Il=o(te,"The "),Dt=n(te,"A",{href:!0});var Up=i(Dt);Dl=o(Up,"pipeline()"),Up.forEach(s),Ol=o(te," can accommodate any model from the "),rt=n(te,"A",{href:!0,rel:!0});var Wp=i(rt);Ll=o(Wp,"Model Hub"),Wp.forEach(s),Rl=o(te,", making it easy to adapt the "),Ot=n(te,"A",{href:!0});var Hp=i(Ot);Ul=o(Hp,"pipeline()"),Hp.forEach(s),Wl=o(te," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),lt=n(te,"A",{href:!0,rel:!0});var Bp=i(lt);Hl=o(Bp,"BERT model"),Bp.forEach(s),Bl=o(te," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),te.forEach(s),so=u(e),_(nt.$$.fragment,e),ao=u(e),Q=n(e,"P",{});var ys=i(Q);Kl=o(ys,"Use the "),Lt=n(ys,"A",{href:!0});var Kp=i(Lt);Gl=o(Kp,"AutoModelForSequenceClassification"),Kp.forEach(s),Ql=o(ys," and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Zs=n(ys,"CODE",{});var Gp=i(Zs);Jl=o(Gp,"AutoClass"),Gp.forEach(s),Yl=o(ys," below):"),ys.forEach(s),oo=u(e),_(it.$$.fragment,e),ro=u(e),J=n(e,"P",{});var $s=i(J);Vl=o($s,"Then you can specify the model and tokenizer in the "),Rt=n($s,"A",{href:!0});var Qp=i(Rt);Zl=o(Qp,"pipeline()"),Qp.forEach(s),Xl=o($s,", and apply the "),Xs=n($s,"CODE",{});var Jp=i(Xs);en=o(Jp,"classifier"),Jp.forEach(s),tn=o($s," on your target text:"),$s.forEach(s),lo=u(e),_(pt.$$.fragment,e),no=u(e),Y=n(e,"P",{});var bs=i(Y);sn=o(bs,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Ut=n(bs,"A",{href:!0});var Yp=i(Ut);an=o(Yp,"fine-tuning tutorial"),Yp.forEach(s),on=o(bs," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Wt=n(bs,"A",{href:!0});var Vp=i(Wt);rn=o(Vp,"here"),Vp.forEach(s),ln=o(bs,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),bs.forEach(s),io=u(e),re=n(e,"H2",{class:!0});var Xo=i(re);we=n(Xo,"A",{id:!0,class:!0,href:!0});var Zp=i(we);ea=n(Zp,"SPAN",{});var Xp=i(ea);_(ft.$$.fragment,Xp),Xp.forEach(s),Zp.forEach(s),nn=u(Xo),ta=n(Xo,"SPAN",{});var ef=i(ta);pn=o(ef,"AutoClass"),ef.forEach(s),Xo.forEach(s),po=u(e),_(ut.$$.fragment,e),fo=u(e),z=n(e,"P",{});var M=i(z);fn=o(M,"Under the hood, the "),Ht=n(M,"A",{href:!0});var tf=i(Ht);un=o(tf,"AutoModelForSequenceClassification"),tf.forEach(s),hn=o(M," and "),Bt=n(M,"A",{href:!0});var sf=i(Bt);mn=o(sf,"AutoTokenizer"),sf.forEach(s),cn=o(M," classes work together to power the "),Kt=n(M,"A",{href:!0});var af=i(Kt);dn=o(af,"pipeline()"),af.forEach(s),_n=o(M,". An "),Gt=n(M,"A",{href:!0});var of=i(Gt);gn=o(of,"AutoClass"),of.forEach(s),vn=o(M," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),sa=n(M,"CODE",{});var rf=i(sa);yn=o(rf,"AutoClass"),rf.forEach(s),$n=o(M," for your task and it\u2019s associated tokenizer with "),Qt=n(M,"A",{href:!0});var lf=i(Qt);bn=o(lf,"AutoTokenizer"),lf.forEach(s),wn=o(M,"."),M.forEach(s),uo=u(e),V=n(e,"P",{});var ws=i(V);kn=o(ws,"Let\u2019s return to our example and see how you can use the "),aa=n(ws,"CODE",{});var nf=i(aa);En=o(nf,"AutoClass"),nf.forEach(s),jn=o(ws," to replicate the results of the "),Jt=n(ws,"A",{href:!0});var pf=i(Jt);An=o(pf,"pipeline()"),pf.forEach(s),Tn=o(ws,"."),ws.forEach(s),ho=u(e),le=n(e,"H3",{class:!0});var er=i(le);ke=n(er,"A",{id:!0,class:!0,href:!0});var ff=i(ke);oa=n(ff,"SPAN",{});var uf=i(oa);_(ht.$$.fragment,uf),uf.forEach(s),ff.forEach(s),qn=u(er),ra=n(er,"SPAN",{});var hf=i(ra);zn=o(hf,"AutoTokenizer"),hf.forEach(s),er.forEach(s),mo=u(e),Z=n(e,"P",{});var ks=i(Z);xn=o(ks,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=n(ks,"EM",{});var mf=i(la);Fn=o(mf,"tokens"),mf.forEach(s),Pn=o(ks,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Yt=n(ks,"A",{href:!0});var cf=i(Yt);Sn=o(cf,"here"),cf.forEach(s),Cn=o(ks,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),ks.forEach(s),co=u(e),Ee=n(e,"P",{});var tr=i(Ee);Mn=o(tr,"Load a tokenizer with "),Vt=n(tr,"A",{href:!0});var df=i(Vt);Nn=o(df,"AutoTokenizer"),df.forEach(s),In=o(tr,":"),tr.forEach(s),_o=u(e),_(mt.$$.fragment,e),go=u(e),je=n(e,"P",{});var sr=i(je);Dn=o(sr,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),na=n(sr,"EM",{});var _f=i(na);On=o(_f,"vocabulary"),_f.forEach(s),Ln=o(sr,"."),sr.forEach(s),vo=u(e),Zt=n(e,"P",{});var gf=i(Zt);Rn=o(gf,"Pass your text to the tokenizer:"),gf.forEach(s),yo=u(e),_(ct.$$.fragment,e),$o=u(e),Xt=n(e,"P",{});var vf=i(Xt);Un=o(vf,"The tokenizer will return a dictionary containing:"),vf.forEach(s),bo=u(e),Ae=n(e,"UL",{});var ar=i(Ae);es=n(ar,"LI",{});var Vi=i(es);ts=n(Vi,"A",{href:!0});var yf=i(ts);Wn=o(yf,"input_ids"),yf.forEach(s),Hn=o(Vi,": numerical representions of your tokens."),Vi.forEach(s),Bn=u(ar),ss=n(ar,"LI",{});var Zi=i(ss);as=n(Zi,"A",{href:!0});var $f=i(as);Kn=o($f,"atttention_mask"),$f.forEach(s),Gn=o(Zi,": indicates which tokens should be attended to."),Zi.forEach(s),ar.forEach(s),wo=u(e),Te=n(e,"P",{});var or=i(Te);Qn=o(or,"Just like the "),os=n(or,"A",{href:!0});var bf=i(os);Jn=o(bf,"pipeline()"),bf.forEach(s),Yn=o(or,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),or.forEach(s),ko=u(e),_(dt.$$.fragment,e),Eo=u(e),qe=n(e,"P",{});var rr=i(qe);Vn=o(rr,"Read the "),rs=n(rr,"A",{href:!0});var wf=i(rs);Zn=o(wf,"preprocessing"),wf.forEach(s),Xn=o(rr," tutorial for more details about tokenization."),rr.forEach(s),jo=u(e),ne=n(e,"H3",{class:!0});var lr=i(ne);ze=n(lr,"A",{id:!0,class:!0,href:!0});var kf=i(ze);ia=n(kf,"SPAN",{});var Ef=i(ia);_(_t.$$.fragment,Ef),Ef.forEach(s),kf.forEach(s),ei=u(lr),pa=n(lr,"SPAN",{});var jf=i(pa);ti=o(jf,"AutoModel"),jf.forEach(s),lr.forEach(s),Ao=u(e),F=n(e,"P",{});var L=i(F);si=o(L,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),ls=n(L,"A",{href:!0});var Af=i(ls);ai=o(Af,"AutoModel"),Af.forEach(s),oi=o(L," like you would load an "),ns=n(L,"A",{href:!0});var Tf=i(ns);ri=o(Tf,"AutoTokenizer"),Tf.forEach(s),li=o(L,". The only difference is selecting the correct "),is=n(L,"A",{href:!0});var qf=i(is);ni=o(qf,"AutoModel"),qf.forEach(s),ii=o(L," for the task. Since you are doing text - or sequence - classification, load "),ps=n(L,"A",{href:!0});var zf=i(ps);pi=o(zf,"AutoModelForSequenceClassification"),zf.forEach(s),fi=o(L,". The TensorFlow equivalent is simply "),fs=n(L,"A",{href:!0});var xf=i(fs);ui=o(xf,"TFAutoModelForSequenceClassification"),xf.forEach(s),hi=o(L,":"),L.forEach(s),To=u(e),_(gt.$$.fragment,e),qo=u(e),_(xe.$$.fragment,e),zo=u(e),Fe=n(e,"P",{});var nr=i(Fe);mi=o(nr,"Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),fa=n(nr,"CODE",{});var Ff=i(fa);ci=o(Ff,"**"),Ff.forEach(s),di=o(nr,". For TensorFlow models, pass the dictionary keys directly to the tensors:"),nr.forEach(s),xo=u(e),_(vt.$$.fragment,e),Fo=u(e),X=n(e,"P",{});var Es=i(X);_i=o(Es,"The model outputs the final activations in the "),ua=n(Es,"CODE",{});var Pf=i(ua);gi=o(Pf,"logits"),Pf.forEach(s),vi=o(Es," attribute. Apply the softmax function to the "),ha=n(Es,"CODE",{});var Sf=i(ha);yi=o(Sf,"logits"),Sf.forEach(s),$i=o(Es," to retrieve the probabilities:"),Es.forEach(s),Po=u(e),_(yt.$$.fragment,e),So=u(e),_(Pe.$$.fragment,e),Co=u(e),x=n(e,"P",{});var N=i(x);bi=o(N,"Models are a standard "),$t=n(N,"A",{href:!0,rel:!0});var Cf=i($t);ma=n(Cf,"CODE",{});var Mf=i(ma);wi=o(Mf,"torch.nn.Module"),Mf.forEach(s),Cf.forEach(s),ki=o(N," or a "),bt=n(N,"A",{href:!0,rel:!0});var Nf=i(bt);ca=n(Nf,"CODE",{});var If=i(ca);Ei=o(If,"tf.keras.Model"),If.forEach(s),Nf.forEach(s),ji=o(N," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),us=n(N,"A",{href:!0});var Df=i(us);Ai=o(Df,"Trainer"),Df.forEach(s),Ti=o(N," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),da=n(N,"CODE",{});var Of=i(da);qi=o(Of,"fit"),Of.forEach(s),zi=o(N," method from "),wt=n(N,"A",{href:!0,rel:!0});var Lf=i(wt);xi=o(Lf,"Keras"),Lf.forEach(s),Fi=o(N,". Refer to the "),hs=n(N,"A",{href:!0});var Rf=i(hs);Pi=o(Rf,"training tutorial"),Rf.forEach(s),Si=o(N," for more details."),N.forEach(s),Mo=u(e),_(Se.$$.fragment,e),No=u(e),ie=n(e,"H3",{class:!0});var ir=i(ie);Ce=n(ir,"A",{id:!0,class:!0,href:!0});var Uf=i(Ce);_a=n(Uf,"SPAN",{});var Wf=i(_a);_(kt.$$.fragment,Wf),Wf.forEach(s),Uf.forEach(s),Ci=u(ir),ga=n(ir,"SPAN",{});var Hf=i(ga);Mi=o(Hf,"Save a model"),Hf.forEach(s),ir.forEach(s),Io=u(e),Me=n(e,"P",{});var pr=i(Me);Ni=o(pr,"Once your model is fine-tuned, you can save it with its tokenizer using "),ms=n(pr,"A",{href:!0});var Bf=i(ms);Ii=o(Bf,"PreTrainedModel.save_pretrained()"),Bf.forEach(s),Di=o(pr,":"),pr.forEach(s),Do=u(e),_(Et.$$.fragment,e),Oo=u(e),Ne=n(e,"P",{});var fr=i(Ne);Oi=o(fr,"When you are ready to use the model again, reload it with "),cs=n(fr,"A",{href:!0});var Kf=i(cs);Li=o(Kf,"PreTrainedModel.from_pretrained()"),Kf.forEach(s),Ri=o(fr,":"),fr.forEach(s),Lo=u(e),_(jt.$$.fragment,e),Ro=u(e),ee=n(e,"P",{});var js=i(ee);Ui=o(js,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),va=n(js,"CODE",{});var Gf=i(va);Wi=o(Gf,"from_pt"),Gf.forEach(s),Hi=o(js," or "),ya=n(js,"CODE",{});var Qf=i(ya);Bi=o(Qf,"from_tf"),Qf.forEach(s),Ki=o(js," parameter can convert the model from one framework to the other:"),js.forEach(s),Uo=u(e),_(At.$$.fragment,e),this.h()},h(){h(m,"name","hf:doc:metadata"),h(m,"content",JSON.stringify(nu)),h(w,"id","quick-tour"),h(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(w,"href","#quick-tour"),h(c,"class","relative group"),h(qt,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(zt,"href","./model_doc/auto"),h(he,"id","pipeline"),h(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(he,"href","#pipeline"),h(se,"class","relative group"),h(xt,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(Ft,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(_e,"id","pipeline-usage"),h(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(_e,"href","#pipeline-usage"),h(ae,"class","relative group"),h(Pt,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(Ct,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(Qe,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),h(Qe,"rel","nofollow"),h(Mt,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(Nt,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(Ve,"href","https://huggingface.co/docs/datasets/"),h(Ve,"rel","nofollow"),h(It,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(et,"href","https://huggingface.co/docs/datasets/quickstart.html"),h(et,"rel","nofollow"),h(tt,"href","https://huggingface.co/datasets/superb"),h(tt,"rel","nofollow"),h(be,"id","use-another-model-and-tokenizer-in-the-pipeline"),h(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(be,"href","#use-another-model-and-tokenizer-in-the-pipeline"),h(oe,"class","relative group"),h(Dt,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(rt,"href","https://huggingface.co/models"),h(rt,"rel","nofollow"),h(Ot,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(lt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),h(lt,"rel","nofollow"),h(Lt,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),h(Rt,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(Ut,"href","./training"),h(Wt,"href","./model_sharing"),h(we,"id","autoclass"),h(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(we,"href","#autoclass"),h(re,"class","relative group"),h(Ht,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),h(Bt,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.AutoTokenizer"),h(Kt,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(Gt,"href","./model_doc/auto"),h(Qt,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.AutoTokenizer"),h(Jt,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(ke,"id","autotokenizer"),h(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ke,"href","#autotokenizer"),h(le,"class","relative group"),h(Yt,"href","./tokenizer_summary"),h(Vt,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.AutoTokenizer"),h(ts,"href","./glossary#input-ids"),h(as,"href",".glossary#attention-mask"),h(os,"href","/docs/transformers/doc-build-test/en/main_classes/pipelines#transformers.pipeline"),h(rs,"href","./preprocessing"),h(ze,"id","automodel"),h(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ze,"href","#automodel"),h(ne,"class","relative group"),h(ls,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.AutoModel"),h(ns,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.AutoTokenizer"),h(is,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.AutoModel"),h(ps,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),h(fs,"href","/docs/transformers/doc-build-test/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),h($t,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h($t,"rel","nofollow"),h(bt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),h(bt,"rel","nofollow"),h(us,"href","/docs/transformers/doc-build-test/en/main_classes/trainer#transformers.Trainer"),h(wt,"href","https://keras.io/"),h(wt,"rel","nofollow"),h(hs,"href","./training"),h(Ce,"id","save-a-model"),h(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Ce,"href","#save-a-model"),h(ie,"class","relative group"),h(ms,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),h(cs,"href","/docs/transformers/doc-build-test/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(e,r){t(document.head,m),p(e,k,r),p(e,c,r),t(c,w),t(w,A),g(b,A,null),t(c,E),t(c,q),t(q,S),p(e,j,r),g(D,e,r),p(e,U,r),p(e,W,r),t(W,ur),t(W,qt),t(qt,hr),t(W,mr),t(W,zt),t(zt,cr),t(W,dr),p(e,Ea,r),g(ue,e,r),p(e,ja,r),p(e,se,r),t(se,he),t(he,As),g(Oe,As,null),t(se,_r),t(se,Ts),t(Ts,gr),p(e,Aa,r),p(e,Le,r),t(Le,xt),t(xt,vr),t(Le,yr),p(e,Ta,r),g(Re,e,r),p(e,qa,r),p(e,me,r),t(me,$r),t(me,Ft),t(Ft,br),t(me,wr),p(e,za,r),p(e,Ue,r),t(Ue,qs),t(qs,kr),t(Ue,Er),p(e,xa,r),p(e,T,r),t(T,zs),t(zs,jr),t(T,Ar),t(T,xs),t(xs,Tr),t(T,qr),t(T,Fs),t(Fs,zr),t(T,xr),t(T,Ps),t(Ps,Fr),t(T,Pr),t(T,Ss),t(Ss,Sr),t(T,Cr),t(T,Cs),t(Cs,Mr),t(T,Nr),t(T,Ms),t(Ms,Ir),t(T,Dr),t(T,Ns),t(Ns,Or),p(e,Fa,r),p(e,We,r),t(We,Is),t(Is,Lr),t(We,Rr),p(e,Pa,r),p(e,H,r),t(H,Ds),t(Ds,Ur),t(H,Wr),t(H,Os),t(Os,Hr),t(H,Br),t(H,Ls),t(Ls,Kr),p(e,Sa,r),p(e,He,r),t(He,Rs),t(Rs,Gr),t(He,Qr),p(e,Ca,r),p(e,ce,r),t(ce,Us),t(Us,Jr),t(ce,Yr),t(ce,Ws),t(Ws,Vr),p(e,Ma,r),g(de,e,r),p(e,Na,r),p(e,ae,r),t(ae,_e),t(_e,Hs),g(Be,Hs,null),t(ae,Zr),t(ae,Bs),t(Bs,Xr),p(e,Ia,r),p(e,ge,r),t(ge,el),t(ge,Pt),t(Pt,tl),t(ge,sl),p(e,Da,r),p(e,St,r),t(St,al),p(e,Oa,r),g(Ke,e,r),p(e,La,r),p(e,ve,r),t(ve,ol),t(ve,Ct),t(Ct,rl),t(ve,ll),p(e,Ra,r),g(Ge,e,r),p(e,Ua,r),p(e,B,r),t(B,nl),t(B,Qe),t(Qe,il),t(B,pl),t(B,Ks),t(Ks,fl),t(B,ul),p(e,Wa,r),g(Je,e,r),p(e,Ha,r),p(e,ye,r),t(ye,hl),t(ye,Mt),t(Mt,ml),t(ye,cl),p(e,Ba,r),g(Ye,e,r),p(e,Ka,r),p(e,K,r),t(K,dl),t(K,Nt),t(Nt,_l),t(K,gl),t(K,Ve),t(Ve,vl),t(K,yl),p(e,Ga,r),g(Ze,e,r),p(e,Qa,r),p(e,O,r),t(O,$l),t(O,It),t(It,bl),t(O,wl),t(O,Gs),t(Gs,kl),t(O,El),t(O,Qs),t(Qs,jl),t(O,Al),p(e,Ja,r),g(Xe,e,r),p(e,Ya,r),p(e,G,r),t(G,Tl),t(G,et),t(et,ql),t(G,zl),t(G,tt),t(tt,xl),t(G,Fl),p(e,Va,r),g(st,e,r),p(e,Za,r),p(e,$e,r),t($e,Pl),t($e,Js),t(Js,Sl),t($e,Cl),p(e,Xa,r),g(at,e,r),p(e,eo,r),p(e,oe,r),t(oe,be),t(be,Ys),g(ot,Ys,null),t(oe,Ml),t(oe,Vs),t(Vs,Nl),p(e,to,r),p(e,C,r),t(C,Il),t(C,Dt),t(Dt,Dl),t(C,Ol),t(C,rt),t(rt,Ll),t(C,Rl),t(C,Ot),t(Ot,Ul),t(C,Wl),t(C,lt),t(lt,Hl),t(C,Bl),p(e,so,r),g(nt,e,r),p(e,ao,r),p(e,Q,r),t(Q,Kl),t(Q,Lt),t(Lt,Gl),t(Q,Ql),t(Q,Zs),t(Zs,Jl),t(Q,Yl),p(e,oo,r),g(it,e,r),p(e,ro,r),p(e,J,r),t(J,Vl),t(J,Rt),t(Rt,Zl),t(J,Xl),t(J,Xs),t(Xs,en),t(J,tn),p(e,lo,r),g(pt,e,r),p(e,no,r),p(e,Y,r),t(Y,sn),t(Y,Ut),t(Ut,an),t(Y,on),t(Y,Wt),t(Wt,rn),t(Y,ln),p(e,io,r),p(e,re,r),t(re,we),t(we,ea),g(ft,ea,null),t(re,nn),t(re,ta),t(ta,pn),p(e,po,r),g(ut,e,r),p(e,fo,r),p(e,z,r),t(z,fn),t(z,Ht),t(Ht,un),t(z,hn),t(z,Bt),t(Bt,mn),t(z,cn),t(z,Kt),t(Kt,dn),t(z,_n),t(z,Gt),t(Gt,gn),t(z,vn),t(z,sa),t(sa,yn),t(z,$n),t(z,Qt),t(Qt,bn),t(z,wn),p(e,uo,r),p(e,V,r),t(V,kn),t(V,aa),t(aa,En),t(V,jn),t(V,Jt),t(Jt,An),t(V,Tn),p(e,ho,r),p(e,le,r),t(le,ke),t(ke,oa),g(ht,oa,null),t(le,qn),t(le,ra),t(ra,zn),p(e,mo,r),p(e,Z,r),t(Z,xn),t(Z,la),t(la,Fn),t(Z,Pn),t(Z,Yt),t(Yt,Sn),t(Z,Cn),p(e,co,r),p(e,Ee,r),t(Ee,Mn),t(Ee,Vt),t(Vt,Nn),t(Ee,In),p(e,_o,r),g(mt,e,r),p(e,go,r),p(e,je,r),t(je,Dn),t(je,na),t(na,On),t(je,Ln),p(e,vo,r),p(e,Zt,r),t(Zt,Rn),p(e,yo,r),g(ct,e,r),p(e,$o,r),p(e,Xt,r),t(Xt,Un),p(e,bo,r),p(e,Ae,r),t(Ae,es),t(es,ts),t(ts,Wn),t(es,Hn),t(Ae,Bn),t(Ae,ss),t(ss,as),t(as,Kn),t(ss,Gn),p(e,wo,r),p(e,Te,r),t(Te,Qn),t(Te,os),t(os,Jn),t(Te,Yn),p(e,ko,r),g(dt,e,r),p(e,Eo,r),p(e,qe,r),t(qe,Vn),t(qe,rs),t(rs,Zn),t(qe,Xn),p(e,jo,r),p(e,ne,r),t(ne,ze),t(ze,ia),g(_t,ia,null),t(ne,ei),t(ne,pa),t(pa,ti),p(e,Ao,r),p(e,F,r),t(F,si),t(F,ls),t(ls,ai),t(F,oi),t(F,ns),t(ns,ri),t(F,li),t(F,is),t(is,ni),t(F,ii),t(F,ps),t(ps,pi),t(F,fi),t(F,fs),t(fs,ui),t(F,hi),p(e,To,r),g(gt,e,r),p(e,qo,r),g(xe,e,r),p(e,zo,r),p(e,Fe,r),t(Fe,mi),t(Fe,fa),t(fa,ci),t(Fe,di),p(e,xo,r),g(vt,e,r),p(e,Fo,r),p(e,X,r),t(X,_i),t(X,ua),t(ua,gi),t(X,vi),t(X,ha),t(ha,yi),t(X,$i),p(e,Po,r),g(yt,e,r),p(e,So,r),g(Pe,e,r),p(e,Co,r),p(e,x,r),t(x,bi),t(x,$t),t($t,ma),t(ma,wi),t(x,ki),t(x,bt),t(bt,ca),t(ca,Ei),t(x,ji),t(x,us),t(us,Ai),t(x,Ti),t(x,da),t(da,qi),t(x,zi),t(x,wt),t(wt,xi),t(x,Fi),t(x,hs),t(hs,Pi),t(x,Si),p(e,Mo,r),g(Se,e,r),p(e,No,r),p(e,ie,r),t(ie,Ce),t(Ce,_a),g(kt,_a,null),t(ie,Ci),t(ie,ga),t(ga,Mi),p(e,Io,r),p(e,Me,r),t(Me,Ni),t(Me,ms),t(ms,Ii),t(Me,Di),p(e,Do,r),g(Et,e,r),p(e,Oo,r),p(e,Ne,r),t(Ne,Oi),t(Ne,cs),t(cs,Li),t(Ne,Ri),p(e,Lo,r),g(jt,e,r),p(e,Ro,r),p(e,ee,r),t(ee,Ui),t(ee,va),t(va,Wi),t(ee,Hi),t(ee,ya),t(ya,Bi),t(ee,Ki),p(e,Uo,r),g(At,e,r),Wo=!0},p(e,[r]){const Tt={};r&2&&(Tt.$$scope={dirty:r,ctx:e}),ue.$set(Tt);const $a={};r&2&&($a.$$scope={dirty:r,ctx:e}),de.$set($a);const ba={};r&2&&(ba.$$scope={dirty:r,ctx:e}),xe.$set(ba);const wa={};r&2&&(wa.$$scope={dirty:r,ctx:e}),Pe.$set(wa);const pe={};r&2&&(pe.$$scope={dirty:r,ctx:e}),Se.$set(pe)},i(e){Wo||(v(b.$$.fragment,e),v(D.$$.fragment,e),v(ue.$$.fragment,e),v(Oe.$$.fragment,e),v(Re.$$.fragment,e),v(de.$$.fragment,e),v(Be.$$.fragment,e),v(Ke.$$.fragment,e),v(Ge.$$.fragment,e),v(Je.$$.fragment,e),v(Ye.$$.fragment,e),v(Ze.$$.fragment,e),v(Xe.$$.fragment,e),v(st.$$.fragment,e),v(at.$$.fragment,e),v(ot.$$.fragment,e),v(nt.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ft.$$.fragment,e),v(ut.$$.fragment,e),v(ht.$$.fragment,e),v(mt.$$.fragment,e),v(ct.$$.fragment,e),v(dt.$$.fragment,e),v(_t.$$.fragment,e),v(gt.$$.fragment,e),v(xe.$$.fragment,e),v(vt.$$.fragment,e),v(yt.$$.fragment,e),v(Pe.$$.fragment,e),v(Se.$$.fragment,e),v(kt.$$.fragment,e),v(Et.$$.fragment,e),v(jt.$$.fragment,e),v(At.$$.fragment,e),Wo=!0)},o(e){y(b.$$.fragment,e),y(D.$$.fragment,e),y(ue.$$.fragment,e),y(Oe.$$.fragment,e),y(Re.$$.fragment,e),y(de.$$.fragment,e),y(Be.$$.fragment,e),y(Ke.$$.fragment,e),y(Ge.$$.fragment,e),y(Je.$$.fragment,e),y(Ye.$$.fragment,e),y(Ze.$$.fragment,e),y(Xe.$$.fragment,e),y(st.$$.fragment,e),y(at.$$.fragment,e),y(ot.$$.fragment,e),y(nt.$$.fragment,e),y(it.$$.fragment,e),y(pt.$$.fragment,e),y(ft.$$.fragment,e),y(ut.$$.fragment,e),y(ht.$$.fragment,e),y(mt.$$.fragment,e),y(ct.$$.fragment,e),y(dt.$$.fragment,e),y(_t.$$.fragment,e),y(gt.$$.fragment,e),y(xe.$$.fragment,e),y(vt.$$.fragment,e),y(yt.$$.fragment,e),y(Pe.$$.fragment,e),y(Se.$$.fragment,e),y(kt.$$.fragment,e),y(Et.$$.fragment,e),y(jt.$$.fragment,e),y(At.$$.fragment,e),Wo=!1},d(e){s(m),e&&s(k),e&&s(c),$(b),e&&s(j),$(D,e),e&&s(U),e&&s(W),e&&s(Ea),$(ue,e),e&&s(ja),e&&s(se),$(Oe),e&&s(Aa),e&&s(Le),e&&s(Ta),$(Re,e),e&&s(qa),e&&s(me),e&&s(za),e&&s(Ue),e&&s(xa),e&&s(T),e&&s(Fa),e&&s(We),e&&s(Pa),e&&s(H),e&&s(Sa),e&&s(He),e&&s(Ca),e&&s(ce),e&&s(Ma),$(de,e),e&&s(Na),e&&s(ae),$(Be),e&&s(Ia),e&&s(ge),e&&s(Da),e&&s(St),e&&s(Oa),$(Ke,e),e&&s(La),e&&s(ve),e&&s(Ra),$(Ge,e),e&&s(Ua),e&&s(B),e&&s(Wa),$(Je,e),e&&s(Ha),e&&s(ye),e&&s(Ba),$(Ye,e),e&&s(Ka),e&&s(K),e&&s(Ga),$(Ze,e),e&&s(Qa),e&&s(O),e&&s(Ja),$(Xe,e),e&&s(Ya),e&&s(G),e&&s(Va),$(st,e),e&&s(Za),e&&s($e),e&&s(Xa),$(at,e),e&&s(eo),e&&s(oe),$(ot),e&&s(to),e&&s(C),e&&s(so),$(nt,e),e&&s(ao),e&&s(Q),e&&s(oo),$(it,e),e&&s(ro),e&&s(J),e&&s(lo),$(pt,e),e&&s(no),e&&s(Y),e&&s(io),e&&s(re),$(ft),e&&s(po),$(ut,e),e&&s(fo),e&&s(z),e&&s(uo),e&&s(V),e&&s(ho),e&&s(le),$(ht),e&&s(mo),e&&s(Z),e&&s(co),e&&s(Ee),e&&s(_o),$(mt,e),e&&s(go),e&&s(je),e&&s(vo),e&&s(Zt),e&&s(yo),$(ct,e),e&&s($o),e&&s(Xt),e&&s(bo),e&&s(Ae),e&&s(wo),e&&s(Te),e&&s(ko),$(dt,e),e&&s(Eo),e&&s(qe),e&&s(jo),e&&s(ne),$(_t),e&&s(Ao),e&&s(F),e&&s(To),$(gt,e),e&&s(qo),$(xe,e),e&&s(zo),e&&s(Fe),e&&s(xo),$(vt,e),e&&s(Fo),e&&s(X),e&&s(Po),$(yt,e),e&&s(So),$(Pe,e),e&&s(Co),e&&s(x),e&&s(Mo),$(Se,e),e&&s(No),e&&s(ie),$(kt),e&&s(Io),e&&s(Me),e&&s(Do),$(Et,e),e&&s(Oo),e&&s(Ne),e&&s(Lo),$(jt,e),e&&s(Ro),e&&s(ee),e&&s(Uo),$(At,e)}}}const nu={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function iu(I,m,k){let{fw:c}=m;return I.$$set=w=>{"fw"in w&&k(0,c=w.fw)},[c]}class gu extends Yf{constructor(m){super();Vf(this,m,iu,lu,Zf,{fw:0})}}export{gu as default,nu as metadata};
