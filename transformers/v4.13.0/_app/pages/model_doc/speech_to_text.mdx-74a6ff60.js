import{S as ll,i as pl,s as hl,e as a,k as d,w as u,t as n,L as ml,c as r,d as o,m as l,a as i,x as _,h as s,b as c,J as e,g as h,y as g,q as v,o as T,B as x}from"../../chunks/vendor-e859c359.js";import{T as zs}from"../../chunks/Tip-edc75249.js";import{D as C}from"../../chunks/Docstring-ade913b3.js";import{C as En}from"../../chunks/CodeBlock-ce4317c2.js";import{I as xe}from"../../chunks/IconCopyLink-5fae3b20.js";import"../../chunks/CopyButton-77addb3d.js";function fl(Y){let m,w,f,b,z,k,y,E;return{c(){m=a("p"),w=n(`This class method is simply calling Speech2TextFeatureExtractor\u2019s
`),f=a("code"),b=n("from_pretrained"),z=n(` and Speech2TextTokenizer\u2019s
`),k=a("code"),y=n("from_pretrained"),E=n(`. Please refer to the
docstrings of the methods above for more information.`)},l(F){m=r(F,"P",{});var $=i(m);w=s($,`This class method is simply calling Speech2TextFeatureExtractor\u2019s
`),f=r($,"CODE",{});var M=i(f);b=s(M,"from_pretrained"),M.forEach(o),z=s($,` and Speech2TextTokenizer\u2019s
`),k=r($,"CODE",{});var L=i(k);y=s(L,"from_pretrained"),L.forEach(o),E=s($,`. Please refer to the
docstrings of the methods above for more information.`),$.forEach(o)},m(F,$){h(F,m,$),e(m,w),e(m,f),e(f,b),e(m,z),e(m,k),e(k,y),e(m,E)},d(F){F&&o(m)}}}function ul(Y){let m,w,f,b,z,k,y,E;return{c(){m=a("p"),w=n("This class method is simply calling "),f=a("code"),b=n("save_pretrained"),z=n(` and
`),k=a("code"),y=n("save_pretrained"),E=n(`. Please refer to the
docstrings of the methods above for more information.`)},l(F){m=r(F,"P",{});var $=i(m);w=s($,"This class method is simply calling "),f=r($,"CODE",{});var M=i(f);b=s(M,"save_pretrained"),M.forEach(o),z=s($,` and
`),k=r($,"CODE",{});var L=i(k);y=s(L,"save_pretrained"),L.forEach(o),E=s($,`. Please refer to the
docstrings of the methods above for more information.`),$.forEach(o)},m(F,$){h(F,m,$),e(m,w),e(m,f),e(f,b),e(m,z),e(m,k),e(k,y),e(m,E)},d(F){F&&o(m)}}}function _l(Y){let m,w,f,b,z;return{c(){m=a("p"),w=n(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=a("code"),b=n("Module"),z=n(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(k){m=r(k,"P",{});var y=i(m);w=s(y,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=r(y,"CODE",{});var E=i(f);b=s(E,"Module"),E.forEach(o),z=s(y,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),y.forEach(o)},m(k,y){h(k,m,y),e(m,w),e(m,f),e(f,b),e(m,z)},d(k){k&&o(m)}}}function gl(Y){let m,w,f,b,z;return{c(){m=a("p"),w=n(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=a("code"),b=n("Module"),z=n(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(k){m=r(k,"P",{});var y=i(m);w=s(y,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=r(y,"CODE",{});var E=i(f);b=s(E,"Module"),E.forEach(o),z=s(y,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),y.forEach(o)},m(k,y){h(k,m,y),e(m,w),e(m,f),e(f,b),e(m,z)},d(k){k&&o(m)}}}function vl(Y){let m,w,f,b,z,k,y,E,F,$,M,L,qo,Ue,Es,Po,qs,qn,O,Ps,Be,js,Cs,He,Fs,Ms,Re,As,Ds,Je,Is,Ns,Pn,Q,Ls,Ye,Os,Gs,Xe,Ws,Vs,jn,ce,ke,jo,Ke,Us,Co,Bs,Cn,be,Hs,Fo,Rs,Js,Fn,G,Ys,Rt,Xs,Ks,Jt,Qs,Zs,Yt,ea,ta,Xt,oa,na,Mn,q,sa,Mo,aa,ra,Ao,ia,ca,Do,da,la,Io,pa,ha,No,ma,fa,Qe,ua,_a,Lo,ga,An,Kt,Oo,va,Dn,Ze,In,Qt,et,Go,Ta,xa,A,ka,Wo,ba,ya,Vo,Sa,wa,Uo,$a,za,Bo,Ea,qa,Ho,Pa,ja,Nn,tt,Ln,ye,Ca,ot,Fa,Ma,On,de,Se,Ro,nt,Aa,Jo,Da,Gn,D,st,Ia,le,Na,Zt,La,Oa,at,Ga,Wa,Va,pe,Ua,eo,Ba,Ha,to,Ra,Ja,Ya,Yo,Xa,Ka,rt,Wn,he,we,Xo,it,Qa,Ko,Za,Vn,P,ct,er,Qo,tr,or,dt,nr,oo,sr,ar,rr,$e,lt,ir,Zo,cr,dr,ze,pt,lr,ht,pr,en,hr,mr,fr,Z,mt,ur,no,_r,so,gr,vr,tn,Tr,xr,on,Un,me,Ee,nn,ft,kr,sn,br,Bn,I,ut,yr,an,Sr,wr,_t,$r,ao,zr,Er,qr,rn,Pr,jr,qe,gt,Cr,cn,Fr,Hn,fe,Pe,dn,vt,Mr,ln,Ar,Rn,S,Tt,Dr,pn,Ir,Nr,W,ro,Lr,Or,io,Gr,Wr,co,Vr,Ur,xt,hn,Br,Hr,Rr,lo,Jr,Yr,Xr,je,kt,Kr,X,Qr,bt,mn,Zr,ei,ti,po,oi,ni,yt,fn,si,ai,ri,ii,ee,St,ci,wt,di,ho,li,pi,hi,Ce,mi,te,$t,fi,ue,ui,un,_i,gi,mo,vi,Ti,xi,Fe,ki,Me,zt,bi,Et,yi,fo,Si,wi,$i,Ae,qt,zi,Pt,Ei,uo,qi,Pi,ji,De,jt,Ci,_n,Fi,Jn,_e,Ie,gn,Ct,Mi,vn,Ai,Yn,H,Ft,Di,Mt,Ii,_o,Ni,Li,Oi,At,Gi,Dt,Wi,Vi,Ui,V,It,Bi,ge,Hi,go,Ri,Ji,Tn,Yi,Xi,Ki,Ne,Qi,xn,Zi,ec,Nt,Xn,ve,Le,kn,Lt,tc,bn,oc,Kn,R,Ot,nc,Gt,sc,vo,ac,rc,ic,Wt,cc,Vt,dc,lc,pc,U,Ut,hc,Te,mc,To,fc,uc,yn,_c,gc,vc,Oe,Tc,Sn,xc,kc,Bt,Qn;return k=new xe({}),Ue=new xe({}),Ke=new xe({}),Ze=new En({props:{code:`import torch
from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
from datasets import load_dataset
import soundfile as sf

model = Speech2TextForConditionalGeneration.from_pretrained("facebook/s2t-small-librispeech-asr")
processor = Speech2TextProcessor.from_pretrained("facebook/s2t-small-librispeech-asr")

def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch

ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

inputs = processor(ds["speech"][0], sampling_rate=16_000, return_tensors="pt")
generated_ids = model.generate(input_ids=inputs["input_features"], attention_mask=inputs["attention_mask"])

transcription = processor.batch_decode(generated_ids),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> torch</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2TextProcessor, Speech2TextForConditionalGeneration</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model = Speech2TextForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-small-librispeech-asr&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">processor = Speech2TextProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-small-librispeech-asr&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">map_to_array</span>(<span class="hljs-params">batch</span>):</span>
<span class="hljs-meta">...</span> <span class="language-python">    speech, _ = sf.read(batch[<span class="hljs-string">&quot;file&quot;</span>])</span>
<span class="hljs-meta">...</span> <span class="language-python">    batch[<span class="hljs-string">&quot;speech&quot;</span>] = speech</span>
<span class="hljs-meta">...</span> <span class="language-python">    <span class="hljs-keyword">return</span> batch</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">ds = ds.<span class="hljs-built_in">map</span>(map_to_array)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">inputs = processor(ds[<span class="hljs-string">&quot;speech&quot;</span>][<span class="hljs-number">0</span>], sampling_rate=<span class="hljs-number">16_000</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">generated_ids = model.generate(input_ids=inputs[<span class="hljs-string">&quot;input_features&quot;</span>], attention_mask=inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>])</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">transcription = processor.batch_decode(generated_ids)</span>`}}),tt=new En({props:{code:`import torch
from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
from datasets import load_dataset
import soundfile as sf

model = Speech2TextForConditionalGeneration.from_pretrained("facebook/s2t-medium-mustc-multilingual-st")
processor = Speech2TextProcessor.from_pretrained("facebook/s2t-medium-mustc-multilingual-st")

def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch

ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

inputs = processor(ds["speech"][0], sampling_rate=16_000, return_tensors="pt")
generated_ids = model.generate(input_ids=inputs["input_features"], attention_mask=inputs["attention_mask], forced_bos_token_id=processor.tokenizer.lang_code_to_id["fr"])

translation = processor.batch_decode(generated_ids),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> torch</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2TextProcessor, Speech2TextForConditionalGeneration</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model = Speech2TextForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-medium-mustc-multilingual-st&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">processor = Speech2TextProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-medium-mustc-multilingual-st&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">map_to_array</span>(<span class="hljs-params">batch</span>):</span>
<span class="hljs-meta">...</span> <span class="language-python">    speech, _ = sf.read(batch[<span class="hljs-string">&quot;file&quot;</span>])</span>
<span class="hljs-meta">...</span> <span class="language-python">    batch[<span class="hljs-string">&quot;speech&quot;</span>] = speech</span>
<span class="hljs-meta">...</span> <span class="language-python">    <span class="hljs-keyword">return</span> batch</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">ds = ds.<span class="hljs-built_in">map</span>(map_to_array)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">inputs = processor(ds[<span class="hljs-string">&quot;speech&quot;</span>][<span class="hljs-number">0</span>], sampling_rate=<span class="hljs-number">16_000</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">generated_ids = model.generate(input_ids=inputs[<span class="hljs-string">&quot;input_features&quot;</span>], attention_mask=inputs[<span class="hljs-string">&quot;attention_mask], forced_bos_token_id=processor.tokenizer.lang_code_to_id[&quot;</span><span class="hljs-string">fr&quot;])</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-string">translation = processor.batch_decode(generated_ids)</span></span>`}}),nt=new xe({}),st=new C({props:{name:"class transformers.Speech2TextConfig",anchor:"transformers.Speech2TextConfig",parameters:[{name:"vocab_size",val:" = 10000"},{name:"encoder_layers",val:" = 12"},{name:"encoder_ffn_dim",val:" = 2048"},{name:"encoder_attention_heads",val:" = 4"},{name:"decoder_layers",val:" = 6"},{name:"decoder_ffn_dim",val:" = 2048"},{name:"decoder_attention_heads",val:" = 4"},{name:"encoder_layerdrop",val:" = 0.0"},{name:"decoder_layerdrop",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"is_encoder_decoder",val:" = True"},{name:"activation_function",val:" = 'relu'"},{name:"d_model",val:" = 256"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.0"},{name:"activation_dropout",val:" = 0.0"},{name:"init_std",val:" = 0.02"},{name:"decoder_start_token_id",val:" = 2"},{name:"classifier_dropout",val:" = 0.0"},{name:"scale_embedding",val:" = True"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"max_source_positions",val:" = 6000"},{name:"max_target_positions",val:" = 1024"},{name:"num_conv_layers",val:" = 2"},{name:"conv_kernel_sizes",val:" = (5, 5)"},{name:"conv_channels",val:" = 1024"},{name:"input_feat_per_channel",val:" = 80"},{name:"input_channels",val:" = 1"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/configuration_speech_to_text.py#L29",parametersDescription:[{anchor:"transformers.Speech2TextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50265) &#x2014;
Vocabulary size of the Speech2Text model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a>`,name:"vocab_size"},{anchor:"transformers.Speech2TextConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.Speech2TextConfig.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"encoder_layers"},{anchor:"transformers.Speech2TextConfig.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.Speech2TextConfig.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.Speech2TextConfig.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.Speech2TextConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.Speech2TextConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.Speech2TextConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string,
<code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.Speech2TextConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.Speech2TextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.Speech2TextConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.Speech2TextConfig.classifier_dropout",description:`<strong>classifier_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for classifier.`,name:"classifier_dropout"},{anchor:"transformers.Speech2TextConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated<em>normal_initializer for initializing all weight matrices.
encoder_layerdrop &#x2014; (<code>float</code>, _optional</em>, defaults to 0.0):
The LayerDrop probability for the encoder. See the [LayerDrop paper](see
<a href="https://arxiv.org/abs/1909.11556" rel="nofollow">https://arxiv.org/abs/1909.11556</a>) for more details.
decoder<em>layerdrop &#x2014; (<code>float</code>, _optional</em>, defaults to 0.0):
The LayerDrop probability for the decoder. See the [LayerDrop paper](see
<a href="https://arxiv.org/abs/1909.11556" rel="nofollow">https://arxiv.org/abs/1909.11556</a>) for more details.`,name:"init_std"},{anchor:"transformers.Speech2TextConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.Speech2TextConfig.max_source_positions",description:`<strong>max_source_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 6000) &#x2014;
The maximum sequence length of log-mel filter-bank features that this model might ever be used with.`,name:"max_source_positions"},{anchor:"transformers.Speech2TextConfig.max_target_positions",description:`<strong>max_target_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_target_positions"},{anchor:"transformers.Speech2TextConfig.num_conv_layers",description:`<strong>num_conv_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of 1D convolutional layers in the conv module.`,name:"num_conv_layers"},{anchor:"transformers.Speech2TextConfig.conv_kernel_sizes",description:`<strong>conv_kernel_sizes</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(5, 5)</code>) &#x2014;
A tuple of integers defining the kernel size of each 1D convolutional layer in the conv module. The length
of <code>conv_kernel_sizes</code> has to match <code>num_conv_layers</code>.`,name:"conv_kernel_sizes"},{anchor:"transformers.Speech2TextConfig.conv_channels",description:`<strong>conv_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
An integer defining the number of output channels of each convolution layers except the final one in the
conv module.`,name:"conv_channels"},{anchor:"transformers.Speech2TextConfig.input_feat_per_channel",description:`<strong>input_feat_per_channel</strong> (<code>int</code>, <em>optional</em>, defaults to 80) &#x2014;
An integer specifying the size of feature vector. This is also the dimensions of log-mel filter-bank
features.`,name:"input_feat_per_channel"},{anchor:"transformers.Speech2TextConfig.input_channels",description:`<strong>input_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
An integer specifying number of input channels of the input feature vector.`,name:"input_channels"}]}}),rt=new En({props:{code:`from transformers import Speech2TextModel, Speech2TextConfig

# Initializing a Speech2Text s2t_transformer_s style configuration
configuration = Speech2TextConfig()

# Initializing a model from the s2t_transformer_s style configuration
model = Speech2TextModel(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2TextModel, Speech2TextConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Speech2Text s2t_transformer_s style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Speech2TextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the s2t_transformer_s style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Speech2TextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),it=new xe({}),ct=new C({props:{name:"class transformers.Speech2TextTokenizer",anchor:"transformers.Speech2TextTokenizer",parameters:[{name:"vocab_file",val:""},{name:"spm_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"unk_token",val:" = '<unk>'"},{name:"do_upper_case",val:" = False"},{name:"do_lower_case",val:" = False"},{name:"tgt_lang",val:" = None"},{name:"lang_codes",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/tokenization_speech_to_text.py#L55",parametersDescription:[{anchor:"transformers.Speech2TextTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.Speech2TextTokenizer.spm_file",description:`<strong>spm_file</strong> (<code>str</code>) &#x2014;
Path to the <a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> model file`,name:"spm_file"},{anchor:"transformers.Speech2TextTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sentence token.`,name:"bos_token"},{anchor:"transformers.Speech2TextTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sentence token.`,name:"eos_token"},{anchor:"transformers.Speech2TextTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.Speech2TextTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.Speech2TextTokenizer.do_upper_case",description:`<strong>do_upper_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to uppercase the output when decoding.`,name:"do_upper_case"},{anchor:"transformers.Speech2TextTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.Speech2TextTokenizer.tgt_lang",description:`<strong>tgt_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
A string representing the target language.`,name:"tgt_lang"},{anchor:"transformers.Speech2TextTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for SentencePiece</a> can be used, among other things, to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"}]}}),lt=new C({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.Speech2TextTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:""},{name:"token_ids_1",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/tokenization_speech_to_text.py#L194"}}),pt=new C({props:{name:"get_special_tokens_mask",anchor:"transformers.Speech2TextTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/tokenization_speech_to_text.py#L201",parametersDescription:[{anchor:"transformers.Speech2TextTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.Speech2TextTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.Speech2TextTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new C({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py#L2806",parametersDescription:[{anchor:"transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],returnDescription:`
<p>The token type ids.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ft=new xe({}),ut=new C({props:{name:"class transformers.Speech2TextFeatureExtractor",anchor:"transformers.Speech2TextFeatureExtractor",parameters:[{name:"feature_size",val:" = 80"},{name:"sampling_rate",val:" = 16000"},{name:"num_mel_bins",val:" = 80"},{name:"padding_value",val:" = 0.0"},{name:"do_ceptral_normalize",val:" = True"},{name:"normalize_means",val:" = True"},{name:"normalize_vars",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py#L34",parametersDescription:[{anchor:"transformers.Speech2TextFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>, defaults to 80) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.Speech2TextFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, defaults to 16000) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in Hertz per second (Hz).`,name:"sampling_rate"},{anchor:"transformers.Speech2TextFeatureExtractor.num_mel_bins",description:`<strong>num_mel_bins</strong> (<code>int</code>, defaults to 80) &#x2014;
Number of Mel-frequency bins.`,name:"num_mel_bins"},{anchor:"transformers.Speech2TextFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>, defaults to 0.0) &#x2014;
The value that is used to fill the padding vectors.`,name:"padding_value"},{anchor:"transformers.Speech2TextFeatureExtractor.do_ceptral_normalize",description:`<strong>do_ceptral_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to apply utterance-level cepstral mean and variance normalization to extracted features.`,name:"do_ceptral_normalize"},{anchor:"transformers.Speech2TextFeatureExtractor.normalize_means",description:`<strong>normalize_means</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to zero-mean normalize the extracted features.`,name:"normalize_means"},{anchor:"transformers.Speech2TextFeatureExtractor.normalize_vars",description:`<strong>normalize_vars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to unit-variance normalize the extracted features.`,name:"normalize_vars"}]}}),gt=new C({props:{name:"__call__",anchor:"transformers.Speech2TextFeatureExtractor.__call__",parameters:[{name:"raw_speech",val:": typing.Union[numpy.ndarray, typing.List[float], typing.List[numpy.ndarray], typing.List[typing.List[float]]]"},{name:"padding",val:": typing.Union[bool, str, transformers.file_utils.PaddingStrategy] = False"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": typing.Optional[int] = None"},{name:"return_tensors",val:": typing.Union[str, transformers.file_utils.TensorType, NoneType] = None"},{name:"sampling_rate",val:": typing.Optional[int] = None"},{name:"return_attention_mask",val:": typing.Optional[bool] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/feature_extraction_speech_to_text.py#L127",parametersDescription:[{anchor:"transformers.Speech2TextFeatureExtractor.__call__.raw_speech",description:`<strong>raw_speech</strong> (<code>np.ndarray</code>, <code>List[float]</code>, <code>List[np.ndarray]</code>, <code>List[List[float]]</code>) &#x2014;
The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float
values, a list of numpy arrays or a list of list of float values.`,name:"raw_speech"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/master/en/internal/file_utils#transformers.file_utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a
single sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the
maximum acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of
different lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <em>max_length</em> to <em>max_length</em>.`,name:"truncation"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability</p>
<blockquote>
<p>= 7.5 (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.</p>
</blockquote>`,name:"pad_to_multiple_of"},{anchor:"transformers.Speech2TextFeatureExtractor.__call__.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}]}}),vt=new xe({}),Tt=new C({props:{name:"class transformers.Speech2TextProcessor",anchor:"transformers.Speech2TextProcessor",parameters:[{name:"feature_extractor",val:""},{name:"tokenizer",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/processing_speech_to_text.py#L24",parametersDescription:[{anchor:"transformers.Speech2TextProcessor.feature_extractor",description:`<strong>feature_extractor</strong> (<code>Speech2TextFeatureExtractor</code>) &#x2014;
An instance of <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor">Speech2TextFeatureExtractor</a>. The feature extractor is a required
input.`,name:"feature_extractor"},{anchor:"transformers.Speech2TextProcessor.tokenizer",description:`<strong>tokenizer</strong> (<code>Speech2TextTokenizer</code>) &#x2014;
An instance of <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer">Speech2TextTokenizer</a>. The tokenizer is a required input.`,name:"tokenizer"}]}}),kt=new C({props:{name:"__call__",anchor:"transformers.Speech2TextProcessor.__call__",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/processing_speech_to_text.py#L110"}}),St=new C({props:{name:"from_pretrained",anchor:"transformers.Speech2TextProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/processing_speech_to_text.py#L77",parametersDescription:[{anchor:"transformers.Speech2TextProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<code>save_pretrained</code> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.
**kwargs &#x2014;
Additional keyword arguments passed along to both <code>PreTrainedFeatureExtractor</code> and
<a href="/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a></li>
</ul>`,name:"pretrained_model_name_or_path"}]}}),Ce=new zs({props:{$$slots:{default:[fl]},$$scope:{ctx:Y}}}),$t=new C({props:{name:"save_pretrained",anchor:"transformers.Speech2TextProcessor.save_pretrained",parameters:[{name:"save_directory",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/processing_speech_to_text.py#L56",parametersDescription:[{anchor:"transformers.Speech2TextProcessor.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file and the tokenizer files will be saved (directory will
be created if it does not exist).`,name:"save_directory"}]}}),Fe=new zs({props:{$$slots:{default:[ul]},$$scope:{ctx:Y}}}),zt=new C({props:{name:"batch_decode",anchor:"transformers.Speech2TextProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/processing_speech_to_text.py#L120"}}),qt=new C({props:{name:"decode",anchor:"transformers.Speech2TextProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/processing_speech_to_text.py#L128"}}),jt=new C({props:{name:"as_target_processor",anchor:"transformers.Speech2TextProcessor.as_target_processor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/processing_speech_to_text.py#L136"}}),Ct=new xe({}),Ft=new C({props:{name:"class transformers.Speech2TextModel",anchor:"transformers.Speech2TextModel",parameters:[{name:"config",val:": Speech2TextConfig"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1128",parametersDescription:[{anchor:"transformers.Speech2TextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),It=new C({props:{name:"forward",anchor:"transformers.Speech2TextModel.forward",parameters:[{name:"input_features",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1150",parametersDescription:[{anchor:"transformers.Speech2TextModel.forward.input_features",description:`<strong>input_features</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length, feature_size)</code>) &#x2014;
Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained
by loading a <code>.flac</code> or <code>.wav</code> audio file into an array of type <code>List[float]</code> or a
<code>numpy.ndarray</code>, <em>e.g.</em> via the soundfile library (<code>pip install soundfile</code>). To prepare the array
into <code>input_features</code>, the <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer">Speech2TextTokenizer</a> should be used for extracting
the fbank features, padding and conversion into a tensor of type <code>torch.FloatTensor</code>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__"><strong>call</strong>()</a>`,name:"input_features"},{anchor:"transformers.Speech2TextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Speech2TextModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>SpeechToTextTokenizer</code>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>SpeechToText uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.Speech2TextModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.</p>
<p>If you want to change padding behavior, you should read
<code>modeling_speech_to_text._prepare_decoder_inputs</code> and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the
paper</a> for more information on the default strategy.`,name:"decoder_attention_mask"},{anchor:"transformers.Speech2TextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Speech2TextModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.Speech2TextModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.Speech2TextModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>:
<code>attentions</code>) <code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>,
<em>optional</em>) is a sequence of hidden-states at the output of the last layer of the encoder. Used in the
cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.Speech2TextModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all \`<code>decoder_input_ids\`\`\` of shape </code>(batch<em>size, sequence_length)<code>. - **decoder_inputs_embeds** (</code>torch.FloatTensor<code>of shape</code>(batch_size, target_sequence_length, hidden_size)\`, _optional</em>) &#x2014; Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded representation. If <code>past_key_values</code> is used, optionally only the last <code>decoder_inputs_embeds</code> have to be input (see <code>past_key_values</code>). This is useful if you want more control over how to convert <code>decoder_input_ids</code> indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>decoder_input_ids</code> and <code>decoder_inputs_embeds</code> are both unset, <code>decoder_inputs_embeds</code>
takes the value of <code>inputs_embeds</code>.`,name:"past_key_values"},{anchor:"transformers.Speech2TextModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Speech2TextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Speech2TextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Speech2TextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>Seq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig"
>Speech2TextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>Seq2SeqModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ne=new zs({props:{$$slots:{default:[_l]},$$scope:{ctx:Y}}}),Nt=new En({props:{code:`from transformers import Speech2TextTokenizer, Speech2TextModel
import torch

tokenizer = Speech2TextTokenizer.from_pretrained('facebook/s2t-small-librispeech-asr')
model = Speech2TextModel.from_pretrained('facebook/s2t-small-librispeech-asr')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2TextTokenizer, Speech2TextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = Speech2TextTokenizer.from_pretrained(<span class="hljs-string">&#x27;facebook/s2t-small-librispeech-asr&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Speech2TextModel.from_pretrained(<span class="hljs-string">&#x27;facebook/s2t-small-librispeech-asr&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Lt=new xe({}),Ot=new C({props:{name:"class transformers.Speech2TextForConditionalGeneration",anchor:"transformers.Speech2TextForConditionalGeneration",parameters:[{name:"config",val:": Speech2TextConfig"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1241",parametersDescription:[{anchor:"transformers.Speech2TextForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ut=new C({props:{name:"forward",anchor:"transformers.Speech2TextForConditionalGeneration.forward",parameters:[{name:"input_features",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/speech_to_text/modeling_speech_to_text.py#L1278",parametersDescription:[{anchor:"transformers.Speech2TextForConditionalGeneration.forward.input_features",description:`<strong>input_features</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length, feature_size)</code>) &#x2014;
Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained
by loading a <code>.flac</code> or <code>.wav</code> audio file into an array of type <code>List[float]</code> or a
<code>numpy.ndarray</code>, <em>e.g.</em> via the soundfile library (<code>pip install soundfile</code>). To prepare the array
into <code>input_features</code>, the <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer">Speech2TextTokenizer</a> should be used for extracting
the fbank features, padding and conversion into a tensor of type <code>torch.FloatTensor</code>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__"><strong>call</strong>()</a>`,name:"input_features"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>SpeechToTextTokenizer</code>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>SpeechToText uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.</p>
<p>If you want to change padding behavior, you should read
<code>modeling_speech_to_text._prepare_decoder_inputs</code> and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the
paper</a> for more information on the default strategy.`,name:"decoder_attention_mask"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>:
<code>attentions</code>) <code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>,
<em>optional</em>) is a sequence of hidden-states at the output of the last layer of the encoder. Used in the
cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all \`<code>decoder_input_ids\`\`\` of shape </code>(batch<em>size, sequence_length)<code>. - **decoder_inputs_embeds** (</code>torch.FloatTensor<code>of shape</code>(batch_size, target_sequence_length, hidden_size)\`, _optional</em>) &#x2014; Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded representation. If <code>past_key_values</code> is used, optionally only the last <code>decoder_inputs_embeds</code> have to be input (see <code>past_key_values</code>). This is useful if you want more control over how to convert <code>decoder_input_ids</code> indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>decoder_input_ids</code> and <code>decoder_inputs_embeds</code> are both unset, <code>decoder_inputs_embeds</code>
takes the value of <code>inputs_embeds</code>.`,name:"past_key_values"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Speech2TextForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig"
>Speech2TextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Oe=new zs({props:{$$slots:{default:[gl]},$$scope:{ctx:Y}}}),Bt=new En({props:{code:`import torch
from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
from datasets import load_dataset
import soundfile as sf

model = Speech2TextForConditionalGeneration.from_pretrained("facebook/s2t-small-librispeech-asr")
processor = Speech2TextProcessor.from_pretrained("facebook/s2t-small-librispeech-asr")

def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch

ds = load_dataset("hf-internal-testing/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

input_features = processor(ds["speech"][0], sampling_rate=16000, return_tensors="pt").input_features  # Batch size 1
generated_ids = model.generate(input_ids=input_features)

transcription = processor.batch_decode(generated_ids),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Speech2TextProcessor, Speech2TextForConditionalGeneration
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Speech2TextForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-small-librispeech-asr&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Speech2TextProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/s2t-small-librispeech-asr&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">map_to_array</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">&gt;&gt;&gt; </span>    speech, _ = sf.read(batch[<span class="hljs-string">&quot;file&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>    batch[<span class="hljs-string">&quot;speech&quot;</span>] = speech
<span class="hljs-meta">&gt;&gt;&gt; </span>    <span class="hljs-keyword">return</span> batch

<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.<span class="hljs-built_in">map</span>(map_to_array)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_features = processor(ds[<span class="hljs-string">&quot;speech&quot;</span>][<span class="hljs-number">0</span>], sampling_rate=<span class="hljs-number">16000</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_features  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(input_ids=input_features)

<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(generated_ids)`}}),{c(){m=a("meta"),w=d(),f=a("h1"),b=a("a"),z=a("span"),u(k.$$.fragment),y=d(),E=a("span"),F=n("Speech2Text"),$=d(),M=a("h2"),L=a("a"),qo=a("span"),u(Ue.$$.fragment),Es=d(),Po=a("span"),qs=n("Overview"),qn=d(),O=a("p"),Ps=n("The Speech2Text model was proposed in "),Be=a("a"),js=n("fairseq S2T: Fast Speech-to-Text Modeling with fairseq"),Cs=n(` by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. It\u2019s a
transformer-based seq2seq (encoder-decoder) model designed for end-to-end Automatic Speech Recognition (ASR) and Speech
Translation (ST). It uses a convolutional downsampler to reduce the length of speech inputs by 3/4th before they are
fed into the encoder. The model is trained with standard autoregressive cross-entropy loss and generates the
transcripts/translations autoregressively. Speech2Text has been fine-tuned on several datasets for ASR and ST:
`),He=a("a"),Fs=n("LibriSpeech"),Ms=n(", "),Re=a("a"),As=n("CoVoST 2"),Ds=n(", "),Je=a("a"),Is=n("MuST-C"),Ns=n("."),Pn=d(),Q=a("p"),Ls=n("This model was contributed by "),Ye=a("a"),Os=n("valhalla"),Gs=n(". The original code can be found "),Xe=a("a"),Ws=n("here"),Vs=n("."),jn=d(),ce=a("h2"),ke=a("a"),jo=a("span"),u(Ke.$$.fragment),Us=d(),Co=a("span"),Bs=n("Inference"),Cn=d(),be=a("p"),Hs=n(`Speech2Text is a speech model that accepts a float tensor of log-mel filter-bank features extracted from the speech
signal. It\u2019s a transformer-based seq2seq model, so the transcripts/translations are generated autoregressively. The
`),Fo=a("code"),Rs=n("generate()"),Js=n(" method can be used for inference."),Fn=d(),G=a("p"),Ys=n("The "),Rt=a("a"),Xs=n("Speech2TextFeatureExtractor"),Ks=n(` class is responsible for extracting the log-mel filter-bank
features. The `),Jt=a("a"),Qs=n("Speech2TextProcessor"),Zs=n(" wraps "),Yt=a("a"),ea=n("Speech2TextFeatureExtractor"),ta=n(` and
`),Xt=a("a"),oa=n("Speech2TextTokenizer"),na=n(` into a single instance to both extract the input features and decode the
predicted token ids.`),Mn=d(),q=a("p"),sa=n("The feature extractor depends on "),Mo=a("code"),aa=n("torchaudio"),ra=n(" and the tokenizer depends on "),Ao=a("code"),ia=n("sentencepiece"),ca=n(` so be sure to
install those packages before running the examples. You could either install those as extra speech dependencies with
`),Do=a("code"),da=n('pip install transformers"[speech, sentencepiece]"'),la=n(" or install the packages seperately with "),Io=a("code"),pa=n("pip install torchaudio sentencepiece"),ha=n(". Also "),No=a("code"),ma=n("torchaudio"),fa=n(" requires the development version of the "),Qe=a("a"),ua=n("libsndfile"),_a=n(` package which can be installed via a system package manager. On Ubuntu it can
be installed as follows: `),Lo=a("code"),ga=n("apt install libsndfile1-dev"),An=d(),Kt=a("ul"),Oo=a("li"),va=n("ASR and Speech Translation"),Dn=d(),u(Ze.$$.fragment),In=d(),Qt=a("ul"),et=a("li"),Go=a("p"),Ta=n("Multilingual speech translation"),xa=d(),A=a("p"),ka=n("For multilingual speech translation models, "),Wo=a("code"),ba=n("eos_token_id"),ya=n(" is used as the "),Vo=a("code"),Sa=n("decoder_start_token_id"),wa=n(` and
the target language id is forced as the first generated token. To force the target language id as the first
generated token, pass the `),Uo=a("code"),$a=n("forced_bos_token_id"),za=n(" parameter to the "),Bo=a("code"),Ea=n("generate()"),qa=n(` method. The following
example shows how to transate English speech to French text using the `),Ho=a("em"),Pa=n("facebook/s2t-medium-mustc-multilingual-st"),ja=n(`
checkpoint.`),Nn=d(),u(tt.$$.fragment),Ln=d(),ye=a("p"),Ca=n("See the "),ot=a("a"),Fa=n("model hub"),Ma=n(" to look for Speech2Text checkpoints."),On=d(),de=a("h2"),Se=a("a"),Ro=a("span"),u(nt.$$.fragment),Aa=d(),Jo=a("span"),Da=n("Speech2TextConfig"),Gn=d(),D=a("div"),u(st.$$.fragment),Ia=d(),le=a("p"),Na=n("This is the configuration class to store the configuration of a "),Zt=a("a"),La=n("Speech2TextModel"),Oa=n(`. It is used
to instantiate an Speech2Text model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Speech2Text
`),at=a("a"),Ga=n("facebook/s2t-small-librispeech-asr"),Wa=n(" architecture."),Va=d(),pe=a("p"),Ua=n("Configuration objects inherit from "),eo=a("a"),Ba=n("PretrainedConfig"),Ha=n(` and can be used to control the model
outputs. Read the documentation from `),to=a("a"),Ra=n("PretrainedConfig"),Ja=n(" for more information."),Ya=d(),Yo=a("p"),Xa=n("Example:"),Ka=d(),u(rt.$$.fragment),Wn=d(),he=a("h2"),we=a("a"),Xo=a("span"),u(it.$$.fragment),Qa=d(),Ko=a("span"),Za=n("Speech2TextTokenizer"),Vn=d(),P=a("div"),u(ct.$$.fragment),er=d(),Qo=a("p"),tr=n("Construct an Speech2Text tokenizer."),or=d(),dt=a("p"),nr=n("This tokenizer inherits from "),oo=a("a"),sr=n("PreTrainedTokenizer"),ar=n(` which contains some of the main methods.
Users should refer to the superclass for more information regarding such methods.`),rr=d(),$e=a("div"),u(lt.$$.fragment),ir=d(),Zo=a("p"),cr=n("Build model inputs from a sequence by appending eos_token_id."),dr=d(),ze=a("div"),u(pt.$$.fragment),lr=d(),ht=a("p"),pr=n(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),en=a("code"),hr=n("prepare_for_model"),mr=n(" method."),fr=d(),Z=a("div"),u(mt.$$.fragment),ur=d(),no=a("p"),_r=n("Create the token type IDs corresponding to the sequences passed. "),so=a("a"),gr=n("What are token type IDs?"),vr=d(),tn=a("p"),Tr=n("Should be overridden in a subclass if the model has a special way of building those."),xr=d(),on=a("div"),Un=d(),me=a("h2"),Ee=a("a"),nn=a("span"),u(ft.$$.fragment),kr=d(),sn=a("span"),br=n("Speech2TextFeatureExtractor"),Bn=d(),I=a("div"),u(ut.$$.fragment),yr=d(),an=a("p"),Sr=n("Constructs a Speech2Text feature extractor."),wr=d(),_t=a("p"),$r=n("This feature extractor inherits from "),ao=a("a"),zr=n("Speech2TextFeatureExtractor"),Er=n(` which contains most of the
main methods. Users should refer to this superclass for more information regarding those methods.`),qr=d(),rn=a("p"),Pr=n(`This class extracts mel-filter bank features from raw speech using TorchAudio and applies utterance-level cepstral
mean and variance normalization to the extracted features.`),jr=d(),qe=a("div"),u(gt.$$.fragment),Cr=d(),cn=a("p"),Fr=n("Main method to featurize and prepare for the model one or several sequence(s). sequences."),Hn=d(),fe=a("h2"),Pe=a("a"),dn=a("span"),u(vt.$$.fragment),Mr=d(),ln=a("span"),Ar=n("Speech2TextProcessor"),Rn=d(),S=a("div"),u(Tt.$$.fragment),Dr=d(),pn=a("p"),Ir=n(`Constructs a Speech2Text processor which wraps a Speech2Text feature extractor and a Speech2Text tokenizer into a
single processor.`),Nr=d(),W=a("p"),ro=a("a"),Lr=n("Speech2TextProcessor"),Or=n(` offers all the functionalities of
`),io=a("a"),Gr=n("Speech2TextFeatureExtractor"),Wr=n(" and "),co=a("a"),Vr=n("Speech2TextTokenizer"),Ur=n(`. See the
`),xt=a("a"),hn=a("strong"),Br=n("call"),Hr=n("()"),Rr=n(" and "),lo=a("a"),Jr=n("decode()"),Yr=n(` for more
information.`),Xr=d(),je=a("div"),u(kt.$$.fragment),Kr=d(),X=a("p"),Qr=n(`When used in normal mode, this method forwards all its arguments to Speech2TextFeatureExtractor\u2019s
`),bt=a("a"),mn=a("strong"),Zr=n("call"),ei=n("()"),ti=n(` and returns its output. If used in the context
`),po=a("a"),oi=n("as_target_processor()"),ni=n(` this method forwards all its arguments to
Speech2TextTokenizer\u2019s `),yt=a("a"),fn=a("strong"),si=n("call"),ai=n("()"),ri=n(`. Please refer to the doctsring of
the above two methods for more information.`),ii=d(),ee=a("div"),u(St.$$.fragment),ci=d(),wt=a("p"),di=n("Instantiate a "),ho=a("a"),li=n("Speech2TextProcessor"),pi=n(" from a pretrained Speech2Text processor."),hi=d(),u(Ce.$$.fragment),mi=d(),te=a("div"),u($t.$$.fragment),fi=d(),ue=a("p"),ui=n(`Save a Speech2Text feature extractor object and Speech2Text tokenizer object to the directory
`),un=a("code"),_i=n("save_directory"),gi=n(`, so that it can be re-loaded using the
`),mo=a("a"),vi=n("from_pretrained()"),Ti=n(" class method."),xi=d(),u(Fe.$$.fragment),ki=d(),Me=a("div"),u(zt.$$.fragment),bi=d(),Et=a("p"),yi=n(`This method forwards all its arguments to Speech2TextTokenizer\u2019s
`),fo=a("a"),Si=n("batch_decode()"),wi=n(`. Please refer to the docstring of this method for more
information.`),$i=d(),Ae=a("div"),u(qt.$$.fragment),zi=d(),Pt=a("p"),Ei=n(`This method forwards all its arguments to Speech2TextTokenizer\u2019s
`),uo=a("a"),qi=n("decode()"),Pi=n(`. Please refer to the docstring of this method for more
information.`),ji=d(),De=a("div"),u(jt.$$.fragment),Ci=d(),_n=a("p"),Fi=n(`Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning
Speech2Text.`),Jn=d(),_e=a("h2"),Ie=a("a"),gn=a("span"),u(Ct.$$.fragment),Mi=d(),vn=a("span"),Ai=n("Speech2TextModel"),Yn=d(),H=a("div"),u(Ft.$$.fragment),Di=d(),Mt=a("p"),Ii=n(`The bare Speech2Text Model outputting raw hidden-states without any specific head on top.
This model inherits from `),_o=a("a"),Ni=n("PreTrainedModel"),Li=n(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Oi=d(),At=a("p"),Gi=n("This model is also a PyTorch "),Dt=a("a"),Wi=n("torch.nn.Module"),Vi=n(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Ui=d(),V=a("div"),u(It.$$.fragment),Bi=d(),ge=a("p"),Hi=n("The "),go=a("a"),Ri=n("Speech2TextModel"),Ji=n(" forward method, overrides the "),Tn=a("code"),Yi=n("__call__"),Xi=n(" special method."),Ki=d(),u(Ne.$$.fragment),Qi=d(),xn=a("p"),Zi=n("Example:"),ec=d(),u(Nt.$$.fragment),Xn=d(),ve=a("h2"),Le=a("a"),kn=a("span"),u(Lt.$$.fragment),tc=d(),bn=a("span"),oc=n("Speech2TextForConditionalGeneration"),Kn=d(),R=a("div"),u(Ot.$$.fragment),nc=d(),Gt=a("p"),sc=n(`The Speech2Text Model with a language modeling head. Can be used for summarization.
This model inherits from `),vo=a("a"),ac=n("PreTrainedModel"),rc=n(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ic=d(),Wt=a("p"),cc=n("This model is also a PyTorch "),Vt=a("a"),dc=n("torch.nn.Module"),lc=n(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),pc=d(),U=a("div"),u(Ut.$$.fragment),hc=d(),Te=a("p"),mc=n("The "),To=a("a"),fc=n("Speech2TextForConditionalGeneration"),uc=n(" forward method, overrides the "),yn=a("code"),_c=n("__call__"),gc=n(" special method."),vc=d(),u(Oe.$$.fragment),Tc=d(),Sn=a("p"),xc=n("Example:"),kc=d(),u(Bt.$$.fragment),this.h()},l(t){const p=ml('[data-svelte="svelte-1phssyn"]',document.head);m=r(p,"META",{name:!0,content:!0}),p.forEach(o),w=l(t),f=r(t,"H1",{class:!0});var Ht=i(f);b=r(Ht,"A",{id:!0,class:!0,href:!0});var wn=i(b);z=r(wn,"SPAN",{});var $n=i(z);_(k.$$.fragment,$n),$n.forEach(o),wn.forEach(o),y=l(Ht),E=r(Ht,"SPAN",{});var zn=i(E);F=s(zn,"Speech2Text"),zn.forEach(o),Ht.forEach(o),$=l(t),M=r(t,"H2",{class:!0});var Zn=i(M);L=r(Zn,"A",{id:!0,class:!0,href:!0});var $c=i(L);qo=r($c,"SPAN",{});var zc=i(qo);_(Ue.$$.fragment,zc),zc.forEach(o),$c.forEach(o),Es=l(Zn),Po=r(Zn,"SPAN",{});var Ec=i(Po);qs=s(Ec,"Overview"),Ec.forEach(o),Zn.forEach(o),qn=l(t),O=r(t,"P",{});var oe=i(O);Ps=s(oe,"The Speech2Text model was proposed in "),Be=r(oe,"A",{href:!0,rel:!0});var qc=i(Be);js=s(qc,"fairseq S2T: Fast Speech-to-Text Modeling with fairseq"),qc.forEach(o),Cs=s(oe,` by Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, Juan Pino. It\u2019s a
transformer-based seq2seq (encoder-decoder) model designed for end-to-end Automatic Speech Recognition (ASR) and Speech
Translation (ST). It uses a convolutional downsampler to reduce the length of speech inputs by 3/4th before they are
fed into the encoder. The model is trained with standard autoregressive cross-entropy loss and generates the
transcripts/translations autoregressively. Speech2Text has been fine-tuned on several datasets for ASR and ST:
`),He=r(oe,"A",{href:!0,rel:!0});var Pc=i(He);Fs=s(Pc,"LibriSpeech"),Pc.forEach(o),Ms=s(oe,", "),Re=r(oe,"A",{href:!0,rel:!0});var jc=i(Re);As=s(jc,"CoVoST 2"),jc.forEach(o),Ds=s(oe,", "),Je=r(oe,"A",{href:!0,rel:!0});var Cc=i(Je);Is=s(Cc,"MuST-C"),Cc.forEach(o),Ns=s(oe,"."),oe.forEach(o),Pn=l(t),Q=r(t,"P",{});var xo=i(Q);Ls=s(xo,"This model was contributed by "),Ye=r(xo,"A",{href:!0,rel:!0});var Fc=i(Ye);Os=s(Fc,"valhalla"),Fc.forEach(o),Gs=s(xo,". The original code can be found "),Xe=r(xo,"A",{href:!0,rel:!0});var Mc=i(Xe);Ws=s(Mc,"here"),Mc.forEach(o),Vs=s(xo,"."),xo.forEach(o),jn=l(t),ce=r(t,"H2",{class:!0});var es=i(ce);ke=r(es,"A",{id:!0,class:!0,href:!0});var Ac=i(ke);jo=r(Ac,"SPAN",{});var Dc=i(jo);_(Ke.$$.fragment,Dc),Dc.forEach(o),Ac.forEach(o),Us=l(es),Co=r(es,"SPAN",{});var Ic=i(Co);Bs=s(Ic,"Inference"),Ic.forEach(o),es.forEach(o),Cn=l(t),be=r(t,"P",{});var ts=i(be);Hs=s(ts,`Speech2Text is a speech model that accepts a float tensor of log-mel filter-bank features extracted from the speech
signal. It\u2019s a transformer-based seq2seq model, so the transcripts/translations are generated autoregressively. The
`),Fo=r(ts,"CODE",{});var Nc=i(Fo);Rs=s(Nc,"generate()"),Nc.forEach(o),Js=s(ts," method can be used for inference."),ts.forEach(o),Fn=l(t),G=r(t,"P",{});var ne=i(G);Ys=s(ne,"The "),Rt=r(ne,"A",{href:!0});var Lc=i(Rt);Xs=s(Lc,"Speech2TextFeatureExtractor"),Lc.forEach(o),Ks=s(ne,` class is responsible for extracting the log-mel filter-bank
features. The `),Jt=r(ne,"A",{href:!0});var Oc=i(Jt);Qs=s(Oc,"Speech2TextProcessor"),Oc.forEach(o),Zs=s(ne," wraps "),Yt=r(ne,"A",{href:!0});var Gc=i(Yt);ea=s(Gc,"Speech2TextFeatureExtractor"),Gc.forEach(o),ta=s(ne,` and
`),Xt=r(ne,"A",{href:!0});var Wc=i(Xt);oa=s(Wc,"Speech2TextTokenizer"),Wc.forEach(o),na=s(ne,` into a single instance to both extract the input features and decode the
predicted token ids.`),ne.forEach(o),Mn=l(t),q=r(t,"P",{});var N=i(q);sa=s(N,"The feature extractor depends on "),Mo=r(N,"CODE",{});var Vc=i(Mo);aa=s(Vc,"torchaudio"),Vc.forEach(o),ra=s(N," and the tokenizer depends on "),Ao=r(N,"CODE",{});var Uc=i(Ao);ia=s(Uc,"sentencepiece"),Uc.forEach(o),ca=s(N,` so be sure to
install those packages before running the examples. You could either install those as extra speech dependencies with
`),Do=r(N,"CODE",{});var Bc=i(Do);da=s(Bc,'pip install transformers"[speech, sentencepiece]"'),Bc.forEach(o),la=s(N," or install the packages seperately with "),Io=r(N,"CODE",{});var Hc=i(Io);pa=s(Hc,"pip install torchaudio sentencepiece"),Hc.forEach(o),ha=s(N,". Also "),No=r(N,"CODE",{});var Rc=i(No);ma=s(Rc,"torchaudio"),Rc.forEach(o),fa=s(N," requires the development version of the "),Qe=r(N,"A",{href:!0,rel:!0});var Jc=i(Qe);ua=s(Jc,"libsndfile"),Jc.forEach(o),_a=s(N,` package which can be installed via a system package manager. On Ubuntu it can
be installed as follows: `),Lo=r(N,"CODE",{});var Yc=i(Lo);ga=s(Yc,"apt install libsndfile1-dev"),Yc.forEach(o),N.forEach(o),An=l(t),Kt=r(t,"UL",{});var Xc=i(Kt);Oo=r(Xc,"LI",{});var Kc=i(Oo);va=s(Kc,"ASR and Speech Translation"),Kc.forEach(o),Xc.forEach(o),Dn=l(t),_(Ze.$$.fragment,t),In=l(t),Qt=r(t,"UL",{});var Qc=i(Qt);et=r(Qc,"LI",{});var os=i(et);Go=r(os,"P",{});var Zc=i(Go);Ta=s(Zc,"Multilingual speech translation"),Zc.forEach(o),xa=l(os),A=r(os,"P",{});var J=i(A);ka=s(J,"For multilingual speech translation models, "),Wo=r(J,"CODE",{});var ed=i(Wo);ba=s(ed,"eos_token_id"),ed.forEach(o),ya=s(J," is used as the "),Vo=r(J,"CODE",{});var td=i(Vo);Sa=s(td,"decoder_start_token_id"),td.forEach(o),wa=s(J,` and
the target language id is forced as the first generated token. To force the target language id as the first
generated token, pass the `),Uo=r(J,"CODE",{});var od=i(Uo);$a=s(od,"forced_bos_token_id"),od.forEach(o),za=s(J," parameter to the "),Bo=r(J,"CODE",{});var nd=i(Bo);Ea=s(nd,"generate()"),nd.forEach(o),qa=s(J,` method. The following
example shows how to transate English speech to French text using the `),Ho=r(J,"EM",{});var sd=i(Ho);Pa=s(sd,"facebook/s2t-medium-mustc-multilingual-st"),sd.forEach(o),ja=s(J,`
checkpoint.`),J.forEach(o),os.forEach(o),Qc.forEach(o),Nn=l(t),_(tt.$$.fragment,t),Ln=l(t),ye=r(t,"P",{});var ns=i(ye);Ca=s(ns,"See the "),ot=r(ns,"A",{href:!0,rel:!0});var ad=i(ot);Fa=s(ad,"model hub"),ad.forEach(o),Ma=s(ns," to look for Speech2Text checkpoints."),ns.forEach(o),On=l(t),de=r(t,"H2",{class:!0});var ss=i(de);Se=r(ss,"A",{id:!0,class:!0,href:!0});var rd=i(Se);Ro=r(rd,"SPAN",{});var id=i(Ro);_(nt.$$.fragment,id),id.forEach(o),rd.forEach(o),Aa=l(ss),Jo=r(ss,"SPAN",{});var cd=i(Jo);Da=s(cd,"Speech2TextConfig"),cd.forEach(o),ss.forEach(o),Gn=l(t),D=r(t,"DIV",{class:!0});var se=i(D);_(st.$$.fragment,se),Ia=l(se),le=r(se,"P",{});var ko=i(le);Na=s(ko,"This is the configuration class to store the configuration of a "),Zt=r(ko,"A",{href:!0});var dd=i(Zt);La=s(dd,"Speech2TextModel"),dd.forEach(o),Oa=s(ko,`. It is used
to instantiate an Speech2Text model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Speech2Text
`),at=r(ko,"A",{href:!0,rel:!0});var ld=i(at);Ga=s(ld,"facebook/s2t-small-librispeech-asr"),ld.forEach(o),Wa=s(ko," architecture."),ko.forEach(o),Va=l(se),pe=r(se,"P",{});var bo=i(pe);Ua=s(bo,"Configuration objects inherit from "),eo=r(bo,"A",{href:!0});var pd=i(eo);Ba=s(pd,"PretrainedConfig"),pd.forEach(o),Ha=s(bo,` and can be used to control the model
outputs. Read the documentation from `),to=r(bo,"A",{href:!0});var hd=i(to);Ra=s(hd,"PretrainedConfig"),hd.forEach(o),Ja=s(bo," for more information."),bo.forEach(o),Ya=l(se),Yo=r(se,"P",{});var md=i(Yo);Xa=s(md,"Example:"),md.forEach(o),Ka=l(se),_(rt.$$.fragment,se),se.forEach(o),Wn=l(t),he=r(t,"H2",{class:!0});var as=i(he);we=r(as,"A",{id:!0,class:!0,href:!0});var fd=i(we);Xo=r(fd,"SPAN",{});var ud=i(Xo);_(it.$$.fragment,ud),ud.forEach(o),fd.forEach(o),Qa=l(as),Ko=r(as,"SPAN",{});var _d=i(Ko);Za=s(_d,"Speech2TextTokenizer"),_d.forEach(o),as.forEach(o),Vn=l(t),P=r(t,"DIV",{class:!0});var B=i(P);_(ct.$$.fragment,B),er=l(B),Qo=r(B,"P",{});var gd=i(Qo);tr=s(gd,"Construct an Speech2Text tokenizer."),gd.forEach(o),or=l(B),dt=r(B,"P",{});var rs=i(dt);nr=s(rs,"This tokenizer inherits from "),oo=r(rs,"A",{href:!0});var vd=i(oo);sr=s(vd,"PreTrainedTokenizer"),vd.forEach(o),ar=s(rs,` which contains some of the main methods.
Users should refer to the superclass for more information regarding such methods.`),rs.forEach(o),rr=l(B),$e=r(B,"DIV",{class:!0});var is=i($e);_(lt.$$.fragment,is),ir=l(is),Zo=r(is,"P",{});var Td=i(Zo);cr=s(Td,"Build model inputs from a sequence by appending eos_token_id."),Td.forEach(o),is.forEach(o),dr=l(B),ze=r(B,"DIV",{class:!0});var cs=i(ze);_(pt.$$.fragment,cs),lr=l(cs),ht=r(cs,"P",{});var ds=i(ht);pr=s(ds,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),en=r(ds,"CODE",{});var xd=i(en);hr=s(xd,"prepare_for_model"),xd.forEach(o),mr=s(ds," method."),ds.forEach(o),cs.forEach(o),fr=l(B),Z=r(B,"DIV",{class:!0});var yo=i(Z);_(mt.$$.fragment,yo),ur=l(yo),no=r(yo,"P",{});var bc=i(no);_r=s(bc,"Create the token type IDs corresponding to the sequences passed. "),so=r(bc,"A",{href:!0});var kd=i(so);gr=s(kd,"What are token type IDs?"),kd.forEach(o),bc.forEach(o),vr=l(yo),tn=r(yo,"P",{});var bd=i(tn);Tr=s(bd,"Should be overridden in a subclass if the model has a special way of building those."),bd.forEach(o),yo.forEach(o),xr=l(B),on=r(B,"DIV",{class:!0}),i(on).forEach(o),B.forEach(o),Un=l(t),me=r(t,"H2",{class:!0});var ls=i(me);Ee=r(ls,"A",{id:!0,class:!0,href:!0});var yd=i(Ee);nn=r(yd,"SPAN",{});var Sd=i(nn);_(ft.$$.fragment,Sd),Sd.forEach(o),yd.forEach(o),kr=l(ls),sn=r(ls,"SPAN",{});var wd=i(sn);br=s(wd,"Speech2TextFeatureExtractor"),wd.forEach(o),ls.forEach(o),Bn=l(t),I=r(t,"DIV",{class:!0});var ae=i(I);_(ut.$$.fragment,ae),yr=l(ae),an=r(ae,"P",{});var $d=i(an);Sr=s($d,"Constructs a Speech2Text feature extractor."),$d.forEach(o),wr=l(ae),_t=r(ae,"P",{});var ps=i(_t);$r=s(ps,"This feature extractor inherits from "),ao=r(ps,"A",{href:!0});var zd=i(ao);zr=s(zd,"Speech2TextFeatureExtractor"),zd.forEach(o),Er=s(ps,` which contains most of the
main methods. Users should refer to this superclass for more information regarding those methods.`),ps.forEach(o),qr=l(ae),rn=r(ae,"P",{});var Ed=i(rn);Pr=s(Ed,`This class extracts mel-filter bank features from raw speech using TorchAudio and applies utterance-level cepstral
mean and variance normalization to the extracted features.`),Ed.forEach(o),jr=l(ae),qe=r(ae,"DIV",{class:!0});var hs=i(qe);_(gt.$$.fragment,hs),Cr=l(hs),cn=r(hs,"P",{});var qd=i(cn);Fr=s(qd,"Main method to featurize and prepare for the model one or several sequence(s). sequences."),qd.forEach(o),hs.forEach(o),ae.forEach(o),Hn=l(t),fe=r(t,"H2",{class:!0});var ms=i(fe);Pe=r(ms,"A",{id:!0,class:!0,href:!0});var Pd=i(Pe);dn=r(Pd,"SPAN",{});var jd=i(dn);_(vt.$$.fragment,jd),jd.forEach(o),Pd.forEach(o),Mr=l(ms),ln=r(ms,"SPAN",{});var Cd=i(ln);Ar=s(Cd,"Speech2TextProcessor"),Cd.forEach(o),ms.forEach(o),Rn=l(t),S=r(t,"DIV",{class:!0});var j=i(S);_(Tt.$$.fragment,j),Dr=l(j),pn=r(j,"P",{});var Fd=i(pn);Ir=s(Fd,`Constructs a Speech2Text processor which wraps a Speech2Text feature extractor and a Speech2Text tokenizer into a
single processor.`),Fd.forEach(o),Nr=l(j),W=r(j,"P",{});var K=i(W);ro=r(K,"A",{href:!0});var Md=i(ro);Lr=s(Md,"Speech2TextProcessor"),Md.forEach(o),Or=s(K,` offers all the functionalities of
`),io=r(K,"A",{href:!0});var Ad=i(io);Gr=s(Ad,"Speech2TextFeatureExtractor"),Ad.forEach(o),Wr=s(K," and "),co=r(K,"A",{href:!0});var Dd=i(co);Vr=s(Dd,"Speech2TextTokenizer"),Dd.forEach(o),Ur=s(K,`. See the
`),xt=r(K,"A",{href:!0});var yc=i(xt);hn=r(yc,"STRONG",{});var Id=i(hn);Br=s(Id,"call"),Id.forEach(o),Hr=s(yc,"()"),yc.forEach(o),Rr=s(K," and "),lo=r(K,"A",{href:!0});var Nd=i(lo);Jr=s(Nd,"decode()"),Nd.forEach(o),Yr=s(K,` for more
information.`),K.forEach(o),Xr=l(j),je=r(j,"DIV",{class:!0});var fs=i(je);_(kt.$$.fragment,fs),Kr=l(fs),X=r(fs,"P",{});var Ge=i(X);Qr=s(Ge,`When used in normal mode, this method forwards all its arguments to Speech2TextFeatureExtractor\u2019s
`),bt=r(Ge,"A",{href:!0});var Sc=i(bt);mn=r(Sc,"STRONG",{});var Ld=i(mn);Zr=s(Ld,"call"),Ld.forEach(o),ei=s(Sc,"()"),Sc.forEach(o),ti=s(Ge,` and returns its output. If used in the context
`),po=r(Ge,"A",{href:!0});var Od=i(po);oi=s(Od,"as_target_processor()"),Od.forEach(o),ni=s(Ge,` this method forwards all its arguments to
Speech2TextTokenizer\u2019s `),yt=r(Ge,"A",{href:!0});var wc=i(yt);fn=r(wc,"STRONG",{});var Gd=i(fn);si=s(Gd,"call"),Gd.forEach(o),ai=s(wc,"()"),wc.forEach(o),ri=s(Ge,`. Please refer to the doctsring of
the above two methods for more information.`),Ge.forEach(o),fs.forEach(o),ii=l(j),ee=r(j,"DIV",{class:!0});var So=i(ee);_(St.$$.fragment,So),ci=l(So),wt=r(So,"P",{});var us=i(wt);di=s(us,"Instantiate a "),ho=r(us,"A",{href:!0});var Wd=i(ho);li=s(Wd,"Speech2TextProcessor"),Wd.forEach(o),pi=s(us," from a pretrained Speech2Text processor."),us.forEach(o),hi=l(So),_(Ce.$$.fragment,So),So.forEach(o),mi=l(j),te=r(j,"DIV",{class:!0});var wo=i(te);_($t.$$.fragment,wo),fi=l(wo),ue=r(wo,"P",{});var $o=i(ue);ui=s($o,`Save a Speech2Text feature extractor object and Speech2Text tokenizer object to the directory
`),un=r($o,"CODE",{});var Vd=i(un);_i=s(Vd,"save_directory"),Vd.forEach(o),gi=s($o,`, so that it can be re-loaded using the
`),mo=r($o,"A",{href:!0});var Ud=i(mo);vi=s(Ud,"from_pretrained()"),Ud.forEach(o),Ti=s($o," class method."),$o.forEach(o),xi=l(wo),_(Fe.$$.fragment,wo),wo.forEach(o),ki=l(j),Me=r(j,"DIV",{class:!0});var _s=i(Me);_(zt.$$.fragment,_s),bi=l(_s),Et=r(_s,"P",{});var gs=i(Et);yi=s(gs,`This method forwards all its arguments to Speech2TextTokenizer\u2019s
`),fo=r(gs,"A",{href:!0});var Bd=i(fo);Si=s(Bd,"batch_decode()"),Bd.forEach(o),wi=s(gs,`. Please refer to the docstring of this method for more
information.`),gs.forEach(o),_s.forEach(o),$i=l(j),Ae=r(j,"DIV",{class:!0});var vs=i(Ae);_(qt.$$.fragment,vs),zi=l(vs),Pt=r(vs,"P",{});var Ts=i(Pt);Ei=s(Ts,`This method forwards all its arguments to Speech2TextTokenizer\u2019s
`),uo=r(Ts,"A",{href:!0});var Hd=i(uo);qi=s(Hd,"decode()"),Hd.forEach(o),Pi=s(Ts,`. Please refer to the docstring of this method for more
information.`),Ts.forEach(o),vs.forEach(o),ji=l(j),De=r(j,"DIV",{class:!0});var xs=i(De);_(jt.$$.fragment,xs),Ci=l(xs),_n=r(xs,"P",{});var Rd=i(_n);Fi=s(Rd,`Temporarily sets the tokenizer for processing the input. Useful for encoding the labels when fine-tuning
Speech2Text.`),Rd.forEach(o),xs.forEach(o),j.forEach(o),Jn=l(t),_e=r(t,"H2",{class:!0});var ks=i(_e);Ie=r(ks,"A",{id:!0,class:!0,href:!0});var Jd=i(Ie);gn=r(Jd,"SPAN",{});var Yd=i(gn);_(Ct.$$.fragment,Yd),Yd.forEach(o),Jd.forEach(o),Mi=l(ks),vn=r(ks,"SPAN",{});var Xd=i(vn);Ai=s(Xd,"Speech2TextModel"),Xd.forEach(o),ks.forEach(o),Yn=l(t),H=r(t,"DIV",{class:!0});var We=i(H);_(Ft.$$.fragment,We),Di=l(We),Mt=r(We,"P",{});var bs=i(Mt);Ii=s(bs,`The bare Speech2Text Model outputting raw hidden-states without any specific head on top.
This model inherits from `),_o=r(bs,"A",{href:!0});var Kd=i(_o);Ni=s(Kd,"PreTrainedModel"),Kd.forEach(o),Li=s(bs,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),bs.forEach(o),Oi=l(We),At=r(We,"P",{});var ys=i(At);Gi=s(ys,"This model is also a PyTorch "),Dt=r(ys,"A",{href:!0,rel:!0});var Qd=i(Dt);Wi=s(Qd,"torch.nn.Module"),Qd.forEach(o),Vi=s(ys,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),ys.forEach(o),Ui=l(We),V=r(We,"DIV",{class:!0});var re=i(V);_(It.$$.fragment,re),Bi=l(re),ge=r(re,"P",{});var zo=i(ge);Hi=s(zo,"The "),go=r(zo,"A",{href:!0});var Zd=i(go);Ri=s(Zd,"Speech2TextModel"),Zd.forEach(o),Ji=s(zo," forward method, overrides the "),Tn=r(zo,"CODE",{});var el=i(Tn);Yi=s(el,"__call__"),el.forEach(o),Xi=s(zo," special method."),zo.forEach(o),Ki=l(re),_(Ne.$$.fragment,re),Qi=l(re),xn=r(re,"P",{});var tl=i(xn);Zi=s(tl,"Example:"),tl.forEach(o),ec=l(re),_(Nt.$$.fragment,re),re.forEach(o),We.forEach(o),Xn=l(t),ve=r(t,"H2",{class:!0});var Ss=i(ve);Le=r(Ss,"A",{id:!0,class:!0,href:!0});var ol=i(Le);kn=r(ol,"SPAN",{});var nl=i(kn);_(Lt.$$.fragment,nl),nl.forEach(o),ol.forEach(o),tc=l(Ss),bn=r(Ss,"SPAN",{});var sl=i(bn);oc=s(sl,"Speech2TextForConditionalGeneration"),sl.forEach(o),Ss.forEach(o),Kn=l(t),R=r(t,"DIV",{class:!0});var Ve=i(R);_(Ot.$$.fragment,Ve),nc=l(Ve),Gt=r(Ve,"P",{});var ws=i(Gt);sc=s(ws,`The Speech2Text Model with a language modeling head. Can be used for summarization.
This model inherits from `),vo=r(ws,"A",{href:!0});var al=i(vo);ac=s(al,"PreTrainedModel"),al.forEach(o),rc=s(ws,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ws.forEach(o),ic=l(Ve),Wt=r(Ve,"P",{});var $s=i(Wt);cc=s($s,"This model is also a PyTorch "),Vt=r($s,"A",{href:!0,rel:!0});var rl=i(Vt);dc=s(rl,"torch.nn.Module"),rl.forEach(o),lc=s($s,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),$s.forEach(o),pc=l(Ve),U=r(Ve,"DIV",{class:!0});var ie=i(U);_(Ut.$$.fragment,ie),hc=l(ie),Te=r(ie,"P",{});var Eo=i(Te);mc=s(Eo,"The "),To=r(Eo,"A",{href:!0});var il=i(To);fc=s(il,"Speech2TextForConditionalGeneration"),il.forEach(o),uc=s(Eo," forward method, overrides the "),yn=r(Eo,"CODE",{});var cl=i(yn);_c=s(cl,"__call__"),cl.forEach(o),gc=s(Eo," special method."),Eo.forEach(o),vc=l(ie),_(Oe.$$.fragment,ie),Tc=l(ie),Sn=r(ie,"P",{});var dl=i(Sn);xc=s(dl,"Example:"),dl.forEach(o),kc=l(ie),_(Bt.$$.fragment,ie),ie.forEach(o),Ve.forEach(o),this.h()},h(){c(m,"name","hf:doc:metadata"),c(m,"content",JSON.stringify(Tl)),c(b,"id","speech2text"),c(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b,"href","#speech2text"),c(f,"class","relative group"),c(L,"id","overview"),c(L,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(L,"href","#overview"),c(M,"class","relative group"),c(Be,"href","https://arxiv.org/abs/2010.05171"),c(Be,"rel","nofollow"),c(He,"href","http://www.openslr.org/12"),c(He,"rel","nofollow"),c(Re,"href","https://github.com/facebookresearch/covost"),c(Re,"rel","nofollow"),c(Je,"href","https://ict.fbk.eu/must-c/"),c(Je,"rel","nofollow"),c(Ye,"href","https://huggingface.co/valhalla"),c(Ye,"rel","nofollow"),c(Xe,"href","https://github.com/pytorch/fairseq/tree/master/examples/speech_to_text"),c(Xe,"rel","nofollow"),c(ke,"id","inference"),c(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ke,"href","#inference"),c(ce,"class","relative group"),c(Rt,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(Jt,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(Yt,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(Xt,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(Qe,"href","http://www.mega-nerd.com/libsndfile/"),c(Qe,"rel","nofollow"),c(ot,"href","https://huggingface.co/models?filter=speech_to_text"),c(ot,"rel","nofollow"),c(Se,"id","transformers.Speech2TextConfig"),c(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Se,"href","#transformers.Speech2TextConfig"),c(de,"class","relative group"),c(Zt,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(at,"href","https://huggingface.co/facebook/s2t-small-librispeech-asr"),c(at,"rel","nofollow"),c(eo,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(to,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(D,"class","docstring"),c(we,"id","transformers.Speech2TextTokenizer"),c(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(we,"href","#transformers.Speech2TextTokenizer"),c(he,"class","relative group"),c(oo,"href","/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c($e,"class","docstring"),c(ze,"class","docstring"),c(so,"href","../glossary#token-type-ids"),c(Z,"class","docstring"),c(on,"class","docstring"),c(P,"class","docstring"),c(Ee,"id","transformers.Speech2TextFeatureExtractor"),c(Ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ee,"href","#transformers.Speech2TextFeatureExtractor"),c(me,"class","relative group"),c(ao,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(qe,"class","docstring"),c(I,"class","docstring"),c(Pe,"id","transformers.Speech2TextProcessor"),c(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pe,"href","#transformers.Speech2TextProcessor"),c(fe,"class","relative group"),c(ro,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(io,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(co,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(xt,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor.__call__"),c(lo,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor.decode"),c(bt,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor.__call__"),c(po,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor.as_target_processor"),c(yt,"href","/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__"),c(je,"class","docstring"),c(ho,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(ee,"class","docstring"),c(mo,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor.from_pretrained"),c(te,"class","docstring"),c(fo,"href","/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.batch_decode"),c(Me,"class","docstring"),c(uo,"href","/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.decode"),c(Ae,"class","docstring"),c(De,"class","docstring"),c(S,"class","docstring"),c(Ie,"id","transformers.Speech2TextModel"),c(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ie,"href","#transformers.Speech2TextModel"),c(_e,"class","relative group"),c(_o,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(Dt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Dt,"rel","nofollow"),c(go,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(V,"class","docstring"),c(H,"class","docstring"),c(Le,"id","transformers.Speech2TextForConditionalGeneration"),c(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Le,"href","#transformers.Speech2TextForConditionalGeneration"),c(ve,"class","relative group"),c(vo,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(Vt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Vt,"rel","nofollow"),c(To,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(U,"class","docstring"),c(R,"class","docstring")},m(t,p){e(document.head,m),h(t,w,p),h(t,f,p),e(f,b),e(b,z),g(k,z,null),e(f,y),e(f,E),e(E,F),h(t,$,p),h(t,M,p),e(M,L),e(L,qo),g(Ue,qo,null),e(M,Es),e(M,Po),e(Po,qs),h(t,qn,p),h(t,O,p),e(O,Ps),e(O,Be),e(Be,js),e(O,Cs),e(O,He),e(He,Fs),e(O,Ms),e(O,Re),e(Re,As),e(O,Ds),e(O,Je),e(Je,Is),e(O,Ns),h(t,Pn,p),h(t,Q,p),e(Q,Ls),e(Q,Ye),e(Ye,Os),e(Q,Gs),e(Q,Xe),e(Xe,Ws),e(Q,Vs),h(t,jn,p),h(t,ce,p),e(ce,ke),e(ke,jo),g(Ke,jo,null),e(ce,Us),e(ce,Co),e(Co,Bs),h(t,Cn,p),h(t,be,p),e(be,Hs),e(be,Fo),e(Fo,Rs),e(be,Js),h(t,Fn,p),h(t,G,p),e(G,Ys),e(G,Rt),e(Rt,Xs),e(G,Ks),e(G,Jt),e(Jt,Qs),e(G,Zs),e(G,Yt),e(Yt,ea),e(G,ta),e(G,Xt),e(Xt,oa),e(G,na),h(t,Mn,p),h(t,q,p),e(q,sa),e(q,Mo),e(Mo,aa),e(q,ra),e(q,Ao),e(Ao,ia),e(q,ca),e(q,Do),e(Do,da),e(q,la),e(q,Io),e(Io,pa),e(q,ha),e(q,No),e(No,ma),e(q,fa),e(q,Qe),e(Qe,ua),e(q,_a),e(q,Lo),e(Lo,ga),h(t,An,p),h(t,Kt,p),e(Kt,Oo),e(Oo,va),h(t,Dn,p),g(Ze,t,p),h(t,In,p),h(t,Qt,p),e(Qt,et),e(et,Go),e(Go,Ta),e(et,xa),e(et,A),e(A,ka),e(A,Wo),e(Wo,ba),e(A,ya),e(A,Vo),e(Vo,Sa),e(A,wa),e(A,Uo),e(Uo,$a),e(A,za),e(A,Bo),e(Bo,Ea),e(A,qa),e(A,Ho),e(Ho,Pa),e(A,ja),h(t,Nn,p),g(tt,t,p),h(t,Ln,p),h(t,ye,p),e(ye,Ca),e(ye,ot),e(ot,Fa),e(ye,Ma),h(t,On,p),h(t,de,p),e(de,Se),e(Se,Ro),g(nt,Ro,null),e(de,Aa),e(de,Jo),e(Jo,Da),h(t,Gn,p),h(t,D,p),g(st,D,null),e(D,Ia),e(D,le),e(le,Na),e(le,Zt),e(Zt,La),e(le,Oa),e(le,at),e(at,Ga),e(le,Wa),e(D,Va),e(D,pe),e(pe,Ua),e(pe,eo),e(eo,Ba),e(pe,Ha),e(pe,to),e(to,Ra),e(pe,Ja),e(D,Ya),e(D,Yo),e(Yo,Xa),e(D,Ka),g(rt,D,null),h(t,Wn,p),h(t,he,p),e(he,we),e(we,Xo),g(it,Xo,null),e(he,Qa),e(he,Ko),e(Ko,Za),h(t,Vn,p),h(t,P,p),g(ct,P,null),e(P,er),e(P,Qo),e(Qo,tr),e(P,or),e(P,dt),e(dt,nr),e(dt,oo),e(oo,sr),e(dt,ar),e(P,rr),e(P,$e),g(lt,$e,null),e($e,ir),e($e,Zo),e(Zo,cr),e(P,dr),e(P,ze),g(pt,ze,null),e(ze,lr),e(ze,ht),e(ht,pr),e(ht,en),e(en,hr),e(ht,mr),e(P,fr),e(P,Z),g(mt,Z,null),e(Z,ur),e(Z,no),e(no,_r),e(no,so),e(so,gr),e(Z,vr),e(Z,tn),e(tn,Tr),e(P,xr),e(P,on),h(t,Un,p),h(t,me,p),e(me,Ee),e(Ee,nn),g(ft,nn,null),e(me,kr),e(me,sn),e(sn,br),h(t,Bn,p),h(t,I,p),g(ut,I,null),e(I,yr),e(I,an),e(an,Sr),e(I,wr),e(I,_t),e(_t,$r),e(_t,ao),e(ao,zr),e(_t,Er),e(I,qr),e(I,rn),e(rn,Pr),e(I,jr),e(I,qe),g(gt,qe,null),e(qe,Cr),e(qe,cn),e(cn,Fr),h(t,Hn,p),h(t,fe,p),e(fe,Pe),e(Pe,dn),g(vt,dn,null),e(fe,Mr),e(fe,ln),e(ln,Ar),h(t,Rn,p),h(t,S,p),g(Tt,S,null),e(S,Dr),e(S,pn),e(pn,Ir),e(S,Nr),e(S,W),e(W,ro),e(ro,Lr),e(W,Or),e(W,io),e(io,Gr),e(W,Wr),e(W,co),e(co,Vr),e(W,Ur),e(W,xt),e(xt,hn),e(hn,Br),e(xt,Hr),e(W,Rr),e(W,lo),e(lo,Jr),e(W,Yr),e(S,Xr),e(S,je),g(kt,je,null),e(je,Kr),e(je,X),e(X,Qr),e(X,bt),e(bt,mn),e(mn,Zr),e(bt,ei),e(X,ti),e(X,po),e(po,oi),e(X,ni),e(X,yt),e(yt,fn),e(fn,si),e(yt,ai),e(X,ri),e(S,ii),e(S,ee),g(St,ee,null),e(ee,ci),e(ee,wt),e(wt,di),e(wt,ho),e(ho,li),e(wt,pi),e(ee,hi),g(Ce,ee,null),e(S,mi),e(S,te),g($t,te,null),e(te,fi),e(te,ue),e(ue,ui),e(ue,un),e(un,_i),e(ue,gi),e(ue,mo),e(mo,vi),e(ue,Ti),e(te,xi),g(Fe,te,null),e(S,ki),e(S,Me),g(zt,Me,null),e(Me,bi),e(Me,Et),e(Et,yi),e(Et,fo),e(fo,Si),e(Et,wi),e(S,$i),e(S,Ae),g(qt,Ae,null),e(Ae,zi),e(Ae,Pt),e(Pt,Ei),e(Pt,uo),e(uo,qi),e(Pt,Pi),e(S,ji),e(S,De),g(jt,De,null),e(De,Ci),e(De,_n),e(_n,Fi),h(t,Jn,p),h(t,_e,p),e(_e,Ie),e(Ie,gn),g(Ct,gn,null),e(_e,Mi),e(_e,vn),e(vn,Ai),h(t,Yn,p),h(t,H,p),g(Ft,H,null),e(H,Di),e(H,Mt),e(Mt,Ii),e(Mt,_o),e(_o,Ni),e(Mt,Li),e(H,Oi),e(H,At),e(At,Gi),e(At,Dt),e(Dt,Wi),e(At,Vi),e(H,Ui),e(H,V),g(It,V,null),e(V,Bi),e(V,ge),e(ge,Hi),e(ge,go),e(go,Ri),e(ge,Ji),e(ge,Tn),e(Tn,Yi),e(ge,Xi),e(V,Ki),g(Ne,V,null),e(V,Qi),e(V,xn),e(xn,Zi),e(V,ec),g(Nt,V,null),h(t,Xn,p),h(t,ve,p),e(ve,Le),e(Le,kn),g(Lt,kn,null),e(ve,tc),e(ve,bn),e(bn,oc),h(t,Kn,p),h(t,R,p),g(Ot,R,null),e(R,nc),e(R,Gt),e(Gt,sc),e(Gt,vo),e(vo,ac),e(Gt,rc),e(R,ic),e(R,Wt),e(Wt,cc),e(Wt,Vt),e(Vt,dc),e(Wt,lc),e(R,pc),e(R,U),g(Ut,U,null),e(U,hc),e(U,Te),e(Te,mc),e(Te,To),e(To,fc),e(Te,uc),e(Te,yn),e(yn,_c),e(Te,gc),e(U,vc),g(Oe,U,null),e(U,Tc),e(U,Sn),e(Sn,xc),e(U,kc),g(Bt,U,null),Qn=!0},p(t,[p]){const Ht={};p&2&&(Ht.$$scope={dirty:p,ctx:t}),Ce.$set(Ht);const wn={};p&2&&(wn.$$scope={dirty:p,ctx:t}),Fe.$set(wn);const $n={};p&2&&($n.$$scope={dirty:p,ctx:t}),Ne.$set($n);const zn={};p&2&&(zn.$$scope={dirty:p,ctx:t}),Oe.$set(zn)},i(t){Qn||(v(k.$$.fragment,t),v(Ue.$$.fragment,t),v(Ke.$$.fragment,t),v(Ze.$$.fragment,t),v(tt.$$.fragment,t),v(nt.$$.fragment,t),v(st.$$.fragment,t),v(rt.$$.fragment,t),v(it.$$.fragment,t),v(ct.$$.fragment,t),v(lt.$$.fragment,t),v(pt.$$.fragment,t),v(mt.$$.fragment,t),v(ft.$$.fragment,t),v(ut.$$.fragment,t),v(gt.$$.fragment,t),v(vt.$$.fragment,t),v(Tt.$$.fragment,t),v(kt.$$.fragment,t),v(St.$$.fragment,t),v(Ce.$$.fragment,t),v($t.$$.fragment,t),v(Fe.$$.fragment,t),v(zt.$$.fragment,t),v(qt.$$.fragment,t),v(jt.$$.fragment,t),v(Ct.$$.fragment,t),v(Ft.$$.fragment,t),v(It.$$.fragment,t),v(Ne.$$.fragment,t),v(Nt.$$.fragment,t),v(Lt.$$.fragment,t),v(Ot.$$.fragment,t),v(Ut.$$.fragment,t),v(Oe.$$.fragment,t),v(Bt.$$.fragment,t),Qn=!0)},o(t){T(k.$$.fragment,t),T(Ue.$$.fragment,t),T(Ke.$$.fragment,t),T(Ze.$$.fragment,t),T(tt.$$.fragment,t),T(nt.$$.fragment,t),T(st.$$.fragment,t),T(rt.$$.fragment,t),T(it.$$.fragment,t),T(ct.$$.fragment,t),T(lt.$$.fragment,t),T(pt.$$.fragment,t),T(mt.$$.fragment,t),T(ft.$$.fragment,t),T(ut.$$.fragment,t),T(gt.$$.fragment,t),T(vt.$$.fragment,t),T(Tt.$$.fragment,t),T(kt.$$.fragment,t),T(St.$$.fragment,t),T(Ce.$$.fragment,t),T($t.$$.fragment,t),T(Fe.$$.fragment,t),T(zt.$$.fragment,t),T(qt.$$.fragment,t),T(jt.$$.fragment,t),T(Ct.$$.fragment,t),T(Ft.$$.fragment,t),T(It.$$.fragment,t),T(Ne.$$.fragment,t),T(Nt.$$.fragment,t),T(Lt.$$.fragment,t),T(Ot.$$.fragment,t),T(Ut.$$.fragment,t),T(Oe.$$.fragment,t),T(Bt.$$.fragment,t),Qn=!1},d(t){o(m),t&&o(w),t&&o(f),x(k),t&&o($),t&&o(M),x(Ue),t&&o(qn),t&&o(O),t&&o(Pn),t&&o(Q),t&&o(jn),t&&o(ce),x(Ke),t&&o(Cn),t&&o(be),t&&o(Fn),t&&o(G),t&&o(Mn),t&&o(q),t&&o(An),t&&o(Kt),t&&o(Dn),x(Ze,t),t&&o(In),t&&o(Qt),t&&o(Nn),x(tt,t),t&&o(Ln),t&&o(ye),t&&o(On),t&&o(de),x(nt),t&&o(Gn),t&&o(D),x(st),x(rt),t&&o(Wn),t&&o(he),x(it),t&&o(Vn),t&&o(P),x(ct),x(lt),x(pt),x(mt),t&&o(Un),t&&o(me),x(ft),t&&o(Bn),t&&o(I),x(ut),x(gt),t&&o(Hn),t&&o(fe),x(vt),t&&o(Rn),t&&o(S),x(Tt),x(kt),x(St),x(Ce),x($t),x(Fe),x(zt),x(qt),x(jt),t&&o(Jn),t&&o(_e),x(Ct),t&&o(Yn),t&&o(H),x(Ft),x(It),x(Ne),x(Nt),t&&o(Xn),t&&o(ve),x(Lt),t&&o(Kn),t&&o(R),x(Ot),x(Ut),x(Oe),x(Bt)}}}const Tl={local:"speech2text",sections:[{local:"overview",title:"Overview"},{local:"inference",title:"Inference"},{local:"transformers.Speech2TextConfig",title:"Speech2TextConfig"},{local:"transformers.Speech2TextTokenizer",title:"Speech2TextTokenizer"},{local:"transformers.Speech2TextFeatureExtractor",title:"Speech2TextFeatureExtractor"},{local:"transformers.Speech2TextProcessor",title:"Speech2TextProcessor"},{local:"transformers.Speech2TextModel",title:"Speech2TextModel"},{local:"transformers.Speech2TextForConditionalGeneration",title:"Speech2TextForConditionalGeneration"}],title:"Speech2Text"};function xl(Y,m,w){let{fw:f}=m;return Y.$$set=b=>{"fw"in b&&w(0,f=b.fw)},[f]}class zl extends ll{constructor(m){super();pl(this,m,xl,vl,hl,{fw:0})}}export{zl as default,Tl as metadata};
