import{S as Em,i as wm,s as ym,e as a,k as p,w as j,t as o,L as km,c as t,d as n,m as c,a as l,x as _,h as r,b as m,J as s,g as h,y as g,q as v,o as E,B as w}from"../chunks/vendor-e859c359.js";import{T as gm}from"../chunks/Tip-edc75249.js";import{Y as vm}from"../chunks/Youtube-ea771e89.js";import{I as Pt}from"../chunks/IconCopyLink-5fae3b20.js";import{C as Y}from"../chunks/CodeBlock-ce4317c2.js";import{C as Nh}from"../chunks/CodeBlockFw-cd1fa549.js";import"../chunks/CopyButton-77addb3d.js";function Tm(Ms){let b,T,f,y,K;return{c(){b=a("p"),T=o(`If you plan on using a pretrained model, it\u2019s important to use the associated pretrained tokenizer: it will split
the text you give it in tokens the same way for the pretraining corpus, and it will use the same correspondence
token to index (that we usually call a `),f=a("em"),y=o("vocab"),K=o(") as during pretraining.")},l(z){b=t(z,"P",{});var H=l(b);T=r(H,`If you plan on using a pretrained model, it\u2019s important to use the associated pretrained tokenizer: it will split
the text you give it in tokens the same way for the pretraining corpus, and it will use the same correspondence
token to index (that we usually call a `),f=t(H,"EM",{});var ks=l(f);y=r(ks,"vocab"),ks.forEach(n),K=r(H,") as during pretraining."),H.forEach(n)},m(z,H){h(z,b,H),s(b,T),s(b,f),s(f,y),s(b,K)},d(z){z&&n(b)}}}function $m(Ms){let b,T;return{c(){b=a("p"),T=o(`Pre-tokenized does not mean your inputs are already tokenized (you wouldn\u2019t need to pass them through the tokenizer
if that was the case) but just split into words (which is often the first step in subword tokenization algorithms
like BPE).`)},l(f){b=t(f,"P",{});var y=l(b);T=r(y,`Pre-tokenized does not mean your inputs are already tokenized (you wouldn\u2019t need to pass them through the tokenizer
if that was the case) but just split into words (which is often the first step in subword tokenization algorithms
like BPE).`),y.forEach(n)},m(f,y){h(f,b,y),s(b,T)},d(f){f&&n(b)}}}function xm(Ms){let b,T,f,y,K,z,H,ks,_o,It,L,go,ge,vo,Eo,ve,wo,yo,Ct,D,ko,Ee,To,$o,$n,xo,qo,xn,zo,Do,Ot,Ts,St,$s,Ao,we,Po,Io,Rt,Ws,Nt,Q,xs,qn,Us,Co,zn,Oo,Yt,Js,Ht,F,So,ye,Ro,No,Dn,Yo,Ho,Lt,Ks,Ft,$,Lo,ke,Fo,Bo,Te,Go,Mo,$e,Wo,Uo,xe,Jo,Ko,Bt,qe,Qo,Gt,Qs,Mt,A,Vo,An,Xo,Zo,Pn,sr,er,In,nr,ar,Wt,ze,tr,Ut,Vs,Jt,De,lr,Kt,Ae,or,Qt,B,Cn,rr,pr,On,cr,ir,Sn,hr,Vt,Pe,ur,Xt,Xs,Zt,qs,dr,Ie,mr,br,sl,zs,fr,Rn,jr,_r,el,Ce,nl,V,Ds,Nn,Zs,gr,Yn,vr,al,se,tl,ee,Er,Hn,wr,ll,Oe,yr,ol,ne,rl,x,kr,Se,Tr,$r,Ln,xr,qr,Fn,zr,Dr,Bn,Ar,Pr,pl,Re,Ir,cl,ae,il,Ne,Cr,hl,te,ul,Ye,Or,dl,As,Sr,Gn,Rr,Nr,ml,le,bl,He,Yr,fl,oe,jl,X,Ps,Mn,re,Hr,Wn,Lr,_l,P,Fr,Un,Br,Gr,Jn,Mr,Wr,Kn,Ur,Jr,gl,G,pe,Le,Qn,Kr,Qr,Vr,Z,Is,Vn,Xr,Zr,Xn,sp,ep,np,I,Zn,ap,tp,sa,lp,op,ea,rp,pp,na,cp,ip,hp,Cs,aa,up,dp,ta,mp,bp,fp,ce,Fe,la,jp,_p,gp,R,q,oa,vp,Ep,ra,wp,yp,pa,kp,Tp,ca,$p,xp,ia,qp,zp,Dp,C,ha,Ap,Pp,ua,Ip,Cp,da,Op,Sp,ma,Rp,Np,Yp,O,ba,Hp,Lp,fa,Fp,Bp,ja,Gp,Mp,_a,Wp,Up,Jp,Os,ga,Kp,Qp,va,Vp,Xp,Zp,Ea,M,wa,sc,ec,ya,nc,ac,ka,tc,lc,vl,k,oc,Ta,rc,pc,$a,cc,ic,xa,hc,uc,qa,dc,mc,za,bc,fc,El,Ss,Da,ss,Aa,jc,_c,Pa,gc,vc,Ia,Ec,wc,u,es,Ca,yc,kc,Oa,Tc,$c,Sa,Ra,xc,qc,ns,wl,zc,Na,Dc,Ac,Be,Ya,Pc,Ic,Cc,as,yl,Oc,kl,Sc,Ha,La,Rc,Nc,ts,Tl,Yc,Fa,Hc,Lc,Ba,Ga,Fc,Bc,ls,$l,Gc,Ma,Mc,Wc,Wa,Ua,Uc,Jc,os,Ja,Kc,Qc,Ka,Vc,Xc,Ge,Qa,Zc,si,ei,rs,xl,ni,ql,ai,Va,Xa,ti,li,ps,zl,oi,Za,ri,pi,Me,st,ci,ii,hi,cs,Dl,ui,Al,di,et,nt,mi,bi,is,Pl,fi,at,ji,_i,We,tt,gi,vi,Ei,hs,Il,wi,Cl,yi,lt,ot,ki,Ti,us,Ol,$i,rt,xi,qi,pt,zi,Di,ds,ct,Ai,Pi,it,Ii,Ci,Ue,ht,Oi,Si,Ri,ms,Sl,Ni,Rl,Yi,ut,dt,Hi,Li,bs,Nl,Fi,mt,Bi,Gi,Je,bt,Mi,Wi,Ui,fs,Yl,Ji,Hl,Ki,ft,jt,Qi,Vi,js,Ll,Xi,_t,Zi,sh,gt,eh,nh,_s,Fl,ah,vt,th,lh,Ke,Et,oh,rh,ph,gs,Bl,ch,Gl,ih,wt,yt,hh,Ml,vs,Rs,kt,ie,uh,Tt,dh,Wl,W,mh,he,bh,fh,ue,jh,_h,Ul,Ns,Jl,Ys,gh,$t,vh,Eh,Kl,de,Ql,Hs,wh,xt,yh,kh,Vl,Qe,Th,Xl,me,Zl,Ve,$h,so,be,eo,Xe,xh,no,fe,ao;return z=new Pt({}),Ts=new gm({props:{$$slots:{default:[Tm]},$$scope:{ctx:Ms}}}),Ws=new Y({props:{code:`from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased'),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)`}}),Us=new Pt({}),Js=new vm({props:{id:"Yffk5aydLzg"}}),Ks=new Y({props:{code:`encoded_input = tokenizer("Hello, I'm a single sentence!")
print(encoded_input),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(<span class="hljs-string">&quot;Hello, I&#x27;m a single sentence!&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">138</span>, <span class="hljs-number">18696</span>, <span class="hljs-number">155</span>, <span class="hljs-number">1942</span>, <span class="hljs-number">3190</span>, <span class="hljs-number">1144</span>, <span class="hljs-number">1572</span>, <span class="hljs-number">13745</span>, <span class="hljs-number">1104</span>, <span class="hljs-number">159</span>, <span class="hljs-number">9664</span>, <span class="hljs-number">2107</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),Qs=new Y({props:{code:'tokenizer.decode(encoded_input["input_ids"]),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(encoded_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&quot;[CLS] Hello, I&#x27;m a single sentence! [SEP]&quot;</span>`}}),Vs=new Y({props:{code:`batch_sentences = ["Hello I'm a single sentence",
                   "And another sentence",
                   "And the very very last one"]
encoded_inputs = tokenizer(batch_sentences)
print(encoded_inputs),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [<span class="hljs-string">&quot;Hello I&#x27;m a single sentence&quot;</span>,
<span class="hljs-meta">... </span>                   <span class="hljs-string">&quot;And another sentence&quot;</span>,
<span class="hljs-meta">... </span>                   <span class="hljs-string">&quot;And the very very last one&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(batch_sentences)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_inputs)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">8667</span>, <span class="hljs-number">146</span>, <span class="hljs-number">112</span>, <span class="hljs-number">182</span>, <span class="hljs-number">170</span>, <span class="hljs-number">1423</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>],
               [<span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1330</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>],
               [<span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1314</span>, <span class="hljs-number">1141</span>, <span class="hljs-number">102</span>]],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                    [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                    [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]}`}}),Xs=new Nh({props:{pt:{code:`batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="pt")
print(batch)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(batch)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tensor([[ <span class="hljs-number">101</span>, <span class="hljs-number">8667</span>,  <span class="hljs-number">146</span>,  <span class="hljs-number">112</span>,  <span class="hljs-number">182</span>,  <span class="hljs-number">170</span>, <span class="hljs-number">1423</span>, <span class="hljs-number">5650</span>,  <span class="hljs-number">102</span>],
                      [ <span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1330</span>, <span class="hljs-number">5650</span>,  <span class="hljs-number">102</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
                      [ <span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1314</span>, <span class="hljs-number">1141</span>,  <span class="hljs-number">102</span>,    <span class="hljs-number">0</span>]]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])}`},tf:{code:`batch = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors="tf")
print(batch)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch = tokenizer(batch_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(batch)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: tf.Tensor([[ <span class="hljs-number">101</span>, <span class="hljs-number">8667</span>,  <span class="hljs-number">146</span>,  <span class="hljs-number">112</span>,  <span class="hljs-number">182</span>,  <span class="hljs-number">170</span>, <span class="hljs-number">1423</span>, <span class="hljs-number">5650</span>,  <span class="hljs-number">102</span>],
                      [ <span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1330</span>, <span class="hljs-number">5650</span>,  <span class="hljs-number">102</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>,    <span class="hljs-number">0</span>],
                      [ <span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1314</span>, <span class="hljs-number">1141</span>,  <span class="hljs-number">102</span>,    <span class="hljs-number">0</span>]]),
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: tf.Tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]), 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: tf.Tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
                           [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])}`}}}),Zs=new Pt({}),se=new vm({props:{id:"0u3ioSwev3s"}}),ne=new Y({props:{code:`encoded_input = tokenizer("How old are you?", "I'm 6 years old")
print(encoded_input),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer(<span class="hljs-string">&quot;How old are you?&quot;</span>, <span class="hljs-string">&quot;I&#x27;m 6 years old&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">1731</span>, <span class="hljs-number">1385</span>, <span class="hljs-number">1132</span>, <span class="hljs-number">1128</span>, <span class="hljs-number">136</span>, <span class="hljs-number">102</span>, <span class="hljs-number">146</span>, <span class="hljs-number">112</span>, <span class="hljs-number">182</span>, <span class="hljs-number">127</span>, <span class="hljs-number">1201</span>, <span class="hljs-number">1385</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),ae=new Y({props:{code:'tokenizer.decode(encoded_input["input_ids"]),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(encoded_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&quot;[CLS] How old are you? [SEP] I&#x27;m 6 years old [SEP]&quot;</span>`}}),te=new Y({props:{code:`batch_sentences = ["Hello I'm a single sentence",
                   "And another sentence",
                   "And the very very last one"]
batch_of_second_sentences = ["I'm a sentence that goes with the first sentence",
                             "And I should be encoded with the second sentence",
                             "And I go with the very last one"]
encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences)
print(encoded_inputs),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>batch_sentences = [<span class="hljs-string">&quot;Hello I&#x27;m a single sentence&quot;</span>,
<span class="hljs-meta">... </span>                   <span class="hljs-string">&quot;And another sentence&quot;</span>,
<span class="hljs-meta">... </span>                   <span class="hljs-string">&quot;And the very very last one&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>batch_of_second_sentences = [<span class="hljs-string">&quot;I&#x27;m a sentence that goes with the first sentence&quot;</span>,
<span class="hljs-meta">... </span>                             <span class="hljs-string">&quot;And I should be encoded with the second sentence&quot;</span>,
<span class="hljs-meta">... </span>                             <span class="hljs-string">&quot;And I go with the very last one&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_inputs)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [[<span class="hljs-number">101</span>, <span class="hljs-number">8667</span>, <span class="hljs-number">146</span>, <span class="hljs-number">112</span>, <span class="hljs-number">182</span>, <span class="hljs-number">170</span>, <span class="hljs-number">1423</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>, <span class="hljs-number">146</span>, <span class="hljs-number">112</span>, <span class="hljs-number">182</span>, <span class="hljs-number">170</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">1115</span>, <span class="hljs-number">2947</span>, <span class="hljs-number">1114</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1148</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1330</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">146</span>, <span class="hljs-number">1431</span>, <span class="hljs-number">1129</span>, <span class="hljs-number">12544</span>, <span class="hljs-number">1114</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1248</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>], 
               [<span class="hljs-number">101</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1314</span>, <span class="hljs-number">1141</span>, <span class="hljs-number">102</span>, <span class="hljs-number">1262</span>, <span class="hljs-number">146</span>, <span class="hljs-number">1301</span>, <span class="hljs-number">1114</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">1304</span>, <span class="hljs-number">1314</span>, <span class="hljs-number">1141</span>, <span class="hljs-number">102</span>]], 
<span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                   [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], 
<span class="hljs-string">&#x27;attention_mask&#x27;</span>: [[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], 
                   [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]}`}}),le=new Y({props:{code:`for ids in encoded_inputs["input_ids"]:
    print(tokenizer.decode(ids)),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> ids <span class="hljs-keyword">in</span> encoded_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]:
<span class="hljs-meta">&gt;&gt;&gt; </span>    <span class="hljs-built_in">print</span>(tokenizer.decode(ids))
[CLS] Hello I<span class="hljs-string">&#x27;m a single sentence [SEP] I&#x27;</span>m a sentence that goes <span class="hljs-keyword">with</span> the first sentence [SEP]
[CLS] And another sentence [SEP] And I should be encoded <span class="hljs-keyword">with</span> the second sentence [SEP]
[CLS] And the very very last one [SEP] And I go <span class="hljs-keyword">with</span> the very last one [SEP]`}}),oe=new Nh({props:{pt:{code:'batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=True, truncation=True, return_tensors="pt")',highlighted:'batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)'},tf:{code:'batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=True, truncation=True, return_tensors="tf")',highlighted:'batch = tokenizer(batch_sentences, batch_of_second_sentences, padding=<span class="hljs-literal">True</span>, truncation=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)'}}}),re=new Pt({}),ie=new Pt({}),Ns=new gm({props:{warning:"&lcub;true}",$$slots:{default:[$m]},$$scope:{ctx:Ms}}}),de=new Y({props:{code:`encoded_input = tokenizer(["Hello", "I'm", "a", "single", "sentence"], is_split_into_words=True)
print(encoded_input),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_input = tokenizer([<span class="hljs-string">&quot;Hello&quot;</span>, <span class="hljs-string">&quot;I&#x27;m&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;single&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>], is_split_into_words=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_input)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">8667</span>, <span class="hljs-number">146</span>, <span class="hljs-number">112</span>, <span class="hljs-number">182</span>, <span class="hljs-number">170</span>, <span class="hljs-number">1423</span>, <span class="hljs-number">5650</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),me=new Y({props:{code:`batch_sentences = [["Hello", "I'm", "a", "single", "sentence"],
                   ["And", "another", "sentence"],
                   ["And", "the", "very", "very", "last", "one"]]
encoded_inputs = tokenizer(batch_sentences, is_split_into_words=True),`,highlighted:`batch_sentences = [[<span class="hljs-string">&quot;Hello&quot;</span>, <span class="hljs-string">&quot;I&#x27;m&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;single&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>],
                   [<span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;another&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>],
                   [<span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;the&quot;</span>, <span class="hljs-string">&quot;very&quot;</span>, <span class="hljs-string">&quot;very&quot;</span>, <span class="hljs-string">&quot;last&quot;</span>, <span class="hljs-string">&quot;one&quot;</span>]]
encoded_inputs = tokenizer(batch_sentences, is_split_into_words=<span class="hljs-literal">True</span>)`}}),be=new Y({props:{code:`batch_of_second_sentences = [["I'm", "a", "sentence", "that", "goes", "with", "the", "first", "sentence"],
                             ["And", "I", "should", "be", "encoded", "with", "the", "second", "sentence"],
                             ["And", "I", "go", "with", "the", "very", "last", "one"]]
encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences, is_split_into_words=True),`,highlighted:`batch_of_second_sentences = [[<span class="hljs-string">&quot;I&#x27;m&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>, <span class="hljs-string">&quot;that&quot;</span>, <span class="hljs-string">&quot;goes&quot;</span>, <span class="hljs-string">&quot;with&quot;</span>, <span class="hljs-string">&quot;the&quot;</span>, <span class="hljs-string">&quot;first&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>],
                             [<span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;I&quot;</span>, <span class="hljs-string">&quot;should&quot;</span>, <span class="hljs-string">&quot;be&quot;</span>, <span class="hljs-string">&quot;encoded&quot;</span>, <span class="hljs-string">&quot;with&quot;</span>, <span class="hljs-string">&quot;the&quot;</span>, <span class="hljs-string">&quot;second&quot;</span>, <span class="hljs-string">&quot;sentence&quot;</span>],
                             [<span class="hljs-string">&quot;And&quot;</span>, <span class="hljs-string">&quot;I&quot;</span>, <span class="hljs-string">&quot;go&quot;</span>, <span class="hljs-string">&quot;with&quot;</span>, <span class="hljs-string">&quot;the&quot;</span>, <span class="hljs-string">&quot;very&quot;</span>, <span class="hljs-string">&quot;last&quot;</span>, <span class="hljs-string">&quot;one&quot;</span>]]
encoded_inputs = tokenizer(batch_sentences, batch_of_second_sentences, is_split_into_words=<span class="hljs-literal">True</span>)`}}),fe=new Nh({props:{pt:{code:`batch = tokenizer(batch_sentences,
                  batch_of_second_sentences,
                  is_split_into_words=True,
                  padding=True,
                  truncation=True,
                  return_tensors="pt")`,highlighted:`batch = tokenizer(batch_sentences,
                  batch_of_second_sentences,
                  is_split_into_words=<span class="hljs-literal">True</span>,
                  padding=<span class="hljs-literal">True</span>,
                  truncation=<span class="hljs-literal">True</span>,
                  return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`},tf:{code:`batch = tokenizer(batch_sentences,
                  batch_of_second_sentences,
                  is_split_into_words=True,
                  padding=True,
                  truncation=True,
                  return_tensors="tf")`,highlighted:`batch = tokenizer(batch_sentences,
                  batch_of_second_sentences,
                  is_split_into_words=<span class="hljs-literal">True</span>,
                  padding=<span class="hljs-literal">True</span>,
                  truncation=<span class="hljs-literal">True</span>,
                  return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}}),{c(){b=a("meta"),T=p(),f=a("h1"),y=a("a"),K=a("span"),j(z.$$.fragment),H=p(),ks=a("span"),_o=o("Preprocessing data"),It=p(),L=a("p"),go=o(`In this tutorial, we\u2019ll explore how to preprocess your data using \u{1F917} Transformers. The main tool for this is what we
call a `),ge=a("a"),vo=o("tokenizer"),Eo=o(`. You can build one using the tokenizer class associated to the model
you would like to use, or directly with the `),ve=a("a"),wo=o("AutoTokenizer"),yo=o(" class."),Ct=p(),D=a("p"),ko=o("As we saw in the "),Ee=a("a"),To=o("quick tour"),$o=o(`, the tokenizer will first split a given text in words (or part of
words, punctuation symbols, etc.) usually called `),$n=a("em"),xo=o("tokens"),qo=o(". Then it will convert those "),xn=a("em"),zo=o("tokens"),Do=o(` into numbers, to be able
to build a tensor out of them and feed them to the model. It will also add any additional inputs the model might expect
to work properly.`),Ot=p(),j(Ts.$$.fragment),St=p(),$s=a("p"),Ao=o(`To automatically download the vocab used during pretraining or fine-tuning a given model, you can use the
`),we=a("a"),Po=o("AutoTokenizer.from_pretrained()"),Io=o(" method:"),Rt=p(),j(Ws.$$.fragment),Nt=p(),Q=a("h2"),xs=a("a"),qn=a("span"),j(Us.$$.fragment),Co=p(),zn=a("span"),Oo=o("Base use"),Yt=p(),j(Js.$$.fragment),Ht=p(),F=a("p"),So=o("A "),ye=a("a"),Ro=o("PreTrainedTokenizer"),No=o(` has many methods, but the only one you need to remember for preprocessing
is its `),Dn=a("code"),Yo=o("__call__"),Ho=o(": you just need to feed your sentence to your tokenizer object."),Lt=p(),j(Ks.$$.fragment),Ft=p(),$=a("p"),Lo=o("This returns a dictionary string to list of ints. The "),ke=a("a"),Fo=o("input_ids"),Bo=o(` are the indices corresponding
to each token in our sentence. We will see below what the `),Te=a("a"),Go=o("attention_mask"),Mo=o(` is used for and
in `),$e=a("a"),Wo=o("the next section"),Uo=o(" the goal of "),xe=a("a"),Jo=o("token_type_ids"),Ko=o("."),Bt=p(),qe=a("p"),Qo=o("The tokenizer can decode a list of token ids in a proper sentence:"),Gt=p(),j(Qs.$$.fragment),Mt=p(),A=a("p"),Vo=o(`As you can see, the tokenizer automatically added some special tokens that the model expects. Not all models need
special tokens; for instance, if we had used `),An=a("em"),Xo=o("gpt2-medium"),Zo=o(" instead of "),Pn=a("em"),sr=o("bert-base-cased"),er=o(` to create our tokenizer, we
would have seen the same sentence as the original one here. You can disable this behavior (which is only advised if you
have added those special tokens yourself) by passing `),In=a("code"),nr=o("add_special_tokens=False"),ar=o("."),Wt=p(),ze=a("p"),tr=o(`If you have several sentences you want to process, you can do this efficiently by sending them as a list to the
tokenizer:`),Ut=p(),j(Vs.$$.fragment),Jt=p(),De=a("p"),lr=o("We get back a dictionary once again, this time with values being lists of lists of ints."),Kt=p(),Ae=a("p"),or=o(`If the purpose of sending several sentences at a time to the tokenizer is to build a batch to feed the model, you will
probably want:`),Qt=p(),B=a("ul"),Cn=a("li"),rr=o("To pad each sentence to the maximum length there is in your batch."),pr=p(),On=a("li"),cr=o("To truncate each sentence to the maximum length the model can accept (if applicable)."),ir=p(),Sn=a("li"),hr=o("To return tensors."),Vt=p(),Pe=a("p"),ur=o("You can do all of this by using the following options when feeding your list of sentences to the tokenizer:"),Xt=p(),j(Xs.$$.fragment),Zt=p(),qs=a("p"),dr=o("It returns a dictionary with string keys and tensor values. We can now see what the "),Ie=a("a"),mr=o("attention_mask"),br=o(` is all about: it points out which tokens the model should pay attention to and which ones
it should not (because they represent padding in this case).`),sl=p(),zs=a("p"),fr=o(`Note that if your model does not have a maximum length associated to it, the command above will throw a warning. You
can safely ignore it. You can also pass `),Rn=a("code"),jr=o("verbose=False"),_r=o(" to stop the tokenizer from throwing those kinds of warnings."),el=p(),Ce=a("a"),nl=p(),V=a("h2"),Ds=a("a"),Nn=a("span"),j(Zs.$$.fragment),gr=p(),Yn=a("span"),vr=o("Preprocessing pairs of sentences"),al=p(),j(se.$$.fragment),tl=p(),ee=a("p"),Er=o(`Sometimes you need to feed a pair of sentences to your model. For instance, if you want to classify if two sentences in
a pair are similar, or for question-answering models, which take a context and a question. For BERT models, the input
is then represented like this: `),Hn=a("code"),wr=o("[CLS] Sequence A [SEP] Sequence B [SEP]"),ll=p(),Oe=a("p"),yr=o(`You can encode a pair of sentences in the format expected by your model by supplying the two sentences as two arguments
(not a list since a list of two sentences will be interpreted as a batch of two single sentences, as we saw before).
This will once again return a dict string to list of ints:`),ol=p(),j(ne.$$.fragment),rl=p(),x=a("p"),kr=o("This shows us what the "),Se=a("a"),Tr=o("token_type_ids"),$r=o(` are for: they indicate to the model which part of
the inputs correspond to the first sentence and which part corresponds to the second sentence. Note that
`),Ln=a("em"),xr=o("token_type_ids"),qr=o(` are not required or handled by all models. By default, a tokenizer will only return the inputs that
its associated model expects. You can force the return (or the non-return) of any of those special arguments by using
`),Fn=a("code"),zr=o("return_input_ids"),Dr=o(" or "),Bn=a("code"),Ar=o("return_token_type_ids"),Pr=o("."),pl=p(),Re=a("p"),Ir=o("If we decode the token ids we obtained, we will see that the special tokens have been properly added."),cl=p(),j(ae.$$.fragment),il=p(),Ne=a("p"),Cr=o(`If you have a list of pairs of sequences you want to process, you should feed them as two lists to your tokenizer: the
list of first sentences and the list of second sentences:`),hl=p(),j(te.$$.fragment),ul=p(),Ye=a("p"),Or=o("As we can see, it returns a dictionary where each value is a list of lists of ints."),dl=p(),As=a("p"),Sr=o("To double-check what is fed to the model, we can decode each list in "),Gn=a("em"),Rr=o("input_ids"),Nr=o(" one by one:"),ml=p(),j(le.$$.fragment),bl=p(),He=a("p"),Yr=o(`Once again, you can automatically pad your inputs to the maximum sentence length in the batch, truncate to the maximum
length the model can accept and return tensors directly with the following:`),fl=p(),j(oe.$$.fragment),jl=p(),X=a("h2"),Ps=a("a"),Mn=a("span"),j(re.$$.fragment),Hr=p(),Wn=a("span"),Lr=o("Everything you always wanted to know about padding and truncation"),_l=p(),P=a("p"),Fr=o(`We have seen the commands that will work for most cases (pad your batch to the length of the maximum sentence and
truncate to the maximum length the model can accept). However, the API supports more strategies if you need them. The
three arguments you need to know for this are `),Un=a("code"),Br=o("padding"),Gr=o(", "),Jn=a("code"),Mr=o("truncation"),Wr=o(" and "),Kn=a("code"),Ur=o("max_length"),Jr=o("."),gl=p(),G=a("ul"),pe=a("li"),Le=a("p"),Qn=a("code"),Kr=o("padding"),Qr=o(" controls the padding. It can be a boolean or a string which should be:"),Vr=p(),Z=a("ul"),Is=a("li"),Vn=a("code"),Xr=o("True"),Zr=o(" or "),Xn=a("code"),sp=o("'longest'"),ep=o(` to pad to the longest sequence in the batch (doing no padding if you only provide
a single sequence).`),np=p(),I=a("li"),Zn=a("code"),ap=o("'max_length'"),tp=o(" to pad to a length specified by the "),sa=a("code"),lp=o("max_length"),op=o(` argument or the maximum length accepted
by the model if no `),ea=a("code"),rp=o("max_length"),pp=o(" is provided ("),na=a("code"),cp=o("max_length=None"),ip=o(`). If you only provide a single sequence,
padding will still be applied to it.`),hp=p(),Cs=a("li"),aa=a("code"),up=o("False"),dp=o(" or "),ta=a("code"),mp=o("'do_not_pad'"),bp=o(` to not pad the sequences. As we have seen before, this is the default
behavior.`),fp=p(),ce=a("li"),Fe=a("p"),la=a("code"),jp=o("truncation"),_p=o(" controls the truncation. It can be a boolean or a string which should be:"),gp=p(),R=a("ul"),q=a("li"),oa=a("code"),vp=o("True"),Ep=o(" or "),ra=a("code"),wp=o("'longest_first'"),yp=o(" truncate to a maximum length specified by the "),pa=a("code"),kp=o("max_length"),Tp=o(` argument or
the maximum length accepted by the model if no `),ca=a("code"),$p=o("max_length"),xp=o(" is provided ("),ia=a("code"),qp=o("max_length=None"),zp=o(`). This will
truncate token by token, removing a token from the longest sequence in the pair until the proper length is
reached.`),Dp=p(),C=a("li"),ha=a("code"),Ap=o("'only_second'"),Pp=o(" truncate to a maximum length specified by the "),ua=a("code"),Ip=o("max_length"),Cp=o(` argument or the maximum
length accepted by the model if no `),da=a("code"),Op=o("max_length"),Sp=o(" is provided ("),ma=a("code"),Rp=o("max_length=None"),Np=o(`). This will only truncate
the second sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.`),Yp=p(),O=a("li"),ba=a("code"),Hp=o("'only_first'"),Lp=o(" truncate to a maximum length specified by the "),fa=a("code"),Fp=o("max_length"),Bp=o(` argument or the maximum
length accepted by the model if no `),ja=a("code"),Gp=o("max_length"),Mp=o(" is provided ("),_a=a("code"),Wp=o("max_length=None"),Up=o(`). This will only truncate
the first sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.`),Jp=p(),Os=a("li"),ga=a("code"),Kp=o("False"),Qp=o(" or "),va=a("code"),Vp=o("'do_not_truncate'"),Xp=o(` to not truncate the sequences. As we have seen before, this is the
default behavior.`),Zp=p(),Ea=a("li"),M=a("p"),wa=a("code"),sc=o("max_length"),ec=o(" to control the length of the padding/truncation. It can be an integer or "),ya=a("code"),nc=o("None"),ac=o(`, in which case
it will default to the maximum length the model can accept. If the model has no specific maximum input length,
truncation/padding to `),ka=a("code"),tc=o("max_length"),lc=o(" is deactivated."),vl=p(),k=a("p"),oc=o(`Here is a table summarizing the recommend way to setup padding and truncation. If you use pair of inputs sequence in
any of the following examples, you can replace `),Ta=a("code"),rc=o("truncation=True"),pc=o(" by a "),$a=a("code"),cc=o("STRATEGY"),ic=o(` selected in
`),xa=a("code"),hc=o("['only_first', 'only_second', 'longest_first']"),uc=o(", i.e. "),qa=a("code"),dc=o("truncation='only_second'"),mc=o(" or "),za=a("code"),bc=o("truncation= 'longest_first'"),fc=o(" to control how both sequence in the pair are truncated as detailed before."),El=p(),Ss=a("table"),Da=a("thead"),ss=a("tr"),Aa=a("th"),jc=o("Truncation"),_c=p(),Pa=a("th"),gc=o("Padding"),vc=p(),Ia=a("th"),Ec=o("Instruction"),wc=p(),u=a("tbody"),es=a("tr"),Ca=a("td"),yc=o("no truncation"),kc=p(),Oa=a("td"),Tc=o("no padding"),$c=p(),Sa=a("td"),Ra=a("code"),xc=o("tokenizer(batch_sentences)"),qc=p(),ns=a("tr"),wl=a("td"),zc=p(),Na=a("td"),Dc=o("padding to max sequence in batch"),Ac=p(),Be=a("td"),Ya=a("code"),Pc=o("tokenizer(batch_sentences, padding=True)"),Ic=o(" or"),Cc=p(),as=a("tr"),yl=a("td"),Oc=p(),kl=a("td"),Sc=p(),Ha=a("td"),La=a("code"),Rc=o("tokenizer(batch_sentences, padding='longest')"),Nc=p(),ts=a("tr"),Tl=a("td"),Yc=p(),Fa=a("td"),Hc=o("padding to max model input length"),Lc=p(),Ba=a("td"),Ga=a("code"),Fc=o("tokenizer(batch_sentences, padding='max_length')"),Bc=p(),ls=a("tr"),$l=a("td"),Gc=p(),Ma=a("td"),Mc=o("padding to specific length"),Wc=p(),Wa=a("td"),Ua=a("code"),Uc=o("tokenizer(batch_sentences, padding='max_length', max_length=42)"),Jc=p(),os=a("tr"),Ja=a("td"),Kc=o("truncation to max model input length"),Qc=p(),Ka=a("td"),Vc=o("no padding"),Xc=p(),Ge=a("td"),Qa=a("code"),Zc=o("tokenizer(batch_sentences, truncation=True)"),si=o(" or"),ei=p(),rs=a("tr"),xl=a("td"),ni=p(),ql=a("td"),ai=p(),Va=a("td"),Xa=a("code"),ti=o("tokenizer(batch_sentences, truncation=STRATEGY)"),li=p(),ps=a("tr"),zl=a("td"),oi=p(),Za=a("td"),ri=o("padding to max sequence in batch"),pi=p(),Me=a("td"),st=a("code"),ci=o("tokenizer(batch_sentences, padding=True, truncation=True)"),ii=o(" or"),hi=p(),cs=a("tr"),Dl=a("td"),ui=p(),Al=a("td"),di=p(),et=a("td"),nt=a("code"),mi=o("tokenizer(batch_sentences, padding=True, truncation=STRATEGY)"),bi=p(),is=a("tr"),Pl=a("td"),fi=p(),at=a("td"),ji=o("padding to max model input length"),_i=p(),We=a("td"),tt=a("code"),gi=o("tokenizer(batch_sentences, padding='max_length', truncation=True)"),vi=o(" or"),Ei=p(),hs=a("tr"),Il=a("td"),wi=p(),Cl=a("td"),yi=p(),lt=a("td"),ot=a("code"),ki=o("tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY)"),Ti=p(),us=a("tr"),Ol=a("td"),$i=p(),rt=a("td"),xi=o("padding to specific length"),qi=p(),pt=a("td"),zi=o("Not possible"),Di=p(),ds=a("tr"),ct=a("td"),Ai=o("truncation to specific length"),Pi=p(),it=a("td"),Ii=o("no padding"),Ci=p(),Ue=a("td"),ht=a("code"),Oi=o("tokenizer(batch_sentences, truncation=True, max_length=42)"),Si=o(" or"),Ri=p(),ms=a("tr"),Sl=a("td"),Ni=p(),Rl=a("td"),Yi=p(),ut=a("td"),dt=a("code"),Hi=o("tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)"),Li=p(),bs=a("tr"),Nl=a("td"),Fi=p(),mt=a("td"),Bi=o("padding to max sequence in batch"),Gi=p(),Je=a("td"),bt=a("code"),Mi=o("tokenizer(batch_sentences, padding=True, truncation=True, max_length=42)"),Wi=o(" or"),Ui=p(),fs=a("tr"),Yl=a("td"),Ji=p(),Hl=a("td"),Ki=p(),ft=a("td"),jt=a("code"),Qi=o("tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)"),Vi=p(),js=a("tr"),Ll=a("td"),Xi=p(),_t=a("td"),Zi=o("padding to max model input length"),sh=p(),gt=a("td"),eh=o("Not possible"),nh=p(),_s=a("tr"),Fl=a("td"),ah=p(),vt=a("td"),th=o("padding to specific length"),lh=p(),Ke=a("td"),Et=a("code"),oh=o("tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42)"),rh=o(" or"),ph=p(),gs=a("tr"),Bl=a("td"),ch=p(),Gl=a("td"),ih=p(),wt=a("td"),yt=a("code"),hh=o("tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42)"),Ml=p(),vs=a("h2"),Rs=a("a"),kt=a("span"),j(ie.$$.fragment),uh=p(),Tt=a("span"),dh=o("Pre-tokenized inputs"),Wl=p(),W=a("p"),mh=o(`The tokenizer also accept pre-tokenized inputs. This is particularly useful when you want to compute labels and extract
predictions in `),he=a("a"),bh=o("named entity recognition (NER)"),fh=o(` or
`),ue=a("a"),jh=o("part-of-speech tagging (POS tagging)"),_h=o("."),Ul=p(),j(Ns.$$.fragment),Jl=p(),Ys=a("p"),gh=o("If you want to use pre-tokenized inputs, just set "),$t=a("code"),vh=o("is_split_into_words=True"),Eh=o(` when passing your inputs to the
tokenizer. For instance, we have:`),Kl=p(),j(de.$$.fragment),Ql=p(),Hs=a("p"),wh=o(`Note that the tokenizer still adds the ids of special tokens (if applicable) unless you pass
`),xt=a("code"),yh=o("add_special_tokens=False"),kh=o("."),Vl=p(),Qe=a("p"),Th=o(`This works exactly as before for batch of sentences or batch of pairs of sentences. You can encode a batch of sentences
like this:`),Xl=p(),j(me.$$.fragment),Zl=p(),Ve=a("p"),$h=o("or a batch of pair sentences like this:"),so=p(),j(be.$$.fragment),eo=p(),Xe=a("p"),xh=o("And you can add padding, truncation as well as directly return tensors like before:"),no=p(),j(fe.$$.fragment),this.h()},l(e){const i=km('[data-svelte="svelte-1phssyn"]',document.head);b=t(i,"META",{name:!0,content:!0}),i.forEach(n),T=c(e),f=t(e,"H1",{class:!0});var je=l(f);y=t(je,"A",{id:!0,class:!0,href:!0});var qt=l(y);K=t(qt,"SPAN",{});var Yh=l(K);_(z.$$.fragment,Yh),Yh.forEach(n),qt.forEach(n),H=c(je),ks=t(je,"SPAN",{});var Hh=l(ks);_o=r(Hh,"Preprocessing data"),Hh.forEach(n),je.forEach(n),It=c(e),L=t(e,"P",{});var Ze=l(L);go=r(Ze,`In this tutorial, we\u2019ll explore how to preprocess your data using \u{1F917} Transformers. The main tool for this is what we
call a `),ge=t(Ze,"A",{href:!0});var Lh=l(ge);vo=r(Lh,"tokenizer"),Lh.forEach(n),Eo=r(Ze,`. You can build one using the tokenizer class associated to the model
you would like to use, or directly with the `),ve=t(Ze,"A",{href:!0});var Fh=l(ve);wo=r(Fh,"AutoTokenizer"),Fh.forEach(n),yo=r(Ze," class."),Ze.forEach(n),Ct=c(e),D=t(e,"P",{});var Ls=l(D);ko=r(Ls,"As we saw in the "),Ee=t(Ls,"A",{href:!0});var Bh=l(Ee);To=r(Bh,"quick tour"),Bh.forEach(n),$o=r(Ls,`, the tokenizer will first split a given text in words (or part of
words, punctuation symbols, etc.) usually called `),$n=t(Ls,"EM",{});var Gh=l($n);xo=r(Gh,"tokens"),Gh.forEach(n),qo=r(Ls,". Then it will convert those "),xn=t(Ls,"EM",{});var Mh=l(xn);zo=r(Mh,"tokens"),Mh.forEach(n),Do=r(Ls,` into numbers, to be able
to build a tensor out of them and feed them to the model. It will also add any additional inputs the model might expect
to work properly.`),Ls.forEach(n),Ot=c(e),_(Ts.$$.fragment,e),St=c(e),$s=t(e,"P",{});var to=l($s);Ao=r(to,`To automatically download the vocab used during pretraining or fine-tuning a given model, you can use the
`),we=t(to,"A",{href:!0});var Wh=l(we);Po=r(Wh,"AutoTokenizer.from_pretrained()"),Wh.forEach(n),Io=r(to," method:"),to.forEach(n),Rt=c(e),_(Ws.$$.fragment,e),Nt=c(e),Q=t(e,"H2",{class:!0});var lo=l(Q);xs=t(lo,"A",{id:!0,class:!0,href:!0});var Uh=l(xs);qn=t(Uh,"SPAN",{});var Jh=l(qn);_(Us.$$.fragment,Jh),Jh.forEach(n),Uh.forEach(n),Co=c(lo),zn=t(lo,"SPAN",{});var Kh=l(zn);Oo=r(Kh,"Base use"),Kh.forEach(n),lo.forEach(n),Yt=c(e),_(Js.$$.fragment,e),Ht=c(e),F=t(e,"P",{});var sn=l(F);So=r(sn,"A "),ye=t(sn,"A",{href:!0});var Qh=l(ye);Ro=r(Qh,"PreTrainedTokenizer"),Qh.forEach(n),No=r(sn,` has many methods, but the only one you need to remember for preprocessing
is its `),Dn=t(sn,"CODE",{});var Vh=l(Dn);Yo=r(Vh,"__call__"),Vh.forEach(n),Ho=r(sn,": you just need to feed your sentence to your tokenizer object."),sn.forEach(n),Lt=c(e),_(Ks.$$.fragment,e),Ft=c(e),$=t(e,"P",{});var U=l($);Lo=r(U,"This returns a dictionary string to list of ints. The "),ke=t(U,"A",{href:!0});var Xh=l(ke);Fo=r(Xh,"input_ids"),Xh.forEach(n),Bo=r(U,` are the indices corresponding
to each token in our sentence. We will see below what the `),Te=t(U,"A",{href:!0});var Zh=l(Te);Go=r(Zh,"attention_mask"),Zh.forEach(n),Mo=r(U,` is used for and
in `),$e=t(U,"A",{href:!0});var su=l($e);Wo=r(su,"the next section"),su.forEach(n),Uo=r(U," the goal of "),xe=t(U,"A",{href:!0});var eu=l(xe);Jo=r(eu,"token_type_ids"),eu.forEach(n),Ko=r(U,"."),U.forEach(n),Bt=c(e),qe=t(e,"P",{});var nu=l(qe);Qo=r(nu,"The tokenizer can decode a list of token ids in a proper sentence:"),nu.forEach(n),Gt=c(e),_(Qs.$$.fragment,e),Mt=c(e),A=t(e,"P",{});var Fs=l(A);Vo=r(Fs,`As you can see, the tokenizer automatically added some special tokens that the model expects. Not all models need
special tokens; for instance, if we had used `),An=t(Fs,"EM",{});var au=l(An);Xo=r(au,"gpt2-medium"),au.forEach(n),Zo=r(Fs," instead of "),Pn=t(Fs,"EM",{});var tu=l(Pn);sr=r(tu,"bert-base-cased"),tu.forEach(n),er=r(Fs,` to create our tokenizer, we
would have seen the same sentence as the original one here. You can disable this behavior (which is only advised if you
have added those special tokens yourself) by passing `),In=t(Fs,"CODE",{});var lu=l(In);nr=r(lu,"add_special_tokens=False"),lu.forEach(n),ar=r(Fs,"."),Fs.forEach(n),Wt=c(e),ze=t(e,"P",{});var ou=l(ze);tr=r(ou,`If you have several sentences you want to process, you can do this efficiently by sending them as a list to the
tokenizer:`),ou.forEach(n),Ut=c(e),_(Vs.$$.fragment,e),Jt=c(e),De=t(e,"P",{});var ru=l(De);lr=r(ru,"We get back a dictionary once again, this time with values being lists of lists of ints."),ru.forEach(n),Kt=c(e),Ae=t(e,"P",{});var pu=l(Ae);or=r(pu,`If the purpose of sending several sentences at a time to the tokenizer is to build a batch to feed the model, you will
probably want:`),pu.forEach(n),Qt=c(e),B=t(e,"UL",{});var en=l(B);Cn=t(en,"LI",{});var cu=l(Cn);rr=r(cu,"To pad each sentence to the maximum length there is in your batch."),cu.forEach(n),pr=c(en),On=t(en,"LI",{});var iu=l(On);cr=r(iu,"To truncate each sentence to the maximum length the model can accept (if applicable)."),iu.forEach(n),ir=c(en),Sn=t(en,"LI",{});var hu=l(Sn);hr=r(hu,"To return tensors."),hu.forEach(n),en.forEach(n),Vt=c(e),Pe=t(e,"P",{});var uu=l(Pe);ur=r(uu,"You can do all of this by using the following options when feeding your list of sentences to the tokenizer:"),uu.forEach(n),Xt=c(e),_(Xs.$$.fragment,e),Zt=c(e),qs=t(e,"P",{});var oo=l(qs);dr=r(oo,"It returns a dictionary with string keys and tensor values. We can now see what the "),Ie=t(oo,"A",{href:!0});var du=l(Ie);mr=r(du,"attention_mask"),du.forEach(n),br=r(oo,` is all about: it points out which tokens the model should pay attention to and which ones
it should not (because they represent padding in this case).`),oo.forEach(n),sl=c(e),zs=t(e,"P",{});var ro=l(zs);fr=r(ro,`Note that if your model does not have a maximum length associated to it, the command above will throw a warning. You
can safely ignore it. You can also pass `),Rn=t(ro,"CODE",{});var mu=l(Rn);jr=r(mu,"verbose=False"),mu.forEach(n),_r=r(ro," to stop the tokenizer from throwing those kinds of warnings."),ro.forEach(n),el=c(e),Ce=t(e,"A",{id:!0}),l(Ce).forEach(n),nl=c(e),V=t(e,"H2",{class:!0});var po=l(V);Ds=t(po,"A",{id:!0,class:!0,href:!0});var bu=l(Ds);Nn=t(bu,"SPAN",{});var fu=l(Nn);_(Zs.$$.fragment,fu),fu.forEach(n),bu.forEach(n),gr=c(po),Yn=t(po,"SPAN",{});var ju=l(Yn);vr=r(ju,"Preprocessing pairs of sentences"),ju.forEach(n),po.forEach(n),al=c(e),_(se.$$.fragment,e),tl=c(e),ee=t(e,"P",{});var qh=l(ee);Er=r(qh,`Sometimes you need to feed a pair of sentences to your model. For instance, if you want to classify if two sentences in
a pair are similar, or for question-answering models, which take a context and a question. For BERT models, the input
is then represented like this: `),Hn=t(qh,"CODE",{});var _u=l(Hn);wr=r(_u,"[CLS] Sequence A [SEP] Sequence B [SEP]"),_u.forEach(n),qh.forEach(n),ll=c(e),Oe=t(e,"P",{});var gu=l(Oe);yr=r(gu,`You can encode a pair of sentences in the format expected by your model by supplying the two sentences as two arguments
(not a list since a list of two sentences will be interpreted as a batch of two single sentences, as we saw before).
This will once again return a dict string to list of ints:`),gu.forEach(n),ol=c(e),_(ne.$$.fragment,e),rl=c(e),x=t(e,"P",{});var J=l(x);kr=r(J,"This shows us what the "),Se=t(J,"A",{href:!0});var vu=l(Se);Tr=r(vu,"token_type_ids"),vu.forEach(n),$r=r(J,` are for: they indicate to the model which part of
the inputs correspond to the first sentence and which part corresponds to the second sentence. Note that
`),Ln=t(J,"EM",{});var Eu=l(Ln);xr=r(Eu,"token_type_ids"),Eu.forEach(n),qr=r(J,` are not required or handled by all models. By default, a tokenizer will only return the inputs that
its associated model expects. You can force the return (or the non-return) of any of those special arguments by using
`),Fn=t(J,"CODE",{});var wu=l(Fn);zr=r(wu,"return_input_ids"),wu.forEach(n),Dr=r(J," or "),Bn=t(J,"CODE",{});var yu=l(Bn);Ar=r(yu,"return_token_type_ids"),yu.forEach(n),Pr=r(J,"."),J.forEach(n),pl=c(e),Re=t(e,"P",{});var ku=l(Re);Ir=r(ku,"If we decode the token ids we obtained, we will see that the special tokens have been properly added."),ku.forEach(n),cl=c(e),_(ae.$$.fragment,e),il=c(e),Ne=t(e,"P",{});var Tu=l(Ne);Cr=r(Tu,`If you have a list of pairs of sequences you want to process, you should feed them as two lists to your tokenizer: the
list of first sentences and the list of second sentences:`),Tu.forEach(n),hl=c(e),_(te.$$.fragment,e),ul=c(e),Ye=t(e,"P",{});var $u=l(Ye);Or=r($u,"As we can see, it returns a dictionary where each value is a list of lists of ints."),$u.forEach(n),dl=c(e),As=t(e,"P",{});var co=l(As);Sr=r(co,"To double-check what is fed to the model, we can decode each list in "),Gn=t(co,"EM",{});var xu=l(Gn);Rr=r(xu,"input_ids"),xu.forEach(n),Nr=r(co," one by one:"),co.forEach(n),ml=c(e),_(le.$$.fragment,e),bl=c(e),He=t(e,"P",{});var qu=l(He);Yr=r(qu,`Once again, you can automatically pad your inputs to the maximum sentence length in the batch, truncate to the maximum
length the model can accept and return tensors directly with the following:`),qu.forEach(n),fl=c(e),_(oe.$$.fragment,e),jl=c(e),X=t(e,"H2",{class:!0});var io=l(X);Ps=t(io,"A",{id:!0,class:!0,href:!0});var zu=l(Ps);Mn=t(zu,"SPAN",{});var Du=l(Mn);_(re.$$.fragment,Du),Du.forEach(n),zu.forEach(n),Hr=c(io),Wn=t(io,"SPAN",{});var Au=l(Wn);Lr=r(Au,"Everything you always wanted to know about padding and truncation"),Au.forEach(n),io.forEach(n),_l=c(e),P=t(e,"P",{});var Bs=l(P);Fr=r(Bs,`We have seen the commands that will work for most cases (pad your batch to the length of the maximum sentence and
truncate to the maximum length the model can accept). However, the API supports more strategies if you need them. The
three arguments you need to know for this are `),Un=t(Bs,"CODE",{});var Pu=l(Un);Br=r(Pu,"padding"),Pu.forEach(n),Gr=r(Bs,", "),Jn=t(Bs,"CODE",{});var Iu=l(Jn);Mr=r(Iu,"truncation"),Iu.forEach(n),Wr=r(Bs," and "),Kn=t(Bs,"CODE",{});var Cu=l(Kn);Ur=r(Cu,"max_length"),Cu.forEach(n),Jr=r(Bs,"."),Bs.forEach(n),gl=c(e),G=t(e,"UL",{});var nn=l(G);pe=t(nn,"LI",{});var ho=l(pe);Le=t(ho,"P",{});var zh=l(Le);Qn=t(zh,"CODE",{});var Ou=l(Qn);Kr=r(Ou,"padding"),Ou.forEach(n),Qr=r(zh," controls the padding. It can be a boolean or a string which should be:"),zh.forEach(n),Vr=c(ho),Z=t(ho,"UL",{});var an=l(Z);Is=t(an,"LI",{});var zt=l(Is);Vn=t(zt,"CODE",{});var Su=l(Vn);Xr=r(Su,"True"),Su.forEach(n),Zr=r(zt," or "),Xn=t(zt,"CODE",{});var Ru=l(Xn);sp=r(Ru,"'longest'"),Ru.forEach(n),ep=r(zt,` to pad to the longest sequence in the batch (doing no padding if you only provide
a single sequence).`),zt.forEach(n),np=c(an),I=t(an,"LI",{});var Es=l(I);Zn=t(Es,"CODE",{});var Nu=l(Zn);ap=r(Nu,"'max_length'"),Nu.forEach(n),tp=r(Es," to pad to a length specified by the "),sa=t(Es,"CODE",{});var Yu=l(sa);lp=r(Yu,"max_length"),Yu.forEach(n),op=r(Es,` argument or the maximum length accepted
by the model if no `),ea=t(Es,"CODE",{});var Hu=l(ea);rp=r(Hu,"max_length"),Hu.forEach(n),pp=r(Es," is provided ("),na=t(Es,"CODE",{});var Lu=l(na);cp=r(Lu,"max_length=None"),Lu.forEach(n),ip=r(Es,`). If you only provide a single sequence,
padding will still be applied to it.`),Es.forEach(n),hp=c(an),Cs=t(an,"LI",{});var Dt=l(Cs);aa=t(Dt,"CODE",{});var Fu=l(aa);up=r(Fu,"False"),Fu.forEach(n),dp=r(Dt," or "),ta=t(Dt,"CODE",{});var Bu=l(ta);mp=r(Bu,"'do_not_pad'"),Bu.forEach(n),bp=r(Dt,` to not pad the sequences. As we have seen before, this is the default
behavior.`),Dt.forEach(n),an.forEach(n),ho.forEach(n),fp=c(nn),ce=t(nn,"LI",{});var uo=l(ce);Fe=t(uo,"P",{});var Dh=l(Fe);la=t(Dh,"CODE",{});var Gu=l(la);jp=r(Gu,"truncation"),Gu.forEach(n),_p=r(Dh," controls the truncation. It can be a boolean or a string which should be:"),Dh.forEach(n),gp=c(uo),R=t(uo,"UL",{});var Gs=l(R);q=t(Gs,"LI",{});var N=l(q);oa=t(N,"CODE",{});var Mu=l(oa);vp=r(Mu,"True"),Mu.forEach(n),Ep=r(N," or "),ra=t(N,"CODE",{});var Wu=l(ra);wp=r(Wu,"'longest_first'"),Wu.forEach(n),yp=r(N," truncate to a maximum length specified by the "),pa=t(N,"CODE",{});var Uu=l(pa);kp=r(Uu,"max_length"),Uu.forEach(n),Tp=r(N,` argument or
the maximum length accepted by the model if no `),ca=t(N,"CODE",{});var Ju=l(ca);$p=r(Ju,"max_length"),Ju.forEach(n),xp=r(N," is provided ("),ia=t(N,"CODE",{});var Ku=l(ia);qp=r(Ku,"max_length=None"),Ku.forEach(n),zp=r(N,`). This will
truncate token by token, removing a token from the longest sequence in the pair until the proper length is
reached.`),N.forEach(n),Dp=c(Gs),C=t(Gs,"LI",{});var ws=l(C);ha=t(ws,"CODE",{});var Qu=l(ha);Ap=r(Qu,"'only_second'"),Qu.forEach(n),Pp=r(ws," truncate to a maximum length specified by the "),ua=t(ws,"CODE",{});var Vu=l(ua);Ip=r(Vu,"max_length"),Vu.forEach(n),Cp=r(ws,` argument or the maximum
length accepted by the model if no `),da=t(ws,"CODE",{});var Xu=l(da);Op=r(Xu,"max_length"),Xu.forEach(n),Sp=r(ws," is provided ("),ma=t(ws,"CODE",{});var Zu=l(ma);Rp=r(Zu,"max_length=None"),Zu.forEach(n),Np=r(ws,`). This will only truncate
the second sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.`),ws.forEach(n),Yp=c(Gs),O=t(Gs,"LI",{});var ys=l(O);ba=t(ys,"CODE",{});var sd=l(ba);Hp=r(sd,"'only_first'"),sd.forEach(n),Lp=r(ys," truncate to a maximum length specified by the "),fa=t(ys,"CODE",{});var ed=l(fa);Fp=r(ed,"max_length"),ed.forEach(n),Bp=r(ys,` argument or the maximum
length accepted by the model if no `),ja=t(ys,"CODE",{});var nd=l(ja);Gp=r(nd,"max_length"),nd.forEach(n),Mp=r(ys," is provided ("),_a=t(ys,"CODE",{});var ad=l(_a);Wp=r(ad,"max_length=None"),ad.forEach(n),Up=r(ys,`). This will only truncate
the first sentence of a pair if a pair of sequence (or a batch of pairs of sequences) is provided.`),ys.forEach(n),Jp=c(Gs),Os=t(Gs,"LI",{});var At=l(Os);ga=t(At,"CODE",{});var td=l(ga);Kp=r(td,"False"),td.forEach(n),Qp=r(At," or "),va=t(At,"CODE",{});var ld=l(va);Vp=r(ld,"'do_not_truncate'"),ld.forEach(n),Xp=r(At,` to not truncate the sequences. As we have seen before, this is the
default behavior.`),At.forEach(n),Gs.forEach(n),uo.forEach(n),Zp=c(nn),Ea=t(nn,"LI",{});var od=l(Ea);M=t(od,"P",{});var _e=l(M);wa=t(_e,"CODE",{});var rd=l(wa);sc=r(rd,"max_length"),rd.forEach(n),ec=r(_e," to control the length of the padding/truncation. It can be an integer or "),ya=t(_e,"CODE",{});var pd=l(ya);nc=r(pd,"None"),pd.forEach(n),ac=r(_e,`, in which case
it will default to the maximum length the model can accept. If the model has no specific maximum input length,
truncation/padding to `),ka=t(_e,"CODE",{});var cd=l(ka);tc=r(cd,"max_length"),cd.forEach(n),lc=r(_e," is deactivated."),_e.forEach(n),od.forEach(n),nn.forEach(n),vl=c(e),k=t(e,"P",{});var S=l(k);oc=r(S,`Here is a table summarizing the recommend way to setup padding and truncation. If you use pair of inputs sequence in
any of the following examples, you can replace `),Ta=t(S,"CODE",{});var id=l(Ta);rc=r(id,"truncation=True"),id.forEach(n),pc=r(S," by a "),$a=t(S,"CODE",{});var hd=l($a);cc=r(hd,"STRATEGY"),hd.forEach(n),ic=r(S,` selected in
`),xa=t(S,"CODE",{});var ud=l(xa);hc=r(ud,"['only_first', 'only_second', 'longest_first']"),ud.forEach(n),uc=r(S,", i.e. "),qa=t(S,"CODE",{});var dd=l(qa);dc=r(dd,"truncation='only_second'"),dd.forEach(n),mc=r(S," or "),za=t(S,"CODE",{});var md=l(za);bc=r(md,"truncation= 'longest_first'"),md.forEach(n),fc=r(S," to control how both sequence in the pair are truncated as detailed before."),S.forEach(n),El=c(e),Ss=t(e,"TABLE",{});var mo=l(Ss);Da=t(mo,"THEAD",{});var bd=l(Da);ss=t(bd,"TR",{});var tn=l(ss);Aa=t(tn,"TH",{});var fd=l(Aa);jc=r(fd,"Truncation"),fd.forEach(n),_c=c(tn),Pa=t(tn,"TH",{});var jd=l(Pa);gc=r(jd,"Padding"),jd.forEach(n),vc=c(tn),Ia=t(tn,"TH",{});var _d=l(Ia);Ec=r(_d,"Instruction"),_d.forEach(n),tn.forEach(n),bd.forEach(n),wc=c(mo),u=t(mo,"TBODY",{});var d=l(u);es=t(d,"TR",{});var ln=l(es);Ca=t(ln,"TD",{});var gd=l(Ca);yc=r(gd,"no truncation"),gd.forEach(n),kc=c(ln),Oa=t(ln,"TD",{});var vd=l(Oa);Tc=r(vd,"no padding"),vd.forEach(n),$c=c(ln),Sa=t(ln,"TD",{});var Ed=l(Sa);Ra=t(Ed,"CODE",{});var wd=l(Ra);xc=r(wd,"tokenizer(batch_sentences)"),wd.forEach(n),Ed.forEach(n),ln.forEach(n),qc=c(d),ns=t(d,"TR",{});var on=l(ns);wl=t(on,"TD",{}),l(wl).forEach(n),zc=c(on),Na=t(on,"TD",{});var yd=l(Na);Dc=r(yd,"padding to max sequence in batch"),yd.forEach(n),Ac=c(on),Be=t(on,"TD",{});var Ah=l(Be);Ya=t(Ah,"CODE",{});var kd=l(Ya);Pc=r(kd,"tokenizer(batch_sentences, padding=True)"),kd.forEach(n),Ic=r(Ah," or"),Ah.forEach(n),on.forEach(n),Cc=c(d),as=t(d,"TR",{});var rn=l(as);yl=t(rn,"TD",{}),l(yl).forEach(n),Oc=c(rn),kl=t(rn,"TD",{}),l(kl).forEach(n),Sc=c(rn),Ha=t(rn,"TD",{});var Td=l(Ha);La=t(Td,"CODE",{});var $d=l(La);Rc=r($d,"tokenizer(batch_sentences, padding='longest')"),$d.forEach(n),Td.forEach(n),rn.forEach(n),Nc=c(d),ts=t(d,"TR",{});var pn=l(ts);Tl=t(pn,"TD",{}),l(Tl).forEach(n),Yc=c(pn),Fa=t(pn,"TD",{});var xd=l(Fa);Hc=r(xd,"padding to max model input length"),xd.forEach(n),Lc=c(pn),Ba=t(pn,"TD",{});var qd=l(Ba);Ga=t(qd,"CODE",{});var zd=l(Ga);Fc=r(zd,"tokenizer(batch_sentences, padding='max_length')"),zd.forEach(n),qd.forEach(n),pn.forEach(n),Bc=c(d),ls=t(d,"TR",{});var cn=l(ls);$l=t(cn,"TD",{}),l($l).forEach(n),Gc=c(cn),Ma=t(cn,"TD",{});var Dd=l(Ma);Mc=r(Dd,"padding to specific length"),Dd.forEach(n),Wc=c(cn),Wa=t(cn,"TD",{});var Ad=l(Wa);Ua=t(Ad,"CODE",{});var Pd=l(Ua);Uc=r(Pd,"tokenizer(batch_sentences, padding='max_length', max_length=42)"),Pd.forEach(n),Ad.forEach(n),cn.forEach(n),Jc=c(d),os=t(d,"TR",{});var hn=l(os);Ja=t(hn,"TD",{});var Id=l(Ja);Kc=r(Id,"truncation to max model input length"),Id.forEach(n),Qc=c(hn),Ka=t(hn,"TD",{});var Cd=l(Ka);Vc=r(Cd,"no padding"),Cd.forEach(n),Xc=c(hn),Ge=t(hn,"TD",{});var Ph=l(Ge);Qa=t(Ph,"CODE",{});var Od=l(Qa);Zc=r(Od,"tokenizer(batch_sentences, truncation=True)"),Od.forEach(n),si=r(Ph," or"),Ph.forEach(n),hn.forEach(n),ei=c(d),rs=t(d,"TR",{});var un=l(rs);xl=t(un,"TD",{}),l(xl).forEach(n),ni=c(un),ql=t(un,"TD",{}),l(ql).forEach(n),ai=c(un),Va=t(un,"TD",{});var Sd=l(Va);Xa=t(Sd,"CODE",{});var Rd=l(Xa);ti=r(Rd,"tokenizer(batch_sentences, truncation=STRATEGY)"),Rd.forEach(n),Sd.forEach(n),un.forEach(n),li=c(d),ps=t(d,"TR",{});var dn=l(ps);zl=t(dn,"TD",{}),l(zl).forEach(n),oi=c(dn),Za=t(dn,"TD",{});var Nd=l(Za);ri=r(Nd,"padding to max sequence in batch"),Nd.forEach(n),pi=c(dn),Me=t(dn,"TD",{});var Ih=l(Me);st=t(Ih,"CODE",{});var Yd=l(st);ci=r(Yd,"tokenizer(batch_sentences, padding=True, truncation=True)"),Yd.forEach(n),ii=r(Ih," or"),Ih.forEach(n),dn.forEach(n),hi=c(d),cs=t(d,"TR",{});var mn=l(cs);Dl=t(mn,"TD",{}),l(Dl).forEach(n),ui=c(mn),Al=t(mn,"TD",{}),l(Al).forEach(n),di=c(mn),et=t(mn,"TD",{});var Hd=l(et);nt=t(Hd,"CODE",{});var Ld=l(nt);mi=r(Ld,"tokenizer(batch_sentences, padding=True, truncation=STRATEGY)"),Ld.forEach(n),Hd.forEach(n),mn.forEach(n),bi=c(d),is=t(d,"TR",{});var bn=l(is);Pl=t(bn,"TD",{}),l(Pl).forEach(n),fi=c(bn),at=t(bn,"TD",{});var Fd=l(at);ji=r(Fd,"padding to max model input length"),Fd.forEach(n),_i=c(bn),We=t(bn,"TD",{});var Ch=l(We);tt=t(Ch,"CODE",{});var Bd=l(tt);gi=r(Bd,"tokenizer(batch_sentences, padding='max_length', truncation=True)"),Bd.forEach(n),vi=r(Ch," or"),Ch.forEach(n),bn.forEach(n),Ei=c(d),hs=t(d,"TR",{});var fn=l(hs);Il=t(fn,"TD",{}),l(Il).forEach(n),wi=c(fn),Cl=t(fn,"TD",{}),l(Cl).forEach(n),yi=c(fn),lt=t(fn,"TD",{});var Gd=l(lt);ot=t(Gd,"CODE",{});var Md=l(ot);ki=r(Md,"tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY)"),Md.forEach(n),Gd.forEach(n),fn.forEach(n),Ti=c(d),us=t(d,"TR",{});var jn=l(us);Ol=t(jn,"TD",{}),l(Ol).forEach(n),$i=c(jn),rt=t(jn,"TD",{});var Wd=l(rt);xi=r(Wd,"padding to specific length"),Wd.forEach(n),qi=c(jn),pt=t(jn,"TD",{});var Ud=l(pt);zi=r(Ud,"Not possible"),Ud.forEach(n),jn.forEach(n),Di=c(d),ds=t(d,"TR",{});var _n=l(ds);ct=t(_n,"TD",{});var Jd=l(ct);Ai=r(Jd,"truncation to specific length"),Jd.forEach(n),Pi=c(_n),it=t(_n,"TD",{});var Kd=l(it);Ii=r(Kd,"no padding"),Kd.forEach(n),Ci=c(_n),Ue=t(_n,"TD",{});var Oh=l(Ue);ht=t(Oh,"CODE",{});var Qd=l(ht);Oi=r(Qd,"tokenizer(batch_sentences, truncation=True, max_length=42)"),Qd.forEach(n),Si=r(Oh," or"),Oh.forEach(n),_n.forEach(n),Ri=c(d),ms=t(d,"TR",{});var gn=l(ms);Sl=t(gn,"TD",{}),l(Sl).forEach(n),Ni=c(gn),Rl=t(gn,"TD",{}),l(Rl).forEach(n),Yi=c(gn),ut=t(gn,"TD",{});var Vd=l(ut);dt=t(Vd,"CODE",{});var Xd=l(dt);Hi=r(Xd,"tokenizer(batch_sentences, truncation=STRATEGY, max_length=42)"),Xd.forEach(n),Vd.forEach(n),gn.forEach(n),Li=c(d),bs=t(d,"TR",{});var vn=l(bs);Nl=t(vn,"TD",{}),l(Nl).forEach(n),Fi=c(vn),mt=t(vn,"TD",{});var Zd=l(mt);Bi=r(Zd,"padding to max sequence in batch"),Zd.forEach(n),Gi=c(vn),Je=t(vn,"TD",{});var Sh=l(Je);bt=t(Sh,"CODE",{});var sm=l(bt);Mi=r(sm,"tokenizer(batch_sentences, padding=True, truncation=True, max_length=42)"),sm.forEach(n),Wi=r(Sh," or"),Sh.forEach(n),vn.forEach(n),Ui=c(d),fs=t(d,"TR",{});var En=l(fs);Yl=t(En,"TD",{}),l(Yl).forEach(n),Ji=c(En),Hl=t(En,"TD",{}),l(Hl).forEach(n),Ki=c(En),ft=t(En,"TD",{});var em=l(ft);jt=t(em,"CODE",{});var nm=l(jt);Qi=r(nm,"tokenizer(batch_sentences, padding=True, truncation=STRATEGY, max_length=42)"),nm.forEach(n),em.forEach(n),En.forEach(n),Vi=c(d),js=t(d,"TR",{});var wn=l(js);Ll=t(wn,"TD",{}),l(Ll).forEach(n),Xi=c(wn),_t=t(wn,"TD",{});var am=l(_t);Zi=r(am,"padding to max model input length"),am.forEach(n),sh=c(wn),gt=t(wn,"TD",{});var tm=l(gt);eh=r(tm,"Not possible"),tm.forEach(n),wn.forEach(n),nh=c(d),_s=t(d,"TR",{});var yn=l(_s);Fl=t(yn,"TD",{}),l(Fl).forEach(n),ah=c(yn),vt=t(yn,"TD",{});var lm=l(vt);th=r(lm,"padding to specific length"),lm.forEach(n),lh=c(yn),Ke=t(yn,"TD",{});var Rh=l(Ke);Et=t(Rh,"CODE",{});var om=l(Et);oh=r(om,"tokenizer(batch_sentences, padding='max_length', truncation=True, max_length=42)"),om.forEach(n),rh=r(Rh," or"),Rh.forEach(n),yn.forEach(n),ph=c(d),gs=t(d,"TR",{});var kn=l(gs);Bl=t(kn,"TD",{}),l(Bl).forEach(n),ch=c(kn),Gl=t(kn,"TD",{}),l(Gl).forEach(n),ih=c(kn),wt=t(kn,"TD",{});var rm=l(wt);yt=t(rm,"CODE",{});var pm=l(yt);hh=r(pm,"tokenizer(batch_sentences, padding='max_length', truncation=STRATEGY, max_length=42)"),pm.forEach(n),rm.forEach(n),kn.forEach(n),d.forEach(n),mo.forEach(n),Ml=c(e),vs=t(e,"H2",{class:!0});var bo=l(vs);Rs=t(bo,"A",{id:!0,class:!0,href:!0});var cm=l(Rs);kt=t(cm,"SPAN",{});var im=l(kt);_(ie.$$.fragment,im),im.forEach(n),cm.forEach(n),uh=c(bo),Tt=t(bo,"SPAN",{});var hm=l(Tt);dh=r(hm,"Pre-tokenized inputs"),hm.forEach(n),bo.forEach(n),Wl=c(e),W=t(e,"P",{});var Tn=l(W);mh=r(Tn,`The tokenizer also accept pre-tokenized inputs. This is particularly useful when you want to compute labels and extract
predictions in `),he=t(Tn,"A",{href:!0,rel:!0});var um=l(he);bh=r(um,"named entity recognition (NER)"),um.forEach(n),fh=r(Tn,` or
`),ue=t(Tn,"A",{href:!0,rel:!0});var dm=l(ue);jh=r(dm,"part-of-speech tagging (POS tagging)"),dm.forEach(n),_h=r(Tn,"."),Tn.forEach(n),Ul=c(e),_(Ns.$$.fragment,e),Jl=c(e),Ys=t(e,"P",{});var fo=l(Ys);gh=r(fo,"If you want to use pre-tokenized inputs, just set "),$t=t(fo,"CODE",{});var mm=l($t);vh=r(mm,"is_split_into_words=True"),mm.forEach(n),Eh=r(fo,` when passing your inputs to the
tokenizer. For instance, we have:`),fo.forEach(n),Kl=c(e),_(de.$$.fragment,e),Ql=c(e),Hs=t(e,"P",{});var jo=l(Hs);wh=r(jo,`Note that the tokenizer still adds the ids of special tokens (if applicable) unless you pass
`),xt=t(jo,"CODE",{});var bm=l(xt);yh=r(bm,"add_special_tokens=False"),bm.forEach(n),kh=r(jo,"."),jo.forEach(n),Vl=c(e),Qe=t(e,"P",{});var fm=l(Qe);Th=r(fm,`This works exactly as before for batch of sentences or batch of pairs of sentences. You can encode a batch of sentences
like this:`),fm.forEach(n),Xl=c(e),_(me.$$.fragment,e),Zl=c(e),Ve=t(e,"P",{});var jm=l(Ve);$h=r(jm,"or a batch of pair sentences like this:"),jm.forEach(n),so=c(e),_(be.$$.fragment,e),eo=c(e),Xe=t(e,"P",{});var _m=l(Xe);xh=r(_m,"And you can add padding, truncation as well as directly return tensors like before:"),_m.forEach(n),no=c(e),_(fe.$$.fragment,e),this.h()},h(){m(b,"name","hf:doc:metadata"),m(b,"content",JSON.stringify(qm)),m(y,"id","preprocessing-data"),m(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(y,"href","#preprocessing-data"),m(f,"class","relative group"),m(ge,"href","main_classes/tokenizer"),m(ve,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),m(Ee,"href","quicktour"),m(we,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),m(xs,"id","base-use"),m(xs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(xs,"href","#base-use"),m(Q,"class","relative group"),m(ye,"href","/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),m(ke,"href","glossary#input-ids"),m(Te,"href","glossary#attention-mask"),m($e,"href","#preprocessing-pairs-of-sentences"),m(xe,"href","glossary#token-type-ids"),m(Ie,"href","glossary#attention-mask"),m(Ce,"id","sentence-pairs"),m(Ds,"id","preprocessing-pairs-of-sentences"),m(Ds,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ds,"href","#preprocessing-pairs-of-sentences"),m(V,"class","relative group"),m(Se,"href","glossary#token-type-ids"),m(Ps,"id","everything-you-always-wanted-to-know-about-padding-and-truncation"),m(Ps,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ps,"href","#everything-you-always-wanted-to-know-about-padding-and-truncation"),m(X,"class","relative group"),m(Rs,"id","pretokenized-inputs"),m(Rs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Rs,"href","#pretokenized-inputs"),m(vs,"class","relative group"),m(he,"href","https://en.wikipedia.org/wiki/Named-entity_recognition"),m(he,"rel","nofollow"),m(ue,"href","https://en.wikipedia.org/wiki/Part-of-speech_tagging"),m(ue,"rel","nofollow")},m(e,i){s(document.head,b),h(e,T,i),h(e,f,i),s(f,y),s(y,K),g(z,K,null),s(f,H),s(f,ks),s(ks,_o),h(e,It,i),h(e,L,i),s(L,go),s(L,ge),s(ge,vo),s(L,Eo),s(L,ve),s(ve,wo),s(L,yo),h(e,Ct,i),h(e,D,i),s(D,ko),s(D,Ee),s(Ee,To),s(D,$o),s(D,$n),s($n,xo),s(D,qo),s(D,xn),s(xn,zo),s(D,Do),h(e,Ot,i),g(Ts,e,i),h(e,St,i),h(e,$s,i),s($s,Ao),s($s,we),s(we,Po),s($s,Io),h(e,Rt,i),g(Ws,e,i),h(e,Nt,i),h(e,Q,i),s(Q,xs),s(xs,qn),g(Us,qn,null),s(Q,Co),s(Q,zn),s(zn,Oo),h(e,Yt,i),g(Js,e,i),h(e,Ht,i),h(e,F,i),s(F,So),s(F,ye),s(ye,Ro),s(F,No),s(F,Dn),s(Dn,Yo),s(F,Ho),h(e,Lt,i),g(Ks,e,i),h(e,Ft,i),h(e,$,i),s($,Lo),s($,ke),s(ke,Fo),s($,Bo),s($,Te),s(Te,Go),s($,Mo),s($,$e),s($e,Wo),s($,Uo),s($,xe),s(xe,Jo),s($,Ko),h(e,Bt,i),h(e,qe,i),s(qe,Qo),h(e,Gt,i),g(Qs,e,i),h(e,Mt,i),h(e,A,i),s(A,Vo),s(A,An),s(An,Xo),s(A,Zo),s(A,Pn),s(Pn,sr),s(A,er),s(A,In),s(In,nr),s(A,ar),h(e,Wt,i),h(e,ze,i),s(ze,tr),h(e,Ut,i),g(Vs,e,i),h(e,Jt,i),h(e,De,i),s(De,lr),h(e,Kt,i),h(e,Ae,i),s(Ae,or),h(e,Qt,i),h(e,B,i),s(B,Cn),s(Cn,rr),s(B,pr),s(B,On),s(On,cr),s(B,ir),s(B,Sn),s(Sn,hr),h(e,Vt,i),h(e,Pe,i),s(Pe,ur),h(e,Xt,i),g(Xs,e,i),h(e,Zt,i),h(e,qs,i),s(qs,dr),s(qs,Ie),s(Ie,mr),s(qs,br),h(e,sl,i),h(e,zs,i),s(zs,fr),s(zs,Rn),s(Rn,jr),s(zs,_r),h(e,el,i),h(e,Ce,i),h(e,nl,i),h(e,V,i),s(V,Ds),s(Ds,Nn),g(Zs,Nn,null),s(V,gr),s(V,Yn),s(Yn,vr),h(e,al,i),g(se,e,i),h(e,tl,i),h(e,ee,i),s(ee,Er),s(ee,Hn),s(Hn,wr),h(e,ll,i),h(e,Oe,i),s(Oe,yr),h(e,ol,i),g(ne,e,i),h(e,rl,i),h(e,x,i),s(x,kr),s(x,Se),s(Se,Tr),s(x,$r),s(x,Ln),s(Ln,xr),s(x,qr),s(x,Fn),s(Fn,zr),s(x,Dr),s(x,Bn),s(Bn,Ar),s(x,Pr),h(e,pl,i),h(e,Re,i),s(Re,Ir),h(e,cl,i),g(ae,e,i),h(e,il,i),h(e,Ne,i),s(Ne,Cr),h(e,hl,i),g(te,e,i),h(e,ul,i),h(e,Ye,i),s(Ye,Or),h(e,dl,i),h(e,As,i),s(As,Sr),s(As,Gn),s(Gn,Rr),s(As,Nr),h(e,ml,i),g(le,e,i),h(e,bl,i),h(e,He,i),s(He,Yr),h(e,fl,i),g(oe,e,i),h(e,jl,i),h(e,X,i),s(X,Ps),s(Ps,Mn),g(re,Mn,null),s(X,Hr),s(X,Wn),s(Wn,Lr),h(e,_l,i),h(e,P,i),s(P,Fr),s(P,Un),s(Un,Br),s(P,Gr),s(P,Jn),s(Jn,Mr),s(P,Wr),s(P,Kn),s(Kn,Ur),s(P,Jr),h(e,gl,i),h(e,G,i),s(G,pe),s(pe,Le),s(Le,Qn),s(Qn,Kr),s(Le,Qr),s(pe,Vr),s(pe,Z),s(Z,Is),s(Is,Vn),s(Vn,Xr),s(Is,Zr),s(Is,Xn),s(Xn,sp),s(Is,ep),s(Z,np),s(Z,I),s(I,Zn),s(Zn,ap),s(I,tp),s(I,sa),s(sa,lp),s(I,op),s(I,ea),s(ea,rp),s(I,pp),s(I,na),s(na,cp),s(I,ip),s(Z,hp),s(Z,Cs),s(Cs,aa),s(aa,up),s(Cs,dp),s(Cs,ta),s(ta,mp),s(Cs,bp),s(G,fp),s(G,ce),s(ce,Fe),s(Fe,la),s(la,jp),s(Fe,_p),s(ce,gp),s(ce,R),s(R,q),s(q,oa),s(oa,vp),s(q,Ep),s(q,ra),s(ra,wp),s(q,yp),s(q,pa),s(pa,kp),s(q,Tp),s(q,ca),s(ca,$p),s(q,xp),s(q,ia),s(ia,qp),s(q,zp),s(R,Dp),s(R,C),s(C,ha),s(ha,Ap),s(C,Pp),s(C,ua),s(ua,Ip),s(C,Cp),s(C,da),s(da,Op),s(C,Sp),s(C,ma),s(ma,Rp),s(C,Np),s(R,Yp),s(R,O),s(O,ba),s(ba,Hp),s(O,Lp),s(O,fa),s(fa,Fp),s(O,Bp),s(O,ja),s(ja,Gp),s(O,Mp),s(O,_a),s(_a,Wp),s(O,Up),s(R,Jp),s(R,Os),s(Os,ga),s(ga,Kp),s(Os,Qp),s(Os,va),s(va,Vp),s(Os,Xp),s(G,Zp),s(G,Ea),s(Ea,M),s(M,wa),s(wa,sc),s(M,ec),s(M,ya),s(ya,nc),s(M,ac),s(M,ka),s(ka,tc),s(M,lc),h(e,vl,i),h(e,k,i),s(k,oc),s(k,Ta),s(Ta,rc),s(k,pc),s(k,$a),s($a,cc),s(k,ic),s(k,xa),s(xa,hc),s(k,uc),s(k,qa),s(qa,dc),s(k,mc),s(k,za),s(za,bc),s(k,fc),h(e,El,i),h(e,Ss,i),s(Ss,Da),s(Da,ss),s(ss,Aa),s(Aa,jc),s(ss,_c),s(ss,Pa),s(Pa,gc),s(ss,vc),s(ss,Ia),s(Ia,Ec),s(Ss,wc),s(Ss,u),s(u,es),s(es,Ca),s(Ca,yc),s(es,kc),s(es,Oa),s(Oa,Tc),s(es,$c),s(es,Sa),s(Sa,Ra),s(Ra,xc),s(u,qc),s(u,ns),s(ns,wl),s(ns,zc),s(ns,Na),s(Na,Dc),s(ns,Ac),s(ns,Be),s(Be,Ya),s(Ya,Pc),s(Be,Ic),s(u,Cc),s(u,as),s(as,yl),s(as,Oc),s(as,kl),s(as,Sc),s(as,Ha),s(Ha,La),s(La,Rc),s(u,Nc),s(u,ts),s(ts,Tl),s(ts,Yc),s(ts,Fa),s(Fa,Hc),s(ts,Lc),s(ts,Ba),s(Ba,Ga),s(Ga,Fc),s(u,Bc),s(u,ls),s(ls,$l),s(ls,Gc),s(ls,Ma),s(Ma,Mc),s(ls,Wc),s(ls,Wa),s(Wa,Ua),s(Ua,Uc),s(u,Jc),s(u,os),s(os,Ja),s(Ja,Kc),s(os,Qc),s(os,Ka),s(Ka,Vc),s(os,Xc),s(os,Ge),s(Ge,Qa),s(Qa,Zc),s(Ge,si),s(u,ei),s(u,rs),s(rs,xl),s(rs,ni),s(rs,ql),s(rs,ai),s(rs,Va),s(Va,Xa),s(Xa,ti),s(u,li),s(u,ps),s(ps,zl),s(ps,oi),s(ps,Za),s(Za,ri),s(ps,pi),s(ps,Me),s(Me,st),s(st,ci),s(Me,ii),s(u,hi),s(u,cs),s(cs,Dl),s(cs,ui),s(cs,Al),s(cs,di),s(cs,et),s(et,nt),s(nt,mi),s(u,bi),s(u,is),s(is,Pl),s(is,fi),s(is,at),s(at,ji),s(is,_i),s(is,We),s(We,tt),s(tt,gi),s(We,vi),s(u,Ei),s(u,hs),s(hs,Il),s(hs,wi),s(hs,Cl),s(hs,yi),s(hs,lt),s(lt,ot),s(ot,ki),s(u,Ti),s(u,us),s(us,Ol),s(us,$i),s(us,rt),s(rt,xi),s(us,qi),s(us,pt),s(pt,zi),s(u,Di),s(u,ds),s(ds,ct),s(ct,Ai),s(ds,Pi),s(ds,it),s(it,Ii),s(ds,Ci),s(ds,Ue),s(Ue,ht),s(ht,Oi),s(Ue,Si),s(u,Ri),s(u,ms),s(ms,Sl),s(ms,Ni),s(ms,Rl),s(ms,Yi),s(ms,ut),s(ut,dt),s(dt,Hi),s(u,Li),s(u,bs),s(bs,Nl),s(bs,Fi),s(bs,mt),s(mt,Bi),s(bs,Gi),s(bs,Je),s(Je,bt),s(bt,Mi),s(Je,Wi),s(u,Ui),s(u,fs),s(fs,Yl),s(fs,Ji),s(fs,Hl),s(fs,Ki),s(fs,ft),s(ft,jt),s(jt,Qi),s(u,Vi),s(u,js),s(js,Ll),s(js,Xi),s(js,_t),s(_t,Zi),s(js,sh),s(js,gt),s(gt,eh),s(u,nh),s(u,_s),s(_s,Fl),s(_s,ah),s(_s,vt),s(vt,th),s(_s,lh),s(_s,Ke),s(Ke,Et),s(Et,oh),s(Ke,rh),s(u,ph),s(u,gs),s(gs,Bl),s(gs,ch),s(gs,Gl),s(gs,ih),s(gs,wt),s(wt,yt),s(yt,hh),h(e,Ml,i),h(e,vs,i),s(vs,Rs),s(Rs,kt),g(ie,kt,null),s(vs,uh),s(vs,Tt),s(Tt,dh),h(e,Wl,i),h(e,W,i),s(W,mh),s(W,he),s(he,bh),s(W,fh),s(W,ue),s(ue,jh),s(W,_h),h(e,Ul,i),g(Ns,e,i),h(e,Jl,i),h(e,Ys,i),s(Ys,gh),s(Ys,$t),s($t,vh),s(Ys,Eh),h(e,Kl,i),g(de,e,i),h(e,Ql,i),h(e,Hs,i),s(Hs,wh),s(Hs,xt),s(xt,yh),s(Hs,kh),h(e,Vl,i),h(e,Qe,i),s(Qe,Th),h(e,Xl,i),g(me,e,i),h(e,Zl,i),h(e,Ve,i),s(Ve,$h),h(e,so,i),g(be,e,i),h(e,eo,i),h(e,Xe,i),s(Xe,xh),h(e,no,i),g(fe,e,i),ao=!0},p(e,[i]){const je={};i&2&&(je.$$scope={dirty:i,ctx:e}),Ts.$set(je);const qt={};i&2&&(qt.$$scope={dirty:i,ctx:e}),Ns.$set(qt)},i(e){ao||(v(z.$$.fragment,e),v(Ts.$$.fragment,e),v(Ws.$$.fragment,e),v(Us.$$.fragment,e),v(Js.$$.fragment,e),v(Ks.$$.fragment,e),v(Qs.$$.fragment,e),v(Vs.$$.fragment,e),v(Xs.$$.fragment,e),v(Zs.$$.fragment,e),v(se.$$.fragment,e),v(ne.$$.fragment,e),v(ae.$$.fragment,e),v(te.$$.fragment,e),v(le.$$.fragment,e),v(oe.$$.fragment,e),v(re.$$.fragment,e),v(ie.$$.fragment,e),v(Ns.$$.fragment,e),v(de.$$.fragment,e),v(me.$$.fragment,e),v(be.$$.fragment,e),v(fe.$$.fragment,e),ao=!0)},o(e){E(z.$$.fragment,e),E(Ts.$$.fragment,e),E(Ws.$$.fragment,e),E(Us.$$.fragment,e),E(Js.$$.fragment,e),E(Ks.$$.fragment,e),E(Qs.$$.fragment,e),E(Vs.$$.fragment,e),E(Xs.$$.fragment,e),E(Zs.$$.fragment,e),E(se.$$.fragment,e),E(ne.$$.fragment,e),E(ae.$$.fragment,e),E(te.$$.fragment,e),E(le.$$.fragment,e),E(oe.$$.fragment,e),E(re.$$.fragment,e),E(ie.$$.fragment,e),E(Ns.$$.fragment,e),E(de.$$.fragment,e),E(me.$$.fragment,e),E(be.$$.fragment,e),E(fe.$$.fragment,e),ao=!1},d(e){n(b),e&&n(T),e&&n(f),w(z),e&&n(It),e&&n(L),e&&n(Ct),e&&n(D),e&&n(Ot),w(Ts,e),e&&n(St),e&&n($s),e&&n(Rt),w(Ws,e),e&&n(Nt),e&&n(Q),w(Us),e&&n(Yt),w(Js,e),e&&n(Ht),e&&n(F),e&&n(Lt),w(Ks,e),e&&n(Ft),e&&n($),e&&n(Bt),e&&n(qe),e&&n(Gt),w(Qs,e),e&&n(Mt),e&&n(A),e&&n(Wt),e&&n(ze),e&&n(Ut),w(Vs,e),e&&n(Jt),e&&n(De),e&&n(Kt),e&&n(Ae),e&&n(Qt),e&&n(B),e&&n(Vt),e&&n(Pe),e&&n(Xt),w(Xs,e),e&&n(Zt),e&&n(qs),e&&n(sl),e&&n(zs),e&&n(el),e&&n(Ce),e&&n(nl),e&&n(V),w(Zs),e&&n(al),w(se,e),e&&n(tl),e&&n(ee),e&&n(ll),e&&n(Oe),e&&n(ol),w(ne,e),e&&n(rl),e&&n(x),e&&n(pl),e&&n(Re),e&&n(cl),w(ae,e),e&&n(il),e&&n(Ne),e&&n(hl),w(te,e),e&&n(ul),e&&n(Ye),e&&n(dl),e&&n(As),e&&n(ml),w(le,e),e&&n(bl),e&&n(He),e&&n(fl),w(oe,e),e&&n(jl),e&&n(X),w(re),e&&n(_l),e&&n(P),e&&n(gl),e&&n(G),e&&n(vl),e&&n(k),e&&n(El),e&&n(Ss),e&&n(Ml),e&&n(vs),w(ie),e&&n(Wl),e&&n(W),e&&n(Ul),w(Ns,e),e&&n(Jl),e&&n(Ys),e&&n(Kl),w(de,e),e&&n(Ql),e&&n(Hs),e&&n(Vl),e&&n(Qe),e&&n(Xl),w(me,e),e&&n(Zl),e&&n(Ve),e&&n(so),w(be,e),e&&n(eo),e&&n(Xe),e&&n(no),w(fe,e)}}}const qm={local:"preprocessing-data",sections:[{local:"base-use",title:"Base use"},{local:"preprocessing-pairs-of-sentences",title:"Preprocessing pairs of sentences"},{local:"everything-you-always-wanted-to-know-about-padding-and-truncation",title:"Everything you always wanted to know about padding and truncation"},{local:"pretokenized-inputs",title:"Pre-tokenized inputs"}],title:"Preprocessing data"};function zm(Ms,b,T){let{fw:f}=b;return Ms.$$set=y=>{"fw"in y&&T(0,f=y.fw)},[f]}class Rm extends Em{constructor(b){super();wm(this,b,zm,xm,ym,{fw:0})}}export{Rm as default,qm as metadata};
