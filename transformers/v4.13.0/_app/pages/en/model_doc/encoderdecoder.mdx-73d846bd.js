import{S as nv,i as rv,s as av,e as r,k as i,w as v,t,L as sv,c as a,d as n,m as l,a as s,x as b,h as o,b as c,J as e,g as m,y as E,q as y,o as k,B as T}from"../../../chunks/vendor-e859c359.js";import{T as Kf}from"../../../chunks/Tip-edc75249.js";import{D as Z}from"../../../chunks/Docstring-ade913b3.js";import{C as xt}from"../../../chunks/CodeBlock-ce4317c2.js";import{I as Za}from"../../../chunks/IconCopyLink-5fae3b20.js";import"../../../chunks/CopyButton-77addb3d.js";function dv(Te){let h,F,f,j,L;return{c(){h=r("p"),F=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=r("code"),j=t("Module"),L=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(M){h=a(M,"P",{});var D=s(h);F=o(D,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=a(D,"CODE",{});var B=s(f);j=o(B,"Module"),B.forEach(n),L=o(D,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),D.forEach(n)},m(M,D){m(M,h,D),e(h,F),e(h,f),e(f,j),e(h,L)},d(M){M&&n(h)}}}function iv(Te){let h,F,f,j,L;return{c(){h=r("p"),F=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=r("code"),j=t("Module"),L=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(M){h=a(M,"P",{});var D=s(h);F=o(D,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=a(D,"CODE",{});var B=s(f);j=o(B,"Module"),B.forEach(n),L=o(D,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),D.forEach(n)},m(M,D){m(M,h,D),e(h,F),e(h,f),e(f,j),e(h,L)},d(M){M&&n(h)}}}function lv(Te){let h,F,f,j,L;return{c(){h=r("p"),F=t(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=r("code"),j=t("Module"),L=t(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(M){h=a(M,"P",{});var D=s(h);F=o(D,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),f=a(D,"CODE",{});var B=s(f);j=o(B,"Module"),B.forEach(n),L=o(D,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),D.forEach(n)},m(M,D){m(M,h,D),e(h,F),e(h,f),e(f,j),e(h,L)},d(M){M&&n(h)}}}function cv(Te){let h,F,f,j,L,M,D,B,Ss,Ka,Xe,Ls,zo,Os,Ns,Qa,et,Bs,zt,Us,Ws,Xa,tt,Vs,jo,Rs,Gs,es,he,Hs,$o,Js,Ys,jt,Zs,Ks,ts,me,Qs,Mn,Xs,ed,Dn,td,od,os,$t,ns,fe,nd,Pt,rd,ad,qt,sd,dd,rs,we,ot,xn,Ct,id,zn,ld,as,C,Ft,cd,nt,Po,pd,hd,qo,md,fd,ud,Me,gd,Co,_d,vd,Fo,bd,Ed,yd,jn,kd,Td,At,wd,rt,It,Md,St,Dd,Ao,xd,zd,jd,at,Lt,$d,De,Pd,$n,qd,Cd,Pn,Fd,Ad,ss,xe,st,qn,Ot,Id,Cn,Sd,ds,$,Nt,Ld,ze,Od,Fn,Nd,Bd,An,Ud,Wd,Vd,Bt,Rd,Ut,Gd,Hd,Jd,In,Yd,Zd,Wt,Kd,Io,Qd,Xd,ei,Vt,ti,Rt,oi,ni,ri,ue,So,ai,si,Sn,di,ii,Ln,li,ci,pi,U,Gt,hi,je,mi,Lo,fi,ui,On,gi,_i,vi,dt,bi,Nn,Ei,yi,Ht,ki,u,Jt,Ti,Bn,wi,Mi,$e,Di,Un,xi,zi,Wn,ji,$i,Pi,Pe,qi,Vn,Ci,Fi,Rn,Ai,Ii,Si,qe,K,Li,Gn,Oi,Ni,Hn,Bi,Ui,Jn,Wi,Vi,Ri,Q,Gi,Yn,Hi,Ji,Oo,Yi,Zi,Zn,Ki,Qi,Xi,O,el,Kn,tl,ol,Qn,nl,rl,Xn,al,sl,er,dl,il,tr,ll,cl,pl,X,hl,or,ml,fl,nr,ul,gl,rr,_l,vl,bl,Ce,ee,El,ar,yl,kl,sr,Tl,wl,dr,Ml,Dl,xl,te,zl,ir,jl,$l,No,Pl,ql,lr,Cl,Fl,Al,N,Il,cr,Sl,Ll,pr,Ol,Nl,hr,Bl,Ul,mr,Wl,Vl,fr,Rl,Gl,Hl,Fe,Jl,ur,Yl,Zl,gr,Kl,Ql,Xl,Ae,ec,_r,tc,oc,vr,nc,rc,ac,Ie,br,sc,dc,Er,ic,lc,yr,cc,pc,Yt,hc,kr,mc,fc,uc,Tr,gc,_c,Zt,is,Se,it,wr,Kt,vc,Mr,bc,ls,P,Qt,Ec,Le,yc,Dr,kc,Tc,xr,wc,Mc,Dc,Xt,xc,eo,zc,jc,$c,zr,Pc,qc,to,Cc,Bo,Fc,Ac,Ic,oo,Sc,no,Lc,Oc,Nc,ge,jr,Bc,Uc,$r,Wc,Vc,Pr,Rc,Gc,Hc,W,ro,Jc,Oe,Yc,Uo,Zc,Kc,qr,Qc,Xc,ep,lt,tp,Cr,op,np,ao,rp,g,so,ap,Fr,sp,dp,Ne,ip,Ar,lp,cp,Ir,pp,hp,mp,Be,oe,fp,Sr,up,gp,Lr,_p,vp,Or,bp,Ep,yp,ne,kp,Nr,Tp,wp,Wo,Mp,Dp,Br,xp,zp,jp,G,$p,Ur,Pp,qp,Wr,Cp,Fp,Vr,Ap,Ip,Rr,Sp,Lp,Op,re,Np,Gr,Bp,Up,Hr,Wp,Vp,Jr,Rp,Gp,Hp,Ue,ae,Jp,Yr,Yp,Zp,Zr,Kp,Qp,Kr,Xp,eh,th,se,oh,Qr,nh,rh,Vo,ah,sh,Xr,dh,ih,lh,H,ch,ea,ph,hh,ta,mh,fh,oa,uh,gh,na,_h,vh,bh,We,Eh,ra,yh,kh,aa,Th,wh,Mh,Ve,Dh,sa,xh,zh,da,jh,$h,Ph,Re,ia,qh,Ch,la,Fh,Ah,ca,Ih,Sh,io,Lh,pa,Oh,Nh,Bh,ha,Uh,Wh,lo,cs,Ge,ct,ma,co,Vh,fa,Rh,ps,q,po,Gh,He,Hh,ua,Jh,Yh,ga,Zh,Kh,Qh,ho,Xh,mo,em,tm,om,_a,nm,rm,fo,am,Ro,sm,dm,im,uo,lm,go,cm,pm,hm,_e,Go,mm,fm,va,um,gm,ba,_m,vm,bm,V,_o,Em,Je,ym,Ho,km,Tm,Ea,wm,Mm,Dm,pt,xm,ya,zm,jm,vo,$m,_,bo,Pm,ka,qm,Cm,Ye,Fm,Ta,Am,Im,wa,Sm,Lm,Om,Eo,de,Nm,Ma,Bm,Um,Da,Wm,Vm,xa,Rm,Gm,Hm,ie,Jm,za,Ym,Zm,Jo,Km,Qm,ja,Xm,ef,tf,le,of,$a,nf,rf,Pa,af,sf,qa,df,lf,cf,yo,ce,pf,Ca,hf,mf,Fa,ff,uf,Aa,gf,_f,vf,pe,bf,Ia,Ef,yf,Yo,kf,Tf,Sa,wf,Mf,Df,Ze,xf,La,zf,jf,Oa,$f,Pf,qf,Ke,Cf,Na,Ff,Af,Ba,If,Sf,Lf,Qe,Ua,Of,Nf,Wa,Bf,Uf,Va,Wf,Vf,ko,Rf,Ra,Gf,Hf,Jf,Ga,Yf,Zf,To,hs;return M=new Za({}),$t=new xt({props:{code:`# a workaround to load from pytorch checkpoint
_model = EncoderDecoderModel.from_pretrained("patrickvonplaten/bert2bert-cnn_dailymail-fp16")
_model.encoder.save_pretrained("./encoder")
_model.decoder.save_pretrained("./decoder")
model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(
    "./encoder", "./decoder", encoder_from_pt=True, decoder_from_pt=True
)
# This is only for copying some specific attributes of this particular model.
model.config = _model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># a workaround to load from pytorch checkpoint</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">_model = EncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/bert2bert-cnn_dailymail-fp16&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">_model.encoder.save_pretrained(<span class="hljs-string">&quot;./encoder&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">_model.decoder.save_pretrained(<span class="hljs-string">&quot;./decoder&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(</span>
<span class="hljs-meta">...</span> <span class="language-python">    <span class="hljs-string">&quot;./encoder&quot;</span>, <span class="hljs-string">&quot;./decoder&quot;</span>, encoder_from_pt=<span class="hljs-literal">True</span>, decoder_from_pt=<span class="hljs-literal">True</span></span>
<span class="hljs-meta">...</span> <span class="language-python">)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># This is only for copying some specific attributes of this particular model.</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model.config = _model.config</span>`}}),Ct=new Za({}),Ft=new Z({props:{name:"class transformers.EncoderDecoderConfig",anchor:"transformers.EncoderDecoderConfig",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/configuration_encoder_decoder.py#L26",parametersDescription:[{anchor:"transformers.EncoderDecoderConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments. Notably:</p>
<ul>
<li><strong>encoder</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration
object that defines the encoder config.</li>
<li><strong>decoder</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014; An instance of a configuration
object that defines the decoder config.</li>
</ul>`,name:"kwargs"}]}}),At=new xt({props:{code:`from transformers import BertConfig, EncoderDecoderConfig, EncoderDecoderModel

# Initializing a BERT bert-base-uncased style configuration
config_encoder = BertConfig()
config_decoder = BertConfig()

config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

# Initializing a Bert2Bert model from the bert-base-uncased style configurations
model = EncoderDecoderModel(config=config)

# Accessing the model configuration
config_encoder = model.config.encoder
config_decoder  = model.config.decoder
# set decoder config to causal lm
config_decoder.is_decoder = True
config_decoder.add_cross_attention = True

# Saving the model, including its configuration
model.save_pretrained('my-model')

# loading model and config from pretrained folder
encoder_decoder_config = EncoderDecoderConfig.from_pretrained('my-model')
model = EncoderDecoderModel.from_pretrained('my-model', config=encoder_decoder_config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertConfig, EncoderDecoderConfig, EncoderDecoderModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BERT bert-base-uncased style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = BertConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder = BertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bert2Bert model from the bert-base-uncased style configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel(config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_encoder = model.config.encoder
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder  = model.config.decoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set decoder config to causal lm</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.is_decoder = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_decoder.add_cross_attention = <span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Saving the model, including its configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&#x27;my-model&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># loading model and config from pretrained folder</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_decoder_config = EncoderDecoderConfig.from_pretrained(<span class="hljs-string">&#x27;my-model&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_pretrained(<span class="hljs-string">&#x27;my-model&#x27;</span>, config=encoder_decoder_config)`}}),It=new Z({props:{name:"from\\_encoder\\_decoder\\_configs",anchor:"transformers.EncoderDecoderConfig.from_encoder_decoder_configs",parameters:[{name:"encoder_config",val:": PretrainedConfig"},{name:"decoder_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/configuration_encoder_decoder.py#L90",returnDescription:`
<p>An instance of a configuration object</p>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"
>EncoderDecoderConfig</a></p>
`}}),Lt=new Z({props:{name:"to\\_dict",anchor:"transformers.EncoderDecoderConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/configuration_encoder_decoder.py#L107",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),Ot=new Za({}),Nt=new Z({props:{name:"class transformers.EncoderDecoderModel",anchor:"transformers.EncoderDecoderModel",parameters:[{name:"config",val:": typing.Optional[transformers.configuration_utils.PretrainedConfig] = None"},{name:"encoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"},{name:"decoder",val:": typing.Optional[transformers.modeling_utils.PreTrainedModel] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py#L171",parametersDescription:[{anchor:"transformers.EncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Gt=new Z({props:{name:"forward",anchor:"transformers.EncoderDecoderModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py#L428",parametersDescription:[{anchor:"transformers.EncoderDecoderModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.EncoderDecoderModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.EncoderDecoderModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>For training, <code>decoder_input_ids</code> are automatically created by the model by shifting the <code>labels</code>
to the right, replacing -100 by the <code>pad_token_id</code> and prepending them with the
<code>decoder_start_token_id</code>.`,name:"decoder_input_ids"},{anchor:"transformers.EncoderDecoderModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.EncoderDecoderModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
This tuple must consist of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>:
<code>attentions</code>) <code>last_hidden_state</code> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) is a tensor of hidden-states at the output of the last layer of the
encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.EncoderDecoderModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.EncoderDecoderModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.EncoderDecoderModel.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>decoder_input_ids</code>
indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.EncoderDecoderModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss for the decoder. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.EncoderDecoderModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.EncoderDecoderModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.EncoderDecoderModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.EncoderDecoderModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>Seq2SeqLMOutput</code> instead of a
plain tuple.
kwargs &#x2014; (<em>optional</em>) Remaining dictionary of keyword arguments. Keyword arguments come in two flavors:</p>
<ul>
<li>Without a prefix which will be input as <code>**encoder_kwargs</code> for the encoder forward function.</li>
<li>With a _decoder__ prefix which will be input as <code>**decoder_kwargs</code> for the decoder forward function.</li>
</ul>`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"
>EncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),dt=new Kf({props:{$$slots:{default:[dv]},$$scope:{ctx:Te}}}),Ht=new xt({props:{code:`from transformers import EncoderDecoderModel, BertTokenizer
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased') # initialize Bert2Bert from pre-trained checkpoints

# training
model.config.decoder_start_token_id = tokenizer.cls_token_id
model.config.pad_token_id = tokenizer.pad_token_id
model.config.vocab_size = model.config.decoder.vocab_size

input_ids = tokenizer("This is a really long text", return_tensors="pt").input_ids
labels = tokenizer("This is the corresponding summary", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, labels=input_ids)
loss, logits = outputs.loss, outputs.logits

# save and load from pretrained
model.save_pretrained("bert2bert")
model = EncoderDecoderModel.from_pretrained("bert2bert")

# generation
generated = model.generate(input_ids),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EncoderDecoderModel, BertTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, <span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>) <span class="hljs-comment"># initialize Bert2Bert from pre-trained checkpoints</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.decoder_start_token_id = tokenizer.cls_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = tokenizer.pad_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.vocab_size = model.config.decoder.vocab_size

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;This is a really long text&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;This is the corresponding summary&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, labels=input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, logits = outputs.loss, outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save and load from pretrained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;bert2bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;bert2bert&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated = model.generate(input_ids)`}}),Jt=new Z({props:{name:"from\\_encoder\\_decoder\\_pretrained",anchor:"transformers.EncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": str = None"},{name:"decoder_pretrained_model_name_or_path",val:": str = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/modeling_encoder_decoder.py#L284"}}),Zt=new xt({props:{code:`from transformers import EncoderDecoderModel
# initialize a bert2bert from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized
model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'bert-base-uncased')
# saving model after fine-tuning
model.save_pretrained("./bert2bert")
# load fine-tuned model
model = EncoderDecoderModel.from_pretrained("./bert2bert"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a bert2bert from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, <span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./bert2bert&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./bert2bert&quot;</span>)`}}),Kt=new Za({}),Qt=new Z({props:{name:"class transformers.TFEncoderDecoderModel",anchor:"transformers.TFEncoderDecoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py#L154",parametersDescription:[{anchor:"transformers.TFEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the
model weights.`,name:"config"}]}}),ro=new Z({props:{name:"call",anchor:"transformers.TFEncoderDecoderModel.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py#L463",parametersDescription:[{anchor:"transformers.TFEncoderDecoderModel.call.input_ids",description:`<strong>input_ids</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFEncoderDecoderModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFEncoderDecoderModel.call.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a></p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>Provide for sequence to sequence training to the decoder. Indices can be obtained using
<a href="/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for details.`,name:"decoder_input_ids"},{anchor:"transformers.TFEncoderDecoderModel.call.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.TFEncoderDecoderModel.call.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(tf.Tensor)</code>, <em>optional</em>) &#x2014;
This tuple must consist of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>:
<code>attentions</code>) <code>last_hidden_state</code> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) is a
tensor of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the
decoder.`,name:"encoder_outputs"},{anchor:"transformers.TFEncoderDecoderModel.call.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(tf.Tensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code>
(those that don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code>
instead of all <code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.TFEncoderDecoderModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFEncoderDecoderModel.call.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>decoder_input_ids</code>
indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.TFEncoderDecoderModel.call.labels",description:`<strong>labels</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss for the decoder. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.TFEncoderDecoderModel.call.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.TFEncoderDecoderModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TFEncoderDecoderModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TFEncoderDecoderModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>Seq2SeqLMOutput</code> instead of a
plain tuple.`,name:"return_dict"},{anchor:"transformers.TFEncoderDecoderModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).
kwargs &#x2014; (<em>optional</em>) Remaining dictionary of keyword arguments. Keyword arguments come in two flavors:</p>
<ul>
<li>Without a prefix which will be input as <code>**encoder_kwargs</code> for the encoder forward function.</li>
<li>With a _decoder__ prefix which will be input as <code>**decoder_kwargs</code> for the decoder forward function.</li>
</ul>`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"
>TFSeq2SeqLMOutput</a> or a tuple of
<code>tf.Tensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"
>EncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[tf.Tensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>tf.Tensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_heads, sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape \`(batch_size, num_heads, sequence_length, sequence_length)\u201C.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_tf_outputs.TFSeq2SeqLMOutput"
>TFSeq2SeqLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),lt=new Kf({props:{$$slots:{default:[iv]},$$scope:{ctx:Te}}}),ao=new xt({props:{code:`from transformers import TFEncoderDecoderModel, BertTokenizer

# initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = TFEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

# forward
input_ids = tokenizer.encode("Hello, my dog is cute", add_special_tokens=True, return_tensors='tf')  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)

# training
outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)
loss, logits = outputs.loss, outputs.logits

# save and load from pretrained
model.save_pretrained("bert2gpt2")
model = TFEncoderDecoderModel.from_pretrained("bert2gpt2")

# generation
generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.bos_token_id),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFEncoderDecoderModel, BertTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a bert2gpt2 from a pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, <span class="hljs-string">&#x27;gpt2&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer.encode(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, add_special_tokens=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;tf&#x27;</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># training</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=input_ids, labels=input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, logits = outputs.loss, outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save and load from pretrained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;bert2gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;bert2gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generation</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated = model.generate(input_ids, decoder_start_token_id=model.config.decoder.bos_token_id)`}}),so=new Z({props:{name:"from\\_encoder\\_decoder\\_pretrained",anchor:"transformers.TFEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": str = None"},{name:"decoder_pretrained_model_name_or_path",val:": str = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py#L300"}}),lo=new xt({props:{code:`from transformers import TFEncoderDecoderModel
# initialize a bert2gpt2 from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized
model = TFEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'gpt2')
# saving model after fine-tuning
model.save_pretrained("./bert2gpt2")
# load fine-tuned model
model = TFEncoderDecoderModel.from_pretrained("./bert2gpt2"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a bert2gpt2 from two pretrained BERT models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, <span class="hljs-string">&#x27;gpt2&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./bert2gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./bert2gpt2&quot;</span>)`}}),co=new Za({}),po=new Z({props:{name:"class transformers.FlaxEncoderDecoderModel",anchor:"transformers.FlaxEncoderDecoderModel",parameters:[{name:"config",val:": EncoderDecoderConfig"},{name:"input_shape",val:": typing.Optional[typing.Tuple] = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py#L310",parametersDescription:[{anchor:"transformers.FlaxEncoderDecoderModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the
model weights.`,name:"config"},{anchor:"transformers.FlaxEncoderDecoderModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on
GPUs) and <code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see
<a href="/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and <a href="/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),_o=new Z({props:{name:"\\_\\_call\\_\\_",anchor:"transformers.FlaxEncoderDecoderModel.__call__",parameters:[{name:"input_ids",val:": ndarray"},{name:"attention_mask",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"decoder_input_ids",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"decoder_attention_mask",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"position_ids",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"decoder_position_ids",val:": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"train",val:": bool = False"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py#L619",parametersDescription:[{anchor:"transformers.FlaxEncoderDecoderModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>For sequence to sequence training, <code>decoder_input_ids</code> should be provided. If no
<code>decoder_input_ids</code> is provided, the model will create this tensor by shifting the <code>input_ids</code> to
the right for denoising pre-training.`,name:"decoder_input_ids"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will
also be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.encoder.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.decoder_position_ids",description:`<strong>decoder_position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each decoder input sequence tokens in the position embeddings. Selected in the
range <code>[0, config.decoder.max_position_embeddings - 1]</code>.`,name:"decoder_position_ids"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxEncoderDecoderModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, the model will return a <code>FlaxSeq2SeqLMOutput</code> instead
of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"
>FlaxSeq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"
>EncoderDecoderConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(jnp.ndarray)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of
shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput"
>FlaxSeq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pt=new Kf({props:{$$slots:{default:[lv]},$$scope:{ctx:Te}}}),vo=new xt({props:{code:`from transformers import FlaxEncoderDecoderModel, BertTokenizer, GPT2Tokenizer

# load a fine-tuned bert2gpt2 model
model = FlaxEncoderDecoderModel.from_pretrained("patrickvonplaten/bert2gpt2-cnn_dailymail-fp16")
# load input & output tokenizer
tokenizer_input = BertTokenizer.from_pretrained('bert-base-cased')
tokenizer_output = GPT2Tokenizer.from_pretrained('gpt2')

article = '''Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members
singing a racist chant. SAE's national chapter suspended the students,
but University of Oklahoma President David Boren took it a step further,
saying the university's affiliation with the fraternity is permanently done.'''

input_ids = tokenizer_input(article, add_special_tokens=True, return_tensors='np').input_ids

# use GPT2's eos_token as the pad as well as eos token
model.config.eos_token_id = model.config.decoder.eos_token_id
model.config.pad_token_id = model.config.eos_token_id

sequences = model.generate(input_ids, num_beams=4, max_length=12).sequences

summary = tokenizer_output.batch_decode(sequences, skip_special_tokens=True)[0]
assert summary == "SAS Alpha Epsilon suspended Sigma Alpha Epsilon members",`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxEncoderDecoderModel, BertTokenizer, GPT2Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load a fine-tuned bert2gpt2 model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/bert2gpt2-cnn_dailymail-fp16&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load input &amp; output tokenizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_input = BertTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_output = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&#x27;gpt2&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&#x27;&#x27;&#x27;Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members
<span class="hljs-meta">... </span>singing a racist chant. SAE&#x27;s national chapter suspended the students,
<span class="hljs-meta">... </span>but University of Oklahoma President David Boren took it a step further,
<span class="hljs-meta">... </span>saying the university&#x27;s affiliation with the fraternity is permanently done.&#x27;&#x27;&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_input(article, add_special_tokens=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&#x27;np&#x27;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># use GPT2&#x27;s eos_token as the pad as well as eos token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.eos_token_id = model.config.decoder.eos_token_id
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>sequences = model.generate(input_ids, num_beams=<span class="hljs-number">4</span>, max_length=<span class="hljs-number">12</span>).sequences

<span class="hljs-meta">&gt;&gt;&gt; </span>summary = tokenizer_output.batch_decode(sequences, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> summary == <span class="hljs-string">&quot;SAS Alpha Epsilon suspended Sigma Alpha Epsilon members&quot;</span>`}}),bo=new Z({props:{name:"from\\_encoder\\_decoder\\_pretrained",anchor:"transformers.FlaxEncoderDecoderModel.from_encoder_decoder_pretrained",parameters:[{name:"encoder_pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"decoder_pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike, NoneType] = None"},{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py#L743"}}),To=new xt({props:{code:`from transformers import FlaxEncoderDecoderModel
# initialize a bert2gpt2 from pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized
model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')
# saving model after fine-tuning
model.save_pretrained("./bert2gpt2")
# load fine-tuned model
model = FlaxEncoderDecoderModel.from_pretrained("./bert2gpt2"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># initialize a bert2gpt2 from pretrained BERT and GPT2 models. Note that the cross-attention layers will be randomly initialized</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, <span class="hljs-string">&#x27;gpt2&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># saving model after fine-tuning</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;./bert2gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load fine-tuned model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;./bert2gpt2&quot;</span>)`}}),{c(){h=r("meta"),F=i(),f=r("h1"),j=r("a"),L=r("span"),v(M.$$.fragment),D=i(),B=r("span"),Ss=t("Encoder Decoder Models"),Ka=i(),Xe=r("p"),Ls=t("The "),zo=r("a"),Os=t("EncoderDecoderModel"),Ns=t(` can be used to initialize a sequence-to-sequence model with any
pretrained autoencoding model as the encoder and any pretrained autoregressive model as the decoder.`),Qa=i(),et=r("p"),Bs=t(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation tasks
was shown in `),zt=r("a"),Us=t("Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),Ws=t(` by
Sascha Rothe, Shashi Narayan, Aliaksei Severyn.`),Xa=i(),tt=r("p"),Vs=t("After such an "),jo=r("a"),Rs=t("EncoderDecoderModel"),Gs=t(` has been trained/fine-tuned, it can be saved/loaded just like
any other models (see the examples for more information).`),es=i(),he=r("p"),Hs=t("An application of this architecture could be to leverage two pretrained "),$o=r("a"),Js=t("BertModel"),Ys=t(` as the encoder
and decoder for a summarization model as was shown in: `),jt=r("a"),Zs=t("Text Summarization with Pretrained Encoders"),Ks=t(" by Yang Liu and Mirella Lapata."),ts=i(),me=r("p"),Qs=t("The "),Mn=r("code"),Xs=t("from_pretrained()"),ed=t(` currently doesn\u2019t support initializing the model from a
pytorch checkpoint. Passing `),Dn=r("code"),td=t("from_pt=True"),od=t(` to this method will throw an exception. If there are only pytorch
checkpoints for a particular encoder-decoder model, a workaround is:`),os=i(),v($t.$$.fragment),ns=i(),fe=r("p"),nd=t("This model was contributed by "),Pt=r("a"),rd=t("thomwolf"),ad=t(`. This model\u2019s TensorFlow and Flax versions
were contributed by `),qt=r("a"),sd=t("ydshieh"),dd=t("."),rs=i(),we=r("h2"),ot=r("a"),xn=r("span"),v(Ct.$$.fragment),id=i(),zn=r("span"),ld=t("EncoderDecoderConfig"),as=i(),C=r("div"),v(Ft.$$.fragment),cd=i(),nt=r("p"),Po=r("a"),pd=t("EncoderDecoderConfig"),hd=t(` is the configuration class to store the configuration of a
`),qo=r("a"),md=t("EncoderDecoderModel"),fd=t(`. It is used to instantiate an Encoder Decoder model according to the
specified arguments, defining the encoder and decoder configs.`),ud=i(),Me=r("p"),gd=t("Configuration objects inherit from "),Co=r("a"),_d=t("PretrainedConfig"),vd=t(` and can be used to control the model
outputs. Read the documentation from `),Fo=r("a"),bd=t("PretrainedConfig"),Ed=t(" for more information."),yd=i(),jn=r("p"),kd=t("Examples:"),Td=i(),v(At.$$.fragment),wd=i(),rt=r("div"),v(It.$$.fragment),Md=i(),St=r("p"),Dd=t("Instantiate a "),Ao=r("a"),xd=t("EncoderDecoderConfig"),zd=t(` (or a derived class) from a pre-trained encoder model
configuration and decoder model configuration.`),jd=i(),at=r("div"),v(Lt.$$.fragment),$d=i(),De=r("p"),Pd=t("Serializes this instance to a Python dictionary. Override the default "),$n=r("em"),qd=t("to_dict()"),Cd=t(" from "),Pn=r("em"),Fd=t("PretrainedConfig"),Ad=t("."),ss=i(),xe=r("h2"),st=r("a"),qn=r("span"),v(Ot.$$.fragment),Id=i(),Cn=r("span"),Sd=t("EncoderDecoderModel"),ds=i(),$=r("div"),v(Nt.$$.fragment),Ld=i(),ze=r("p"),Od=t(`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),Fn=r("code"),Nd=t("from_pretrained()"),Bd=t(` function and the decoder is loaded via
`),An=r("code"),Ud=t("from_pretrained()"),Wd=t(` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),Vd=i(),Bt=r("p"),Rd=t(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Ut=r("a"),Gd=t("Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),Hd=t(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Jd=i(),In=r("p"),Yd=t(`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),Zd=i(),Wt=r("p"),Kd=t("This model inherits from "),Io=r("a"),Qd=t("PreTrainedModel"),Xd=t(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ei=i(),Vt=r("p"),ti=t("This model is also a PyTorch "),Rt=r("a"),oi=t("torch.nn.Module"),ni=t(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),ri=i(),ue=r("p"),So=r("a"),ai=t("EncoderDecoderModel"),si=t(` is a generic model class that will be instantiated as a transformer
architecture with one of the base model classes of the library as encoder and another one as decoder when created
with the :meth`),Sn=r("em"),di=t("~transformers.AutoModel.from_pretrained"),ii=t(` class method for the encoder and
:meth`),Ln=r("em"),li=t("~transformers.AutoModelForCausalLM.from_pretrained"),ci=t(" class method for the decoder."),pi=i(),U=r("div"),v(Gt.$$.fragment),hi=i(),je=r("p"),mi=t("The "),Lo=r("a"),fi=t("EncoderDecoderModel"),ui=t(" forward method, overrides the "),On=r("code"),gi=t("__call__"),_i=t(" special method."),vi=i(),v(dt.$$.fragment),bi=i(),Nn=r("p"),Ei=t("Examples:"),yi=i(),v(Ht.$$.fragment),ki=i(),u=r("div"),v(Jt.$$.fragment),Ti=i(),Bn=r("p"),wi=t(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Mi=i(),$e=r("p"),Di=t("The model is set in evaluation mode by default using "),Un=r("code"),xi=t("model.eval()"),zi=t(` (Dropout modules are deactivated). To
train the model, you need to first set it back in training mode with `),Wn=r("code"),ji=t("model.train()"),$i=t("."),Pi=i(),Pe=r("p"),qi=t(`Params:
encoder`),Vn=r("em"),Ci=t("pretrained_model_name_or_path (:obj: _str"),Fi=t(", "),Rn=r("em"),Ai=t("optional"),Ii=t(`):
Information necessary to initiate the encoder. Can be either:`),Si=i(),qe=r("ul"),K=r("li"),Li=t("A string, the "),Gn=r("em"),Oi=t("model id"),Ni=t(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Hn=r("code"),Bi=t("bert-base-uncased"),Ui=t(`, or namespaced under
a user or organization name, like `),Jn=r("code"),Wi=t("dbmdz/bert-base-german-cased"),Vi=t("."),Ri=i(),Q=r("li"),Gi=t("A path to a "),Yn=r("em"),Hi=t("directory"),Ji=t(` containing model weights saved using
`),Oo=r("a"),Yi=t("save_pretrained()"),Zi=t(", e.g., "),Zn=r("code"),Ki=t("./my_model_directory/"),Qi=t("."),Xi=i(),O=r("li"),el=t("A path or url to a "),Kn=r("em"),tl=t("tensorflow index checkpoint file"),ol=t(" (e.g, "),Qn=r("code"),nl=t("./tf_model/model.ckpt.index"),rl=t(`). In
this case, `),Xn=r("code"),al=t("from_tf"),sl=t(" should be set to "),er=r("code"),dl=t("True"),il=t(` and a configuration object should be provided
as `),tr=r("code"),ll=t("config"),cl=t(` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),pl=i(),X=r("p"),hl=t("decoder"),or=r("em"),ml=t("pretrained_model_name_or_path (:obj: _str"),fl=t(", "),nr=r("em"),ul=t("optional"),gl=t(", defaults to "),rr=r("em"),_l=t("None"),vl=t(`):
Information necessary to initiate the decoder. Can be either:`),bl=i(),Ce=r("ul"),ee=r("li"),El=t("A string, the "),ar=r("em"),yl=t("model id"),kl=t(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),sr=r("code"),Tl=t("bert-base-uncased"),wl=t(`, or namespaced under
a user or organization name, like `),dr=r("code"),Ml=t("dbmdz/bert-base-german-cased"),Dl=t("."),xl=i(),te=r("li"),zl=t("A path to a "),ir=r("em"),jl=t("directory"),$l=t(` containing model weights saved using
`),No=r("a"),Pl=t("save_pretrained()"),ql=t(", e.g., "),lr=r("code"),Cl=t("./my_model_directory/"),Fl=t("."),Al=i(),N=r("li"),Il=t("A path or url to a "),cr=r("em"),Sl=t("tensorflow index checkpoint file"),Ll=t(" (e.g, "),pr=r("code"),Ol=t("./tf_model/model.ckpt.index"),Nl=t(`). In
this case, `),hr=r("code"),Bl=t("from_tf"),Ul=t(" should be set to "),mr=r("code"),Wl=t("True"),Vl=t(` and a configuration object should be provided
as `),fr=r("code"),Rl=t("config"),Gl=t(` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),Hl=i(),Fe=r("p"),Jl=t("model"),ur=r("em"),Yl=t("args (remaining positional arguments, _optional"),Zl=t(`):
All remaining positional arguments will be passed to the underlying model\u2019s `),gr=r("code"),Kl=t("__init__"),Ql=t(" method."),Xl=i(),Ae=r("p"),ec=t("kwargs (remaining dictionary of keyword arguments, "),_r=r("em"),tc=t("optional"),oc=t(`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),vr=r("code"),nc=t("output_attentions=True"),rc=t(")."),ac=i(),Ie=r("ul"),br=r("li"),sc=t("To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),dc=i(),Er=r("li"),ic=t("To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),lc=i(),yr=r("li"),cc=t("To update the parent model configuration, do not use a prefix for each configuration parameter."),pc=i(),Yt=r("p"),hc=t("Behaves differently depending on whether a "),kr=r("code"),mc=t("config"),fc=t(" is provided or automatically loaded."),uc=i(),Tr=r("p"),gc=t("Example:"),_c=i(),v(Zt.$$.fragment),is=i(),Se=r("h2"),it=r("a"),wr=r("span"),v(Kt.$$.fragment),vc=i(),Mr=r("span"),bc=t("TFEncoderDecoderModel"),ls=i(),P=r("div"),v(Qt.$$.fragment),Ec=i(),Le=r("p"),yc=t(`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),Dr=r("code"),kc=t("from_pretrained()"),Tc=t(` function and the decoder is loaded via
`),xr=r("code"),wc=t("from_pretrained()"),Mc=t(` function. Cross-attention layers are automatically
added to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),Dc=i(),Xt=r("p"),xc=t(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),eo=r("a"),zc=t("Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),jc=t(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),$c=i(),zr=r("p"),Pc=t(`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),qc=i(),to=r("p"),Cc=t("This model inherits from "),Bo=r("a"),Fc=t("TFPreTrainedModel"),Ac=t(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Ic=i(),oo=r("p"),Sc=t("This model is also a "),no=r("a"),Lc=t("tf.keras.Model"),Oc=t(` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),Nc=i(),ge=r("p"),jr=r("code"),Bc=t("TFEncoderDecoder"),Uc=t(` is a generic model class that will be instantiated as a transformer
architecture with one of the base model classes of the library as encoder and another one as decoder when created
with the :meth`),$r=r("em"),Wc=t("~transformers.TFAutoModel.from_pretrained"),Vc=t(` class method for the encoder and
:meth`),Pr=r("em"),Rc=t("~transformers.TFAutoModelForCausalLM.from_pretrained"),Gc=t(" class method for the decoder."),Hc=i(),W=r("div"),v(ro.$$.fragment),Jc=i(),Oe=r("p"),Yc=t("The "),Uo=r("a"),Zc=t("TFEncoderDecoderModel"),Kc=t(" forward method, overrides the "),qr=r("code"),Qc=t("__call__"),Xc=t(" special method."),ep=i(),v(lt.$$.fragment),tp=i(),Cr=r("p"),op=t("Examples:"),np=i(),v(ao.$$.fragment),rp=i(),g=r("div"),v(so.$$.fragment),ap=i(),Fr=r("p"),sp=t(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),dp=i(),Ne=r("p"),ip=t(`Params:
encoder`),Ar=r("em"),lp=t("pretrained_model_name_or_path (:obj: _str"),cp=t(", "),Ir=r("em"),pp=t("optional"),hp=t(`):
Information necessary to initiate the encoder. Can be either:`),mp=i(),Be=r("ul"),oe=r("li"),fp=t("A string, the "),Sr=r("em"),up=t("model id"),gp=t(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Lr=r("code"),_p=t("bert-base-uncased"),vp=t(`, or namespaced under
a user or organization name, like `),Or=r("code"),bp=t("dbmdz/bert-base-german-cased"),Ep=t("."),yp=i(),ne=r("li"),kp=t("A path to a "),Nr=r("em"),Tp=t("directory"),wp=t(` containing model weights saved using
`),Wo=r("a"),Mp=t("save_pretrained()"),Dp=t(", e.g., "),Br=r("code"),xp=t("./my_model_directory/"),zp=t("."),jp=i(),G=r("li"),$p=t("A path or url to a "),Ur=r("em"),Pp=t("pytorch index checkpoint file"),qp=t(" (e.g, "),Wr=r("code"),Cp=t("./pt_model/"),Fp=t(`). In this case,
`),Vr=r("code"),Ap=t("encoder_from_pt"),Ip=t(" should be set to "),Rr=r("code"),Sp=t("True"),Lp=t("."),Op=i(),re=r("p"),Np=t("decoder"),Gr=r("em"),Bp=t("pretrained_model_name_or_path (:obj: _str"),Up=t(", "),Hr=r("em"),Wp=t("optional"),Vp=t(", defaults to "),Jr=r("em"),Rp=t("None"),Gp=t(`):
Information necessary to initiate the decoder. Can be either:`),Hp=i(),Ue=r("ul"),ae=r("li"),Jp=t("A string, the "),Yr=r("em"),Yp=t("model id"),Zp=t(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Zr=r("code"),Kp=t("bert-base-uncased"),Qp=t(`, or namespaced under
a user or organization name, like `),Kr=r("code"),Xp=t("dbmdz/bert-base-german-cased"),eh=t("."),th=i(),se=r("li"),oh=t("A path to a "),Qr=r("em"),nh=t("directory"),rh=t(` containing model weights saved using
`),Vo=r("a"),ah=t("save_pretrained()"),sh=t(", e.g., "),Xr=r("code"),dh=t("./my_model_directory/"),ih=t("."),lh=i(),H=r("li"),ch=t("A path or url to a "),ea=r("em"),ph=t("pytorch checkpoint file"),hh=t(" (e.g, "),ta=r("code"),mh=t("./pt_model/"),fh=t(`). In this case,
`),oa=r("code"),uh=t("decoder_from_pt"),gh=t(" should be set to "),na=r("code"),_h=t("True"),vh=t("."),bh=i(),We=r("p"),Eh=t("model"),ra=r("em"),yh=t("args (remaining positional arguments, _optional"),kh=t(`):
All remaning positional arguments will be passed to the underlying model\u2019s `),aa=r("code"),Th=t("__init__"),wh=t(" method."),Mh=i(),Ve=r("p"),Dh=t("kwargs (remaining dictionary of keyword arguments, "),sa=r("em"),xh=t("optional"),zh=t(`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),da=r("code"),jh=t("output_attentions=True"),$h=t(")."),Ph=i(),Re=r("ul"),ia=r("li"),qh=t("To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),Ch=i(),la=r("li"),Fh=t("To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),Ah=i(),ca=r("li"),Ih=t("To update the parent model configuration, do not use a prefix for each configuration parameter."),Sh=i(),io=r("p"),Lh=t("Behaves differently depending on whether a "),pa=r("code"),Oh=t("config"),Nh=t(" is provided or automatically loaded."),Bh=i(),ha=r("p"),Uh=t("Example:"),Wh=i(),v(lo.$$.fragment),cs=i(),Ge=r("h2"),ct=r("a"),ma=r("span"),v(co.$$.fragment),Vh=i(),fa=r("span"),Rh=t("FlaxEncoderDecoderModel"),ps=i(),q=r("div"),v(po.$$.fragment),Gh=i(),He=r("p"),Hh=t(`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),ua=r("code"),Jh=t("from_pretrained()"),Yh=t(` function and the decoder is loaded via
`),ga=r("code"),Zh=t("from_pretrained()"),Kh=t(` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),Qh=i(),ho=r("p"),Xh=t(`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),mo=r("a"),em=t("Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),tm=t(` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),om=i(),_a=r("p"),nm=t(`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),rm=i(),fo=r("p"),am=t("This model inherits from "),Ro=r("a"),sm=t("FlaxPreTrainedModel"),dm=t(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),im=i(),uo=r("p"),lm=t("This model is also a Flax Linen "),go=r("a"),cm=t("flax.nn.Module"),pm=t(` subclass. Use it as a regular Flax
Module and refer to the Flax documentation for all matter related to general usage and behavior.`),hm=i(),_e=r("p"),Go=r("a"),mm=t("FlaxEncoderDecoderModel"),fm=t(` is a generic model class that will be instantiated as a transformer
architecture with the module (flax.nn.Module) of one of the base model classes of the library as encoder module and
another one as decoder module when created with the :meth`),va=r("em"),um=t("~transformers.FlaxAutoModel.from_pretrained"),gm=t(` class method
for the encoder and :meth`),ba=r("em"),_m=t("~transformers.FlaxAutoModelForCausalLM.from_pretrained"),vm=t(" class method for the decoder."),bm=i(),V=r("div"),v(_o.$$.fragment),Em=i(),Je=r("p"),ym=t("The "),Ho=r("a"),km=t("FlaxEncoderDecoderModel"),Tm=t(" forward method, overrides the "),Ea=r("code"),wm=t("__call__"),Mm=t(" special method."),Dm=i(),v(pt.$$.fragment),xm=i(),ya=r("p"),zm=t("Examples:"),jm=i(),v(vo.$$.fragment),$m=i(),_=r("div"),v(bo.$$.fragment),Pm=i(),ka=r("p"),qm=t(`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Cm=i(),Ye=r("p"),Fm=t(`Params:
encoder`),Ta=r("em"),Am=t("pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),Im=t(", "),wa=r("em"),Sm=t("optional"),Lm=t(`):
Information necessary to initiate the encoder. Can be either:`),Om=i(),Eo=r("ul"),de=r("li"),Nm=t("A string, the "),Ma=r("em"),Bm=t("model id"),Um=t(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Da=r("code"),Wm=t("bert-base-uncased"),Vm=t(`, or namespaced under
a user or organization name, like `),xa=r("code"),Rm=t("dbmdz/bert-base-german-cased"),Gm=t("."),Hm=i(),ie=r("li"),Jm=t("A path to a "),za=r("em"),Ym=t("directory"),Zm=t(` containing model weights saved using
`),Jo=r("a"),Km=t("save_pretrained()"),Qm=t(", e.g., "),ja=r("code"),Xm=t("./my_model_directory/"),ef=t("."),tf=i(),le=r("p"),of=t("decoder"),$a=r("em"),nf=t("pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),rf=t(", "),Pa=r("em"),af=t("optional"),sf=t(", defaults to "),qa=r("em"),df=t("None"),lf=t(`):
Information necessary to initiate the decoder. Can be either:`),cf=i(),yo=r("ul"),ce=r("li"),pf=t("A string, the "),Ca=r("em"),hf=t("model id"),mf=t(` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Fa=r("code"),ff=t("bert-base-uncased"),uf=t(`, or namespaced under
a user or organization name, like `),Aa=r("code"),gf=t("dbmdz/bert-base-german-cased"),_f=t("."),vf=i(),pe=r("li"),bf=t("A path to a "),Ia=r("em"),Ef=t("directory"),yf=t(` containing model weights saved using
`),Yo=r("a"),kf=t("save_pretrained()"),Tf=t(", e.g., "),Sa=r("code"),wf=t("./my_model_directory/"),Mf=t("."),Df=i(),Ze=r("p"),xf=t("model"),La=r("em"),zf=t("args (remaining positional arguments, _optional"),jf=t(`):
All remaning positional arguments will be passed to the underlying model\u2019s `),Oa=r("code"),$f=t("__init__"),Pf=t(" method."),qf=i(),Ke=r("p"),Cf=t("kwargs (remaining dictionary of keyword arguments, "),Na=r("em"),Ff=t("optional"),Af=t(`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),Ba=r("code"),If=t("output_attentions=True"),Sf=t(")."),Lf=i(),Qe=r("ul"),Ua=r("li"),Of=t("To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),Nf=i(),Wa=r("li"),Bf=t("To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),Uf=i(),Va=r("li"),Wf=t("To update the parent model configuration, do not use a prefix for each configuration parameter."),Vf=i(),ko=r("p"),Rf=t("Behaves differently depending on whether a "),Ra=r("code"),Gf=t("config"),Hf=t(" is provided or automatically loaded."),Jf=i(),Ga=r("p"),Yf=t("Example:"),Zf=i(),v(To.$$.fragment),this.h()},l(d){const p=sv('[data-svelte="svelte-1phssyn"]',document.head);h=a(p,"META",{name:!0,content:!0}),p.forEach(n),F=l(d),f=a(d,"H1",{class:!0});var wo=s(f);j=a(wo,"A",{id:!0,class:!0,href:!0});var Ha=s(j);L=a(Ha,"SPAN",{});var Ja=s(L);b(M.$$.fragment,Ja),Ja.forEach(n),Ha.forEach(n),D=l(wo),B=a(wo,"SPAN",{});var Qf=s(B);Ss=o(Qf,"Encoder Decoder Models"),Qf.forEach(n),wo.forEach(n),Ka=l(d),Xe=a(d,"P",{});var ms=s(Xe);Ls=o(ms,"The "),zo=a(ms,"A",{href:!0});var Xf=s(zo);Os=o(Xf,"EncoderDecoderModel"),Xf.forEach(n),Ns=o(ms,` can be used to initialize a sequence-to-sequence model with any
pretrained autoencoding model as the encoder and any pretrained autoregressive model as the decoder.`),ms.forEach(n),Qa=l(d),et=a(d,"P",{});var fs=s(et);Bs=o(fs,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation tasks
was shown in `),zt=a(fs,"A",{href:!0,rel:!0});var eu=s(zt);Us=o(eu,"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),eu.forEach(n),Ws=o(fs,` by
Sascha Rothe, Shashi Narayan, Aliaksei Severyn.`),fs.forEach(n),Xa=l(d),tt=a(d,"P",{});var us=s(tt);Vs=o(us,"After such an "),jo=a(us,"A",{href:!0});var tu=s(jo);Rs=o(tu,"EncoderDecoderModel"),tu.forEach(n),Gs=o(us,` has been trained/fine-tuned, it can be saved/loaded just like
any other models (see the examples for more information).`),us.forEach(n),es=l(d),he=a(d,"P",{});var Zo=s(he);Hs=o(Zo,"An application of this architecture could be to leverage two pretrained "),$o=a(Zo,"A",{href:!0});var ou=s($o);Js=o(ou,"BertModel"),ou.forEach(n),Ys=o(Zo,` as the encoder
and decoder for a summarization model as was shown in: `),jt=a(Zo,"A",{href:!0,rel:!0});var nu=s(jt);Zs=o(nu,"Text Summarization with Pretrained Encoders"),nu.forEach(n),Ks=o(Zo," by Yang Liu and Mirella Lapata."),Zo.forEach(n),ts=l(d),me=a(d,"P",{});var Ko=s(me);Qs=o(Ko,"The "),Mn=a(Ko,"CODE",{});var ru=s(Mn);Xs=o(ru,"from_pretrained()"),ru.forEach(n),ed=o(Ko,` currently doesn\u2019t support initializing the model from a
pytorch checkpoint. Passing `),Dn=a(Ko,"CODE",{});var au=s(Dn);td=o(au,"from_pt=True"),au.forEach(n),od=o(Ko,` to this method will throw an exception. If there are only pytorch
checkpoints for a particular encoder-decoder model, a workaround is:`),Ko.forEach(n),os=l(d),b($t.$$.fragment,d),ns=l(d),fe=a(d,"P",{});var Qo=s(fe);nd=o(Qo,"This model was contributed by "),Pt=a(Qo,"A",{href:!0,rel:!0});var su=s(Pt);rd=o(su,"thomwolf"),su.forEach(n),ad=o(Qo,`. This model\u2019s TensorFlow and Flax versions
were contributed by `),qt=a(Qo,"A",{href:!0,rel:!0});var du=s(qt);sd=o(du,"ydshieh"),du.forEach(n),dd=o(Qo,"."),Qo.forEach(n),rs=l(d),we=a(d,"H2",{class:!0});var gs=s(we);ot=a(gs,"A",{id:!0,class:!0,href:!0});var iu=s(ot);xn=a(iu,"SPAN",{});var lu=s(xn);b(Ct.$$.fragment,lu),lu.forEach(n),iu.forEach(n),id=l(gs),zn=a(gs,"SPAN",{});var cu=s(zn);ld=o(cu,"EncoderDecoderConfig"),cu.forEach(n),gs.forEach(n),as=l(d),C=a(d,"DIV",{class:!0});var R=s(C);b(Ft.$$.fragment,R),cd=l(R),nt=a(R,"P",{});var Ya=s(nt);Po=a(Ya,"A",{href:!0});var pu=s(Po);pd=o(pu,"EncoderDecoderConfig"),pu.forEach(n),hd=o(Ya,` is the configuration class to store the configuration of a
`),qo=a(Ya,"A",{href:!0});var hu=s(qo);md=o(hu,"EncoderDecoderModel"),hu.forEach(n),fd=o(Ya,`. It is used to instantiate an Encoder Decoder model according to the
specified arguments, defining the encoder and decoder configs.`),Ya.forEach(n),ud=l(R),Me=a(R,"P",{});var Xo=s(Me);gd=o(Xo,"Configuration objects inherit from "),Co=a(Xo,"A",{href:!0});var mu=s(Co);_d=o(mu,"PretrainedConfig"),mu.forEach(n),vd=o(Xo,` and can be used to control the model
outputs. Read the documentation from `),Fo=a(Xo,"A",{href:!0});var fu=s(Fo);bd=o(fu,"PretrainedConfig"),fu.forEach(n),Ed=o(Xo," for more information."),Xo.forEach(n),yd=l(R),jn=a(R,"P",{});var uu=s(jn);kd=o(uu,"Examples:"),uu.forEach(n),Td=l(R),b(At.$$.fragment,R),wd=l(R),rt=a(R,"DIV",{class:!0});var _s=s(rt);b(It.$$.fragment,_s),Md=l(_s),St=a(_s,"P",{});var vs=s(St);Dd=o(vs,"Instantiate a "),Ao=a(vs,"A",{href:!0});var gu=s(Ao);xd=o(gu,"EncoderDecoderConfig"),gu.forEach(n),zd=o(vs,` (or a derived class) from a pre-trained encoder model
configuration and decoder model configuration.`),vs.forEach(n),_s.forEach(n),jd=l(R),at=a(R,"DIV",{class:!0});var bs=s(at);b(Lt.$$.fragment,bs),$d=l(bs),De=a(bs,"P",{});var en=s(De);Pd=o(en,"Serializes this instance to a Python dictionary. Override the default "),$n=a(en,"EM",{});var _u=s($n);qd=o(_u,"to_dict()"),_u.forEach(n),Cd=o(en," from "),Pn=a(en,"EM",{});var vu=s(Pn);Fd=o(vu,"PretrainedConfig"),vu.forEach(n),Ad=o(en,"."),en.forEach(n),bs.forEach(n),R.forEach(n),ss=l(d),xe=a(d,"H2",{class:!0});var Es=s(xe);st=a(Es,"A",{id:!0,class:!0,href:!0});var bu=s(st);qn=a(bu,"SPAN",{});var Eu=s(qn);b(Ot.$$.fragment,Eu),Eu.forEach(n),bu.forEach(n),Id=l(Es),Cn=a(Es,"SPAN",{});var yu=s(Cn);Sd=o(yu,"EncoderDecoderModel"),yu.forEach(n),Es.forEach(n),ds=l(d),$=a(d,"DIV",{class:!0});var A=s($);b(Nt.$$.fragment,A),Ld=l(A),ze=a(A,"P",{});var tn=s(ze);Od=o(tn,`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),Fn=a(tn,"CODE",{});var ku=s(Fn);Nd=o(ku,"from_pretrained()"),ku.forEach(n),Bd=o(tn,` function and the decoder is loaded via
`),An=a(tn,"CODE",{});var Tu=s(An);Ud=o(Tu,"from_pretrained()"),Tu.forEach(n),Wd=o(tn,` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),tn.forEach(n),Vd=l(A),Bt=a(A,"P",{});var ys=s(Bt);Rd=o(ys,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),Ut=a(ys,"A",{href:!0,rel:!0});var wu=s(Ut);Gd=o(wu,"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),wu.forEach(n),Hd=o(ys,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),ys.forEach(n),Jd=l(A),In=a(A,"P",{});var Mu=s(In);Yd=o(Mu,`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),Mu.forEach(n),Zd=l(A),Wt=a(A,"P",{});var ks=s(Wt);Kd=o(ks,"This model inherits from "),Io=a(ks,"A",{href:!0});var Du=s(Io);Qd=o(Du,"PreTrainedModel"),Du.forEach(n),Xd=o(ks,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ks.forEach(n),ei=l(A),Vt=a(A,"P",{});var Ts=s(Vt);ti=o(Ts,"This model is also a PyTorch "),Rt=a(Ts,"A",{href:!0,rel:!0});var xu=s(Rt);oi=o(xu,"torch.nn.Module"),xu.forEach(n),ni=o(Ts,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Ts.forEach(n),ri=l(A),ue=a(A,"P",{});var Mo=s(ue);So=a(Mo,"A",{href:!0});var zu=s(So);ai=o(zu,"EncoderDecoderModel"),zu.forEach(n),si=o(Mo,` is a generic model class that will be instantiated as a transformer
architecture with one of the base model classes of the library as encoder and another one as decoder when created
with the :meth`),Sn=a(Mo,"EM",{});var ju=s(Sn);di=o(ju,"~transformers.AutoModel.from_pretrained"),ju.forEach(n),ii=o(Mo,` class method for the encoder and
:meth`),Ln=a(Mo,"EM",{});var $u=s(Ln);li=o($u,"~transformers.AutoModelForCausalLM.from_pretrained"),$u.forEach(n),ci=o(Mo," class method for the decoder."),Mo.forEach(n),pi=l(A),U=a(A,"DIV",{class:!0});var ve=s(U);b(Gt.$$.fragment,ve),hi=l(ve),je=a(ve,"P",{});var on=s(je);mi=o(on,"The "),Lo=a(on,"A",{href:!0});var Pu=s(Lo);fi=o(Pu,"EncoderDecoderModel"),Pu.forEach(n),ui=o(on," forward method, overrides the "),On=a(on,"CODE",{});var qu=s(On);gi=o(qu,"__call__"),qu.forEach(n),_i=o(on," special method."),on.forEach(n),vi=l(ve),b(dt.$$.fragment,ve),bi=l(ve),Nn=a(ve,"P",{});var Cu=s(Nn);Ei=o(Cu,"Examples:"),Cu.forEach(n),yi=l(ve),b(Ht.$$.fragment,ve),ve.forEach(n),ki=l(A),u=a(A,"DIV",{class:!0});var w=s(u);b(Jt.$$.fragment,w),Ti=l(w),Bn=a(w,"P",{});var Fu=s(Bn);wi=o(Fu,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Fu.forEach(n),Mi=l(w),$e=a(w,"P",{});var nn=s($e);Di=o(nn,"The model is set in evaluation mode by default using "),Un=a(nn,"CODE",{});var Au=s(Un);xi=o(Au,"model.eval()"),Au.forEach(n),zi=o(nn,` (Dropout modules are deactivated). To
train the model, you need to first set it back in training mode with `),Wn=a(nn,"CODE",{});var Iu=s(Wn);ji=o(Iu,"model.train()"),Iu.forEach(n),$i=o(nn,"."),nn.forEach(n),Pi=l(w),Pe=a(w,"P",{});var rn=s(Pe);qi=o(rn,`Params:
encoder`),Vn=a(rn,"EM",{});var Su=s(Vn);Ci=o(Su,"pretrained_model_name_or_path (:obj: _str"),Su.forEach(n),Fi=o(rn,", "),Rn=a(rn,"EM",{});var Lu=s(Rn);Ai=o(Lu,"optional"),Lu.forEach(n),Ii=o(rn,`):
Information necessary to initiate the encoder. Can be either:`),rn.forEach(n),Si=l(w),qe=a(w,"UL",{});var an=s(qe);K=a(an,"LI",{});var ht=s(K);Li=o(ht,"A string, the "),Gn=a(ht,"EM",{});var Ou=s(Gn);Oi=o(Ou,"model id"),Ou.forEach(n),Ni=o(ht,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Hn=a(ht,"CODE",{});var Nu=s(Hn);Bi=o(Nu,"bert-base-uncased"),Nu.forEach(n),Ui=o(ht,`, or namespaced under
a user or organization name, like `),Jn=a(ht,"CODE",{});var Bu=s(Jn);Wi=o(Bu,"dbmdz/bert-base-german-cased"),Bu.forEach(n),Vi=o(ht,"."),ht.forEach(n),Ri=l(an),Q=a(an,"LI",{});var mt=s(Q);Gi=o(mt,"A path to a "),Yn=a(mt,"EM",{});var Uu=s(Yn);Hi=o(Uu,"directory"),Uu.forEach(n),Ji=o(mt,` containing model weights saved using
`),Oo=a(mt,"A",{href:!0});var Wu=s(Oo);Yi=o(Wu,"save_pretrained()"),Wu.forEach(n),Zi=o(mt,", e.g., "),Zn=a(mt,"CODE",{});var Vu=s(Zn);Ki=o(Vu,"./my_model_directory/"),Vu.forEach(n),Qi=o(mt,"."),mt.forEach(n),Xi=l(an),O=a(an,"LI",{});var J=s(O);el=o(J,"A path or url to a "),Kn=a(J,"EM",{});var Ru=s(Kn);tl=o(Ru,"tensorflow index checkpoint file"),Ru.forEach(n),ol=o(J," (e.g, "),Qn=a(J,"CODE",{});var Gu=s(Qn);nl=o(Gu,"./tf_model/model.ckpt.index"),Gu.forEach(n),rl=o(J,`). In
this case, `),Xn=a(J,"CODE",{});var Hu=s(Xn);al=o(Hu,"from_tf"),Hu.forEach(n),sl=o(J," should be set to "),er=a(J,"CODE",{});var Ju=s(er);dl=o(Ju,"True"),Ju.forEach(n),il=o(J,` and a configuration object should be provided
as `),tr=a(J,"CODE",{});var Yu=s(tr);ll=o(Yu,"config"),Yu.forEach(n),cl=o(J,` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),J.forEach(n),an.forEach(n),pl=l(w),X=a(w,"P",{});var ft=s(X);hl=o(ft,"decoder"),or=a(ft,"EM",{});var Zu=s(or);ml=o(Zu,"pretrained_model_name_or_path (:obj: _str"),Zu.forEach(n),fl=o(ft,", "),nr=a(ft,"EM",{});var Ku=s(nr);ul=o(Ku,"optional"),Ku.forEach(n),gl=o(ft,", defaults to "),rr=a(ft,"EM",{});var Qu=s(rr);_l=o(Qu,"None"),Qu.forEach(n),vl=o(ft,`):
Information necessary to initiate the decoder. Can be either:`),ft.forEach(n),bl=l(w),Ce=a(w,"UL",{});var sn=s(Ce);ee=a(sn,"LI",{});var ut=s(ee);El=o(ut,"A string, the "),ar=a(ut,"EM",{});var Xu=s(ar);yl=o(Xu,"model id"),Xu.forEach(n),kl=o(ut,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),sr=a(ut,"CODE",{});var eg=s(sr);Tl=o(eg,"bert-base-uncased"),eg.forEach(n),wl=o(ut,`, or namespaced under
a user or organization name, like `),dr=a(ut,"CODE",{});var tg=s(dr);Ml=o(tg,"dbmdz/bert-base-german-cased"),tg.forEach(n),Dl=o(ut,"."),ut.forEach(n),xl=l(sn),te=a(sn,"LI",{});var gt=s(te);zl=o(gt,"A path to a "),ir=a(gt,"EM",{});var og=s(ir);jl=o(og,"directory"),og.forEach(n),$l=o(gt,` containing model weights saved using
`),No=a(gt,"A",{href:!0});var ng=s(No);Pl=o(ng,"save_pretrained()"),ng.forEach(n),ql=o(gt,", e.g., "),lr=a(gt,"CODE",{});var rg=s(lr);Cl=o(rg,"./my_model_directory/"),rg.forEach(n),Fl=o(gt,"."),gt.forEach(n),Al=l(sn),N=a(sn,"LI",{});var Y=s(N);Il=o(Y,"A path or url to a "),cr=a(Y,"EM",{});var ag=s(cr);Sl=o(ag,"tensorflow index checkpoint file"),ag.forEach(n),Ll=o(Y," (e.g, "),pr=a(Y,"CODE",{});var sg=s(pr);Ol=o(sg,"./tf_model/model.ckpt.index"),sg.forEach(n),Nl=o(Y,`). In
this case, `),hr=a(Y,"CODE",{});var dg=s(hr);Bl=o(dg,"from_tf"),dg.forEach(n),Ul=o(Y," should be set to "),mr=a(Y,"CODE",{});var ig=s(mr);Wl=o(ig,"True"),ig.forEach(n),Vl=o(Y,` and a configuration object should be provided
as `),fr=a(Y,"CODE",{});var lg=s(fr);Rl=o(lg,"config"),lg.forEach(n),Gl=o(Y,` argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.`),Y.forEach(n),sn.forEach(n),Hl=l(w),Fe=a(w,"P",{});var dn=s(Fe);Jl=o(dn,"model"),ur=a(dn,"EM",{});var cg=s(ur);Yl=o(cg,"args (remaining positional arguments, _optional"),cg.forEach(n),Zl=o(dn,`):
All remaining positional arguments will be passed to the underlying model\u2019s `),gr=a(dn,"CODE",{});var pg=s(gr);Kl=o(pg,"__init__"),pg.forEach(n),Ql=o(dn," method."),dn.forEach(n),Xl=l(w),Ae=a(w,"P",{});var ln=s(Ae);ec=o(ln,"kwargs (remaining dictionary of keyword arguments, "),_r=a(ln,"EM",{});var hg=s(_r);tc=o(hg,"optional"),hg.forEach(n),oc=o(ln,`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),vr=a(ln,"CODE",{});var mg=s(vr);nc=o(mg,"output_attentions=True"),mg.forEach(n),rc=o(ln,")."),ln.forEach(n),ac=l(w),Ie=a(w,"UL",{});var cn=s(Ie);br=a(cn,"LI",{});var fg=s(br);sc=o(fg,"To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),fg.forEach(n),dc=l(cn),Er=a(cn,"LI",{});var ug=s(Er);ic=o(ug,"To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),ug.forEach(n),lc=l(cn),yr=a(cn,"LI",{});var gg=s(yr);cc=o(gg,"To update the parent model configuration, do not use a prefix for each configuration parameter."),gg.forEach(n),cn.forEach(n),pc=l(w),Yt=a(w,"P",{});var ws=s(Yt);hc=o(ws,"Behaves differently depending on whether a "),kr=a(ws,"CODE",{});var _g=s(kr);mc=o(_g,"config"),_g.forEach(n),fc=o(ws," is provided or automatically loaded."),ws.forEach(n),uc=l(w),Tr=a(w,"P",{});var vg=s(Tr);gc=o(vg,"Example:"),vg.forEach(n),_c=l(w),b(Zt.$$.fragment,w),w.forEach(n),A.forEach(n),is=l(d),Se=a(d,"H2",{class:!0});var Ms=s(Se);it=a(Ms,"A",{id:!0,class:!0,href:!0});var bg=s(it);wr=a(bg,"SPAN",{});var Eg=s(wr);b(Kt.$$.fragment,Eg),Eg.forEach(n),bg.forEach(n),vc=l(Ms),Mr=a(Ms,"SPAN",{});var yg=s(Mr);bc=o(yg,"TFEncoderDecoderModel"),yg.forEach(n),Ms.forEach(n),ls=l(d),P=a(d,"DIV",{class:!0});var I=s(P);b(Qt.$$.fragment,I),Ec=l(I),Le=a(I,"P",{});var pn=s(Le);yc=o(pn,`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),Dr=a(pn,"CODE",{});var kg=s(Dr);kc=o(kg,"from_pretrained()"),kg.forEach(n),Tc=o(pn,` function and the decoder is loaded via
`),xr=a(pn,"CODE",{});var Tg=s(xr);wc=o(Tg,"from_pretrained()"),Tg.forEach(n),Mc=o(pn,` function. Cross-attention layers are automatically
added to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),pn.forEach(n),Dc=l(I),Xt=a(I,"P",{});var Ds=s(Xt);xc=o(Ds,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),eo=a(Ds,"A",{href:!0,rel:!0});var wg=s(eo);zc=o(wg,"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),wg.forEach(n),jc=o(Ds,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Ds.forEach(n),$c=l(I),zr=a(I,"P",{});var Mg=s(zr);Pc=o(Mg,`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),Mg.forEach(n),qc=l(I),to=a(I,"P",{});var xs=s(to);Cc=o(xs,"This model inherits from "),Bo=a(xs,"A",{href:!0});var Dg=s(Bo);Fc=o(Dg,"TFPreTrainedModel"),Dg.forEach(n),Ac=o(xs,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),xs.forEach(n),Ic=l(I),oo=a(I,"P",{});var zs=s(oo);Sc=o(zs,"This model is also a "),no=a(zs,"A",{href:!0,rel:!0});var xg=s(no);Lc=o(xg,"tf.keras.Model"),xg.forEach(n),Oc=o(zs,` subclass. Use
it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage
and behavior.`),zs.forEach(n),Nc=l(I),ge=a(I,"P",{});var Do=s(ge);jr=a(Do,"CODE",{});var zg=s(jr);Bc=o(zg,"TFEncoderDecoder"),zg.forEach(n),Uc=o(Do,` is a generic model class that will be instantiated as a transformer
architecture with one of the base model classes of the library as encoder and another one as decoder when created
with the :meth`),$r=a(Do,"EM",{});var jg=s($r);Wc=o(jg,"~transformers.TFAutoModel.from_pretrained"),jg.forEach(n),Vc=o(Do,` class method for the encoder and
:meth`),Pr=a(Do,"EM",{});var $g=s(Pr);Rc=o($g,"~transformers.TFAutoModelForCausalLM.from_pretrained"),$g.forEach(n),Gc=o(Do," class method for the decoder."),Do.forEach(n),Hc=l(I),W=a(I,"DIV",{class:!0});var be=s(W);b(ro.$$.fragment,be),Jc=l(be),Oe=a(be,"P",{});var hn=s(Oe);Yc=o(hn,"The "),Uo=a(hn,"A",{href:!0});var Pg=s(Uo);Zc=o(Pg,"TFEncoderDecoderModel"),Pg.forEach(n),Kc=o(hn," forward method, overrides the "),qr=a(hn,"CODE",{});var qg=s(qr);Qc=o(qg,"__call__"),qg.forEach(n),Xc=o(hn," special method."),hn.forEach(n),ep=l(be),b(lt.$$.fragment,be),tp=l(be),Cr=a(be,"P",{});var Cg=s(Cr);op=o(Cg,"Examples:"),Cg.forEach(n),np=l(be),b(ao.$$.fragment,be),be.forEach(n),rp=l(I),g=a(I,"DIV",{class:!0});var x=s(g);b(so.$$.fragment,x),ap=l(x),Fr=a(x,"P",{});var Fg=s(Fr);sp=o(Fg,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),Fg.forEach(n),dp=l(x),Ne=a(x,"P",{});var mn=s(Ne);ip=o(mn,`Params:
encoder`),Ar=a(mn,"EM",{});var Ag=s(Ar);lp=o(Ag,"pretrained_model_name_or_path (:obj: _str"),Ag.forEach(n),cp=o(mn,", "),Ir=a(mn,"EM",{});var Ig=s(Ir);pp=o(Ig,"optional"),Ig.forEach(n),hp=o(mn,`):
Information necessary to initiate the encoder. Can be either:`),mn.forEach(n),mp=l(x),Be=a(x,"UL",{});var fn=s(Be);oe=a(fn,"LI",{});var _t=s(oe);fp=o(_t,"A string, the "),Sr=a(_t,"EM",{});var Sg=s(Sr);up=o(Sg,"model id"),Sg.forEach(n),gp=o(_t,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Lr=a(_t,"CODE",{});var Lg=s(Lr);_p=o(Lg,"bert-base-uncased"),Lg.forEach(n),vp=o(_t,`, or namespaced under
a user or organization name, like `),Or=a(_t,"CODE",{});var Og=s(Or);bp=o(Og,"dbmdz/bert-base-german-cased"),Og.forEach(n),Ep=o(_t,"."),_t.forEach(n),yp=l(fn),ne=a(fn,"LI",{});var vt=s(ne);kp=o(vt,"A path to a "),Nr=a(vt,"EM",{});var Ng=s(Nr);Tp=o(Ng,"directory"),Ng.forEach(n),wp=o(vt,` containing model weights saved using
`),Wo=a(vt,"A",{href:!0});var Bg=s(Wo);Mp=o(Bg,"save_pretrained()"),Bg.forEach(n),Dp=o(vt,", e.g., "),Br=a(vt,"CODE",{});var Ug=s(Br);xp=o(Ug,"./my_model_directory/"),Ug.forEach(n),zp=o(vt,"."),vt.forEach(n),jp=l(fn),G=a(fn,"LI",{});var Ee=s(G);$p=o(Ee,"A path or url to a "),Ur=a(Ee,"EM",{});var Wg=s(Ur);Pp=o(Wg,"pytorch index checkpoint file"),Wg.forEach(n),qp=o(Ee," (e.g, "),Wr=a(Ee,"CODE",{});var Vg=s(Wr);Cp=o(Vg,"./pt_model/"),Vg.forEach(n),Fp=o(Ee,`). In this case,
`),Vr=a(Ee,"CODE",{});var Rg=s(Vr);Ap=o(Rg,"encoder_from_pt"),Rg.forEach(n),Ip=o(Ee," should be set to "),Rr=a(Ee,"CODE",{});var Gg=s(Rr);Sp=o(Gg,"True"),Gg.forEach(n),Lp=o(Ee,"."),Ee.forEach(n),fn.forEach(n),Op=l(x),re=a(x,"P",{});var bt=s(re);Np=o(bt,"decoder"),Gr=a(bt,"EM",{});var Hg=s(Gr);Bp=o(Hg,"pretrained_model_name_or_path (:obj: _str"),Hg.forEach(n),Up=o(bt,", "),Hr=a(bt,"EM",{});var Jg=s(Hr);Wp=o(Jg,"optional"),Jg.forEach(n),Vp=o(bt,", defaults to "),Jr=a(bt,"EM",{});var Yg=s(Jr);Rp=o(Yg,"None"),Yg.forEach(n),Gp=o(bt,`):
Information necessary to initiate the decoder. Can be either:`),bt.forEach(n),Hp=l(x),Ue=a(x,"UL",{});var un=s(Ue);ae=a(un,"LI",{});var Et=s(ae);Jp=o(Et,"A string, the "),Yr=a(Et,"EM",{});var Zg=s(Yr);Yp=o(Zg,"model id"),Zg.forEach(n),Zp=o(Et,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Zr=a(Et,"CODE",{});var Kg=s(Zr);Kp=o(Kg,"bert-base-uncased"),Kg.forEach(n),Qp=o(Et,`, or namespaced under
a user or organization name, like `),Kr=a(Et,"CODE",{});var Qg=s(Kr);Xp=o(Qg,"dbmdz/bert-base-german-cased"),Qg.forEach(n),eh=o(Et,"."),Et.forEach(n),th=l(un),se=a(un,"LI",{});var yt=s(se);oh=o(yt,"A path to a "),Qr=a(yt,"EM",{});var Xg=s(Qr);nh=o(Xg,"directory"),Xg.forEach(n),rh=o(yt,` containing model weights saved using
`),Vo=a(yt,"A",{href:!0});var e_=s(Vo);ah=o(e_,"save_pretrained()"),e_.forEach(n),sh=o(yt,", e.g., "),Xr=a(yt,"CODE",{});var t_=s(Xr);dh=o(t_,"./my_model_directory/"),t_.forEach(n),ih=o(yt,"."),yt.forEach(n),lh=l(un),H=a(un,"LI",{});var ye=s(H);ch=o(ye,"A path or url to a "),ea=a(ye,"EM",{});var o_=s(ea);ph=o(o_,"pytorch checkpoint file"),o_.forEach(n),hh=o(ye," (e.g, "),ta=a(ye,"CODE",{});var n_=s(ta);mh=o(n_,"./pt_model/"),n_.forEach(n),fh=o(ye,`). In this case,
`),oa=a(ye,"CODE",{});var r_=s(oa);uh=o(r_,"decoder_from_pt"),r_.forEach(n),gh=o(ye," should be set to "),na=a(ye,"CODE",{});var a_=s(na);_h=o(a_,"True"),a_.forEach(n),vh=o(ye,"."),ye.forEach(n),un.forEach(n),bh=l(x),We=a(x,"P",{});var gn=s(We);Eh=o(gn,"model"),ra=a(gn,"EM",{});var s_=s(ra);yh=o(s_,"args (remaining positional arguments, _optional"),s_.forEach(n),kh=o(gn,`):
All remaning positional arguments will be passed to the underlying model\u2019s `),aa=a(gn,"CODE",{});var d_=s(aa);Th=o(d_,"__init__"),d_.forEach(n),wh=o(gn," method."),gn.forEach(n),Mh=l(x),Ve=a(x,"P",{});var _n=s(Ve);Dh=o(_n,"kwargs (remaining dictionary of keyword arguments, "),sa=a(_n,"EM",{});var i_=s(sa);xh=o(i_,"optional"),i_.forEach(n),zh=o(_n,`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),da=a(_n,"CODE",{});var l_=s(da);jh=o(l_,"output_attentions=True"),l_.forEach(n),$h=o(_n,")."),_n.forEach(n),Ph=l(x),Re=a(x,"UL",{});var vn=s(Re);ia=a(vn,"LI",{});var c_=s(ia);qh=o(c_,"To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),c_.forEach(n),Ch=l(vn),la=a(vn,"LI",{});var p_=s(la);Fh=o(p_,"To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),p_.forEach(n),Ah=l(vn),ca=a(vn,"LI",{});var h_=s(ca);Ih=o(h_,"To update the parent model configuration, do not use a prefix for each configuration parameter."),h_.forEach(n),vn.forEach(n),Sh=l(x),io=a(x,"P",{});var js=s(io);Lh=o(js,"Behaves differently depending on whether a "),pa=a(js,"CODE",{});var m_=s(pa);Oh=o(m_,"config"),m_.forEach(n),Nh=o(js," is provided or automatically loaded."),js.forEach(n),Bh=l(x),ha=a(x,"P",{});var f_=s(ha);Uh=o(f_,"Example:"),f_.forEach(n),Wh=l(x),b(lo.$$.fragment,x),x.forEach(n),I.forEach(n),cs=l(d),Ge=a(d,"H2",{class:!0});var $s=s(Ge);ct=a($s,"A",{id:!0,class:!0,href:!0});var u_=s(ct);ma=a(u_,"SPAN",{});var g_=s(ma);b(co.$$.fragment,g_),g_.forEach(n),u_.forEach(n),Vh=l($s),fa=a($s,"SPAN",{});var __=s(fa);Rh=o(__,"FlaxEncoderDecoderModel"),__.forEach(n),$s.forEach(n),ps=l(d),q=a(d,"DIV",{class:!0});var S=s(q);b(po.$$.fragment,S),Gh=l(S),He=a(S,"P",{});var bn=s(He);Hh=o(bn,`This class can be used to initialize a sequence-to-sequence model with any pretrained autoencoding model as the
encoder and any pretrained autoregressive model as the decoder. The encoder is loaded via
`),ua=a(bn,"CODE",{});var v_=s(ua);Jh=o(v_,"from_pretrained()"),v_.forEach(n),Yh=o(bn,` function and the decoder is loaded via
`),ga=a(bn,"CODE",{});var b_=s(ga);Zh=o(b_,"from_pretrained()"),b_.forEach(n),Kh=o(bn,` function. Cross-attention layers are automatically added
to the decoder and should be fine-tuned on a downstream generative task, like summarization.`),bn.forEach(n),Qh=l(S),ho=a(S,"P",{});var Ps=s(ho);Xh=o(Ps,`The effectiveness of initializing sequence-to-sequence models with pretrained checkpoints for sequence generation
tasks was shown in `),mo=a(Ps,"A",{href:!0,rel:!0});var E_=s(mo);em=o(E_,"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"),E_.forEach(n),tm=o(Ps,` by Sascha Rothe, Shashi Narayan, Aliaksei Severyn. Michael Matena, Yanqi
Zhou, Wei Li, Peter J. Liu.`),Ps.forEach(n),om=l(S),_a=a(S,"P",{});var y_=s(_a);nm=o(y_,`After such an Encoder Decoder model has been trained/fine-tuned, it can be saved/loaded just like any other models
(see the examples for more information).`),y_.forEach(n),rm=l(S),fo=a(S,"P",{});var qs=s(fo);am=o(qs,"This model inherits from "),Ro=a(qs,"A",{href:!0});var k_=s(Ro);sm=o(k_,"FlaxPreTrainedModel"),k_.forEach(n),dm=o(qs,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),qs.forEach(n),im=l(S),uo=a(S,"P",{});var Cs=s(uo);lm=o(Cs,"This model is also a Flax Linen "),go=a(Cs,"A",{href:!0,rel:!0});var T_=s(go);cm=o(T_,"flax.nn.Module"),T_.forEach(n),pm=o(Cs,` subclass. Use it as a regular Flax
Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Cs.forEach(n),hm=l(S),_e=a(S,"P",{});var xo=s(_e);Go=a(xo,"A",{href:!0});var w_=s(Go);mm=o(w_,"FlaxEncoderDecoderModel"),w_.forEach(n),fm=o(xo,` is a generic model class that will be instantiated as a transformer
architecture with the module (flax.nn.Module) of one of the base model classes of the library as encoder module and
another one as decoder module when created with the :meth`),va=a(xo,"EM",{});var M_=s(va);um=o(M_,"~transformers.FlaxAutoModel.from_pretrained"),M_.forEach(n),gm=o(xo,` class method
for the encoder and :meth`),ba=a(xo,"EM",{});var D_=s(ba);_m=o(D_,"~transformers.FlaxAutoModelForCausalLM.from_pretrained"),D_.forEach(n),vm=o(xo," class method for the decoder."),xo.forEach(n),bm=l(S),V=a(S,"DIV",{class:!0});var ke=s(V);b(_o.$$.fragment,ke),Em=l(ke),Je=a(ke,"P",{});var En=s(Je);ym=o(En,"The "),Ho=a(En,"A",{href:!0});var x_=s(Ho);km=o(x_,"FlaxEncoderDecoderModel"),x_.forEach(n),Tm=o(En," forward method, overrides the "),Ea=a(En,"CODE",{});var z_=s(Ea);wm=o(z_,"__call__"),z_.forEach(n),Mm=o(En," special method."),En.forEach(n),Dm=l(ke),b(pt.$$.fragment,ke),xm=l(ke),ya=a(ke,"P",{});var j_=s(ya);zm=o(j_,"Examples:"),j_.forEach(n),jm=l(ke),b(vo.$$.fragment,ke),ke.forEach(n),$m=l(S),_=a(S,"DIV",{class:!0});var z=s(_);b(bo.$$.fragment,z),Pm=l(z),ka=a(z,"P",{});var $_=s(ka);qm=o($_,`Instantiate an encoder and a decoder from one or two base classes of the library from pretrained model
checkpoints.`),$_.forEach(n),Cm=l(z),Ye=a(z,"P",{});var yn=s(Ye);Fm=o(yn,`Params:
encoder`),Ta=a(yn,"EM",{});var P_=s(Ta);Am=o(P_,"pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),P_.forEach(n),Im=o(yn,", "),wa=a(yn,"EM",{});var q_=s(wa);Sm=o(q_,"optional"),q_.forEach(n),Lm=o(yn,`):
Information necessary to initiate the encoder. Can be either:`),yn.forEach(n),Om=l(z),Eo=a(z,"UL",{});var Fs=s(Eo);de=a(Fs,"LI",{});var kt=s(de);Nm=o(kt,"A string, the "),Ma=a(kt,"EM",{});var C_=s(Ma);Bm=o(C_,"model id"),C_.forEach(n),Um=o(kt,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Da=a(kt,"CODE",{});var F_=s(Da);Wm=o(F_,"bert-base-uncased"),F_.forEach(n),Vm=o(kt,`, or namespaced under
a user or organization name, like `),xa=a(kt,"CODE",{});var A_=s(xa);Rm=o(A_,"dbmdz/bert-base-german-cased"),A_.forEach(n),Gm=o(kt,"."),kt.forEach(n),Hm=l(Fs),ie=a(Fs,"LI",{});var Tt=s(ie);Jm=o(Tt,"A path to a "),za=a(Tt,"EM",{});var I_=s(za);Ym=o(I_,"directory"),I_.forEach(n),Zm=o(Tt,` containing model weights saved using
`),Jo=a(Tt,"A",{href:!0});var S_=s(Jo);Km=o(S_,"save_pretrained()"),S_.forEach(n),Qm=o(Tt,", e.g., "),ja=a(Tt,"CODE",{});var L_=s(ja);Xm=o(L_,"./my_model_directory/"),L_.forEach(n),ef=o(Tt,"."),Tt.forEach(n),Fs.forEach(n),tf=l(z),le=a(z,"P",{});var wt=s(le);of=o(wt,"decoder"),$a=a(wt,"EM",{});var O_=s($a);nf=o(O_,"pretrained_model_name_or_path (:obj: _Union[str, os.PathLike]"),O_.forEach(n),rf=o(wt,", "),Pa=a(wt,"EM",{});var N_=s(Pa);af=o(N_,"optional"),N_.forEach(n),sf=o(wt,", defaults to "),qa=a(wt,"EM",{});var B_=s(qa);df=o(B_,"None"),B_.forEach(n),lf=o(wt,`):
Information necessary to initiate the decoder. Can be either:`),wt.forEach(n),cf=l(z),yo=a(z,"UL",{});var As=s(yo);ce=a(As,"LI",{});var Mt=s(ce);pf=o(Mt,"A string, the "),Ca=a(Mt,"EM",{});var U_=s(Ca);hf=o(U_,"model id"),U_.forEach(n),mf=o(Mt,` of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Fa=a(Mt,"CODE",{});var W_=s(Fa);ff=o(W_,"bert-base-uncased"),W_.forEach(n),uf=o(Mt,`, or namespaced under
a user or organization name, like `),Aa=a(Mt,"CODE",{});var V_=s(Aa);gf=o(V_,"dbmdz/bert-base-german-cased"),V_.forEach(n),_f=o(Mt,"."),Mt.forEach(n),vf=l(As),pe=a(As,"LI",{});var Dt=s(pe);bf=o(Dt,"A path to a "),Ia=a(Dt,"EM",{});var R_=s(Ia);Ef=o(R_,"directory"),R_.forEach(n),yf=o(Dt,` containing model weights saved using
`),Yo=a(Dt,"A",{href:!0});var G_=s(Yo);kf=o(G_,"save_pretrained()"),G_.forEach(n),Tf=o(Dt,", e.g., "),Sa=a(Dt,"CODE",{});var H_=s(Sa);wf=o(H_,"./my_model_directory/"),H_.forEach(n),Mf=o(Dt,"."),Dt.forEach(n),As.forEach(n),Df=l(z),Ze=a(z,"P",{});var kn=s(Ze);xf=o(kn,"model"),La=a(kn,"EM",{});var J_=s(La);zf=o(J_,"args (remaining positional arguments, _optional"),J_.forEach(n),jf=o(kn,`):
All remaning positional arguments will be passed to the underlying model\u2019s `),Oa=a(kn,"CODE",{});var Y_=s(Oa);$f=o(Y_,"__init__"),Y_.forEach(n),Pf=o(kn," method."),kn.forEach(n),qf=l(z),Ke=a(z,"P",{});var Tn=s(Ke);Cf=o(Tn,"kwargs (remaining dictionary of keyword arguments, "),Na=a(Tn,"EM",{});var Z_=s(Na);Ff=o(Z_,"optional"),Z_.forEach(n),Af=o(Tn,`):
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
`),Ba=a(Tn,"CODE",{});var K_=s(Ba);If=o(K_,"output_attentions=True"),K_.forEach(n),Sf=o(Tn,")."),Tn.forEach(n),Lf=l(z),Qe=a(z,"UL",{});var wn=s(Qe);Ua=a(wn,"LI",{});var Q_=s(Ua);Of=o(Q_,"To update the encoder configuration, use the prefix _encoder__ for each configuration parameter."),Q_.forEach(n),Nf=l(wn),Wa=a(wn,"LI",{});var X_=s(Wa);Bf=o(X_,"To update the decoder configuration, use the prefix _decoder__ for each configuration parameter."),X_.forEach(n),Uf=l(wn),Va=a(wn,"LI",{});var ev=s(Va);Wf=o(ev,"To update the parent model configuration, do not use a prefix for each configuration parameter."),ev.forEach(n),wn.forEach(n),Vf=l(z),ko=a(z,"P",{});var Is=s(ko);Rf=o(Is,"Behaves differently depending on whether a "),Ra=a(Is,"CODE",{});var tv=s(Ra);Gf=o(tv,"config"),tv.forEach(n),Hf=o(Is," is provided or automatically loaded."),Is.forEach(n),Jf=l(z),Ga=a(z,"P",{});var ov=s(Ga);Yf=o(ov,"Example:"),ov.forEach(n),Zf=l(z),b(To.$$.fragment,z),z.forEach(n),S.forEach(n),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(pv)),c(j,"id","encoder-decoder-models"),c(j,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(j,"href","#encoder-decoder-models"),c(f,"class","relative group"),c(zo,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c(zt,"href","https://arxiv.org/abs/1907.12461"),c(zt,"rel","nofollow"),c(jo,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c($o,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),c(jt,"href","https://arxiv.org/abs/1908.08345"),c(jt,"rel","nofollow"),c(Pt,"href","https://github.com/thomwolf"),c(Pt,"rel","nofollow"),c(qt,"href","https://github.com/ydshieh"),c(qt,"rel","nofollow"),c(ot,"id","transformers.EncoderDecoderConfig"),c(ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ot,"href","#transformers.EncoderDecoderConfig"),c(we,"class","relative group"),c(Po,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"),c(qo,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c(Co,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(Fo,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ao,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"),c(rt,"class","docstring"),c(at,"class","docstring"),c(C,"class","docstring"),c(st,"id","transformers.EncoderDecoderModel"),c(st,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(st,"href","#transformers.EncoderDecoderModel"),c(xe,"class","relative group"),c(Ut,"href","https://arxiv.org/abs/1907.12461"),c(Ut,"rel","nofollow"),c(Io,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(Rt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Rt,"rel","nofollow"),c(So,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c(Lo,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c(U,"class","docstring"),c(Oo,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),c(No,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),c(u,"class","docstring"),c($,"class","docstring"),c(it,"id","transformers.TFEncoderDecoderModel"),c(it,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(it,"href","#transformers.TFEncoderDecoderModel"),c(Se,"class","relative group"),c(eo,"href","https://arxiv.org/abs/1907.12461"),c(eo,"rel","nofollow"),c(Bo,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(no,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(no,"rel","nofollow"),c(Uo,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.TFEncoderDecoderModel"),c(W,"class","docstring"),c(Wo,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained"),c(Vo,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained"),c(g,"class","docstring"),c(P,"class","docstring"),c(ct,"id","transformers.FlaxEncoderDecoderModel"),c(ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ct,"href","#transformers.FlaxEncoderDecoderModel"),c(Ge,"class","relative group"),c(mo,"href","https://arxiv.org/abs/1907.12461"),c(mo,"rel","nofollow"),c(Ro,"href","/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(go,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),c(go,"rel","nofollow"),c(Go,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.FlaxEncoderDecoderModel"),c(Ho,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.FlaxEncoderDecoderModel"),c(V,"class","docstring"),c(Jo,"href","/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained"),c(Yo,"href","/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.save_pretrained"),c(_,"class","docstring"),c(q,"class","docstring")},m(d,p){e(document.head,h),m(d,F,p),m(d,f,p),e(f,j),e(j,L),E(M,L,null),e(f,D),e(f,B),e(B,Ss),m(d,Ka,p),m(d,Xe,p),e(Xe,Ls),e(Xe,zo),e(zo,Os),e(Xe,Ns),m(d,Qa,p),m(d,et,p),e(et,Bs),e(et,zt),e(zt,Us),e(et,Ws),m(d,Xa,p),m(d,tt,p),e(tt,Vs),e(tt,jo),e(jo,Rs),e(tt,Gs),m(d,es,p),m(d,he,p),e(he,Hs),e(he,$o),e($o,Js),e(he,Ys),e(he,jt),e(jt,Zs),e(he,Ks),m(d,ts,p),m(d,me,p),e(me,Qs),e(me,Mn),e(Mn,Xs),e(me,ed),e(me,Dn),e(Dn,td),e(me,od),m(d,os,p),E($t,d,p),m(d,ns,p),m(d,fe,p),e(fe,nd),e(fe,Pt),e(Pt,rd),e(fe,ad),e(fe,qt),e(qt,sd),e(fe,dd),m(d,rs,p),m(d,we,p),e(we,ot),e(ot,xn),E(Ct,xn,null),e(we,id),e(we,zn),e(zn,ld),m(d,as,p),m(d,C,p),E(Ft,C,null),e(C,cd),e(C,nt),e(nt,Po),e(Po,pd),e(nt,hd),e(nt,qo),e(qo,md),e(nt,fd),e(C,ud),e(C,Me),e(Me,gd),e(Me,Co),e(Co,_d),e(Me,vd),e(Me,Fo),e(Fo,bd),e(Me,Ed),e(C,yd),e(C,jn),e(jn,kd),e(C,Td),E(At,C,null),e(C,wd),e(C,rt),E(It,rt,null),e(rt,Md),e(rt,St),e(St,Dd),e(St,Ao),e(Ao,xd),e(St,zd),e(C,jd),e(C,at),E(Lt,at,null),e(at,$d),e(at,De),e(De,Pd),e(De,$n),e($n,qd),e(De,Cd),e(De,Pn),e(Pn,Fd),e(De,Ad),m(d,ss,p),m(d,xe,p),e(xe,st),e(st,qn),E(Ot,qn,null),e(xe,Id),e(xe,Cn),e(Cn,Sd),m(d,ds,p),m(d,$,p),E(Nt,$,null),e($,Ld),e($,ze),e(ze,Od),e(ze,Fn),e(Fn,Nd),e(ze,Bd),e(ze,An),e(An,Ud),e(ze,Wd),e($,Vd),e($,Bt),e(Bt,Rd),e(Bt,Ut),e(Ut,Gd),e(Bt,Hd),e($,Jd),e($,In),e(In,Yd),e($,Zd),e($,Wt),e(Wt,Kd),e(Wt,Io),e(Io,Qd),e(Wt,Xd),e($,ei),e($,Vt),e(Vt,ti),e(Vt,Rt),e(Rt,oi),e(Vt,ni),e($,ri),e($,ue),e(ue,So),e(So,ai),e(ue,si),e(ue,Sn),e(Sn,di),e(ue,ii),e(ue,Ln),e(Ln,li),e(ue,ci),e($,pi),e($,U),E(Gt,U,null),e(U,hi),e(U,je),e(je,mi),e(je,Lo),e(Lo,fi),e(je,ui),e(je,On),e(On,gi),e(je,_i),e(U,vi),E(dt,U,null),e(U,bi),e(U,Nn),e(Nn,Ei),e(U,yi),E(Ht,U,null),e($,ki),e($,u),E(Jt,u,null),e(u,Ti),e(u,Bn),e(Bn,wi),e(u,Mi),e(u,$e),e($e,Di),e($e,Un),e(Un,xi),e($e,zi),e($e,Wn),e(Wn,ji),e($e,$i),e(u,Pi),e(u,Pe),e(Pe,qi),e(Pe,Vn),e(Vn,Ci),e(Pe,Fi),e(Pe,Rn),e(Rn,Ai),e(Pe,Ii),e(u,Si),e(u,qe),e(qe,K),e(K,Li),e(K,Gn),e(Gn,Oi),e(K,Ni),e(K,Hn),e(Hn,Bi),e(K,Ui),e(K,Jn),e(Jn,Wi),e(K,Vi),e(qe,Ri),e(qe,Q),e(Q,Gi),e(Q,Yn),e(Yn,Hi),e(Q,Ji),e(Q,Oo),e(Oo,Yi),e(Q,Zi),e(Q,Zn),e(Zn,Ki),e(Q,Qi),e(qe,Xi),e(qe,O),e(O,el),e(O,Kn),e(Kn,tl),e(O,ol),e(O,Qn),e(Qn,nl),e(O,rl),e(O,Xn),e(Xn,al),e(O,sl),e(O,er),e(er,dl),e(O,il),e(O,tr),e(tr,ll),e(O,cl),e(u,pl),e(u,X),e(X,hl),e(X,or),e(or,ml),e(X,fl),e(X,nr),e(nr,ul),e(X,gl),e(X,rr),e(rr,_l),e(X,vl),e(u,bl),e(u,Ce),e(Ce,ee),e(ee,El),e(ee,ar),e(ar,yl),e(ee,kl),e(ee,sr),e(sr,Tl),e(ee,wl),e(ee,dr),e(dr,Ml),e(ee,Dl),e(Ce,xl),e(Ce,te),e(te,zl),e(te,ir),e(ir,jl),e(te,$l),e(te,No),e(No,Pl),e(te,ql),e(te,lr),e(lr,Cl),e(te,Fl),e(Ce,Al),e(Ce,N),e(N,Il),e(N,cr),e(cr,Sl),e(N,Ll),e(N,pr),e(pr,Ol),e(N,Nl),e(N,hr),e(hr,Bl),e(N,Ul),e(N,mr),e(mr,Wl),e(N,Vl),e(N,fr),e(fr,Rl),e(N,Gl),e(u,Hl),e(u,Fe),e(Fe,Jl),e(Fe,ur),e(ur,Yl),e(Fe,Zl),e(Fe,gr),e(gr,Kl),e(Fe,Ql),e(u,Xl),e(u,Ae),e(Ae,ec),e(Ae,_r),e(_r,tc),e(Ae,oc),e(Ae,vr),e(vr,nc),e(Ae,rc),e(u,ac),e(u,Ie),e(Ie,br),e(br,sc),e(Ie,dc),e(Ie,Er),e(Er,ic),e(Ie,lc),e(Ie,yr),e(yr,cc),e(u,pc),e(u,Yt),e(Yt,hc),e(Yt,kr),e(kr,mc),e(Yt,fc),e(u,uc),e(u,Tr),e(Tr,gc),e(u,_c),E(Zt,u,null),m(d,is,p),m(d,Se,p),e(Se,it),e(it,wr),E(Kt,wr,null),e(Se,vc),e(Se,Mr),e(Mr,bc),m(d,ls,p),m(d,P,p),E(Qt,P,null),e(P,Ec),e(P,Le),e(Le,yc),e(Le,Dr),e(Dr,kc),e(Le,Tc),e(Le,xr),e(xr,wc),e(Le,Mc),e(P,Dc),e(P,Xt),e(Xt,xc),e(Xt,eo),e(eo,zc),e(Xt,jc),e(P,$c),e(P,zr),e(zr,Pc),e(P,qc),e(P,to),e(to,Cc),e(to,Bo),e(Bo,Fc),e(to,Ac),e(P,Ic),e(P,oo),e(oo,Sc),e(oo,no),e(no,Lc),e(oo,Oc),e(P,Nc),e(P,ge),e(ge,jr),e(jr,Bc),e(ge,Uc),e(ge,$r),e($r,Wc),e(ge,Vc),e(ge,Pr),e(Pr,Rc),e(ge,Gc),e(P,Hc),e(P,W),E(ro,W,null),e(W,Jc),e(W,Oe),e(Oe,Yc),e(Oe,Uo),e(Uo,Zc),e(Oe,Kc),e(Oe,qr),e(qr,Qc),e(Oe,Xc),e(W,ep),E(lt,W,null),e(W,tp),e(W,Cr),e(Cr,op),e(W,np),E(ao,W,null),e(P,rp),e(P,g),E(so,g,null),e(g,ap),e(g,Fr),e(Fr,sp),e(g,dp),e(g,Ne),e(Ne,ip),e(Ne,Ar),e(Ar,lp),e(Ne,cp),e(Ne,Ir),e(Ir,pp),e(Ne,hp),e(g,mp),e(g,Be),e(Be,oe),e(oe,fp),e(oe,Sr),e(Sr,up),e(oe,gp),e(oe,Lr),e(Lr,_p),e(oe,vp),e(oe,Or),e(Or,bp),e(oe,Ep),e(Be,yp),e(Be,ne),e(ne,kp),e(ne,Nr),e(Nr,Tp),e(ne,wp),e(ne,Wo),e(Wo,Mp),e(ne,Dp),e(ne,Br),e(Br,xp),e(ne,zp),e(Be,jp),e(Be,G),e(G,$p),e(G,Ur),e(Ur,Pp),e(G,qp),e(G,Wr),e(Wr,Cp),e(G,Fp),e(G,Vr),e(Vr,Ap),e(G,Ip),e(G,Rr),e(Rr,Sp),e(G,Lp),e(g,Op),e(g,re),e(re,Np),e(re,Gr),e(Gr,Bp),e(re,Up),e(re,Hr),e(Hr,Wp),e(re,Vp),e(re,Jr),e(Jr,Rp),e(re,Gp),e(g,Hp),e(g,Ue),e(Ue,ae),e(ae,Jp),e(ae,Yr),e(Yr,Yp),e(ae,Zp),e(ae,Zr),e(Zr,Kp),e(ae,Qp),e(ae,Kr),e(Kr,Xp),e(ae,eh),e(Ue,th),e(Ue,se),e(se,oh),e(se,Qr),e(Qr,nh),e(se,rh),e(se,Vo),e(Vo,ah),e(se,sh),e(se,Xr),e(Xr,dh),e(se,ih),e(Ue,lh),e(Ue,H),e(H,ch),e(H,ea),e(ea,ph),e(H,hh),e(H,ta),e(ta,mh),e(H,fh),e(H,oa),e(oa,uh),e(H,gh),e(H,na),e(na,_h),e(H,vh),e(g,bh),e(g,We),e(We,Eh),e(We,ra),e(ra,yh),e(We,kh),e(We,aa),e(aa,Th),e(We,wh),e(g,Mh),e(g,Ve),e(Ve,Dh),e(Ve,sa),e(sa,xh),e(Ve,zh),e(Ve,da),e(da,jh),e(Ve,$h),e(g,Ph),e(g,Re),e(Re,ia),e(ia,qh),e(Re,Ch),e(Re,la),e(la,Fh),e(Re,Ah),e(Re,ca),e(ca,Ih),e(g,Sh),e(g,io),e(io,Lh),e(io,pa),e(pa,Oh),e(io,Nh),e(g,Bh),e(g,ha),e(ha,Uh),e(g,Wh),E(lo,g,null),m(d,cs,p),m(d,Ge,p),e(Ge,ct),e(ct,ma),E(co,ma,null),e(Ge,Vh),e(Ge,fa),e(fa,Rh),m(d,ps,p),m(d,q,p),E(po,q,null),e(q,Gh),e(q,He),e(He,Hh),e(He,ua),e(ua,Jh),e(He,Yh),e(He,ga),e(ga,Zh),e(He,Kh),e(q,Qh),e(q,ho),e(ho,Xh),e(ho,mo),e(mo,em),e(ho,tm),e(q,om),e(q,_a),e(_a,nm),e(q,rm),e(q,fo),e(fo,am),e(fo,Ro),e(Ro,sm),e(fo,dm),e(q,im),e(q,uo),e(uo,lm),e(uo,go),e(go,cm),e(uo,pm),e(q,hm),e(q,_e),e(_e,Go),e(Go,mm),e(_e,fm),e(_e,va),e(va,um),e(_e,gm),e(_e,ba),e(ba,_m),e(_e,vm),e(q,bm),e(q,V),E(_o,V,null),e(V,Em),e(V,Je),e(Je,ym),e(Je,Ho),e(Ho,km),e(Je,Tm),e(Je,Ea),e(Ea,wm),e(Je,Mm),e(V,Dm),E(pt,V,null),e(V,xm),e(V,ya),e(ya,zm),e(V,jm),E(vo,V,null),e(q,$m),e(q,_),E(bo,_,null),e(_,Pm),e(_,ka),e(ka,qm),e(_,Cm),e(_,Ye),e(Ye,Fm),e(Ye,Ta),e(Ta,Am),e(Ye,Im),e(Ye,wa),e(wa,Sm),e(Ye,Lm),e(_,Om),e(_,Eo),e(Eo,de),e(de,Nm),e(de,Ma),e(Ma,Bm),e(de,Um),e(de,Da),e(Da,Wm),e(de,Vm),e(de,xa),e(xa,Rm),e(de,Gm),e(Eo,Hm),e(Eo,ie),e(ie,Jm),e(ie,za),e(za,Ym),e(ie,Zm),e(ie,Jo),e(Jo,Km),e(ie,Qm),e(ie,ja),e(ja,Xm),e(ie,ef),e(_,tf),e(_,le),e(le,of),e(le,$a),e($a,nf),e(le,rf),e(le,Pa),e(Pa,af),e(le,sf),e(le,qa),e(qa,df),e(le,lf),e(_,cf),e(_,yo),e(yo,ce),e(ce,pf),e(ce,Ca),e(Ca,hf),e(ce,mf),e(ce,Fa),e(Fa,ff),e(ce,uf),e(ce,Aa),e(Aa,gf),e(ce,_f),e(yo,vf),e(yo,pe),e(pe,bf),e(pe,Ia),e(Ia,Ef),e(pe,yf),e(pe,Yo),e(Yo,kf),e(pe,Tf),e(pe,Sa),e(Sa,wf),e(pe,Mf),e(_,Df),e(_,Ze),e(Ze,xf),e(Ze,La),e(La,zf),e(Ze,jf),e(Ze,Oa),e(Oa,$f),e(Ze,Pf),e(_,qf),e(_,Ke),e(Ke,Cf),e(Ke,Na),e(Na,Ff),e(Ke,Af),e(Ke,Ba),e(Ba,If),e(Ke,Sf),e(_,Lf),e(_,Qe),e(Qe,Ua),e(Ua,Of),e(Qe,Nf),e(Qe,Wa),e(Wa,Bf),e(Qe,Uf),e(Qe,Va),e(Va,Wf),e(_,Vf),e(_,ko),e(ko,Rf),e(ko,Ra),e(Ra,Gf),e(ko,Hf),e(_,Jf),e(_,Ga),e(Ga,Yf),e(_,Zf),E(To,_,null),hs=!0},p(d,[p]){const wo={};p&2&&(wo.$$scope={dirty:p,ctx:d}),dt.$set(wo);const Ha={};p&2&&(Ha.$$scope={dirty:p,ctx:d}),lt.$set(Ha);const Ja={};p&2&&(Ja.$$scope={dirty:p,ctx:d}),pt.$set(Ja)},i(d){hs||(y(M.$$.fragment,d),y($t.$$.fragment,d),y(Ct.$$.fragment,d),y(Ft.$$.fragment,d),y(At.$$.fragment,d),y(It.$$.fragment,d),y(Lt.$$.fragment,d),y(Ot.$$.fragment,d),y(Nt.$$.fragment,d),y(Gt.$$.fragment,d),y(dt.$$.fragment,d),y(Ht.$$.fragment,d),y(Jt.$$.fragment,d),y(Zt.$$.fragment,d),y(Kt.$$.fragment,d),y(Qt.$$.fragment,d),y(ro.$$.fragment,d),y(lt.$$.fragment,d),y(ao.$$.fragment,d),y(so.$$.fragment,d),y(lo.$$.fragment,d),y(co.$$.fragment,d),y(po.$$.fragment,d),y(_o.$$.fragment,d),y(pt.$$.fragment,d),y(vo.$$.fragment,d),y(bo.$$.fragment,d),y(To.$$.fragment,d),hs=!0)},o(d){k(M.$$.fragment,d),k($t.$$.fragment,d),k(Ct.$$.fragment,d),k(Ft.$$.fragment,d),k(At.$$.fragment,d),k(It.$$.fragment,d),k(Lt.$$.fragment,d),k(Ot.$$.fragment,d),k(Nt.$$.fragment,d),k(Gt.$$.fragment,d),k(dt.$$.fragment,d),k(Ht.$$.fragment,d),k(Jt.$$.fragment,d),k(Zt.$$.fragment,d),k(Kt.$$.fragment,d),k(Qt.$$.fragment,d),k(ro.$$.fragment,d),k(lt.$$.fragment,d),k(ao.$$.fragment,d),k(so.$$.fragment,d),k(lo.$$.fragment,d),k(co.$$.fragment,d),k(po.$$.fragment,d),k(_o.$$.fragment,d),k(pt.$$.fragment,d),k(vo.$$.fragment,d),k(bo.$$.fragment,d),k(To.$$.fragment,d),hs=!1},d(d){n(h),d&&n(F),d&&n(f),T(M),d&&n(Ka),d&&n(Xe),d&&n(Qa),d&&n(et),d&&n(Xa),d&&n(tt),d&&n(es),d&&n(he),d&&n(ts),d&&n(me),d&&n(os),T($t,d),d&&n(ns),d&&n(fe),d&&n(rs),d&&n(we),T(Ct),d&&n(as),d&&n(C),T(Ft),T(At),T(It),T(Lt),d&&n(ss),d&&n(xe),T(Ot),d&&n(ds),d&&n($),T(Nt),T(Gt),T(dt),T(Ht),T(Jt),T(Zt),d&&n(is),d&&n(Se),T(Kt),d&&n(ls),d&&n(P),T(Qt),T(ro),T(lt),T(ao),T(so),T(lo),d&&n(cs),d&&n(Ge),T(co),d&&n(ps),d&&n(q),T(po),T(_o),T(pt),T(vo),T(bo),T(To)}}}const pv={local:"encoder-decoder-models",sections:[{local:"transformers.EncoderDecoderConfig",title:"EncoderDecoderConfig"},{local:"transformers.EncoderDecoderModel",title:"EncoderDecoderModel"},{local:"transformers.TFEncoderDecoderModel",title:"TFEncoderDecoderModel"},{local:"transformers.FlaxEncoderDecoderModel",title:"FlaxEncoderDecoderModel"}],title:"Encoder Decoder Models"};function hv(Te,h,F){let{fw:f}=h;return Te.$$set=j=>{"fw"in j&&F(0,f=j.fw)},[f]}class bv extends nv{constructor(h){super();rv(this,h,hv,cv,av,{fw:0})}}export{bv as default,pv as metadata};
