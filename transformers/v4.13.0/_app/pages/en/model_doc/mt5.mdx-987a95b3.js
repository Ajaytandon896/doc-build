import{S as op,i as ip,s as lp,e as r,k as d,w as f,t as o,L as dp,c as a,d as s,m as p,a as n,x as h,h as i,b as l,J as e,g as c,y as u,K as pp,q as g,o as _,B as v}from"../../../chunks/vendor-e859c359.js";import{D as k}from"../../../chunks/Docstring-ade913b3.js";import{C as Ce}from"../../../chunks/CodeBlock-ce4317c2.js";import{I as C}from"../../../chunks/IconCopyLink-5fae3b20.js";import"../../../chunks/CopyButton-77addb3d.js";function mp(Ga){let N,Bt,P,A,ws,Pe,Oa,ys,Va,Mr,U,re,Ms,Ae,Ua,Es,Wa,Er,ae,Ba,Se,Ha,Ra,zr,Ht,Xa,br,Rt,zs,Ka,qr,ne,Ja,Le,Qa,Ya,jr,Xt,Za,Fr,y,bs,qs,Ne,en,tn,js,Fs,De,sn,rn,xs,Cs,Ie,an,nn,Ps,As,Ge,on,ln,Ss,Kt,Oe,dn,pn,xr,D,mn,Ve,cn,fn,Ue,hn,un,Cr,W,oe,Ls,We,gn,Ns,_n,Pr,S,Be,vn,L,Tn,Jt,kn,$n,Qt,wn,yn,He,Mn,En,zn,B,bn,Yt,qn,jn,Zt,Fn,xn,Ar,H,ie,Ds,Re,Cn,Is,Pn,Sr,T,Xe,An,Ke,Sn,Je,Ln,Nn,Dn,Qe,In,es,Gn,On,Vn,Ye,Un,Ze,Wn,Gs,Bn,Hn,Rn,Xn,I,et,Kn,Os,Jn,Qn,tt,ts,Yn,Vs,Zn,eo,ss,to,Us,so,ro,le,st,ao,Ws,no,oo,de,rt,io,Bs,lo,po,pe,at,mo,nt,co,Hs,fo,ho,Lr,me,uo,rs,go,_o,Nr,R,ce,Rs,ot,vo,Xs,To,Dr,w,it,ko,X,$o,Ks,wo,yo,lt,Mo,Eo,zo,dt,bo,as,qo,jo,Fo,G,pt,xo,Js,Co,Po,mt,ns,Ao,Qs,So,Lo,os,No,Ys,Do,Io,fe,ct,Go,Zs,Oo,Ir,he,Vo,is,Uo,Wo,Gr,K,ue,er,ft,Bo,tr,Ho,Or,M,ht,Ro,ut,Xo,ls,Ko,Jo,Qo,sr,Yo,Zo,gt,Vr,J,ge,rr,_t,ei,ar,ti,Ur,E,vt,si,Tt,ri,ds,ai,ni,oi,nr,ii,li,kt,Wr,Q,_e,or,$t,di,ir,pi,Br,z,wt,mi,yt,ci,ps,fi,hi,ui,lr,gi,_i,Mt,Hr,Y,ve,dr,Et,vi,pr,Ti,Rr,b,zt,ki,bt,$i,ms,wi,yi,Mi,mr,Ei,zi,qt,Xr,Z,Te,cr,jt,bi,fr,qi,Kr,q,Ft,ji,xt,Fi,cs,xi,Ci,Pi,hr,Ai,Si,Ct,Jr,ee,ke,ur,Pt,Li,gr,Ni,Qr,j,At,Di,St,Ii,fs,Gi,Oi,Vi,_r,Ui,Wi,Lt,Yr,te,$e,vr,Nt,Bi,Tr,Hi,Zr,F,Dt,Ri,It,Xi,hs,Ki,Ji,Qi,kr,Yi,Zi,Gt,ea,se,we,$r,Ot,el,wr,tl,ta,x,Vt,sl,Ut,rl,us,al,nl,ol,yr,il,ll,Wt,sa;return Pe=new C({}),Ae=new C({}),We=new C({}),Be=new k({props:{name:"class transformers.MT5Config",anchor:"transformers.MT5Config",parameters:[{name:"vocab_size",val:" = 250112"},{name:"d_model",val:" = 512"},{name:"d_kv",val:" = 64"},{name:"d_ff",val:" = 1024"},{name:"num_layers",val:" = 8"},{name:"num_decoder_layers",val:" = None"},{name:"num_heads",val:" = 6"},{name:"relative_attention_num_buckets",val:" = 32"},{name:"dropout_rate",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-06"},{name:"initializer_factor",val:" = 1.0"},{name:"feed_forward_proj",val:" = 'gated-gelu'"},{name:"is_encoder_decoder",val:" = True"},{name:"use_cache",val:" = True"},{name:"tokenizer_class",val:" = 'T5Tokenizer'"},{name:"tie_word_embeddings",val:" = False"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"decoder_start_token_id",val:" = 0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/mt5/configuration_mt5.py#L24",parametersDescription:[{anchor:"transformers.MT5Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250112) &#x2014;
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Model">T5Model</a> or <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a>.`,name:"vocab_size"},{anchor:"transformers.MT5Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.MT5Config.d_kv",description:`<strong>d_kv</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Size of the key, query, value projections per attention head. <code>d_kv</code> has to be equal to <code>d_model // num_heads</code>.`,name:"d_kv"},{anchor:"transformers.MT5Config.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the intermediate feed forward layer in each <code>T5Block</code>.`,name:"d_ff"},{anchor:"transformers.MT5Config.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.MT5Config.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of hidden layers in the Transformer decoder. Will use the same value as <code>num_layers</code> if not
set.`,name:"num_decoder_layers"},{anchor:"transformers.MT5Config.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.MT5Config.relative_attention_num_buckets",description:`<strong>relative_attention_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer.`,name:"relative_attention_num_buckets"},{anchor:"transformers.MT5Config.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.MT5Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MT5Config.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.MT5Config.feed_forward_proj",description:`<strong>feed_forward_proj</strong> (<code>string</code>, <em>optional</em>, defaults to <code>&quot;gated-gelu&quot;</code>) &#x2014;
Type of feed forward layer to be used. Should be one of <code>&quot;relu&quot;</code> or <code>&quot;gated-gelu&quot;</code>.`,name:"feed_forward_proj"},{anchor:"transformers.MT5Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}]}}),Re=new C({}),Xe=new k({props:{name:"class transformers.T5Tokenizer",anchor:"transformers.T5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/tokenization_t5.py#L53",parametersDescription:[{anchor:"transformers.T5Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"}]}}),et=new k({props:{name:"build\\_inputs\\_with\\_special\\_tokens",anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/tokenization_t5.py#L219",parametersDescription:[{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),st=new k({props:{name:"convert\\_tokens\\_to\\_string",anchor:"transformers.T5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/tokenization_t5.py#L280"}}),rt=new k({props:{name:"create\\_token\\_type\\_ids\\_from\\_sequences",anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/tokenization_t5.py#L197",parametersDescription:[{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),at=new k({props:{name:"get\\_special\\_tokens\\_mask",anchor:"transformers.T5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/tokenization_t5.py#L159",parametersDescription:[{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ot=new C({}),it=new k({props:{name:"class transformers.T5TokenizerFast",anchor:"transformers.T5TokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/tokenization_t5_fast.py#L63",parametersDescription:[{anchor:"transformers.T5TokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5TokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"}]}}),pt=new k({props:{name:"build\\_inputs\\_with\\_special\\_tokens",anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/tokenization_t5_fast.py#L163",parametersDescription:[{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new k({props:{name:"create\\_token\\_type\\_ids\\_from\\_sequences",anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/t5/tokenization_t5_fast.py#L189",parametersDescription:[{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ft=new C({}),ht=new k({props:{name:"class transformers.MT5Model",anchor:"transformers.MT5Model",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/mt5/modeling_mt5.py#L28"}}),gt=new Ce({props:{code:`from transformers import MT5Model, T5Tokenizer
model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5Model, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),_t=new C({}),vt=new k({props:{name:"class transformers.MT5ForConditionalGeneration",anchor:"transformers.MT5ForConditionalGeneration",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/mt5/modeling_mt5.py#L60"}}),kt=new Ce({props:{code:`from transformers import MT5ForConditionalGeneration, T5Tokenizer
model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(**inputs,labels=labels["input_ids"])
loss = outputs.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5ForConditionalGeneration, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs,labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),$t=new C({}),wt=new k({props:{name:"class transformers.MT5EncoderModel",anchor:"transformers.MT5EncoderModel",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/mt5/modeling_mt5.py#L90"}}),Mt=new Ce({props:{code:`from transformers import MT5EncoderModel, T5Tokenizer
model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5EncoderModel, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),Et=new C({}),zt=new k({props:{name:"class transformers.TFMT5Model",anchor:"transformers.TFMT5Model",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/mt5/modeling_tf_mt5.py#L28"}}),qt=new Ce({props:{code:`from transformers import TFMT5Model, T5Tokenizer
model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5Model, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),jt=new C({}),Ft=new k({props:{name:"class transformers.TFMT5ForConditionalGeneration",anchor:"transformers.TFMT5ForConditionalGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/mt5/modeling_tf_mt5.py#L51"}}),Ct=new Ce({props:{code:`from transformers import TFMT5ForConditionalGeneration, T5Tokenizer
model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(**inputs,labels=labels["input_ids"])
loss = outputs.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5ForConditionalGeneration, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs,labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),Pt=new C({}),At=new k({props:{name:"class transformers.TFMT5EncoderModel",anchor:"transformers.TFMT5EncoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/mt5/modeling_tf_mt5.py#L75"}}),Lt=new Ce({props:{code:`from transformers import TFMT5EncoderModel, T5Tokenizer
model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5EncoderModel, T5Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),Nt=new C({}),Dt=new k({props:{name:"class transformers.FlaxMT5Model",anchor:"transformers.FlaxMT5Model",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/mt5/modeling_flax_mt5.py#L28"}}),Gt=new Ce({props:{code:`from transformers import FlaxMT5Model, T5Tokenizer

model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),Ot=new C({}),Vt=new k({props:{name:"class transformers.FlaxMT5ForConditionalGeneration",anchor:"transformers.FlaxMT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/mt5/modeling_flax_mt5.py#L54"}}),Wt=new Ce({props:{code:`from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer

model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){N=r("meta"),Bt=d(),P=r("h1"),A=r("a"),ws=r("span"),f(Pe.$$.fragment),Oa=d(),ys=r("span"),Va=o("mT5"),Mr=d(),U=r("h2"),re=r("a"),Ms=r("span"),f(Ae.$$.fragment),Ua=d(),Es=r("span"),Wa=o("Overview"),Er=d(),ae=r("p"),Ba=o("The mT5 model was presented in "),Se=r("a"),Ha=o("mT5: A massively multilingual pre-trained text-to-text transformer"),Ra=o(` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),zr=d(),Ht=r("p"),Xa=o("The abstract from the paper is the following:"),br=d(),Rt=r("p"),zs=r("em"),Ka=o(`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),qr=d(),ne=r("p"),Ja=o("Note: mT5 was only pre-trained on "),Le=r("a"),Qa=o("mC4"),Ya=o(` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),jr=d(),Xt=r("p"),Za=o("Google has released the following variants:"),Fr=d(),y=r("ul"),bs=r("li"),qs=r("p"),Ne=r("a"),en=o("google/mt5-small"),tn=d(),js=r("li"),Fs=r("p"),De=r("a"),sn=o("google/mt5-base"),rn=d(),xs=r("li"),Cs=r("p"),Ie=r("a"),an=o("google/mt5-large"),nn=d(),Ps=r("li"),As=r("p"),Ge=r("a"),on=o("google/mt5-xl"),ln=d(),Ss=r("li"),Kt=r("p"),Oe=r("a"),dn=o("google/mt5-xxl"),pn=o("."),xr=d(),D=r("p"),mn=o("This model was contributed by "),Ve=r("a"),cn=o("patrickvonplaten"),fn=o(`. The original code can be
found `),Ue=r("a"),hn=o("here"),un=o("."),Cr=d(),W=r("h2"),oe=r("a"),Ls=r("span"),f(We.$$.fragment),gn=d(),Ns=r("span"),_n=o("MT5Config"),Pr=d(),S=r("div"),f(Be.$$.fragment),vn=d(),L=r("p"),Tn=o("This is the configuration class to store the configuration of a "),Jt=r("a"),kn=o("MT5Model"),$n=o(` or a
`),Qt=r("a"),wn=o("TFMT5Model"),yn=o(`. It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 `),He=r("a"),Mn=o("google/mt5-small"),En=o(" architecture."),zn=d(),B=r("p"),bn=o("Configuration objects inherit from "),Yt=r("a"),qn=o("PretrainedConfig"),jn=o(` and can be used to control the model
outputs. Read the documentation from `),Zt=r("a"),Fn=o("PretrainedConfig"),xn=o(" for more information."),Ar=d(),H=r("h2"),ie=r("a"),Ds=r("span"),f(Re.$$.fragment),Cn=d(),Is=r("span"),Pn=o("MT5Tokenizer"),Sr=d(),T=r("div"),f(Xe.$$.fragment),An=d(),Ke=r("p"),Sn=o("Construct a T5 tokenizer. Based on "),Je=r("a"),Ln=o("SentencePiece"),Nn=o("."),Dn=d(),Qe=r("p"),In=o("This tokenizer inherits from "),es=r("a"),Gn=o("PreTrainedTokenizer"),On=o(` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Vn=d(),Ye=r("p"),Un=o(`Attributes:
sp`),Ze=r("em"),Wn=o("model ("),Gs=r("code"),Bn=o("SentencePieceProcessor"),Hn=o(`):
The _SentencePiece`),Rn=o(" processor that is used for every conversion (string, tokens and IDs)."),Xn=d(),I=r("div"),f(et.$$.fragment),Kn=d(),Os=r("p"),Jn=o(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Qn=d(),tt=r("ul"),ts=r("li"),Yn=o("single sequence: "),Vs=r("code"),Zn=o("X </s>"),eo=d(),ss=r("li"),to=o("pair of sequences: "),Us=r("code"),so=o("A </s> B </s>"),ro=d(),le=r("div"),f(st.$$.fragment),ao=d(),Ws=r("p"),no=o("Converts a sequence of tokens (string) in a single string."),oo=d(),de=r("div"),f(rt.$$.fragment),io=d(),Bs=r("p"),lo=o(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),po=d(),pe=r("div"),f(at.$$.fragment),mo=d(),nt=r("p"),co=o(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Hs=r("code"),fo=o("prepare_for_model"),ho=o(" method."),Lr=d(),me=r("p"),uo=o("See "),rs=r("a"),go=o("T5Tokenizer"),_o=o(" for all details."),Nr=d(),R=r("h2"),ce=r("a"),Rs=r("span"),f(ot.$$.fragment),vo=d(),Xs=r("span"),To=o("MT5TokenizerFast"),Dr=d(),w=r("div"),f(it.$$.fragment),ko=d(),X=r("p"),$o=o("Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),Ks=r("em"),wo=o("tokenizers"),yo=o(" library). Based on "),lt=r("a"),Mo=o("Unigram"),Eo=o("."),zo=d(),dt=r("p"),bo=o("This tokenizer inherits from "),as=r("a"),qo=o("PreTrainedTokenizerFast"),jo=o(` which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.`),Fo=d(),G=r("div"),f(pt.$$.fragment),xo=d(),Js=r("p"),Co=o(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Po=d(),mt=r("ul"),ns=r("li"),Ao=o("single sequence: "),Qs=r("code"),So=o("X </s>"),Lo=d(),os=r("li"),No=o("pair of sequences: "),Ys=r("code"),Do=o("A </s> B </s>"),Io=d(),fe=r("div"),f(ct.$$.fragment),Go=d(),Zs=r("p"),Oo=o(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Ir=d(),he=r("p"),Vo=o("See "),is=r("a"),Uo=o("T5TokenizerFast"),Wo=o(" for all details."),Gr=d(),K=r("h2"),ue=r("a"),er=r("span"),f(ft.$$.fragment),Bo=d(),tr=r("span"),Ho=o("MT5Model"),Or=d(),M=r("div"),f(ht.$$.fragment),Ro=d(),ut=r("p"),Xo=o("This class overrides "),ls=r("a"),Ko=o("T5Model"),Jo=o(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Qo=d(),sr=r("p"),Yo=o("Examples:"),Zo=d(),f(gt.$$.fragment),Vr=d(),J=r("h2"),ge=r("a"),rr=r("span"),f(_t.$$.fragment),ei=d(),ar=r("span"),ti=o("MT5ForConditionalGeneration"),Ur=d(),E=r("div"),f(vt.$$.fragment),si=d(),Tt=r("p"),ri=o("This class overrides "),ds=r("a"),ai=o("T5ForConditionalGeneration"),ni=o(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),oi=d(),nr=r("p"),ii=o("Examples:"),li=d(),f(kt.$$.fragment),Wr=d(),Q=r("h2"),_e=r("a"),or=r("span"),f($t.$$.fragment),di=d(),ir=r("span"),pi=o("MT5EncoderModel"),Br=d(),z=r("div"),f(wt.$$.fragment),mi=d(),yt=r("p"),ci=o("This class overrides "),ps=r("a"),fi=o("T5EncoderModel"),hi=o(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ui=d(),lr=r("p"),gi=o("Examples:"),_i=d(),f(Mt.$$.fragment),Hr=d(),Y=r("h2"),ve=r("a"),dr=r("span"),f(Et.$$.fragment),vi=d(),pr=r("span"),Ti=o("TFMT5Model"),Rr=d(),b=r("div"),f(zt.$$.fragment),ki=d(),bt=r("p"),$i=o("This class overrides "),ms=r("a"),wi=o("TFT5Model"),yi=o(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Mi=d(),mr=r("p"),Ei=o("Examples:"),zi=d(),f(qt.$$.fragment),Xr=d(),Z=r("h2"),Te=r("a"),cr=r("span"),f(jt.$$.fragment),bi=d(),fr=r("span"),qi=o("TFMT5ForConditionalGeneration"),Kr=d(),q=r("div"),f(Ft.$$.fragment),ji=d(),xt=r("p"),Fi=o("This class overrides "),cs=r("a"),xi=o("TFT5ForConditionalGeneration"),Ci=o(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),Pi=d(),hr=r("p"),Ai=o("Examples:"),Si=d(),f(Ct.$$.fragment),Jr=d(),ee=r("h2"),ke=r("a"),ur=r("span"),f(Pt.$$.fragment),Li=d(),gr=r("span"),Ni=o("TFMT5EncoderModel"),Qr=d(),j=r("div"),f(At.$$.fragment),Di=d(),St=r("p"),Ii=o("This class overrides "),fs=r("a"),Gi=o("TFT5EncoderModel"),Oi=o(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Vi=d(),_r=r("p"),Ui=o("Examples:"),Wi=d(),f(Lt.$$.fragment),Yr=d(),te=r("h2"),$e=r("a"),vr=r("span"),f(Nt.$$.fragment),Bi=d(),Tr=r("span"),Hi=o("FlaxMT5Model"),Zr=d(),F=r("div"),f(Dt.$$.fragment),Ri=d(),It=r("p"),Xi=o("This class overrides "),hs=r("a"),Ki=o("FlaxT5Model"),Ji=o(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Qi=d(),kr=r("p"),Yi=o("Examples:"),Zi=d(),f(Gt.$$.fragment),ea=d(),se=r("h2"),we=r("a"),$r=r("span"),f(Ot.$$.fragment),el=d(),wr=r("span"),tl=o("FlaxMT5ForConditionalGeneration"),ta=d(),x=r("div"),f(Vt.$$.fragment),sl=d(),Ut=r("p"),rl=o("This class overrides "),us=r("a"),al=o("FlaxT5ForConditionalGeneration"),nl=o(`. Please check the superclass for the
appropriate documentation alongside usage examples.`),ol=d(),yr=r("p"),il=o("Examples:"),ll=d(),f(Wt.$$.fragment),this.h()},l(t){const m=dp('[data-svelte="svelte-1phssyn"]',document.head);N=a(m,"META",{name:!0,content:!0}),m.forEach(s),Bt=p(t),P=a(t,"H1",{class:!0});var ra=n(P);A=a(ra,"A",{id:!0,class:!0,href:!0});var hl=n(A);ws=a(hl,"SPAN",{});var ul=n(ws);h(Pe.$$.fragment,ul),ul.forEach(s),hl.forEach(s),Oa=p(ra),ys=a(ra,"SPAN",{});var gl=n(ys);Va=i(gl,"mT5"),gl.forEach(s),ra.forEach(s),Mr=p(t),U=a(t,"H2",{class:!0});var aa=n(U);re=a(aa,"A",{id:!0,class:!0,href:!0});var _l=n(re);Ms=a(_l,"SPAN",{});var vl=n(Ms);h(Ae.$$.fragment,vl),vl.forEach(s),_l.forEach(s),Ua=p(aa),Es=a(aa,"SPAN",{});var Tl=n(Es);Wa=i(Tl,"Overview"),Tl.forEach(s),aa.forEach(s),Er=p(t),ae=a(t,"P",{});var na=n(ae);Ba=i(na,"The mT5 model was presented in "),Se=a(na,"A",{href:!0,rel:!0});var kl=n(Se);Ha=i(kl,"mT5: A massively multilingual pre-trained text-to-text transformer"),kl.forEach(s),Ra=i(na,` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),na.forEach(s),zr=p(t),Ht=a(t,"P",{});var $l=n(Ht);Xa=i($l,"The abstract from the paper is the following:"),$l.forEach(s),br=p(t),Rt=a(t,"P",{});var wl=n(Rt);zs=a(wl,"EM",{});var yl=n(zs);Ka=i(yl,`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),yl.forEach(s),wl.forEach(s),qr=p(t),ne=a(t,"P",{});var oa=n(ne);Ja=i(oa,"Note: mT5 was only pre-trained on "),Le=a(oa,"A",{href:!0,rel:!0});var Ml=n(Le);Qa=i(Ml,"mC4"),Ml.forEach(s),Ya=i(oa,` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),oa.forEach(s),jr=p(t),Xt=a(t,"P",{});var El=n(Xt);Za=i(El,"Google has released the following variants:"),El.forEach(s),Fr=p(t),y=a(t,"UL",{});var O=n(y);bs=a(O,"LI",{});var zl=n(bs);qs=a(zl,"P",{});var bl=n(qs);Ne=a(bl,"A",{href:!0,rel:!0});var ql=n(Ne);en=i(ql,"google/mt5-small"),ql.forEach(s),bl.forEach(s),zl.forEach(s),tn=p(O),js=a(O,"LI",{});var jl=n(js);Fs=a(jl,"P",{});var Fl=n(Fs);De=a(Fl,"A",{href:!0,rel:!0});var xl=n(De);sn=i(xl,"google/mt5-base"),xl.forEach(s),Fl.forEach(s),jl.forEach(s),rn=p(O),xs=a(O,"LI",{});var Cl=n(xs);Cs=a(Cl,"P",{});var Pl=n(Cs);Ie=a(Pl,"A",{href:!0,rel:!0});var Al=n(Ie);an=i(Al,"google/mt5-large"),Al.forEach(s),Pl.forEach(s),Cl.forEach(s),nn=p(O),Ps=a(O,"LI",{});var Sl=n(Ps);As=a(Sl,"P",{});var Ll=n(As);Ge=a(Ll,"A",{href:!0,rel:!0});var Nl=n(Ge);on=i(Nl,"google/mt5-xl"),Nl.forEach(s),Ll.forEach(s),Sl.forEach(s),ln=p(O),Ss=a(O,"LI",{});var Dl=n(Ss);Kt=a(Dl,"P",{});var dl=n(Kt);Oe=a(dl,"A",{href:!0,rel:!0});var Il=n(Oe);dn=i(Il,"google/mt5-xxl"),Il.forEach(s),pn=i(dl,"."),dl.forEach(s),Dl.forEach(s),O.forEach(s),xr=p(t),D=a(t,"P",{});var gs=n(D);mn=i(gs,"This model was contributed by "),Ve=a(gs,"A",{href:!0,rel:!0});var Gl=n(Ve);cn=i(Gl,"patrickvonplaten"),Gl.forEach(s),fn=i(gs,`. The original code can be
found `),Ue=a(gs,"A",{href:!0,rel:!0});var Ol=n(Ue);hn=i(Ol,"here"),Ol.forEach(s),un=i(gs,"."),gs.forEach(s),Cr=p(t),W=a(t,"H2",{class:!0});var ia=n(W);oe=a(ia,"A",{id:!0,class:!0,href:!0});var Vl=n(oe);Ls=a(Vl,"SPAN",{});var Ul=n(Ls);h(We.$$.fragment,Ul),Ul.forEach(s),Vl.forEach(s),gn=p(ia),Ns=a(ia,"SPAN",{});var Wl=n(Ns);_n=i(Wl,"MT5Config"),Wl.forEach(s),ia.forEach(s),Pr=p(t),S=a(t,"DIV",{class:!0});var _s=n(S);h(Be.$$.fragment,_s),vn=p(_s),L=a(_s,"P",{});var ye=n(L);Tn=i(ye,"This is the configuration class to store the configuration of a "),Jt=a(ye,"A",{href:!0});var Bl=n(Jt);kn=i(Bl,"MT5Model"),Bl.forEach(s),$n=i(ye,` or a
`),Qt=a(ye,"A",{href:!0});var Hl=n(Qt);wn=i(Hl,"TFMT5Model"),Hl.forEach(s),yn=i(ye,`. It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 `),He=a(ye,"A",{href:!0,rel:!0});var Rl=n(He);Mn=i(Rl,"google/mt5-small"),Rl.forEach(s),En=i(ye," architecture."),ye.forEach(s),zn=p(_s),B=a(_s,"P",{});var vs=n(B);bn=i(vs,"Configuration objects inherit from "),Yt=a(vs,"A",{href:!0});var Xl=n(Yt);qn=i(Xl,"PretrainedConfig"),Xl.forEach(s),jn=i(vs,` and can be used to control the model
outputs. Read the documentation from `),Zt=a(vs,"A",{href:!0});var Kl=n(Zt);Fn=i(Kl,"PretrainedConfig"),Kl.forEach(s),xn=i(vs," for more information."),vs.forEach(s),_s.forEach(s),Ar=p(t),H=a(t,"H2",{class:!0});var la=n(H);ie=a(la,"A",{id:!0,class:!0,href:!0});var Jl=n(ie);Ds=a(Jl,"SPAN",{});var Ql=n(Ds);h(Re.$$.fragment,Ql),Ql.forEach(s),Jl.forEach(s),Cn=p(la),Is=a(la,"SPAN",{});var Yl=n(Is);Pn=i(Yl,"MT5Tokenizer"),Yl.forEach(s),la.forEach(s),Sr=p(t),T=a(t,"DIV",{class:!0});var $=n(T);h(Xe.$$.fragment,$),An=p($),Ke=a($,"P",{});var da=n(Ke);Sn=i(da,"Construct a T5 tokenizer. Based on "),Je=a(da,"A",{href:!0,rel:!0});var Zl=n(Je);Ln=i(Zl,"SentencePiece"),Zl.forEach(s),Nn=i(da,"."),da.forEach(s),Dn=p($),Qe=a($,"P",{});var pa=n(Qe);In=i(pa,"This tokenizer inherits from "),es=a(pa,"A",{href:!0});var ed=n(es);Gn=i(ed,"PreTrainedTokenizer"),ed.forEach(s),On=i(pa,` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),pa.forEach(s),Vn=p($),Ye=a($,"P",{});var ma=n(Ye);Un=i(ma,`Attributes:
sp`),Ze=a(ma,"EM",{});var ca=n(Ze);Wn=i(ca,"model ("),Gs=a(ca,"CODE",{});var td=n(Gs);Bn=i(td,"SentencePieceProcessor"),td.forEach(s),Hn=i(ca,`):
The _SentencePiece`),ca.forEach(s),Rn=i(ma," processor that is used for every conversion (string, tokens and IDs)."),ma.forEach(s),Xn=p($),I=a($,"DIV",{class:!0});var Ts=n(I);h(et.$$.fragment,Ts),Kn=p(Ts),Os=a(Ts,"P",{});var sd=n(Os);Jn=i(sd,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),sd.forEach(s),Qn=p(Ts),tt=a(Ts,"UL",{});var fa=n(tt);ts=a(fa,"LI",{});var pl=n(ts);Yn=i(pl,"single sequence: "),Vs=a(pl,"CODE",{});var rd=n(Vs);Zn=i(rd,"X </s>"),rd.forEach(s),pl.forEach(s),eo=p(fa),ss=a(fa,"LI",{});var ml=n(ss);to=i(ml,"pair of sequences: "),Us=a(ml,"CODE",{});var ad=n(Us);so=i(ad,"A </s> B </s>"),ad.forEach(s),ml.forEach(s),fa.forEach(s),Ts.forEach(s),ro=p($),le=a($,"DIV",{class:!0});var ha=n(le);h(st.$$.fragment,ha),ao=p(ha),Ws=a(ha,"P",{});var nd=n(Ws);no=i(nd,"Converts a sequence of tokens (string) in a single string."),nd.forEach(s),ha.forEach(s),oo=p($),de=a($,"DIV",{class:!0});var ua=n(de);h(rt.$$.fragment,ua),io=p(ua),Bs=a(ua,"P",{});var od=n(Bs);lo=i(od,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),od.forEach(s),ua.forEach(s),po=p($),pe=a($,"DIV",{class:!0});var ga=n(pe);h(at.$$.fragment,ga),mo=p(ga),nt=a(ga,"P",{});var _a=n(nt);co=i(_a,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Hs=a(_a,"CODE",{});var id=n(Hs);fo=i(id,"prepare_for_model"),id.forEach(s),ho=i(_a," method."),_a.forEach(s),ga.forEach(s),$.forEach(s),Lr=p(t),me=a(t,"P",{});var va=n(me);uo=i(va,"See "),rs=a(va,"A",{href:!0});var ld=n(rs);go=i(ld,"T5Tokenizer"),ld.forEach(s),_o=i(va," for all details."),va.forEach(s),Nr=p(t),R=a(t,"H2",{class:!0});var Ta=n(R);ce=a(Ta,"A",{id:!0,class:!0,href:!0});var dd=n(ce);Rs=a(dd,"SPAN",{});var pd=n(Rs);h(ot.$$.fragment,pd),pd.forEach(s),dd.forEach(s),vo=p(Ta),Xs=a(Ta,"SPAN",{});var md=n(Xs);To=i(md,"MT5TokenizerFast"),md.forEach(s),Ta.forEach(s),Dr=p(t),w=a(t,"DIV",{class:!0});var V=n(w);h(it.$$.fragment,V),ko=p(V),X=a(V,"P",{});var ks=n(X);$o=i(ks,"Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),Ks=a(ks,"EM",{});var cd=n(Ks);wo=i(cd,"tokenizers"),cd.forEach(s),yo=i(ks," library). Based on "),lt=a(ks,"A",{href:!0,rel:!0});var fd=n(lt);Mo=i(fd,"Unigram"),fd.forEach(s),Eo=i(ks,"."),ks.forEach(s),zo=p(V),dt=a(V,"P",{});var ka=n(dt);bo=i(ka,"This tokenizer inherits from "),as=a(ka,"A",{href:!0});var hd=n(as);qo=i(hd,"PreTrainedTokenizerFast"),hd.forEach(s),jo=i(ka,` which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.`),ka.forEach(s),Fo=p(V),G=a(V,"DIV",{class:!0});var $s=n(G);h(pt.$$.fragment,$s),xo=p($s),Js=a($s,"P",{});var ud=n(Js);Co=i(ud,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),ud.forEach(s),Po=p($s),mt=a($s,"UL",{});var $a=n(mt);ns=a($a,"LI",{});var cl=n(ns);Ao=i(cl,"single sequence: "),Qs=a(cl,"CODE",{});var gd=n(Qs);So=i(gd,"X </s>"),gd.forEach(s),cl.forEach(s),Lo=p($a),os=a($a,"LI",{});var fl=n(os);No=i(fl,"pair of sequences: "),Ys=a(fl,"CODE",{});var _d=n(Ys);Do=i(_d,"A </s> B </s>"),_d.forEach(s),fl.forEach(s),$a.forEach(s),$s.forEach(s),Io=p(V),fe=a(V,"DIV",{class:!0});var wa=n(fe);h(ct.$$.fragment,wa),Go=p(wa),Zs=a(wa,"P",{});var vd=n(Zs);Oo=i(vd,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),vd.forEach(s),wa.forEach(s),V.forEach(s),Ir=p(t),he=a(t,"P",{});var ya=n(he);Vo=i(ya,"See "),is=a(ya,"A",{href:!0});var Td=n(is);Uo=i(Td,"T5TokenizerFast"),Td.forEach(s),Wo=i(ya," for all details."),ya.forEach(s),Gr=p(t),K=a(t,"H2",{class:!0});var Ma=n(K);ue=a(Ma,"A",{id:!0,class:!0,href:!0});var kd=n(ue);er=a(kd,"SPAN",{});var $d=n(er);h(ft.$$.fragment,$d),$d.forEach(s),kd.forEach(s),Bo=p(Ma),tr=a(Ma,"SPAN",{});var wd=n(tr);Ho=i(wd,"MT5Model"),wd.forEach(s),Ma.forEach(s),Or=p(t),M=a(t,"DIV",{class:!0});var Me=n(M);h(ht.$$.fragment,Me),Ro=p(Me),ut=a(Me,"P",{});var Ea=n(ut);Xo=i(Ea,"This class overrides "),ls=a(Ea,"A",{href:!0});var yd=n(ls);Ko=i(yd,"T5Model"),yd.forEach(s),Jo=i(Ea,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Ea.forEach(s),Qo=p(Me),sr=a(Me,"P",{});var Md=n(sr);Yo=i(Md,"Examples:"),Md.forEach(s),Zo=p(Me),h(gt.$$.fragment,Me),Me.forEach(s),Vr=p(t),J=a(t,"H2",{class:!0});var za=n(J);ge=a(za,"A",{id:!0,class:!0,href:!0});var Ed=n(ge);rr=a(Ed,"SPAN",{});var zd=n(rr);h(_t.$$.fragment,zd),zd.forEach(s),Ed.forEach(s),ei=p(za),ar=a(za,"SPAN",{});var bd=n(ar);ti=i(bd,"MT5ForConditionalGeneration"),bd.forEach(s),za.forEach(s),Ur=p(t),E=a(t,"DIV",{class:!0});var Ee=n(E);h(vt.$$.fragment,Ee),si=p(Ee),Tt=a(Ee,"P",{});var ba=n(Tt);ri=i(ba,"This class overrides "),ds=a(ba,"A",{href:!0});var qd=n(ds);ai=i(qd,"T5ForConditionalGeneration"),qd.forEach(s),ni=i(ba,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),ba.forEach(s),oi=p(Ee),nr=a(Ee,"P",{});var jd=n(nr);ii=i(jd,"Examples:"),jd.forEach(s),li=p(Ee),h(kt.$$.fragment,Ee),Ee.forEach(s),Wr=p(t),Q=a(t,"H2",{class:!0});var qa=n(Q);_e=a(qa,"A",{id:!0,class:!0,href:!0});var Fd=n(_e);or=a(Fd,"SPAN",{});var xd=n(or);h($t.$$.fragment,xd),xd.forEach(s),Fd.forEach(s),di=p(qa),ir=a(qa,"SPAN",{});var Cd=n(ir);pi=i(Cd,"MT5EncoderModel"),Cd.forEach(s),qa.forEach(s),Br=p(t),z=a(t,"DIV",{class:!0});var ze=n(z);h(wt.$$.fragment,ze),mi=p(ze),yt=a(ze,"P",{});var ja=n(yt);ci=i(ja,"This class overrides "),ps=a(ja,"A",{href:!0});var Pd=n(ps);fi=i(Pd,"T5EncoderModel"),Pd.forEach(s),hi=i(ja,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ja.forEach(s),ui=p(ze),lr=a(ze,"P",{});var Ad=n(lr);gi=i(Ad,"Examples:"),Ad.forEach(s),_i=p(ze),h(Mt.$$.fragment,ze),ze.forEach(s),Hr=p(t),Y=a(t,"H2",{class:!0});var Fa=n(Y);ve=a(Fa,"A",{id:!0,class:!0,href:!0});var Sd=n(ve);dr=a(Sd,"SPAN",{});var Ld=n(dr);h(Et.$$.fragment,Ld),Ld.forEach(s),Sd.forEach(s),vi=p(Fa),pr=a(Fa,"SPAN",{});var Nd=n(pr);Ti=i(Nd,"TFMT5Model"),Nd.forEach(s),Fa.forEach(s),Rr=p(t),b=a(t,"DIV",{class:!0});var be=n(b);h(zt.$$.fragment,be),ki=p(be),bt=a(be,"P",{});var xa=n(bt);$i=i(xa,"This class overrides "),ms=a(xa,"A",{href:!0});var Dd=n(ms);wi=i(Dd,"TFT5Model"),Dd.forEach(s),yi=i(xa,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),xa.forEach(s),Mi=p(be),mr=a(be,"P",{});var Id=n(mr);Ei=i(Id,"Examples:"),Id.forEach(s),zi=p(be),h(qt.$$.fragment,be),be.forEach(s),Xr=p(t),Z=a(t,"H2",{class:!0});var Ca=n(Z);Te=a(Ca,"A",{id:!0,class:!0,href:!0});var Gd=n(Te);cr=a(Gd,"SPAN",{});var Od=n(cr);h(jt.$$.fragment,Od),Od.forEach(s),Gd.forEach(s),bi=p(Ca),fr=a(Ca,"SPAN",{});var Vd=n(fr);qi=i(Vd,"TFMT5ForConditionalGeneration"),Vd.forEach(s),Ca.forEach(s),Kr=p(t),q=a(t,"DIV",{class:!0});var qe=n(q);h(Ft.$$.fragment,qe),ji=p(qe),xt=a(qe,"P",{});var Pa=n(xt);Fi=i(Pa,"This class overrides "),cs=a(Pa,"A",{href:!0});var Ud=n(cs);xi=i(Ud,"TFT5ForConditionalGeneration"),Ud.forEach(s),Ci=i(Pa,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),Pa.forEach(s),Pi=p(qe),hr=a(qe,"P",{});var Wd=n(hr);Ai=i(Wd,"Examples:"),Wd.forEach(s),Si=p(qe),h(Ct.$$.fragment,qe),qe.forEach(s),Jr=p(t),ee=a(t,"H2",{class:!0});var Aa=n(ee);ke=a(Aa,"A",{id:!0,class:!0,href:!0});var Bd=n(ke);ur=a(Bd,"SPAN",{});var Hd=n(ur);h(Pt.$$.fragment,Hd),Hd.forEach(s),Bd.forEach(s),Li=p(Aa),gr=a(Aa,"SPAN",{});var Rd=n(gr);Ni=i(Rd,"TFMT5EncoderModel"),Rd.forEach(s),Aa.forEach(s),Qr=p(t),j=a(t,"DIV",{class:!0});var je=n(j);h(At.$$.fragment,je),Di=p(je),St=a(je,"P",{});var Sa=n(St);Ii=i(Sa,"This class overrides "),fs=a(Sa,"A",{href:!0});var Xd=n(fs);Gi=i(Xd,"TFT5EncoderModel"),Xd.forEach(s),Oi=i(Sa,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Sa.forEach(s),Vi=p(je),_r=a(je,"P",{});var Kd=n(_r);Ui=i(Kd,"Examples:"),Kd.forEach(s),Wi=p(je),h(Lt.$$.fragment,je),je.forEach(s),Yr=p(t),te=a(t,"H2",{class:!0});var La=n(te);$e=a(La,"A",{id:!0,class:!0,href:!0});var Jd=n($e);vr=a(Jd,"SPAN",{});var Qd=n(vr);h(Nt.$$.fragment,Qd),Qd.forEach(s),Jd.forEach(s),Bi=p(La),Tr=a(La,"SPAN",{});var Yd=n(Tr);Hi=i(Yd,"FlaxMT5Model"),Yd.forEach(s),La.forEach(s),Zr=p(t),F=a(t,"DIV",{class:!0});var Fe=n(F);h(Dt.$$.fragment,Fe),Ri=p(Fe),It=a(Fe,"P",{});var Na=n(It);Xi=i(Na,"This class overrides "),hs=a(Na,"A",{href:!0});var Zd=n(hs);Ki=i(Zd,"FlaxT5Model"),Zd.forEach(s),Ji=i(Na,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Na.forEach(s),Qi=p(Fe),kr=a(Fe,"P",{});var ep=n(kr);Yi=i(ep,"Examples:"),ep.forEach(s),Zi=p(Fe),h(Gt.$$.fragment,Fe),Fe.forEach(s),ea=p(t),se=a(t,"H2",{class:!0});var Da=n(se);we=a(Da,"A",{id:!0,class:!0,href:!0});var tp=n(we);$r=a(tp,"SPAN",{});var sp=n($r);h(Ot.$$.fragment,sp),sp.forEach(s),tp.forEach(s),el=p(Da),wr=a(Da,"SPAN",{});var rp=n(wr);tl=i(rp,"FlaxMT5ForConditionalGeneration"),rp.forEach(s),Da.forEach(s),ta=p(t),x=a(t,"DIV",{class:!0});var xe=n(x);h(Vt.$$.fragment,xe),sl=p(xe),Ut=a(xe,"P",{});var Ia=n(Ut);rl=i(Ia,"This class overrides "),us=a(Ia,"A",{href:!0});var ap=n(us);al=i(ap,"FlaxT5ForConditionalGeneration"),ap.forEach(s),nl=i(Ia,`. Please check the superclass for the
appropriate documentation alongside usage examples.`),Ia.forEach(s),ol=p(xe),yr=a(xe,"P",{});var np=n(yr);il=i(np,"Examples:"),np.forEach(s),ll=p(xe),h(Wt.$$.fragment,xe),xe.forEach(s),this.h()},h(){l(N,"name","hf:doc:metadata"),l(N,"content",JSON.stringify(cp)),l(A,"id","mt5"),l(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(A,"href","#mt5"),l(P,"class","relative group"),l(re,"id","overview"),l(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(re,"href","#overview"),l(U,"class","relative group"),l(Se,"href","https://arxiv.org/abs/2010.11934"),l(Se,"rel","nofollow"),l(Le,"href","https://huggingface.co/datasets/mc4"),l(Le,"rel","nofollow"),l(Ne,"href","https://huggingface.co/google/mt5-small"),l(Ne,"rel","nofollow"),l(De,"href","https://huggingface.co/google/mt5-base"),l(De,"rel","nofollow"),l(Ie,"href","https://huggingface.co/google/mt5-large"),l(Ie,"rel","nofollow"),l(Ge,"href","https://huggingface.co/google/mt5-xl"),l(Ge,"rel","nofollow"),l(Oe,"href","https://huggingface.co/google/mt5-xxl"),l(Oe,"rel","nofollow"),l(Ve,"href","https://huggingface.co/patrickvonplaten"),l(Ve,"rel","nofollow"),l(Ue,"href","https://github.com/google-research/multilingual-t5"),l(Ue,"rel","nofollow"),l(oe,"id","transformers.MT5Config"),l(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(oe,"href","#transformers.MT5Config"),l(W,"class","relative group"),l(Jt,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model"),l(Qt,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model"),l(He,"href","https://huggingface.co/google/mt5-small"),l(He,"rel","nofollow"),l(Yt,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),l(Zt,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),l(S,"class","docstring"),l(ie,"id","transformers.T5Tokenizer"),l(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ie,"href","#transformers.T5Tokenizer"),l(H,"class","relative group"),l(Je,"href","https://github.com/google/sentencepiece"),l(Je,"rel","nofollow"),l(es,"href","/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),l(I,"class","docstring"),l(le,"class","docstring"),l(de,"class","docstring"),l(pe,"class","docstring"),l(T,"class","docstring"),l(rs,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),l(ce,"id","transformers.T5TokenizerFast"),l(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ce,"href","#transformers.T5TokenizerFast"),l(R,"class","relative group"),l(lt,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models"),l(lt,"rel","nofollow"),l(as,"href","/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),l(G,"class","docstring"),l(fe,"class","docstring"),l(w,"class","docstring"),l(is,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),l(ue,"id","transformers.MT5Model"),l(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ue,"href","#transformers.MT5Model"),l(K,"class","relative group"),l(ls,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Model"),l(M,"class","docstring"),l(ge,"id","transformers.MT5ForConditionalGeneration"),l(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ge,"href","#transformers.MT5ForConditionalGeneration"),l(J,"class","relative group"),l(ds,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),l(E,"class","docstring"),l(_e,"id","transformers.MT5EncoderModel"),l(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_e,"href","#transformers.MT5EncoderModel"),l(Q,"class","relative group"),l(ps,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5EncoderModel"),l(z,"class","docstring"),l(ve,"id","transformers.TFMT5Model"),l(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ve,"href","#transformers.TFMT5Model"),l(Y,"class","relative group"),l(ms,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model"),l(b,"class","docstring"),l(Te,"id","transformers.TFMT5ForConditionalGeneration"),l(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Te,"href","#transformers.TFMT5ForConditionalGeneration"),l(Z,"class","relative group"),l(cs,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),l(q,"class","docstring"),l(ke,"id","transformers.TFMT5EncoderModel"),l(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ke,"href","#transformers.TFMT5EncoderModel"),l(ee,"class","relative group"),l(fs,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5EncoderModel"),l(j,"class","docstring"),l($e,"id","transformers.FlaxMT5Model"),l($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l($e,"href","#transformers.FlaxMT5Model"),l(te,"class","relative group"),l(hs,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model"),l(F,"class","docstring"),l(we,"id","transformers.FlaxMT5ForConditionalGeneration"),l(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(we,"href","#transformers.FlaxMT5ForConditionalGeneration"),l(se,"class","relative group"),l(us,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),l(x,"class","docstring")},m(t,m){e(document.head,N),c(t,Bt,m),c(t,P,m),e(P,A),e(A,ws),u(Pe,ws,null),e(P,Oa),e(P,ys),e(ys,Va),c(t,Mr,m),c(t,U,m),e(U,re),e(re,Ms),u(Ae,Ms,null),e(U,Ua),e(U,Es),e(Es,Wa),c(t,Er,m),c(t,ae,m),e(ae,Ba),e(ae,Se),e(Se,Ha),e(ae,Ra),c(t,zr,m),c(t,Ht,m),e(Ht,Xa),c(t,br,m),c(t,Rt,m),e(Rt,zs),e(zs,Ka),c(t,qr,m),c(t,ne,m),e(ne,Ja),e(ne,Le),e(Le,Qa),e(ne,Ya),c(t,jr,m),c(t,Xt,m),e(Xt,Za),c(t,Fr,m),c(t,y,m),e(y,bs),e(bs,qs),e(qs,Ne),e(Ne,en),e(y,tn),e(y,js),e(js,Fs),e(Fs,De),e(De,sn),e(y,rn),e(y,xs),e(xs,Cs),e(Cs,Ie),e(Ie,an),e(y,nn),e(y,Ps),e(Ps,As),e(As,Ge),e(Ge,on),e(y,ln),e(y,Ss),e(Ss,Kt),e(Kt,Oe),e(Oe,dn),e(Kt,pn),c(t,xr,m),c(t,D,m),e(D,mn),e(D,Ve),e(Ve,cn),e(D,fn),e(D,Ue),e(Ue,hn),e(D,un),c(t,Cr,m),c(t,W,m),e(W,oe),e(oe,Ls),u(We,Ls,null),e(W,gn),e(W,Ns),e(Ns,_n),c(t,Pr,m),c(t,S,m),u(Be,S,null),e(S,vn),e(S,L),e(L,Tn),e(L,Jt),e(Jt,kn),e(L,$n),e(L,Qt),e(Qt,wn),e(L,yn),e(L,He),e(He,Mn),e(L,En),e(S,zn),e(S,B),e(B,bn),e(B,Yt),e(Yt,qn),e(B,jn),e(B,Zt),e(Zt,Fn),e(B,xn),c(t,Ar,m),c(t,H,m),e(H,ie),e(ie,Ds),u(Re,Ds,null),e(H,Cn),e(H,Is),e(Is,Pn),c(t,Sr,m),c(t,T,m),u(Xe,T,null),e(T,An),e(T,Ke),e(Ke,Sn),e(Ke,Je),e(Je,Ln),e(Ke,Nn),e(T,Dn),e(T,Qe),e(Qe,In),e(Qe,es),e(es,Gn),e(Qe,On),e(T,Vn),e(T,Ye),e(Ye,Un),e(Ye,Ze),e(Ze,Wn),e(Ze,Gs),e(Gs,Bn),e(Ze,Hn),e(Ye,Rn),e(T,Xn),e(T,I),u(et,I,null),e(I,Kn),e(I,Os),e(Os,Jn),e(I,Qn),e(I,tt),e(tt,ts),e(ts,Yn),e(ts,Vs),e(Vs,Zn),e(tt,eo),e(tt,ss),e(ss,to),e(ss,Us),e(Us,so),e(T,ro),e(T,le),u(st,le,null),e(le,ao),e(le,Ws),e(Ws,no),e(T,oo),e(T,de),u(rt,de,null),e(de,io),e(de,Bs),e(Bs,lo),e(T,po),e(T,pe),u(at,pe,null),e(pe,mo),e(pe,nt),e(nt,co),e(nt,Hs),e(Hs,fo),e(nt,ho),c(t,Lr,m),c(t,me,m),e(me,uo),e(me,rs),e(rs,go),e(me,_o),c(t,Nr,m),c(t,R,m),e(R,ce),e(ce,Rs),u(ot,Rs,null),e(R,vo),e(R,Xs),e(Xs,To),c(t,Dr,m),c(t,w,m),u(it,w,null),e(w,ko),e(w,X),e(X,$o),e(X,Ks),e(Ks,wo),e(X,yo),e(X,lt),e(lt,Mo),e(X,Eo),e(w,zo),e(w,dt),e(dt,bo),e(dt,as),e(as,qo),e(dt,jo),e(w,Fo),e(w,G),u(pt,G,null),e(G,xo),e(G,Js),e(Js,Co),e(G,Po),e(G,mt),e(mt,ns),e(ns,Ao),e(ns,Qs),e(Qs,So),e(mt,Lo),e(mt,os),e(os,No),e(os,Ys),e(Ys,Do),e(w,Io),e(w,fe),u(ct,fe,null),e(fe,Go),e(fe,Zs),e(Zs,Oo),c(t,Ir,m),c(t,he,m),e(he,Vo),e(he,is),e(is,Uo),e(he,Wo),c(t,Gr,m),c(t,K,m),e(K,ue),e(ue,er),u(ft,er,null),e(K,Bo),e(K,tr),e(tr,Ho),c(t,Or,m),c(t,M,m),u(ht,M,null),e(M,Ro),e(M,ut),e(ut,Xo),e(ut,ls),e(ls,Ko),e(ut,Jo),e(M,Qo),e(M,sr),e(sr,Yo),e(M,Zo),u(gt,M,null),c(t,Vr,m),c(t,J,m),e(J,ge),e(ge,rr),u(_t,rr,null),e(J,ei),e(J,ar),e(ar,ti),c(t,Ur,m),c(t,E,m),u(vt,E,null),e(E,si),e(E,Tt),e(Tt,ri),e(Tt,ds),e(ds,ai),e(Tt,ni),e(E,oi),e(E,nr),e(nr,ii),e(E,li),u(kt,E,null),c(t,Wr,m),c(t,Q,m),e(Q,_e),e(_e,or),u($t,or,null),e(Q,di),e(Q,ir),e(ir,pi),c(t,Br,m),c(t,z,m),u(wt,z,null),e(z,mi),e(z,yt),e(yt,ci),e(yt,ps),e(ps,fi),e(yt,hi),e(z,ui),e(z,lr),e(lr,gi),e(z,_i),u(Mt,z,null),c(t,Hr,m),c(t,Y,m),e(Y,ve),e(ve,dr),u(Et,dr,null),e(Y,vi),e(Y,pr),e(pr,Ti),c(t,Rr,m),c(t,b,m),u(zt,b,null),e(b,ki),e(b,bt),e(bt,$i),e(bt,ms),e(ms,wi),e(bt,yi),e(b,Mi),e(b,mr),e(mr,Ei),e(b,zi),u(qt,b,null),c(t,Xr,m),c(t,Z,m),e(Z,Te),e(Te,cr),u(jt,cr,null),e(Z,bi),e(Z,fr),e(fr,qi),c(t,Kr,m),c(t,q,m),u(Ft,q,null),e(q,ji),e(q,xt),e(xt,Fi),e(xt,cs),e(cs,xi),e(xt,Ci),e(q,Pi),e(q,hr),e(hr,Ai),e(q,Si),u(Ct,q,null),c(t,Jr,m),c(t,ee,m),e(ee,ke),e(ke,ur),u(Pt,ur,null),e(ee,Li),e(ee,gr),e(gr,Ni),c(t,Qr,m),c(t,j,m),u(At,j,null),e(j,Di),e(j,St),e(St,Ii),e(St,fs),e(fs,Gi),e(St,Oi),e(j,Vi),e(j,_r),e(_r,Ui),e(j,Wi),u(Lt,j,null),c(t,Yr,m),c(t,te,m),e(te,$e),e($e,vr),u(Nt,vr,null),e(te,Bi),e(te,Tr),e(Tr,Hi),c(t,Zr,m),c(t,F,m),u(Dt,F,null),e(F,Ri),e(F,It),e(It,Xi),e(It,hs),e(hs,Ki),e(It,Ji),e(F,Qi),e(F,kr),e(kr,Yi),e(F,Zi),u(Gt,F,null),c(t,ea,m),c(t,se,m),e(se,we),e(we,$r),u(Ot,$r,null),e(se,el),e(se,wr),e(wr,tl),c(t,ta,m),c(t,x,m),u(Vt,x,null),e(x,sl),e(x,Ut),e(Ut,rl),e(Ut,us),e(us,al),e(Ut,nl),e(x,ol),e(x,yr),e(yr,il),e(x,ll),u(Wt,x,null),sa=!0},p:pp,i(t){sa||(g(Pe.$$.fragment,t),g(Ae.$$.fragment,t),g(We.$$.fragment,t),g(Be.$$.fragment,t),g(Re.$$.fragment,t),g(Xe.$$.fragment,t),g(et.$$.fragment,t),g(st.$$.fragment,t),g(rt.$$.fragment,t),g(at.$$.fragment,t),g(ot.$$.fragment,t),g(it.$$.fragment,t),g(pt.$$.fragment,t),g(ct.$$.fragment,t),g(ft.$$.fragment,t),g(ht.$$.fragment,t),g(gt.$$.fragment,t),g(_t.$$.fragment,t),g(vt.$$.fragment,t),g(kt.$$.fragment,t),g($t.$$.fragment,t),g(wt.$$.fragment,t),g(Mt.$$.fragment,t),g(Et.$$.fragment,t),g(zt.$$.fragment,t),g(qt.$$.fragment,t),g(jt.$$.fragment,t),g(Ft.$$.fragment,t),g(Ct.$$.fragment,t),g(Pt.$$.fragment,t),g(At.$$.fragment,t),g(Lt.$$.fragment,t),g(Nt.$$.fragment,t),g(Dt.$$.fragment,t),g(Gt.$$.fragment,t),g(Ot.$$.fragment,t),g(Vt.$$.fragment,t),g(Wt.$$.fragment,t),sa=!0)},o(t){_(Pe.$$.fragment,t),_(Ae.$$.fragment,t),_(We.$$.fragment,t),_(Be.$$.fragment,t),_(Re.$$.fragment,t),_(Xe.$$.fragment,t),_(et.$$.fragment,t),_(st.$$.fragment,t),_(rt.$$.fragment,t),_(at.$$.fragment,t),_(ot.$$.fragment,t),_(it.$$.fragment,t),_(pt.$$.fragment,t),_(ct.$$.fragment,t),_(ft.$$.fragment,t),_(ht.$$.fragment,t),_(gt.$$.fragment,t),_(_t.$$.fragment,t),_(vt.$$.fragment,t),_(kt.$$.fragment,t),_($t.$$.fragment,t),_(wt.$$.fragment,t),_(Mt.$$.fragment,t),_(Et.$$.fragment,t),_(zt.$$.fragment,t),_(qt.$$.fragment,t),_(jt.$$.fragment,t),_(Ft.$$.fragment,t),_(Ct.$$.fragment,t),_(Pt.$$.fragment,t),_(At.$$.fragment,t),_(Lt.$$.fragment,t),_(Nt.$$.fragment,t),_(Dt.$$.fragment,t),_(Gt.$$.fragment,t),_(Ot.$$.fragment,t),_(Vt.$$.fragment,t),_(Wt.$$.fragment,t),sa=!1},d(t){s(N),t&&s(Bt),t&&s(P),v(Pe),t&&s(Mr),t&&s(U),v(Ae),t&&s(Er),t&&s(ae),t&&s(zr),t&&s(Ht),t&&s(br),t&&s(Rt),t&&s(qr),t&&s(ne),t&&s(jr),t&&s(Xt),t&&s(Fr),t&&s(y),t&&s(xr),t&&s(D),t&&s(Cr),t&&s(W),v(We),t&&s(Pr),t&&s(S),v(Be),t&&s(Ar),t&&s(H),v(Re),t&&s(Sr),t&&s(T),v(Xe),v(et),v(st),v(rt),v(at),t&&s(Lr),t&&s(me),t&&s(Nr),t&&s(R),v(ot),t&&s(Dr),t&&s(w),v(it),v(pt),v(ct),t&&s(Ir),t&&s(he),t&&s(Gr),t&&s(K),v(ft),t&&s(Or),t&&s(M),v(ht),v(gt),t&&s(Vr),t&&s(J),v(_t),t&&s(Ur),t&&s(E),v(vt),v(kt),t&&s(Wr),t&&s(Q),v($t),t&&s(Br),t&&s(z),v(wt),v(Mt),t&&s(Hr),t&&s(Y),v(Et),t&&s(Rr),t&&s(b),v(zt),v(qt),t&&s(Xr),t&&s(Z),v(jt),t&&s(Kr),t&&s(q),v(Ft),v(Ct),t&&s(Jr),t&&s(ee),v(Pt),t&&s(Qr),t&&s(j),v(At),v(Lt),t&&s(Yr),t&&s(te),v(Nt),t&&s(Zr),t&&s(F),v(Dt),v(Gt),t&&s(ea),t&&s(se),v(Ot),t&&s(ta),t&&s(x),v(Vt),v(Wt)}}}const cp={local:"mt5",sections:[{local:"overview",title:"Overview"},{local:"transformers.MT5Config",title:"MT5Config"},{local:"transformers.T5Tokenizer",title:"MT5Tokenizer"},{local:"transformers.T5TokenizerFast",title:"MT5TokenizerFast"},{local:"transformers.MT5Model",title:"MT5Model"},{local:"transformers.MT5ForConditionalGeneration",title:"MT5ForConditionalGeneration"},{local:"transformers.MT5EncoderModel",title:"MT5EncoderModel"},{local:"transformers.TFMT5Model",title:"TFMT5Model"},{local:"transformers.TFMT5ForConditionalGeneration",title:"TFMT5ForConditionalGeneration"},{local:"transformers.TFMT5EncoderModel",title:"TFMT5EncoderModel"},{local:"transformers.FlaxMT5Model",title:"FlaxMT5Model"},{local:"transformers.FlaxMT5ForConditionalGeneration",title:"FlaxMT5ForConditionalGeneration"}],title:"mT5"};function fp(Ga,N,Bt){let{fw:P}=N;return Ga.$$set=A=>{"fw"in A&&Bt(0,P=A.fw)},[P]}class Tp extends op{constructor(N){super();ip(this,N,fp,mp,lp,{fw:0})}}export{Tp as default,cp as metadata};
