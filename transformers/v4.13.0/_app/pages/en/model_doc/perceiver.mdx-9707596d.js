import{S as s1,i as n1,s as a1,e as r,k as c,w as f,t as a,L as i1,c as s,d as o,m as l,a as n,x as u,h as i,b as d,J as e,g as m,y as g,q as v,o as _,B as P}from"../../../chunks/vendor-e859c359.js";import{T as so}from"../../../chunks/Tip-edc75249.js";import{D as x}from"../../../chunks/Docstring-ade913b3.js";import{C as Me}from"../../../chunks/CodeBlock-ce4317c2.js";import{I as C}from"../../../chunks/IconCopyLink-5fae3b20.js";import"../../../chunks/CopyButton-77addb3d.js";function c1(M){let h,k,w,$,T;return{c(){h=r("p"),k=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=r("code"),$=a("Module"),T=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(y){h=s(y,"P",{});var b=n(h);k=i(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=s(b,"CODE",{});var E=n(w);$=i(E,"Module"),E.forEach(o),T=i(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(y,b){m(y,h,b),e(h,k),e(h,w),e(w,$),e(h,T)},d(y){y&&o(h)}}}function l1(M){let h,k,w,$,T;return{c(){h=r("p"),k=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=r("code"),$=a("Module"),T=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(y){h=s(y,"P",{});var b=n(h);k=i(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=s(b,"CODE",{});var E=n(w);$=i(E,"Module"),E.forEach(o),T=i(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(y,b){m(y,h,b),e(h,k),e(h,w),e(w,$),e(h,T)},d(y){y&&o(h)}}}function d1(M){let h,k,w,$,T;return{c(){h=r("p"),k=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=r("code"),$=a("Module"),T=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(y){h=s(y,"P",{});var b=n(h);k=i(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=s(b,"CODE",{});var E=n(w);$=i(E,"Module"),E.forEach(o),T=i(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(y,b){m(y,h,b),e(h,k),e(h,w),e(w,$),e(h,T)},d(y){y&&o(h)}}}function p1(M){let h,k,w,$,T;return{c(){h=r("p"),k=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=r("code"),$=a("Module"),T=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(y){h=s(y,"P",{});var b=n(h);k=i(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=s(b,"CODE",{});var E=n(w);$=i(E,"Module"),E.forEach(o),T=i(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(y,b){m(y,h,b),e(h,k),e(h,w),e(w,$),e(h,T)},d(y){y&&o(h)}}}function m1(M){let h,k,w,$,T;return{c(){h=r("p"),k=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=r("code"),$=a("Module"),T=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(y){h=s(y,"P",{});var b=n(h);k=i(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=s(b,"CODE",{});var E=n(w);$=i(E,"Module"),E.forEach(o),T=i(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(y,b){m(y,h,b),e(h,k),e(h,w),e(w,$),e(h,T)},d(y){y&&o(h)}}}function h1(M){let h,k,w,$,T;return{c(){h=r("p"),k=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=r("code"),$=a("Module"),T=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(y){h=s(y,"P",{});var b=n(h);k=i(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=s(b,"CODE",{});var E=n(w);$=i(E,"Module"),E.forEach(o),T=i(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(y,b){m(y,h,b),e(h,k),e(h,w),e(w,$),e(h,T)},d(y){y&&o(h)}}}function f1(M){let h,k,w,$,T;return{c(){h=r("p"),k=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=r("code"),$=a("Module"),T=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(y){h=s(y,"P",{});var b=n(h);k=i(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=s(b,"CODE",{});var E=n(w);$=i(E,"Module"),E.forEach(o),T=i(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(y,b){m(y,h,b),e(h,k),e(h,w),e(w,$),e(h,T)},d(y){y&&o(h)}}}function u1(M){let h,k,w,$,T;return{c(){h=r("p"),k=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=r("code"),$=a("Module"),T=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(y){h=s(y,"P",{});var b=n(h);k=i(b,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),w=s(b,"CODE",{});var E=n(w);$=i(E,"Module"),E.forEach(o),T=i(b,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),b.forEach(o)},m(y,b){m(y,h,b),e(h,k),e(h,w),e(w,$),e(h,T)},d(y){y&&o(h)}}}function g1(M){let h,k,w,$,T,y,b,E,wl,yi,Ie,bt,Ds,no,bl,Ns,yl,$i,yt,$l,ao,kl,Tl,ki,$t,xl,io,El,Fl,Ti,Gr,Cl,xi,Zr,Ls,Ml,Ei,Xr,Il,Fi,Qr,zl,Ci,N,ql,Yr,jl,Al,Ss,Ol,Dl,Hs,Nl,Ll,Bs,Sl,Hl,Ws,Bl,Wl,Mi,es,Vl,Ii,z,Ul,Vs,Rl,Kl,Us,Jl,Gl,Rs,Zl,Xl,Ks,Ql,Yl,Js,ed,td,Gs,od,rd,zi,ge,sd,co,nd,ad,lo,id,cd,qi,ze,kt,Zs,po,ld,Xs,dd,ji,qe,mo,pd,Qs,md,Ai,je,ho,hd,Ys,fd,Oi,Ae,fo,ud,en,gd,Di,Oe,uo,vd,tn,_d,Ni,De,Tt,on,go,Pd,rn,wd,Li,L,vo,bd,Ne,yd,ts,$d,kd,_o,Td,xd,Ed,Le,Fd,os,Cd,Md,rs,Id,zd,qd,sn,jd,Ad,Po,Si,Se,xt,nn,wo,Od,an,Dd,Hi,I,bo,Nd,cn,Ld,Sd,yo,Hd,ss,Bd,Wd,Vd,ve,$o,Ud,ln,Rd,Kd,ko,ns,Jd,dn,Gd,Zd,as,Xd,pn,Qd,Yd,Et,To,ep,xo,tp,mn,op,rp,sp,_e,Eo,np,is,ap,cs,ip,cp,hn,lp,dp,fn,Bi,He,Ft,un,Fo,pp,gn,mp,Wi,te,Co,hp,vn,fp,up,Mo,gp,ls,vp,_p,Pp,Ct,Io,wp,Be,bp,_n,yp,$p,Pn,kp,Tp,Vi,We,Mt,wn,zo,xp,bn,Ep,Ui,Ve,qo,Fp,yn,Cp,Ri,Ue,It,$n,jo,Mp,kn,Ip,Ki,ce,Ao,zp,Tn,qp,jp,oe,Ap,xn,Op,Dp,En,Np,Lp,Fn,Sp,Hp,Cn,Bp,Wp,Ji,Re,zt,Mn,Oo,Vp,In,Up,Gi,Ke,Do,Rp,zn,Kp,Zi,Je,qt,qn,No,Jp,jn,Gp,Xi,Ge,Lo,Zp,An,Xp,Qi,Ze,jt,On,So,Qp,Dn,Yp,Yi,le,Ho,em,Nn,tm,om,Ln,rm,ec,Xe,At,Sn,Bo,sm,Hn,nm,tc,Qe,Wo,am,Bn,im,oc,Ye,Ot,Wn,Vo,cm,Vn,lm,rc,et,Uo,dm,Un,pm,sc,tt,Dt,Rn,Ro,mm,Kn,hm,nc,ot,Ko,fm,Jn,um,ac,rt,Nt,Gn,Jo,gm,Zn,vm,ic,st,Go,_m,Xn,Pm,cc,nt,Lt,Qn,Zo,wm,Yn,bm,lc,de,Xo,ym,Qo,$m,Yo,km,Tm,xm,B,er,Em,at,Fm,ds,Cm,Mm,ea,Im,zm,qm,St,jm,ta,Am,Om,tr,dc,it,Ht,oa,or,Dm,ra,Nm,pc,pe,rr,Lm,sr,Sm,nr,Hm,Bm,Wm,W,ar,Vm,ct,Um,ps,Rm,Km,sa,Jm,Gm,Zm,Bt,Xm,na,Qm,Ym,ir,mc,lt,Wt,aa,cr,eh,ia,th,hc,me,lr,oh,dr,rh,pr,sh,nh,ah,q,mr,ih,dt,ch,ms,lh,dh,ca,ph,mh,hh,Vt,fh,la,uh,gh,hr,vh,da,_h,Ph,fr,fc,pt,Ut,pa,ur,wh,ma,bh,uc,A,gr,yh,ha,$h,kh,fa,Th,xh,V,ua,Eh,Fh,ga,Ch,Mh,va,Ih,zh,_a,qh,jh,Pa,Ah,Oh,Dh,vr,Nh,_r,Lh,Sh,Hh,U,Pr,Bh,mt,Wh,hs,Vh,Uh,wa,Rh,Kh,Jh,Rt,Gh,ba,Zh,Xh,wr,gc,ht,Kt,ya,br,Qh,$a,Yh,vc,O,yr,ef,ka,tf,of,Ta,rf,sf,R,xa,nf,af,Ea,cf,lf,Fa,df,pf,Ca,mf,hf,Ma,ff,uf,gf,$r,vf,kr,_f,Pf,wf,K,Tr,bf,ft,yf,fs,$f,kf,Ia,Tf,xf,Ef,Jt,Ff,za,Cf,Mf,xr,_c,ut,Gt,qa,Er,If,ja,zf,Pc,D,Fr,qf,Aa,jf,Af,Oa,Of,Df,J,Da,Nf,Lf,Na,Sf,Hf,La,Bf,Wf,Sa,Vf,Uf,Ha,Rf,Kf,Jf,Cr,Gf,Mr,Zf,Xf,Qf,G,Ir,Yf,gt,eu,us,tu,ou,Ba,ru,su,nu,Zt,au,Wa,iu,cu,zr,wc,vt,Xt,Va,qr,lu,Ua,du,bc,S,jr,pu,H,mu,Ra,hu,fu,Ka,uu,gu,Ja,vu,_u,Ga,Pu,wu,Za,bu,yu,$u,Xa,ku,Tu,Ar,xu,Or,Eu,Fu,Cu,Z,Dr,Mu,_t,Iu,gs,zu,qu,Qa,ju,Au,Ou,Qt,Du,Ya,Nu,Lu,Nr,yc,Pt,Yt,ei,Lr,Su,ti,Hu,$c,F,Sr,Bu,oi,Wu,Vu,eo,ri,Uu,Ru,si,Ku,Ju,Gu,Pe,ni,Zu,Xu,ai,Qu,Yu,ii,eg,tg,og,to,ci,rg,sg,li,ng,ag,ig,Hr,cg,di,lg,dg,pg,pi,mg,hg,Br,fg,Wr,ug,gg,vg,X,Vr,_g,wt,Pg,vs,wg,bg,mi,yg,$g,kg,oo,Tg,hi,xg,Eg,Ur,kc;return y=new C({}),no=new C({}),po=new C({}),mo=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput",parameters:[{name:"logits",val:": FloatTensor = None"},{name:"last_hidden_state",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L67",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.`,name:"attentions"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder&#x2019;s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.`,name:"cross_attentions"}]}}),ho=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverDecoderOutput",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverDecoderOutput",parameters:[{name:"logits",val:": FloatTensor = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L98",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverDecoderOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_labels)</code>) &#x2014;
Output of the basic decoder.`,name:"logits"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverDecoderOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder&#x2019;s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.`,name:"cross_attentions"}]}}),fo=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L116",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Masked language modeling (MLM) loss.`,name:"loss"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).`,name:"logits"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, num_latents, num_latents)</code>. Attentions weights after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder&#x2019;s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.`,name:"cross_attentions"}]}}),uo=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L147",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Classification (or regression if config.num_labels==1) loss.`,name:"loss"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) &#x2014;
Classification (or regression if config.num_labels==1) scores (before SoftMax).`,name:"logits"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.`,name:"attentions"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder&#x2019;s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.`,name:"cross_attentions"}]}}),go=new C({}),vo=new x({props:{name:"class transformers.PerceiverConfig",anchor:"transformers.PerceiverConfig",parameters:[{name:"num_latents",val:" = 256"},{name:"d_latents",val:" = 1280"},{name:"d_model",val:" = 768"},{name:"num_blocks",val:" = 1"},{name:"num_self_attends_per_block",val:" = 26"},{name:"num_self_attention_heads",val:" = 8"},{name:"num_cross_attention_heads",val:" = 8"},{name:"qk_channels",val:" = None"},{name:"v_channels",val:" = None"},{name:"cross_attention_shape_for_attention",val:" = 'kv'"},{name:"self_attention_widening_factor",val:" = 1"},{name:"cross_attention_widening_factor",val:" = 1"},{name:"hidden_act",val:" = 'gelu'"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"position_embedding_init_scale",val:" = 0.02"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"is_encoder_decoder",val:" = False"},{name:"use_query_residual",val:" = True"},{name:"vocab_size",val:" = 262"},{name:"max_position_embeddings",val:" = 2048"},{name:"image_size",val:" = 56"},{name:"train_size",val:" = [368, 496]"},{name:"num_frames",val:" = 16"},{name:"audio_samples_per_frame",val:" = 1920"},{name:"samples_per_patch",val:" = 16"},{name:"output_shape",val:" = [1, 16, 224, 224]"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/configuration_perceiver.py#L29",parametersDescription:[{anchor:"transformers.PerceiverConfig.num_latents",description:`<strong>num_latents</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The number of latents.`,name:"num_latents"},{anchor:"transformers.PerceiverConfig.d_latents",description:`<strong>d_latents</strong> (<code>int</code>, <em>optional</em>, defaults to 1280) &#x2014;
Dimension of the latent embeddings.`,name:"d_latents"},{anchor:"transformers.PerceiverConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimension of the inputs.`,name:"d_model"},{anchor:"transformers.PerceiverConfig.num_blocks",description:`<strong>num_blocks</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of blocks in the Transformer encoder.`,name:"num_blocks"},{anchor:"transformers.PerceiverConfig.num_self_attends_per_block",description:`<strong>num_self_attends_per_block</strong> (<code>int</code>, <em>optional</em>, defaults to 26) &#x2014;
The number of self-attention layers per block.`,name:"num_self_attends_per_block"},{anchor:"transformers.PerceiverConfig.num_self_attention_heads",description:`<strong>num_self_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each self-attention layer in the Transformer encoder.`,name:"num_self_attention_heads"},{anchor:"transformers.PerceiverConfig.num_cross_attention_heads",description:`<strong>num_cross_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each cross-attention layer in the Transformer encoder.`,name:"num_cross_attention_heads"},{anchor:"transformers.PerceiverConfig.qk_channels",description:`<strong>qk_channels</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Dimension to project the queries + keys before applying attention in the cross-attention and self-attention
layers of the encoder. Will default to preserving the dimension of the queries if not specified.`,name:"qk_channels"},{anchor:"transformers.PerceiverConfig.v_channels",description:`<strong>v_channels</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Dimension to project the values before applying attention in the cross-attention and self-attention layers
of the encoder. Will default to preserving the dimension of the queries if not specified.`,name:"v_channels"},{anchor:"transformers.PerceiverConfig.cross_attention_shape_for_attention",description:`<strong>cross_attention_shape_for_attention</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&apos;kv&apos;</code>) &#x2014;
Dimension to use when downsampling the queries and keys in the cross-attention layer of the encoder.`,name:"cross_attention_shape_for_attention"},{anchor:"transformers.PerceiverConfig.self_attention_widening_factor",description:`<strong>self_attention_widening_factor</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Dimension of the feed-forward layer in the cross-attention layer of the Transformer encoder.`,name:"self_attention_widening_factor"},{anchor:"transformers.PerceiverConfig.cross_attention_widening_factor",description:`<strong>cross_attention_widening_factor</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Dimension of the feed-forward layer in the self-attention layers of the Transformer encoder.`,name:"cross_attention_widening_factor"},{anchor:"transformers.PerceiverConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string,
<code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.PerceiverConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.PerceiverConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.PerceiverConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.PerceiverConfig.use_query_residual",description:`<strong>use_query_residual</strong> (<code>float</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a query residual in the cross-attention layer of the encoder.`,name:"use_query_residual"},{anchor:"transformers.PerceiverConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 262) &#x2014;
Vocabulary size for the masked language modeling model.`,name:"vocab_size"},{anchor:"transformers.PerceiverConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that the masked language modeling model might ever be used with. Typically set
this to something large just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.PerceiverConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 56) &#x2014;
Size of the images after preprocessing, for <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a>.`,name:"image_size"},{anchor:"transformers.PerceiverConfig.train_size",description:`<strong>train_size</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [368, 496]) &#x2014;
Training size of the images for the optical flow model.`,name:"train_size"},{anchor:"transformers.PerceiverConfig.num_frames",description:`<strong>num_frames</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of video frames used for the multimodal autoencoding model.`,name:"num_frames"},{anchor:"transformers.PerceiverConfig.audio_samples_per_frame",description:`<strong>audio_samples_per_frame</strong> (<code>int</code>, <em>optional</em>, defaults to 1920) &#x2014;
Number of audio samples per frame for the multimodal autoencoding model.`,name:"audio_samples_per_frame"},{anchor:"transformers.PerceiverConfig.samples_per_patch",description:`<strong>samples_per_patch</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of audio samples per patch when preprocessing the audio for the multimodal autoencoding model.`,name:"samples_per_patch"},{anchor:"transformers.PerceiverConfig.output_shape",description:`<strong>output_shape</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 16, 224, 224]</code>) &#x2014;
Shape of the output for the multimodal autoencoding model.`,name:"output_shape"}]}}),Po=new Me({props:{code:`from transformers import PerceiverModel, PerceiverConfig

# Initializing a Perceiver deepmind/language-perceiver style configuration
configuration = PerceiverConfig()

# Initializing a model from the deepmind/language-perceiver style configuration
model = PerceiverModel(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PerceiverModel, PerceiverConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Perceiver deepmind/language-perceiver style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = PerceiverConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the deepmind/language-perceiver style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PerceiverModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),wo=new C({}),bo=new x({props:{name:"class transformers.PerceiverTokenizer",anchor:"transformers.PerceiverTokenizer",parameters:[{name:"pad_token",val:" = '[PAD]'"},{name:"bos_token",val:" = '[BOS]'"},{name:"eos_token",val:" = '[EOS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"model_max_length",val:" = 2048"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/tokenization_perceiver.py#L27",parametersDescription:[{anchor:"transformers.PerceiverTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.PerceiverTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[BOS]&quot;</code>) &#x2014;
The BOS token (reserved in the vocab, but not actually used).`,name:"bos_token"},{anchor:"transformers.PerceiverTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[EOS]&quot;</code>) &#x2014;
The end of sequence token (reserved in the vocab, but not actually used).`,name:"eos_token"}]}}),$o=new x({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.PerceiverTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/tokenization_perceiver.py#L133",parametersDescription:[{anchor:"transformers.PerceiverTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.PerceiverTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),To=new x({props:{name:"get_special_tokens_mask",anchor:"transformers.PerceiverTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/tokenization_perceiver.py#L105",parametersDescription:[{anchor:"transformers.PerceiverTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PerceiverTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.PerceiverTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Eo=new x({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_utils_base.py#L2806",parametersDescription:[{anchor:"transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.PreTrainedTokenizerBase.create_token_type_ids_from_sequences.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],returnDescription:`
<p>The token type ids.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Fo=new C({}),Co=new x({props:{name:"class transformers.PerceiverFeatureExtractor",anchor:"transformers.PerceiverFeatureExtractor",parameters:[{name:"do_center_crop",val:" = True"},{name:"crop_size",val:" = 256"},{name:"do_resize",val:" = True"},{name:"size",val:" = 224"},{name:"resample",val:" = 3"},{name:"do_normalize",val:" = True"},{name:"image_mean",val:" = None"},{name:"image_std",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/feature_extraction_perceiver.py#L37",parametersDescription:[{anchor:"transformers.PerceiverFeatureExtractor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input at the center. If the input size is smaller than <code>crop_size</code> along any edge,
the image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.PerceiverFeatureExtractor.crop_size",description:`<strong>crop_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Desired output size when applying center-cropping. Only has an effect if <code>do_center_crop</code> is set to
<code>True</code>.`,name:"crop_size"},{anchor:"transformers.PerceiverFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.PerceiverFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple(int)</code>, <em>optional</em>, defaults to 224) &#x2014;
Resize the input to the given size. If a tuple is provided, it should be (width, height). If only an
integer is provided, then the input will be resized to (size, size). Only has an effect if <code>do_resize</code>
is set to <code>True</code>.`,name:"size"},{anchor:"transformers.PerceiverFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BICUBIC</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>.
Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.PerceiverFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input with <code>image_mean</code> and <code>image_std</code>.`,name:"do_normalize"},{anchor:"transformers.PerceiverFeatureExtractor.image_mean",description:`<strong>image_mean</strong> (<code>List[int]</code>, defaults to <code>[0.485, 0.456, 0.406]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images.`,name:"image_mean"},{anchor:"transformers.PerceiverFeatureExtractor.image_std",description:`<strong>image_std</strong> (<code>List[int]</code>, defaults to <code>[0.229, 0.224, 0.225]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images.`,name:"image_std"}]}}),Io=new x({props:{name:"center_crop",anchor:"transformers.PerceiverFeatureExtractor.center_crop",parameters:[{name:"image",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/feature_extraction_perceiver.py#L93",parametersDescription:[{anchor:"transformers.PerceiverFeatureExtractor.center_crop.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"}]}}),zo=new C({}),qo=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverTextPreprocessor",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverTextPreprocessor",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L2542"}}),jo=new C({}),Ao=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor",parameters:[{name:"config",val:""},{name:"prep_type",val:" = 'conv'"},{name:"spatial_downsample",val:": int = 4"},{name:"temporal_downsample",val:": int = 1"},{name:"position_encoding_type",val:": str = 'fourier'"},{name:"in_channels",val:": int = 3"},{name:"out_channels",val:": int = 64"},{name:"conv_after_patching",val:": bool = False"},{name:"conv_after_patching_in_channels",val:": int = 54"},{name:"conv2d_use_batchnorm",val:": bool = True"},{name:"concat_or_add_pos",val:": str = 'concat'"},{name:"project_pos_dim",val:": int = -1"},{name:"**position_encoding_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L2684",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.config",description:`<strong>config</strong> (<code>PerceiverConfig</code>) &#x2014;
Model configuration.`,name:"config"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.prep_type",description:`<strong>prep_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;conv&quot;</code>) &#x2014;
Preprocessing type. Can be &#x201C;conv1x1&#x201D;, &#x201C;conv&#x201D;, &#x201C;patches&#x201D;, &#x201C;pixels&#x201D;.`,name:"prep_type"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.spatial_downsample",description:`<strong>spatial_downsample</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Spatial downsampling factor.`,name:"spatial_downsample"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.temporal_downsample",description:`<strong>temporal_downsample</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Temporal downsampling factor (only relevant in case a time dimension is present).`,name:"temporal_downsample"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.position_encoding_type",description:`<strong>position_encoding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;fourier&quot;</code>) &#x2014;
Position encoding type. Can be &#x201C;fourier&#x201D; or &#x201C;trainable&#x201D;.`,name:"position_encoding_type"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.in_channels",description:`<strong>in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
Number of channels in the input.`,name:"in_channels"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.out_channels",description:`<strong>out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Number of channels in the output.`,name:"out_channels"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.conv_after_patching",description:`<strong>conv_after_patching</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to apply a convolutional layer after patching.`,name:"conv_after_patching"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.conv_after_patching_in_channels",description:`<strong>conv_after_patching_in_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 54) &#x2014;
Number of channels in the input of the convolutional layer after patching.`,name:"conv_after_patching_in_channels"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.conv2d_use_batchnorm",description:`<strong>conv2d_use_batchnorm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use batch normalization in the convolutional layer.`,name:"conv2d_use_batchnorm"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.concat_or_add_pos",description:`<strong>concat_or_add_pos</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;concat&quot;</code>) &#x2014;
How to concatenate the position encoding to the input. Can be &#x201C;concat&#x201D; or &#x201C;add&#x201D;.`,name:"concat_or_add_pos"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.project_pos_dim",description:`<strong>project_pos_dim</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
Dimension of the position encoding to project to. If -1, no projection is applied.`,name:"project_pos_dim"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor.*position_encoding_kwargs",description:`*<strong>*position_encoding_kwargs</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Keyword arguments for the position encoding.`,name:"*position_encoding_kwargs"}]}}),Oo=new C({}),Do=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverOneHotPreprocessor",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverOneHotPreprocessor",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L2913",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverOneHotPreprocessor.config",description:`<strong>config</strong> (<code>PerceiverConfig</code>) &#x2014;
Model configuration.`,name:"config"}]}}),No=new C({}),Lo=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor",parameters:[{name:"config",val:""},{name:"prep_type",val:": str = 'patches'"},{name:"samples_per_patch",val:": int = 96"},{name:"position_encoding_type",val:": str = 'fourier'"},{name:"concat_or_add_pos",val:": str = 'concat'"},{name:"out_channels",val:" = 64"},{name:"project_pos_dim",val:" = -1"},{name:"**position_encoding_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L2939",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor.config",description:`<strong>config</strong> (<code>PerceiverConfig</code>) &#x2014;
Model configuration.`,name:"config"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor.prep_type",description:`<strong>prep_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;patches&quot;</code>) &#x2014;
Preprocessor type to use. Only &#x201C;patches&#x201D; is supported.`,name:"prep_type"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor.samples_per_patch",description:`<strong>samples_per_patch</strong> (<code>int</code>, <em>optional</em>, defaults to 96) &#x2014;
Number of samples per patch.`,name:"samples_per_patch"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor.position_encoding_type",description:`<strong>position_encoding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;fourier&quot;</code>) &#x2014;
Type of position encoding to use. Can be &#x201C;trainable&#x201D; or &#x201C;fourier&#x201D;.`,name:"position_encoding_type"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor.concat_or_add_pos",description:`<strong>concat_or_add_pos</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;concat&quot;</code>) &#x2014;
How to concatenate the position encoding to the input. Can be &#x201C;concat&#x201D; or &#x201C;add&#x201D;.`,name:"concat_or_add_pos"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor.out_channels",description:`<strong>out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Number of channels in the output.`,name:"out_channels"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor.project_pos_dim",description:`<strong>project_pos_dim</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
Dimension of the position encoding to project to. If -1, no projection is applied.`,name:"project_pos_dim"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor.*position_encoding_kwargs",description:`*<strong>*position_encoding_kwargs</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Keyword arguments for the position encoding.`,name:"*position_encoding_kwargs"}]}}),So=new C({}),Ho=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor",parameters:[{name:"modalities",val:": typing.Mapping[str, typing.Callable[..., typing.Tuple[torch.Tensor, typing.Optional[torch.Tensor], torch.Tensor]]]"},{name:"mask_probs",val:": typing.Union[typing.Mapping[str, float], NoneType] = None"},{name:"min_padding_size",val:": int = 2"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L3036",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor.modalities",description:`<strong>modalities</strong> (<code>Dict[str, PreprocessorType]</code>) &#x2014;
Dict mapping modality name to preprocessor.`,name:"modalities"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor.mask_probs",description:`<strong>mask_probs</strong> (<code>Dict[str, float]</code>) &#x2014;
Dict mapping modality name to masking probability of that modality.`,name:"mask_probs"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor.min_padding_size",description:`<strong>min_padding_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The minimum padding size for all modalities. The final output will have num_channels equal to the maximum
channels across all modalities plus min_padding_size.`,name:"min_padding_size"}]}}),Bo=new C({}),Wo=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverProjectionPostprocessor",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverProjectionPostprocessor",parameters:[{name:"in_channels",val:""},{name:"out_channels",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L2663",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverProjectionPostprocessor.in_channels",description:`<strong>in_channels</strong> (<code>int</code>) &#x2014;
Number of channels in the input.`,name:"in_channels"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverProjectionPostprocessor.out_channels",description:`<strong>out_channels</strong> (<code>int</code>) &#x2014;
Number of channels in the output.`,name:"out_channels"}]}}),Vo=new C({}),Uo=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor",parameters:[{name:"config",val:""},{name:"in_channels",val:""},{name:"postproc_type",val:": str = 'patches'"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L2635",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor.config",description:`<strong>config</strong> (<code>PerceiverConfig</code>) &#x2014;
Model configuration.`,name:"config"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor.in_channels",description:`<strong>in_channels</strong> (<code>int</code>) &#x2014;
Number of channels in the input.`,name:"in_channels"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor.postproc_type",description:`<strong>postproc_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;patches&quot;</code>) &#x2014;
Postprocessor type to use. Currently, only &#x201C;patches&#x201D; is supported.`,name:"postproc_type"}]}}),Ro=new C({}),Ko=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverClassificationPostprocessor",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverClassificationPostprocessor",parameters:[{name:"config",val:""},{name:"in_channels",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L2615",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverClassificationPostprocessor.config",description:`<strong>config</strong> (<code>PerceiverConfig</code>) &#x2014;
Model configuration.`,name:"config"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverClassificationPostprocessor.in_channels",description:`<strong>in_channels</strong> (<code>int</code>) &#x2014;
Number of channels in the input.`,name:"in_channels"}]}}),Jo=new C({}),Go=new x({props:{name:"class transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPostprocessor",anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPostprocessor",parameters:[{name:"modalities",val:": typing.Mapping[str, typing.Callable[..., typing.Any]]"},{name:"input_is_dict",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L2582",parametersDescription:[{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPostprocessor.modalities",description:`<strong>modalities</strong> (<code>Dict[str, PostprocessorType]</code>) &#x2014;
Dictionary mapping modality name to postprocessor class for that modality.`,name:"modalities"},{anchor:"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPostprocessor.input_is_dict",description:`<strong>input_is_dict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If True, input is assumed to be dictionary structured, and outputs keep the same dictionary shape. If
False, input is a tensor which is sliced up during postprocessing by <em>modality_sizes</em>.`,name:"input_is_dict"}]}}),Zo=new C({}),Xo=new x({props:{name:"class transformers.PerceiverModel",anchor:"transformers.PerceiverModel",parameters:[{name:"config",val:""},{name:"decoder",val:" = None"},{name:"input_preprocessor",val:": typing.Callable[..., typing.Tuple[torch.Tensor, typing.Optional[torch.Tensor], torch.Tensor]] = None"},{name:"output_postprocessor",val:": typing.Callable[..., typing.Any] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L723",parametersDescription:[{anchor:"transformers.PerceiverModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"},{anchor:"transformers.PerceiverModel.decoder",description:`<strong>decoder</strong> (<em>DecoderType</em>, <em>optional</em>) &#x2014;
Optional decoder to use to decode the latent representation of the encoder. Examples include
<em>transformers.models.perceiver.modeling_perceiver.PerceiverBasicDecoder</em>,
<em>transformers.models.perceiver.modeling_perceiver.PerceiverClassificationDecoder</em>,
<em>transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalDecoder</em>.`,name:"decoder"},{anchor:"transformers.PerceiverModel.input_preprocessor",description:`<strong>input_preprocessor</strong> (<em>PreprocessorType</em>, <em>optional</em>) &#x2014;
Optional input preprocessor to use. Examples include
<em>transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor</em>,
<em>transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor</em>,
<em>transformers.models.perceiver.modeling_perceiver.PerceiverTextPreprocessor</em>,
<em>transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor</em>.`,name:"input_preprocessor"},{anchor:"transformers.PerceiverModel.output_postprocessor",description:`<strong>output_postprocessor</strong> (<em>PostprocessorType</em>, <em>optional</em>) &#x2014;
Optional output postprocessor to use. Examples include
<em>transformers.models.perceiver.modeling_perceiver.PerceiverImagePostprocessor</em>,
<em>transformers.models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor</em>,
<em>transformers.models.perceiver.modeling_perceiver.PerceiverClassificationPostprocessor</em>,
<em>transformers.models.perceiver.modeling_perceiver.PerceiverProjectionPostprocessor</em>,
<em>transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPostprocessor</em>.`,name:"output_postprocessor"},{anchor:"transformers.PerceiverModel.Note",description:"<strong>Note</strong> that you can define your own decoders, preprocessors and/or postprocessors to fit your use-case. &#x2014;",name:"Note"}]}}),er=new x({props:{name:"forward",anchor:"transformers.PerceiverModel.forward",parameters:[{name:"inputs",val:""},{name:"attention_mask",val:" = None"},{name:"subsampled_output_points",val:" = None"},{name:"head_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L757",parametersDescription:[{anchor:"transformers.PerceiverModel.forward.inputs",description:`<strong>inputs</strong> (<code>torch.FloatTensor</code>) &#x2014;
Inputs to the perceiver. Can be anything: images, text, audio, video, etc.`,name:"inputs"},{anchor:"transformers.PerceiverModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.PerceiverModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.PerceiverModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PerceiverModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PerceiverModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput"
>PerceiverModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"
>PerceiverConfig</a>) and inputs.</p>
<ul>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</li>
<li><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder\u2019s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput"
>PerceiverModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),St=new so({props:{$$slots:{default:[c1]},$$scope:{ctx:M}}}),tr=new Me({props:{code:`from transformers import PerceiverTokenizer, PerceiverModel
import torch

tokenizer = PerceiverTokenizer.from_pretrained('deepmind/language-perceiver')
model = PerceiverModel.from_pretrained('deepmind/language-perceiver')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PerceiverTokenizer, PerceiverModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = PerceiverTokenizer.from_pretrained(<span class="hljs-string">&#x27;deepmind/language-perceiver&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PerceiverModel.from_pretrained(<span class="hljs-string">&#x27;deepmind/language-perceiver&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),or=new C({}),rr=new x({props:{name:"class transformers.PerceiverForMaskedLM",anchor:"transformers.PerceiverForMaskedLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L873",parametersDescription:[{anchor:"transformers.PerceiverForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),ar=new x({props:{name:"forward",anchor:"transformers.PerceiverForMaskedLM.forward",parameters:[{name:"inputs",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"labels",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L902",parametersDescription:[{anchor:"transformers.PerceiverForMaskedLM.forward.inputs",description:`<strong>inputs</strong> (<code>torch.FloatTensor</code>) &#x2014;
Inputs to the perceiver. Can be anything: images, text, audio, video, etc.`,name:"inputs"},{anchor:"transformers.PerceiverForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>batch_size, sequence_length</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.PerceiverForMaskedLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.PerceiverForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PerceiverForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PerceiverForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.PerceiverForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput"
>PerceiverMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"
>PerceiverConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, num_latents, num_latents)</code>. Attentions weights after the attention softmax, used to compute the weighted average in the
self-attention heads.</li>
<li><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder\u2019s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverMaskedLMOutput"
>PerceiverMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Bt=new so({props:{$$slots:{default:[l1]},$$scope:{ctx:M}}}),ir=new Me({props:{code:`from transformers import PerceiverTokenizer, PerceiverForMaskedLM
import torch

tokenizer = PerceiverTokenizer.from_pretrained('deepmind/language-perceiver')
model = PerceiverForMaskedLM.from_pretrained('deepmind/language-perceiver')

inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")
labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]

outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PerceiverTokenizer, PerceiverForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = PerceiverTokenizer.from_pretrained(<span class="hljs-string">&#x27;deepmind/language-perceiver&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PerceiverForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;deepmind/language-perceiver&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),cr=new C({}),lr=new x({props:{name:"class transformers.PerceiverForSequenceClassification",anchor:"transformers.PerceiverForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L960",parametersDescription:[{anchor:"transformers.PerceiverForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),mr=new x({props:{name:"forward",anchor:"transformers.PerceiverForSequenceClassification.forward",parameters:[{name:"inputs",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"labels",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L981",parametersDescription:[{anchor:"transformers.PerceiverForSequenceClassification.forward.inputs",description:`<strong>inputs</strong> (<code>torch.FloatTensor</code>) &#x2014;
Inputs to the perceiver. Can be anything: images, text, audio, video, etc.`,name:"inputs"},{anchor:"transformers.PerceiverForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>batch_size, sequence_length</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.PerceiverForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.PerceiverForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PerceiverForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PerceiverForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.PerceiverForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),
If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).</p>
<p>Returns &#x2014;</p>
<p>Examples &#x2014;:</p>
<blockquote>
<blockquote>
<blockquote>
<p>from transformers import PerceiverTokenizer, PerceiverForSequenceClassification</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>tokenizer = PerceiverTokenizer.from_pretrained(&#x2018;deepmind/language-perceiver&#x2019;)
model = PerceiverForSequenceClassification.from_pretrained(&#x2018;deepmind/language-perceiver&#x2019;)</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<p>text = &#x201C;hello world&#x201D;
inputs = tokenizer(images=image, return_tensors=&#x201C;pt&#x201D;).input_ids
outputs = model(inputs=inputs)
logits = outputs.logits</p>
</blockquote>
</blockquote>
</blockquote>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"
>PerceiverConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</li>
<li><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder\u2019s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Vt=new so({props:{$$slots:{default:[d1]},$$scope:{ctx:M}}}),hr=new Me({props:{code:`from transformers import PerceiverTokenizer, PerceiverForSequenceClassification
import torch

tokenizer = PerceiverTokenizer.from_pretrained('deepmind/language-perceiver')
model = PerceiverForSequenceClassification.from_pretrained('deepmind/language-perceiver')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PerceiverTokenizer, PerceiverForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = PerceiverTokenizer.from_pretrained(<span class="hljs-string">&#x27;deepmind/language-perceiver&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PerceiverForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;deepmind/language-perceiver&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),fr=new Me({props:{code:`from transformers import PerceiverTokenizer, PerceiverForSequenceClassification
import torch

tokenizer = PerceiverTokenizer.from_pretrained('deepmind/language-perceiver')
model = PerceiverForSequenceClassification.from_pretrained('deepmind/language-perceiver', problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float) # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PerceiverTokenizer, PerceiverForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = PerceiverTokenizer.from_pretrained(<span class="hljs-string">&#x27;deepmind/language-perceiver&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PerceiverForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;deepmind/language-perceiver&#x27;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ur=new C({}),gr=new x({props:{name:"class transformers.PerceiverForImageClassificationLearned",anchor:"transformers.PerceiverForImageClassificationLearned",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L1082",parametersDescription:[{anchor:"transformers.PerceiverForImageClassificationLearned.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Pr=new x({props:{name:"forward",anchor:"transformers.PerceiverForImageClassificationLearned.forward",parameters:[{name:"inputs",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"labels",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L1113",parametersDescription:[{anchor:"transformers.PerceiverForImageClassificationLearned.forward.inputs",description:`<strong>inputs</strong> (<code>torch.FloatTensor</code>) &#x2014;
Inputs to the perceiver. Can be anything: images, text, audio, video, etc.`,name:"inputs"},{anchor:"transformers.PerceiverForImageClassificationLearned.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>batch_size, sequence_length</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.PerceiverForImageClassificationLearned.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.PerceiverForImageClassificationLearned.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PerceiverForImageClassificationLearned.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PerceiverForImageClassificationLearned.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.PerceiverForImageClassificationLearned.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),
If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"
>PerceiverConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</li>
<li><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder\u2019s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Rt=new so({props:{$$slots:{default:[p1]},$$scope:{ctx:M}}}),wr=new Me({props:{code:`from transformers import PerceiverFeatureExtractor, PerceiverForImageClassificationLearned
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = PerceiverFeatureExtractor.from_pretrained('deepmind/vision-perceiver-learned')
model = PerceiverForImageClassificationLearned.from_pretrained('deepmind/vision-perceiver-learned')

inputs = feature_extractor(images=image, return_tensors="pt").pixel_values
outputs = model(inputs=inputs)
logits = outputs.logits
# model predicts one of the 1000 ImageNet classes
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PerceiverFeatureExtractor, PerceiverForImageClassificationLearned
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = PerceiverFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;deepmind/vision-perceiver-learned&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PerceiverForImageClassificationLearned.from_pretrained(<span class="hljs-string">&#x27;deepmind/vision-perceiver-learned&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs=inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_idx = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])`}}),br=new C({}),yr=new x({props:{name:"class transformers.PerceiverForImageClassificationFourier",anchor:"transformers.PerceiverForImageClassificationFourier",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L1214",parametersDescription:[{anchor:"transformers.PerceiverForImageClassificationFourier.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Tr=new x({props:{name:"forward",anchor:"transformers.PerceiverForImageClassificationFourier.forward",parameters:[{name:"inputs",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"labels",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L1243",parametersDescription:[{anchor:"transformers.PerceiverForImageClassificationFourier.forward.inputs",description:`<strong>inputs</strong> (<code>torch.FloatTensor</code>) &#x2014;
Inputs to the perceiver. Can be anything: images, text, audio, video, etc.`,name:"inputs"},{anchor:"transformers.PerceiverForImageClassificationFourier.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>batch_size, sequence_length</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.PerceiverForImageClassificationFourier.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.PerceiverForImageClassificationFourier.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PerceiverForImageClassificationFourier.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PerceiverForImageClassificationFourier.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.PerceiverForImageClassificationFourier.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),
If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"
>PerceiverConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</li>
<li><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder\u2019s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Jt=new so({props:{$$slots:{default:[m1]},$$scope:{ctx:M}}}),xr=new Me({props:{code:`from transformers import PerceiverFeatureExtractor, PerceiverForImageClassificationFourier
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = PerceiverFeatureExtractor.from_pretrained('deepmind/vision-perceiver-fourier')
model = PerceiverForImageClassificationFourier.from_pretrained('deepmind/vision-perceiver-fourier')

inputs = feature_extractor(images=image, return_tensors="pt").pixel_values
outputs = model(inputs=inputs)
logits = outputs.logits
# model predicts one of the 1000 ImageNet classes
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PerceiverFeatureExtractor, PerceiverForImageClassificationFourier
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = PerceiverFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;deepmind/vision-perceiver-fourier&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PerceiverForImageClassificationFourier.from_pretrained(<span class="hljs-string">&#x27;deepmind/vision-perceiver-fourier&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs=inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_idx = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])`}}),Er=new C({}),Fr=new x({props:{name:"class transformers.PerceiverForImageClassificationConvProcessing",anchor:"transformers.PerceiverForImageClassificationConvProcessing",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L1344",parametersDescription:[{anchor:"transformers.PerceiverForImageClassificationConvProcessing.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Ir=new x({props:{name:"forward",anchor:"transformers.PerceiverForImageClassificationConvProcessing.forward",parameters:[{name:"inputs",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"labels",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L1374",parametersDescription:[{anchor:"transformers.PerceiverForImageClassificationConvProcessing.forward.inputs",description:`<strong>inputs</strong> (<code>torch.FloatTensor</code>) &#x2014;
Inputs to the perceiver. Can be anything: images, text, audio, video, etc.`,name:"inputs"},{anchor:"transformers.PerceiverForImageClassificationConvProcessing.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>batch_size, sequence_length</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.PerceiverForImageClassificationConvProcessing.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.PerceiverForImageClassificationConvProcessing.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PerceiverForImageClassificationConvProcessing.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PerceiverForImageClassificationConvProcessing.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.PerceiverForImageClassificationConvProcessing.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),
If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"
>PerceiverConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</li>
<li><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder\u2019s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Zt=new so({props:{$$slots:{default:[h1]},$$scope:{ctx:M}}}),zr=new Me({props:{code:`from transformers import PerceiverFeatureExtractor, PerceiverForImageClassificationConvProcessing
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = PerceiverFeatureExtractor.from_pretrained('deepmind/vision-perceiver-conv')
model = PerceiverForImageClassificationConvProcessing.from_pretrained('deepmind/vision-perceiver-conv')

inputs = feature_extractor(images=image, return_tensors="pt").pixel_values
outputs = model(inputs=inputs)
logits = outputs.logits
# model predicts one of the 1000 ImageNet classes
predicted_class_idx = logits.argmax(-1).item()
print("Predicted class:", model.config.id2label[predicted_class_idx]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PerceiverFeatureExtractor, PerceiverForImageClassificationConvProcessing
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = PerceiverFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;deepmind/vision-perceiver-conv&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = PerceiverForImageClassificationConvProcessing.from_pretrained(<span class="hljs-string">&#x27;deepmind/vision-perceiver-conv&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs=inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_idx = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])`}}),qr=new C({}),jr=new x({props:{name:"class transformers.PerceiverForOpticalFlow",anchor:"transformers.PerceiverForOpticalFlow",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L1475",parametersDescription:[{anchor:"transformers.PerceiverForOpticalFlow.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Dr=new x({props:{name:"forward",anchor:"transformers.PerceiverForOpticalFlow.forward",parameters:[{name:"inputs",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"labels",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L1520",parametersDescription:[{anchor:"transformers.PerceiverForOpticalFlow.forward.inputs",description:`<strong>inputs</strong> (<code>torch.FloatTensor</code>) &#x2014;
Inputs to the perceiver. Can be anything: images, text, audio, video, etc.`,name:"inputs"},{anchor:"transformers.PerceiverForOpticalFlow.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>batch_size, sequence_length</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.PerceiverForOpticalFlow.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.PerceiverForOpticalFlow.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PerceiverForOpticalFlow.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PerceiverForOpticalFlow.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.PerceiverForOpticalFlow.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the optical flow loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"
>PerceiverConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</li>
<li><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder\u2019s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Qt=new so({props:{$$slots:{default:[f1]},$$scope:{ctx:M}}}),Nr=new Me({props:{code:`from transformers import PerceiverForOpticalFlow
import torch

model = PerceiverForOpticalFlow.from_pretrained('deepmind/optical-flow-perceiver')

# in the Perceiver IO paper, the authors extract a 3 x 3 patch around each pixel,
# leading to 3 x 3 x 3 = 27 values for each pixel (as each pixel also has 3 color channels)
# patches have shape (batch_size, num_frames, num_channels, height, width)
# the authors train on resolutions of 368 x 496
patches = torch.randn(1, 2, 27, 368, 496)
outputs = model(inputs=patches)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PerceiverForOpticalFlow
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>model = PerceiverForOpticalFlow.from_pretrained(<span class="hljs-string">&#x27;deepmind/optical-flow-perceiver&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># in the Perceiver IO paper, the authors extract a 3 x 3 patch around each pixel,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># leading to 3 x 3 x 3 = 27 values for each pixel (as each pixel also has 3 color channels)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># patches have shape (batch_size, num_frames, num_channels, height, width)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the authors train on resolutions of 368 x 496</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>patches = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">27</span>, <span class="hljs-number">368</span>, <span class="hljs-number">496</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs=patches)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Lr=new C({}),Sr=new x({props:{name:"class transformers.PerceiverForMultimodalAutoencoding",anchor:"transformers.PerceiverForMultimodalAutoencoding",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L1612",parametersDescription:[{anchor:"transformers.PerceiverForMultimodalAutoencoding.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Vr=new x({props:{name:"forward",anchor:"transformers.PerceiverForMultimodalAutoencoding.forward",parameters:[{name:"inputs",val:" = None"},{name:"attention_mask",val:" = None"},{name:"subsampled_output_points",val:" = None"},{name:"head_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"labels",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/perceiver/modeling_perceiver.py#L1728",parametersDescription:[{anchor:"transformers.PerceiverForMultimodalAutoencoding.forward.inputs",description:`<strong>inputs</strong> (<code>torch.FloatTensor</code>) &#x2014;
Inputs to the perceiver. Can be anything: images, text, audio, video, etc.`,name:"inputs"},{anchor:"transformers.PerceiverForMultimodalAutoencoding.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>batch_size, sequence_length</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.PerceiverForMultimodalAutoencoding.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.PerceiverForMultimodalAutoencoding.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.PerceiverForMultimodalAutoencoding.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.PerceiverForMultimodalAutoencoding.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.PerceiverForMultimodalAutoencoding.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),
If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"
>PerceiverConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of
each layer plus the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the
weighted average in the self-attention heads.</li>
<li><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights of the decoder\u2019s cross-attention layer, after the
attention softmax, used to compute the weighted average in the cross-attention heads.</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/perceiver#transformers.models.perceiver.modeling_perceiver.PerceiverClassifierOutput"
>PerceiverClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oo=new so({props:{$$slots:{default:[u1]},$$scope:{ctx:M}}}),Ur=new Me({props:{code:`from transformers import PerceiverForMultimodalAutoencoding
import torch

images = torch.randn((1, 16, 3, 224, 224))
audio = torch.randn((1, 30720, 1))
inputs = dict(image=images, audio=audio, label=torch.zeros((images.shape[0], 700)))

model = PerceiverForMultimodalAutoencoding.from_pretrained('deepmind/multimodal-perceiver')

outputs = model(inputs=inputs)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PerceiverForMultimodalAutoencoding
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>images = torch.randn((<span class="hljs-number">1</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>audio = torch.randn((<span class="hljs-number">1</span>, <span class="hljs-number">30720</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = <span class="hljs-built_in">dict</span>(image=images, audio=audio, label=torch.zeros((images.shape[<span class="hljs-number">0</span>], <span class="hljs-number">700</span>)))

<span class="hljs-meta">&gt;&gt;&gt; </span>model = PerceiverForMultimodalAutoencoding.from_pretrained(<span class="hljs-string">&#x27;deepmind/multimodal-perceiver&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs=inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){h=r("meta"),k=c(),w=r("h1"),$=r("a"),T=r("span"),f(y.$$.fragment),b=c(),E=r("span"),wl=a("Perceiver"),yi=c(),Ie=r("h2"),bt=r("a"),Ds=r("span"),f(no.$$.fragment),bl=c(),Ns=r("span"),yl=a("Overview"),$i=c(),yt=r("p"),$l=a("The Perceiver IO model was proposed in "),ao=r("a"),kl=a("Perceiver IO: A General Architecture for Structured Inputs & Outputs"),Tl=a(` by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch,
Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\xE9naff, Matthew M.
Botvinick, Andrew Zisserman, Oriol Vinyals, Jo\xE3o Carreira.`),ki=c(),$t=r("p"),xl=a("Perceiver IO is a generalization of "),io=r("a"),El=a("Perceiver"),Fl=a(` to handle arbitrary outputs in
addition to arbitrary inputs. The original Perceiver only produced a single classification label. In addition to
classification labels, Perceiver IO can produce (for example) language, optical flow, and multimodal videos with audio.
This is done using the same building blocks as the original Perceiver. The computational complexity of Perceiver IO is
linear in the input and output size and the bulk of the processing occurs in the latent space, allowing us to process
inputs and outputs that are much larger than can be handled by standard Transformers. This means, for example,
Perceiver IO can do BERT-style masked language modeling directly using bytes instead of tokenized inputs.`),Ti=c(),Gr=r("p"),Cl=a("The abstract from the paper is the following:"),xi=c(),Zr=r("p"),Ls=r("em"),Ml=a(`The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point
clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of
inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without
sacrificing the original\u2019s appealing properties by learning to flexibly query the model\u2019s latent space to produce
outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales
linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves
strong results on tasks with highly structured output spaces, such as natural language and visual understanding,
StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT
baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art
performance on Sintel optical flow estimation.`),Ei=c(),Xr=r("p"),Il=a("Here\u2019s a TLDR explaining how Perceiver works:"),Fi=c(),Qr=r("p"),zl=a(`The main problem with the self-attention mechanism of the Transformer is that the time and memory requirements scale
quadratically with the sequence length. Hence, models like BERT and RoBERTa are limited to a max sequence length of 512
tokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a set
of latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements don\u2019t
depend on the length of the inputs anymore, as one uses a fixed amount of latent variables, like 256 or 512. These are
randomly initialized, after which they are trained end-to-end using backpropagation.`),Ci=c(),N=r("p"),ql=a("Internally, "),Yr=r("a"),jl=a("PerceiverModel"),Al=a(` will create the latents, which is a tensor of shape
`),Ss=r("code"),Ol=a("(batch_size, num_latents, d_latents)"),Dl=a(". One must provide "),Hs=r("code"),Nl=a("inputs"),Ll=a(` (which could be text, images, audio, you
name it!) to the model, which it will use to perform cross-attention with the latents. The output of the Perceiver
encoder is a tensor of the same shape. One can then, similar to BERT, convert the last hidden states of the latents to
classification logits by averaging along the sequence dimension, and placing a linear layer on top of that to project
the `),Bs=r("code"),Sl=a("d_latents"),Hl=a(" to "),Ws=r("code"),Bl=a("num_labels"),Wl=a("."),Mi=c(),es=r("p"),Vl=a(`This was the idea of the original Perceiver paper. However, it could only output classification logits. In a follow-up
work, PerceiverIO, they generalized it to let the model also produce outputs of arbitrary size. How, you might ask? The
idea is actually relatively simple: one defines outputs of an arbitrary size, and then applies cross-attention with the
last hidden states of the latents, using the outputs as queries, and the latents as keys and values.`),Ii=c(),z=r("p"),Ul=a(`So let\u2019s say one wants to perform masked language modeling (BERT-style) with the Perceiver. As the Perceiver\u2019s input
length will not have an impact on the computation time of the self-attention layers, one can provide raw bytes,
providing `),Vs=r("code"),Rl=a("inputs"),Kl=a(` of length 2048 to the model. If one now masks out certain of these 2048 tokens, one can define
the `),Us=r("code"),Jl=a("outputs"),Gl=a(" as being of shape: "),Rs=r("code"),Zl=a("(batch_size, 2048, 768)"),Xl=a(`. Next, one performs cross-attention with the final
hidden states of the latents to update the `),Ks=r("code"),Ql=a("outputs"),Yl=a(` tensor. After cross-attention, one still has a tensor of
shape `),Js=r("code"),ed=a("(batch_size, 2048, 768)"),td=a(`. One can then place a regular language modeling head on top, to project the last
dimension to the vocabulary size of the model, i.e. creating logits of shape `),Gs=r("code"),od=a("(batch_size, 2048, 262)"),rd=a(` (as
Perceiver uses a vocabulary size of 262 byte IDs).`),zi=c(),ge=r("p"),sd=a("This model was contributed by "),co=r("a"),nd=a("<nielsr>"),ad=a(". The original code can be found "),lo=r("a"),id=a("here"),cd=a("."),qi=c(),ze=r("h2"),kt=r("a"),Zs=r("span"),f(po.$$.fragment),ld=c(),Xs=r("span"),dd=a("Perceiver specific outputs"),ji=c(),qe=r("div"),f(mo.$$.fragment),pd=c(),Qs=r("p"),md=a("Base class for Perceiver base model\u2019s outputs, with potential hidden states, attentions and cross-attentions."),Ai=c(),je=r("div"),f(ho.$$.fragment),hd=c(),Ys=r("p"),fd=a("Base class for Perceiver decoder outputs, with potential cross-attentions."),Oi=c(),Ae=r("div"),f(fo.$$.fragment),ud=c(),en=r("p"),gd=a("Base class for Perceiver\u2019s masked language model outputs."),Di=c(),Oe=r("div"),f(uo.$$.fragment),vd=c(),tn=r("p"),_d=a(`Base class for Perceiver\u2019s outputs of sequence/image classification models, optical flow and multimodal
autoencoding.`),Ni=c(),De=r("h2"),Tt=r("a"),on=r("span"),f(go.$$.fragment),Pd=c(),rn=r("span"),wd=a("PerceiverConfig"),Li=c(),L=r("div"),f(vo.$$.fragment),bd=c(),Ne=r("p"),yd=a("This is the configuration class to store the configuration of a "),ts=r("a"),$d=a("PerceiverModel"),kd=a(`. It is used
to instantiate an Perceiver model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Perceiver
`),_o=r("a"),Td=a("deepmind/language-perceiver"),xd=a(" architecture."),Ed=c(),Le=r("p"),Fd=a("Configuration objects inherit from "),os=r("a"),Cd=a("PretrainedConfig"),Md=a(` and can be used to control the model
outputs. Read the documentation from `),rs=r("a"),Id=a("PretrainedConfig"),zd=a(" for more information."),qd=c(),sn=r("p"),jd=a("Example:"),Ad=c(),f(Po.$$.fragment),Si=c(),Se=r("h2"),xt=r("a"),nn=r("span"),f(wo.$$.fragment),Od=c(),an=r("span"),Dd=a("PerceiverTokenizer"),Hi=c(),I=r("div"),f(bo.$$.fragment),Nd=c(),cn=r("p"),Ld=a("Construct a Perceiver tokenizer. The Perceiver simply uses raw bytes utf-8 encoding."),Sd=c(),yo=r("p"),Hd=a("This tokenizer inherits from "),ss=r("a"),Bd=a("PreTrainedTokenizer"),Wd=a(` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Vd=c(),ve=r("div"),f($o.$$.fragment),Ud=c(),ln=r("p"),Rd=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks. A sequence has the
following format:`),Kd=c(),ko=r("ul"),ns=r("li"),Jd=a("single sequence: "),dn=r("code"),Gd=a("[CLS] X [SEP]"),Zd=c(),as=r("li"),Xd=a("pair of sequences: "),pn=r("code"),Qd=a("[CLS] A [SEP] B [SEP]"),Yd=c(),Et=r("div"),f(To.$$.fragment),ep=c(),xo=r("p"),tp=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),mn=r("code"),op=a("prepare_for_model"),rp=a(" method."),sp=c(),_e=r("div"),f(Eo.$$.fragment),np=c(),is=r("p"),ap=a("Create the token type IDs corresponding to the sequences passed. "),cs=r("a"),ip=a("What are token type IDs?"),cp=c(),hn=r("p"),lp=a("Should be overridden in a subclass if the model has a special way of building those."),dp=c(),fn=r("div"),Bi=c(),He=r("h2"),Ft=r("a"),un=r("span"),f(Fo.$$.fragment),pp=c(),gn=r("span"),mp=a("PerceiverFeatureExtractor"),Wi=c(),te=r("div"),f(Co.$$.fragment),hp=c(),vn=r("p"),fp=a("Constructs a Perceiver feature extractor."),up=c(),Mo=r("p"),gp=a("This feature extractor inherits from "),ls=r("a"),vp=a("ImageFeatureExtractionMixin"),_p=a(` which contains most of the
main methods. Users should refer to this superclass for more information regarding those methods.`),Pp=c(),Ct=r("div"),f(Io.$$.fragment),wp=c(),Be=r("p"),bp=a("Crops "),_n=r("code"),yp=a("image"),$p=a(" to "),Pn=r("em"),kp=a("self.crop_size"),Tp=a(` using a center crop. Note that if the image is too small to be cropped
to the size given, it will be padded (so the returned result has the size asked).`),Vi=c(),We=r("h2"),Mt=r("a"),wn=r("span"),f(zo.$$.fragment),xp=c(),bn=r("span"),Ep=a("PerceiverTextPreprocessor"),Ui=c(),Ve=r("div"),f(qo.$$.fragment),Fp=c(),yn=r("p"),Cp=a("Text preprocessing for Perceiver Encoder."),Ri=c(),Ue=r("h2"),It=r("a"),$n=r("span"),f(jo.$$.fragment),Mp=c(),kn=r("span"),Ip=a("PerceiverImagePreprocessor"),Ki=c(),ce=r("div"),f(Ao.$$.fragment),zp=c(),Tn=r("p"),qp=a("Image preprocessing for Perceiver Encoder."),jp=c(),oe=r("p"),Ap=a("Note: the "),xn=r("em"),Op=a("out_channels"),Dp=a(" argument refers to the output channels of a convolutional layer, if "),En=r("em"),Np=a("prep_type"),Lp=a(` is set to
\u201Cconv1x1\u201D or \u201Cconv\u201D. If one adds absolute position embeddings, one must make sure the `),Fn=r("em"),Sp=a("num_channels"),Hp=a(` of the
position encoding kwargs are set equal to the `),Cn=r("em"),Bp=a("out_channels"),Wp=a("."),Ji=c(),Re=r("h2"),zt=r("a"),Mn=r("span"),f(Oo.$$.fragment),Vp=c(),In=r("span"),Up=a("PerceiverOneHotPreprocessor"),Gi=c(),Ke=r("div"),f(Do.$$.fragment),Rp=c(),zn=r("p"),Kp=a("One-hot preprocessor for Perceiver Encoder. Can be used to add a dummy index dimension to the input."),Zi=c(),Je=r("h2"),qt=r("a"),qn=r("span"),f(No.$$.fragment),Jp=c(),jn=r("span"),Gp=a("PerceiverAudioPreprocessor"),Xi=c(),Ge=r("div"),f(Lo.$$.fragment),Zp=c(),An=r("p"),Xp=a("Audio preprocessing for Perceiver Encoder."),Qi=c(),Ze=r("h2"),jt=r("a"),On=r("span"),f(So.$$.fragment),Qp=c(),Dn=r("span"),Yp=a("PerceiverMultimodalPreprocessor"),Yi=c(),le=r("div"),f(Ho.$$.fragment),em=c(),Nn=r("p"),tm=a("Multimodal preprocessing for Perceiver Encoder."),om=c(),Ln=r("p"),rm=a(`Inputs for each modality are preprocessed, then padded with trainable position embeddings to have the same number
of channels.`),ec=c(),Xe=r("h2"),At=r("a"),Sn=r("span"),f(Bo.$$.fragment),sm=c(),Hn=r("span"),nm=a("PerceiverProjectionPostprocessor"),tc=c(),Qe=r("div"),f(Wo.$$.fragment),am=c(),Bn=r("p"),im=a(`Projection postprocessing for Perceiver. Can be used to convert the project the channels of the decoder output to a
lower dimension.`),oc=c(),Ye=r("h2"),Ot=r("a"),Wn=r("span"),f(Vo.$$.fragment),cm=c(),Vn=r("span"),lm=a("PerceiverAudioPostprocessor"),rc=c(),et=r("div"),f(Uo.$$.fragment),dm=c(),Un=r("p"),pm=a("Audio postprocessing for Perceiver. Can be used to convert the decoder output to audio features."),sc=c(),tt=r("h2"),Dt=r("a"),Rn=r("span"),f(Ro.$$.fragment),mm=c(),Kn=r("span"),hm=a("PerceiverClassificationPostprocessor"),nc=c(),ot=r("div"),f(Ko.$$.fragment),fm=c(),Jn=r("p"),um=a("Classification postprocessing for Perceiver. Can be used to convert the decoder output to classification logits."),ac=c(),rt=r("h2"),Nt=r("a"),Gn=r("span"),f(Jo.$$.fragment),gm=c(),Zn=r("span"),vm=a("PerceiverMultimodalPostprocessor"),ic=c(),st=r("div"),f(Go.$$.fragment),_m=c(),Xn=r("p"),Pm=a("Multimodal postprocessing for Perceiver."),cc=c(),nt=r("h2"),Lt=r("a"),Qn=r("span"),f(Zo.$$.fragment),wm=c(),Yn=r("span"),bm=a("PerceiverModel"),lc=c(),de=r("div"),f(Xo.$$.fragment),ym=c(),Qo=r("p"),$m=a(`The Perceiver: a scalable, fully attentional architecture.
This model is a PyTorch `),Yo=r("a"),km=a("torch.nn.Module"),Tm=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),xm=c(),B=r("div"),f(er.$$.fragment),Em=c(),at=r("p"),Fm=a("The "),ds=r("a"),Cm=a("PerceiverModel"),Mm=a(" forward method, overrides the "),ea=r("code"),Im=a("__call__"),zm=a(" special method."),qm=c(),f(St.$$.fragment),jm=c(),ta=r("p"),Am=a("Example:"),Om=c(),f(tr.$$.fragment),dc=c(),it=r("h2"),Ht=r("a"),oa=r("span"),f(or.$$.fragment),Dm=c(),ra=r("span"),Nm=a("PerceiverForMaskedLM"),pc=c(),pe=r("div"),f(rr.$$.fragment),Lm=c(),sr=r("p"),Sm=a(`Example use of Perceiver for masked language modeling.
This model is a PyTorch `),nr=r("a"),Hm=a("torch.nn.Module"),Bm=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Wm=c(),W=r("div"),f(ar.$$.fragment),Vm=c(),ct=r("p"),Um=a("The "),ps=r("a"),Rm=a("PerceiverForMaskedLM"),Km=a(" forward method, overrides the "),sa=r("code"),Jm=a("__call__"),Gm=a(" special method."),Zm=c(),f(Bt.$$.fragment),Xm=c(),na=r("p"),Qm=a("Example:"),Ym=c(),f(ir.$$.fragment),mc=c(),lt=r("h2"),Wt=r("a"),aa=r("span"),f(cr.$$.fragment),eh=c(),ia=r("span"),th=a("PerceiverForSequenceClassification"),hc=c(),me=r("div"),f(lr.$$.fragment),oh=c(),dr=r("p"),rh=a(`Example use of Perceiver for text classification.
This model is a PyTorch `),pr=r("a"),sh=a("torch.nn.Module"),nh=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ah=c(),q=r("div"),f(mr.$$.fragment),ih=c(),dt=r("p"),ch=a("The "),ms=r("a"),lh=a("PerceiverForSequenceClassification"),dh=a(" forward method, overrides the "),ca=r("code"),ph=a("__call__"),mh=a(" special method."),hh=c(),f(Vt.$$.fragment),fh=c(),la=r("p"),uh=a("Example of single-label classification:"),gh=c(),f(hr.$$.fragment),vh=c(),da=r("p"),_h=a("Example of multi-label classification:"),Ph=c(),f(fr.$$.fragment),fc=c(),pt=r("h2"),Ut=r("a"),pa=r("span"),f(ur.$$.fragment),wh=c(),ma=r("span"),bh=a("PerceiverForImageClassificationLearned"),uc=c(),A=r("div"),f(gr.$$.fragment),yh=c(),ha=r("p"),$h=a("Example use of Perceiver for image classification, for tasks such as ImageNet."),kh=c(),fa=r("p"),Th=a(`This model uses learned position embeddings. In other words, this model is not given any privileged information about
the structure of images. As shown in the paper, this model can achieve a top-1 accuracy of 72.7 on ImageNet.`),xh=c(),V=r("p"),ua=r("em"),Eh=a("PerceiverForImageClassificationLearned"),Fh=a(` uses
`),ga=r("em"),Ch=a("transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor"),Mh=a(" (with "),va=r("em"),Ih=a("prep_type"),zh=a(` = \u201Cconv1x1\u201D) to
preprocess the input images, and `),_a=r("em"),qh=a("transformers.models.perceiver.modeling_perceiver.PerceiverClassificationDecoder"),jh=a(` to
decode the latent representation of `),Pa=r("em"),Ah=a("~transformers.PerceiverModel"),Oh=a(" into classification logits."),Dh=c(),vr=r("p"),Nh=a("This model is a PyTorch "),_r=r("a"),Lh=a("torch.nn.Module"),Sh=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Hh=c(),U=r("div"),f(Pr.$$.fragment),Bh=c(),mt=r("p"),Wh=a("The "),hs=r("a"),Vh=a("PerceiverForImageClassificationLearned"),Uh=a(" forward method, overrides the "),wa=r("code"),Rh=a("__call__"),Kh=a(" special method."),Jh=c(),f(Rt.$$.fragment),Gh=c(),ba=r("p"),Zh=a("Examples:"),Xh=c(),f(wr.$$.fragment),gc=c(),ht=r("h2"),Kt=r("a"),ya=r("span"),f(br.$$.fragment),Qh=c(),$a=r("span"),Yh=a("PerceiverForImageClassificationFourier"),vc=c(),O=r("div"),f(yr.$$.fragment),ef=c(),ka=r("p"),tf=a("Example use of Perceiver for image classification, for tasks such as ImageNet."),of=c(),Ta=r("p"),rf=a(`This model uses fixed 2D Fourier position embeddings. As shown in the paper, this model can achieve a top-1 accuracy of
79.0 on ImageNet, and 84.5 when pre-trained on a large-scale dataset (i.e. JFT).`),sf=c(),R=r("p"),xa=r("em"),nf=a("PerceiverForImageClassificationLearned"),af=a(` uses
`),Ea=r("em"),cf=a("transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor"),lf=a(" (with "),Fa=r("em"),df=a("prep_type"),pf=a(` = \u201Cpixels\u201D) to
preprocess the input images, and `),Ca=r("em"),mf=a("transformers.models.perceiver.modeling_perceiver.PerceiverClassificationDecoder"),hf=a(` to
decode the latent representation of `),Ma=r("em"),ff=a("~transformers.PerceiverModel"),uf=a(" into classification logits."),gf=c(),$r=r("p"),vf=a("This model is a PyTorch "),kr=r("a"),_f=a("torch.nn.Module"),Pf=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),wf=c(),K=r("div"),f(Tr.$$.fragment),bf=c(),ft=r("p"),yf=a("The "),fs=r("a"),$f=a("PerceiverForImageClassificationFourier"),kf=a(" forward method, overrides the "),Ia=r("code"),Tf=a("__call__"),xf=a(" special method."),Ef=c(),f(Jt.$$.fragment),Ff=c(),za=r("p"),Cf=a("Examples:"),Mf=c(),f(xr.$$.fragment),_c=c(),ut=r("h2"),Gt=r("a"),qa=r("span"),f(Er.$$.fragment),If=c(),ja=r("span"),zf=a("PerceiverForImageClassificationConvProcessing"),Pc=c(),D=r("div"),f(Fr.$$.fragment),qf=c(),Aa=r("p"),jf=a("Example use of Perceiver for image classification, for tasks such as ImageNet."),Af=c(),Oa=r("p"),Of=a(`This model uses a 2D conv+maxpool preprocessing network. As shown in the paper, this model can achieve a top-1 accuracy
of 82.1 on ImageNet.`),Df=c(),J=r("p"),Da=r("em"),Nf=a("PerceiverForImageClassificationLearned"),Lf=a(` uses
`),Na=r("em"),Sf=a("transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor"),Hf=a(" (with "),La=r("em"),Bf=a("prep_type"),Wf=a(` = \u201Cconv\u201D) to preprocess
the input images, and `),Sa=r("em"),Vf=a("transformers.models.perceiver.modeling_perceiver.PerceiverClassificationDecoder"),Uf=a(` to decode the
latent representation of `),Ha=r("em"),Rf=a("~transformers.PerceiverModel"),Kf=a(" into classification logits."),Jf=c(),Cr=r("p"),Gf=a("This model is a PyTorch "),Mr=r("a"),Zf=a("torch.nn.Module"),Xf=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Qf=c(),G=r("div"),f(Ir.$$.fragment),Yf=c(),gt=r("p"),eu=a("The "),us=r("a"),tu=a("PerceiverForImageClassificationConvProcessing"),ou=a(" forward method, overrides the "),Ba=r("code"),ru=a("__call__"),su=a(" special method."),nu=c(),f(Zt.$$.fragment),au=c(),Wa=r("p"),iu=a("Examples:"),cu=c(),f(zr.$$.fragment),wc=c(),vt=r("h2"),Xt=r("a"),Va=r("span"),f(qr.$$.fragment),lu=c(),Ua=r("span"),du=a("PerceiverForOpticalFlow"),bc=c(),S=r("div"),f(jr.$$.fragment),pu=c(),H=r("p"),mu=a("Example use of Perceiver for optical flow, for tasks such as Sintel and KITTI. "),Ra=r("em"),hu=a("PerceiverForOpticalFlow"),fu=a(` uses
`),Ka=r("em"),uu=a("transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor"),gu=a(" (with "),Ja=r("em"),vu=a("prep_type"),_u=a(` = \u201Cpatches\u201D) to
preprocess the input images, and `),Ga=r("em"),Pu=a("transformers.models.perceiver.modeling_perceiver.PerceiverOpticalFlowDecoder"),wu=a(` to
decode the latent representation of `),Za=r("em"),bu=a("~transformers.PerceiverModel"),yu=a("."),$u=c(),Xa=r("p"),ku=a(`As input, one concatenates 2 subsequent frames along the channel dimension and extract a 3 x 3 patch around each pixel
(leading to 3 x 3 x 3 x 2 = 54 values for each pixel). Fixed Fourier position encodings are used to encode the position
of each pixel in the patch. Next, one applies the Perceiver encoder. To decode, one queries the latent representation
using the same encoding used for the input.`),Tu=c(),Ar=r("p"),xu=a("This model is a PyTorch "),Or=r("a"),Eu=a("torch.nn.Module"),Fu=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Cu=c(),Z=r("div"),f(Dr.$$.fragment),Mu=c(),_t=r("p"),Iu=a("The "),gs=r("a"),zu=a("PerceiverForOpticalFlow"),qu=a(" forward method, overrides the "),Qa=r("code"),ju=a("__call__"),Au=a(" special method."),Ou=c(),f(Qt.$$.fragment),Du=c(),Ya=r("p"),Nu=a("Examples:"),Lu=c(),f(Nr.$$.fragment),yc=c(),Pt=r("h2"),Yt=r("a"),ei=r("span"),f(Lr.$$.fragment),Su=c(),ti=r("span"),Hu=a("PerceiverForMultimodalAutoencoding"),$c=c(),F=r("div"),f(Sr.$$.fragment),Bu=c(),oi=r("p"),Wu=a("Example use of Perceiver for multimodal (video) autoencoding, for tasks such as Kinetics-700."),Vu=c(),eo=r("p"),ri=r("em"),Uu=a("PerceiverForMultimodalAutoencoding"),Ru=a(` uses
`),si=r("em"),Ku=a("transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor"),Ju=a(` to preprocess the 3 modalities:
images, audio and class labels. This preprocessor uses modality-specific preprocessors to preprocess every modality
separately, after which they are concatenated. Trainable position embeddings are used to pad each modality to the same
number of channels to make concatenation along the time dimension possible. Next, one applies the Perceiver encoder.`),Gu=c(),Pe=r("p"),ni=r("em"),Zu=a("transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalDecoder"),Xu=a(` is used to decode the latent
representation of `),ai=r("em"),Qu=a("~transformers.PerceiverModel"),Yu=a(`. This decoder uses each modality-specific decoder to construct
queries. The decoder queries are created based on the inputs after preprocessing. However, autoencoding an entire video
in a single forward pass is computationally infeasible, hence one only uses parts of the decoder queries to do
cross-attention with the latent representation. This is determined by the subsampled indices for each modality, which
can be provided as additional input to the forward pass of `),ii=r("em"),eg=a("PerceiverForMultimodalAutoencoding"),tg=a("."),og=c(),to=r("p"),ci=r("em"),rg=a("transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalDecoder"),sg=a(` also pads the decoder queries of the
different modalities to the same number of channels, in order to concatenate them along the time dimension. Next,
cross-attention is performed with the latent representation of `),li=r("em"),ng=a("PerceiverModel"),ag=a("."),ig=c(),Hr=r("p"),cg=a("Finally, "),di=r("em"),lg=a("transformers.models.perceiver.modeling_perceiver.PerceiverMultiModalPostprocessor"),dg=a(` is used to turn this
tensor into an actual video. It first splits up the output into the different modalities, and then applies the
respective postprocessor for each modality.`),pg=c(),pi=r("p"),mg=a(`Note that, by masking the classification label during evaluation (i.e. simply providing a tensor of zeros for the
\u201Clabel\u201D modality), this auto-encoding model becomes a Kinetics 700 video classifier.`),hg=c(),Br=r("p"),fg=a("This model is a PyTorch "),Wr=r("a"),ug=a("torch.nn.Module"),gg=a(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),vg=c(),X=r("div"),f(Vr.$$.fragment),_g=c(),wt=r("p"),Pg=a("The "),vs=r("a"),wg=a("PerceiverForMultimodalAutoencoding"),bg=a(" forward method, overrides the "),mi=r("code"),yg=a("__call__"),$g=a(" special method."),kg=c(),f(oo.$$.fragment),Tg=c(),hi=r("p"),xg=a("Examples:"),Eg=c(),f(Ur.$$.fragment),this.h()},l(t){const p=i1('[data-svelte="svelte-1phssyn"]',document.head);h=s(p,"META",{name:!0,content:!0}),p.forEach(o),k=l(t),w=s(t,"H1",{class:!0});var Rr=n(w);$=s(Rr,"A",{id:!0,class:!0,href:!0});var fi=n($);T=s(fi,"SPAN",{});var ui=n(T);u(y.$$.fragment,ui),ui.forEach(o),fi.forEach(o),b=l(Rr),E=s(Rr,"SPAN",{});var gi=n(E);wl=i(gi,"Perceiver"),gi.forEach(o),Rr.forEach(o),yi=l(t),Ie=s(t,"H2",{class:!0});var Kr=n(Ie);bt=s(Kr,"A",{id:!0,class:!0,href:!0});var vi=n(bt);Ds=s(vi,"SPAN",{});var _i=n(Ds);u(no.$$.fragment,_i),_i.forEach(o),vi.forEach(o),bl=l(Kr),Ns=s(Kr,"SPAN",{});var Pi=n(Ns);yl=i(Pi,"Overview"),Pi.forEach(o),Kr.forEach(o),$i=l(t),yt=s(t,"P",{});var Tc=n(yt);$l=i(Tc,"The Perceiver IO model was proposed in "),ao=s(Tc,"A",{href:!0,rel:!0});var Ig=n(ao);kl=i(Ig,"Perceiver IO: A General Architecture for Structured Inputs & Outputs"),Ig.forEach(o),Tl=i(Tc,` by Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch,
Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier H\xE9naff, Matthew M.
Botvinick, Andrew Zisserman, Oriol Vinyals, Jo\xE3o Carreira.`),Tc.forEach(o),ki=l(t),$t=s(t,"P",{});var xc=n($t);xl=i(xc,"Perceiver IO is a generalization of "),io=s(xc,"A",{href:!0,rel:!0});var zg=n(io);El=i(zg,"Perceiver"),zg.forEach(o),Fl=i(xc,` to handle arbitrary outputs in
addition to arbitrary inputs. The original Perceiver only produced a single classification label. In addition to
classification labels, Perceiver IO can produce (for example) language, optical flow, and multimodal videos with audio.
This is done using the same building blocks as the original Perceiver. The computational complexity of Perceiver IO is
linear in the input and output size and the bulk of the processing occurs in the latent space, allowing us to process
inputs and outputs that are much larger than can be handled by standard Transformers. This means, for example,
Perceiver IO can do BERT-style masked language modeling directly using bytes instead of tokenized inputs.`),xc.forEach(o),Ti=l(t),Gr=s(t,"P",{});var qg=n(Gr);Cl=i(qg,"The abstract from the paper is the following:"),qg.forEach(o),xi=l(t),Zr=s(t,"P",{});var jg=n(Zr);Ls=s(jg,"EM",{});var Ag=n(Ls);Ml=i(Ag,`The recently-proposed Perceiver model obtains good results on several domains (images, audio, multimodal, point
clouds) while scaling linearly in compute and memory with the input size. While the Perceiver supports many kinds of
inputs, it can only produce very simple outputs such as class scores. Perceiver IO overcomes this limitation without
sacrificing the original\u2019s appealing properties by learning to flexibly query the model\u2019s latent space to produce
outputs of arbitrary size and semantics. Perceiver IO still decouples model depth from data size and still scales
linearly with data size, but now with respect to both input and output sizes. The full Perceiver IO model achieves
strong results on tasks with highly structured output spaces, such as natural language and visual understanding,
StarCraft II, and multi-task and multi-modal domains. As highlights, Perceiver IO matches a Transformer-based BERT
baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art
performance on Sintel optical flow estimation.`),Ag.forEach(o),jg.forEach(o),Ei=l(t),Xr=s(t,"P",{});var Og=n(Xr);Il=i(Og,"Here\u2019s a TLDR explaining how Perceiver works:"),Og.forEach(o),Fi=l(t),Qr=s(t,"P",{});var Dg=n(Qr);zl=i(Dg,`The main problem with the self-attention mechanism of the Transformer is that the time and memory requirements scale
quadratically with the sequence length. Hence, models like BERT and RoBERTa are limited to a max sequence length of 512
tokens. Perceiver aims to solve this issue by, instead of performing self-attention on the inputs, perform it on a set
of latent variables, and only use the inputs for cross-attention. In this way, the time and memory requirements don\u2019t
depend on the length of the inputs anymore, as one uses a fixed amount of latent variables, like 256 or 512. These are
randomly initialized, after which they are trained end-to-end using backpropagation.`),Dg.forEach(o),Ci=l(t),N=s(t,"P",{});var re=n(N);ql=i(re,"Internally, "),Yr=s(re,"A",{href:!0});var Ng=n(Yr);jl=i(Ng,"PerceiverModel"),Ng.forEach(o),Al=i(re,` will create the latents, which is a tensor of shape
`),Ss=s(re,"CODE",{});var Lg=n(Ss);Ol=i(Lg,"(batch_size, num_latents, d_latents)"),Lg.forEach(o),Dl=i(re,". One must provide "),Hs=s(re,"CODE",{});var Sg=n(Hs);Nl=i(Sg,"inputs"),Sg.forEach(o),Ll=i(re,` (which could be text, images, audio, you
name it!) to the model, which it will use to perform cross-attention with the latents. The output of the Perceiver
encoder is a tensor of the same shape. One can then, similar to BERT, convert the last hidden states of the latents to
classification logits by averaging along the sequence dimension, and placing a linear layer on top of that to project
the `),Bs=s(re,"CODE",{});var Hg=n(Bs);Sl=i(Hg,"d_latents"),Hg.forEach(o),Hl=i(re," to "),Ws=s(re,"CODE",{});var Bg=n(Ws);Bl=i(Bg,"num_labels"),Bg.forEach(o),Wl=i(re,"."),re.forEach(o),Mi=l(t),es=s(t,"P",{});var Wg=n(es);Vl=i(Wg,`This was the idea of the original Perceiver paper. However, it could only output classification logits. In a follow-up
work, PerceiverIO, they generalized it to let the model also produce outputs of arbitrary size. How, you might ask? The
idea is actually relatively simple: one defines outputs of an arbitrary size, and then applies cross-attention with the
last hidden states of the latents, using the outputs as queries, and the latents as keys and values.`),Wg.forEach(o),Ii=l(t),z=s(t,"P",{});var Q=n(z);Ul=i(Q,`So let\u2019s say one wants to perform masked language modeling (BERT-style) with the Perceiver. As the Perceiver\u2019s input
length will not have an impact on the computation time of the self-attention layers, one can provide raw bytes,
providing `),Vs=s(Q,"CODE",{});var Vg=n(Vs);Rl=i(Vg,"inputs"),Vg.forEach(o),Kl=i(Q,` of length 2048 to the model. If one now masks out certain of these 2048 tokens, one can define
the `),Us=s(Q,"CODE",{});var Ug=n(Us);Jl=i(Ug,"outputs"),Ug.forEach(o),Gl=i(Q," as being of shape: "),Rs=s(Q,"CODE",{});var Rg=n(Rs);Zl=i(Rg,"(batch_size, 2048, 768)"),Rg.forEach(o),Xl=i(Q,`. Next, one performs cross-attention with the final
hidden states of the latents to update the `),Ks=s(Q,"CODE",{});var Kg=n(Ks);Ql=i(Kg,"outputs"),Kg.forEach(o),Yl=i(Q,` tensor. After cross-attention, one still has a tensor of
shape `),Js=s(Q,"CODE",{});var Jg=n(Js);ed=i(Jg,"(batch_size, 2048, 768)"),Jg.forEach(o),td=i(Q,`. One can then place a regular language modeling head on top, to project the last
dimension to the vocabulary size of the model, i.e. creating logits of shape `),Gs=s(Q,"CODE",{});var Gg=n(Gs);od=i(Gg,"(batch_size, 2048, 262)"),Gg.forEach(o),rd=i(Q,` (as
Perceiver uses a vocabulary size of 262 byte IDs).`),Q.forEach(o),zi=l(t),ge=s(t,"P",{});var _s=n(ge);sd=i(_s,"This model was contributed by "),co=s(_s,"A",{href:!0,rel:!0});var Zg=n(co);nd=i(Zg,"<nielsr>"),Zg.forEach(o),ad=i(_s,". The original code can be found "),lo=s(_s,"A",{href:!0,rel:!0});var Xg=n(lo);id=i(Xg,"here"),Xg.forEach(o),cd=i(_s,"."),_s.forEach(o),qi=l(t),ze=s(t,"H2",{class:!0});var Ec=n(ze);kt=s(Ec,"A",{id:!0,class:!0,href:!0});var Qg=n(kt);Zs=s(Qg,"SPAN",{});var Yg=n(Zs);u(po.$$.fragment,Yg),Yg.forEach(o),Qg.forEach(o),ld=l(Ec),Xs=s(Ec,"SPAN",{});var ev=n(Xs);dd=i(ev,"Perceiver specific outputs"),ev.forEach(o),Ec.forEach(o),ji=l(t),qe=s(t,"DIV",{class:!0});var Fc=n(qe);u(mo.$$.fragment,Fc),pd=l(Fc),Qs=s(Fc,"P",{});var tv=n(Qs);md=i(tv,"Base class for Perceiver base model\u2019s outputs, with potential hidden states, attentions and cross-attentions."),tv.forEach(o),Fc.forEach(o),Ai=l(t),je=s(t,"DIV",{class:!0});var Cc=n(je);u(ho.$$.fragment,Cc),hd=l(Cc),Ys=s(Cc,"P",{});var ov=n(Ys);fd=i(ov,"Base class for Perceiver decoder outputs, with potential cross-attentions."),ov.forEach(o),Cc.forEach(o),Oi=l(t),Ae=s(t,"DIV",{class:!0});var Mc=n(Ae);u(fo.$$.fragment,Mc),ud=l(Mc),en=s(Mc,"P",{});var rv=n(en);gd=i(rv,"Base class for Perceiver\u2019s masked language model outputs."),rv.forEach(o),Mc.forEach(o),Di=l(t),Oe=s(t,"DIV",{class:!0});var Ic=n(Oe);u(uo.$$.fragment,Ic),vd=l(Ic),tn=s(Ic,"P",{});var sv=n(tn);_d=i(sv,`Base class for Perceiver\u2019s outputs of sequence/image classification models, optical flow and multimodal
autoencoding.`),sv.forEach(o),Ic.forEach(o),Ni=l(t),De=s(t,"H2",{class:!0});var zc=n(De);Tt=s(zc,"A",{id:!0,class:!0,href:!0});var nv=n(Tt);on=s(nv,"SPAN",{});var av=n(on);u(go.$$.fragment,av),av.forEach(o),nv.forEach(o),Pd=l(zc),rn=s(zc,"SPAN",{});var iv=n(rn);wd=i(iv,"PerceiverConfig"),iv.forEach(o),zc.forEach(o),Li=l(t),L=s(t,"DIV",{class:!0});var we=n(L);u(vo.$$.fragment,we),bd=l(we),Ne=s(we,"P",{});var Ps=n(Ne);yd=i(Ps,"This is the configuration class to store the configuration of a "),ts=s(Ps,"A",{href:!0});var cv=n(ts);$d=i(cv,"PerceiverModel"),cv.forEach(o),kd=i(Ps,`. It is used
to instantiate an Perceiver model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the Perceiver
`),_o=s(Ps,"A",{href:!0,rel:!0});var lv=n(_o);Td=i(lv,"deepmind/language-perceiver"),lv.forEach(o),xd=i(Ps," architecture."),Ps.forEach(o),Ed=l(we),Le=s(we,"P",{});var ws=n(Le);Fd=i(ws,"Configuration objects inherit from "),os=s(ws,"A",{href:!0});var dv=n(os);Cd=i(dv,"PretrainedConfig"),dv.forEach(o),Md=i(ws,` and can be used to control the model
outputs. Read the documentation from `),rs=s(ws,"A",{href:!0});var pv=n(rs);Id=i(pv,"PretrainedConfig"),pv.forEach(o),zd=i(ws," for more information."),ws.forEach(o),qd=l(we),sn=s(we,"P",{});var mv=n(sn);jd=i(mv,"Example:"),mv.forEach(o),Ad=l(we),u(Po.$$.fragment,we),we.forEach(o),Si=l(t),Se=s(t,"H2",{class:!0});var qc=n(Se);xt=s(qc,"A",{id:!0,class:!0,href:!0});var hv=n(xt);nn=s(hv,"SPAN",{});var fv=n(nn);u(wo.$$.fragment,fv),fv.forEach(o),hv.forEach(o),Od=l(qc),an=s(qc,"SPAN",{});var uv=n(an);Dd=i(uv,"PerceiverTokenizer"),uv.forEach(o),qc.forEach(o),Hi=l(t),I=s(t,"DIV",{class:!0});var Y=n(I);u(bo.$$.fragment,Y),Nd=l(Y),cn=s(Y,"P",{});var gv=n(cn);Ld=i(gv,"Construct a Perceiver tokenizer. The Perceiver simply uses raw bytes utf-8 encoding."),gv.forEach(o),Sd=l(Y),yo=s(Y,"P",{});var jc=n(yo);Hd=i(jc,"This tokenizer inherits from "),ss=s(jc,"A",{href:!0});var vv=n(ss);Bd=i(vv,"PreTrainedTokenizer"),vv.forEach(o),Wd=i(jc,` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),jc.forEach(o),Vd=l(Y),ve=s(Y,"DIV",{class:!0});var bs=n(ve);u($o.$$.fragment,bs),Ud=l(bs),ln=s(bs,"P",{});var _v=n(ln);Rd=i(_v,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks. A sequence has the
following format:`),_v.forEach(o),Kd=l(bs),ko=s(bs,"UL",{});var Ac=n(ko);ns=s(Ac,"LI",{});var Fg=n(ns);Jd=i(Fg,"single sequence: "),dn=s(Fg,"CODE",{});var Pv=n(dn);Gd=i(Pv,"[CLS] X [SEP]"),Pv.forEach(o),Fg.forEach(o),Zd=l(Ac),as=s(Ac,"LI",{});var Cg=n(as);Xd=i(Cg,"pair of sequences: "),pn=s(Cg,"CODE",{});var wv=n(pn);Qd=i(wv,"[CLS] A [SEP] B [SEP]"),wv.forEach(o),Cg.forEach(o),Ac.forEach(o),bs.forEach(o),Yd=l(Y),Et=s(Y,"DIV",{class:!0});var Oc=n(Et);u(To.$$.fragment,Oc),ep=l(Oc),xo=s(Oc,"P",{});var Dc=n(xo);tp=i(Dc,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),mn=s(Dc,"CODE",{});var bv=n(mn);op=i(bv,"prepare_for_model"),bv.forEach(o),rp=i(Dc," method."),Dc.forEach(o),Oc.forEach(o),sp=l(Y),_e=s(Y,"DIV",{class:!0});var ys=n(_e);u(Eo.$$.fragment,ys),np=l(ys),is=s(ys,"P",{});var Mg=n(is);ap=i(Mg,"Create the token type IDs corresponding to the sequences passed. "),cs=s(Mg,"A",{href:!0});var yv=n(cs);ip=i(yv,"What are token type IDs?"),yv.forEach(o),Mg.forEach(o),cp=l(ys),hn=s(ys,"P",{});var $v=n(hn);lp=i($v,"Should be overridden in a subclass if the model has a special way of building those."),$v.forEach(o),ys.forEach(o),dp=l(Y),fn=s(Y,"DIV",{class:!0}),n(fn).forEach(o),Y.forEach(o),Bi=l(t),He=s(t,"H2",{class:!0});var Nc=n(He);Ft=s(Nc,"A",{id:!0,class:!0,href:!0});var kv=n(Ft);un=s(kv,"SPAN",{});var Tv=n(un);u(Fo.$$.fragment,Tv),Tv.forEach(o),kv.forEach(o),pp=l(Nc),gn=s(Nc,"SPAN",{});var xv=n(gn);mp=i(xv,"PerceiverFeatureExtractor"),xv.forEach(o),Nc.forEach(o),Wi=l(t),te=s(t,"DIV",{class:!0});var ro=n(te);u(Co.$$.fragment,ro),hp=l(ro),vn=s(ro,"P",{});var Ev=n(vn);fp=i(Ev,"Constructs a Perceiver feature extractor."),Ev.forEach(o),up=l(ro),Mo=s(ro,"P",{});var Lc=n(Mo);gp=i(Lc,"This feature extractor inherits from "),ls=s(Lc,"A",{href:!0});var Fv=n(ls);vp=i(Fv,"ImageFeatureExtractionMixin"),Fv.forEach(o),_p=i(Lc,` which contains most of the
main methods. Users should refer to this superclass for more information regarding those methods.`),Lc.forEach(o),Pp=l(ro),Ct=s(ro,"DIV",{class:!0});var Sc=n(Ct);u(Io.$$.fragment,Sc),wp=l(Sc),Be=s(Sc,"P",{});var $s=n(Be);bp=i($s,"Crops "),_n=s($s,"CODE",{});var Cv=n(_n);yp=i(Cv,"image"),Cv.forEach(o),$p=i($s," to "),Pn=s($s,"EM",{});var Mv=n(Pn);kp=i(Mv,"self.crop_size"),Mv.forEach(o),Tp=i($s,` using a center crop. Note that if the image is too small to be cropped
to the size given, it will be padded (so the returned result has the size asked).`),$s.forEach(o),Sc.forEach(o),ro.forEach(o),Vi=l(t),We=s(t,"H2",{class:!0});var Hc=n(We);Mt=s(Hc,"A",{id:!0,class:!0,href:!0});var Iv=n(Mt);wn=s(Iv,"SPAN",{});var zv=n(wn);u(zo.$$.fragment,zv),zv.forEach(o),Iv.forEach(o),xp=l(Hc),bn=s(Hc,"SPAN",{});var qv=n(bn);Ep=i(qv,"PerceiverTextPreprocessor"),qv.forEach(o),Hc.forEach(o),Ui=l(t),Ve=s(t,"DIV",{class:!0});var Bc=n(Ve);u(qo.$$.fragment,Bc),Fp=l(Bc),yn=s(Bc,"P",{});var jv=n(yn);Cp=i(jv,"Text preprocessing for Perceiver Encoder."),jv.forEach(o),Bc.forEach(o),Ri=l(t),Ue=s(t,"H2",{class:!0});var Wc=n(Ue);It=s(Wc,"A",{id:!0,class:!0,href:!0});var Av=n(It);$n=s(Av,"SPAN",{});var Ov=n($n);u(jo.$$.fragment,Ov),Ov.forEach(o),Av.forEach(o),Mp=l(Wc),kn=s(Wc,"SPAN",{});var Dv=n(kn);Ip=i(Dv,"PerceiverImagePreprocessor"),Dv.forEach(o),Wc.forEach(o),Ki=l(t),ce=s(t,"DIV",{class:!0});var ks=n(ce);u(Ao.$$.fragment,ks),zp=l(ks),Tn=s(ks,"P",{});var Nv=n(Tn);qp=i(Nv,"Image preprocessing for Perceiver Encoder."),Nv.forEach(o),jp=l(ks),oe=s(ks,"P",{});var be=n(oe);Ap=i(be,"Note: the "),xn=s(be,"EM",{});var Lv=n(xn);Op=i(Lv,"out_channels"),Lv.forEach(o),Dp=i(be," argument refers to the output channels of a convolutional layer, if "),En=s(be,"EM",{});var Sv=n(En);Np=i(Sv,"prep_type"),Sv.forEach(o),Lp=i(be,` is set to
\u201Cconv1x1\u201D or \u201Cconv\u201D. If one adds absolute position embeddings, one must make sure the `),Fn=s(be,"EM",{});var Hv=n(Fn);Sp=i(Hv,"num_channels"),Hv.forEach(o),Hp=i(be,` of the
position encoding kwargs are set equal to the `),Cn=s(be,"EM",{});var Bv=n(Cn);Bp=i(Bv,"out_channels"),Bv.forEach(o),Wp=i(be,"."),be.forEach(o),ks.forEach(o),Ji=l(t),Re=s(t,"H2",{class:!0});var Vc=n(Re);zt=s(Vc,"A",{id:!0,class:!0,href:!0});var Wv=n(zt);Mn=s(Wv,"SPAN",{});var Vv=n(Mn);u(Oo.$$.fragment,Vv),Vv.forEach(o),Wv.forEach(o),Vp=l(Vc),In=s(Vc,"SPAN",{});var Uv=n(In);Up=i(Uv,"PerceiverOneHotPreprocessor"),Uv.forEach(o),Vc.forEach(o),Gi=l(t),Ke=s(t,"DIV",{class:!0});var Uc=n(Ke);u(Do.$$.fragment,Uc),Rp=l(Uc),zn=s(Uc,"P",{});var Rv=n(zn);Kp=i(Rv,"One-hot preprocessor for Perceiver Encoder. Can be used to add a dummy index dimension to the input."),Rv.forEach(o),Uc.forEach(o),Zi=l(t),Je=s(t,"H2",{class:!0});var Rc=n(Je);qt=s(Rc,"A",{id:!0,class:!0,href:!0});var Kv=n(qt);qn=s(Kv,"SPAN",{});var Jv=n(qn);u(No.$$.fragment,Jv),Jv.forEach(o),Kv.forEach(o),Jp=l(Rc),jn=s(Rc,"SPAN",{});var Gv=n(jn);Gp=i(Gv,"PerceiverAudioPreprocessor"),Gv.forEach(o),Rc.forEach(o),Xi=l(t),Ge=s(t,"DIV",{class:!0});var Kc=n(Ge);u(Lo.$$.fragment,Kc),Zp=l(Kc),An=s(Kc,"P",{});var Zv=n(An);Xp=i(Zv,"Audio preprocessing for Perceiver Encoder."),Zv.forEach(o),Kc.forEach(o),Qi=l(t),Ze=s(t,"H2",{class:!0});var Jc=n(Ze);jt=s(Jc,"A",{id:!0,class:!0,href:!0});var Xv=n(jt);On=s(Xv,"SPAN",{});var Qv=n(On);u(So.$$.fragment,Qv),Qv.forEach(o),Xv.forEach(o),Qp=l(Jc),Dn=s(Jc,"SPAN",{});var Yv=n(Dn);Yp=i(Yv,"PerceiverMultimodalPreprocessor"),Yv.forEach(o),Jc.forEach(o),Yi=l(t),le=s(t,"DIV",{class:!0});var Ts=n(le);u(Ho.$$.fragment,Ts),em=l(Ts),Nn=s(Ts,"P",{});var e_=n(Nn);tm=i(e_,"Multimodal preprocessing for Perceiver Encoder."),e_.forEach(o),om=l(Ts),Ln=s(Ts,"P",{});var t_=n(Ln);rm=i(t_,`Inputs for each modality are preprocessed, then padded with trainable position embeddings to have the same number
of channels.`),t_.forEach(o),Ts.forEach(o),ec=l(t),Xe=s(t,"H2",{class:!0});var Gc=n(Xe);At=s(Gc,"A",{id:!0,class:!0,href:!0});var o_=n(At);Sn=s(o_,"SPAN",{});var r_=n(Sn);u(Bo.$$.fragment,r_),r_.forEach(o),o_.forEach(o),sm=l(Gc),Hn=s(Gc,"SPAN",{});var s_=n(Hn);nm=i(s_,"PerceiverProjectionPostprocessor"),s_.forEach(o),Gc.forEach(o),tc=l(t),Qe=s(t,"DIV",{class:!0});var Zc=n(Qe);u(Wo.$$.fragment,Zc),am=l(Zc),Bn=s(Zc,"P",{});var n_=n(Bn);im=i(n_,`Projection postprocessing for Perceiver. Can be used to convert the project the channels of the decoder output to a
lower dimension.`),n_.forEach(o),Zc.forEach(o),oc=l(t),Ye=s(t,"H2",{class:!0});var Xc=n(Ye);Ot=s(Xc,"A",{id:!0,class:!0,href:!0});var a_=n(Ot);Wn=s(a_,"SPAN",{});var i_=n(Wn);u(Vo.$$.fragment,i_),i_.forEach(o),a_.forEach(o),cm=l(Xc),Vn=s(Xc,"SPAN",{});var c_=n(Vn);lm=i(c_,"PerceiverAudioPostprocessor"),c_.forEach(o),Xc.forEach(o),rc=l(t),et=s(t,"DIV",{class:!0});var Qc=n(et);u(Uo.$$.fragment,Qc),dm=l(Qc),Un=s(Qc,"P",{});var l_=n(Un);pm=i(l_,"Audio postprocessing for Perceiver. Can be used to convert the decoder output to audio features."),l_.forEach(o),Qc.forEach(o),sc=l(t),tt=s(t,"H2",{class:!0});var Yc=n(tt);Dt=s(Yc,"A",{id:!0,class:!0,href:!0});var d_=n(Dt);Rn=s(d_,"SPAN",{});var p_=n(Rn);u(Ro.$$.fragment,p_),p_.forEach(o),d_.forEach(o),mm=l(Yc),Kn=s(Yc,"SPAN",{});var m_=n(Kn);hm=i(m_,"PerceiverClassificationPostprocessor"),m_.forEach(o),Yc.forEach(o),nc=l(t),ot=s(t,"DIV",{class:!0});var el=n(ot);u(Ko.$$.fragment,el),fm=l(el),Jn=s(el,"P",{});var h_=n(Jn);um=i(h_,"Classification postprocessing for Perceiver. Can be used to convert the decoder output to classification logits."),h_.forEach(o),el.forEach(o),ac=l(t),rt=s(t,"H2",{class:!0});var tl=n(rt);Nt=s(tl,"A",{id:!0,class:!0,href:!0});var f_=n(Nt);Gn=s(f_,"SPAN",{});var u_=n(Gn);u(Jo.$$.fragment,u_),u_.forEach(o),f_.forEach(o),gm=l(tl),Zn=s(tl,"SPAN",{});var g_=n(Zn);vm=i(g_,"PerceiverMultimodalPostprocessor"),g_.forEach(o),tl.forEach(o),ic=l(t),st=s(t,"DIV",{class:!0});var ol=n(st);u(Go.$$.fragment,ol),_m=l(ol),Xn=s(ol,"P",{});var v_=n(Xn);Pm=i(v_,"Multimodal postprocessing for Perceiver."),v_.forEach(o),ol.forEach(o),cc=l(t),nt=s(t,"H2",{class:!0});var rl=n(nt);Lt=s(rl,"A",{id:!0,class:!0,href:!0});var __=n(Lt);Qn=s(__,"SPAN",{});var P_=n(Qn);u(Zo.$$.fragment,P_),P_.forEach(o),__.forEach(o),wm=l(rl),Yn=s(rl,"SPAN",{});var w_=n(Yn);bm=i(w_,"PerceiverModel"),w_.forEach(o),rl.forEach(o),lc=l(t),de=s(t,"DIV",{class:!0});var xs=n(de);u(Xo.$$.fragment,xs),ym=l(xs),Qo=s(xs,"P",{});var sl=n(Qo);$m=i(sl,`The Perceiver: a scalable, fully attentional architecture.
This model is a PyTorch `),Yo=s(sl,"A",{href:!0,rel:!0});var b_=n(Yo);km=i(b_,"torch.nn.Module"),b_.forEach(o),Tm=i(sl,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),sl.forEach(o),xm=l(xs),B=s(xs,"DIV",{class:!0});var ye=n(B);u(er.$$.fragment,ye),Em=l(ye),at=s(ye,"P",{});var Es=n(at);Fm=i(Es,"The "),ds=s(Es,"A",{href:!0});var y_=n(ds);Cm=i(y_,"PerceiverModel"),y_.forEach(o),Mm=i(Es," forward method, overrides the "),ea=s(Es,"CODE",{});var $_=n(ea);Im=i($_,"__call__"),$_.forEach(o),zm=i(Es," special method."),Es.forEach(o),qm=l(ye),u(St.$$.fragment,ye),jm=l(ye),ta=s(ye,"P",{});var k_=n(ta);Am=i(k_,"Example:"),k_.forEach(o),Om=l(ye),u(tr.$$.fragment,ye),ye.forEach(o),xs.forEach(o),dc=l(t),it=s(t,"H2",{class:!0});var nl=n(it);Ht=s(nl,"A",{id:!0,class:!0,href:!0});var T_=n(Ht);oa=s(T_,"SPAN",{});var x_=n(oa);u(or.$$.fragment,x_),x_.forEach(o),T_.forEach(o),Dm=l(nl),ra=s(nl,"SPAN",{});var E_=n(ra);Nm=i(E_,"PerceiverForMaskedLM"),E_.forEach(o),nl.forEach(o),pc=l(t),pe=s(t,"DIV",{class:!0});var Fs=n(pe);u(rr.$$.fragment,Fs),Lm=l(Fs),sr=s(Fs,"P",{});var al=n(sr);Sm=i(al,`Example use of Perceiver for masked language modeling.
This model is a PyTorch `),nr=s(al,"A",{href:!0,rel:!0});var F_=n(nr);Hm=i(F_,"torch.nn.Module"),F_.forEach(o),Bm=i(al,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),al.forEach(o),Wm=l(Fs),W=s(Fs,"DIV",{class:!0});var $e=n(W);u(ar.$$.fragment,$e),Vm=l($e),ct=s($e,"P",{});var Cs=n(ct);Um=i(Cs,"The "),ps=s(Cs,"A",{href:!0});var C_=n(ps);Rm=i(C_,"PerceiverForMaskedLM"),C_.forEach(o),Km=i(Cs," forward method, overrides the "),sa=s(Cs,"CODE",{});var M_=n(sa);Jm=i(M_,"__call__"),M_.forEach(o),Gm=i(Cs," special method."),Cs.forEach(o),Zm=l($e),u(Bt.$$.fragment,$e),Xm=l($e),na=s($e,"P",{});var I_=n(na);Qm=i(I_,"Example:"),I_.forEach(o),Ym=l($e),u(ir.$$.fragment,$e),$e.forEach(o),Fs.forEach(o),mc=l(t),lt=s(t,"H2",{class:!0});var il=n(lt);Wt=s(il,"A",{id:!0,class:!0,href:!0});var z_=n(Wt);aa=s(z_,"SPAN",{});var q_=n(aa);u(cr.$$.fragment,q_),q_.forEach(o),z_.forEach(o),eh=l(il),ia=s(il,"SPAN",{});var j_=n(ia);th=i(j_,"PerceiverForSequenceClassification"),j_.forEach(o),il.forEach(o),hc=l(t),me=s(t,"DIV",{class:!0});var Ms=n(me);u(lr.$$.fragment,Ms),oh=l(Ms),dr=s(Ms,"P",{});var cl=n(dr);rh=i(cl,`Example use of Perceiver for text classification.
This model is a PyTorch `),pr=s(cl,"A",{href:!0,rel:!0});var A_=n(pr);sh=i(A_,"torch.nn.Module"),A_.forEach(o),nh=i(cl,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),cl.forEach(o),ah=l(Ms),q=s(Ms,"DIV",{class:!0});var ee=n(q);u(mr.$$.fragment,ee),ih=l(ee),dt=s(ee,"P",{});var Is=n(dt);ch=i(Is,"The "),ms=s(Is,"A",{href:!0});var O_=n(ms);lh=i(O_,"PerceiverForSequenceClassification"),O_.forEach(o),dh=i(Is," forward method, overrides the "),ca=s(Is,"CODE",{});var D_=n(ca);ph=i(D_,"__call__"),D_.forEach(o),mh=i(Is," special method."),Is.forEach(o),hh=l(ee),u(Vt.$$.fragment,ee),fh=l(ee),la=s(ee,"P",{});var N_=n(la);uh=i(N_,"Example of single-label classification:"),N_.forEach(o),gh=l(ee),u(hr.$$.fragment,ee),vh=l(ee),da=s(ee,"P",{});var L_=n(da);_h=i(L_,"Example of multi-label classification:"),L_.forEach(o),Ph=l(ee),u(fr.$$.fragment,ee),ee.forEach(o),Ms.forEach(o),fc=l(t),pt=s(t,"H2",{class:!0});var ll=n(pt);Ut=s(ll,"A",{id:!0,class:!0,href:!0});var S_=n(Ut);pa=s(S_,"SPAN",{});var H_=n(pa);u(ur.$$.fragment,H_),H_.forEach(o),S_.forEach(o),wh=l(ll),ma=s(ll,"SPAN",{});var B_=n(ma);bh=i(B_,"PerceiverForImageClassificationLearned"),B_.forEach(o),ll.forEach(o),uc=l(t),A=s(t,"DIV",{class:!0});var se=n(A);u(gr.$$.fragment,se),yh=l(se),ha=s(se,"P",{});var W_=n(ha);$h=i(W_,"Example use of Perceiver for image classification, for tasks such as ImageNet."),W_.forEach(o),kh=l(se),fa=s(se,"P",{});var V_=n(fa);Th=i(V_,`This model uses learned position embeddings. In other words, this model is not given any privileged information about
the structure of images. As shown in the paper, this model can achieve a top-1 accuracy of 72.7 on ImageNet.`),V_.forEach(o),xh=l(se),V=s(se,"P",{});var he=n(V);ua=s(he,"EM",{});var U_=n(ua);Eh=i(U_,"PerceiverForImageClassificationLearned"),U_.forEach(o),Fh=i(he,` uses
`),ga=s(he,"EM",{});var R_=n(ga);Ch=i(R_,"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor"),R_.forEach(o),Mh=i(he," (with "),va=s(he,"EM",{});var K_=n(va);Ih=i(K_,"prep_type"),K_.forEach(o),zh=i(he,` = \u201Cconv1x1\u201D) to
preprocess the input images, and `),_a=s(he,"EM",{});var J_=n(_a);qh=i(J_,"transformers.models.perceiver.modeling_perceiver.PerceiverClassificationDecoder"),J_.forEach(o),jh=i(he,` to
decode the latent representation of `),Pa=s(he,"EM",{});var G_=n(Pa);Ah=i(G_,"~transformers.PerceiverModel"),G_.forEach(o),Oh=i(he," into classification logits."),he.forEach(o),Dh=l(se),vr=s(se,"P",{});var dl=n(vr);Nh=i(dl,"This model is a PyTorch "),_r=s(dl,"A",{href:!0,rel:!0});var Z_=n(_r);Lh=i(Z_,"torch.nn.Module"),Z_.forEach(o),Sh=i(dl,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),dl.forEach(o),Hh=l(se),U=s(se,"DIV",{class:!0});var ke=n(U);u(Pr.$$.fragment,ke),Bh=l(ke),mt=s(ke,"P",{});var zs=n(mt);Wh=i(zs,"The "),hs=s(zs,"A",{href:!0});var X_=n(hs);Vh=i(X_,"PerceiverForImageClassificationLearned"),X_.forEach(o),Uh=i(zs," forward method, overrides the "),wa=s(zs,"CODE",{});var Q_=n(wa);Rh=i(Q_,"__call__"),Q_.forEach(o),Kh=i(zs," special method."),zs.forEach(o),Jh=l(ke),u(Rt.$$.fragment,ke),Gh=l(ke),ba=s(ke,"P",{});var Y_=n(ba);Zh=i(Y_,"Examples:"),Y_.forEach(o),Xh=l(ke),u(wr.$$.fragment,ke),ke.forEach(o),se.forEach(o),gc=l(t),ht=s(t,"H2",{class:!0});var pl=n(ht);Kt=s(pl,"A",{id:!0,class:!0,href:!0});var eP=n(Kt);ya=s(eP,"SPAN",{});var tP=n(ya);u(br.$$.fragment,tP),tP.forEach(o),eP.forEach(o),Qh=l(pl),$a=s(pl,"SPAN",{});var oP=n($a);Yh=i(oP,"PerceiverForImageClassificationFourier"),oP.forEach(o),pl.forEach(o),vc=l(t),O=s(t,"DIV",{class:!0});var ne=n(O);u(yr.$$.fragment,ne),ef=l(ne),ka=s(ne,"P",{});var rP=n(ka);tf=i(rP,"Example use of Perceiver for image classification, for tasks such as ImageNet."),rP.forEach(o),of=l(ne),Ta=s(ne,"P",{});var sP=n(Ta);rf=i(sP,`This model uses fixed 2D Fourier position embeddings. As shown in the paper, this model can achieve a top-1 accuracy of
79.0 on ImageNet, and 84.5 when pre-trained on a large-scale dataset (i.e. JFT).`),sP.forEach(o),sf=l(ne),R=s(ne,"P",{});var fe=n(R);xa=s(fe,"EM",{});var nP=n(xa);nf=i(nP,"PerceiverForImageClassificationLearned"),nP.forEach(o),af=i(fe,` uses
`),Ea=s(fe,"EM",{});var aP=n(Ea);cf=i(aP,"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor"),aP.forEach(o),lf=i(fe," (with "),Fa=s(fe,"EM",{});var iP=n(Fa);df=i(iP,"prep_type"),iP.forEach(o),pf=i(fe,` = \u201Cpixels\u201D) to
preprocess the input images, and `),Ca=s(fe,"EM",{});var cP=n(Ca);mf=i(cP,"transformers.models.perceiver.modeling_perceiver.PerceiverClassificationDecoder"),cP.forEach(o),hf=i(fe,` to
decode the latent representation of `),Ma=s(fe,"EM",{});var lP=n(Ma);ff=i(lP,"~transformers.PerceiverModel"),lP.forEach(o),uf=i(fe," into classification logits."),fe.forEach(o),gf=l(ne),$r=s(ne,"P",{});var ml=n($r);vf=i(ml,"This model is a PyTorch "),kr=s(ml,"A",{href:!0,rel:!0});var dP=n(kr);_f=i(dP,"torch.nn.Module"),dP.forEach(o),Pf=i(ml,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ml.forEach(o),wf=l(ne),K=s(ne,"DIV",{class:!0});var Te=n(K);u(Tr.$$.fragment,Te),bf=l(Te),ft=s(Te,"P",{});var qs=n(ft);yf=i(qs,"The "),fs=s(qs,"A",{href:!0});var pP=n(fs);$f=i(pP,"PerceiverForImageClassificationFourier"),pP.forEach(o),kf=i(qs," forward method, overrides the "),Ia=s(qs,"CODE",{});var mP=n(Ia);Tf=i(mP,"__call__"),mP.forEach(o),xf=i(qs," special method."),qs.forEach(o),Ef=l(Te),u(Jt.$$.fragment,Te),Ff=l(Te),za=s(Te,"P",{});var hP=n(za);Cf=i(hP,"Examples:"),hP.forEach(o),Mf=l(Te),u(xr.$$.fragment,Te),Te.forEach(o),ne.forEach(o),_c=l(t),ut=s(t,"H2",{class:!0});var hl=n(ut);Gt=s(hl,"A",{id:!0,class:!0,href:!0});var fP=n(Gt);qa=s(fP,"SPAN",{});var uP=n(qa);u(Er.$$.fragment,uP),uP.forEach(o),fP.forEach(o),If=l(hl),ja=s(hl,"SPAN",{});var gP=n(ja);zf=i(gP,"PerceiverForImageClassificationConvProcessing"),gP.forEach(o),hl.forEach(o),Pc=l(t),D=s(t,"DIV",{class:!0});var ae=n(D);u(Fr.$$.fragment,ae),qf=l(ae),Aa=s(ae,"P",{});var vP=n(Aa);jf=i(vP,"Example use of Perceiver for image classification, for tasks such as ImageNet."),vP.forEach(o),Af=l(ae),Oa=s(ae,"P",{});var _P=n(Oa);Of=i(_P,`This model uses a 2D conv+maxpool preprocessing network. As shown in the paper, this model can achieve a top-1 accuracy
of 82.1 on ImageNet.`),_P.forEach(o),Df=l(ae),J=s(ae,"P",{});var ue=n(J);Da=s(ue,"EM",{});var PP=n(Da);Nf=i(PP,"PerceiverForImageClassificationLearned"),PP.forEach(o),Lf=i(ue,` uses
`),Na=s(ue,"EM",{});var wP=n(Na);Sf=i(wP,"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor"),wP.forEach(o),Hf=i(ue," (with "),La=s(ue,"EM",{});var bP=n(La);Bf=i(bP,"prep_type"),bP.forEach(o),Wf=i(ue,` = \u201Cconv\u201D) to preprocess
the input images, and `),Sa=s(ue,"EM",{});var yP=n(Sa);Vf=i(yP,"transformers.models.perceiver.modeling_perceiver.PerceiverClassificationDecoder"),yP.forEach(o),Uf=i(ue,` to decode the
latent representation of `),Ha=s(ue,"EM",{});var $P=n(Ha);Rf=i($P,"~transformers.PerceiverModel"),$P.forEach(o),Kf=i(ue," into classification logits."),ue.forEach(o),Jf=l(ae),Cr=s(ae,"P",{});var fl=n(Cr);Gf=i(fl,"This model is a PyTorch "),Mr=s(fl,"A",{href:!0,rel:!0});var kP=n(Mr);Zf=i(kP,"torch.nn.Module"),kP.forEach(o),Xf=i(fl,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),fl.forEach(o),Qf=l(ae),G=s(ae,"DIV",{class:!0});var xe=n(G);u(Ir.$$.fragment,xe),Yf=l(xe),gt=s(xe,"P",{});var js=n(gt);eu=i(js,"The "),us=s(js,"A",{href:!0});var TP=n(us);tu=i(TP,"PerceiverForImageClassificationConvProcessing"),TP.forEach(o),ou=i(js," forward method, overrides the "),Ba=s(js,"CODE",{});var xP=n(Ba);ru=i(xP,"__call__"),xP.forEach(o),su=i(js," special method."),js.forEach(o),nu=l(xe),u(Zt.$$.fragment,xe),au=l(xe),Wa=s(xe,"P",{});var EP=n(Wa);iu=i(EP,"Examples:"),EP.forEach(o),cu=l(xe),u(zr.$$.fragment,xe),xe.forEach(o),ae.forEach(o),wc=l(t),vt=s(t,"H2",{class:!0});var ul=n(vt);Xt=s(ul,"A",{id:!0,class:!0,href:!0});var FP=n(Xt);Va=s(FP,"SPAN",{});var CP=n(Va);u(qr.$$.fragment,CP),CP.forEach(o),FP.forEach(o),lu=l(ul),Ua=s(ul,"SPAN",{});var MP=n(Ua);du=i(MP,"PerceiverForOpticalFlow"),MP.forEach(o),ul.forEach(o),bc=l(t),S=s(t,"DIV",{class:!0});var Ee=n(S);u(jr.$$.fragment,Ee),pu=l(Ee),H=s(Ee,"P",{});var ie=n(H);mu=i(ie,"Example use of Perceiver for optical flow, for tasks such as Sintel and KITTI. "),Ra=s(ie,"EM",{});var IP=n(Ra);hu=i(IP,"PerceiverForOpticalFlow"),IP.forEach(o),fu=i(ie,` uses
`),Ka=s(ie,"EM",{});var zP=n(Ka);uu=i(zP,"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor"),zP.forEach(o),gu=i(ie," (with "),Ja=s(ie,"EM",{});var qP=n(Ja);vu=i(qP,"prep_type"),qP.forEach(o),_u=i(ie,` = \u201Cpatches\u201D) to
preprocess the input images, and `),Ga=s(ie,"EM",{});var jP=n(Ga);Pu=i(jP,"transformers.models.perceiver.modeling_perceiver.PerceiverOpticalFlowDecoder"),jP.forEach(o),wu=i(ie,` to
decode the latent representation of `),Za=s(ie,"EM",{});var AP=n(Za);bu=i(AP,"~transformers.PerceiverModel"),AP.forEach(o),yu=i(ie,"."),ie.forEach(o),$u=l(Ee),Xa=s(Ee,"P",{});var OP=n(Xa);ku=i(OP,`As input, one concatenates 2 subsequent frames along the channel dimension and extract a 3 x 3 patch around each pixel
(leading to 3 x 3 x 3 x 2 = 54 values for each pixel). Fixed Fourier position encodings are used to encode the position
of each pixel in the patch. Next, one applies the Perceiver encoder. To decode, one queries the latent representation
using the same encoding used for the input.`),OP.forEach(o),Tu=l(Ee),Ar=s(Ee,"P",{});var gl=n(Ar);xu=i(gl,"This model is a PyTorch "),Or=s(gl,"A",{href:!0,rel:!0});var DP=n(Or);Eu=i(DP,"torch.nn.Module"),DP.forEach(o),Fu=i(gl,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),gl.forEach(o),Cu=l(Ee),Z=s(Ee,"DIV",{class:!0});var Fe=n(Z);u(Dr.$$.fragment,Fe),Mu=l(Fe),_t=s(Fe,"P",{});var As=n(_t);Iu=i(As,"The "),gs=s(As,"A",{href:!0});var NP=n(gs);zu=i(NP,"PerceiverForOpticalFlow"),NP.forEach(o),qu=i(As," forward method, overrides the "),Qa=s(As,"CODE",{});var LP=n(Qa);ju=i(LP,"__call__"),LP.forEach(o),Au=i(As," special method."),As.forEach(o),Ou=l(Fe),u(Qt.$$.fragment,Fe),Du=l(Fe),Ya=s(Fe,"P",{});var SP=n(Ya);Nu=i(SP,"Examples:"),SP.forEach(o),Lu=l(Fe),u(Nr.$$.fragment,Fe),Fe.forEach(o),Ee.forEach(o),yc=l(t),Pt=s(t,"H2",{class:!0});var vl=n(Pt);Yt=s(vl,"A",{id:!0,class:!0,href:!0});var HP=n(Yt);ei=s(HP,"SPAN",{});var BP=n(ei);u(Lr.$$.fragment,BP),BP.forEach(o),HP.forEach(o),Su=l(vl),ti=s(vl,"SPAN",{});var WP=n(ti);Hu=i(WP,"PerceiverForMultimodalAutoencoding"),WP.forEach(o),vl.forEach(o),$c=l(t),F=s(t,"DIV",{class:!0});var j=n(F);u(Sr.$$.fragment,j),Bu=l(j),oi=s(j,"P",{});var VP=n(oi);Wu=i(VP,"Example use of Perceiver for multimodal (video) autoencoding, for tasks such as Kinetics-700."),VP.forEach(o),Vu=l(j),eo=s(j,"P",{});var wi=n(eo);ri=s(wi,"EM",{});var UP=n(ri);Uu=i(UP,"PerceiverForMultimodalAutoencoding"),UP.forEach(o),Ru=i(wi,` uses
`),si=s(wi,"EM",{});var RP=n(si);Ku=i(RP,"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor"),RP.forEach(o),Ju=i(wi,` to preprocess the 3 modalities:
images, audio and class labels. This preprocessor uses modality-specific preprocessors to preprocess every modality
separately, after which they are concatenated. Trainable position embeddings are used to pad each modality to the same
number of channels to make concatenation along the time dimension possible. Next, one applies the Perceiver encoder.`),wi.forEach(o),Gu=l(j),Pe=s(j,"P",{});var Jr=n(Pe);ni=s(Jr,"EM",{});var KP=n(ni);Zu=i(KP,"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalDecoder"),KP.forEach(o),Xu=i(Jr,` is used to decode the latent
representation of `),ai=s(Jr,"EM",{});var JP=n(ai);Qu=i(JP,"~transformers.PerceiverModel"),JP.forEach(o),Yu=i(Jr,`. This decoder uses each modality-specific decoder to construct
queries. The decoder queries are created based on the inputs after preprocessing. However, autoencoding an entire video
in a single forward pass is computationally infeasible, hence one only uses parts of the decoder queries to do
cross-attention with the latent representation. This is determined by the subsampled indices for each modality, which
can be provided as additional input to the forward pass of `),ii=s(Jr,"EM",{});var GP=n(ii);eg=i(GP,"PerceiverForMultimodalAutoencoding"),GP.forEach(o),tg=i(Jr,"."),Jr.forEach(o),og=l(j),to=s(j,"P",{});var bi=n(to);ci=s(bi,"EM",{});var ZP=n(ci);rg=i(ZP,"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalDecoder"),ZP.forEach(o),sg=i(bi,` also pads the decoder queries of the
different modalities to the same number of channels, in order to concatenate them along the time dimension. Next,
cross-attention is performed with the latent representation of `),li=s(bi,"EM",{});var XP=n(li);ng=i(XP,"PerceiverModel"),XP.forEach(o),ag=i(bi,"."),bi.forEach(o),ig=l(j),Hr=s(j,"P",{});var _l=n(Hr);cg=i(_l,"Finally, "),di=s(_l,"EM",{});var QP=n(di);lg=i(QP,"transformers.models.perceiver.modeling_perceiver.PerceiverMultiModalPostprocessor"),QP.forEach(o),dg=i(_l,` is used to turn this
tensor into an actual video. It first splits up the output into the different modalities, and then applies the
respective postprocessor for each modality.`),_l.forEach(o),pg=l(j),pi=s(j,"P",{});var YP=n(pi);mg=i(YP,`Note that, by masking the classification label during evaluation (i.e. simply providing a tensor of zeros for the
\u201Clabel\u201D modality), this auto-encoding model becomes a Kinetics 700 video classifier.`),YP.forEach(o),hg=l(j),Br=s(j,"P",{});var Pl=n(Br);fg=i(Pl,"This model is a PyTorch "),Wr=s(Pl,"A",{href:!0,rel:!0});var e1=n(Wr);ug=i(e1,"torch.nn.Module"),e1.forEach(o),gg=i(Pl,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Pl.forEach(o),vg=l(j),X=s(j,"DIV",{class:!0});var Ce=n(X);u(Vr.$$.fragment,Ce),_g=l(Ce),wt=s(Ce,"P",{});var Os=n(wt);Pg=i(Os,"The "),vs=s(Os,"A",{href:!0});var t1=n(vs);wg=i(t1,"PerceiverForMultimodalAutoencoding"),t1.forEach(o),bg=i(Os," forward method, overrides the "),mi=s(Os,"CODE",{});var o1=n(mi);yg=i(o1,"__call__"),o1.forEach(o),$g=i(Os," special method."),Os.forEach(o),kg=l(Ce),u(oo.$$.fragment,Ce),Tg=l(Ce),hi=s(Ce,"P",{});var r1=n(hi);xg=i(r1,"Examples:"),r1.forEach(o),Eg=l(Ce),u(Ur.$$.fragment,Ce),Ce.forEach(o),j.forEach(o),this.h()},h(){d(h,"name","hf:doc:metadata"),d(h,"content",JSON.stringify(v1)),d($,"id","perceiver"),d($,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($,"href","#perceiver"),d(w,"class","relative group"),d(bt,"id","overview"),d(bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(bt,"href","#overview"),d(Ie,"class","relative group"),d(ao,"href","https://arxiv.org/abs/2107.14795"),d(ao,"rel","nofollow"),d(io,"href","https://arxiv.org/abs/2103.03206"),d(io,"rel","nofollow"),d(Yr,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel"),d(co,"href","https://huggingface.co/nielsr"),d(co,"rel","nofollow"),d(lo,"href","https://github.com/deepmind/deepmind-research/tree/master/perceiver"),d(lo,"rel","nofollow"),d(kt,"id","transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput"),d(kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(kt,"href","#transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput"),d(ze,"class","relative group"),d(qe,"class","docstring"),d(je,"class","docstring"),d(Ae,"class","docstring"),d(Oe,"class","docstring"),d(Tt,"id","transformers.PerceiverConfig"),d(Tt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Tt,"href","#transformers.PerceiverConfig"),d(De,"class","relative group"),d(ts,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel"),d(_o,"href","https://huggingface.co/deepmind/language-perceiver"),d(_o,"rel","nofollow"),d(os,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),d(rs,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),d(L,"class","docstring"),d(xt,"id","transformers.PerceiverTokenizer"),d(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xt,"href","#transformers.PerceiverTokenizer"),d(Se,"class","relative group"),d(ss,"href","/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(ve,"class","docstring"),d(Et,"class","docstring"),d(cs,"href","../glossary#token-type-ids"),d(_e,"class","docstring"),d(fn,"class","docstring"),d(I,"class","docstring"),d(Ft,"id","transformers.PerceiverFeatureExtractor"),d(Ft,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ft,"href","#transformers.PerceiverFeatureExtractor"),d(He,"class","relative group"),d(ls,"href","/docs/transformers/master/en/main_classes/feature_extractor#transformers.ImageFeatureExtractionMixin"),d(Ct,"class","docstring"),d(te,"class","docstring"),d(Mt,"id","transformers.models.perceiver.modeling_perceiver.PerceiverTextPreprocessor"),d(Mt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Mt,"href","#transformers.models.perceiver.modeling_perceiver.PerceiverTextPreprocessor"),d(We,"class","relative group"),d(Ve,"class","docstring"),d(It,"id","transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor"),d(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(It,"href","#transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor"),d(Ue,"class","relative group"),d(ce,"class","docstring"),d(zt,"id","transformers.models.perceiver.modeling_perceiver.PerceiverOneHotPreprocessor"),d(zt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(zt,"href","#transformers.models.perceiver.modeling_perceiver.PerceiverOneHotPreprocessor"),d(Re,"class","relative group"),d(Ke,"class","docstring"),d(qt,"id","transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor"),d(qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qt,"href","#transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor"),d(Je,"class","relative group"),d(Ge,"class","docstring"),d(jt,"id","transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor"),d(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(jt,"href","#transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor"),d(Ze,"class","relative group"),d(le,"class","docstring"),d(At,"id","transformers.models.perceiver.modeling_perceiver.PerceiverProjectionPostprocessor"),d(At,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(At,"href","#transformers.models.perceiver.modeling_perceiver.PerceiverProjectionPostprocessor"),d(Xe,"class","relative group"),d(Qe,"class","docstring"),d(Ot,"id","transformers.models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor"),d(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ot,"href","#transformers.models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor"),d(Ye,"class","relative group"),d(et,"class","docstring"),d(Dt,"id","transformers.models.perceiver.modeling_perceiver.PerceiverClassificationPostprocessor"),d(Dt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Dt,"href","#transformers.models.perceiver.modeling_perceiver.PerceiverClassificationPostprocessor"),d(tt,"class","relative group"),d(ot,"class","docstring"),d(Nt,"id","transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPostprocessor"),d(Nt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Nt,"href","#transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPostprocessor"),d(rt,"class","relative group"),d(st,"class","docstring"),d(Lt,"id","transformers.PerceiverModel"),d(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Lt,"href","#transformers.PerceiverModel"),d(nt,"class","relative group"),d(Yo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Yo,"rel","nofollow"),d(ds,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel"),d(B,"class","docstring"),d(de,"class","docstring"),d(Ht,"id","transformers.PerceiverForMaskedLM"),d(Ht,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ht,"href","#transformers.PerceiverForMaskedLM"),d(it,"class","relative group"),d(nr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(nr,"rel","nofollow"),d(ps,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),d(W,"class","docstring"),d(pe,"class","docstring"),d(Wt,"id","transformers.PerceiverForSequenceClassification"),d(Wt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Wt,"href","#transformers.PerceiverForSequenceClassification"),d(lt,"class","relative group"),d(pr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(pr,"rel","nofollow"),d(ms,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),d(q,"class","docstring"),d(me,"class","docstring"),d(Ut,"id","transformers.PerceiverForImageClassificationLearned"),d(Ut,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ut,"href","#transformers.PerceiverForImageClassificationLearned"),d(pt,"class","relative group"),d(_r,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(_r,"rel","nofollow"),d(hs,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),d(U,"class","docstring"),d(A,"class","docstring"),d(Kt,"id","transformers.PerceiverForImageClassificationFourier"),d(Kt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Kt,"href","#transformers.PerceiverForImageClassificationFourier"),d(ht,"class","relative group"),d(kr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(kr,"rel","nofollow"),d(fs,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),d(K,"class","docstring"),d(O,"class","docstring"),d(Gt,"id","transformers.PerceiverForImageClassificationConvProcessing"),d(Gt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Gt,"href","#transformers.PerceiverForImageClassificationConvProcessing"),d(ut,"class","relative group"),d(Mr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Mr,"rel","nofollow"),d(us,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),d(G,"class","docstring"),d(D,"class","docstring"),d(Xt,"id","transformers.PerceiverForOpticalFlow"),d(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xt,"href","#transformers.PerceiverForOpticalFlow"),d(vt,"class","relative group"),d(Or,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Or,"rel","nofollow"),d(gs,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForOpticalFlow"),d(Z,"class","docstring"),d(S,"class","docstring"),d(Yt,"id","transformers.PerceiverForMultimodalAutoencoding"),d(Yt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Yt,"href","#transformers.PerceiverForMultimodalAutoencoding"),d(Pt,"class","relative group"),d(Wr,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),d(Wr,"rel","nofollow"),d(vs,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMultimodalAutoencoding"),d(X,"class","docstring"),d(F,"class","docstring")},m(t,p){e(document.head,h),m(t,k,p),m(t,w,p),e(w,$),e($,T),g(y,T,null),e(w,b),e(w,E),e(E,wl),m(t,yi,p),m(t,Ie,p),e(Ie,bt),e(bt,Ds),g(no,Ds,null),e(Ie,bl),e(Ie,Ns),e(Ns,yl),m(t,$i,p),m(t,yt,p),e(yt,$l),e(yt,ao),e(ao,kl),e(yt,Tl),m(t,ki,p),m(t,$t,p),e($t,xl),e($t,io),e(io,El),e($t,Fl),m(t,Ti,p),m(t,Gr,p),e(Gr,Cl),m(t,xi,p),m(t,Zr,p),e(Zr,Ls),e(Ls,Ml),m(t,Ei,p),m(t,Xr,p),e(Xr,Il),m(t,Fi,p),m(t,Qr,p),e(Qr,zl),m(t,Ci,p),m(t,N,p),e(N,ql),e(N,Yr),e(Yr,jl),e(N,Al),e(N,Ss),e(Ss,Ol),e(N,Dl),e(N,Hs),e(Hs,Nl),e(N,Ll),e(N,Bs),e(Bs,Sl),e(N,Hl),e(N,Ws),e(Ws,Bl),e(N,Wl),m(t,Mi,p),m(t,es,p),e(es,Vl),m(t,Ii,p),m(t,z,p),e(z,Ul),e(z,Vs),e(Vs,Rl),e(z,Kl),e(z,Us),e(Us,Jl),e(z,Gl),e(z,Rs),e(Rs,Zl),e(z,Xl),e(z,Ks),e(Ks,Ql),e(z,Yl),e(z,Js),e(Js,ed),e(z,td),e(z,Gs),e(Gs,od),e(z,rd),m(t,zi,p),m(t,ge,p),e(ge,sd),e(ge,co),e(co,nd),e(ge,ad),e(ge,lo),e(lo,id),e(ge,cd),m(t,qi,p),m(t,ze,p),e(ze,kt),e(kt,Zs),g(po,Zs,null),e(ze,ld),e(ze,Xs),e(Xs,dd),m(t,ji,p),m(t,qe,p),g(mo,qe,null),e(qe,pd),e(qe,Qs),e(Qs,md),m(t,Ai,p),m(t,je,p),g(ho,je,null),e(je,hd),e(je,Ys),e(Ys,fd),m(t,Oi,p),m(t,Ae,p),g(fo,Ae,null),e(Ae,ud),e(Ae,en),e(en,gd),m(t,Di,p),m(t,Oe,p),g(uo,Oe,null),e(Oe,vd),e(Oe,tn),e(tn,_d),m(t,Ni,p),m(t,De,p),e(De,Tt),e(Tt,on),g(go,on,null),e(De,Pd),e(De,rn),e(rn,wd),m(t,Li,p),m(t,L,p),g(vo,L,null),e(L,bd),e(L,Ne),e(Ne,yd),e(Ne,ts),e(ts,$d),e(Ne,kd),e(Ne,_o),e(_o,Td),e(Ne,xd),e(L,Ed),e(L,Le),e(Le,Fd),e(Le,os),e(os,Cd),e(Le,Md),e(Le,rs),e(rs,Id),e(Le,zd),e(L,qd),e(L,sn),e(sn,jd),e(L,Ad),g(Po,L,null),m(t,Si,p),m(t,Se,p),e(Se,xt),e(xt,nn),g(wo,nn,null),e(Se,Od),e(Se,an),e(an,Dd),m(t,Hi,p),m(t,I,p),g(bo,I,null),e(I,Nd),e(I,cn),e(cn,Ld),e(I,Sd),e(I,yo),e(yo,Hd),e(yo,ss),e(ss,Bd),e(yo,Wd),e(I,Vd),e(I,ve),g($o,ve,null),e(ve,Ud),e(ve,ln),e(ln,Rd),e(ve,Kd),e(ve,ko),e(ko,ns),e(ns,Jd),e(ns,dn),e(dn,Gd),e(ko,Zd),e(ko,as),e(as,Xd),e(as,pn),e(pn,Qd),e(I,Yd),e(I,Et),g(To,Et,null),e(Et,ep),e(Et,xo),e(xo,tp),e(xo,mn),e(mn,op),e(xo,rp),e(I,sp),e(I,_e),g(Eo,_e,null),e(_e,np),e(_e,is),e(is,ap),e(is,cs),e(cs,ip),e(_e,cp),e(_e,hn),e(hn,lp),e(I,dp),e(I,fn),m(t,Bi,p),m(t,He,p),e(He,Ft),e(Ft,un),g(Fo,un,null),e(He,pp),e(He,gn),e(gn,mp),m(t,Wi,p),m(t,te,p),g(Co,te,null),e(te,hp),e(te,vn),e(vn,fp),e(te,up),e(te,Mo),e(Mo,gp),e(Mo,ls),e(ls,vp),e(Mo,_p),e(te,Pp),e(te,Ct),g(Io,Ct,null),e(Ct,wp),e(Ct,Be),e(Be,bp),e(Be,_n),e(_n,yp),e(Be,$p),e(Be,Pn),e(Pn,kp),e(Be,Tp),m(t,Vi,p),m(t,We,p),e(We,Mt),e(Mt,wn),g(zo,wn,null),e(We,xp),e(We,bn),e(bn,Ep),m(t,Ui,p),m(t,Ve,p),g(qo,Ve,null),e(Ve,Fp),e(Ve,yn),e(yn,Cp),m(t,Ri,p),m(t,Ue,p),e(Ue,It),e(It,$n),g(jo,$n,null),e(Ue,Mp),e(Ue,kn),e(kn,Ip),m(t,Ki,p),m(t,ce,p),g(Ao,ce,null),e(ce,zp),e(ce,Tn),e(Tn,qp),e(ce,jp),e(ce,oe),e(oe,Ap),e(oe,xn),e(xn,Op),e(oe,Dp),e(oe,En),e(En,Np),e(oe,Lp),e(oe,Fn),e(Fn,Sp),e(oe,Hp),e(oe,Cn),e(Cn,Bp),e(oe,Wp),m(t,Ji,p),m(t,Re,p),e(Re,zt),e(zt,Mn),g(Oo,Mn,null),e(Re,Vp),e(Re,In),e(In,Up),m(t,Gi,p),m(t,Ke,p),g(Do,Ke,null),e(Ke,Rp),e(Ke,zn),e(zn,Kp),m(t,Zi,p),m(t,Je,p),e(Je,qt),e(qt,qn),g(No,qn,null),e(Je,Jp),e(Je,jn),e(jn,Gp),m(t,Xi,p),m(t,Ge,p),g(Lo,Ge,null),e(Ge,Zp),e(Ge,An),e(An,Xp),m(t,Qi,p),m(t,Ze,p),e(Ze,jt),e(jt,On),g(So,On,null),e(Ze,Qp),e(Ze,Dn),e(Dn,Yp),m(t,Yi,p),m(t,le,p),g(Ho,le,null),e(le,em),e(le,Nn),e(Nn,tm),e(le,om),e(le,Ln),e(Ln,rm),m(t,ec,p),m(t,Xe,p),e(Xe,At),e(At,Sn),g(Bo,Sn,null),e(Xe,sm),e(Xe,Hn),e(Hn,nm),m(t,tc,p),m(t,Qe,p),g(Wo,Qe,null),e(Qe,am),e(Qe,Bn),e(Bn,im),m(t,oc,p),m(t,Ye,p),e(Ye,Ot),e(Ot,Wn),g(Vo,Wn,null),e(Ye,cm),e(Ye,Vn),e(Vn,lm),m(t,rc,p),m(t,et,p),g(Uo,et,null),e(et,dm),e(et,Un),e(Un,pm),m(t,sc,p),m(t,tt,p),e(tt,Dt),e(Dt,Rn),g(Ro,Rn,null),e(tt,mm),e(tt,Kn),e(Kn,hm),m(t,nc,p),m(t,ot,p),g(Ko,ot,null),e(ot,fm),e(ot,Jn),e(Jn,um),m(t,ac,p),m(t,rt,p),e(rt,Nt),e(Nt,Gn),g(Jo,Gn,null),e(rt,gm),e(rt,Zn),e(Zn,vm),m(t,ic,p),m(t,st,p),g(Go,st,null),e(st,_m),e(st,Xn),e(Xn,Pm),m(t,cc,p),m(t,nt,p),e(nt,Lt),e(Lt,Qn),g(Zo,Qn,null),e(nt,wm),e(nt,Yn),e(Yn,bm),m(t,lc,p),m(t,de,p),g(Xo,de,null),e(de,ym),e(de,Qo),e(Qo,$m),e(Qo,Yo),e(Yo,km),e(Qo,Tm),e(de,xm),e(de,B),g(er,B,null),e(B,Em),e(B,at),e(at,Fm),e(at,ds),e(ds,Cm),e(at,Mm),e(at,ea),e(ea,Im),e(at,zm),e(B,qm),g(St,B,null),e(B,jm),e(B,ta),e(ta,Am),e(B,Om),g(tr,B,null),m(t,dc,p),m(t,it,p),e(it,Ht),e(Ht,oa),g(or,oa,null),e(it,Dm),e(it,ra),e(ra,Nm),m(t,pc,p),m(t,pe,p),g(rr,pe,null),e(pe,Lm),e(pe,sr),e(sr,Sm),e(sr,nr),e(nr,Hm),e(sr,Bm),e(pe,Wm),e(pe,W),g(ar,W,null),e(W,Vm),e(W,ct),e(ct,Um),e(ct,ps),e(ps,Rm),e(ct,Km),e(ct,sa),e(sa,Jm),e(ct,Gm),e(W,Zm),g(Bt,W,null),e(W,Xm),e(W,na),e(na,Qm),e(W,Ym),g(ir,W,null),m(t,mc,p),m(t,lt,p),e(lt,Wt),e(Wt,aa),g(cr,aa,null),e(lt,eh),e(lt,ia),e(ia,th),m(t,hc,p),m(t,me,p),g(lr,me,null),e(me,oh),e(me,dr),e(dr,rh),e(dr,pr),e(pr,sh),e(dr,nh),e(me,ah),e(me,q),g(mr,q,null),e(q,ih),e(q,dt),e(dt,ch),e(dt,ms),e(ms,lh),e(dt,dh),e(dt,ca),e(ca,ph),e(dt,mh),e(q,hh),g(Vt,q,null),e(q,fh),e(q,la),e(la,uh),e(q,gh),g(hr,q,null),e(q,vh),e(q,da),e(da,_h),e(q,Ph),g(fr,q,null),m(t,fc,p),m(t,pt,p),e(pt,Ut),e(Ut,pa),g(ur,pa,null),e(pt,wh),e(pt,ma),e(ma,bh),m(t,uc,p),m(t,A,p),g(gr,A,null),e(A,yh),e(A,ha),e(ha,$h),e(A,kh),e(A,fa),e(fa,Th),e(A,xh),e(A,V),e(V,ua),e(ua,Eh),e(V,Fh),e(V,ga),e(ga,Ch),e(V,Mh),e(V,va),e(va,Ih),e(V,zh),e(V,_a),e(_a,qh),e(V,jh),e(V,Pa),e(Pa,Ah),e(V,Oh),e(A,Dh),e(A,vr),e(vr,Nh),e(vr,_r),e(_r,Lh),e(vr,Sh),e(A,Hh),e(A,U),g(Pr,U,null),e(U,Bh),e(U,mt),e(mt,Wh),e(mt,hs),e(hs,Vh),e(mt,Uh),e(mt,wa),e(wa,Rh),e(mt,Kh),e(U,Jh),g(Rt,U,null),e(U,Gh),e(U,ba),e(ba,Zh),e(U,Xh),g(wr,U,null),m(t,gc,p),m(t,ht,p),e(ht,Kt),e(Kt,ya),g(br,ya,null),e(ht,Qh),e(ht,$a),e($a,Yh),m(t,vc,p),m(t,O,p),g(yr,O,null),e(O,ef),e(O,ka),e(ka,tf),e(O,of),e(O,Ta),e(Ta,rf),e(O,sf),e(O,R),e(R,xa),e(xa,nf),e(R,af),e(R,Ea),e(Ea,cf),e(R,lf),e(R,Fa),e(Fa,df),e(R,pf),e(R,Ca),e(Ca,mf),e(R,hf),e(R,Ma),e(Ma,ff),e(R,uf),e(O,gf),e(O,$r),e($r,vf),e($r,kr),e(kr,_f),e($r,Pf),e(O,wf),e(O,K),g(Tr,K,null),e(K,bf),e(K,ft),e(ft,yf),e(ft,fs),e(fs,$f),e(ft,kf),e(ft,Ia),e(Ia,Tf),e(ft,xf),e(K,Ef),g(Jt,K,null),e(K,Ff),e(K,za),e(za,Cf),e(K,Mf),g(xr,K,null),m(t,_c,p),m(t,ut,p),e(ut,Gt),e(Gt,qa),g(Er,qa,null),e(ut,If),e(ut,ja),e(ja,zf),m(t,Pc,p),m(t,D,p),g(Fr,D,null),e(D,qf),e(D,Aa),e(Aa,jf),e(D,Af),e(D,Oa),e(Oa,Of),e(D,Df),e(D,J),e(J,Da),e(Da,Nf),e(J,Lf),e(J,Na),e(Na,Sf),e(J,Hf),e(J,La),e(La,Bf),e(J,Wf),e(J,Sa),e(Sa,Vf),e(J,Uf),e(J,Ha),e(Ha,Rf),e(J,Kf),e(D,Jf),e(D,Cr),e(Cr,Gf),e(Cr,Mr),e(Mr,Zf),e(Cr,Xf),e(D,Qf),e(D,G),g(Ir,G,null),e(G,Yf),e(G,gt),e(gt,eu),e(gt,us),e(us,tu),e(gt,ou),e(gt,Ba),e(Ba,ru),e(gt,su),e(G,nu),g(Zt,G,null),e(G,au),e(G,Wa),e(Wa,iu),e(G,cu),g(zr,G,null),m(t,wc,p),m(t,vt,p),e(vt,Xt),e(Xt,Va),g(qr,Va,null),e(vt,lu),e(vt,Ua),e(Ua,du),m(t,bc,p),m(t,S,p),g(jr,S,null),e(S,pu),e(S,H),e(H,mu),e(H,Ra),e(Ra,hu),e(H,fu),e(H,Ka),e(Ka,uu),e(H,gu),e(H,Ja),e(Ja,vu),e(H,_u),e(H,Ga),e(Ga,Pu),e(H,wu),e(H,Za),e(Za,bu),e(H,yu),e(S,$u),e(S,Xa),e(Xa,ku),e(S,Tu),e(S,Ar),e(Ar,xu),e(Ar,Or),e(Or,Eu),e(Ar,Fu),e(S,Cu),e(S,Z),g(Dr,Z,null),e(Z,Mu),e(Z,_t),e(_t,Iu),e(_t,gs),e(gs,zu),e(_t,qu),e(_t,Qa),e(Qa,ju),e(_t,Au),e(Z,Ou),g(Qt,Z,null),e(Z,Du),e(Z,Ya),e(Ya,Nu),e(Z,Lu),g(Nr,Z,null),m(t,yc,p),m(t,Pt,p),e(Pt,Yt),e(Yt,ei),g(Lr,ei,null),e(Pt,Su),e(Pt,ti),e(ti,Hu),m(t,$c,p),m(t,F,p),g(Sr,F,null),e(F,Bu),e(F,oi),e(oi,Wu),e(F,Vu),e(F,eo),e(eo,ri),e(ri,Uu),e(eo,Ru),e(eo,si),e(si,Ku),e(eo,Ju),e(F,Gu),e(F,Pe),e(Pe,ni),e(ni,Zu),e(Pe,Xu),e(Pe,ai),e(ai,Qu),e(Pe,Yu),e(Pe,ii),e(ii,eg),e(Pe,tg),e(F,og),e(F,to),e(to,ci),e(ci,rg),e(to,sg),e(to,li),e(li,ng),e(to,ag),e(F,ig),e(F,Hr),e(Hr,cg),e(Hr,di),e(di,lg),e(Hr,dg),e(F,pg),e(F,pi),e(pi,mg),e(F,hg),e(F,Br),e(Br,fg),e(Br,Wr),e(Wr,ug),e(Br,gg),e(F,vg),e(F,X),g(Vr,X,null),e(X,_g),e(X,wt),e(wt,Pg),e(wt,vs),e(vs,wg),e(wt,bg),e(wt,mi),e(mi,yg),e(wt,$g),e(X,kg),g(oo,X,null),e(X,Tg),e(X,hi),e(hi,xg),e(X,Eg),g(Ur,X,null),kc=!0},p(t,[p]){const Rr={};p&2&&(Rr.$$scope={dirty:p,ctx:t}),St.$set(Rr);const fi={};p&2&&(fi.$$scope={dirty:p,ctx:t}),Bt.$set(fi);const ui={};p&2&&(ui.$$scope={dirty:p,ctx:t}),Vt.$set(ui);const gi={};p&2&&(gi.$$scope={dirty:p,ctx:t}),Rt.$set(gi);const Kr={};p&2&&(Kr.$$scope={dirty:p,ctx:t}),Jt.$set(Kr);const vi={};p&2&&(vi.$$scope={dirty:p,ctx:t}),Zt.$set(vi);const _i={};p&2&&(_i.$$scope={dirty:p,ctx:t}),Qt.$set(_i);const Pi={};p&2&&(Pi.$$scope={dirty:p,ctx:t}),oo.$set(Pi)},i(t){kc||(v(y.$$.fragment,t),v(no.$$.fragment,t),v(po.$$.fragment,t),v(mo.$$.fragment,t),v(ho.$$.fragment,t),v(fo.$$.fragment,t),v(uo.$$.fragment,t),v(go.$$.fragment,t),v(vo.$$.fragment,t),v(Po.$$.fragment,t),v(wo.$$.fragment,t),v(bo.$$.fragment,t),v($o.$$.fragment,t),v(To.$$.fragment,t),v(Eo.$$.fragment,t),v(Fo.$$.fragment,t),v(Co.$$.fragment,t),v(Io.$$.fragment,t),v(zo.$$.fragment,t),v(qo.$$.fragment,t),v(jo.$$.fragment,t),v(Ao.$$.fragment,t),v(Oo.$$.fragment,t),v(Do.$$.fragment,t),v(No.$$.fragment,t),v(Lo.$$.fragment,t),v(So.$$.fragment,t),v(Ho.$$.fragment,t),v(Bo.$$.fragment,t),v(Wo.$$.fragment,t),v(Vo.$$.fragment,t),v(Uo.$$.fragment,t),v(Ro.$$.fragment,t),v(Ko.$$.fragment,t),v(Jo.$$.fragment,t),v(Go.$$.fragment,t),v(Zo.$$.fragment,t),v(Xo.$$.fragment,t),v(er.$$.fragment,t),v(St.$$.fragment,t),v(tr.$$.fragment,t),v(or.$$.fragment,t),v(rr.$$.fragment,t),v(ar.$$.fragment,t),v(Bt.$$.fragment,t),v(ir.$$.fragment,t),v(cr.$$.fragment,t),v(lr.$$.fragment,t),v(mr.$$.fragment,t),v(Vt.$$.fragment,t),v(hr.$$.fragment,t),v(fr.$$.fragment,t),v(ur.$$.fragment,t),v(gr.$$.fragment,t),v(Pr.$$.fragment,t),v(Rt.$$.fragment,t),v(wr.$$.fragment,t),v(br.$$.fragment,t),v(yr.$$.fragment,t),v(Tr.$$.fragment,t),v(Jt.$$.fragment,t),v(xr.$$.fragment,t),v(Er.$$.fragment,t),v(Fr.$$.fragment,t),v(Ir.$$.fragment,t),v(Zt.$$.fragment,t),v(zr.$$.fragment,t),v(qr.$$.fragment,t),v(jr.$$.fragment,t),v(Dr.$$.fragment,t),v(Qt.$$.fragment,t),v(Nr.$$.fragment,t),v(Lr.$$.fragment,t),v(Sr.$$.fragment,t),v(Vr.$$.fragment,t),v(oo.$$.fragment,t),v(Ur.$$.fragment,t),kc=!0)},o(t){_(y.$$.fragment,t),_(no.$$.fragment,t),_(po.$$.fragment,t),_(mo.$$.fragment,t),_(ho.$$.fragment,t),_(fo.$$.fragment,t),_(uo.$$.fragment,t),_(go.$$.fragment,t),_(vo.$$.fragment,t),_(Po.$$.fragment,t),_(wo.$$.fragment,t),_(bo.$$.fragment,t),_($o.$$.fragment,t),_(To.$$.fragment,t),_(Eo.$$.fragment,t),_(Fo.$$.fragment,t),_(Co.$$.fragment,t),_(Io.$$.fragment,t),_(zo.$$.fragment,t),_(qo.$$.fragment,t),_(jo.$$.fragment,t),_(Ao.$$.fragment,t),_(Oo.$$.fragment,t),_(Do.$$.fragment,t),_(No.$$.fragment,t),_(Lo.$$.fragment,t),_(So.$$.fragment,t),_(Ho.$$.fragment,t),_(Bo.$$.fragment,t),_(Wo.$$.fragment,t),_(Vo.$$.fragment,t),_(Uo.$$.fragment,t),_(Ro.$$.fragment,t),_(Ko.$$.fragment,t),_(Jo.$$.fragment,t),_(Go.$$.fragment,t),_(Zo.$$.fragment,t),_(Xo.$$.fragment,t),_(er.$$.fragment,t),_(St.$$.fragment,t),_(tr.$$.fragment,t),_(or.$$.fragment,t),_(rr.$$.fragment,t),_(ar.$$.fragment,t),_(Bt.$$.fragment,t),_(ir.$$.fragment,t),_(cr.$$.fragment,t),_(lr.$$.fragment,t),_(mr.$$.fragment,t),_(Vt.$$.fragment,t),_(hr.$$.fragment,t),_(fr.$$.fragment,t),_(ur.$$.fragment,t),_(gr.$$.fragment,t),_(Pr.$$.fragment,t),_(Rt.$$.fragment,t),_(wr.$$.fragment,t),_(br.$$.fragment,t),_(yr.$$.fragment,t),_(Tr.$$.fragment,t),_(Jt.$$.fragment,t),_(xr.$$.fragment,t),_(Er.$$.fragment,t),_(Fr.$$.fragment,t),_(Ir.$$.fragment,t),_(Zt.$$.fragment,t),_(zr.$$.fragment,t),_(qr.$$.fragment,t),_(jr.$$.fragment,t),_(Dr.$$.fragment,t),_(Qt.$$.fragment,t),_(Nr.$$.fragment,t),_(Lr.$$.fragment,t),_(Sr.$$.fragment,t),_(Vr.$$.fragment,t),_(oo.$$.fragment,t),_(Ur.$$.fragment,t),kc=!1},d(t){o(h),t&&o(k),t&&o(w),P(y),t&&o(yi),t&&o(Ie),P(no),t&&o($i),t&&o(yt),t&&o(ki),t&&o($t),t&&o(Ti),t&&o(Gr),t&&o(xi),t&&o(Zr),t&&o(Ei),t&&o(Xr),t&&o(Fi),t&&o(Qr),t&&o(Ci),t&&o(N),t&&o(Mi),t&&o(es),t&&o(Ii),t&&o(z),t&&o(zi),t&&o(ge),t&&o(qi),t&&o(ze),P(po),t&&o(ji),t&&o(qe),P(mo),t&&o(Ai),t&&o(je),P(ho),t&&o(Oi),t&&o(Ae),P(fo),t&&o(Di),t&&o(Oe),P(uo),t&&o(Ni),t&&o(De),P(go),t&&o(Li),t&&o(L),P(vo),P(Po),t&&o(Si),t&&o(Se),P(wo),t&&o(Hi),t&&o(I),P(bo),P($o),P(To),P(Eo),t&&o(Bi),t&&o(He),P(Fo),t&&o(Wi),t&&o(te),P(Co),P(Io),t&&o(Vi),t&&o(We),P(zo),t&&o(Ui),t&&o(Ve),P(qo),t&&o(Ri),t&&o(Ue),P(jo),t&&o(Ki),t&&o(ce),P(Ao),t&&o(Ji),t&&o(Re),P(Oo),t&&o(Gi),t&&o(Ke),P(Do),t&&o(Zi),t&&o(Je),P(No),t&&o(Xi),t&&o(Ge),P(Lo),t&&o(Qi),t&&o(Ze),P(So),t&&o(Yi),t&&o(le),P(Ho),t&&o(ec),t&&o(Xe),P(Bo),t&&o(tc),t&&o(Qe),P(Wo),t&&o(oc),t&&o(Ye),P(Vo),t&&o(rc),t&&o(et),P(Uo),t&&o(sc),t&&o(tt),P(Ro),t&&o(nc),t&&o(ot),P(Ko),t&&o(ac),t&&o(rt),P(Jo),t&&o(ic),t&&o(st),P(Go),t&&o(cc),t&&o(nt),P(Zo),t&&o(lc),t&&o(de),P(Xo),P(er),P(St),P(tr),t&&o(dc),t&&o(it),P(or),t&&o(pc),t&&o(pe),P(rr),P(ar),P(Bt),P(ir),t&&o(mc),t&&o(lt),P(cr),t&&o(hc),t&&o(me),P(lr),P(mr),P(Vt),P(hr),P(fr),t&&o(fc),t&&o(pt),P(ur),t&&o(uc),t&&o(A),P(gr),P(Pr),P(Rt),P(wr),t&&o(gc),t&&o(ht),P(br),t&&o(vc),t&&o(O),P(yr),P(Tr),P(Jt),P(xr),t&&o(_c),t&&o(ut),P(Er),t&&o(Pc),t&&o(D),P(Fr),P(Ir),P(Zt),P(zr),t&&o(wc),t&&o(vt),P(qr),t&&o(bc),t&&o(S),P(jr),P(Dr),P(Qt),P(Nr),t&&o(yc),t&&o(Pt),P(Lr),t&&o($c),t&&o(F),P(Sr),P(Vr),P(oo),P(Ur)}}}const v1={local:"perceiver",sections:[{local:"overview",title:"Overview"},{local:"transformers.models.perceiver.modeling_perceiver.PerceiverModelOutput",title:"Perceiver specific outputs"},{local:"transformers.PerceiverConfig",title:"PerceiverConfig"},{local:"transformers.PerceiverTokenizer",title:"PerceiverTokenizer"},{local:"transformers.PerceiverFeatureExtractor",title:"PerceiverFeatureExtractor"},{local:"transformers.models.perceiver.modeling_perceiver.PerceiverTextPreprocessor",title:"PerceiverTextPreprocessor"},{local:"transformers.models.perceiver.modeling_perceiver.PerceiverImagePreprocessor",title:"PerceiverImagePreprocessor"},{local:"transformers.models.perceiver.modeling_perceiver.PerceiverOneHotPreprocessor",title:"PerceiverOneHotPreprocessor"},{local:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPreprocessor",title:"PerceiverAudioPreprocessor"},{local:"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPreprocessor",title:"PerceiverMultimodalPreprocessor"},{local:"transformers.models.perceiver.modeling_perceiver.PerceiverProjectionPostprocessor",title:"PerceiverProjectionPostprocessor"},{local:"transformers.models.perceiver.modeling_perceiver.PerceiverAudioPostprocessor",title:"PerceiverAudioPostprocessor"},{local:"transformers.models.perceiver.modeling_perceiver.PerceiverClassificationPostprocessor",title:"PerceiverClassificationPostprocessor"},{local:"transformers.models.perceiver.modeling_perceiver.PerceiverMultimodalPostprocessor",title:"PerceiverMultimodalPostprocessor"},{local:"transformers.PerceiverModel",title:"PerceiverModel"},{local:"transformers.PerceiverForMaskedLM",title:"PerceiverForMaskedLM"},{local:"transformers.PerceiverForSequenceClassification",title:"PerceiverForSequenceClassification"},{local:"transformers.PerceiverForImageClassificationLearned",title:"PerceiverForImageClassificationLearned"},{local:"transformers.PerceiverForImageClassificationFourier",title:"PerceiverForImageClassificationFourier"},{local:"transformers.PerceiverForImageClassificationConvProcessing",title:"PerceiverForImageClassificationConvProcessing"},{local:"transformers.PerceiverForOpticalFlow",title:"PerceiverForOpticalFlow"},{local:"transformers.PerceiverForMultimodalAutoencoding",title:"PerceiverForMultimodalAutoencoding"}],title:"Perceiver"};function _1(M,h,k){let{fw:w}=h;return M.$$set=$=>{"fw"in $&&k(0,w=$.fw)},[w]}class T1 extends s1{constructor(h){super();n1(this,h,_1,g1,a1,{fw:0})}}export{T1 as default,v1 as metadata};
