import{S as ai,i as si,s as ni,e as n,k as d,w as v,t as a,L as ri,c as r,d as o,m as c,a as i,x as T,h as s,b as l,M as ii,J as e,g as p,y as I,q as b,o as y,B as w}from"../../../chunks/vendor-e859c359.js";import{T as oa}from"../../../chunks/Tip-edc75249.js";import{D as re}from"../../../chunks/Docstring-ade913b3.js";import{C as aa}from"../../../chunks/CodeBlock-ce4317c2.js";import{I as ct}from"../../../chunks/IconCopyLink-5fae3b20.js";import"../../../chunks/CopyButton-77addb3d.js";function li(S){let h,P;return{c(){h=n("p"),P=a(`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`)},l(g){h=r(g,"P",{});var u=i(h);P=s(u,`NumPy arrays and PyTorch tensors are converted to PIL images when resizing, so the most efficient is to pass
PIL images.`),u.forEach(o)},m(g,u){p(g,h,u),e(h,P)},d(g){g&&o(h)}}}function di(S){let h,P,g,u,x;return{c(){h=n("p"),P=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),g=n("code"),u=a("Module"),x=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(f){h=r(f,"P",{});var _=i(h);P=s(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),g=r(_,"CODE",{});var C=i(g);u=s(C,"Module"),C.forEach(o),x=s(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(o)},m(f,_){p(f,h,_),e(h,P),e(h,g),e(g,u),e(h,x)},d(f){f&&o(h)}}}function ci(S){let h,P,g,u,x;return{c(){h=n("p"),P=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),g=n("code"),u=a("Module"),x=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(f){h=r(f,"P",{});var _=i(h);P=s(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),g=r(_,"CODE",{});var C=i(g);u=s(C,"Module"),C.forEach(o),x=s(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(o)},m(f,_){p(f,h,_),e(h,P),e(h,g),e(g,u),e(h,x)},d(f){f&&o(h)}}}function mi(S){let h,P,g,u,x;return{c(){h=n("p"),P=a(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),g=n("code"),u=a("Module"),x=a(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(f){h=r(f,"P",{});var _=i(h);P=s(_,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),g=r(_,"CODE",{});var C=i(g);u=s(C,"Module"),C.forEach(o),x=s(_,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),_.forEach(o)},m(f,_){p(f,h,_),e(h,P),e(h,g),e(g,u),e(h,x)},d(f){f&&o(h)}}}function pi(S){let h,P,g,u,x,f,_,C,sa,uo,Q,ie,Mt,ye,na,zt,ra,fo,le,ia,we,la,da,_o,mt,ca,vo,pt,jt,ma,To,de,pa,Pe,ha,ga,Io,xe,Vn,bo,ht,ua,yo,M,ke,fa,gt,_a,va,Ta,Ge,Ia,ut,ba,ya,wa,$e,Pa,qt,xa,ka,Ga,Ee,$a,ft,Ea,Fa,Ca,At,Ma,wo,k,za,Nt,ja,qa,Lt,Aa,Na,St,La,Sa,Dt,Da,Oa,Ot,Wa,Ra,Po,N,Ba,Fe,Ha,Va,Ce,Ua,Ja,Me,Xa,Qa,xo,K,ce,Wt,ze,Ka,Rt,Za,ko,G,je,Ya,D,es,_t,ts,os,Bt,as,ss,qe,ns,rs,is,Z,ls,vt,ds,cs,Tt,ms,ps,hs,Ht,gs,us,Ae,Go,Y,me,Vt,Ne,fs,Ut,_s,$o,A,Le,vs,Jt,Ts,Is,Se,bs,Xt,ys,ws,Ps,O,De,xs,Qt,ks,Gs,pe,Eo,ee,he,Kt,Oe,$s,Zt,Es,Fo,$,We,Fs,Yt,Cs,Ms,Re,zs,It,js,qs,As,Be,Ns,He,Ls,Ss,Ds,z,Ve,Os,te,Ws,bt,Rs,Bs,eo,Hs,Vs,Us,ge,Js,to,Xs,Qs,Ue,Co,oe,ue,oo,Je,Ks,ao,Zs,Mo,E,Xe,Ys,so,en,tn,Qe,on,yt,an,sn,nn,Ke,rn,Ze,ln,dn,cn,j,Ye,mn,ae,pn,wt,hn,gn,no,un,fn,_n,fe,vn,ro,Tn,In,et,zo,se,_e,io,tt,bn,lo,yn,jo,F,ot,wn,at,Pn,Pt,xn,kn,Gn,st,$n,xt,En,Fn,Cn,nt,Mn,rt,zn,jn,qn,q,it,An,ne,Nn,kt,Ln,Sn,co,Dn,On,Wn,ve,Rn,mo,Bn,Hn,lt,qo;return f=new ct({}),ye=new ct({}),ze=new ct({}),je=new re({props:{name:"class transformers.ImageGPTConfig",anchor:"transformers.ImageGPTConfig",parameters:[{name:"vocab_size",val:" = 513"},{name:"n_positions",val:" = 1024"},{name:"n_embd",val:" = 512"},{name:"n_layer",val:" = 24"},{name:"n_head",val:" = 8"},{name:"n_inner",val:" = None"},{name:"activation_function",val:" = 'quick_gelu'"},{name:"resid_pdrop",val:" = 0.1"},{name:"embd_pdrop",val:" = 0.1"},{name:"attn_pdrop",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-05"},{name:"initializer_range",val:" = 0.02"},{name:"scale_attn_weights",val:" = True"},{name:"use_cache",val:" = True"},{name:"tie_word_embeddings",val:" = False"},{name:"scale_attn_by_inverse_layer_idx",val:" = False"},{name:"reorder_and_upcast_attn",val:" = False"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/imagegpt/configuration_imagegpt.py#L30",parametersDescription:[{anchor:"transformers.ImageGPTConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Vocabulary size of the GPT-2 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> or
<code>TFImageGPTModel</code>.`,name:"vocab_size"},{anchor:"transformers.ImageGPTConfig.n_positions",description:`<strong>n_positions</strong> (<code>int</code>, <em>optional</em>, defaults to 32*32) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"n_positions"},{anchor:"transformers.ImageGPTConfig.n_embd",description:`<strong>n_embd</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the embeddings and hidden states.`,name:"n_embd"},{anchor:"transformers.ImageGPTConfig.n_layer",description:`<strong>n_layer</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"n_layer"},{anchor:"transformers.ImageGPTConfig.n_head",description:`<strong>n_head</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_head"},{anchor:"transformers.ImageGPTConfig.n_inner",description:`<strong>n_inner</strong> (<code>int</code>, <em>optional</em>, defaults to None) &#x2014;
Dimensionality of the inner feed-forward layers. <code>None</code> will set it to 4 times n_embd`,name:"n_inner"},{anchor:"transformers.ImageGPTConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
Activation function (can be one of the activation functions defined in src/transformers/activations.py).
Defaults to &#x201C;quick_gelu&#x201D;.`,name:"activation_function"},{anchor:"transformers.ImageGPTConfig.resid_pdrop",description:`<strong>resid_pdrop</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"resid_pdrop"},{anchor:"transformers.ImageGPTConfig.embd_pdrop",description:`<strong>embd_pdrop</strong> (<code>int</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the embeddings.`,name:"embd_pdrop"},{anchor:"transformers.ImageGPTConfig.attn_pdrop",description:`<strong>attn_pdrop</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention.`,name:"attn_pdrop"},{anchor:"transformers.ImageGPTConfig.layer_norm_epsilon",description:`<strong>layer_norm_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon to use in the layer normalization layers.`,name:"layer_norm_epsilon"},{anchor:"transformers.ImageGPTConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ImageGPTConfig.scale_attn_weights",description:`<strong>scale_attn_weights</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Scale attention weights by dividing by sqrt(hidden_size)..`,name:"scale_attn_weights"},{anchor:"transformers.ImageGPTConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.ImageGPTConfig.scale_attn_by_inverse_layer_idx",description:`<strong>scale_attn_by_inverse_layer_idx</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to additionally scale attention weights by <code>1 / layer_idx + 1</code>.`,name:"scale_attn_by_inverse_layer_idx"},{anchor:"transformers.ImageGPTConfig.reorder_and_upcast_attn",description:`<strong>reorder_and_upcast_attn</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to scale keys (K) prior to computing attention (dot-product) and upcast attention
dot-product/softmax to float() when training with mixed precision.`,name:"reorder_and_upcast_attn"}]}}),Ae=new aa({props:{code:`from transformers import ImageGPTModel, ImageGPTConfig

# Initializing a ImageGPT configuration
configuration = ImageGPTConfig()

# Initializing a model from the configuration
model = ImageGPTModel(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ImageGPTModel, ImageGPTConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ImageGPT configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ImageGPTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ImageGPTModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Ne=new ct({}),Le=new re({props:{name:"class transformers.ImageGPTFeatureExtractor",anchor:"transformers.ImageGPTFeatureExtractor",parameters:[{name:"clusters",val:""},{name:"do_resize",val:" = True"},{name:"size",val:" = 32"},{name:"resample",val:" = 2"},{name:"do_normalize",val:" = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/imagegpt/feature_extraction_imagegpt.py#L46",parametersDescription:[{anchor:"transformers.ImageGPTFeatureExtractor.clusters",description:`<strong>clusters</strong> (<code>np.ndarray</code>) &#x2014;
The color clusters to use, as a <code>np.ndarray</code> of shape <code>(n_clusters, 3)</code>.`,name:"clusters"},{anchor:"transformers.ImageGPTFeatureExtractor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input to a certain <code>size</code>.`,name:"do_resize"},{anchor:"transformers.ImageGPTFeatureExtractor.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple(int)</code>, <em>optional</em>, defaults to 32) &#x2014;
Resize the input to the given size. If a tuple is provided, it should be (width, height). If only an
integer is provided, then the input will be resized to (size, size). Only has an effect if <code>do_resize</code>
is set to <code>True</code>.`,name:"size"},{anchor:"transformers.ImageGPTFeatureExtractor.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BILINEAR</code>) &#x2014;
An optional resampling filter. This can be one of <code>PIL.Image.NEAREST</code>, <code>PIL.Image.BOX</code>,
<code>PIL.Image.BILINEAR</code>, <code>PIL.Image.HAMMING</code>, <code>PIL.Image.BICUBIC</code> or <code>PIL.Image.LANCZOS</code>.
Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.ImageGPTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input to the range between -1 and +1.`,name:"do_normalize"}]}}),De=new re({props:{name:"\\_\\_call\\_\\_",anchor:"transformers.ImageGPTFeatureExtractor.__call__",parameters:[{name:"images",val:": typing.Union[PIL.Image.Image, numpy.ndarray, ForwardRef('torch.Tensor'), typing.List[PIL.Image.Image], typing.List[numpy.ndarray], typing.List[ForwardRef('torch.Tensor')]]"},{name:"return_tensors",val:": typing.Union[str, transformers.file_utils.TensorType, NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/imagegpt/feature_extraction_imagegpt.py#L97",parametersDescription:[{anchor:"transformers.ImageGPTFeatureExtractor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>List[PIL.Image.Image]</code>, <code>List[np.ndarray]</code>, <code>List[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a
number of channels, H and W are image height and width.`,name:"images"},{anchor:"transformers.ImageGPTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/master/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>, defaults to <code>&apos;np&apos;</code>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a> with the following fields:</p>
<ul>
<li><strong>pixel_values</strong> \u2014 Pixel values to be fed to a model, of shape (batch_size, num_channels, height,
width).</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/feature_extractor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),pe=new oa({props:{warning:!0,$$slots:{default:[li]},$$scope:{ctx:S}}}),Oe=new ct({}),We=new re({props:{name:"class transformers.ImageGPTModel",anchor:"transformers.ImageGPTModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/imagegpt/modeling_imagegpt.py#L620",parametersDescription:[{anchor:"transformers.ImageGPTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Ve=new re({props:{name:"forward",anchor:"transformers.ImageGPTModel.forward",parameters:[{name:"pixel_values",val:" = None"},{name:"past_key_values",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/imagegpt/modeling_imagegpt.py#L655",parametersDescription:[{anchor:"transformers.ImageGPTModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, pixel_values_length)</code>) &#x2014;
<code>pixel_values_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>pixel_values</code> that do not have their past calculated should be
passed as <code>pixel_values</code>.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTFeatureExtractor">ImageGPTFeatureExtractor</a>. See
<a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTFeatureExtractor.__call__">transformers.ImageGPTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.ImageGPTModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.n_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>pixel_values</code>
which have their past given to this model should not be passed as <code>pixel_values</code> as they have already
been computed.`,name:"past_key_values"},{anchor:"transformers.ImageGPTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ImageGPTModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, pixel_values_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.ImageGPTModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ImageGPTModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ImageGPTModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>pixel_values</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>pixel_values</code> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.ImageGPTModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ImageGPTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ImageGPTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ImageGPTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ImageGPTModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = pixel_values</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to
<code>-100</code> are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
>BaseModelOutputWithPastAndCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig"
>ImageGPTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
>BaseModelOutputWithPastAndCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ge=new oa({props:{$$slots:{default:[di]},$$scope:{ctx:S}}}),Ue=new aa({props:{code:`from transformers import ImageGPTFeatureExtractor, ImageGPTModel
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ImageGPTFeatureExtractor.from_pretrained('openai/imagegpt-small')
model = ImageGPTModel.from_pretrained('openai/imagegpt-small')

inputs = feature_extractor(images=image, return_tensors="pt")
outputs = model(**inputs)
last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ImageGPTFeatureExtractor, ImageGPTModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = ImageGPTFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;openai/imagegpt-small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ImageGPTModel.from_pretrained(<span class="hljs-string">&#x27;openai/imagegpt-small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Je=new ct({}),Xe=new re({props:{name:"class transformers.ImageGPTForCausalImageModeling",anchor:"transformers.ImageGPTForCausalImageModeling",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/imagegpt/modeling_imagegpt.py#L884",parametersDescription:[{anchor:"transformers.ImageGPTForCausalImageModeling.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Ye=new re({props:{name:"forward",anchor:"transformers.ImageGPTForCausalImageModeling.forward",parameters:[{name:"pixel_values",val:" = None"},{name:"past_key_values",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/imagegpt/modeling_imagegpt.py#L932",parametersDescription:[{anchor:"transformers.ImageGPTForCausalImageModeling.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, pixel_values_length)</code>) &#x2014;
<code>pixel_values_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>pixel_values</code> that do not have their past calculated should be
passed as <code>pixel_values</code>.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTFeatureExtractor">ImageGPTFeatureExtractor</a>. See
<a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTFeatureExtractor.__call__">transformers.ImageGPTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.n_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>pixel_values</code>
which have their past given to this model should not be passed as <code>pixel_values</code> as they have already
been computed.`,name:"past_key_values"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, pixel_values_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>pixel_values</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>pixel_values</code> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ImageGPTForCausalImageModeling.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = pixel_values</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to
<code>-100</code> are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>CausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig"
>ImageGPTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the
cached key, value states of the self-attention and the cross-attention layers if model is used in
encoder-decoder setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>CausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new oa({props:{$$slots:{default:[ci]},$$scope:{ctx:S}}}),et=new aa({props:{code:`from transformers import ImageGPTFeatureExtractor, ImageGPTForCausalImageModeling
import torch
import matplotlib.pyplot as plt
import numpy as np

feature_extractor = ImageGPTFeatureExtractor.from_pretrained('openai/imagegpt-small')
model = ImageGPTForCausalImageModeling.from_pretrained('openai/imagegpt-small')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# unconditional generation of 8 images
batch_size = 8
context = torch.full((batch_size, 1), model.config.vocab_size - 1) #initialize with SOS token
context = torch.tensor(context).to(device)
output = model.generate(pixel_values=context, max_length=model.config.n_positions + 1, temperature=1.0, do_sample=True, top_k=40)

clusters = feature_extractor.clusters
n_px = feature_extractor.size

samples = output[:,1:].cpu().detach().numpy()
samples_img = [np.reshape(np.rint(127.5 * (clusters[s] + 1.0)), [n_px, n_px, 3]).astype(np.uint8) for s in samples] # convert color cluster tokens back to pixels
f, axes = plt.subplots(1, batch_size, dpi=300)

for img, ax in zip(samples_img, axes):
   ax.axis('off')
   ax.imshow(img),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ImageGPTFeatureExtractor, ImageGPTForCausalImageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = ImageGPTFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;openai/imagegpt-small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ImageGPTForCausalImageModeling.from_pretrained(<span class="hljs-string">&#x27;openai/imagegpt-small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># unconditional generation of 8 images</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">8</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>context = torch.full((batch_size, <span class="hljs-number">1</span>), model.config.vocab_size - <span class="hljs-number">1</span>) <span class="hljs-comment">#initialize with SOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>context = torch.tensor(context).to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>output = model.generate(pixel_values=context, max_length=model.config.n_positions + <span class="hljs-number">1</span>, temperature=<span class="hljs-number">1.0</span>, do_sample=<span class="hljs-literal">True</span>, top_k=<span class="hljs-number">40</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>clusters = feature_extractor.clusters
<span class="hljs-meta">&gt;&gt;&gt; </span>n_px = feature_extractor.size

<span class="hljs-meta">&gt;&gt;&gt; </span>samples = output[:,<span class="hljs-number">1</span>:].cpu().detach().numpy()
<span class="hljs-meta">&gt;&gt;&gt; </span>samples_img = [np.reshape(np.rint(<span class="hljs-number">127.5</span> * (clusters[s] + <span class="hljs-number">1.0</span>)), [n_px, n_px, <span class="hljs-number">3</span>]).astype(np.uint8) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> samples] <span class="hljs-comment"># convert color cluster tokens back to pixels</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>f, axes = plt.subplots(<span class="hljs-number">1</span>, batch_size, dpi=<span class="hljs-number">300</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> img, ax <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(samples_img, axes):
<span class="hljs-meta">... </span>   ax.axis(<span class="hljs-string">&#x27;off&#x27;</span>)
<span class="hljs-meta">... </span>   ax.imshow(img)`}}),tt=new ct({}),ot=new re({props:{name:"class transformers.ImageGPTForImageClassification",anchor:"transformers.ImageGPTForImageClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/imagegpt/modeling_imagegpt.py#L1053",parametersDescription:[{anchor:"transformers.ImageGPTForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),it=new re({props:{name:"forward",anchor:"transformers.ImageGPTForImageClassification.forward",parameters:[{name:"pixel_values",val:" = None"},{name:"past_key_values",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/imagegpt/modeling_imagegpt.py#L1065",parametersDescription:[{anchor:"transformers.ImageGPTForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, pixel_values_length)</code>) &#x2014;
<code>pixel_values_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>pixel_values</code> that do not have their past calculated should be
passed as <code>pixel_values</code>.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTFeatureExtractor">ImageGPTFeatureExtractor</a>. See
<a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTFeatureExtractor.__call__">transformers.ImageGPTFeatureExtractor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.ImageGPTForImageClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.n_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>pixel_values</code>
which have their past given to this model should not be passed as <code>pixel_values</code> as they have already
been computed.`,name:"past_key_values"},{anchor:"transformers.ImageGPTForImageClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ImageGPTForImageClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, pixel_values_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.ImageGPTForImageClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.ImageGPTForImageClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ImageGPTForImageClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>pixel_values</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <code>pixel_values</code> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.ImageGPTForImageClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ImageGPTForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ImageGPTForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ImageGPTForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ImageGPTForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),
If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <code>SequenceClassifierOutputWithPast</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig"
>ImageGPTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>SequenceClassifierOutputWithPast</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ve=new oa({props:{$$slots:{default:[mi]},$$scope:{ctx:S}}}),lt=new aa({props:{code:`from transformers import ImageGPTFeatureExtractor, ImageGPTForImageClassification
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

feature_extractor = ImageGPTFeatureExtractor.from_pretrained('openai/imagegpt-small')
model = ImageGPTForImageClassification.from_pretrained('openai/imagegpt-small')

inputs = feature_extractor(images=image, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ImageGPTFeatureExtractor, ImageGPTForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = ImageGPTFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;openai/imagegpt-small&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ImageGPTForImageClassification.from_pretrained(<span class="hljs-string">&#x27;openai/imagegpt-small&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){h=n("meta"),P=d(),g=n("h1"),u=n("a"),x=n("span"),v(f.$$.fragment),_=d(),C=n("span"),sa=a("ImageGPT"),uo=d(),Q=n("h2"),ie=n("a"),Mt=n("span"),v(ye.$$.fragment),na=d(),zt=n("span"),ra=a("Overview"),fo=d(),le=n("p"),ia=a("The ImageGPT model was proposed in "),we=n("a"),la=a("Generative Pretraining from Pixels"),da=a(` by Mark
Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever. ImageGPT (iGPT) is a GPT-2-like
model trained to predict the next pixel value, allowing for both unconditional and conditional image generation.`),_o=d(),mt=n("p"),ca=a("The abstract from the paper is the following:"),vo=d(),pt=n("p"),jt=n("em"),ma=a(`Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models
can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels,
without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels,
we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and
low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide
ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also
competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0%
top-1 accuracy on a linear probe of our features.`),To=d(),de=n("p"),pa=a("The figure below summarizes the approach (taken from the "),Pe=n("a"),ha=a("original paper"),ga=a("):"),Io=d(),xe=n("img"),bo=d(),ht=n("p"),ua=a("Tips:"),yo=d(),M=n("ul"),ke=n("li"),fa=a("ImageGPT is almost exactly the same as "),gt=n("a"),_a=a("GPT-2"),va=a(`, with the exception that a different activation function
is used (namely \u201Cquick gelu\u201D), and the layer normalization layers don\u2019t mean center the inputs. ImageGPT also doesn\u2019t
have tied input- and output embeddings.`),Ta=d(),Ge=n("li"),Ia=a(`As the time- and memory requirements of the attention mechanism of Transformers scales quadratically in the sequence
length, the authors pre-trained ImageGPT on smaller input resolutions, such as 32x32 and 64x64. However, feeding a
sequence of 32x32x3=3072 tokens from 0..255 into a Transformer is still prohibitively large. Therefore, the authors
applied k-means clustering to the (R,G,B) pixel values with k=512. This way, we only have a 32*32 = 1024-long
sequence, but now of integers in the range 0..511. So we are shrinking the sequence length at the cost of a bigger
embedding matrix. In other words, the vocabulary size of ImageGPT is 512, + 1 for a special \u201Cstart of sentence\u201D (SOS)
token, used at the beginning of every sequence. One can use `),ut=n("a"),ba=a("ImageGPTFeatureExtractor"),ya=a(` to
prepare images for the model.`),wa=d(),$e=n("li"),Pa=a(`Despite being pre-trained entirely unsupervised (i.e. without the use of any labels), ImageGPT produces fairly
performant image features useful for downstream tasks, such as image classification. The authors showed that the
features in the middle of the network are the most performant, and can be used as-is to train a linear model (such as
a sklearn logistic regression model for example). This is also referred to as \u201Clinear probing\u201D. Features can be
easily obtained by first forwarding the image through the model, then specifying `),qt=n("em"),xa=a("output_hidden_states=True"),ka=a(`, and
then average-pool the hidden states at whatever layer you like.`),Ga=d(),Ee=n("li"),$a=a(`Alternatively, one can further fine-tune the entire model on a downstream dataset, similar to BERT. For this, you can
use `),ft=n("a"),Ea=a("ImageGPTForImageClassification"),Fa=a("."),Ca=d(),At=n("li"),Ma=a(`ImageGPT comes in different sizes: there\u2019s ImageGPT-small, ImageGPT-medium and ImageGPT-large. The authors did also
train an XL variant, which they didn\u2019t release. The differences in size are summarized in the following table:`),wo=d(),k=n("p"),za=a("| "),Nt=n("strong"),ja=a("Model variant"),qa=a(" | "),Lt=n("strong"),Aa=a("Number of layers"),Na=a(" | "),St=n("strong"),La=a("Hidden size"),Sa=a(" | "),Dt=n("strong"),Da=a("Number of heads"),Oa=a(" | "),Ot=n("strong"),Wa=a("# params"),Ra=a(` |
| iGPT-small        | 24                   | 512             | 8                   | 76 million   |
| iGPT-medium       | 36                   | 1024            | 8                   | 455 million  |
| iGPT-large        | 48                   | 1536            | 16                  | 1.4 million  |
| iGPT-XL           | 60                   | 3072            | not specified       | 6.8 billion  |`),Po=d(),N=n("p"),Ba=a("This model was contributed by "),Fe=n("a"),Ha=a("nielsr"),Va=a(", based on "),Ce=n("a"),Ua=a("this issue"),Ja=a(". The original code can be found "),Me=n("a"),Xa=a("here"),Qa=a("."),xo=d(),K=n("h2"),ce=n("a"),Wt=n("span"),v(ze.$$.fragment),Ka=d(),Rt=n("span"),Za=a("ImageGPTConfig"),ko=d(),G=n("div"),v(je.$$.fragment),Ya=d(),D=n("p"),es=a("This is the configuration class to store the configuration of a "),_t=n("a"),ts=a("ImageGPTModel"),os=a(` or a
`),Bt=n("code"),as=a("TFImageGPTModel"),ss=a(`. It is used to instantiate a GPT-2 model according to the specified
arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar
configuration to that of the ImageGPT `),qe=n("a"),ns=a("small"),rs=a(" architecture."),is=d(),Z=n("p"),ls=a("Configuration objects inherit from "),vt=n("a"),ds=a("PretrainedConfig"),cs=a(` and can be used to control the model
outputs. Read the documentation from `),Tt=n("a"),ms=a("PretrainedConfig"),ps=a(" for more information."),hs=d(),Ht=n("p"),gs=a("Example:"),us=d(),v(Ae.$$.fragment),Go=d(),Y=n("h2"),me=n("a"),Vt=n("span"),v(Ne.$$.fragment),fs=d(),Ut=n("span"),_s=a("ImageGPTFeatureExtractor"),$o=d(),A=n("div"),v(Le.$$.fragment),vs=d(),Jt=n("p"),Ts=a(`Constructs an ImageGPT feature extractor. This feature extractor can be used to resize images to a smaller
resolution (such as 32x32 or 64x64), normalize them and finally color quantize them to obtain sequences of \u201Cpixel
values\u201D (color clusters).`),Is=d(),Se=n("p"),bs=a("This feature extractor inherits from "),Xt=n("code"),ys=a("FeatureExtractionMixin"),ws=a(` which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.`),Ps=d(),O=n("div"),v(De.$$.fragment),xs=d(),Qt=n("p"),ks=a("Main method to prepare for the model one or several image(s)."),Gs=d(),v(pe.$$.fragment),Eo=d(),ee=n("h2"),he=n("a"),Kt=n("span"),v(Oe.$$.fragment),$s=d(),Zt=n("span"),Es=a("ImageGPTModel"),Fo=d(),$=n("div"),v(We.$$.fragment),Fs=d(),Yt=n("p"),Cs=a("The bare ImageGPT Model transformer outputting raw hidden-states without any specific head on top."),Ms=d(),Re=n("p"),zs=a("This model inherits from "),It=n("a"),js=a("PreTrainedModel"),qs=a(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),As=d(),Be=n("p"),Ns=a("This model is also a PyTorch "),He=n("a"),Ls=a("torch.nn.Module"),Ss=a(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Ds=d(),z=n("div"),v(Ve.$$.fragment),Os=d(),te=n("p"),Ws=a("The "),bt=n("a"),Rs=a("ImageGPTModel"),Bs=a(" forward method, overrides the "),eo=n("code"),Hs=a("__call__"),Vs=a(" special method."),Us=d(),v(ge.$$.fragment),Js=d(),to=n("p"),Xs=a("Examples:"),Qs=d(),v(Ue.$$.fragment),Co=d(),oe=n("h2"),ue=n("a"),oo=n("span"),v(Je.$$.fragment),Ks=d(),ao=n("span"),Zs=a("ImageGPTForCausalImageModeling"),Mo=d(),E=n("div"),v(Xe.$$.fragment),Ys=d(),so=n("p"),en=a(`The ImageGPT Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),tn=d(),Qe=n("p"),on=a("This model inherits from "),yt=n("a"),an=a("PreTrainedModel"),sn=a(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),nn=d(),Ke=n("p"),rn=a("This model is also a PyTorch "),Ze=n("a"),ln=a("torch.nn.Module"),dn=a(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),cn=d(),j=n("div"),v(Ye.$$.fragment),mn=d(),ae=n("p"),pn=a("The "),wt=n("a"),hn=a("ImageGPTForCausalImageModeling"),gn=a(" forward method, overrides the "),no=n("code"),un=a("__call__"),fn=a(" special method."),_n=d(),v(fe.$$.fragment),vn=d(),ro=n("p"),Tn=a("Examples:"),In=d(),v(et.$$.fragment),zo=d(),se=n("h2"),_e=n("a"),io=n("span"),v(tt.$$.fragment),bn=d(),lo=n("span"),yn=a("ImageGPTForImageClassification"),jo=d(),F=n("div"),v(ot.$$.fragment),wn=d(),at=n("p"),Pn=a(`The ImageGPT Model transformer with an image classification head on top (linear layer).
`),Pt=n("a"),xn=a("ImageGPTForImageClassification"),kn=a(` average-pools the hidden states in order to do the
classification.`),Gn=d(),st=n("p"),$n=a("This model inherits from "),xt=n("a"),En=a("PreTrainedModel"),Fn=a(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Cn=d(),nt=n("p"),Mn=a("This model is also a PyTorch "),rt=n("a"),zn=a("torch.nn.Module"),jn=a(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),qn=d(),q=n("div"),v(it.$$.fragment),An=d(),ne=n("p"),Nn=a("The "),kt=n("a"),Ln=a("ImageGPTForImageClassification"),Sn=a(" forward method, overrides the "),co=n("code"),Dn=a("__call__"),On=a(" special method."),Wn=d(),v(ve.$$.fragment),Rn=d(),mo=n("p"),Bn=a("Examples:"),Hn=d(),v(lt.$$.fragment),this.h()},l(t){const m=ri('[data-svelte="svelte-1phssyn"]',document.head);h=r(m,"META",{name:!0,content:!0}),m.forEach(o),P=c(t),g=r(t,"H1",{class:!0});var dt=i(g);u=r(dt,"A",{id:!0,class:!0,href:!0});var po=i(u);x=r(po,"SPAN",{});var ho=i(x);T(f.$$.fragment,ho),ho.forEach(o),po.forEach(o),_=c(dt),C=r(dt,"SPAN",{});var go=i(C);sa=s(go,"ImageGPT"),go.forEach(o),dt.forEach(o),uo=c(t),Q=r(t,"H2",{class:!0});var Ao=i(Q);ie=r(Ao,"A",{id:!0,class:!0,href:!0});var Un=i(ie);Mt=r(Un,"SPAN",{});var Jn=i(Mt);T(ye.$$.fragment,Jn),Jn.forEach(o),Un.forEach(o),na=c(Ao),zt=r(Ao,"SPAN",{});var Xn=i(zt);ra=s(Xn,"Overview"),Xn.forEach(o),Ao.forEach(o),fo=c(t),le=r(t,"P",{});var No=i(le);ia=s(No,"The ImageGPT model was proposed in "),we=r(No,"A",{href:!0,rel:!0});var Qn=i(we);la=s(Qn,"Generative Pretraining from Pixels"),Qn.forEach(o),da=s(No,` by Mark
Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever. ImageGPT (iGPT) is a GPT-2-like
model trained to predict the next pixel value, allowing for both unconditional and conditional image generation.`),No.forEach(o),_o=c(t),mt=r(t,"P",{});var Kn=i(mt);ca=s(Kn,"The abstract from the paper is the following:"),Kn.forEach(o),vo=c(t),pt=r(t,"P",{});var Zn=i(pt);jt=r(Zn,"EM",{});var Yn=i(jt);ma=s(Yn,`Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models
can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels,
without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels,
we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and
low-data classification. On CIFAR-10, we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide
ResNet, and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also
competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0%
top-1 accuracy on a linear probe of our features.`),Yn.forEach(o),Zn.forEach(o),To=c(t),de=r(t,"P",{});var Lo=i(de);pa=s(Lo,"The figure below summarizes the approach (taken from the "),Pe=r(Lo,"A",{href:!0,rel:!0});var er=i(Pe);ha=s(er,"original paper"),er.forEach(o),ga=s(Lo,"):"),Lo.forEach(o),Io=c(t),xe=r(t,"IMG",{width:!0,src:!0}),bo=c(t),ht=r(t,"P",{});var tr=i(ht);ua=s(tr,"Tips:"),tr.forEach(o),yo=c(t),M=r(t,"UL",{});var W=i(M);ke=r(W,"LI",{});var So=i(ke);fa=s(So,"ImageGPT is almost exactly the same as "),gt=r(So,"A",{href:!0});var or=i(gt);_a=s(or,"GPT-2"),or.forEach(o),va=s(So,`, with the exception that a different activation function
is used (namely \u201Cquick gelu\u201D), and the layer normalization layers don\u2019t mean center the inputs. ImageGPT also doesn\u2019t
have tied input- and output embeddings.`),So.forEach(o),Ta=c(W),Ge=r(W,"LI",{});var Do=i(Ge);Ia=s(Do,`As the time- and memory requirements of the attention mechanism of Transformers scales quadratically in the sequence
length, the authors pre-trained ImageGPT on smaller input resolutions, such as 32x32 and 64x64. However, feeding a
sequence of 32x32x3=3072 tokens from 0..255 into a Transformer is still prohibitively large. Therefore, the authors
applied k-means clustering to the (R,G,B) pixel values with k=512. This way, we only have a 32*32 = 1024-long
sequence, but now of integers in the range 0..511. So we are shrinking the sequence length at the cost of a bigger
embedding matrix. In other words, the vocabulary size of ImageGPT is 512, + 1 for a special \u201Cstart of sentence\u201D (SOS)
token, used at the beginning of every sequence. One can use `),ut=r(Do,"A",{href:!0});var ar=i(ut);ba=s(ar,"ImageGPTFeatureExtractor"),ar.forEach(o),ya=s(Do,` to
prepare images for the model.`),Do.forEach(o),wa=c(W),$e=r(W,"LI",{});var Oo=i($e);Pa=s(Oo,`Despite being pre-trained entirely unsupervised (i.e. without the use of any labels), ImageGPT produces fairly
performant image features useful for downstream tasks, such as image classification. The authors showed that the
features in the middle of the network are the most performant, and can be used as-is to train a linear model (such as
a sklearn logistic regression model for example). This is also referred to as \u201Clinear probing\u201D. Features can be
easily obtained by first forwarding the image through the model, then specifying `),qt=r(Oo,"EM",{});var sr=i(qt);xa=s(sr,"output_hidden_states=True"),sr.forEach(o),ka=s(Oo,`, and
then average-pool the hidden states at whatever layer you like.`),Oo.forEach(o),Ga=c(W),Ee=r(W,"LI",{});var Wo=i(Ee);$a=s(Wo,`Alternatively, one can further fine-tune the entire model on a downstream dataset, similar to BERT. For this, you can
use `),ft=r(Wo,"A",{href:!0});var nr=i(ft);Ea=s(nr,"ImageGPTForImageClassification"),nr.forEach(o),Fa=s(Wo,"."),Wo.forEach(o),Ca=c(W),At=r(W,"LI",{});var rr=i(At);Ma=s(rr,`ImageGPT comes in different sizes: there\u2019s ImageGPT-small, ImageGPT-medium and ImageGPT-large. The authors did also
train an XL variant, which they didn\u2019t release. The differences in size are summarized in the following table:`),rr.forEach(o),W.forEach(o),wo=c(t),k=r(t,"P",{});var L=i(k);za=s(L,"| "),Nt=r(L,"STRONG",{});var ir=i(Nt);ja=s(ir,"Model variant"),ir.forEach(o),qa=s(L," | "),Lt=r(L,"STRONG",{});var lr=i(Lt);Aa=s(lr,"Number of layers"),lr.forEach(o),Na=s(L," | "),St=r(L,"STRONG",{});var dr=i(St);La=s(dr,"Hidden size"),dr.forEach(o),Sa=s(L," | "),Dt=r(L,"STRONG",{});var cr=i(Dt);Da=s(cr,"Number of heads"),cr.forEach(o),Oa=s(L," | "),Ot=r(L,"STRONG",{});var mr=i(Ot);Wa=s(mr,"# params"),mr.forEach(o),Ra=s(L,` |
| iGPT-small        | 24                   | 512             | 8                   | 76 million   |
| iGPT-medium       | 36                   | 1024            | 8                   | 455 million  |
| iGPT-large        | 48                   | 1536            | 16                  | 1.4 million  |
| iGPT-XL           | 60                   | 3072            | not specified       | 6.8 billion  |`),L.forEach(o),Po=c(t),N=r(t,"P",{});var Te=i(N);Ba=s(Te,"This model was contributed by "),Fe=r(Te,"A",{href:!0,rel:!0});var pr=i(Fe);Ha=s(pr,"nielsr"),pr.forEach(o),Va=s(Te,", based on "),Ce=r(Te,"A",{href:!0,rel:!0});var hr=i(Ce);Ua=s(hr,"this issue"),hr.forEach(o),Ja=s(Te,". The original code can be found "),Me=r(Te,"A",{href:!0,rel:!0});var gr=i(Me);Xa=s(gr,"here"),gr.forEach(o),Qa=s(Te,"."),Te.forEach(o),xo=c(t),K=r(t,"H2",{class:!0});var Ro=i(K);ce=r(Ro,"A",{id:!0,class:!0,href:!0});var ur=i(ce);Wt=r(ur,"SPAN",{});var fr=i(Wt);T(ze.$$.fragment,fr),fr.forEach(o),ur.forEach(o),Ka=c(Ro),Rt=r(Ro,"SPAN",{});var _r=i(Rt);Za=s(_r,"ImageGPTConfig"),_r.forEach(o),Ro.forEach(o),ko=c(t),G=r(t,"DIV",{class:!0});var R=i(G);T(je.$$.fragment,R),Ya=c(R),D=r(R,"P",{});var Ie=i(D);es=s(Ie,"This is the configuration class to store the configuration of a "),_t=r(Ie,"A",{href:!0});var vr=i(_t);ts=s(vr,"ImageGPTModel"),vr.forEach(o),os=s(Ie,` or a
`),Bt=r(Ie,"CODE",{});var Tr=i(Bt);as=s(Tr,"TFImageGPTModel"),Tr.forEach(o),ss=s(Ie,`. It is used to instantiate a GPT-2 model according to the specified
arguments, defining the model architecture. Instantiating a configuration with the defaults will yield a similar
configuration to that of the ImageGPT `),qe=r(Ie,"A",{href:!0,rel:!0});var Ir=i(qe);ns=s(Ir,"small"),Ir.forEach(o),rs=s(Ie," architecture."),Ie.forEach(o),is=c(R),Z=r(R,"P",{});var Gt=i(Z);ls=s(Gt,"Configuration objects inherit from "),vt=r(Gt,"A",{href:!0});var br=i(vt);ds=s(br,"PretrainedConfig"),br.forEach(o),cs=s(Gt,` and can be used to control the model
outputs. Read the documentation from `),Tt=r(Gt,"A",{href:!0});var yr=i(Tt);ms=s(yr,"PretrainedConfig"),yr.forEach(o),ps=s(Gt," for more information."),Gt.forEach(o),hs=c(R),Ht=r(R,"P",{});var wr=i(Ht);gs=s(wr,"Example:"),wr.forEach(o),us=c(R),T(Ae.$$.fragment,R),R.forEach(o),Go=c(t),Y=r(t,"H2",{class:!0});var Bo=i(Y);me=r(Bo,"A",{id:!0,class:!0,href:!0});var Pr=i(me);Vt=r(Pr,"SPAN",{});var xr=i(Vt);T(Ne.$$.fragment,xr),xr.forEach(o),Pr.forEach(o),fs=c(Bo),Ut=r(Bo,"SPAN",{});var kr=i(Ut);_s=s(kr,"ImageGPTFeatureExtractor"),kr.forEach(o),Bo.forEach(o),$o=c(t),A=r(t,"DIV",{class:!0});var be=i(A);T(Le.$$.fragment,be),vs=c(be),Jt=r(be,"P",{});var Gr=i(Jt);Ts=s(Gr,`Constructs an ImageGPT feature extractor. This feature extractor can be used to resize images to a smaller
resolution (such as 32x32 or 64x64), normalize them and finally color quantize them to obtain sequences of \u201Cpixel
values\u201D (color clusters).`),Gr.forEach(o),Is=c(be),Se=r(be,"P",{});var Ho=i(Se);bs=s(Ho,"This feature extractor inherits from "),Xt=r(Ho,"CODE",{});var $r=i(Xt);ys=s($r,"FeatureExtractionMixin"),$r.forEach(o),ws=s(Ho,` which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.`),Ho.forEach(o),Ps=c(be),O=r(be,"DIV",{class:!0});var $t=i(O);T(De.$$.fragment,$t),xs=c($t),Qt=r($t,"P",{});var Er=i(Qt);ks=s(Er,"Main method to prepare for the model one or several image(s)."),Er.forEach(o),Gs=c($t),T(pe.$$.fragment,$t),$t.forEach(o),be.forEach(o),Eo=c(t),ee=r(t,"H2",{class:!0});var Vo=i(ee);he=r(Vo,"A",{id:!0,class:!0,href:!0});var Fr=i(he);Kt=r(Fr,"SPAN",{});var Cr=i(Kt);T(Oe.$$.fragment,Cr),Cr.forEach(o),Fr.forEach(o),$s=c(Vo),Zt=r(Vo,"SPAN",{});var Mr=i(Zt);Es=s(Mr,"ImageGPTModel"),Mr.forEach(o),Vo.forEach(o),Fo=c(t),$=r(t,"DIV",{class:!0});var B=i($);T(We.$$.fragment,B),Fs=c(B),Yt=r(B,"P",{});var zr=i(Yt);Cs=s(zr,"The bare ImageGPT Model transformer outputting raw hidden-states without any specific head on top."),zr.forEach(o),Ms=c(B),Re=r(B,"P",{});var Uo=i(Re);zs=s(Uo,"This model inherits from "),It=r(Uo,"A",{href:!0});var jr=i(It);js=s(jr,"PreTrainedModel"),jr.forEach(o),qs=s(Uo,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Uo.forEach(o),As=c(B),Be=r(B,"P",{});var Jo=i(Be);Ns=s(Jo,"This model is also a PyTorch "),He=r(Jo,"A",{href:!0,rel:!0});var qr=i(He);Ls=s(qr,"torch.nn.Module"),qr.forEach(o),Ss=s(Jo,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Jo.forEach(o),Ds=c(B),z=r(B,"DIV",{class:!0});var H=i(z);T(Ve.$$.fragment,H),Os=c(H),te=r(H,"P",{});var Et=i(te);Ws=s(Et,"The "),bt=r(Et,"A",{href:!0});var Ar=i(bt);Rs=s(Ar,"ImageGPTModel"),Ar.forEach(o),Bs=s(Et," forward method, overrides the "),eo=r(Et,"CODE",{});var Nr=i(eo);Hs=s(Nr,"__call__"),Nr.forEach(o),Vs=s(Et," special method."),Et.forEach(o),Us=c(H),T(ge.$$.fragment,H),Js=c(H),to=r(H,"P",{});var Lr=i(to);Xs=s(Lr,"Examples:"),Lr.forEach(o),Qs=c(H),T(Ue.$$.fragment,H),H.forEach(o),B.forEach(o),Co=c(t),oe=r(t,"H2",{class:!0});var Xo=i(oe);ue=r(Xo,"A",{id:!0,class:!0,href:!0});var Sr=i(ue);oo=r(Sr,"SPAN",{});var Dr=i(oo);T(Je.$$.fragment,Dr),Dr.forEach(o),Sr.forEach(o),Ks=c(Xo),ao=r(Xo,"SPAN",{});var Or=i(ao);Zs=s(Or,"ImageGPTForCausalImageModeling"),Or.forEach(o),Xo.forEach(o),Mo=c(t),E=r(t,"DIV",{class:!0});var V=i(E);T(Xe.$$.fragment,V),Ys=c(V),so=r(V,"P",{});var Wr=i(so);en=s(Wr,`The ImageGPT Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Wr.forEach(o),tn=c(V),Qe=r(V,"P",{});var Qo=i(Qe);on=s(Qo,"This model inherits from "),yt=r(Qo,"A",{href:!0});var Rr=i(yt);an=s(Rr,"PreTrainedModel"),Rr.forEach(o),sn=s(Qo,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Qo.forEach(o),nn=c(V),Ke=r(V,"P",{});var Ko=i(Ke);rn=s(Ko,"This model is also a PyTorch "),Ze=r(Ko,"A",{href:!0,rel:!0});var Br=i(Ze);ln=s(Br,"torch.nn.Module"),Br.forEach(o),dn=s(Ko,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Ko.forEach(o),cn=c(V),j=r(V,"DIV",{class:!0});var U=i(j);T(Ye.$$.fragment,U),mn=c(U),ae=r(U,"P",{});var Ft=i(ae);pn=s(Ft,"The "),wt=r(Ft,"A",{href:!0});var Hr=i(wt);hn=s(Hr,"ImageGPTForCausalImageModeling"),Hr.forEach(o),gn=s(Ft," forward method, overrides the "),no=r(Ft,"CODE",{});var Vr=i(no);un=s(Vr,"__call__"),Vr.forEach(o),fn=s(Ft," special method."),Ft.forEach(o),_n=c(U),T(fe.$$.fragment,U),vn=c(U),ro=r(U,"P",{});var Ur=i(ro);Tn=s(Ur,"Examples:"),Ur.forEach(o),In=c(U),T(et.$$.fragment,U),U.forEach(o),V.forEach(o),zo=c(t),se=r(t,"H2",{class:!0});var Zo=i(se);_e=r(Zo,"A",{id:!0,class:!0,href:!0});var Jr=i(_e);io=r(Jr,"SPAN",{});var Xr=i(io);T(tt.$$.fragment,Xr),Xr.forEach(o),Jr.forEach(o),bn=c(Zo),lo=r(Zo,"SPAN",{});var Qr=i(lo);yn=s(Qr,"ImageGPTForImageClassification"),Qr.forEach(o),Zo.forEach(o),jo=c(t),F=r(t,"DIV",{class:!0});var J=i(F);T(ot.$$.fragment,J),wn=c(J),at=r(J,"P",{});var Yo=i(at);Pn=s(Yo,`The ImageGPT Model transformer with an image classification head on top (linear layer).
`),Pt=r(Yo,"A",{href:!0});var Kr=i(Pt);xn=s(Kr,"ImageGPTForImageClassification"),Kr.forEach(o),kn=s(Yo,` average-pools the hidden states in order to do the
classification.`),Yo.forEach(o),Gn=c(J),st=r(J,"P",{});var ea=i(st);$n=s(ea,"This model inherits from "),xt=r(ea,"A",{href:!0});var Zr=i(xt);En=s(Zr,"PreTrainedModel"),Zr.forEach(o),Fn=s(ea,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ea.forEach(o),Cn=c(J),nt=r(J,"P",{});var ta=i(nt);Mn=s(ta,"This model is also a PyTorch "),rt=r(ta,"A",{href:!0,rel:!0});var Yr=i(rt);zn=s(Yr,"torch.nn.Module"),Yr.forEach(o),jn=s(ta,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),ta.forEach(o),qn=c(J),q=r(J,"DIV",{class:!0});var X=i(q);T(it.$$.fragment,X),An=c(X),ne=r(X,"P",{});var Ct=i(ne);Nn=s(Ct,"The "),kt=r(Ct,"A",{href:!0});var ei=i(kt);Ln=s(ei,"ImageGPTForImageClassification"),ei.forEach(o),Sn=s(Ct," forward method, overrides the "),co=r(Ct,"CODE",{});var ti=i(co);Dn=s(ti,"__call__"),ti.forEach(o),On=s(Ct," special method."),Ct.forEach(o),Wn=c(X),T(ve.$$.fragment,X),Rn=c(X),mo=r(X,"P",{});var oi=i(mo);Bn=s(oi,"Examples:"),oi.forEach(o),Hn=c(X),T(lt.$$.fragment,X),X.forEach(o),J.forEach(o),this.h()},h(){l(h,"name","hf:doc:metadata"),l(h,"content",JSON.stringify(hi)),l(u,"id","imagegpt"),l(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(u,"href","#imagegpt"),l(g,"class","relative group"),l(ie,"id","overview"),l(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ie,"href","#overview"),l(Q,"class","relative group"),l(we,"href","https://openai.com/blog/image-gpt/"),l(we,"rel","nofollow"),l(Pe,"href","https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf"),l(Pe,"rel","nofollow"),l(xe,"width","600"),ii(xe.src,Vn="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/imagegpt_architecture.png")||l(xe,"src",Vn),l(gt,"href","/docs/transformers/master/en/gpt2"),l(ut,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTFeatureExtractor"),l(ft,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),l(Fe,"href","https://huggingface.co/nielsr"),l(Fe,"rel","nofollow"),l(Ce,"href","https://github.com/openai/image-gpt/issues/7"),l(Ce,"rel","nofollow"),l(Me,"href","https://github.com/openai/image-gpt"),l(Me,"rel","nofollow"),l(ce,"id","transformers.ImageGPTConfig"),l(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ce,"href","#transformers.ImageGPTConfig"),l(K,"class","relative group"),l(_t,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel"),l(qe,"href","https://huggingface.co/imagegpt"),l(qe,"rel","nofollow"),l(vt,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),l(Tt,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),l(G,"class","docstring"),l(me,"id","transformers.ImageGPTFeatureExtractor"),l(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(me,"href","#transformers.ImageGPTFeatureExtractor"),l(Y,"class","relative group"),l(O,"class","docstring"),l(A,"class","docstring"),l(he,"id","transformers.ImageGPTModel"),l(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(he,"href","#transformers.ImageGPTModel"),l(ee,"class","relative group"),l(It,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),l(He,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(He,"rel","nofollow"),l(bt,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel"),l(z,"class","docstring"),l($,"class","docstring"),l(ue,"id","transformers.ImageGPTForCausalImageModeling"),l(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ue,"href","#transformers.ImageGPTForCausalImageModeling"),l(oe,"class","relative group"),l(yt,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),l(Ze,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Ze,"rel","nofollow"),l(wt,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForCausalImageModeling"),l(j,"class","docstring"),l(E,"class","docstring"),l(_e,"id","transformers.ImageGPTForImageClassification"),l(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_e,"href","#transformers.ImageGPTForImageClassification"),l(se,"class","relative group"),l(Pt,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),l(xt,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),l(rt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(rt,"rel","nofollow"),l(kt,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),l(q,"class","docstring"),l(F,"class","docstring")},m(t,m){e(document.head,h),p(t,P,m),p(t,g,m),e(g,u),e(u,x),I(f,x,null),e(g,_),e(g,C),e(C,sa),p(t,uo,m),p(t,Q,m),e(Q,ie),e(ie,Mt),I(ye,Mt,null),e(Q,na),e(Q,zt),e(zt,ra),p(t,fo,m),p(t,le,m),e(le,ia),e(le,we),e(we,la),e(le,da),p(t,_o,m),p(t,mt,m),e(mt,ca),p(t,vo,m),p(t,pt,m),e(pt,jt),e(jt,ma),p(t,To,m),p(t,de,m),e(de,pa),e(de,Pe),e(Pe,ha),e(de,ga),p(t,Io,m),p(t,xe,m),p(t,bo,m),p(t,ht,m),e(ht,ua),p(t,yo,m),p(t,M,m),e(M,ke),e(ke,fa),e(ke,gt),e(gt,_a),e(ke,va),e(M,Ta),e(M,Ge),e(Ge,Ia),e(Ge,ut),e(ut,ba),e(Ge,ya),e(M,wa),e(M,$e),e($e,Pa),e($e,qt),e(qt,xa),e($e,ka),e(M,Ga),e(M,Ee),e(Ee,$a),e(Ee,ft),e(ft,Ea),e(Ee,Fa),e(M,Ca),e(M,At),e(At,Ma),p(t,wo,m),p(t,k,m),e(k,za),e(k,Nt),e(Nt,ja),e(k,qa),e(k,Lt),e(Lt,Aa),e(k,Na),e(k,St),e(St,La),e(k,Sa),e(k,Dt),e(Dt,Da),e(k,Oa),e(k,Ot),e(Ot,Wa),e(k,Ra),p(t,Po,m),p(t,N,m),e(N,Ba),e(N,Fe),e(Fe,Ha),e(N,Va),e(N,Ce),e(Ce,Ua),e(N,Ja),e(N,Me),e(Me,Xa),e(N,Qa),p(t,xo,m),p(t,K,m),e(K,ce),e(ce,Wt),I(ze,Wt,null),e(K,Ka),e(K,Rt),e(Rt,Za),p(t,ko,m),p(t,G,m),I(je,G,null),e(G,Ya),e(G,D),e(D,es),e(D,_t),e(_t,ts),e(D,os),e(D,Bt),e(Bt,as),e(D,ss),e(D,qe),e(qe,ns),e(D,rs),e(G,is),e(G,Z),e(Z,ls),e(Z,vt),e(vt,ds),e(Z,cs),e(Z,Tt),e(Tt,ms),e(Z,ps),e(G,hs),e(G,Ht),e(Ht,gs),e(G,us),I(Ae,G,null),p(t,Go,m),p(t,Y,m),e(Y,me),e(me,Vt),I(Ne,Vt,null),e(Y,fs),e(Y,Ut),e(Ut,_s),p(t,$o,m),p(t,A,m),I(Le,A,null),e(A,vs),e(A,Jt),e(Jt,Ts),e(A,Is),e(A,Se),e(Se,bs),e(Se,Xt),e(Xt,ys),e(Se,ws),e(A,Ps),e(A,O),I(De,O,null),e(O,xs),e(O,Qt),e(Qt,ks),e(O,Gs),I(pe,O,null),p(t,Eo,m),p(t,ee,m),e(ee,he),e(he,Kt),I(Oe,Kt,null),e(ee,$s),e(ee,Zt),e(Zt,Es),p(t,Fo,m),p(t,$,m),I(We,$,null),e($,Fs),e($,Yt),e(Yt,Cs),e($,Ms),e($,Re),e(Re,zs),e(Re,It),e(It,js),e(Re,qs),e($,As),e($,Be),e(Be,Ns),e(Be,He),e(He,Ls),e(Be,Ss),e($,Ds),e($,z),I(Ve,z,null),e(z,Os),e(z,te),e(te,Ws),e(te,bt),e(bt,Rs),e(te,Bs),e(te,eo),e(eo,Hs),e(te,Vs),e(z,Us),I(ge,z,null),e(z,Js),e(z,to),e(to,Xs),e(z,Qs),I(Ue,z,null),p(t,Co,m),p(t,oe,m),e(oe,ue),e(ue,oo),I(Je,oo,null),e(oe,Ks),e(oe,ao),e(ao,Zs),p(t,Mo,m),p(t,E,m),I(Xe,E,null),e(E,Ys),e(E,so),e(so,en),e(E,tn),e(E,Qe),e(Qe,on),e(Qe,yt),e(yt,an),e(Qe,sn),e(E,nn),e(E,Ke),e(Ke,rn),e(Ke,Ze),e(Ze,ln),e(Ke,dn),e(E,cn),e(E,j),I(Ye,j,null),e(j,mn),e(j,ae),e(ae,pn),e(ae,wt),e(wt,hn),e(ae,gn),e(ae,no),e(no,un),e(ae,fn),e(j,_n),I(fe,j,null),e(j,vn),e(j,ro),e(ro,Tn),e(j,In),I(et,j,null),p(t,zo,m),p(t,se,m),e(se,_e),e(_e,io),I(tt,io,null),e(se,bn),e(se,lo),e(lo,yn),p(t,jo,m),p(t,F,m),I(ot,F,null),e(F,wn),e(F,at),e(at,Pn),e(at,Pt),e(Pt,xn),e(at,kn),e(F,Gn),e(F,st),e(st,$n),e(st,xt),e(xt,En),e(st,Fn),e(F,Cn),e(F,nt),e(nt,Mn),e(nt,rt),e(rt,zn),e(nt,jn),e(F,qn),e(F,q),I(it,q,null),e(q,An),e(q,ne),e(ne,Nn),e(ne,kt),e(kt,Ln),e(ne,Sn),e(ne,co),e(co,Dn),e(ne,On),e(q,Wn),I(ve,q,null),e(q,Rn),e(q,mo),e(mo,Bn),e(q,Hn),I(lt,q,null),qo=!0},p(t,[m]){const dt={};m&2&&(dt.$$scope={dirty:m,ctx:t}),pe.$set(dt);const po={};m&2&&(po.$$scope={dirty:m,ctx:t}),ge.$set(po);const ho={};m&2&&(ho.$$scope={dirty:m,ctx:t}),fe.$set(ho);const go={};m&2&&(go.$$scope={dirty:m,ctx:t}),ve.$set(go)},i(t){qo||(b(f.$$.fragment,t),b(ye.$$.fragment,t),b(ze.$$.fragment,t),b(je.$$.fragment,t),b(Ae.$$.fragment,t),b(Ne.$$.fragment,t),b(Le.$$.fragment,t),b(De.$$.fragment,t),b(pe.$$.fragment,t),b(Oe.$$.fragment,t),b(We.$$.fragment,t),b(Ve.$$.fragment,t),b(ge.$$.fragment,t),b(Ue.$$.fragment,t),b(Je.$$.fragment,t),b(Xe.$$.fragment,t),b(Ye.$$.fragment,t),b(fe.$$.fragment,t),b(et.$$.fragment,t),b(tt.$$.fragment,t),b(ot.$$.fragment,t),b(it.$$.fragment,t),b(ve.$$.fragment,t),b(lt.$$.fragment,t),qo=!0)},o(t){y(f.$$.fragment,t),y(ye.$$.fragment,t),y(ze.$$.fragment,t),y(je.$$.fragment,t),y(Ae.$$.fragment,t),y(Ne.$$.fragment,t),y(Le.$$.fragment,t),y(De.$$.fragment,t),y(pe.$$.fragment,t),y(Oe.$$.fragment,t),y(We.$$.fragment,t),y(Ve.$$.fragment,t),y(ge.$$.fragment,t),y(Ue.$$.fragment,t),y(Je.$$.fragment,t),y(Xe.$$.fragment,t),y(Ye.$$.fragment,t),y(fe.$$.fragment,t),y(et.$$.fragment,t),y(tt.$$.fragment,t),y(ot.$$.fragment,t),y(it.$$.fragment,t),y(ve.$$.fragment,t),y(lt.$$.fragment,t),qo=!1},d(t){o(h),t&&o(P),t&&o(g),w(f),t&&o(uo),t&&o(Q),w(ye),t&&o(fo),t&&o(le),t&&o(_o),t&&o(mt),t&&o(vo),t&&o(pt),t&&o(To),t&&o(de),t&&o(Io),t&&o(xe),t&&o(bo),t&&o(ht),t&&o(yo),t&&o(M),t&&o(wo),t&&o(k),t&&o(Po),t&&o(N),t&&o(xo),t&&o(K),w(ze),t&&o(ko),t&&o(G),w(je),w(Ae),t&&o(Go),t&&o(Y),w(Ne),t&&o($o),t&&o(A),w(Le),w(De),w(pe),t&&o(Eo),t&&o(ee),w(Oe),t&&o(Fo),t&&o($),w(We),w(Ve),w(ge),w(Ue),t&&o(Co),t&&o(oe),w(Je),t&&o(Mo),t&&o(E),w(Xe),w(Ye),w(fe),w(et),t&&o(zo),t&&o(se),w(tt),t&&o(jo),t&&o(F),w(ot),w(it),w(ve),w(lt)}}}const hi={local:"imagegpt",sections:[{local:"overview",title:"Overview"},{local:"transformers.ImageGPTConfig",title:"ImageGPTConfig"},{local:"transformers.ImageGPTFeatureExtractor",title:"ImageGPTFeatureExtractor"},{local:"transformers.ImageGPTModel",title:"ImageGPTModel"},{local:"transformers.ImageGPTForCausalImageModeling",title:"ImageGPTForCausalImageModeling"},{local:"transformers.ImageGPTForImageClassification",title:"ImageGPTForImageClassification"}],title:"ImageGPT"};function gi(S,h,P){let{fw:g}=h;return S.$$set=u=>{"fw"in u&&P(0,g=u.fw)},[g]}class bi extends ai{constructor(h){super();si(this,h,gi,pi,ni,{fw:0})}}export{bi as default,hi as metadata};
