import{S as Ul,i as Vl,s as Jl,e as n,k as d,w as _,t as r,L as Rl,c as s,d as o,m as c,a,x as T,h as i,b as l,J as e,g as u,y as v,q as y,o as k,B as b}from"../../../chunks/vendor-e859c359.js";import{T as sn}from"../../../chunks/Tip-edc75249.js";import{D as V}from"../../../chunks/Docstring-ade913b3.js";import{C as jt}from"../../../chunks/CodeBlock-ce4317c2.js";import{I as ge}from"../../../chunks/IconCopyLink-5fae3b20.js";import"../../../chunks/CopyButton-77addb3d.js";function Xl(B){let p,w,m,P,N;return{c(){p=n("p"),w=r(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=n("code"),P=r("Module"),N=r(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);w=i(f,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s(f,"CODE",{});var x=a(m);P=i(x,"Module"),x.forEach(o),N=i(f,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,w),e(p,m),e(m,P),e(p,N)},d(g){g&&o(p)}}}function Kl(B){let p,w,m,P,N;return{c(){p=n("p"),w=r(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=n("code"),P=r("Module"),N=r(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);w=i(f,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s(f,"CODE",{});var x=a(m);P=i(x,"Module"),x.forEach(o),N=i(f,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,w),e(p,m),e(m,P),e(p,N)},d(g){g&&o(p)}}}function Ql(B){let p,w,m,P,N;return{c(){p=n("p"),w=r(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=n("code"),P=r("Module"),N=r(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);w=i(f,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s(f,"CODE",{});var x=a(m);P=i(x,"Module"),x.forEach(o),N=i(f,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,w),e(p,m),e(m,P),e(p,N)},d(g){g&&o(p)}}}function Yl(B){let p,w,m,P,N;return{c(){p=n("p"),w=r(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=n("code"),P=r("Module"),N=r(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);w=i(f,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s(f,"CODE",{});var x=a(m);P=i(x,"Module"),x.forEach(o),N=i(f,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,w),e(p,m),e(m,P),e(p,N)},d(g){g&&o(p)}}}function Zl(B){let p,w,m,P,N;return{c(){p=n("p"),w=r(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=n("code"),P=r("Module"),N=r(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(g){p=s(g,"P",{});var f=a(p);w=i(f,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),m=s(f,"CODE",{});var x=a(m);P=i(x,"Module"),x.forEach(o),N=i(f,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),f.forEach(o)},m(g,f){u(g,p,f),e(p,w),e(p,m),e(m,P),e(p,N)},d(g){g&&o(p)}}}function ed(B){let p,w,m,P,N,g,f,x,Un,an,te,_e,ao,qe,Vn,ro,Jn,rn,J,Rn,je,Xn,Kn,Ae,Qn,Yn,ln,At,Zn,dn,Te,es,Ie,ts,os,cn,oe,ve,io,Le,ns,lo,ss,pn,ye,as,co,rs,is,hn,Se,un,ne,ke,po,De,ls,ho,ds,mn,W,Be,cs,se,ps,It,hs,us,Oe,ms,fs,gs,ae,_s,Lt,Ts,vs,St,ys,ks,fn,re,be,uo,We,bs,mo,Ps,gn,z,He,ws,fo,Ns,xs,Ue,$s,Dt,Gs,Ms,Fs,Ve,zs,Je,Es,Cs,qs,C,Re,js,ie,As,Bt,Is,Ls,go,Ss,Ds,Bs,Pe,Os,_o,Ws,Hs,Xe,_n,le,we,To,Ke,Us,vo,Vs,Tn,E,Qe,Js,yo,Rs,Xs,Ye,Ks,Ot,Qs,Ys,Zs,Ze,ea,et,ta,oa,na,q,tt,sa,de,aa,Wt,ra,ia,ko,la,da,ca,Ne,pa,bo,ha,ua,ot,vn,ce,xe,Po,nt,ma,wo,fa,yn,$,st,ga,No,_a,Ta,Ht,Ut,va,ya,ka,O,ba,xo,Pa,wa,$o,Na,xa,Go,$a,Ga,Mo,Ma,Fa,za,at,Ea,Vt,Ca,qa,ja,rt,Aa,it,Ia,La,Sa,F,lt,Da,pe,Ba,Jt,Oa,Wa,Fo,Ha,Ua,Va,$e,Ja,zo,Ra,Xa,dt,Ka,Eo,Qa,Ya,ct,kn,he,Ge,Co,pt,Za,qo,er,bn,G,ht,tr,jo,or,nr,ut,sr,Rt,ar,rr,ir,mt,lr,ft,dr,cr,pr,Ao,hr,ur,H,Io,gt,mr,fr,Lo,_t,gr,_r,So,Tt,Tr,vr,Do,vt,yr,kr,j,yt,br,ue,Pr,Bo,wr,Nr,Oo,xr,$r,Gr,Me,Mr,Wo,Fr,zr,kt,Pn,me,Fe,Ho,bt,Er,Uo,Cr,wn,M,Pt,qr,Vo,jr,Ar,wt,Ir,Xt,Lr,Sr,Dr,Nt,Br,xt,Or,Wr,Hr,Jo,Ur,Vr,U,Ro,$t,Jr,Rr,Xo,Gt,Xr,Kr,Ko,Mt,Qr,Yr,Qo,Ft,Zr,ei,A,zt,ti,fe,oi,Yo,ni,si,Zo,ai,ri,ii,ze,li,en,di,ci,Et,Nn;return g=new ge({}),qe=new ge({}),Le=new ge({}),Se=new jt({props:{code:`from transformers import GPTNeoForCausalLM, GPT2Tokenizer
model = GPTNeoForCausalLM.from_pretrained("EleutherAI/gpt-neo-1.3B")
tokenizer = GPT2Tokenizer.from_pretrained("EleutherAI/gpt-neo-1.3B")

prompt = "In a shocking finding, scientists discovered a herd of unicorns living in a remote, " \\
         "previously unexplored valley, in the Andes Mountains. Even more surprising to the " \\
         "researchers was the fact that the unicorns spoke perfect English."

input_ids = tokenizer(prompt, return_tensors="pt").input_ids

gen_tokens = model.generate(input_ids, do_sample=True, temperature=0.9, max_length=100,)
gen_text = tokenizer.batch_decode(gen_tokens)[0],`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoForCausalLM, GPT2Tokenizer</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">model = GPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&quot;EleutherAI/gpt-neo-1.3B&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">prompt = <span class="hljs-string">&quot;In a shocking finding, scientists discovered a herd of unicorns living in a remote, &quot;</span> \\</span>
<span class="hljs-meta">...</span> <span class="language-python">         <span class="hljs-string">&quot;previously unexplored valley, in the Andes Mountains. Even more surprising to the &quot;</span> \\</span>
<span class="hljs-meta">...</span> <span class="language-python">         <span class="hljs-string">&quot;researchers was the fact that the unicorns spoke perfect English.&quot;</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">gen_tokens = model.generate(input_ids, do_sample=<span class="hljs-literal">True</span>, temperature=<span class="hljs-number">0.9</span>, max_length=<span class="hljs-number">100</span>,)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">gen_text = tokenizer.batch_decode(gen_tokens)[<span class="hljs-number">0</span>]</span>`}}),De=new ge({}),Be=new V({props:{name:"class transformers.GPTNeoConfig",anchor:"transformers.GPTNeoConfig",parameters:[{name:"vocab_size",val:" = 50257"},{name:"max_position_embeddings",val:" = 2048"},{name:"hidden_size",val:" = 2048"},{name:"num_layers",val:" = 24"},{name:"attention_types",val:" = [[['global', 'local'], 12]]"},{name:"num_heads",val:" = 16"},{name:"intermediate_size",val:" = None"},{name:"window_size",val:" = 256"},{name:"activation_function",val:" = 'gelu_new'"},{name:"resid_dropout",val:" = 0.0"},{name:"embed_dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"layer_norm_epsilon",val:" = 1e-05"},{name:"initializer_range",val:" = 0.02"},{name:"summary_type",val:" = 'cls_index'"},{name:"summary_use_proj",val:" = True"},{name:"summary_activation",val:" = None"},{name:"summary_proj_to_labels",val:" = True"},{name:"summary_first_dropout",val:" = 0.1"},{name:"use_cache",val:" = True"},{name:"bos_token_id",val:" = 50256"},{name:"eos_token_id",val:" = 50256"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/configuration_gpt_neo.py#L34",parametersDescription:[{anchor:"transformers.GPTNeoConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50257) &#x2014;
Vocabulary size of the GPT Neo model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>. Vocabulary size of the model.
Defines the different tokens that can be represented by the <em>inputs_ids</em> passed to the forward method of
<a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>.`,name:"vocab_size"},{anchor:"transformers.GPTNeoConfig.attention_types",description:`<strong>attention_types</strong> (<code>List</code>, <em>optional</em>, defaults to <code>[[[&quot;global&quot;, &quot;local&quot;], 12]]</code>) &#x2014;
The type of attention for each layer in a <code>List</code> of the following format <code>[[[&quot;attention_type&quot;], num_layerss]]</code> e.g. for a 24 layer model <code>[[[&quot;global&quot;], 24]]</code> or <code>[[[&quot;global&quot;, &quot;local&quot;], 12]]</code>
Choose the value of <code>attention_type</code> from <code>[&quot;global&quot;, &quot;local&quot;]</code>`,name:"attention_types"},{anchor:"transformers.GPTNeoConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GPTNeoConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.GPTNeoConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.GPTNeoConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.GPTNeoConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string,
<code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.GPTNeoConfig.embed_dropout",description:`<strong>embed_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"embed_dropout"},{anchor:"transformers.GPTNeoConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.GPTNeoConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.GPTNeoConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.GPTNeoConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.GPTNeoConfig.layer_norm_epsilon",description:`<strong>layer_norm_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_epsilon"},{anchor:"transformers.GPTNeoConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.</p>
<p>Example &#x2014;:</p>
<blockquote>
<blockquote>
<blockquote>
<p>from transformers import GPTNeoModel, GPTNeoConfig</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<h1 class="relative group">
	<a id="initializing-a-gptneo-eleutherai/gpt-neo-1.3b-style-configuration" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#initializing-a-gptneo-eleutherai/gpt-neo-1.3b-style-configuration">
		<span><iconcopylink></iconcopylink></span>
	</a>
	<span>
		Initializing a GPTNeo EleutherAI/gpt-neo-1.3B style configuration
	</span>
</h1>

<p>configuration = GPTNeoConfig()</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<h1 class="relative group">
	<a id="initializing-a-model-from-the-eleutherai/gpt-neo-1.3b-style-configuration" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#initializing-a-model-from-the-eleutherai/gpt-neo-1.3b-style-configuration">
		<span><iconcopylink></iconcopylink></span>
	</a>
	<span>
		Initializing a model from the EleutherAI/gpt-neo-1.3B style configuration
	</span>
</h1>

<p>model = GPTNeoModel(configuration)</p>
</blockquote>
</blockquote>
</blockquote>
<blockquote>
<blockquote>
<blockquote>
<h1 class="relative group">
	<a id="accessing-the-model-configuration" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#accessing-the-model-configuration">
		<span><iconcopylink></iconcopylink></span>
	</a>
	<span>
		Accessing the model configuration
	</span>
</h1>

<p>configuration = model.config</p>
</blockquote>
</blockquote>
</blockquote>`,name:"use_cache"}]}}),We=new ge({}),He=new V({props:{name:"class transformers.GPTNeoModel",anchor:"transformers.GPTNeoModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L478",parametersDescription:[{anchor:"transformers.GPTNeoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Re=new V({props:{name:"forward",anchor:"transformers.GPTNeoModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"past_key_values",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L499",parametersDescription:[{anchor:"transformers.GPTNeoModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be
passed as <code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which
have their past given to this model should not be passed as <code>input_ids</code> as they have already been
computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
>BaseModelOutputWithPastAndCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions"
>BaseModelOutputWithPastAndCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Pe=new sn({props:{$$slots:{default:[Xl]},$$scope:{ctx:B}}}),Xe=new jt({props:{code:`from transformers import GPT2Tokenizer, GPTNeoModel
import torch

tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')
model = GPTNeoModel.from_pretrained('EleutherAI/gpt-neo-1.3B')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoModel.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Ke=new ge({}),Qe=new V({props:{name:"class transformers.GPTNeoForCausalLM",anchor:"transformers.GPTNeoForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L666",parametersDescription:[{anchor:"transformers.GPTNeoForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),tt=new V({props:{name:"forward",anchor:"transformers.GPTNeoForCausalLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"past_key_values",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L716",parametersDescription:[{anchor:"transformers.GPTNeoForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be
passed as <code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which
have their past given to this model should not be passed as <code>input_ids</code> as they have already been
computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoForCausalLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for language modeling. Note that the labels <strong>are shifted</strong> inside the model, i.e. you can set
<code>labels = input_ids</code> Indices are selected in <code>[-100, 0, ..., config.vocab_size]</code> All labels set to
<code>-100</code> are ignored (masked), the loss is only computed for labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>CausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> tuples of length <code>config.n_layers</code>, with each tuple containing the
cached key, value states of the self-attention and the cross-attention layers if model is used in
encoder-decoder setting. Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithCrossAttentions"
>CausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Ne=new sn({props:{$$slots:{default:[Kl]},$$scope:{ctx:B}}}),ot=new jt({props:{code:`import torch
from transformers import GPT2Tokenizer, GPTNeoForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')
model = GPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs, labels=inputs["input_ids"])
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),nt=new ge({}),st=new V({props:{name:"class transformers.GPTNeoForSequenceClassification",anchor:"transformers.GPTNeoForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L819",parametersDescription:[{anchor:"transformers.GPTNeoForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),lt=new V({props:{name:"forward",anchor:"transformers.GPTNeoForSequenceClassification.forward",parameters:[{name:"input_ids",val:" = None"},{name:"past_key_values",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_gpt_neo.py#L831",parametersDescription:[{anchor:"transformers.GPTNeoForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code> if <code>past_key_values</code> is <code>None</code> else
<code>past_key_values[0][0].shape[-2]</code> (<code>sequence_length</code> of input past key value states). Indices of input
sequence tokens in the vocabulary.</p>
<p>If <code>past_key_values</code> is used, only <code>input_ids</code> that do not have their past calculated should be
passed as <code>input_ids</code>.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple[Tuple[torch.Tensor]]</code> of length <code>config.num_layers</code>) &#x2014;
Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see
<code>past_key_values</code> output below). Can be used to speed up sequential decoding. The <code>input_ids</code> which
have their past given to this model should not be passed as <code>input_ids</code> as they have already been
computed.`,name:"past_key_values"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, input_ids_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>input_ids</code> indices into associated
vectors than the model&#x2019;s internal embedding lookup matrix.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>inputs_embeds</code> have to be input (see
<code>past_key_values</code>).`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up
decoding (see <code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss),
If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <code>SequenceClassifierOutputWithPast</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors
of shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>SequenceClassifierOutputWithPast</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),$e=new sn({props:{$$slots:{default:[Ql]},$$scope:{ctx:B}}}),dt=new jt({props:{code:`from transformers import GPT2Tokenizer, GPTNeoForSequenceClassification
import torch

tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')
model = GPTNeoForSequenceClassification.from_pretrained('EleutherAI/gpt-neo-1.3B')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([<span class="hljs-number">1</span>]).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ct=new jt({props:{code:`from transformers import GPT2Tokenizer, GPTNeoForSequenceClassification
import torch

tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')
model = GPTNeoForSequenceClassification.from_pretrained('EleutherAI/gpt-neo-1.3B', problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([[1, 1]], dtype=torch.float) # need dtype=float for BCEWithLogitsLoss
outputs = model(**inputs, labels=labels)
loss = outputs.loss
logits = outputs.logits,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, GPTNeoForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># need dtype=float for BCEWithLogitsLoss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),pt=new ge({}),ht=new V({props:{name:"class transformers.FlaxGPTNeoModel",anchor:"transformers.FlaxGPTNeoModel",parameters:[{name:"config",val:": GPTNeoConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L583",parametersDescription:[{anchor:"transformers.FlaxGPTNeoModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the
model weights.`,name:"config"},{anchor:"transformers.FlaxGPTNeoModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on
GPUs) and <code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see
<a href="/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and <a href="/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),yt=new V({props:{name:"__call__",anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L393",parametersDescription:[{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code>. Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput"
>FlaxBaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput"
>FlaxBaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Me=new sn({props:{$$slots:{default:[Yl]},$$scope:{ctx:B}}}),kt=new jt({props:{code:`from transformers import GPT2Tokenizer, FlaxGPTNeoModel

tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')
model = FlaxGPTNeoModel.from_pretrained('EleutherAI/gpt-neo-1.3B')

inputs = tokenizer("Hello, my dog is cute", return_tensors='jax')
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, FlaxGPTNeoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxGPTNeoModel.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&#x27;jax&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),bt=new ge({}),Pt=new V({props:{name:"class transformers.FlaxGPTNeoForCausalLM",anchor:"transformers.FlaxGPTNeoForCausalLM",parameters:[{name:"config",val:": GPTNeoConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L648",parametersDescription:[{anchor:"transformers.FlaxGPTNeoForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the
model weights.`,name:"config"},{anchor:"transformers.FlaxGPTNeoForCausalLM.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on
GPUs) and <code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see
<a href="/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and <a href="/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}]}}),zt=new V({props:{name:"__call__",anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py#L393",parametersDescription:[{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
<code>input_ids_length</code> = <code>sequence_length</code>. Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <code>GPTNeoTokenizer</code>. See
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">transformers.PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">transformers.PreTrainedTokenizer.<strong>call</strong>()</a> for
details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxGPTNeoPreTrainedModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"
>GPTNeoConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ze=new sn({props:{$$slots:{default:[Zl]},$$scope:{ctx:B}}}),Et=new jt({props:{code:`from transformers import GPT2Tokenizer, FlaxGPTNeoForCausalLM

tokenizer = GPT2Tokenizer.from_pretrained('EleutherAI/gpt-neo-1.3B')
model = FlaxGPTNeoForCausalLM.from_pretrained('EleutherAI/gpt-neo-1.3B')

inputs = tokenizer("Hello, my dog is cute", return_tensors="np")
outputs = model(**inputs)

# retrieve logts for next token
next_token_logits = outputs.logits[:, -1],`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2Tokenizer, FlaxGPTNeoForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPT2Tokenizer.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxGPTNeoForCausalLM.from_pretrained(<span class="hljs-string">&#x27;EleutherAI/gpt-neo-1.3B&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve logts for next token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>next_token_logits = outputs.logits[:, -<span class="hljs-number">1</span>]`}}),{c(){p=n("meta"),w=d(),m=n("h1"),P=n("a"),N=n("span"),_(g.$$.fragment),f=d(),x=n("span"),Un=r("GPT Neo"),an=d(),te=n("h2"),_e=n("a"),ao=n("span"),_(qe.$$.fragment),Vn=d(),ro=n("span"),Jn=r("Overview"),rn=d(),J=n("p"),Rn=r("The GPTNeo model was released in the "),je=n("a"),Xn=r("EleutherAI/gpt-neo"),Kn=r(` repository by Sid
Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like causal language model trained on the
`),Ae=n("a"),Qn=r("Pile"),Yn=r(" dataset."),ln=d(),At=n("p"),Zn=r(`The architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of
256 tokens.`),dn=d(),Te=n("p"),es=r("This model was contributed by "),Ie=n("a"),ts=r("valhalla"),os=r("."),cn=d(),oe=n("h3"),ve=n("a"),io=n("span"),_(Le.$$.fragment),ns=d(),lo=n("span"),ss=r("Generation"),pn=d(),ye=n("p"),as=r("The "),co=n("code"),rs=r("generate()"),is=r(" method can be used to generate text using GPT Neo model."),hn=d(),_(Se.$$.fragment),un=d(),ne=n("h2"),ke=n("a"),po=n("span"),_(De.$$.fragment),ls=d(),ho=n("span"),ds=r("GPTNeoConfig"),mn=d(),W=n("div"),_(Be.$$.fragment),cs=d(),se=n("p"),ps=r("This is the configuration class to store the configuration of a "),It=n("a"),hs=r("GPTNeoModel"),us=r(`. It is used to
instantiate a GPT Neo model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the GPTNeo `),Oe=n("a"),ms=r("gpt-neo-1.3B"),fs=r(" architecture."),gs=d(),ae=n("p"),_s=r("Configuration objects inherit from "),Lt=n("a"),Ts=r("PretrainedConfig"),vs=r(` and can be used to control the model
outputs. Read the documentation from `),St=n("a"),ys=r("PretrainedConfig"),ks=r(" for more information."),fn=d(),re=n("h2"),be=n("a"),uo=n("span"),_(We.$$.fragment),bs=d(),mo=n("span"),Ps=r("GPTNeoModel"),gn=d(),z=n("div"),_(He.$$.fragment),ws=d(),fo=n("p"),Ns=r("The bare GPT Neo Model transformer outputting raw hidden-states without any specific head on top."),xs=d(),Ue=n("p"),$s=r("This model inherits from "),Dt=n("a"),Gs=r("PreTrainedModel"),Ms=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Fs=d(),Ve=n("p"),zs=r("This model is also a PyTorch "),Je=n("a"),Es=r("torch.nn.Module"),Cs=r(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),qs=d(),C=n("div"),_(Re.$$.fragment),js=d(),ie=n("p"),As=r("The "),Bt=n("a"),Is=r("GPTNeoModel"),Ls=r(" forward method, overrides the "),go=n("code"),Ss=r("__call__"),Ds=r(" special method."),Bs=d(),_(Pe.$$.fragment),Os=d(),_o=n("p"),Ws=r("Example:"),Hs=d(),_(Xe.$$.fragment),_n=d(),le=n("h2"),we=n("a"),To=n("span"),_(Ke.$$.fragment),Us=d(),vo=n("span"),Vs=r("GPTNeoForCausalLM"),Tn=d(),E=n("div"),_(Qe.$$.fragment),Js=d(),yo=n("p"),Rs=r(`The GPT Neo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Xs=d(),Ye=n("p"),Ks=r("This model inherits from "),Ot=n("a"),Qs=r("PreTrainedModel"),Ys=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),Zs=d(),Ze=n("p"),ea=r("This model is also a PyTorch "),et=n("a"),ta=r("torch.nn.Module"),oa=r(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),na=d(),q=n("div"),_(tt.$$.fragment),sa=d(),de=n("p"),aa=r("The "),Wt=n("a"),ra=r("GPTNeoForCausalLM"),ia=r(" forward method, overrides the "),ko=n("code"),la=r("__call__"),da=r(" special method."),ca=d(),_(Ne.$$.fragment),pa=d(),bo=n("p"),ha=r("Example:"),ua=d(),_(ot.$$.fragment),vn=d(),ce=n("h2"),xe=n("a"),Po=n("span"),_(nt.$$.fragment),ma=d(),wo=n("span"),fa=r("GPTNeoForSequenceClassification"),yn=d(),$=n("div"),_(st.$$.fragment),ga=d(),No=n("p"),_a=r("The GPTNeo Model transformer with a sequence classification head on top (linear layer)."),Ta=d(),Ht=n("p"),Ut=n("a"),va=r("GPTNeoForSequenceClassification"),ya=r(` uses the last token in order to do the classification, as
other causal models (e.g. GPT-1) do.`),ka=d(),O=n("p"),ba=r(`Since it does classification on the last token, it requires to know the position of the last token. If a
`),xo=n("code"),Pa=r("pad_token_id"),wa=r(` is defined in the configuration, it finds the last token that is not a padding token in each
row. If no `),$o=n("code"),Na=r("pad_token_id"),xa=r(` is defined, it simply takes the last value in each row of the batch. Since it cannot
guess the padding tokens when `),Go=n("code"),$a=r("inputs_embeds"),Ga=r(" are passed instead of "),Mo=n("code"),Ma=r("input_ids"),Fa=r(`, it does the same (take
the last value in each row of the batch).`),za=d(),at=n("p"),Ea=r("This model inherits from "),Vt=n("a"),Ca=r("PreTrainedModel"),qa=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),ja=d(),rt=n("p"),Aa=r("This model is also a PyTorch "),it=n("a"),Ia=r("torch.nn.Module"),La=r(`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Sa=d(),F=n("div"),_(lt.$$.fragment),Da=d(),pe=n("p"),Ba=r("The "),Jt=n("a"),Oa=r("GPTNeoForSequenceClassification"),Wa=r(" forward method, overrides the "),Fo=n("code"),Ha=r("__call__"),Ua=r(" special method."),Va=d(),_($e.$$.fragment),Ja=d(),zo=n("p"),Ra=r("Example of single-label classification:"),Xa=d(),_(dt.$$.fragment),Ka=d(),Eo=n("p"),Qa=r("Example of multi-label classification:"),Ya=d(),_(ct.$$.fragment),kn=d(),he=n("h2"),Ge=n("a"),Co=n("span"),_(pt.$$.fragment),Za=d(),qo=n("span"),er=r("FlaxGPTNeoModel"),bn=d(),G=n("div"),_(ht.$$.fragment),tr=d(),jo=n("p"),or=r("The bare GPTNeo Model transformer outputting raw hidden-states without any specific head on top."),nr=d(),ut=n("p"),sr=r("This model inherits from "),Rt=n("a"),ar=r("FlaxPreTrainedModel"),rr=r(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),ir=d(),mt=n("p"),lr=r("This model is also a Flax Linen "),ft=n("a"),dr=r("flax.nn.Module"),cr=r(` subclass. Use it as a regular Flax
Module and refer to the Flax documentation for all matter related to general usage and behavior.`),pr=d(),Ao=n("p"),hr=r("Finally, this model supports inherent JAX features such as:"),ur=d(),H=n("ul"),Io=n("li"),gt=n("a"),mr=r("Just-In-Time (JIT) compilation"),fr=d(),Lo=n("li"),_t=n("a"),gr=r("Automatic Differentiation"),_r=d(),So=n("li"),Tt=n("a"),Tr=r("Vectorization"),vr=d(),Do=n("li"),vt=n("a"),yr=r("Parallelization"),kr=d(),j=n("div"),_(yt.$$.fragment),br=d(),ue=n("p"),Pr=r("The "),Bo=n("code"),wr=r("FlaxGPTNeoPreTrainedModel"),Nr=r(" forward method, overrides the "),Oo=n("code"),xr=r("__call__"),$r=r(" special method."),Gr=d(),_(Me.$$.fragment),Mr=d(),Wo=n("p"),Fr=r("Example:"),zr=d(),_(kt.$$.fragment),Pn=d(),me=n("h2"),Fe=n("a"),Ho=n("span"),_(bt.$$.fragment),Er=d(),Uo=n("span"),Cr=r("FlaxGPTNeoForCausalLM"),wn=d(),M=n("div"),_(Pt.$$.fragment),qr=d(),Vo=n("p"),jr=r(`The GPTNeo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Ar=d(),wt=n("p"),Ir=r("This model inherits from "),Xt=n("a"),Lr=r("FlaxPreTrainedModel"),Sr=r(`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Dr=d(),Nt=n("p"),Br=r("This model is also a Flax Linen "),xt=n("a"),Or=r("flax.nn.Module"),Wr=r(` subclass. Use it as a regular Flax
Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Hr=d(),Jo=n("p"),Ur=r("Finally, this model supports inherent JAX features such as:"),Vr=d(),U=n("ul"),Ro=n("li"),$t=n("a"),Jr=r("Just-In-Time (JIT) compilation"),Rr=d(),Xo=n("li"),Gt=n("a"),Xr=r("Automatic Differentiation"),Kr=d(),Ko=n("li"),Mt=n("a"),Qr=r("Vectorization"),Yr=d(),Qo=n("li"),Ft=n("a"),Zr=r("Parallelization"),ei=d(),A=n("div"),_(zt.$$.fragment),ti=d(),fe=n("p"),oi=r("The "),Yo=n("code"),ni=r("FlaxGPTNeoPreTrainedModel"),si=r(" forward method, overrides the "),Zo=n("code"),ai=r("__call__"),ri=r(" special method."),ii=d(),_(ze.$$.fragment),li=d(),en=n("p"),di=r("Example:"),ci=d(),_(Et.$$.fragment),this.h()},l(t){const h=Rl('[data-svelte="svelte-1phssyn"]',document.head);p=s(h,"META",{name:!0,content:!0}),h.forEach(o),w=c(t),m=s(t,"H1",{class:!0});var Ct=a(m);P=s(Ct,"A",{id:!0,class:!0,href:!0});var tn=a(P);N=s(tn,"SPAN",{});var on=a(N);T(g.$$.fragment,on),on.forEach(o),tn.forEach(o),f=c(Ct),x=s(Ct,"SPAN",{});var nn=a(x);Un=i(nn,"GPT Neo"),nn.forEach(o),Ct.forEach(o),an=c(t),te=s(t,"H2",{class:!0});var qt=a(te);_e=s(qt,"A",{id:!0,class:!0,href:!0});var hi=a(_e);ao=s(hi,"SPAN",{});var ui=a(ao);T(qe.$$.fragment,ui),ui.forEach(o),hi.forEach(o),Vn=c(qt),ro=s(qt,"SPAN",{});var mi=a(ro);Jn=i(mi,"Overview"),mi.forEach(o),qt.forEach(o),rn=c(t),J=s(t,"P",{});var Kt=a(J);Rn=i(Kt,"The GPTNeo model was released in the "),je=s(Kt,"A",{href:!0,rel:!0});var fi=a(je);Xn=i(fi,"EleutherAI/gpt-neo"),fi.forEach(o),Kn=i(Kt,` repository by Sid
Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like causal language model trained on the
`),Ae=s(Kt,"A",{href:!0,rel:!0});var gi=a(Ae);Qn=i(gi,"Pile"),gi.forEach(o),Yn=i(Kt," dataset."),Kt.forEach(o),ln=c(t),At=s(t,"P",{});var _i=a(At);Zn=i(_i,`The architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of
256 tokens.`),_i.forEach(o),dn=c(t),Te=s(t,"P",{});var xn=a(Te);es=i(xn,"This model was contributed by "),Ie=s(xn,"A",{href:!0,rel:!0});var Ti=a(Ie);ts=i(Ti,"valhalla"),Ti.forEach(o),os=i(xn,"."),xn.forEach(o),cn=c(t),oe=s(t,"H3",{class:!0});var $n=a(oe);ve=s($n,"A",{id:!0,class:!0,href:!0});var vi=a(ve);io=s(vi,"SPAN",{});var yi=a(io);T(Le.$$.fragment,yi),yi.forEach(o),vi.forEach(o),ns=c($n),lo=s($n,"SPAN",{});var ki=a(lo);ss=i(ki,"Generation"),ki.forEach(o),$n.forEach(o),pn=c(t),ye=s(t,"P",{});var Gn=a(ye);as=i(Gn,"The "),co=s(Gn,"CODE",{});var bi=a(co);rs=i(bi,"generate()"),bi.forEach(o),is=i(Gn," method can be used to generate text using GPT Neo model."),Gn.forEach(o),hn=c(t),T(Se.$$.fragment,t),un=c(t),ne=s(t,"H2",{class:!0});var Mn=a(ne);ke=s(Mn,"A",{id:!0,class:!0,href:!0});var Pi=a(ke);po=s(Pi,"SPAN",{});var wi=a(po);T(De.$$.fragment,wi),wi.forEach(o),Pi.forEach(o),ls=c(Mn),ho=s(Mn,"SPAN",{});var Ni=a(ho);ds=i(Ni,"GPTNeoConfig"),Ni.forEach(o),Mn.forEach(o),mn=c(t),W=s(t,"DIV",{class:!0});var Qt=a(W);T(Be.$$.fragment,Qt),cs=c(Qt),se=s(Qt,"P",{});var Yt=a(se);ps=i(Yt,"This is the configuration class to store the configuration of a "),It=s(Yt,"A",{href:!0});var xi=a(It);hs=i(xi,"GPTNeoModel"),xi.forEach(o),us=i(Yt,`. It is used to
instantiate a GPT Neo model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the GPTNeo `),Oe=s(Yt,"A",{href:!0,rel:!0});var $i=a(Oe);ms=i($i,"gpt-neo-1.3B"),$i.forEach(o),fs=i(Yt," architecture."),Yt.forEach(o),gs=c(Qt),ae=s(Qt,"P",{});var Zt=a(ae);_s=i(Zt,"Configuration objects inherit from "),Lt=s(Zt,"A",{href:!0});var Gi=a(Lt);Ts=i(Gi,"PretrainedConfig"),Gi.forEach(o),vs=i(Zt,` and can be used to control the model
outputs. Read the documentation from `),St=s(Zt,"A",{href:!0});var Mi=a(St);ys=i(Mi,"PretrainedConfig"),Mi.forEach(o),ks=i(Zt," for more information."),Zt.forEach(o),Qt.forEach(o),fn=c(t),re=s(t,"H2",{class:!0});var Fn=a(re);be=s(Fn,"A",{id:!0,class:!0,href:!0});var Fi=a(be);uo=s(Fi,"SPAN",{});var zi=a(uo);T(We.$$.fragment,zi),zi.forEach(o),Fi.forEach(o),bs=c(Fn),mo=s(Fn,"SPAN",{});var Ei=a(mo);Ps=i(Ei,"GPTNeoModel"),Ei.forEach(o),Fn.forEach(o),gn=c(t),z=s(t,"DIV",{class:!0});var R=a(z);T(He.$$.fragment,R),ws=c(R),fo=s(R,"P",{});var Ci=a(fo);Ns=i(Ci,"The bare GPT Neo Model transformer outputting raw hidden-states without any specific head on top."),Ci.forEach(o),xs=c(R),Ue=s(R,"P",{});var zn=a(Ue);$s=i(zn,"This model inherits from "),Dt=s(zn,"A",{href:!0});var qi=a(Dt);Gs=i(qi,"PreTrainedModel"),qi.forEach(o),Ms=i(zn,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),zn.forEach(o),Fs=c(R),Ve=s(R,"P",{});var En=a(Ve);zs=i(En,"This model is also a PyTorch "),Je=s(En,"A",{href:!0,rel:!0});var ji=a(Je);Es=i(ji,"torch.nn.Module"),ji.forEach(o),Cs=i(En,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),En.forEach(o),qs=c(R),C=s(R,"DIV",{class:!0});var X=a(C);T(Re.$$.fragment,X),js=c(X),ie=s(X,"P",{});var eo=a(ie);As=i(eo,"The "),Bt=s(eo,"A",{href:!0});var Ai=a(Bt);Is=i(Ai,"GPTNeoModel"),Ai.forEach(o),Ls=i(eo," forward method, overrides the "),go=s(eo,"CODE",{});var Ii=a(go);Ss=i(Ii,"__call__"),Ii.forEach(o),Ds=i(eo," special method."),eo.forEach(o),Bs=c(X),T(Pe.$$.fragment,X),Os=c(X),_o=s(X,"P",{});var Li=a(_o);Ws=i(Li,"Example:"),Li.forEach(o),Hs=c(X),T(Xe.$$.fragment,X),X.forEach(o),R.forEach(o),_n=c(t),le=s(t,"H2",{class:!0});var Cn=a(le);we=s(Cn,"A",{id:!0,class:!0,href:!0});var Si=a(we);To=s(Si,"SPAN",{});var Di=a(To);T(Ke.$$.fragment,Di),Di.forEach(o),Si.forEach(o),Us=c(Cn),vo=s(Cn,"SPAN",{});var Bi=a(vo);Vs=i(Bi,"GPTNeoForCausalLM"),Bi.forEach(o),Cn.forEach(o),Tn=c(t),E=s(t,"DIV",{class:!0});var K=a(E);T(Qe.$$.fragment,K),Js=c(K),yo=s(K,"P",{});var Oi=a(yo);Rs=i(Oi,`The GPT Neo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Oi.forEach(o),Xs=c(K),Ye=s(K,"P",{});var qn=a(Ye);Ks=i(qn,"This model inherits from "),Ot=s(qn,"A",{href:!0});var Wi=a(Ot);Qs=i(Wi,"PreTrainedModel"),Wi.forEach(o),Ys=i(qn,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),qn.forEach(o),Zs=c(K),Ze=s(K,"P",{});var jn=a(Ze);ea=i(jn,"This model is also a PyTorch "),et=s(jn,"A",{href:!0,rel:!0});var Hi=a(et);ta=i(Hi,"torch.nn.Module"),Hi.forEach(o),oa=i(jn,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),jn.forEach(o),na=c(K),q=s(K,"DIV",{class:!0});var Q=a(q);T(tt.$$.fragment,Q),sa=c(Q),de=s(Q,"P",{});var to=a(de);aa=i(to,"The "),Wt=s(to,"A",{href:!0});var Ui=a(Wt);ra=i(Ui,"GPTNeoForCausalLM"),Ui.forEach(o),ia=i(to," forward method, overrides the "),ko=s(to,"CODE",{});var Vi=a(ko);la=i(Vi,"__call__"),Vi.forEach(o),da=i(to," special method."),to.forEach(o),ca=c(Q),T(Ne.$$.fragment,Q),pa=c(Q),bo=s(Q,"P",{});var Ji=a(bo);ha=i(Ji,"Example:"),Ji.forEach(o),ua=c(Q),T(ot.$$.fragment,Q),Q.forEach(o),K.forEach(o),vn=c(t),ce=s(t,"H2",{class:!0});var An=a(ce);xe=s(An,"A",{id:!0,class:!0,href:!0});var Ri=a(xe);Po=s(Ri,"SPAN",{});var Xi=a(Po);T(nt.$$.fragment,Xi),Xi.forEach(o),Ri.forEach(o),ma=c(An),wo=s(An,"SPAN",{});var Ki=a(wo);fa=i(Ki,"GPTNeoForSequenceClassification"),Ki.forEach(o),An.forEach(o),yn=c(t),$=s(t,"DIV",{class:!0});var I=a($);T(st.$$.fragment,I),ga=c(I),No=s(I,"P",{});var Qi=a(No);_a=i(Qi,"The GPTNeo Model transformer with a sequence classification head on top (linear layer)."),Qi.forEach(o),Ta=c(I),Ht=s(I,"P",{});var pi=a(Ht);Ut=s(pi,"A",{href:!0});var Yi=a(Ut);va=i(Yi,"GPTNeoForSequenceClassification"),Yi.forEach(o),ya=i(pi,` uses the last token in order to do the classification, as
other causal models (e.g. GPT-1) do.`),pi.forEach(o),ka=c(I),O=s(I,"P",{});var Y=a(O);ba=i(Y,`Since it does classification on the last token, it requires to know the position of the last token. If a
`),xo=s(Y,"CODE",{});var Zi=a(xo);Pa=i(Zi,"pad_token_id"),Zi.forEach(o),wa=i(Y,` is defined in the configuration, it finds the last token that is not a padding token in each
row. If no `),$o=s(Y,"CODE",{});var el=a($o);Na=i(el,"pad_token_id"),el.forEach(o),xa=i(Y,` is defined, it simply takes the last value in each row of the batch. Since it cannot
guess the padding tokens when `),Go=s(Y,"CODE",{});var tl=a(Go);$a=i(tl,"inputs_embeds"),tl.forEach(o),Ga=i(Y," are passed instead of "),Mo=s(Y,"CODE",{});var ol=a(Mo);Ma=i(ol,"input_ids"),ol.forEach(o),Fa=i(Y,`, it does the same (take
the last value in each row of the batch).`),Y.forEach(o),za=c(I),at=s(I,"P",{});var In=a(at);Ea=i(In,"This model inherits from "),Vt=s(In,"A",{href:!0});var nl=a(Vt);Ca=i(nl,"PreTrainedModel"),nl.forEach(o),qa=i(In,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
pruning heads etc.)`),In.forEach(o),ja=c(I),rt=s(I,"P",{});var Ln=a(rt);Aa=i(Ln,"This model is also a PyTorch "),it=s(Ln,"A",{href:!0,rel:!0});var sl=a(it);Ia=i(sl,"torch.nn.Module"),sl.forEach(o),La=i(Ln,`
subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
general usage and behavior.`),Ln.forEach(o),Sa=c(I),F=s(I,"DIV",{class:!0});var L=a(F);T(lt.$$.fragment,L),Da=c(L),pe=s(L,"P",{});var oo=a(pe);Ba=i(oo,"The "),Jt=s(oo,"A",{href:!0});var al=a(Jt);Oa=i(al,"GPTNeoForSequenceClassification"),al.forEach(o),Wa=i(oo," forward method, overrides the "),Fo=s(oo,"CODE",{});var rl=a(Fo);Ha=i(rl,"__call__"),rl.forEach(o),Ua=i(oo," special method."),oo.forEach(o),Va=c(L),T($e.$$.fragment,L),Ja=c(L),zo=s(L,"P",{});var il=a(zo);Ra=i(il,"Example of single-label classification:"),il.forEach(o),Xa=c(L),T(dt.$$.fragment,L),Ka=c(L),Eo=s(L,"P",{});var ll=a(Eo);Qa=i(ll,"Example of multi-label classification:"),ll.forEach(o),Ya=c(L),T(ct.$$.fragment,L),L.forEach(o),I.forEach(o),kn=c(t),he=s(t,"H2",{class:!0});var Sn=a(he);Ge=s(Sn,"A",{id:!0,class:!0,href:!0});var dl=a(Ge);Co=s(dl,"SPAN",{});var cl=a(Co);T(pt.$$.fragment,cl),cl.forEach(o),dl.forEach(o),Za=c(Sn),qo=s(Sn,"SPAN",{});var pl=a(qo);er=i(pl,"FlaxGPTNeoModel"),pl.forEach(o),Sn.forEach(o),bn=c(t),G=s(t,"DIV",{class:!0});var S=a(G);T(ht.$$.fragment,S),tr=c(S),jo=s(S,"P",{});var hl=a(jo);or=i(hl,"The bare GPTNeo Model transformer outputting raw hidden-states without any specific head on top."),hl.forEach(o),nr=c(S),ut=s(S,"P",{});var Dn=a(ut);sr=i(Dn,"This model inherits from "),Rt=s(Dn,"A",{href:!0});var ul=a(Rt);ar=i(ul,"FlaxPreTrainedModel"),ul.forEach(o),rr=i(Dn,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Dn.forEach(o),ir=c(S),mt=s(S,"P",{});var Bn=a(mt);lr=i(Bn,"This model is also a Flax Linen "),ft=s(Bn,"A",{href:!0,rel:!0});var ml=a(ft);dr=i(ml,"flax.nn.Module"),ml.forEach(o),cr=i(Bn,` subclass. Use it as a regular Flax
Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Bn.forEach(o),pr=c(S),Ao=s(S,"P",{});var fl=a(Ao);hr=i(fl,"Finally, this model supports inherent JAX features such as:"),fl.forEach(o),ur=c(S),H=s(S,"UL",{});var Ee=a(H);Io=s(Ee,"LI",{});var gl=a(Io);gt=s(gl,"A",{href:!0,rel:!0});var _l=a(gt);mr=i(_l,"Just-In-Time (JIT) compilation"),_l.forEach(o),gl.forEach(o),fr=c(Ee),Lo=s(Ee,"LI",{});var Tl=a(Lo);_t=s(Tl,"A",{href:!0,rel:!0});var vl=a(_t);gr=i(vl,"Automatic Differentiation"),vl.forEach(o),Tl.forEach(o),_r=c(Ee),So=s(Ee,"LI",{});var yl=a(So);Tt=s(yl,"A",{href:!0,rel:!0});var kl=a(Tt);Tr=i(kl,"Vectorization"),kl.forEach(o),yl.forEach(o),vr=c(Ee),Do=s(Ee,"LI",{});var bl=a(Do);vt=s(bl,"A",{href:!0,rel:!0});var Pl=a(vt);yr=i(Pl,"Parallelization"),Pl.forEach(o),bl.forEach(o),Ee.forEach(o),kr=c(S),j=s(S,"DIV",{class:!0});var Z=a(j);T(yt.$$.fragment,Z),br=c(Z),ue=s(Z,"P",{});var no=a(ue);Pr=i(no,"The "),Bo=s(no,"CODE",{});var wl=a(Bo);wr=i(wl,"FlaxGPTNeoPreTrainedModel"),wl.forEach(o),Nr=i(no," forward method, overrides the "),Oo=s(no,"CODE",{});var Nl=a(Oo);xr=i(Nl,"__call__"),Nl.forEach(o),$r=i(no," special method."),no.forEach(o),Gr=c(Z),T(Me.$$.fragment,Z),Mr=c(Z),Wo=s(Z,"P",{});var xl=a(Wo);Fr=i(xl,"Example:"),xl.forEach(o),zr=c(Z),T(kt.$$.fragment,Z),Z.forEach(o),S.forEach(o),Pn=c(t),me=s(t,"H2",{class:!0});var On=a(me);Fe=s(On,"A",{id:!0,class:!0,href:!0});var $l=a(Fe);Ho=s($l,"SPAN",{});var Gl=a(Ho);T(bt.$$.fragment,Gl),Gl.forEach(o),$l.forEach(o),Er=c(On),Uo=s(On,"SPAN",{});var Ml=a(Uo);Cr=i(Ml,"FlaxGPTNeoForCausalLM"),Ml.forEach(o),On.forEach(o),wn=c(t),M=s(t,"DIV",{class:!0});var D=a(M);T(Pt.$$.fragment,D),qr=c(D),Vo=s(D,"P",{});var Fl=a(Vo);jr=i(Fl,`The GPTNeo Model transformer with a language modeling head on top (linear layer with weights tied to the input
embeddings).`),Fl.forEach(o),Ar=c(D),wt=s(D,"P",{});var Wn=a(wt);Ir=i(Wn,"This model inherits from "),Xt=s(Wn,"A",{href:!0});var zl=a(Xt);Lr=i(zl,"FlaxPreTrainedModel"),zl.forEach(o),Sr=i(Wn,`. Check the superclass documentation for the
generic methods the library implements for all its model (such as downloading or saving, resizing the input
embeddings, pruning heads etc.)`),Wn.forEach(o),Dr=c(D),Nt=s(D,"P",{});var Hn=a(Nt);Br=i(Hn,"This model is also a Flax Linen "),xt=s(Hn,"A",{href:!0,rel:!0});var El=a(xt);Or=i(El,"flax.nn.Module"),El.forEach(o),Wr=i(Hn,` subclass. Use it as a regular Flax
Module and refer to the Flax documentation for all matter related to general usage and behavior.`),Hn.forEach(o),Hr=c(D),Jo=s(D,"P",{});var Cl=a(Jo);Ur=i(Cl,"Finally, this model supports inherent JAX features such as:"),Cl.forEach(o),Vr=c(D),U=s(D,"UL",{});var Ce=a(U);Ro=s(Ce,"LI",{});var ql=a(Ro);$t=s(ql,"A",{href:!0,rel:!0});var jl=a($t);Jr=i(jl,"Just-In-Time (JIT) compilation"),jl.forEach(o),ql.forEach(o),Rr=c(Ce),Xo=s(Ce,"LI",{});var Al=a(Xo);Gt=s(Al,"A",{href:!0,rel:!0});var Il=a(Gt);Xr=i(Il,"Automatic Differentiation"),Il.forEach(o),Al.forEach(o),Kr=c(Ce),Ko=s(Ce,"LI",{});var Ll=a(Ko);Mt=s(Ll,"A",{href:!0,rel:!0});var Sl=a(Mt);Qr=i(Sl,"Vectorization"),Sl.forEach(o),Ll.forEach(o),Yr=c(Ce),Qo=s(Ce,"LI",{});var Dl=a(Qo);Ft=s(Dl,"A",{href:!0,rel:!0});var Bl=a(Ft);Zr=i(Bl,"Parallelization"),Bl.forEach(o),Dl.forEach(o),Ce.forEach(o),ei=c(D),A=s(D,"DIV",{class:!0});var ee=a(A);T(zt.$$.fragment,ee),ti=c(ee),fe=s(ee,"P",{});var so=a(fe);oi=i(so,"The "),Yo=s(so,"CODE",{});var Ol=a(Yo);ni=i(Ol,"FlaxGPTNeoPreTrainedModel"),Ol.forEach(o),si=i(so," forward method, overrides the "),Zo=s(so,"CODE",{});var Wl=a(Zo);ai=i(Wl,"__call__"),Wl.forEach(o),ri=i(so," special method."),so.forEach(o),ii=c(ee),T(ze.$$.fragment,ee),li=c(ee),en=s(ee,"P",{});var Hl=a(en);di=i(Hl,"Example:"),Hl.forEach(o),ci=c(ee),T(Et.$$.fragment,ee),ee.forEach(o),D.forEach(o),this.h()},h(){l(p,"name","hf:doc:metadata"),l(p,"content",JSON.stringify(td)),l(P,"id","gpt-neo"),l(P,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(P,"href","#gpt-neo"),l(m,"class","relative group"),l(_e,"id","overview"),l(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_e,"href","#overview"),l(te,"class","relative group"),l(je,"href","https://github.com/EleutherAI/gpt-neo"),l(je,"rel","nofollow"),l(Ae,"href","https://pile.eleuther.ai/"),l(Ae,"rel","nofollow"),l(Ie,"href","https://huggingface.co/valhalla"),l(Ie,"rel","nofollow"),l(ve,"id","generation"),l(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ve,"href","#generation"),l(oe,"class","relative group"),l(ke,"id","transformers.GPTNeoConfig"),l(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ke,"href","#transformers.GPTNeoConfig"),l(ne,"class","relative group"),l(It,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel"),l(Oe,"href","https://huggingface.co/EleutherAI/gpt-neo-1.3B"),l(Oe,"rel","nofollow"),l(Lt,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),l(St,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),l(W,"class","docstring"),l(be,"id","transformers.GPTNeoModel"),l(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(be,"href","#transformers.GPTNeoModel"),l(re,"class","relative group"),l(Dt,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),l(Je,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Je,"rel","nofollow"),l(Bt,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel"),l(C,"class","docstring"),l(z,"class","docstring"),l(we,"id","transformers.GPTNeoForCausalLM"),l(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(we,"href","#transformers.GPTNeoForCausalLM"),l(le,"class","relative group"),l(Ot,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),l(et,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(et,"rel","nofollow"),l(Wt,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),l(q,"class","docstring"),l(E,"class","docstring"),l(xe,"id","transformers.GPTNeoForSequenceClassification"),l(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(xe,"href","#transformers.GPTNeoForSequenceClassification"),l(ce,"class","relative group"),l(Ut,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),l(Vt,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),l(it,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(it,"rel","nofollow"),l(Jt,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),l(F,"class","docstring"),l($,"class","docstring"),l(Ge,"id","transformers.FlaxGPTNeoModel"),l(Ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Ge,"href","#transformers.FlaxGPTNeoModel"),l(he,"class","relative group"),l(Rt,"href","/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel"),l(ft,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),l(ft,"rel","nofollow"),l(gt,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),l(gt,"rel","nofollow"),l(_t,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),l(_t,"rel","nofollow"),l(Tt,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),l(Tt,"rel","nofollow"),l(vt,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),l(vt,"rel","nofollow"),l(j,"class","docstring"),l(G,"class","docstring"),l(Fe,"id","transformers.FlaxGPTNeoForCausalLM"),l(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(Fe,"href","#transformers.FlaxGPTNeoForCausalLM"),l(me,"class","relative group"),l(Xt,"href","/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel"),l(xt,"href","https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html"),l(xt,"rel","nofollow"),l($t,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),l($t,"rel","nofollow"),l(Gt,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),l(Gt,"rel","nofollow"),l(Mt,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),l(Mt,"rel","nofollow"),l(Ft,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),l(Ft,"rel","nofollow"),l(A,"class","docstring"),l(M,"class","docstring")},m(t,h){e(document.head,p),u(t,w,h),u(t,m,h),e(m,P),e(P,N),v(g,N,null),e(m,f),e(m,x),e(x,Un),u(t,an,h),u(t,te,h),e(te,_e),e(_e,ao),v(qe,ao,null),e(te,Vn),e(te,ro),e(ro,Jn),u(t,rn,h),u(t,J,h),e(J,Rn),e(J,je),e(je,Xn),e(J,Kn),e(J,Ae),e(Ae,Qn),e(J,Yn),u(t,ln,h),u(t,At,h),e(At,Zn),u(t,dn,h),u(t,Te,h),e(Te,es),e(Te,Ie),e(Ie,ts),e(Te,os),u(t,cn,h),u(t,oe,h),e(oe,ve),e(ve,io),v(Le,io,null),e(oe,ns),e(oe,lo),e(lo,ss),u(t,pn,h),u(t,ye,h),e(ye,as),e(ye,co),e(co,rs),e(ye,is),u(t,hn,h),v(Se,t,h),u(t,un,h),u(t,ne,h),e(ne,ke),e(ke,po),v(De,po,null),e(ne,ls),e(ne,ho),e(ho,ds),u(t,mn,h),u(t,W,h),v(Be,W,null),e(W,cs),e(W,se),e(se,ps),e(se,It),e(It,hs),e(se,us),e(se,Oe),e(Oe,ms),e(se,fs),e(W,gs),e(W,ae),e(ae,_s),e(ae,Lt),e(Lt,Ts),e(ae,vs),e(ae,St),e(St,ys),e(ae,ks),u(t,fn,h),u(t,re,h),e(re,be),e(be,uo),v(We,uo,null),e(re,bs),e(re,mo),e(mo,Ps),u(t,gn,h),u(t,z,h),v(He,z,null),e(z,ws),e(z,fo),e(fo,Ns),e(z,xs),e(z,Ue),e(Ue,$s),e(Ue,Dt),e(Dt,Gs),e(Ue,Ms),e(z,Fs),e(z,Ve),e(Ve,zs),e(Ve,Je),e(Je,Es),e(Ve,Cs),e(z,qs),e(z,C),v(Re,C,null),e(C,js),e(C,ie),e(ie,As),e(ie,Bt),e(Bt,Is),e(ie,Ls),e(ie,go),e(go,Ss),e(ie,Ds),e(C,Bs),v(Pe,C,null),e(C,Os),e(C,_o),e(_o,Ws),e(C,Hs),v(Xe,C,null),u(t,_n,h),u(t,le,h),e(le,we),e(we,To),v(Ke,To,null),e(le,Us),e(le,vo),e(vo,Vs),u(t,Tn,h),u(t,E,h),v(Qe,E,null),e(E,Js),e(E,yo),e(yo,Rs),e(E,Xs),e(E,Ye),e(Ye,Ks),e(Ye,Ot),e(Ot,Qs),e(Ye,Ys),e(E,Zs),e(E,Ze),e(Ze,ea),e(Ze,et),e(et,ta),e(Ze,oa),e(E,na),e(E,q),v(tt,q,null),e(q,sa),e(q,de),e(de,aa),e(de,Wt),e(Wt,ra),e(de,ia),e(de,ko),e(ko,la),e(de,da),e(q,ca),v(Ne,q,null),e(q,pa),e(q,bo),e(bo,ha),e(q,ua),v(ot,q,null),u(t,vn,h),u(t,ce,h),e(ce,xe),e(xe,Po),v(nt,Po,null),e(ce,ma),e(ce,wo),e(wo,fa),u(t,yn,h),u(t,$,h),v(st,$,null),e($,ga),e($,No),e(No,_a),e($,Ta),e($,Ht),e(Ht,Ut),e(Ut,va),e(Ht,ya),e($,ka),e($,O),e(O,ba),e(O,xo),e(xo,Pa),e(O,wa),e(O,$o),e($o,Na),e(O,xa),e(O,Go),e(Go,$a),e(O,Ga),e(O,Mo),e(Mo,Ma),e(O,Fa),e($,za),e($,at),e(at,Ea),e(at,Vt),e(Vt,Ca),e(at,qa),e($,ja),e($,rt),e(rt,Aa),e(rt,it),e(it,Ia),e(rt,La),e($,Sa),e($,F),v(lt,F,null),e(F,Da),e(F,pe),e(pe,Ba),e(pe,Jt),e(Jt,Oa),e(pe,Wa),e(pe,Fo),e(Fo,Ha),e(pe,Ua),e(F,Va),v($e,F,null),e(F,Ja),e(F,zo),e(zo,Ra),e(F,Xa),v(dt,F,null),e(F,Ka),e(F,Eo),e(Eo,Qa),e(F,Ya),v(ct,F,null),u(t,kn,h),u(t,he,h),e(he,Ge),e(Ge,Co),v(pt,Co,null),e(he,Za),e(he,qo),e(qo,er),u(t,bn,h),u(t,G,h),v(ht,G,null),e(G,tr),e(G,jo),e(jo,or),e(G,nr),e(G,ut),e(ut,sr),e(ut,Rt),e(Rt,ar),e(ut,rr),e(G,ir),e(G,mt),e(mt,lr),e(mt,ft),e(ft,dr),e(mt,cr),e(G,pr),e(G,Ao),e(Ao,hr),e(G,ur),e(G,H),e(H,Io),e(Io,gt),e(gt,mr),e(H,fr),e(H,Lo),e(Lo,_t),e(_t,gr),e(H,_r),e(H,So),e(So,Tt),e(Tt,Tr),e(H,vr),e(H,Do),e(Do,vt),e(vt,yr),e(G,kr),e(G,j),v(yt,j,null),e(j,br),e(j,ue),e(ue,Pr),e(ue,Bo),e(Bo,wr),e(ue,Nr),e(ue,Oo),e(Oo,xr),e(ue,$r),e(j,Gr),v(Me,j,null),e(j,Mr),e(j,Wo),e(Wo,Fr),e(j,zr),v(kt,j,null),u(t,Pn,h),u(t,me,h),e(me,Fe),e(Fe,Ho),v(bt,Ho,null),e(me,Er),e(me,Uo),e(Uo,Cr),u(t,wn,h),u(t,M,h),v(Pt,M,null),e(M,qr),e(M,Vo),e(Vo,jr),e(M,Ar),e(M,wt),e(wt,Ir),e(wt,Xt),e(Xt,Lr),e(wt,Sr),e(M,Dr),e(M,Nt),e(Nt,Br),e(Nt,xt),e(xt,Or),e(Nt,Wr),e(M,Hr),e(M,Jo),e(Jo,Ur),e(M,Vr),e(M,U),e(U,Ro),e(Ro,$t),e($t,Jr),e(U,Rr),e(U,Xo),e(Xo,Gt),e(Gt,Xr),e(U,Kr),e(U,Ko),e(Ko,Mt),e(Mt,Qr),e(U,Yr),e(U,Qo),e(Qo,Ft),e(Ft,Zr),e(M,ei),e(M,A),v(zt,A,null),e(A,ti),e(A,fe),e(fe,oi),e(fe,Yo),e(Yo,ni),e(fe,si),e(fe,Zo),e(Zo,ai),e(fe,ri),e(A,ii),v(ze,A,null),e(A,li),e(A,en),e(en,di),e(A,ci),v(Et,A,null),Nn=!0},p(t,[h]){const Ct={};h&2&&(Ct.$$scope={dirty:h,ctx:t}),Pe.$set(Ct);const tn={};h&2&&(tn.$$scope={dirty:h,ctx:t}),Ne.$set(tn);const on={};h&2&&(on.$$scope={dirty:h,ctx:t}),$e.$set(on);const nn={};h&2&&(nn.$$scope={dirty:h,ctx:t}),Me.$set(nn);const qt={};h&2&&(qt.$$scope={dirty:h,ctx:t}),ze.$set(qt)},i(t){Nn||(y(g.$$.fragment,t),y(qe.$$.fragment,t),y(Le.$$.fragment,t),y(Se.$$.fragment,t),y(De.$$.fragment,t),y(Be.$$.fragment,t),y(We.$$.fragment,t),y(He.$$.fragment,t),y(Re.$$.fragment,t),y(Pe.$$.fragment,t),y(Xe.$$.fragment,t),y(Ke.$$.fragment,t),y(Qe.$$.fragment,t),y(tt.$$.fragment,t),y(Ne.$$.fragment,t),y(ot.$$.fragment,t),y(nt.$$.fragment,t),y(st.$$.fragment,t),y(lt.$$.fragment,t),y($e.$$.fragment,t),y(dt.$$.fragment,t),y(ct.$$.fragment,t),y(pt.$$.fragment,t),y(ht.$$.fragment,t),y(yt.$$.fragment,t),y(Me.$$.fragment,t),y(kt.$$.fragment,t),y(bt.$$.fragment,t),y(Pt.$$.fragment,t),y(zt.$$.fragment,t),y(ze.$$.fragment,t),y(Et.$$.fragment,t),Nn=!0)},o(t){k(g.$$.fragment,t),k(qe.$$.fragment,t),k(Le.$$.fragment,t),k(Se.$$.fragment,t),k(De.$$.fragment,t),k(Be.$$.fragment,t),k(We.$$.fragment,t),k(He.$$.fragment,t),k(Re.$$.fragment,t),k(Pe.$$.fragment,t),k(Xe.$$.fragment,t),k(Ke.$$.fragment,t),k(Qe.$$.fragment,t),k(tt.$$.fragment,t),k(Ne.$$.fragment,t),k(ot.$$.fragment,t),k(nt.$$.fragment,t),k(st.$$.fragment,t),k(lt.$$.fragment,t),k($e.$$.fragment,t),k(dt.$$.fragment,t),k(ct.$$.fragment,t),k(pt.$$.fragment,t),k(ht.$$.fragment,t),k(yt.$$.fragment,t),k(Me.$$.fragment,t),k(kt.$$.fragment,t),k(bt.$$.fragment,t),k(Pt.$$.fragment,t),k(zt.$$.fragment,t),k(ze.$$.fragment,t),k(Et.$$.fragment,t),Nn=!1},d(t){o(p),t&&o(w),t&&o(m),b(g),t&&o(an),t&&o(te),b(qe),t&&o(rn),t&&o(J),t&&o(ln),t&&o(At),t&&o(dn),t&&o(Te),t&&o(cn),t&&o(oe),b(Le),t&&o(pn),t&&o(ye),t&&o(hn),b(Se,t),t&&o(un),t&&o(ne),b(De),t&&o(mn),t&&o(W),b(Be),t&&o(fn),t&&o(re),b(We),t&&o(gn),t&&o(z),b(He),b(Re),b(Pe),b(Xe),t&&o(_n),t&&o(le),b(Ke),t&&o(Tn),t&&o(E),b(Qe),b(tt),b(Ne),b(ot),t&&o(vn),t&&o(ce),b(nt),t&&o(yn),t&&o($),b(st),b(lt),b($e),b(dt),b(ct),t&&o(kn),t&&o(he),b(pt),t&&o(bn),t&&o(G),b(ht),b(yt),b(Me),b(kt),t&&o(Pn),t&&o(me),b(bt),t&&o(wn),t&&o(M),b(Pt),b(zt),b(ze),b(Et)}}}const td={local:"gpt-neo",sections:[{local:"overview",sections:[{local:"generation",title:"Generation"}],title:"Overview"},{local:"transformers.GPTNeoConfig",title:"GPTNeoConfig"},{local:"transformers.GPTNeoModel",title:"GPTNeoModel"},{local:"transformers.GPTNeoForCausalLM",title:"GPTNeoForCausalLM"},{local:"transformers.GPTNeoForSequenceClassification",title:"GPTNeoForSequenceClassification"},{local:"transformers.FlaxGPTNeoModel",title:"FlaxGPTNeoModel"},{local:"transformers.FlaxGPTNeoForCausalLM",title:"FlaxGPTNeoForCausalLM"}],title:"GPT Neo"};function od(B,p,w){let{fw:m}=p;return B.$$set=P=>{"fw"in P&&w(0,m=P.fw)},[m]}class dd extends Ul{constructor(p){super();Vl(this,p,od,ed,Jl,{fw:0})}}export{dd as default,td as metadata};
