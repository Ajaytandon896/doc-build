import{S as Ht,i as Kt,s as Ut,e as s,k as u,w as F,t as l,L as Ft,c as o,d as t,m as h,a as r,x as V,h as p,b as c,J as a,g as i,y as G,K as Vt,q as Q,o as X,B as Y}from"../../../chunks/vendor-e859c359.js";import{D as Gt}from"../../../chunks/Docstring-ade913b3.js";import{C as Wt}from"../../../chunks/CodeBlock-ce4317c2.js";import{I as kt}from"../../../chunks/IconCopyLink-5fae3b20.js";import"../../../chunks/CopyButton-77addb3d.js";function Qt(Me){let _,I,f,d,Z,$,Ce,ee,Se,ce,k,w,te,x,Le,ae,Ie,ue,N,Ne,he,R,Re,fe,z,g,De,A,Oe,We,B,He,Ke,Ue,se,Fe,me,m,Ve,oe,Ge,Qe,ne,Xe,Ye,re,Ze,et,de,T,tt,J,at,st,_e,D,ot,ke,q,ge,O,nt,ve,P,be,W,rt,we,H,M,lt,K,pt,it,ze,j,ct,C,ut,ht,Te,v,y,le,S,ft,pe,mt,je,b,L,dt,ie,_t,ye;return $=new kt({}),x=new kt({}),q=new Wt({props:{code:`import torch
from transformers import AutoModel, AutoTokenizer 

bertjapanese = AutoModel.from_pretrained("cl-tohoku/bert-base-japanese")
tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese")

## Input Japanese Text
line = "\u543E\u8F29\u306F\u732B\u3067\u3042\u308B\u3002"

inputs = tokenizer(line, return_tensors="pt")

print(tokenizer.decode(inputs['input_ids'][0]))

outputs = bertjapanese(**inputs),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> torch</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer </span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">bertjapanese = AutoModel.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment">## Input Japanese Text</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">line = <span class="hljs-string">&quot;\u543E\u8F29\u306F\u732B\u3067\u3042\u308B\u3002&quot;</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">inputs = tokenizer(line, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>][<span class="hljs-number">0</span>]))</span>
[CLS] \u543E\u8F29 \u306F \u732B \u3067 \u3042\u308B \u3002 [SEP]

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">outputs = bertjapanese(**inputs)</span>`}}),P=new Wt({props:{code:`bertjapanese = AutoModel.from_pretrained("cl-tohoku/bert-base-japanese-char")
tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-char")

## Input Japanese Text
line = "\u543E\u8F29\u306F\u732B\u3067\u3042\u308B\u3002"

inputs = tokenizer(line, return_tensors="pt")

print(tokenizer.decode(inputs['input_ids'][0]))

outputs = bertjapanese(**inputs),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">bertjapanese = AutoModel.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese-char&quot;</span>)</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese-char&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment">## Input Japanese Text</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">line = <span class="hljs-string">&quot;\u543E\u8F29\u306F\u732B\u3067\u3042\u308B\u3002&quot;</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">inputs = tokenizer(line, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&#x27;input_ids&#x27;</span>][<span class="hljs-number">0</span>]))</span>
[CLS] \u543E \u8F29 \u306F \u732B \u3067 \u3042 \u308B \u3002 [SEP]

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">outputs = bertjapanese(**inputs)</span>`}}),S=new kt({}),L=new Gt({props:{name:"class transformers.BertJapaneseTokenizer",anchor:"transformers.BertJapaneseTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = False"},{name:"do_word_tokenize",val:" = True"},{name:"do_subword_tokenize",val:" = True"},{name:"word_tokenizer_type",val:" = 'basic'"},{name:"subword_tokenizer_type",val:" = 'wordpiece'"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"mecab_kwargs",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L72",parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to a one-wordpiece-per-line vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertJapaneseTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to lower case the input. Only has an effect when do_basic_tokenize=True.`,name:"do_lower_case"},{anchor:"transformers.BertJapaneseTokenizer.do_word_tokenize",description:`<strong>do_word_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to do word tokenization.`,name:"do_word_tokenize"},{anchor:"transformers.BertJapaneseTokenizer.do_subword_tokenize",description:`<strong>do_subword_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to do subword tokenization.`,name:"do_subword_tokenize"},{anchor:"transformers.BertJapaneseTokenizer.word_tokenizer_type",description:`<strong>word_tokenizer_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;basic&quot;</code>) &#x2014;
Type of word tokenizer.`,name:"word_tokenizer_type"},{anchor:"transformers.BertJapaneseTokenizer.subword_tokenizer_type",description:`<strong>subword_tokenizer_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;wordpiece&quot;</code>) &#x2014;
Type of subword tokenizer.`,name:"subword_tokenizer_type"},{anchor:"transformers.BertJapaneseTokenizer.mecab_kwargs",description:`<strong>mecab_kwargs</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Dictionary passed to the <code>MecabTokenizer</code> constructor.`,name:"mecab_kwargs"}]}}),{c(){_=s("meta"),I=u(),f=s("h1"),d=s("a"),Z=s("span"),F($.$$.fragment),Ce=u(),ee=s("span"),Se=l("BertJapanese"),ce=u(),k=s("h2"),w=s("a"),te=s("span"),F(x.$$.fragment),Le=u(),ae=s("span"),Ie=l("Overview"),ue=u(),N=s("p"),Ne=l("The BERT models trained on Japanese text."),he=u(),R=s("p"),Re=l("There are models with two different tokenization methods:"),fe=u(),z=s("ul"),g=s("li"),De=l("Tokenize with MeCab and WordPiece. This requires some extra dependencies, "),A=s("a"),Oe=l("fugashi"),We=l(" which is a wrapper around "),B=s("a"),He=l("MeCab"),Ke=l("."),Ue=u(),se=s("li"),Fe=l("Tokenize into characters."),me=u(),m=s("p"),Ve=l("To use "),oe=s("em"),Ge=l("MecabTokenizer"),Qe=l(", you should "),ne=s("code"),Xe=l('pip install transformers["ja"]'),Ye=l(" (or "),re=s("code"),Ze=l('pip install -e .["ja"]'),et=l(` if you install
from source) to install dependencies.`),de=u(),T=s("p"),tt=l("See "),J=s("a"),at=l("details on cl-tohoku repository"),st=l("."),_e=u(),D=s("p"),ot=l("Example of using a model with MeCab and WordPiece tokenization:"),ke=u(),F(q.$$.fragment),ge=u(),O=s("p"),nt=l("Example of using a model with Character tokenization:"),ve=u(),F(P.$$.fragment),be=u(),W=s("p"),rt=l("Tips:"),we=u(),H=s("ul"),M=s("li"),lt=l("This implementation is the same as BERT, except for tokenization method. Refer to the "),K=s("a"),pt=l("documentation of BERT"),it=l(" for more usage examples."),ze=u(),j=s("p"),ct=l("This model was contributed by "),C=s("a"),ut=l("cl-tohoku"),ht=l("."),Te=u(),v=s("h2"),y=s("a"),le=s("span"),F(S.$$.fragment),ft=u(),pe=s("span"),mt=l("BertJapaneseTokenizer"),je=u(),b=s("div"),F(L.$$.fragment),dt=u(),ie=s("p"),_t=l("Construct a BERT tokenizer for Japanese text, based on a MecabTokenizer."),this.h()},l(e){const n=Ft('[data-svelte="svelte-1phssyn"]',document.head);_=o(n,"META",{name:!0,content:!0}),n.forEach(t),I=h(e),f=o(e,"H1",{class:!0});var Ee=r(f);d=o(Ee,"A",{id:!0,class:!0,href:!0});var gt=r(d);Z=o(gt,"SPAN",{});var vt=r(Z);V($.$$.fragment,vt),vt.forEach(t),gt.forEach(t),Ce=h(Ee),ee=o(Ee,"SPAN",{});var bt=r(ee);Se=p(bt,"BertJapanese"),bt.forEach(t),Ee.forEach(t),ce=h(e),k=o(e,"H2",{class:!0});var $e=r(k);w=o($e,"A",{id:!0,class:!0,href:!0});var wt=r(w);te=o(wt,"SPAN",{});var zt=r(te);V(x.$$.fragment,zt),zt.forEach(t),wt.forEach(t),Le=h($e),ae=o($e,"SPAN",{});var Tt=r(ae);Ie=p(Tt,"Overview"),Tt.forEach(t),$e.forEach(t),ue=h(e),N=o(e,"P",{});var jt=r(N);Ne=p(jt,"The BERT models trained on Japanese text."),jt.forEach(t),he=h(e),R=o(e,"P",{});var yt=r(R);Re=p(yt,"There are models with two different tokenization methods:"),yt.forEach(t),fe=h(e),z=o(e,"UL",{});var xe=r(z);g=o(xe,"LI",{});var U=r(g);De=p(U,"Tokenize with MeCab and WordPiece. This requires some extra dependencies, "),A=o(U,"A",{href:!0,rel:!0});var Et=r(A);Oe=p(Et,"fugashi"),Et.forEach(t),We=p(U," which is a wrapper around "),B=o(U,"A",{href:!0,rel:!0});var $t=r(B);He=p($t,"MeCab"),$t.forEach(t),Ke=p(U,"."),U.forEach(t),Ue=h(xe),se=o(xe,"LI",{});var xt=r(se);Fe=p(xt,"Tokenize into characters."),xt.forEach(t),xe.forEach(t),me=h(e),m=o(e,"P",{});var E=r(m);Ve=p(E,"To use "),oe=o(E,"EM",{});var At=r(oe);Ge=p(At,"MecabTokenizer"),At.forEach(t),Qe=p(E,", you should "),ne=o(E,"CODE",{});var Bt=r(ne);Xe=p(Bt,'pip install transformers["ja"]'),Bt.forEach(t),Ye=p(E," (or "),re=o(E,"CODE",{});var Jt=r(re);Ze=p(Jt,'pip install -e .["ja"]'),Jt.forEach(t),et=p(E,` if you install
from source) to install dependencies.`),E.forEach(t),de=h(e),T=o(e,"P",{});var Ae=r(T);tt=p(Ae,"See "),J=o(Ae,"A",{href:!0,rel:!0});var qt=r(J);at=p(qt,"details on cl-tohoku repository"),qt.forEach(t),st=p(Ae,"."),Ae.forEach(t),_e=h(e),D=o(e,"P",{});var Pt=r(D);ot=p(Pt,"Example of using a model with MeCab and WordPiece tokenization:"),Pt.forEach(t),ke=h(e),V(q.$$.fragment,e),ge=h(e),O=o(e,"P",{});var Mt=r(O);nt=p(Mt,"Example of using a model with Character tokenization:"),Mt.forEach(t),ve=h(e),V(P.$$.fragment,e),be=h(e),W=o(e,"P",{});var Ct=r(W);rt=p(Ct,"Tips:"),Ct.forEach(t),we=h(e),H=o(e,"UL",{});var St=r(H);M=o(St,"LI",{});var Be=r(M);lt=p(Be,"This implementation is the same as BERT, except for tokenization method. Refer to the "),K=o(Be,"A",{href:!0});var Lt=r(K);pt=p(Lt,"documentation of BERT"),Lt.forEach(t),it=p(Be," for more usage examples."),Be.forEach(t),St.forEach(t),ze=h(e),j=o(e,"P",{});var Je=r(j);ct=p(Je,"This model was contributed by "),C=o(Je,"A",{href:!0,rel:!0});var It=r(C);ut=p(It,"cl-tohoku"),It.forEach(t),ht=p(Je,"."),Je.forEach(t),Te=h(e),v=o(e,"H2",{class:!0});var qe=r(v);y=o(qe,"A",{id:!0,class:!0,href:!0});var Nt=r(y);le=o(Nt,"SPAN",{});var Rt=r(le);V(S.$$.fragment,Rt),Rt.forEach(t),Nt.forEach(t),ft=h(qe),pe=o(qe,"SPAN",{});var Dt=r(pe);mt=p(Dt,"BertJapaneseTokenizer"),Dt.forEach(t),qe.forEach(t),je=h(e),b=o(e,"DIV",{class:!0});var Pe=r(b);V(L.$$.fragment,Pe),dt=h(Pe),ie=o(Pe,"P",{});var Ot=r(ie);_t=p(Ot,"Construct a BERT tokenizer for Japanese text, based on a MecabTokenizer."),Ot.forEach(t),Pe.forEach(t),this.h()},h(){c(_,"name","hf:doc:metadata"),c(_,"content",JSON.stringify(Xt)),c(d,"id","bertjapanese"),c(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(d,"href","#bertjapanese"),c(f,"class","relative group"),c(w,"id","overview"),c(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(w,"href","#overview"),c(k,"class","relative group"),c(A,"href","https://github.com/polm/fugashi"),c(A,"rel","nofollow"),c(B,"href","https://taku910.github.io/mecab/"),c(B,"rel","nofollow"),c(J,"href","https://github.com/cl-tohoku/bert-japanese"),c(J,"rel","nofollow"),c(K,"href","/docs/transformers/master/en/bert"),c(C,"href","https://huggingface.co/cl-tohoku"),c(C,"rel","nofollow"),c(y,"id","transformers.BertJapaneseTokenizer"),c(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(y,"href","#transformers.BertJapaneseTokenizer"),c(v,"class","relative group"),c(b,"class","docstring")},m(e,n){a(document.head,_),i(e,I,n),i(e,f,n),a(f,d),a(d,Z),G($,Z,null),a(f,Ce),a(f,ee),a(ee,Se),i(e,ce,n),i(e,k,n),a(k,w),a(w,te),G(x,te,null),a(k,Le),a(k,ae),a(ae,Ie),i(e,ue,n),i(e,N,n),a(N,Ne),i(e,he,n),i(e,R,n),a(R,Re),i(e,fe,n),i(e,z,n),a(z,g),a(g,De),a(g,A),a(A,Oe),a(g,We),a(g,B),a(B,He),a(g,Ke),a(z,Ue),a(z,se),a(se,Fe),i(e,me,n),i(e,m,n),a(m,Ve),a(m,oe),a(oe,Ge),a(m,Qe),a(m,ne),a(ne,Xe),a(m,Ye),a(m,re),a(re,Ze),a(m,et),i(e,de,n),i(e,T,n),a(T,tt),a(T,J),a(J,at),a(T,st),i(e,_e,n),i(e,D,n),a(D,ot),i(e,ke,n),G(q,e,n),i(e,ge,n),i(e,O,n),a(O,nt),i(e,ve,n),G(P,e,n),i(e,be,n),i(e,W,n),a(W,rt),i(e,we,n),i(e,H,n),a(H,M),a(M,lt),a(M,K),a(K,pt),a(M,it),i(e,ze,n),i(e,j,n),a(j,ct),a(j,C),a(C,ut),a(j,ht),i(e,Te,n),i(e,v,n),a(v,y),a(y,le),G(S,le,null),a(v,ft),a(v,pe),a(pe,mt),i(e,je,n),i(e,b,n),G(L,b,null),a(b,dt),a(b,ie),a(ie,_t),ye=!0},p:Vt,i(e){ye||(Q($.$$.fragment,e),Q(x.$$.fragment,e),Q(q.$$.fragment,e),Q(P.$$.fragment,e),Q(S.$$.fragment,e),Q(L.$$.fragment,e),ye=!0)},o(e){X($.$$.fragment,e),X(x.$$.fragment,e),X(q.$$.fragment,e),X(P.$$.fragment,e),X(S.$$.fragment,e),X(L.$$.fragment,e),ye=!1},d(e){t(_),e&&t(I),e&&t(f),Y($),e&&t(ce),e&&t(k),Y(x),e&&t(ue),e&&t(N),e&&t(he),e&&t(R),e&&t(fe),e&&t(z),e&&t(me),e&&t(m),e&&t(de),e&&t(T),e&&t(_e),e&&t(D),e&&t(ke),Y(q,e),e&&t(ge),e&&t(O),e&&t(ve),Y(P,e),e&&t(be),e&&t(W),e&&t(we),e&&t(H),e&&t(ze),e&&t(j),e&&t(Te),e&&t(v),Y(S),e&&t(je),e&&t(b),Y(L)}}}const Xt={local:"bertjapanese",sections:[{local:"overview",title:"Overview"},{local:"transformers.BertJapaneseTokenizer",title:"BertJapaneseTokenizer"}],title:"BertJapanese"};function Yt(Me,_,I){let{fw:f}=_;return Me.$$set=d=>{"fw"in d&&I(0,f=d.fw)},[f]}class oa extends Ht{constructor(_){super();Kt(this,_,Yt,Qt,Ut,{fw:0})}}export{oa as default,Xt as metadata};
