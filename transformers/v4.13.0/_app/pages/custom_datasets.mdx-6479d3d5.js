import{S as hm,i as dm,s as um,e as l,k as f,w as u,t as n,L as cm,c as i,d as t,m as h,a as p,x as c,h as o,b as d,J as s,g as r,y as m,q as _,o as w,B as $}from"../chunks/vendor-e859c359.js";import{T as Cd}from"../chunks/Tip-edc75249.js";import{I as A}from"../chunks/IconCopyLink-5fae3b20.js";import{C as g}from"../chunks/CodeBlock-ce4317c2.js";import"../chunks/CopyButton-77addb3d.js";function mm(G){let b,E,k,y,q,v,z,T;return{c(){b=l("p"),E=n(`For a more in-depth example of how to fine-tune a model for text classification, take a look at the corresponding
`),k=l("a"),y=n("PyTorch notebook"),q=n(`
or `),v=l("a"),z=n("TensorFlow notebook"),T=n("."),this.h()},l(j){b=i(j,"P",{});var x=p(b);E=o(x,`For a more in-depth example of how to fine-tune a model for text classification, take a look at the corresponding
`),k=i(x,"A",{href:!0,rel:!0});var P=p(k);y=o(P,"PyTorch notebook"),P.forEach(t),q=o(x,`
or `),v=i(x,"A",{href:!0,rel:!0});var C=p(v);z=o(C,"TensorFlow notebook"),C.forEach(t),T=o(x,"."),x.forEach(t),this.h()},h(){d(k,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb"),d(k,"rel","nofollow"),d(v,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb"),d(v,"rel","nofollow")},m(j,x){r(j,b,x),s(b,E),s(b,k),s(k,y),s(b,q),s(b,v),s(v,z),s(b,T)},d(j){j&&t(b)}}}function _m(G){let b,E,k,y,q,v,z,T;return{c(){b=l("p"),E=n(`For a more in-depth example of how to fine-tune a model for token classification, take a look at the corresponding
`),k=l("a"),y=n("PyTorch notebook"),q=n(`
or `),v=l("a"),z=n("TensorFlow notebook"),T=n("."),this.h()},l(j){b=i(j,"P",{});var x=p(b);E=o(x,`For a more in-depth example of how to fine-tune a model for token classification, take a look at the corresponding
`),k=i(x,"A",{href:!0,rel:!0});var P=p(k);y=o(P,"PyTorch notebook"),P.forEach(t),q=o(x,`
or `),v=i(x,"A",{href:!0,rel:!0});var C=p(v);z=o(C,"TensorFlow notebook"),C.forEach(t),T=o(x,"."),x.forEach(t),this.h()},h(){d(k,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb"),d(k,"rel","nofollow"),d(v,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification-tf.ipynb"),d(v,"rel","nofollow")},m(j,x){r(j,b,x),s(b,E),s(b,k),s(k,y),s(b,q),s(b,v),s(v,z),s(b,T)},d(j){j&&t(b)}}}function wm(G){let b,E,k,y,q,v,z,T;return{c(){b=l("p"),E=n(`For a more in-depth example of how to fine-tune a model for question answering, take a look at the corresponding
`),k=l("a"),y=n("PyTorch notebook"),q=n(`
or `),v=l("a"),z=n("TensorFlow notebook"),T=n("."),this.h()},l(j){b=i(j,"P",{});var x=p(b);E=o(x,`For a more in-depth example of how to fine-tune a model for question answering, take a look at the corresponding
`),k=i(x,"A",{href:!0,rel:!0});var P=p(k);y=o(P,"PyTorch notebook"),P.forEach(t),q=o(x,`
or `),v=i(x,"A",{href:!0,rel:!0});var C=p(v);z=o(C,"TensorFlow notebook"),C.forEach(t),T=o(x,"."),x.forEach(t),this.h()},h(){d(k,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering.ipynb"),d(k,"rel","nofollow"),d(v,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/question_answering-tf.ipynb"),d(v,"rel","nofollow")},m(j,x){r(j,b,x),s(b,E),s(b,k),s(k,y),s(b,q),s(b,v),s(v,z),s(b,T)},d(j){j&&t(b)}}}function $m(G){let b,E,k,y,q,v,z,T,j,x,P,C,eo,de,wi,st,$i,gi,to,at,so,Es,bi,ao,N,Ca,qs,ki,vi,Da,Ts,xi,yi,Sa,As,ji,no,zs,oo,J,ue,Ia,nt,Ei,Na,qi,ro,ce,Ti,ot,Ai,zi,lo,me,io,K,_e,Ma,rt,Pi,La,Fi,po,Ps,Ci,fo,lt,ho,we,Di,Ba,Si,Ii,uo,it,co,X,$e,Oa,pt,Ni,Ha,Mi,mo,M,Li,Fs,Bi,Oi,ft,Hi,Wi,_o,ht,wo,Cs,Qi,$o,dt,go,L,Ri,Wa,Ui,Vi,Qa,Yi,Gi,bo,ut,ko,F,Ji,Ra,Ki,Xi,Ua,Zi,ep,Va,tp,sp,Ya,ap,np,vo,ct,xo,Z,ge,Ga,mt,op,Ja,rp,yo,be,lp,Ds,ip,pp,jo,_t,Eo,Ss,fp,qo,B,wt,hp,Is,dp,up,cp,$t,mp,Ns,_p,wp,$p,gt,gp,Ka,bp,kp,To,bt,Ao,ee,ke,Xa,kt,vp,Za,xp,zo,Ms,yp,Po,D,jp,Ls,Ep,qp,en,Tp,Ap,tn,zp,Pp,Fo,vt,Co,S,Fp,sn,Cp,Dp,an,Sp,Ip,nn,Np,Mp,Do,xt,So,Bs,Lp,Io,yt,No,ve,Bp,Os,Op,Hp,Mo,jt,Lo,Hs,Wp,Bo,Et,Oo,xe,Qp,on,Rp,Up,Ho,qt,Wo,Ws,Qo,te,ye,rn,Tt,Vp,ln,Yp,Ro,je,Gp,At,Jp,Kp,Uo,Ee,Vo,se,qe,pn,zt,Xp,fn,Zp,Yo,Qs,ef,Go,Pt,Jo,Rs,tf,Ko,Ft,Xo,Us,sf,Zo,Ct,er,Vs,af,tr,O,Ys,hn,nf,of,rf,H,dn,lf,pf,un,ff,hf,cn,df,uf,cf,Gs,mn,mf,_f,sr,ae,Te,_n,Dt,wf,wn,$f,ar,Ae,gf,Js,bf,kf,nr,St,or,ze,vf,$n,xf,yf,rr,It,lr,W,jf,gn,Ef,qf,bn,Tf,Af,ir,Q,Nt,zf,kn,Pf,Ff,Cf,ne,Df,vn,Sf,If,xn,Nf,Mf,Lf,Mt,Bf,yn,Of,Hf,pr,Ks,Wf,fr,Lt,hr,Pe,Qf,jn,Rf,Uf,dr,Bt,ur,Xs,Vf,cr,Ot,mr,oe,Fe,En,Ht,Yf,qn,Gf,_r,Ce,Jf,Zs,Kf,Xf,wr,Wt,$r,De,Zf,ea,eh,th,gr,Qt,br,Se,sh,ta,ah,nh,kr,Rt,vr,sa,oh,xr,Ut,yr,re,Ie,Tn,Vt,rh,An,lh,jr,aa,ih,Er,Yt,qr,R,ph,zn,fh,hh,Pn,dh,uh,Tr,Gt,Ar,Ne,ch,na,mh,_h,zr,Jt,Pr,oa,wh,Fr,Kt,Cr,ra,$h,Dr,Xt,Sr,Me,gh,Fn,bh,kh,Ir,Zt,Nr,la,Mr,le,Le,Cn,es,vh,Dn,xh,Lr,Be,yh,ts,jh,Eh,Br,Oe,Or,ie,He,Sn,ss,qh,In,Th,Hr,ia,Ah,Wr,as,Qr,pa,zh,Rr,ns,Ur,pe,We,Nn,os,Ph,Mn,Fh,Vr,Qe,Ch,fa,Dh,Sh,Yr,rs,Gr,ha,Ih,Jr,U,I,Nh,Ln,Mh,Lh,Bn,Bh,Oh,On,Hh,Wh,Qh,ls,Rh,Hn,Uh,Vh,Yh,is,Gh,Wn,Jh,Kh,Kr,da,Xh,Xr,ps,Zr,Re,Zh,Qn,ed,td,el,fs,tl,ua,sd,sl,hs,al,fe,Ue,Rn,ds,ad,Un,nd,nl,Ve,od,ca,rd,ld,ol,us,rl,Ye,id,ma,pd,fd,ll,cs,il,Ge,hd,_a,dd,ud,pl,ms,fl,wa,cd,hl,_s,dl,he,Je,Vn,ws,md,Yn,_d,ul,$a,wd,cl,$s,ml,V,$d,Gn,gd,bd,Jn,kd,vd,_l,gs,wl,ga,xd,$l,bs,gl,Ke,yd,ba,jd,Ed,bl,ks,kl,ka,qd,vl,vs,xl,Xe,Td,Kn,Ad,zd,yl,xs,jl;return v=new A({}),at=new g({props:{code:"pip install datasets,",highlighted:"pip install datasets"}}),nt=new A({}),me=new Cd({props:{$$slots:{default:[mm]},$$scope:{ctx:G}}}),rt=new A({}),lt=new g({props:{code:`from datasets import load_dataset
imdb = load_dataset("imdb"),`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
imdb = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)`}}),it=new g({props:{code:`imdb["train"][0]
{'label': 1,
 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as "Teachers". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is "Teachers". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'
},`,highlighted:`imdb[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">1</span>,
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as &quot;Teachers&quot;. My 35 years in the teaching profession lead me to believe that Bromwell High\\&#x27;s satire is much closer to reality than is &quot;Teachers&quot;. The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\&#x27; pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\&#x27;m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\&#x27;t!&#x27;</span>
}`}}),pt=new A({}),ht=new g({props:{code:`from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),dt=new g({props:{code:`def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True),`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),ut=new g({props:{code:"tokenized_imdb = imdb.map(preprocess_function, batched=True),",highlighted:'tokenized_imdb = imdb.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),ct=new g({props:{code:`from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`}}),mt=new A({}),_t=new g({props:{code:`from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),bt=new g({props:{code:`from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_imdb["train"],
    eval_dataset=tokenized_imdb["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train(),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir=<span class="hljs-string">&#x27;./results&#x27;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    per_device_train_batch_size=<span class="hljs-number">16</span>,
    per_device_eval_batch_size=<span class="hljs-number">16</span>,
    num_train_epochs=<span class="hljs-number">5</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`}}),kt=new A({}),vt=new g({props:{code:`from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),xt=new g({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=['attention_mask', 'input_ids', 'label'],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=['attention_mask', 'input_ids', 'label'],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
),`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&#x27;attention_mask&#x27;</span>, <span class="hljs-string">&#x27;input_ids&#x27;</span>, <span class="hljs-string">&#x27;label&#x27;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),yt=new g({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(
    init_lr=2e-5, 
    num_warmup_steps=0, 
    num_train_steps=total_train_steps
),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">2e-5</span>, 
    num_warmup_steps=<span class="hljs-number">0</span>, 
    num_train_steps=total_train_steps
)`}}),jt=new g({props:{code:`from transformers import TFAutoModelForSequenceClassification
model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification
model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),Et=new g({props:{code:`import tensorflow as tf
model.compile(optimizer=optimizer),`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),qt=new g({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
),`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),Tt=new A({}),Ee=new Cd({props:{$$slots:{default:[_m]},$$scope:{ctx:G}}}),zt=new A({}),Pt=new g({props:{code:`from datasets import load_dataset
wnut = load_dataset("wnut_17"),`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
wnut = load_dataset(<span class="hljs-string">&quot;wnut_17&quot;</span>)`}}),Ft=new g({props:{code:`wnut["train"][0]
{'id': '0',
 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0],
 'tokens': ['@paulwalk', 'It', "'s", 'the', 'view', 'from', 'where', 'I', "'m", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']
},`,highlighted:`wnut[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;0&#x27;</span>,
 <span class="hljs-string">&#x27;ner_tags&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>, <span class="hljs-number">0</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;tokens&#x27;</span>: [<span class="hljs-string">&#x27;@paulwalk&#x27;</span>, <span class="hljs-string">&#x27;It&#x27;</span>, <span class="hljs-string">&quot;&#x27;s&quot;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;where&#x27;</span>, <span class="hljs-string">&#x27;I&#x27;</span>, <span class="hljs-string">&quot;&#x27;m&quot;</span>, <span class="hljs-string">&#x27;living&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;weeks&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;Empire&#x27;</span>, <span class="hljs-string">&#x27;State&#x27;</span>, <span class="hljs-string">&#x27;Building&#x27;</span>, <span class="hljs-string">&#x27;=&#x27;</span>, <span class="hljs-string">&#x27;ESB&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;Pretty&#x27;</span>, <span class="hljs-string">&#x27;bad&#x27;</span>, <span class="hljs-string">&#x27;storm&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;last&#x27;</span>, <span class="hljs-string">&#x27;evening&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>]
}`}}),Ct=new g({props:{code:`label_list = wnut["train"].features[f"ner_tags"].feature.names
label_list
['O',
 'B-corporation',
 'I-corporation',
 'B-creative-work',
 'I-creative-work',
 'B-group',
 'I-group',
 'B-location',
 'I-location',
 'B-person',
 'I-person',
 'B-product',
 'I-product'
],`,highlighted:`label_list = wnut[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">f&quot;ner_tags&quot;</span>].feature.names
label_list
[<span class="hljs-string">&#x27;O&#x27;</span>,
 <span class="hljs-string">&#x27;B-corporation&#x27;</span>,
 <span class="hljs-string">&#x27;I-corporation&#x27;</span>,
 <span class="hljs-string">&#x27;B-creative-work&#x27;</span>,
 <span class="hljs-string">&#x27;I-creative-work&#x27;</span>,
 <span class="hljs-string">&#x27;B-group&#x27;</span>,
 <span class="hljs-string">&#x27;I-group&#x27;</span>,
 <span class="hljs-string">&#x27;B-location&#x27;</span>,
 <span class="hljs-string">&#x27;I-location&#x27;</span>,
 <span class="hljs-string">&#x27;B-person&#x27;</span>,
 <span class="hljs-string">&#x27;I-person&#x27;</span>,
 <span class="hljs-string">&#x27;B-product&#x27;</span>,
 <span class="hljs-string">&#x27;I-product&#x27;</span>
]`}}),Dt=new A({}),St=new g({props:{code:`from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),It=new g({props:{code:`tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])
tokens
['[CLS]', '@', 'paul', '##walk', 'it', "'", 's', 'the', 'view', 'from', 'where', 'i', "'", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]'],`,highlighted:`tokenized_input = tokenizer(example[<span class="hljs-string">&quot;tokens&quot;</span>], is_split_into_words=<span class="hljs-literal">True</span>)
tokens = tokenizer.convert_ids_to_tokens(tokenized_input[<span class="hljs-string">&quot;input_ids&quot;</span>])
tokens
[<span class="hljs-string">&#x27;[CLS]&#x27;</span>, <span class="hljs-string">&#x27;@&#x27;</span>, <span class="hljs-string">&#x27;paul&#x27;</span>, <span class="hljs-string">&#x27;##walk&#x27;</span>, <span class="hljs-string">&#x27;it&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;s&#x27;</span>, <span class="hljs-string">&#x27;the&#x27;</span>, <span class="hljs-string">&#x27;view&#x27;</span>, <span class="hljs-string">&#x27;from&#x27;</span>, <span class="hljs-string">&#x27;where&#x27;</span>, <span class="hljs-string">&#x27;i&#x27;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&#x27;m&#x27;</span>, <span class="hljs-string">&#x27;living&#x27;</span>, <span class="hljs-string">&#x27;for&#x27;</span>, <span class="hljs-string">&#x27;two&#x27;</span>, <span class="hljs-string">&#x27;weeks&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;empire&#x27;</span>, <span class="hljs-string">&#x27;state&#x27;</span>, <span class="hljs-string">&#x27;building&#x27;</span>, <span class="hljs-string">&#x27;=&#x27;</span>, <span class="hljs-string">&#x27;es&#x27;</span>, <span class="hljs-string">&#x27;##b&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;pretty&#x27;</span>, <span class="hljs-string">&#x27;bad&#x27;</span>, <span class="hljs-string">&#x27;storm&#x27;</span>, <span class="hljs-string">&#x27;here&#x27;</span>, <span class="hljs-string">&#x27;last&#x27;</span>, <span class="hljs-string">&#x27;evening&#x27;</span>, <span class="hljs-string">&#x27;.&#x27;</span>, <span class="hljs-string">&#x27;[SEP]&#x27;</span>]`}}),Lt=new g({props:{code:`def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

    labels = []
    for i, label in enumerate(examples[f"ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:                            # Set the special tokens to -100.
            if word_idx is None:
                label_ids.append(-100)
            elif word_idx != previous_word_idx:              # Only label the first token of a given word.
                label_ids.append(label[word_idx])

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs,`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_and_align_labels</span>(<span class="hljs-params">examples</span>):
    tokenized_inputs = tokenizer(examples[<span class="hljs-string">&quot;tokens&quot;</span>], truncation=<span class="hljs-literal">True</span>, is_split_into_words=<span class="hljs-literal">True</span>)

    labels = []
    <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(examples[<span class="hljs-string">f&quot;ner_tags&quot;</span>]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)  <span class="hljs-comment"># Map tokens to their respective word.</span>
        previous_word_idx = <span class="hljs-literal">None</span>
        label_ids = []
        <span class="hljs-keyword">for</span> word_idx <span class="hljs-keyword">in</span> word_ids:                            <span class="hljs-comment"># Set the special tokens to -100.</span>
            <span class="hljs-keyword">if</span> word_idx <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:
                label_ids.append(-<span class="hljs-number">100</span>)
            <span class="hljs-keyword">elif</span> word_idx != previous_word_idx:              <span class="hljs-comment"># Only label the first token of a given word.</span>
                label_ids.append(label[word_idx])

        labels.append(label_ids)

    tokenized_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels
    <span class="hljs-keyword">return</span> tokenized_inputs`}}),Bt=new g({props:{code:"tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True),",highlighted:'tokenized_wnut = wnut.<span class="hljs-built_in">map</span>(tokenize_and_align_labels, batched=<span class="hljs-literal">True</span>)'}}),Ot=new g({props:{code:`from transformers import DataCollatorForTokenClassification
data_collator = DataCollatorForTokenClassification(tokenizer),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification
data_collator = DataCollatorForTokenClassification(tokenizer)`}}),Ht=new A({}),Wt=new g({props:{code:`from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer
model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased", num_labels=len(label_list)),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForTokenClassification, TrainingArguments, Trainer
model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-built_in">len</span>(label_list))`}}),Qt=new g({props:{code:`training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
),`,highlighted:`training_args = TrainingArguments(
    output_dir=<span class="hljs-string">&#x27;./results&#x27;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    per_device_train_batch_size=<span class="hljs-number">16</span>,
    per_device_eval_batch_size=<span class="hljs-number">16</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)`}}),Rt=new g({props:{code:`trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_wnut["train"],
    eval_dataset=tokenized_wnut["test"],
    data_collator=data_collator,
    tokenizer=tokenizer,
),`,highlighted:`trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_wnut[<span class="hljs-string">&quot;test&quot;</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
)`}}),Ut=new g({props:{code:"trainer.train(),",highlighted:"trainer.train()"}}),Vt=new A({}),Yt=new g({props:{code:`from transformers import DataCollatorForTokenClassification
data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors="tf"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForTokenClassification
data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),Gt=new g({props:{code:`tf_train_set = tokenized_wnut["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_wnut["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
),`,highlighted:`tf_train_set = tokenized_wnut[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_wnut[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),Jt=new g({props:{code:`from transformers import TFAutoModelForTokenClassification
model = TFAutoModelForTokenClassification.from_pretrained("distilbert-base-uncased", num_labels=len(label_list)),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForTokenClassification
model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-built_in">len</span>(label_list))`}}),Kt=new g({props:{code:`from transformers import create_optimizer

batch_size = 16
num_train_epochs = 3
num_train_steps = (len(tokenized_datasets["train"]) // batch_size) * num_train_epochs
optimizer, lr_schedule = create_optimizer(
    init_lr=2e-5,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
    num_warmup_steps=0,
),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer

batch_size = <span class="hljs-number">16</span>
num_train_epochs = <span class="hljs-number">3</span>
num_train_steps = (<span class="hljs-built_in">len</span>(tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size) * num_train_epochs
optimizer, lr_schedule = create_optimizer(
    init_lr=<span class="hljs-number">2e-5</span>,
    num_train_steps=num_train_steps,
    weight_decay_rate=<span class="hljs-number">0.01</span>,
    num_warmup_steps=<span class="hljs-number">0</span>,
)`}}),Xt=new g({props:{code:`import tensorflow as tf
model.compile(optimizer=optimizer),`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),Zt=new g({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
),`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),es=new A({}),Oe=new Cd({props:{$$slots:{default:[wm]},$$scope:{ctx:G}}}),ss=new A({}),as=new g({props:{code:`from datasets import load_dataset
squad = load_dataset("squad"),`,highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
squad = load_dataset(<span class="hljs-string">&quot;squad&quot;</span>)`}}),ns=new g({props:{code:`squad["train"][0]
{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},
 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',
 'id': '5733be284776f41900661182',
 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',
 'title': 'University_of_Notre_Dame'
},`,highlighted:`squad[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers&#x27;</span>: {<span class="hljs-string">&#x27;answer_start&#x27;</span>: [<span class="hljs-number">515</span>], <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&#x27;Saint Bernadette Soubirous&#x27;</span>]},
 <span class="hljs-string">&#x27;context&#x27;</span>: <span class="hljs-string">&#x27;Architecturally, the school has a Catholic character. Atop the Main Building\\&#x27;s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend &quot;Venite Ad Me Omnes&quot;. Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.&#x27;</span>,
 <span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;5733be284776f41900661182&#x27;</span>,
 <span class="hljs-string">&#x27;question&#x27;</span>: <span class="hljs-string">&#x27;To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;University_of_Notre_Dame&#x27;</span>
}`}}),os=new A({}),rs=new g({props:{code:`from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),ps=new g({props:{code:`def preprocess_function(examples):
    questions = [q.strip() for q in examples["question"]]
    inputs = tokenizer(
        questions,
        examples["context"],
        max_length=384,
        truncation="only_second",
        return_offsets_mapping=True,
        padding="max_length",
    )

    offset_mapping = inputs.pop("offset_mapping")
    answers = examples["answers"]
    start_positions = []
    end_positions = []

    for i, offset in enumerate(offset_mapping):
        answer = answers[i]
        start_char = answer["answer_start"][0]
        end_char = answer["answer_start"][0] + len(answer["text"][0])
        sequence_ids = inputs.sequence_ids(i)

        # Find the start and end of the context
        idx = 0
        while sequence_ids[idx] != 1:
            idx += 1
        context_start = idx
        while sequence_ids[idx] == 1:
            idx += 1
        context_end = idx - 1

        # If the answer is not fully inside the context, label it (0, 0)
        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:
            start_positions.append(0)
            end_positions.append(0)
        else:
            # Otherwise it's the start and end token positions
            idx = context_start
            while idx <= context_end and offset[idx][0] <= start_char:
                idx += 1
            start_positions.append(idx - 1)

            idx = context_end
            while idx >= context_start and offset[idx][1] >= end_char:
                idx -= 1
            end_positions.append(idx + 1)

    inputs["start_positions"] = start_positions
    inputs["end_positions"] = end_positions
    return inputs,`,highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
    questions = [q.strip() <span class="hljs-keyword">for</span> q <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;question&quot;</span>]]
    inputs = tokenizer(
        questions,
        examples[<span class="hljs-string">&quot;context&quot;</span>],
        max_length=<span class="hljs-number">384</span>,
        truncation=<span class="hljs-string">&quot;only_second&quot;</span>,
        return_offsets_mapping=<span class="hljs-literal">True</span>,
        padding=<span class="hljs-string">&quot;max_length&quot;</span>,
    )

    offset_mapping = inputs.pop(<span class="hljs-string">&quot;offset_mapping&quot;</span>)
    answers = examples[<span class="hljs-string">&quot;answers&quot;</span>]
    start_positions = []
    end_positions = []

    <span class="hljs-keyword">for</span> i, offset <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(offset_mapping):
        answer = answers[i]
        start_char = answer[<span class="hljs-string">&quot;answer_start&quot;</span>][<span class="hljs-number">0</span>]
        end_char = answer[<span class="hljs-string">&quot;answer_start&quot;</span>][<span class="hljs-number">0</span>] + <span class="hljs-built_in">len</span>(answer[<span class="hljs-string">&quot;text&quot;</span>][<span class="hljs-number">0</span>])
        sequence_ids = inputs.sequence_ids(i)

        <span class="hljs-comment"># Find the start and end of the context</span>
        idx = <span class="hljs-number">0</span>
        <span class="hljs-keyword">while</span> sequence_ids[idx] != <span class="hljs-number">1</span>:
            idx += <span class="hljs-number">1</span>
        context_start = idx
        <span class="hljs-keyword">while</span> sequence_ids[idx] == <span class="hljs-number">1</span>:
            idx += <span class="hljs-number">1</span>
        context_end = idx - <span class="hljs-number">1</span>

        <span class="hljs-comment"># If the answer is not fully inside the context, label it (0, 0)</span>
        <span class="hljs-keyword">if</span> offset[context_start][<span class="hljs-number">0</span>] &gt; end_char <span class="hljs-keyword">or</span> offset[context_end][<span class="hljs-number">1</span>] &lt; start_char:
            start_positions.append(<span class="hljs-number">0</span>)
            end_positions.append(<span class="hljs-number">0</span>)
        <span class="hljs-keyword">else</span>:
            <span class="hljs-comment"># Otherwise it&#x27;s the start and end token positions</span>
            idx = context_start
            <span class="hljs-keyword">while</span> idx &lt;= context_end <span class="hljs-keyword">and</span> offset[idx][<span class="hljs-number">0</span>] &lt;= start_char:
                idx += <span class="hljs-number">1</span>
            start_positions.append(idx - <span class="hljs-number">1</span>)

            idx = context_end
            <span class="hljs-keyword">while</span> idx &gt;= context_start <span class="hljs-keyword">and</span> offset[idx][<span class="hljs-number">1</span>] &gt;= end_char:
                idx -= <span class="hljs-number">1</span>
            end_positions.append(idx + <span class="hljs-number">1</span>)

    inputs[<span class="hljs-string">&quot;start_positions&quot;</span>] = start_positions
    inputs[<span class="hljs-string">&quot;end_positions&quot;</span>] = end_positions
    <span class="hljs-keyword">return</span> inputs`}}),fs=new g({props:{code:'tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad["train"].column_names),',highlighted:'tokenized_squad = squad.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>, remove_columns=squad[<span class="hljs-string">&quot;train&quot;</span>].column_names)'}}),hs=new g({props:{code:`from transformers import default_data_collator
data_collator = default_data_collator,`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> default_data_collator
data_collator = default_data_collator`}}),ds=new A({}),us=new g({props:{code:`from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer
model = AutoModelForQuestionAnswering.from_pretrained("distilbert-base-uncased"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForQuestionAnswering, TrainingArguments, Trainer
model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),cs=new g({props:{code:`training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
),`,highlighted:`training_args = TrainingArguments(
    output_dir=<span class="hljs-string">&#x27;./results&#x27;</span>,
    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
    learning_rate=<span class="hljs-number">2e-5</span>,
    per_device_train_batch_size=<span class="hljs-number">16</span>,
    per_device_eval_batch_size=<span class="hljs-number">16</span>,
    num_train_epochs=<span class="hljs-number">3</span>,
    weight_decay=<span class="hljs-number">0.01</span>,
)`}}),ms=new g({props:{code:`trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_squad["train"],
    eval_dataset=tokenized_squad["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
),`,highlighted:`trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_squad[<span class="hljs-string">&quot;train&quot;</span>],
    eval_dataset=tokenized_squad[<span class="hljs-string">&quot;validation&quot;</span>],
    data_collator=data_collator,
    tokenizer=tokenizer,
)`}}),_s=new g({props:{code:"trainer.train(),",highlighted:"trainer.train()"}}),ws=new A({}),$s=new g({props:{code:`from transformers.data.data_collator import tf_default_collator
data_collator = tf_default_collator,`,highlighted:`<span class="hljs-keyword">from</span> transformers.data.data_collator <span class="hljs-keyword">import</span> tf_default_collator
data_collator = tf_default_collator`}}),gs=new g({props:{code:`tf_train_set = tokenized_squad["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "start_positions", "end_positions"],
    dummy_labels=True,
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_squad["validation"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "start_positions", "end_positions"],
    dummy_labels=True,
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
),`,highlighted:`tf_train_set = tokenized_squad[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;start_positions&quot;</span>, <span class="hljs-string">&quot;end_positions&quot;</span>],
    dummy_labels=<span class="hljs-literal">True</span>,
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_set = tokenized_squad[<span class="hljs-string">&quot;validation&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;start_positions&quot;</span>, <span class="hljs-string">&quot;end_positions&quot;</span>],
    dummy_labels=<span class="hljs-literal">True</span>,
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),bs=new g({props:{code:`from transformers import create_optimizer

batch_size = 16
num_epochs = 2
total_train_steps = (len(tokenized_squad["train"]) // batch_size) * num_epochs
optimizer, schedule = create_optimizer(
    init_lr=2e-5, 
    num_warmup_steps=0, 
    num_train_steps=total_train_steps,
),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">2</span>
total_train_steps = (<span class="hljs-built_in">len</span>(tokenized_squad[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size) * num_epochs
optimizer, schedule = create_optimizer(
    init_lr=<span class="hljs-number">2e-5</span>, 
    num_warmup_steps=<span class="hljs-number">0</span>, 
    num_train_steps=total_train_steps,
)`}}),ks=new g({props:{code:`from transformers import TFAutoModelForQuestionAnswering
model = TFAutoModelForQuestionAnswering("distilbert-base-uncased"),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForQuestionAnswering
model = TFAutoModelForQuestionAnswering(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),vs=new g({props:{code:`import tensorflow as tf
model.compile(optimizer=optimizer),`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),xs=new g({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
),`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){b=l("meta"),E=f(),k=l("h1"),y=l("a"),q=l("span"),u(v.$$.fragment),z=f(),T=l("span"),j=n("How to fine-tune a model for common downstream tasks"),x=f(),P=l("p"),C=n(`This guide will show you how to fine-tune \u{1F917} Transformers models for common downstream tasks. You will use the \u{1F917}
Datasets library to quickly load and preprocess the datasets, getting them ready for training with PyTorch and
TensorFlow.`),eo=f(),de=l("p"),wi=n(`Before you begin, make sure you have the \u{1F917} Datasets library installed. For more detailed installation instructions,
refer to the \u{1F917} Datasets `),st=l("a"),$i=n("installation page"),gi=n(`. All of the
examples in this guide will use \u{1F917} Datasets to load and preprocess a dataset.`),to=f(),u(at.$$.fragment),so=f(),Es=l("p"),bi=n("Learn how to fine-tune a model for:"),ao=f(),N=l("ul"),Ca=l("li"),qs=l("a"),ki=n("seq_imdb"),vi=f(),Da=l("li"),Ts=l("a"),xi=n("tok_ner"),yi=f(),Sa=l("li"),As=l("a"),ji=n("qa_squad"),no=f(),zs=l("a"),oo=f(),J=l("h2"),ue=l("a"),Ia=l("span"),u(nt.$$.fragment),Ei=f(),Na=l("span"),qi=n("Sequence classification with IMDb reviews"),ro=f(),ce=l("p"),Ti=n(`Sequence classification refers to the task of classifying sequences of text according to a given number of classes. In
this example, learn how to fine-tune a model on the `),ot=l("a"),Ai=n("IMDb dataset"),zi=n(` to determine
whether a review is positive or negative.`),lo=f(),u(me.$$.fragment),io=f(),K=l("h3"),_e=l("a"),Ma=l("span"),u(rt.$$.fragment),Pi=f(),La=l("span"),Fi=n("Load IMDb dataset"),po=f(),Ps=l("p"),Ci=n("The \u{1F917} Datasets library makes it simple to load a dataset:"),fo=f(),u(lt.$$.fragment),ho=f(),we=l("p"),Di=n("This loads a "),Ba=l("code"),Si=n("DatasetDict"),Ii=n(" object which you can index into to view an example:"),uo=f(),u(it.$$.fragment),co=f(),X=l("h3"),$e=l("a"),Oa=l("span"),u(pt.$$.fragment),Ni=f(),Ha=l("span"),Mi=n("Preprocess"),mo=f(),M=l("p"),Li=n(`The next step is to tokenize the text into a readable format by the model. It is important to load the same tokenizer a
model was trained with to ensure appropriately tokenized words. Load the DistilBERT tokenizer with the
`),Fs=l("a"),Bi=n("AutoTokenizer"),Oi=n(" because we will eventually train a classifier using a pretrained "),ft=l("a"),Hi=n("DistilBERT"),Wi=n(" model:"),_o=f(),u(ht.$$.fragment),wo=f(),Cs=l("p"),Qi=n(`Now that you have instantiated a tokenizer, create a function that will tokenize the text. You should also truncate
longer sequences in the text to be no longer than the model\u2019s maximum input length:`),$o=f(),u(dt.$$.fragment),go=f(),L=l("p"),Ri=n("Use \u{1F917} Datasets "),Wa=l("code"),Ui=n("map"),Vi=n(` function to apply the preprocessing function to the entire dataset. You can also set
`),Qa=l("code"),Yi=n("batched=True"),Gi=n(` to apply the preprocessing function to multiple elements of the dataset at once for faster
preprocessing:`),bo=f(),u(ut.$$.fragment),ko=f(),F=l("p"),Ji=n("Lastly, pad your text so they are a uniform length. While it is possible to pad your text in the "),Ra=l("code"),Ki=n("tokenizer"),Xi=n(` function
by setting `),Ua=l("code"),Zi=n("padding=True"),ep=n(`, it is more efficient to only pad the text to the length of the longest element in its
batch. This is known as `),Va=l("strong"),tp=n("dynamic padding"),sp=n(". You can do this with the "),Ya=l("code"),ap=n("DataCollatorWithPadding"),np=n(" function:"),vo=f(),u(ct.$$.fragment),xo=f(),Z=l("h3"),ge=l("a"),Ga=l("span"),u(mt.$$.fragment),op=f(),Ja=l("span"),rp=n("Fine-tune with the Trainer API"),yo=f(),be=l("p"),lp=n("Now load your model with the "),Ds=l("a"),ip=n("AutoModelForSequenceClassification"),pp=n(" class along with the number of expected labels:"),jo=f(),u(_t.$$.fragment),Eo=f(),Ss=l("p"),fp=n("At this point, only three steps remain:"),qo=f(),B=l("ol"),wt=l("li"),hp=n("Define your training hyperparameters in "),Is=l("a"),dp=n("TrainingArguments"),up=n("."),cp=f(),$t=l("li"),mp=n("Pass the training arguments to a "),Ns=l("a"),_p=n("Trainer"),wp=n(" along with the model, dataset, tokenizer, and data collator."),$p=f(),gt=l("li"),gp=n("Call "),Ka=l("code"),bp=n("Trainer.train()"),kp=n(" to fine-tune your model."),To=f(),u(bt.$$.fragment),Ao=f(),ee=l("h3"),ke=l("a"),Xa=l("span"),u(kt.$$.fragment),vp=f(),Za=l("span"),xp=n("Fine-tune with TensorFlow"),zo=f(),Ms=l("p"),yp=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),Po=f(),D=l("p"),jp=n("Start by batching the processed examples together with dynamic padding using the "),Ls=l("a"),Ep=n("DataCollatorWithPadding"),qp=n(` function.
Make sure you set `),en=l("code"),Tp=n('return_tensors="tf"'),Ap=n(" to return "),tn=l("code"),zp=n("tf.Tensor"),Pp=n(" outputs instead of PyTorch tensors!"),Fo=f(),u(vt.$$.fragment),Co=f(),S=l("p"),Fp=n("Next, convert your datasets to the "),sn=l("code"),Cp=n("tf.data.Dataset"),Dp=n(" format with "),an=l("code"),Sp=n("to_tf_dataset"),Ip=n(`. Specify inputs and labels in the
`),nn=l("code"),Np=n("columns"),Mp=n(" argument:"),Do=f(),u(xt.$$.fragment),So=f(),Bs=l("p"),Lp=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Io=f(),u(yt.$$.fragment),No=f(),ve=l("p"),Bp=n("Load your model with the "),Os=l("a"),Op=n("TFAutoModelForSequenceClassification"),Hp=n(" class along with the number of expected labels:"),Mo=f(),u(jt.$$.fragment),Lo=f(),Hs=l("p"),Wp=n("Compile the model:"),Bo=f(),u(Et.$$.fragment),Oo=f(),xe=l("p"),Qp=n("Finally, fine-tune the model by calling "),on=l("code"),Rp=n("model.fit"),Up=n(":"),Ho=f(),u(qt.$$.fragment),Wo=f(),Ws=l("a"),Qo=f(),te=l("h2"),ye=l("a"),rn=l("span"),u(Tt.$$.fragment),Vp=f(),ln=l("span"),Yp=n("Token classification with WNUT emerging entities"),Ro=f(),je=l("p"),Gp=n(`Token classification refers to the task of classifying individual tokens in a sentence. One of the most common token
classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence,
such as a person, location, or organization. In this example, learn how to fine-tune a model on the `),At=l("a"),Jp=n("WNUT 17"),Kp=n(" dataset to detect new entities."),Uo=f(),u(Ee.$$.fragment),Vo=f(),se=l("h3"),qe=l("a"),pn=l("span"),u(zt.$$.fragment),Xp=f(),fn=l("span"),Zp=n("Load WNUT 17 dataset"),Yo=f(),Qs=l("p"),ef=n("Load the WNUT 17 dataset from the \u{1F917} Datasets library:"),Go=f(),u(Pt.$$.fragment),Jo=f(),Rs=l("p"),tf=n("A quick look at the dataset shows the labels associated with each word in the sentence:"),Ko=f(),u(Ft.$$.fragment),Xo=f(),Us=l("p"),sf=n("View the specific NER tags by:"),Zo=f(),u(Ct.$$.fragment),er=f(),Vs=l("p"),af=n("A letter prefixes each NER tag which can mean:"),tr=f(),O=l("ul"),Ys=l("li"),hn=l("code"),nf=n("B-"),of=n(" indicates the beginning of an entity."),rf=f(),H=l("li"),dn=l("code"),lf=n("I-"),pf=n(" indicates a token is contained inside the same entity (e.g., the "),un=l("code"),ff=n("State"),hf=n(` token is a part of an entity like
`),cn=l("code"),df=n("Empire State Building"),uf=n(")."),cf=f(),Gs=l("li"),mn=l("code"),mf=n("0"),_f=n(" indicates the token doesn\u2019t correspond to any entity."),sr=f(),ae=l("h3"),Te=l("a"),_n=l("span"),u(Dt.$$.fragment),wf=f(),wn=l("span"),$f=n("Preprocess"),ar=f(),Ae=l("p"),gf=n("Now you need to tokenize the text. Load the DistilBERT tokenizer with an "),Js=l("a"),bf=n("AutoTokenizer"),kf=n(":"),nr=f(),u(St.$$.fragment),or=f(),ze=l("p"),vf=n("Since the input has already been split into words, set "),$n=l("code"),xf=n("is_split_into_words=True"),yf=n(` to tokenize the words into
subwords:`),rr=f(),u(It.$$.fragment),lr=f(),W=l("p"),jf=n("The addition of the special tokens "),gn=l("code"),Ef=n("[CLS]"),qf=n(" and "),bn=l("code"),Tf=n("[SEP]"),Af=n(` and subword tokenization creates a mismatch between the
input and labels. Realign the labels and tokens by:`),ir=f(),Q=l("ol"),Nt=l("li"),zf=n("Mapping all tokens to their corresponding word with the "),kn=l("code"),Pf=n("word_ids"),Ff=n(" method."),Cf=f(),ne=l("li"),Df=n("Assigning the label "),vn=l("code"),Sf=n("-100"),If=n(" to the special tokens "),xn=l("code"),Nf=n("[CLS]"),Mf=n(" and \u201C[SEP]``` so the PyTorch loss function ignores\nthem."),Lf=f(),Mt=l("li"),Bf=n("Only labeling the first token of a given word. Assign "),yn=l("code"),Of=n("-100"),Hf=n(" to the other subtokens from the same word."),pr=f(),Ks=l("p"),Wf=n("Here is how you can create a function that will realign the labels and tokens:"),fr=f(),u(Lt.$$.fragment),hr=f(),Pe=l("p"),Qf=n("Now tokenize and align the labels over the entire dataset with \u{1F917} Datasets "),jn=l("code"),Rf=n("map"),Uf=n(" function:"),dr=f(),u(Bt.$$.fragment),ur=f(),Xs=l("p"),Vf=n("Finally, pad your text and labels, so they are a uniform length:"),cr=f(),u(Ot.$$.fragment),mr=f(),oe=l("h3"),Fe=l("a"),En=l("span"),u(Ht.$$.fragment),Yf=f(),qn=l("span"),Gf=n("Fine-tune with the Trainer API"),_r=f(),Ce=l("p"),Jf=n("Load your model with the "),Zs=l("a"),Kf=n("AutoModelForTokenClassification"),Xf=n(" class along with the number of expected labels:"),wr=f(),u(Wt.$$.fragment),$r=f(),De=l("p"),Zf=n("Gather your training arguments in "),ea=l("a"),eh=n("TrainingArguments"),th=n(":"),gr=f(),u(Qt.$$.fragment),br=f(),Se=l("p"),sh=n("Collect your model, training arguments, dataset, data collator, and tokenizer in "),ta=l("a"),ah=n("Trainer"),nh=n(":"),kr=f(),u(Rt.$$.fragment),vr=f(),sa=l("p"),oh=n("Fine-tune your model:"),xr=f(),u(Ut.$$.fragment),yr=f(),re=l("h3"),Ie=l("a"),Tn=l("span"),u(Vt.$$.fragment),rh=f(),An=l("span"),lh=n("Fine-tune with TensorFlow"),jr=f(),aa=l("p"),ih=n("Batch your examples together and pad your text and labels, so they are a uniform length:"),Er=f(),u(Yt.$$.fragment),qr=f(),R=l("p"),ph=n("Convert your datasets to the "),zn=l("code"),fh=n("tf.data.Dataset"),hh=n(" format with "),Pn=l("code"),dh=n("to_tf_dataset"),uh=n(":"),Tr=f(),u(Gt.$$.fragment),Ar=f(),Ne=l("p"),ch=n("Load the model with the "),na=l("a"),mh=n("TFAutoModelForTokenClassification"),_h=n(" class along with the number of expected labels:"),zr=f(),u(Jt.$$.fragment),Pr=f(),oa=l("p"),wh=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Fr=f(),u(Kt.$$.fragment),Cr=f(),ra=l("p"),$h=n("Compile the model:"),Dr=f(),u(Xt.$$.fragment),Sr=f(),Me=l("p"),gh=n("Call "),Fn=l("code"),bh=n("model.fit"),kh=n(" to fine-tune your model:"),Ir=f(),u(Zt.$$.fragment),Nr=f(),la=l("a"),Mr=f(),le=l("h2"),Le=l("a"),Cn=l("span"),u(es.$$.fragment),vh=f(),Dn=l("span"),xh=n("Question Answering with SQuAD"),Lr=f(),Be=l("p"),yh=n(`There are many types of question answering (QA) tasks. Extractive QA focuses on identifying the answer from the text
given a question. In this example, learn how to fine-tune a model on the `),ts=l("a"),jh=n("SQuAD"),Eh=n(" dataset."),Br=f(),u(Oe.$$.fragment),Or=f(),ie=l("h3"),He=l("a"),Sn=l("span"),u(ss.$$.fragment),qh=f(),In=l("span"),Th=n("Load SQuAD dataset"),Hr=f(),ia=l("p"),Ah=n("Load the SQuAD dataset from the \u{1F917} Datasets library:"),Wr=f(),u(as.$$.fragment),Qr=f(),pa=l("p"),zh=n("Take a look at an example from the dataset:"),Rr=f(),u(ns.$$.fragment),Ur=f(),pe=l("h3"),We=l("a"),Nn=l("span"),u(os.$$.fragment),Ph=f(),Mn=l("span"),Fh=n("Preprocess"),Vr=f(),Qe=l("p"),Ch=n("Load the DistilBERT tokenizer with an "),fa=l("a"),Dh=n("AutoTokenizer"),Sh=n(":"),Yr=f(),u(rs.$$.fragment),Gr=f(),ha=l("p"),Ih=n("There are a few things to be aware of when preprocessing text for question answering:"),Jr=f(),U=l("ol"),I=l("li"),Nh=n("Some examples in a dataset may have a very long "),Ln=l("code"),Mh=n("context"),Lh=n(` that exceeds the maximum input length of the model. You
can deal with this by truncating the `),Bn=l("code"),Bh=n("context"),Oh=n(" and set "),On=l("code"),Hh=n('truncation="only_second"'),Wh=n("."),Qh=f(),ls=l("li"),Rh=n(`Next, you need to map the start and end positions of the answer to the original context. Set
`),Hn=l("code"),Uh=n("return_offset_mapping=True"),Vh=n(" to handle this."),Yh=f(),is=l("li"),Gh=n("With the mapping in hand, you can find the start and end tokens of the answer. Use the "),Wn=l("code"),Jh=n("sequence_ids"),Kh=n(` method to
find which part of the offset corresponds to the question, and which part of the offset corresponds to the context.`),Kr=f(),da=l("p"),Xh=n("Assemble everything in a preprocessing function as shown below:"),Xr=f(),u(ps.$$.fragment),Zr=f(),Re=l("p"),Zh=n("Apply the preprocessing function over the entire dataset with \u{1F917} Datasets "),Qn=l("code"),ed=n("map"),td=n(" function:"),el=f(),u(fs.$$.fragment),tl=f(),ua=l("p"),sd=n("Batch the processed examples together:"),sl=f(),u(hs.$$.fragment),al=f(),fe=l("h3"),Ue=l("a"),Rn=l("span"),u(ds.$$.fragment),ad=f(),Un=l("span"),nd=n("Fine-tune with the Trainer API"),nl=f(),Ve=l("p"),od=n("Load your model with the "),ca=l("a"),rd=n("AutoModelForQuestionAnswering"),ld=n(" class:"),ol=f(),u(us.$$.fragment),rl=f(),Ye=l("p"),id=n("Gather your training arguments in "),ma=l("a"),pd=n("TrainingArguments"),fd=n(":"),ll=f(),u(cs.$$.fragment),il=f(),Ge=l("p"),hd=n("Collect your model, training arguments, dataset, data collator, and tokenizer in "),_a=l("a"),dd=n("Trainer"),ud=n(":"),pl=f(),u(ms.$$.fragment),fl=f(),wa=l("p"),cd=n("Fine-tune your model:"),hl=f(),u(_s.$$.fragment),dl=f(),he=l("h3"),Je=l("a"),Vn=l("span"),u(ws.$$.fragment),md=f(),Yn=l("span"),_d=n("Fine-tune with TensorFlow"),ul=f(),$a=l("p"),wd=n("Batch the processed examples together with a TensorFlow default data collator:"),cl=f(),u($s.$$.fragment),ml=f(),V=l("p"),$d=n("Convert your datasets to the "),Gn=l("code"),gd=n("tf.data.Dataset"),bd=n(" format with the "),Jn=l("code"),kd=n("to_tf_dataset"),vd=n(" function:"),_l=f(),u(gs.$$.fragment),wl=f(),ga=l("p"),xd=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),$l=f(),u(bs.$$.fragment),gl=f(),Ke=l("p"),yd=n("Load your model with the "),ba=l("a"),jd=n("TFAutoModelForQuestionAnswering"),Ed=n(" class:"),bl=f(),u(ks.$$.fragment),kl=f(),ka=l("p"),qd=n("Compile the model:"),vl=f(),u(vs.$$.fragment),xl=f(),Xe=l("p"),Td=n("Call "),Kn=l("code"),Ad=n("model.fit"),zd=n(" to fine-tune the model:"),yl=f(),u(xs.$$.fragment),this.h()},l(e){const a=cm('[data-svelte="svelte-1phssyn"]',document.head);b=i(a,"META",{name:!0,content:!0}),a.forEach(t),E=h(e),k=i(e,"H1",{class:!0});var ys=p(k);y=i(ys,"A",{id:!0,class:!0,href:!0});var Xn=p(y);q=i(Xn,"SPAN",{});var Zn=p(q);c(v.$$.fragment,Zn),Zn.forEach(t),Xn.forEach(t),z=h(ys),T=i(ys,"SPAN",{});var Dd=p(T);j=o(Dd,"How to fine-tune a model for common downstream tasks"),Dd.forEach(t),ys.forEach(t),x=h(e),P=i(e,"P",{});var Sd=p(P);C=o(Sd,`This guide will show you how to fine-tune \u{1F917} Transformers models for common downstream tasks. You will use the \u{1F917}
Datasets library to quickly load and preprocess the datasets, getting them ready for training with PyTorch and
TensorFlow.`),Sd.forEach(t),eo=h(e),de=i(e,"P",{});var El=p(de);wi=o(El,`Before you begin, make sure you have the \u{1F917} Datasets library installed. For more detailed installation instructions,
refer to the \u{1F917} Datasets `),st=i(El,"A",{href:!0,rel:!0});var Id=p(st);$i=o(Id,"installation page"),Id.forEach(t),gi=o(El,`. All of the
examples in this guide will use \u{1F917} Datasets to load and preprocess a dataset.`),El.forEach(t),to=h(e),c(at.$$.fragment,e),so=h(e),Es=i(e,"P",{});var Nd=p(Es);bi=o(Nd,"Learn how to fine-tune a model for:"),Nd.forEach(t),ao=h(e),N=i(e,"UL",{});var va=p(N);Ca=i(va,"LI",{});var Md=p(Ca);qs=i(Md,"A",{href:!0});var Ld=p(qs);ki=o(Ld,"seq_imdb"),Ld.forEach(t),Md.forEach(t),vi=h(va),Da=i(va,"LI",{});var Bd=p(Da);Ts=i(Bd,"A",{href:!0});var Od=p(Ts);xi=o(Od,"tok_ner"),Od.forEach(t),Bd.forEach(t),yi=h(va),Sa=i(va,"LI",{});var Hd=p(Sa);As=i(Hd,"A",{href:!0});var Wd=p(As);ji=o(Wd,"qa_squad"),Wd.forEach(t),Hd.forEach(t),va.forEach(t),no=h(e),zs=i(e,"A",{id:!0}),p(zs).forEach(t),oo=h(e),J=i(e,"H2",{class:!0});var ql=p(J);ue=i(ql,"A",{id:!0,class:!0,href:!0});var Qd=p(ue);Ia=i(Qd,"SPAN",{});var Rd=p(Ia);c(nt.$$.fragment,Rd),Rd.forEach(t),Qd.forEach(t),Ei=h(ql),Na=i(ql,"SPAN",{});var Ud=p(Na);qi=o(Ud,"Sequence classification with IMDb reviews"),Ud.forEach(t),ql.forEach(t),ro=h(e),ce=i(e,"P",{});var Tl=p(ce);Ti=o(Tl,`Sequence classification refers to the task of classifying sequences of text according to a given number of classes. In
this example, learn how to fine-tune a model on the `),ot=i(Tl,"A",{href:!0,rel:!0});var Vd=p(ot);Ai=o(Vd,"IMDb dataset"),Vd.forEach(t),zi=o(Tl,` to determine
whether a review is positive or negative.`),Tl.forEach(t),lo=h(e),c(me.$$.fragment,e),io=h(e),K=i(e,"H3",{class:!0});var Al=p(K);_e=i(Al,"A",{id:!0,class:!0,href:!0});var Yd=p(_e);Ma=i(Yd,"SPAN",{});var Gd=p(Ma);c(rt.$$.fragment,Gd),Gd.forEach(t),Yd.forEach(t),Pi=h(Al),La=i(Al,"SPAN",{});var Jd=p(La);Fi=o(Jd,"Load IMDb dataset"),Jd.forEach(t),Al.forEach(t),po=h(e),Ps=i(e,"P",{});var Kd=p(Ps);Ci=o(Kd,"The \u{1F917} Datasets library makes it simple to load a dataset:"),Kd.forEach(t),fo=h(e),c(lt.$$.fragment,e),ho=h(e),we=i(e,"P",{});var zl=p(we);Di=o(zl,"This loads a "),Ba=i(zl,"CODE",{});var Xd=p(Ba);Si=o(Xd,"DatasetDict"),Xd.forEach(t),Ii=o(zl," object which you can index into to view an example:"),zl.forEach(t),uo=h(e),c(it.$$.fragment,e),co=h(e),X=i(e,"H3",{class:!0});var Pl=p(X);$e=i(Pl,"A",{id:!0,class:!0,href:!0});var Zd=p($e);Oa=i(Zd,"SPAN",{});var eu=p(Oa);c(pt.$$.fragment,eu),eu.forEach(t),Zd.forEach(t),Ni=h(Pl),Ha=i(Pl,"SPAN",{});var tu=p(Ha);Mi=o(tu,"Preprocess"),tu.forEach(t),Pl.forEach(t),mo=h(e),M=i(e,"P",{});var xa=p(M);Li=o(xa,`The next step is to tokenize the text into a readable format by the model. It is important to load the same tokenizer a
model was trained with to ensure appropriately tokenized words. Load the DistilBERT tokenizer with the
`),Fs=i(xa,"A",{href:!0});var su=p(Fs);Bi=o(su,"AutoTokenizer"),su.forEach(t),Oi=o(xa," because we will eventually train a classifier using a pretrained "),ft=i(xa,"A",{href:!0,rel:!0});var au=p(ft);Hi=o(au,"DistilBERT"),au.forEach(t),Wi=o(xa," model:"),xa.forEach(t),_o=h(e),c(ht.$$.fragment,e),wo=h(e),Cs=i(e,"P",{});var nu=p(Cs);Qi=o(nu,`Now that you have instantiated a tokenizer, create a function that will tokenize the text. You should also truncate
longer sequences in the text to be no longer than the model\u2019s maximum input length:`),nu.forEach(t),$o=h(e),c(dt.$$.fragment,e),go=h(e),L=i(e,"P",{});var ya=p(L);Ri=o(ya,"Use \u{1F917} Datasets "),Wa=i(ya,"CODE",{});var ou=p(Wa);Ui=o(ou,"map"),ou.forEach(t),Vi=o(ya,` function to apply the preprocessing function to the entire dataset. You can also set
`),Qa=i(ya,"CODE",{});var ru=p(Qa);Yi=o(ru,"batched=True"),ru.forEach(t),Gi=o(ya,` to apply the preprocessing function to multiple elements of the dataset at once for faster
preprocessing:`),ya.forEach(t),bo=h(e),c(ut.$$.fragment,e),ko=h(e),F=i(e,"P",{});var Y=p(F);Ji=o(Y,"Lastly, pad your text so they are a uniform length. While it is possible to pad your text in the "),Ra=i(Y,"CODE",{});var lu=p(Ra);Ki=o(lu,"tokenizer"),lu.forEach(t),Xi=o(Y,` function
by setting `),Ua=i(Y,"CODE",{});var iu=p(Ua);Zi=o(iu,"padding=True"),iu.forEach(t),ep=o(Y,`, it is more efficient to only pad the text to the length of the longest element in its
batch. This is known as `),Va=i(Y,"STRONG",{});var pu=p(Va);tp=o(pu,"dynamic padding"),pu.forEach(t),sp=o(Y,". You can do this with the "),Ya=i(Y,"CODE",{});var fu=p(Ya);ap=o(fu,"DataCollatorWithPadding"),fu.forEach(t),np=o(Y," function:"),Y.forEach(t),vo=h(e),c(ct.$$.fragment,e),xo=h(e),Z=i(e,"H3",{class:!0});var Fl=p(Z);ge=i(Fl,"A",{id:!0,class:!0,href:!0});var hu=p(ge);Ga=i(hu,"SPAN",{});var du=p(Ga);c(mt.$$.fragment,du),du.forEach(t),hu.forEach(t),op=h(Fl),Ja=i(Fl,"SPAN",{});var uu=p(Ja);rp=o(uu,"Fine-tune with the Trainer API"),uu.forEach(t),Fl.forEach(t),yo=h(e),be=i(e,"P",{});var Cl=p(be);lp=o(Cl,"Now load your model with the "),Ds=i(Cl,"A",{href:!0});var cu=p(Ds);ip=o(cu,"AutoModelForSequenceClassification"),cu.forEach(t),pp=o(Cl," class along with the number of expected labels:"),Cl.forEach(t),jo=h(e),c(_t.$$.fragment,e),Eo=h(e),Ss=i(e,"P",{});var mu=p(Ss);fp=o(mu,"At this point, only three steps remain:"),mu.forEach(t),qo=h(e),B=i(e,"OL",{});var ja=p(B);wt=i(ja,"LI",{});var Dl=p(wt);hp=o(Dl,"Define your training hyperparameters in "),Is=i(Dl,"A",{href:!0});var _u=p(Is);dp=o(_u,"TrainingArguments"),_u.forEach(t),up=o(Dl,"."),Dl.forEach(t),cp=h(ja),$t=i(ja,"LI",{});var Sl=p($t);mp=o(Sl,"Pass the training arguments to a "),Ns=i(Sl,"A",{href:!0});var wu=p(Ns);_p=o(wu,"Trainer"),wu.forEach(t),wp=o(Sl," along with the model, dataset, tokenizer, and data collator."),Sl.forEach(t),$p=h(ja),gt=i(ja,"LI",{});var Il=p(gt);gp=o(Il,"Call "),Ka=i(Il,"CODE",{});var $u=p(Ka);bp=o($u,"Trainer.train()"),$u.forEach(t),kp=o(Il," to fine-tune your model."),Il.forEach(t),ja.forEach(t),To=h(e),c(bt.$$.fragment,e),Ao=h(e),ee=i(e,"H3",{class:!0});var Nl=p(ee);ke=i(Nl,"A",{id:!0,class:!0,href:!0});var gu=p(ke);Xa=i(gu,"SPAN",{});var bu=p(Xa);c(kt.$$.fragment,bu),bu.forEach(t),gu.forEach(t),vp=h(Nl),Za=i(Nl,"SPAN",{});var ku=p(Za);xp=o(ku,"Fine-tune with TensorFlow"),ku.forEach(t),Nl.forEach(t),zo=h(e),Ms=i(e,"P",{});var vu=p(Ms);yp=o(vu,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),vu.forEach(t),Po=h(e),D=i(e,"P",{});var Ze=p(D);jp=o(Ze,"Start by batching the processed examples together with dynamic padding using the "),Ls=i(Ze,"A",{href:!0});var xu=p(Ls);Ep=o(xu,"DataCollatorWithPadding"),xu.forEach(t),qp=o(Ze,` function.
Make sure you set `),en=i(Ze,"CODE",{});var yu=p(en);Tp=o(yu,'return_tensors="tf"'),yu.forEach(t),Ap=o(Ze," to return "),tn=i(Ze,"CODE",{});var ju=p(tn);zp=o(ju,"tf.Tensor"),ju.forEach(t),Pp=o(Ze," outputs instead of PyTorch tensors!"),Ze.forEach(t),Fo=h(e),c(vt.$$.fragment,e),Co=h(e),S=i(e,"P",{});var et=p(S);Fp=o(et,"Next, convert your datasets to the "),sn=i(et,"CODE",{});var Eu=p(sn);Cp=o(Eu,"tf.data.Dataset"),Eu.forEach(t),Dp=o(et," format with "),an=i(et,"CODE",{});var qu=p(an);Sp=o(qu,"to_tf_dataset"),qu.forEach(t),Ip=o(et,`. Specify inputs and labels in the
`),nn=i(et,"CODE",{});var Tu=p(nn);Np=o(Tu,"columns"),Tu.forEach(t),Mp=o(et," argument:"),et.forEach(t),Do=h(e),c(xt.$$.fragment,e),So=h(e),Bs=i(e,"P",{});var Au=p(Bs);Lp=o(Au,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Au.forEach(t),Io=h(e),c(yt.$$.fragment,e),No=h(e),ve=i(e,"P",{});var Ml=p(ve);Bp=o(Ml,"Load your model with the "),Os=i(Ml,"A",{href:!0});var zu=p(Os);Op=o(zu,"TFAutoModelForSequenceClassification"),zu.forEach(t),Hp=o(Ml," class along with the number of expected labels:"),Ml.forEach(t),Mo=h(e),c(jt.$$.fragment,e),Lo=h(e),Hs=i(e,"P",{});var Pu=p(Hs);Wp=o(Pu,"Compile the model:"),Pu.forEach(t),Bo=h(e),c(Et.$$.fragment,e),Oo=h(e),xe=i(e,"P",{});var Ll=p(xe);Qp=o(Ll,"Finally, fine-tune the model by calling "),on=i(Ll,"CODE",{});var Fu=p(on);Rp=o(Fu,"model.fit"),Fu.forEach(t),Up=o(Ll,":"),Ll.forEach(t),Ho=h(e),c(qt.$$.fragment,e),Wo=h(e),Ws=i(e,"A",{id:!0}),p(Ws).forEach(t),Qo=h(e),te=i(e,"H2",{class:!0});var Bl=p(te);ye=i(Bl,"A",{id:!0,class:!0,href:!0});var Cu=p(ye);rn=i(Cu,"SPAN",{});var Du=p(rn);c(Tt.$$.fragment,Du),Du.forEach(t),Cu.forEach(t),Vp=h(Bl),ln=i(Bl,"SPAN",{});var Su=p(ln);Yp=o(Su,"Token classification with WNUT emerging entities"),Su.forEach(t),Bl.forEach(t),Ro=h(e),je=i(e,"P",{});var Ol=p(je);Gp=o(Ol,`Token classification refers to the task of classifying individual tokens in a sentence. One of the most common token
classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence,
such as a person, location, or organization. In this example, learn how to fine-tune a model on the `),At=i(Ol,"A",{href:!0,rel:!0});var Iu=p(At);Jp=o(Iu,"WNUT 17"),Iu.forEach(t),Kp=o(Ol," dataset to detect new entities."),Ol.forEach(t),Uo=h(e),c(Ee.$$.fragment,e),Vo=h(e),se=i(e,"H3",{class:!0});var Hl=p(se);qe=i(Hl,"A",{id:!0,class:!0,href:!0});var Nu=p(qe);pn=i(Nu,"SPAN",{});var Mu=p(pn);c(zt.$$.fragment,Mu),Mu.forEach(t),Nu.forEach(t),Xp=h(Hl),fn=i(Hl,"SPAN",{});var Lu=p(fn);Zp=o(Lu,"Load WNUT 17 dataset"),Lu.forEach(t),Hl.forEach(t),Yo=h(e),Qs=i(e,"P",{});var Bu=p(Qs);ef=o(Bu,"Load the WNUT 17 dataset from the \u{1F917} Datasets library:"),Bu.forEach(t),Go=h(e),c(Pt.$$.fragment,e),Jo=h(e),Rs=i(e,"P",{});var Ou=p(Rs);tf=o(Ou,"A quick look at the dataset shows the labels associated with each word in the sentence:"),Ou.forEach(t),Ko=h(e),c(Ft.$$.fragment,e),Xo=h(e),Us=i(e,"P",{});var Hu=p(Us);sf=o(Hu,"View the specific NER tags by:"),Hu.forEach(t),Zo=h(e),c(Ct.$$.fragment,e),er=h(e),Vs=i(e,"P",{});var Wu=p(Vs);af=o(Wu,"A letter prefixes each NER tag which can mean:"),Wu.forEach(t),tr=h(e),O=i(e,"UL",{});var Ea=p(O);Ys=i(Ea,"LI",{});var Pd=p(Ys);hn=i(Pd,"CODE",{});var Qu=p(hn);nf=o(Qu,"B-"),Qu.forEach(t),of=o(Pd," indicates the beginning of an entity."),Pd.forEach(t),rf=h(Ea),H=i(Ea,"LI",{});var js=p(H);dn=i(js,"CODE",{});var Ru=p(dn);lf=o(Ru,"I-"),Ru.forEach(t),pf=o(js," indicates a token is contained inside the same entity (e.g., the "),un=i(js,"CODE",{});var Uu=p(un);ff=o(Uu,"State"),Uu.forEach(t),hf=o(js,` token is a part of an entity like
`),cn=i(js,"CODE",{});var Vu=p(cn);df=o(Vu,"Empire State Building"),Vu.forEach(t),uf=o(js,")."),js.forEach(t),cf=h(Ea),Gs=i(Ea,"LI",{});var Fd=p(Gs);mn=i(Fd,"CODE",{});var Yu=p(mn);mf=o(Yu,"0"),Yu.forEach(t),_f=o(Fd," indicates the token doesn\u2019t correspond to any entity."),Fd.forEach(t),Ea.forEach(t),sr=h(e),ae=i(e,"H3",{class:!0});var Wl=p(ae);Te=i(Wl,"A",{id:!0,class:!0,href:!0});var Gu=p(Te);_n=i(Gu,"SPAN",{});var Ju=p(_n);c(Dt.$$.fragment,Ju),Ju.forEach(t),Gu.forEach(t),wf=h(Wl),wn=i(Wl,"SPAN",{});var Ku=p(wn);$f=o(Ku,"Preprocess"),Ku.forEach(t),Wl.forEach(t),ar=h(e),Ae=i(e,"P",{});var Ql=p(Ae);gf=o(Ql,"Now you need to tokenize the text. Load the DistilBERT tokenizer with an "),Js=i(Ql,"A",{href:!0});var Xu=p(Js);bf=o(Xu,"AutoTokenizer"),Xu.forEach(t),kf=o(Ql,":"),Ql.forEach(t),nr=h(e),c(St.$$.fragment,e),or=h(e),ze=i(e,"P",{});var Rl=p(ze);vf=o(Rl,"Since the input has already been split into words, set "),$n=i(Rl,"CODE",{});var Zu=p($n);xf=o(Zu,"is_split_into_words=True"),Zu.forEach(t),yf=o(Rl,` to tokenize the words into
subwords:`),Rl.forEach(t),rr=h(e),c(It.$$.fragment,e),lr=h(e),W=i(e,"P",{});var qa=p(W);jf=o(qa,"The addition of the special tokens "),gn=i(qa,"CODE",{});var ec=p(gn);Ef=o(ec,"[CLS]"),ec.forEach(t),qf=o(qa," and "),bn=i(qa,"CODE",{});var tc=p(bn);Tf=o(tc,"[SEP]"),tc.forEach(t),Af=o(qa,` and subword tokenization creates a mismatch between the
input and labels. Realign the labels and tokens by:`),qa.forEach(t),ir=h(e),Q=i(e,"OL",{});var Ta=p(Q);Nt=i(Ta,"LI",{});var Ul=p(Nt);zf=o(Ul,"Mapping all tokens to their corresponding word with the "),kn=i(Ul,"CODE",{});var sc=p(kn);Pf=o(sc,"word_ids"),sc.forEach(t),Ff=o(Ul," method."),Ul.forEach(t),Cf=h(Ta),ne=i(Ta,"LI",{});var Aa=p(ne);Df=o(Aa,"Assigning the label "),vn=i(Aa,"CODE",{});var ac=p(vn);Sf=o(ac,"-100"),ac.forEach(t),If=o(Aa," to the special tokens "),xn=i(Aa,"CODE",{});var nc=p(xn);Nf=o(nc,"[CLS]"),nc.forEach(t),Mf=o(Aa," and \u201C[SEP]``` so the PyTorch loss function ignores\nthem."),Aa.forEach(t),Lf=h(Ta),Mt=i(Ta,"LI",{});var Vl=p(Mt);Bf=o(Vl,"Only labeling the first token of a given word. Assign "),yn=i(Vl,"CODE",{});var oc=p(yn);Of=o(oc,"-100"),oc.forEach(t),Hf=o(Vl," to the other subtokens from the same word."),Vl.forEach(t),Ta.forEach(t),pr=h(e),Ks=i(e,"P",{});var rc=p(Ks);Wf=o(rc,"Here is how you can create a function that will realign the labels and tokens:"),rc.forEach(t),fr=h(e),c(Lt.$$.fragment,e),hr=h(e),Pe=i(e,"P",{});var Yl=p(Pe);Qf=o(Yl,"Now tokenize and align the labels over the entire dataset with \u{1F917} Datasets "),jn=i(Yl,"CODE",{});var lc=p(jn);Rf=o(lc,"map"),lc.forEach(t),Uf=o(Yl," function:"),Yl.forEach(t),dr=h(e),c(Bt.$$.fragment,e),ur=h(e),Xs=i(e,"P",{});var ic=p(Xs);Vf=o(ic,"Finally, pad your text and labels, so they are a uniform length:"),ic.forEach(t),cr=h(e),c(Ot.$$.fragment,e),mr=h(e),oe=i(e,"H3",{class:!0});var Gl=p(oe);Fe=i(Gl,"A",{id:!0,class:!0,href:!0});var pc=p(Fe);En=i(pc,"SPAN",{});var fc=p(En);c(Ht.$$.fragment,fc),fc.forEach(t),pc.forEach(t),Yf=h(Gl),qn=i(Gl,"SPAN",{});var hc=p(qn);Gf=o(hc,"Fine-tune with the Trainer API"),hc.forEach(t),Gl.forEach(t),_r=h(e),Ce=i(e,"P",{});var Jl=p(Ce);Jf=o(Jl,"Load your model with the "),Zs=i(Jl,"A",{href:!0});var dc=p(Zs);Kf=o(dc,"AutoModelForTokenClassification"),dc.forEach(t),Xf=o(Jl," class along with the number of expected labels:"),Jl.forEach(t),wr=h(e),c(Wt.$$.fragment,e),$r=h(e),De=i(e,"P",{});var Kl=p(De);Zf=o(Kl,"Gather your training arguments in "),ea=i(Kl,"A",{href:!0});var uc=p(ea);eh=o(uc,"TrainingArguments"),uc.forEach(t),th=o(Kl,":"),Kl.forEach(t),gr=h(e),c(Qt.$$.fragment,e),br=h(e),Se=i(e,"P",{});var Xl=p(Se);sh=o(Xl,"Collect your model, training arguments, dataset, data collator, and tokenizer in "),ta=i(Xl,"A",{href:!0});var cc=p(ta);ah=o(cc,"Trainer"),cc.forEach(t),nh=o(Xl,":"),Xl.forEach(t),kr=h(e),c(Rt.$$.fragment,e),vr=h(e),sa=i(e,"P",{});var mc=p(sa);oh=o(mc,"Fine-tune your model:"),mc.forEach(t),xr=h(e),c(Ut.$$.fragment,e),yr=h(e),re=i(e,"H3",{class:!0});var Zl=p(re);Ie=i(Zl,"A",{id:!0,class:!0,href:!0});var _c=p(Ie);Tn=i(_c,"SPAN",{});var wc=p(Tn);c(Vt.$$.fragment,wc),wc.forEach(t),_c.forEach(t),rh=h(Zl),An=i(Zl,"SPAN",{});var $c=p(An);lh=o($c,"Fine-tune with TensorFlow"),$c.forEach(t),Zl.forEach(t),jr=h(e),aa=i(e,"P",{});var gc=p(aa);ih=o(gc,"Batch your examples together and pad your text and labels, so they are a uniform length:"),gc.forEach(t),Er=h(e),c(Yt.$$.fragment,e),qr=h(e),R=i(e,"P",{});var za=p(R);ph=o(za,"Convert your datasets to the "),zn=i(za,"CODE",{});var bc=p(zn);fh=o(bc,"tf.data.Dataset"),bc.forEach(t),hh=o(za," format with "),Pn=i(za,"CODE",{});var kc=p(Pn);dh=o(kc,"to_tf_dataset"),kc.forEach(t),uh=o(za,":"),za.forEach(t),Tr=h(e),c(Gt.$$.fragment,e),Ar=h(e),Ne=i(e,"P",{});var ei=p(Ne);ch=o(ei,"Load the model with the "),na=i(ei,"A",{href:!0});var vc=p(na);mh=o(vc,"TFAutoModelForTokenClassification"),vc.forEach(t),_h=o(ei," class along with the number of expected labels:"),ei.forEach(t),zr=h(e),c(Jt.$$.fragment,e),Pr=h(e),oa=i(e,"P",{});var xc=p(oa);wh=o(xc,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),xc.forEach(t),Fr=h(e),c(Kt.$$.fragment,e),Cr=h(e),ra=i(e,"P",{});var yc=p(ra);$h=o(yc,"Compile the model:"),yc.forEach(t),Dr=h(e),c(Xt.$$.fragment,e),Sr=h(e),Me=i(e,"P",{});var ti=p(Me);gh=o(ti,"Call "),Fn=i(ti,"CODE",{});var jc=p(Fn);bh=o(jc,"model.fit"),jc.forEach(t),kh=o(ti," to fine-tune your model:"),ti.forEach(t),Ir=h(e),c(Zt.$$.fragment,e),Nr=h(e),la=i(e,"A",{id:!0}),p(la).forEach(t),Mr=h(e),le=i(e,"H2",{class:!0});var si=p(le);Le=i(si,"A",{id:!0,class:!0,href:!0});var Ec=p(Le);Cn=i(Ec,"SPAN",{});var qc=p(Cn);c(es.$$.fragment,qc),qc.forEach(t),Ec.forEach(t),vh=h(si),Dn=i(si,"SPAN",{});var Tc=p(Dn);xh=o(Tc,"Question Answering with SQuAD"),Tc.forEach(t),si.forEach(t),Lr=h(e),Be=i(e,"P",{});var ai=p(Be);yh=o(ai,`There are many types of question answering (QA) tasks. Extractive QA focuses on identifying the answer from the text
given a question. In this example, learn how to fine-tune a model on the `),ts=i(ai,"A",{href:!0,rel:!0});var Ac=p(ts);jh=o(Ac,"SQuAD"),Ac.forEach(t),Eh=o(ai," dataset."),ai.forEach(t),Br=h(e),c(Oe.$$.fragment,e),Or=h(e),ie=i(e,"H3",{class:!0});var ni=p(ie);He=i(ni,"A",{id:!0,class:!0,href:!0});var zc=p(He);Sn=i(zc,"SPAN",{});var Pc=p(Sn);c(ss.$$.fragment,Pc),Pc.forEach(t),zc.forEach(t),qh=h(ni),In=i(ni,"SPAN",{});var Fc=p(In);Th=o(Fc,"Load SQuAD dataset"),Fc.forEach(t),ni.forEach(t),Hr=h(e),ia=i(e,"P",{});var Cc=p(ia);Ah=o(Cc,"Load the SQuAD dataset from the \u{1F917} Datasets library:"),Cc.forEach(t),Wr=h(e),c(as.$$.fragment,e),Qr=h(e),pa=i(e,"P",{});var Dc=p(pa);zh=o(Dc,"Take a look at an example from the dataset:"),Dc.forEach(t),Rr=h(e),c(ns.$$.fragment,e),Ur=h(e),pe=i(e,"H3",{class:!0});var oi=p(pe);We=i(oi,"A",{id:!0,class:!0,href:!0});var Sc=p(We);Nn=i(Sc,"SPAN",{});var Ic=p(Nn);c(os.$$.fragment,Ic),Ic.forEach(t),Sc.forEach(t),Ph=h(oi),Mn=i(oi,"SPAN",{});var Nc=p(Mn);Fh=o(Nc,"Preprocess"),Nc.forEach(t),oi.forEach(t),Vr=h(e),Qe=i(e,"P",{});var ri=p(Qe);Ch=o(ri,"Load the DistilBERT tokenizer with an "),fa=i(ri,"A",{href:!0});var Mc=p(fa);Dh=o(Mc,"AutoTokenizer"),Mc.forEach(t),Sh=o(ri,":"),ri.forEach(t),Yr=h(e),c(rs.$$.fragment,e),Gr=h(e),ha=i(e,"P",{});var Lc=p(ha);Ih=o(Lc,"There are a few things to be aware of when preprocessing text for question answering:"),Lc.forEach(t),Jr=h(e),U=i(e,"OL",{});var Pa=p(U);I=i(Pa,"LI",{});var tt=p(I);Nh=o(tt,"Some examples in a dataset may have a very long "),Ln=i(tt,"CODE",{});var Bc=p(Ln);Mh=o(Bc,"context"),Bc.forEach(t),Lh=o(tt,` that exceeds the maximum input length of the model. You
can deal with this by truncating the `),Bn=i(tt,"CODE",{});var Oc=p(Bn);Bh=o(Oc,"context"),Oc.forEach(t),Oh=o(tt," and set "),On=i(tt,"CODE",{});var Hc=p(On);Hh=o(Hc,'truncation="only_second"'),Hc.forEach(t),Wh=o(tt,"."),tt.forEach(t),Qh=h(Pa),ls=i(Pa,"LI",{});var li=p(ls);Rh=o(li,`Next, you need to map the start and end positions of the answer to the original context. Set
`),Hn=i(li,"CODE",{});var Wc=p(Hn);Uh=o(Wc,"return_offset_mapping=True"),Wc.forEach(t),Vh=o(li," to handle this."),li.forEach(t),Yh=h(Pa),is=i(Pa,"LI",{});var ii=p(is);Gh=o(ii,"With the mapping in hand, you can find the start and end tokens of the answer. Use the "),Wn=i(ii,"CODE",{});var Qc=p(Wn);Jh=o(Qc,"sequence_ids"),Qc.forEach(t),Kh=o(ii,` method to
find which part of the offset corresponds to the question, and which part of the offset corresponds to the context.`),ii.forEach(t),Pa.forEach(t),Kr=h(e),da=i(e,"P",{});var Rc=p(da);Xh=o(Rc,"Assemble everything in a preprocessing function as shown below:"),Rc.forEach(t),Xr=h(e),c(ps.$$.fragment,e),Zr=h(e),Re=i(e,"P",{});var pi=p(Re);Zh=o(pi,"Apply the preprocessing function over the entire dataset with \u{1F917} Datasets "),Qn=i(pi,"CODE",{});var Uc=p(Qn);ed=o(Uc,"map"),Uc.forEach(t),td=o(pi," function:"),pi.forEach(t),el=h(e),c(fs.$$.fragment,e),tl=h(e),ua=i(e,"P",{});var Vc=p(ua);sd=o(Vc,"Batch the processed examples together:"),Vc.forEach(t),sl=h(e),c(hs.$$.fragment,e),al=h(e),fe=i(e,"H3",{class:!0});var fi=p(fe);Ue=i(fi,"A",{id:!0,class:!0,href:!0});var Yc=p(Ue);Rn=i(Yc,"SPAN",{});var Gc=p(Rn);c(ds.$$.fragment,Gc),Gc.forEach(t),Yc.forEach(t),ad=h(fi),Un=i(fi,"SPAN",{});var Jc=p(Un);nd=o(Jc,"Fine-tune with the Trainer API"),Jc.forEach(t),fi.forEach(t),nl=h(e),Ve=i(e,"P",{});var hi=p(Ve);od=o(hi,"Load your model with the "),ca=i(hi,"A",{href:!0});var Kc=p(ca);rd=o(Kc,"AutoModelForQuestionAnswering"),Kc.forEach(t),ld=o(hi," class:"),hi.forEach(t),ol=h(e),c(us.$$.fragment,e),rl=h(e),Ye=i(e,"P",{});var di=p(Ye);id=o(di,"Gather your training arguments in "),ma=i(di,"A",{href:!0});var Xc=p(ma);pd=o(Xc,"TrainingArguments"),Xc.forEach(t),fd=o(di,":"),di.forEach(t),ll=h(e),c(cs.$$.fragment,e),il=h(e),Ge=i(e,"P",{});var ui=p(Ge);hd=o(ui,"Collect your model, training arguments, dataset, data collator, and tokenizer in "),_a=i(ui,"A",{href:!0});var Zc=p(_a);dd=o(Zc,"Trainer"),Zc.forEach(t),ud=o(ui,":"),ui.forEach(t),pl=h(e),c(ms.$$.fragment,e),fl=h(e),wa=i(e,"P",{});var em=p(wa);cd=o(em,"Fine-tune your model:"),em.forEach(t),hl=h(e),c(_s.$$.fragment,e),dl=h(e),he=i(e,"H3",{class:!0});var ci=p(he);Je=i(ci,"A",{id:!0,class:!0,href:!0});var tm=p(Je);Vn=i(tm,"SPAN",{});var sm=p(Vn);c(ws.$$.fragment,sm),sm.forEach(t),tm.forEach(t),md=h(ci),Yn=i(ci,"SPAN",{});var am=p(Yn);_d=o(am,"Fine-tune with TensorFlow"),am.forEach(t),ci.forEach(t),ul=h(e),$a=i(e,"P",{});var nm=p($a);wd=o(nm,"Batch the processed examples together with a TensorFlow default data collator:"),nm.forEach(t),cl=h(e),c($s.$$.fragment,e),ml=h(e),V=i(e,"P",{});var Fa=p(V);$d=o(Fa,"Convert your datasets to the "),Gn=i(Fa,"CODE",{});var om=p(Gn);gd=o(om,"tf.data.Dataset"),om.forEach(t),bd=o(Fa," format with the "),Jn=i(Fa,"CODE",{});var rm=p(Jn);kd=o(rm,"to_tf_dataset"),rm.forEach(t),vd=o(Fa," function:"),Fa.forEach(t),_l=h(e),c(gs.$$.fragment,e),wl=h(e),ga=i(e,"P",{});var lm=p(ga);xd=o(lm,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),lm.forEach(t),$l=h(e),c(bs.$$.fragment,e),gl=h(e),Ke=i(e,"P",{});var mi=p(Ke);yd=o(mi,"Load your model with the "),ba=i(mi,"A",{href:!0});var im=p(ba);jd=o(im,"TFAutoModelForQuestionAnswering"),im.forEach(t),Ed=o(mi," class:"),mi.forEach(t),bl=h(e),c(ks.$$.fragment,e),kl=h(e),ka=i(e,"P",{});var pm=p(ka);qd=o(pm,"Compile the model:"),pm.forEach(t),vl=h(e),c(vs.$$.fragment,e),xl=h(e),Xe=i(e,"P",{});var _i=p(Xe);Td=o(_i,"Call "),Kn=i(_i,"CODE",{});var fm=p(Kn);Ad=o(fm,"model.fit"),fm.forEach(t),zd=o(_i," to fine-tune the model:"),_i.forEach(t),yl=h(e),c(xs.$$.fragment,e),this.h()},h(){d(b,"name","hf:doc:metadata"),d(b,"content",JSON.stringify(gm)),d(y,"id","how-to-finetune-a-model-for-common-downstream-tasks"),d(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(y,"href","#how-to-finetune-a-model-for-common-downstream-tasks"),d(k,"class","relative group"),d(st,"href","https://huggingface.co/docs/datasets/installation.html"),d(st,"rel","nofollow"),d(qs,"href","#seq_imdb"),d(Ts,"href","#tok_ner"),d(As,"href","#qa_squad"),d(zs,"id","seq_imdb"),d(ue,"id","sequence-classification-with-imdb-reviews"),d(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ue,"href","#sequence-classification-with-imdb-reviews"),d(J,"class","relative group"),d(ot,"href","https://huggingface.co/datasets/imdb"),d(ot,"rel","nofollow"),d(_e,"id","load-imdb-dataset"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#load-imdb-dataset"),d(K,"class","relative group"),d($e,"id","preprocess"),d($e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d($e,"href","#preprocess"),d(X,"class","relative group"),d(Fs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),d(ft,"href","https://huggingface.co/distilbert-base-uncased"),d(ft,"rel","nofollow"),d(ge,"id","finetune-with-the-trainer-api"),d(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ge,"href","#finetune-with-the-trainer-api"),d(Z,"class","relative group"),d(Ds,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),d(Is,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments"),d(Ns,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),d(ke,"id","finetune-with-tensorflow"),d(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ke,"href","#finetune-with-tensorflow"),d(ee,"class","relative group"),d(Ls,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),d(Os,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),d(Ws,"id","tok_ner"),d(ye,"id","token-classification-with-wnut-emerging-entities"),d(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ye,"href","#token-classification-with-wnut-emerging-entities"),d(te,"class","relative group"),d(At,"href","https://huggingface.co/datasets/wnut_17"),d(At,"rel","nofollow"),d(qe,"id","load-wnut-17-dataset"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#load-wnut-17-dataset"),d(se,"class","relative group"),d(Te,"id","preprocess"),d(Te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Te,"href","#preprocess"),d(ae,"class","relative group"),d(Js,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),d(Fe,"id","finetune-with-the-trainer-api"),d(Fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Fe,"href","#finetune-with-the-trainer-api"),d(oe,"class","relative group"),d(Zs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForTokenClassification"),d(ea,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments"),d(ta,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),d(Ie,"id","finetune-with-tensorflow"),d(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ie,"href","#finetune-with-tensorflow"),d(re,"class","relative group"),d(na,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForTokenClassification"),d(la,"id","qa_squad"),d(Le,"id","question-answering-with-squad"),d(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Le,"href","#question-answering-with-squad"),d(le,"class","relative group"),d(ts,"href","https://huggingface.co/datasets/squad"),d(ts,"rel","nofollow"),d(He,"id","load-squad-dataset"),d(He,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(He,"href","#load-squad-dataset"),d(ie,"class","relative group"),d(We,"id","preprocess"),d(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(We,"href","#preprocess"),d(pe,"class","relative group"),d(fa,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),d(Ue,"id","finetune-with-the-trainer-api"),d(Ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ue,"href","#finetune-with-the-trainer-api"),d(fe,"class","relative group"),d(ca,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForQuestionAnswering"),d(ma,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments"),d(_a,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),d(Je,"id","finetune-with-tensorflow"),d(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Je,"href","#finetune-with-tensorflow"),d(he,"class","relative group"),d(ba,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForQuestionAnswering")},m(e,a){s(document.head,b),r(e,E,a),r(e,k,a),s(k,y),s(y,q),m(v,q,null),s(k,z),s(k,T),s(T,j),r(e,x,a),r(e,P,a),s(P,C),r(e,eo,a),r(e,de,a),s(de,wi),s(de,st),s(st,$i),s(de,gi),r(e,to,a),m(at,e,a),r(e,so,a),r(e,Es,a),s(Es,bi),r(e,ao,a),r(e,N,a),s(N,Ca),s(Ca,qs),s(qs,ki),s(N,vi),s(N,Da),s(Da,Ts),s(Ts,xi),s(N,yi),s(N,Sa),s(Sa,As),s(As,ji),r(e,no,a),r(e,zs,a),r(e,oo,a),r(e,J,a),s(J,ue),s(ue,Ia),m(nt,Ia,null),s(J,Ei),s(J,Na),s(Na,qi),r(e,ro,a),r(e,ce,a),s(ce,Ti),s(ce,ot),s(ot,Ai),s(ce,zi),r(e,lo,a),m(me,e,a),r(e,io,a),r(e,K,a),s(K,_e),s(_e,Ma),m(rt,Ma,null),s(K,Pi),s(K,La),s(La,Fi),r(e,po,a),r(e,Ps,a),s(Ps,Ci),r(e,fo,a),m(lt,e,a),r(e,ho,a),r(e,we,a),s(we,Di),s(we,Ba),s(Ba,Si),s(we,Ii),r(e,uo,a),m(it,e,a),r(e,co,a),r(e,X,a),s(X,$e),s($e,Oa),m(pt,Oa,null),s(X,Ni),s(X,Ha),s(Ha,Mi),r(e,mo,a),r(e,M,a),s(M,Li),s(M,Fs),s(Fs,Bi),s(M,Oi),s(M,ft),s(ft,Hi),s(M,Wi),r(e,_o,a),m(ht,e,a),r(e,wo,a),r(e,Cs,a),s(Cs,Qi),r(e,$o,a),m(dt,e,a),r(e,go,a),r(e,L,a),s(L,Ri),s(L,Wa),s(Wa,Ui),s(L,Vi),s(L,Qa),s(Qa,Yi),s(L,Gi),r(e,bo,a),m(ut,e,a),r(e,ko,a),r(e,F,a),s(F,Ji),s(F,Ra),s(Ra,Ki),s(F,Xi),s(F,Ua),s(Ua,Zi),s(F,ep),s(F,Va),s(Va,tp),s(F,sp),s(F,Ya),s(Ya,ap),s(F,np),r(e,vo,a),m(ct,e,a),r(e,xo,a),r(e,Z,a),s(Z,ge),s(ge,Ga),m(mt,Ga,null),s(Z,op),s(Z,Ja),s(Ja,rp),r(e,yo,a),r(e,be,a),s(be,lp),s(be,Ds),s(Ds,ip),s(be,pp),r(e,jo,a),m(_t,e,a),r(e,Eo,a),r(e,Ss,a),s(Ss,fp),r(e,qo,a),r(e,B,a),s(B,wt),s(wt,hp),s(wt,Is),s(Is,dp),s(wt,up),s(B,cp),s(B,$t),s($t,mp),s($t,Ns),s(Ns,_p),s($t,wp),s(B,$p),s(B,gt),s(gt,gp),s(gt,Ka),s(Ka,bp),s(gt,kp),r(e,To,a),m(bt,e,a),r(e,Ao,a),r(e,ee,a),s(ee,ke),s(ke,Xa),m(kt,Xa,null),s(ee,vp),s(ee,Za),s(Za,xp),r(e,zo,a),r(e,Ms,a),s(Ms,yp),r(e,Po,a),r(e,D,a),s(D,jp),s(D,Ls),s(Ls,Ep),s(D,qp),s(D,en),s(en,Tp),s(D,Ap),s(D,tn),s(tn,zp),s(D,Pp),r(e,Fo,a),m(vt,e,a),r(e,Co,a),r(e,S,a),s(S,Fp),s(S,sn),s(sn,Cp),s(S,Dp),s(S,an),s(an,Sp),s(S,Ip),s(S,nn),s(nn,Np),s(S,Mp),r(e,Do,a),m(xt,e,a),r(e,So,a),r(e,Bs,a),s(Bs,Lp),r(e,Io,a),m(yt,e,a),r(e,No,a),r(e,ve,a),s(ve,Bp),s(ve,Os),s(Os,Op),s(ve,Hp),r(e,Mo,a),m(jt,e,a),r(e,Lo,a),r(e,Hs,a),s(Hs,Wp),r(e,Bo,a),m(Et,e,a),r(e,Oo,a),r(e,xe,a),s(xe,Qp),s(xe,on),s(on,Rp),s(xe,Up),r(e,Ho,a),m(qt,e,a),r(e,Wo,a),r(e,Ws,a),r(e,Qo,a),r(e,te,a),s(te,ye),s(ye,rn),m(Tt,rn,null),s(te,Vp),s(te,ln),s(ln,Yp),r(e,Ro,a),r(e,je,a),s(je,Gp),s(je,At),s(At,Jp),s(je,Kp),r(e,Uo,a),m(Ee,e,a),r(e,Vo,a),r(e,se,a),s(se,qe),s(qe,pn),m(zt,pn,null),s(se,Xp),s(se,fn),s(fn,Zp),r(e,Yo,a),r(e,Qs,a),s(Qs,ef),r(e,Go,a),m(Pt,e,a),r(e,Jo,a),r(e,Rs,a),s(Rs,tf),r(e,Ko,a),m(Ft,e,a),r(e,Xo,a),r(e,Us,a),s(Us,sf),r(e,Zo,a),m(Ct,e,a),r(e,er,a),r(e,Vs,a),s(Vs,af),r(e,tr,a),r(e,O,a),s(O,Ys),s(Ys,hn),s(hn,nf),s(Ys,of),s(O,rf),s(O,H),s(H,dn),s(dn,lf),s(H,pf),s(H,un),s(un,ff),s(H,hf),s(H,cn),s(cn,df),s(H,uf),s(O,cf),s(O,Gs),s(Gs,mn),s(mn,mf),s(Gs,_f),r(e,sr,a),r(e,ae,a),s(ae,Te),s(Te,_n),m(Dt,_n,null),s(ae,wf),s(ae,wn),s(wn,$f),r(e,ar,a),r(e,Ae,a),s(Ae,gf),s(Ae,Js),s(Js,bf),s(Ae,kf),r(e,nr,a),m(St,e,a),r(e,or,a),r(e,ze,a),s(ze,vf),s(ze,$n),s($n,xf),s(ze,yf),r(e,rr,a),m(It,e,a),r(e,lr,a),r(e,W,a),s(W,jf),s(W,gn),s(gn,Ef),s(W,qf),s(W,bn),s(bn,Tf),s(W,Af),r(e,ir,a),r(e,Q,a),s(Q,Nt),s(Nt,zf),s(Nt,kn),s(kn,Pf),s(Nt,Ff),s(Q,Cf),s(Q,ne),s(ne,Df),s(ne,vn),s(vn,Sf),s(ne,If),s(ne,xn),s(xn,Nf),s(ne,Mf),s(Q,Lf),s(Q,Mt),s(Mt,Bf),s(Mt,yn),s(yn,Of),s(Mt,Hf),r(e,pr,a),r(e,Ks,a),s(Ks,Wf),r(e,fr,a),m(Lt,e,a),r(e,hr,a),r(e,Pe,a),s(Pe,Qf),s(Pe,jn),s(jn,Rf),s(Pe,Uf),r(e,dr,a),m(Bt,e,a),r(e,ur,a),r(e,Xs,a),s(Xs,Vf),r(e,cr,a),m(Ot,e,a),r(e,mr,a),r(e,oe,a),s(oe,Fe),s(Fe,En),m(Ht,En,null),s(oe,Yf),s(oe,qn),s(qn,Gf),r(e,_r,a),r(e,Ce,a),s(Ce,Jf),s(Ce,Zs),s(Zs,Kf),s(Ce,Xf),r(e,wr,a),m(Wt,e,a),r(e,$r,a),r(e,De,a),s(De,Zf),s(De,ea),s(ea,eh),s(De,th),r(e,gr,a),m(Qt,e,a),r(e,br,a),r(e,Se,a),s(Se,sh),s(Se,ta),s(ta,ah),s(Se,nh),r(e,kr,a),m(Rt,e,a),r(e,vr,a),r(e,sa,a),s(sa,oh),r(e,xr,a),m(Ut,e,a),r(e,yr,a),r(e,re,a),s(re,Ie),s(Ie,Tn),m(Vt,Tn,null),s(re,rh),s(re,An),s(An,lh),r(e,jr,a),r(e,aa,a),s(aa,ih),r(e,Er,a),m(Yt,e,a),r(e,qr,a),r(e,R,a),s(R,ph),s(R,zn),s(zn,fh),s(R,hh),s(R,Pn),s(Pn,dh),s(R,uh),r(e,Tr,a),m(Gt,e,a),r(e,Ar,a),r(e,Ne,a),s(Ne,ch),s(Ne,na),s(na,mh),s(Ne,_h),r(e,zr,a),m(Jt,e,a),r(e,Pr,a),r(e,oa,a),s(oa,wh),r(e,Fr,a),m(Kt,e,a),r(e,Cr,a),r(e,ra,a),s(ra,$h),r(e,Dr,a),m(Xt,e,a),r(e,Sr,a),r(e,Me,a),s(Me,gh),s(Me,Fn),s(Fn,bh),s(Me,kh),r(e,Ir,a),m(Zt,e,a),r(e,Nr,a),r(e,la,a),r(e,Mr,a),r(e,le,a),s(le,Le),s(Le,Cn),m(es,Cn,null),s(le,vh),s(le,Dn),s(Dn,xh),r(e,Lr,a),r(e,Be,a),s(Be,yh),s(Be,ts),s(ts,jh),s(Be,Eh),r(e,Br,a),m(Oe,e,a),r(e,Or,a),r(e,ie,a),s(ie,He),s(He,Sn),m(ss,Sn,null),s(ie,qh),s(ie,In),s(In,Th),r(e,Hr,a),r(e,ia,a),s(ia,Ah),r(e,Wr,a),m(as,e,a),r(e,Qr,a),r(e,pa,a),s(pa,zh),r(e,Rr,a),m(ns,e,a),r(e,Ur,a),r(e,pe,a),s(pe,We),s(We,Nn),m(os,Nn,null),s(pe,Ph),s(pe,Mn),s(Mn,Fh),r(e,Vr,a),r(e,Qe,a),s(Qe,Ch),s(Qe,fa),s(fa,Dh),s(Qe,Sh),r(e,Yr,a),m(rs,e,a),r(e,Gr,a),r(e,ha,a),s(ha,Ih),r(e,Jr,a),r(e,U,a),s(U,I),s(I,Nh),s(I,Ln),s(Ln,Mh),s(I,Lh),s(I,Bn),s(Bn,Bh),s(I,Oh),s(I,On),s(On,Hh),s(I,Wh),s(U,Qh),s(U,ls),s(ls,Rh),s(ls,Hn),s(Hn,Uh),s(ls,Vh),s(U,Yh),s(U,is),s(is,Gh),s(is,Wn),s(Wn,Jh),s(is,Kh),r(e,Kr,a),r(e,da,a),s(da,Xh),r(e,Xr,a),m(ps,e,a),r(e,Zr,a),r(e,Re,a),s(Re,Zh),s(Re,Qn),s(Qn,ed),s(Re,td),r(e,el,a),m(fs,e,a),r(e,tl,a),r(e,ua,a),s(ua,sd),r(e,sl,a),m(hs,e,a),r(e,al,a),r(e,fe,a),s(fe,Ue),s(Ue,Rn),m(ds,Rn,null),s(fe,ad),s(fe,Un),s(Un,nd),r(e,nl,a),r(e,Ve,a),s(Ve,od),s(Ve,ca),s(ca,rd),s(Ve,ld),r(e,ol,a),m(us,e,a),r(e,rl,a),r(e,Ye,a),s(Ye,id),s(Ye,ma),s(ma,pd),s(Ye,fd),r(e,ll,a),m(cs,e,a),r(e,il,a),r(e,Ge,a),s(Ge,hd),s(Ge,_a),s(_a,dd),s(Ge,ud),r(e,pl,a),m(ms,e,a),r(e,fl,a),r(e,wa,a),s(wa,cd),r(e,hl,a),m(_s,e,a),r(e,dl,a),r(e,he,a),s(he,Je),s(Je,Vn),m(ws,Vn,null),s(he,md),s(he,Yn),s(Yn,_d),r(e,ul,a),r(e,$a,a),s($a,wd),r(e,cl,a),m($s,e,a),r(e,ml,a),r(e,V,a),s(V,$d),s(V,Gn),s(Gn,gd),s(V,bd),s(V,Jn),s(Jn,kd),s(V,vd),r(e,_l,a),m(gs,e,a),r(e,wl,a),r(e,ga,a),s(ga,xd),r(e,$l,a),m(bs,e,a),r(e,gl,a),r(e,Ke,a),s(Ke,yd),s(Ke,ba),s(ba,jd),s(Ke,Ed),r(e,bl,a),m(ks,e,a),r(e,kl,a),r(e,ka,a),s(ka,qd),r(e,vl,a),m(vs,e,a),r(e,xl,a),r(e,Xe,a),s(Xe,Td),s(Xe,Kn),s(Kn,Ad),s(Xe,zd),r(e,yl,a),m(xs,e,a),jl=!0},p(e,[a]){const ys={};a&2&&(ys.$$scope={dirty:a,ctx:e}),me.$set(ys);const Xn={};a&2&&(Xn.$$scope={dirty:a,ctx:e}),Ee.$set(Xn);const Zn={};a&2&&(Zn.$$scope={dirty:a,ctx:e}),Oe.$set(Zn)},i(e){jl||(_(v.$$.fragment,e),_(at.$$.fragment,e),_(nt.$$.fragment,e),_(me.$$.fragment,e),_(rt.$$.fragment,e),_(lt.$$.fragment,e),_(it.$$.fragment,e),_(pt.$$.fragment,e),_(ht.$$.fragment,e),_(dt.$$.fragment,e),_(ut.$$.fragment,e),_(ct.$$.fragment,e),_(mt.$$.fragment,e),_(_t.$$.fragment,e),_(bt.$$.fragment,e),_(kt.$$.fragment,e),_(vt.$$.fragment,e),_(xt.$$.fragment,e),_(yt.$$.fragment,e),_(jt.$$.fragment,e),_(Et.$$.fragment,e),_(qt.$$.fragment,e),_(Tt.$$.fragment,e),_(Ee.$$.fragment,e),_(zt.$$.fragment,e),_(Pt.$$.fragment,e),_(Ft.$$.fragment,e),_(Ct.$$.fragment,e),_(Dt.$$.fragment,e),_(St.$$.fragment,e),_(It.$$.fragment,e),_(Lt.$$.fragment,e),_(Bt.$$.fragment,e),_(Ot.$$.fragment,e),_(Ht.$$.fragment,e),_(Wt.$$.fragment,e),_(Qt.$$.fragment,e),_(Rt.$$.fragment,e),_(Ut.$$.fragment,e),_(Vt.$$.fragment,e),_(Yt.$$.fragment,e),_(Gt.$$.fragment,e),_(Jt.$$.fragment,e),_(Kt.$$.fragment,e),_(Xt.$$.fragment,e),_(Zt.$$.fragment,e),_(es.$$.fragment,e),_(Oe.$$.fragment,e),_(ss.$$.fragment,e),_(as.$$.fragment,e),_(ns.$$.fragment,e),_(os.$$.fragment,e),_(rs.$$.fragment,e),_(ps.$$.fragment,e),_(fs.$$.fragment,e),_(hs.$$.fragment,e),_(ds.$$.fragment,e),_(us.$$.fragment,e),_(cs.$$.fragment,e),_(ms.$$.fragment,e),_(_s.$$.fragment,e),_(ws.$$.fragment,e),_($s.$$.fragment,e),_(gs.$$.fragment,e),_(bs.$$.fragment,e),_(ks.$$.fragment,e),_(vs.$$.fragment,e),_(xs.$$.fragment,e),jl=!0)},o(e){w(v.$$.fragment,e),w(at.$$.fragment,e),w(nt.$$.fragment,e),w(me.$$.fragment,e),w(rt.$$.fragment,e),w(lt.$$.fragment,e),w(it.$$.fragment,e),w(pt.$$.fragment,e),w(ht.$$.fragment,e),w(dt.$$.fragment,e),w(ut.$$.fragment,e),w(ct.$$.fragment,e),w(mt.$$.fragment,e),w(_t.$$.fragment,e),w(bt.$$.fragment,e),w(kt.$$.fragment,e),w(vt.$$.fragment,e),w(xt.$$.fragment,e),w(yt.$$.fragment,e),w(jt.$$.fragment,e),w(Et.$$.fragment,e),w(qt.$$.fragment,e),w(Tt.$$.fragment,e),w(Ee.$$.fragment,e),w(zt.$$.fragment,e),w(Pt.$$.fragment,e),w(Ft.$$.fragment,e),w(Ct.$$.fragment,e),w(Dt.$$.fragment,e),w(St.$$.fragment,e),w(It.$$.fragment,e),w(Lt.$$.fragment,e),w(Bt.$$.fragment,e),w(Ot.$$.fragment,e),w(Ht.$$.fragment,e),w(Wt.$$.fragment,e),w(Qt.$$.fragment,e),w(Rt.$$.fragment,e),w(Ut.$$.fragment,e),w(Vt.$$.fragment,e),w(Yt.$$.fragment,e),w(Gt.$$.fragment,e),w(Jt.$$.fragment,e),w(Kt.$$.fragment,e),w(Xt.$$.fragment,e),w(Zt.$$.fragment,e),w(es.$$.fragment,e),w(Oe.$$.fragment,e),w(ss.$$.fragment,e),w(as.$$.fragment,e),w(ns.$$.fragment,e),w(os.$$.fragment,e),w(rs.$$.fragment,e),w(ps.$$.fragment,e),w(fs.$$.fragment,e),w(hs.$$.fragment,e),w(ds.$$.fragment,e),w(us.$$.fragment,e),w(cs.$$.fragment,e),w(ms.$$.fragment,e),w(_s.$$.fragment,e),w(ws.$$.fragment,e),w($s.$$.fragment,e),w(gs.$$.fragment,e),w(bs.$$.fragment,e),w(ks.$$.fragment,e),w(vs.$$.fragment,e),w(xs.$$.fragment,e),jl=!1},d(e){t(b),e&&t(E),e&&t(k),$(v),e&&t(x),e&&t(P),e&&t(eo),e&&t(de),e&&t(to),$(at,e),e&&t(so),e&&t(Es),e&&t(ao),e&&t(N),e&&t(no),e&&t(zs),e&&t(oo),e&&t(J),$(nt),e&&t(ro),e&&t(ce),e&&t(lo),$(me,e),e&&t(io),e&&t(K),$(rt),e&&t(po),e&&t(Ps),e&&t(fo),$(lt,e),e&&t(ho),e&&t(we),e&&t(uo),$(it,e),e&&t(co),e&&t(X),$(pt),e&&t(mo),e&&t(M),e&&t(_o),$(ht,e),e&&t(wo),e&&t(Cs),e&&t($o),$(dt,e),e&&t(go),e&&t(L),e&&t(bo),$(ut,e),e&&t(ko),e&&t(F),e&&t(vo),$(ct,e),e&&t(xo),e&&t(Z),$(mt),e&&t(yo),e&&t(be),e&&t(jo),$(_t,e),e&&t(Eo),e&&t(Ss),e&&t(qo),e&&t(B),e&&t(To),$(bt,e),e&&t(Ao),e&&t(ee),$(kt),e&&t(zo),e&&t(Ms),e&&t(Po),e&&t(D),e&&t(Fo),$(vt,e),e&&t(Co),e&&t(S),e&&t(Do),$(xt,e),e&&t(So),e&&t(Bs),e&&t(Io),$(yt,e),e&&t(No),e&&t(ve),e&&t(Mo),$(jt,e),e&&t(Lo),e&&t(Hs),e&&t(Bo),$(Et,e),e&&t(Oo),e&&t(xe),e&&t(Ho),$(qt,e),e&&t(Wo),e&&t(Ws),e&&t(Qo),e&&t(te),$(Tt),e&&t(Ro),e&&t(je),e&&t(Uo),$(Ee,e),e&&t(Vo),e&&t(se),$(zt),e&&t(Yo),e&&t(Qs),e&&t(Go),$(Pt,e),e&&t(Jo),e&&t(Rs),e&&t(Ko),$(Ft,e),e&&t(Xo),e&&t(Us),e&&t(Zo),$(Ct,e),e&&t(er),e&&t(Vs),e&&t(tr),e&&t(O),e&&t(sr),e&&t(ae),$(Dt),e&&t(ar),e&&t(Ae),e&&t(nr),$(St,e),e&&t(or),e&&t(ze),e&&t(rr),$(It,e),e&&t(lr),e&&t(W),e&&t(ir),e&&t(Q),e&&t(pr),e&&t(Ks),e&&t(fr),$(Lt,e),e&&t(hr),e&&t(Pe),e&&t(dr),$(Bt,e),e&&t(ur),e&&t(Xs),e&&t(cr),$(Ot,e),e&&t(mr),e&&t(oe),$(Ht),e&&t(_r),e&&t(Ce),e&&t(wr),$(Wt,e),e&&t($r),e&&t(De),e&&t(gr),$(Qt,e),e&&t(br),e&&t(Se),e&&t(kr),$(Rt,e),e&&t(vr),e&&t(sa),e&&t(xr),$(Ut,e),e&&t(yr),e&&t(re),$(Vt),e&&t(jr),e&&t(aa),e&&t(Er),$(Yt,e),e&&t(qr),e&&t(R),e&&t(Tr),$(Gt,e),e&&t(Ar),e&&t(Ne),e&&t(zr),$(Jt,e),e&&t(Pr),e&&t(oa),e&&t(Fr),$(Kt,e),e&&t(Cr),e&&t(ra),e&&t(Dr),$(Xt,e),e&&t(Sr),e&&t(Me),e&&t(Ir),$(Zt,e),e&&t(Nr),e&&t(la),e&&t(Mr),e&&t(le),$(es),e&&t(Lr),e&&t(Be),e&&t(Br),$(Oe,e),e&&t(Or),e&&t(ie),$(ss),e&&t(Hr),e&&t(ia),e&&t(Wr),$(as,e),e&&t(Qr),e&&t(pa),e&&t(Rr),$(ns,e),e&&t(Ur),e&&t(pe),$(os),e&&t(Vr),e&&t(Qe),e&&t(Yr),$(rs,e),e&&t(Gr),e&&t(ha),e&&t(Jr),e&&t(U),e&&t(Kr),e&&t(da),e&&t(Xr),$(ps,e),e&&t(Zr),e&&t(Re),e&&t(el),$(fs,e),e&&t(tl),e&&t(ua),e&&t(sl),$(hs,e),e&&t(al),e&&t(fe),$(ds),e&&t(nl),e&&t(Ve),e&&t(ol),$(us,e),e&&t(rl),e&&t(Ye),e&&t(ll),$(cs,e),e&&t(il),e&&t(Ge),e&&t(pl),$(ms,e),e&&t(fl),e&&t(wa),e&&t(hl),$(_s,e),e&&t(dl),e&&t(he),$(ws),e&&t(ul),e&&t($a),e&&t(cl),$($s,e),e&&t(ml),e&&t(V),e&&t(_l),$(gs,e),e&&t(wl),e&&t(ga),e&&t($l),$(bs,e),e&&t(gl),e&&t(Ke),e&&t(bl),$(ks,e),e&&t(kl),e&&t(ka),e&&t(vl),$(vs,e),e&&t(xl),e&&t(Xe),e&&t(yl),$(xs,e)}}}const gm={local:"how-to-finetune-a-model-for-common-downstream-tasks",sections:[{local:"sequence-classification-with-imdb-reviews",sections:[{local:"load-imdb-dataset",title:"Load IMDb dataset"},{local:"preprocess",title:"Preprocess"},{local:"finetune-with-the-trainer-api",title:"Fine-tune with the Trainer API"},{local:"finetune-with-tensorflow",title:"Fine-tune with TensorFlow"}],title:"Sequence classification with IMDb reviews"},{local:"token-classification-with-wnut-emerging-entities",sections:[{local:"load-wnut-17-dataset",title:"Load WNUT 17 dataset"},{local:"preprocess",title:"Preprocess"},{local:"finetune-with-the-trainer-api",title:"Fine-tune with the Trainer API"},{local:"finetune-with-tensorflow",title:"Fine-tune with TensorFlow"}],title:"Token classification with WNUT emerging entities"},{local:"question-answering-with-squad",sections:[{local:"load-squad-dataset",title:"Load SQuAD dataset"},{local:"preprocess",title:"Preprocess"},{local:"finetune-with-the-trainer-api",title:"Fine-tune with the Trainer API"},{local:"finetune-with-tensorflow",title:"Fine-tune with TensorFlow"}],title:"Question Answering with SQuAD"}],title:"How to fine-tune a model for common downstream tasks"};function bm(G,b,E){let{fw:k}=b;return G.$$set=y=>{"fw"in y&&E(0,k=y.fw)},[k]}class Em extends hm{constructor(b){super();dm(this,b,bm,$m,um,{fw:0})}}export{Em as default,gm as metadata};
