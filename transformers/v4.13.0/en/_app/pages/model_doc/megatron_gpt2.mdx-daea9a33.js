import{S as Pt,i as bt,s as $t,e as r,k as c,w as D,t as n,M as At,c as i,d as t,m as f,a as m,x as H,h as l,b as p,F as a,g as s,y as V,L as Et,q as j,o as J,B as K}from"../../chunks/vendor-68651a14.js";import{I as Tt}from"../../chunks/IconCopyLink-8ff17449.js";import{C as be}from"../../chunks/CodeBlock-b98730f5.js";import"../../chunks/CopyButton-e88c769b.js";function kt($e){let u,R,h,d,Q,b,Ae,X,Ee,oe,w,y,Y,$,ke,Z,Oe,re,T,Me,A,Ge,Se,ne,I,Ce,ie,x,ee,Le,le,z,Ne,se,P,Re,E,Ie,xe,me,g,ze,k,Be,Ue,O,Fe,We,pe,B,qe,ce,M,fe,G,he,U,De,de,v,He,te,Ve,je,ae,Je,Ke,ue,S,ge,C,ve,_,Qe,L,Xe,Ye,N,Ze,et,_e;return b=new Tt({}),$=new Tt({}),M=new be({props:{code:",",highlighted:""}}),G=new be({props:{code:`wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O
megatron_gpt2_345m_v0_0.zip,`,highlighted:`wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/megatron_lm_345m/versions/v0.0/zip -O
megatron_gpt2_345m_v0_0.zip`}}),S=new be({props:{code:",",highlighted:""}}),C=new be({props:{code:"python3 $PATH_TO_TRANSFORMERS/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py megatron_gpt2_345m_v0_0.zip,",highlighted:'python3 <span class="hljs-variable">$PATH_TO_TRANSFORMERS</span>/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py megatron_gpt2_345m_v0_0.zip'}}),{c(){u=r("meta"),R=c(),h=r("h1"),d=r("a"),Q=r("span"),D(b.$$.fragment),Ae=c(),X=r("span"),Ee=n("MegatronGPT2"),oe=c(),w=r("h2"),y=r("a"),Y=r("span"),D($.$$.fragment),ke=c(),Z=r("span"),Oe=n("Overview"),re=c(),T=r("p"),Me=n("The MegatronGPT2 model was proposed in "),A=r("a"),Ge=n(`Megatron-LM: Training Multi-Billion Parameter Language Models Using Model
Parallelism`),Se=n(` by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,
Jared Casper and Bryan Catanzaro.`),ne=c(),I=r("p"),Ce=n("The abstract from the paper is the following:"),ie=c(),x=r("p"),ee=r("em"),Le=n(`Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance
the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9
billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in
BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we
achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA
accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy
of 89.4%).`),le=c(),z=r("p"),Ne=n("Tips:"),se=c(),P=r("p"),Re=n("We have provided pretrained "),E=r("a"),Ie=n("GPT2-345M"),xe=n(` checkpoints
for use to evaluate or finetuning downstream tasks.`),me=c(),g=r("p"),ze=n("To access these checkpoints, first "),k=r("a"),Be=n("sign up"),Ue=n(` for and setup the NVIDIA GPU Cloud (NGC)
Registry CLI. Further documentation for downloading models can be found in the `),O=r("a"),Fe=n("NGC documentation"),We=n("."),pe=c(),B=r("p"),qe=n("Alternatively, you can directly download the checkpoints using:"),ce=c(),D(M.$$.fragment),fe=c(),D(G.$$.fragment),he=c(),U=r("p"),De=n(`Once you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a format that will easily
be loaded by Hugging Face Transformers GPT2 implementation.`),de=c(),v=r("p"),He=n("The following command allows you to do the conversion. We assume that the folder "),te=r("code"),Ve=n("models/megatron_gpt2"),je=n(` contains
`),ae=r("code"),Je=n("megatron_gpt2_345m_v0_0.zip"),Ke=n(" and that the command is run from that folder:"),ue=c(),D(S.$$.fragment),ge=c(),D(C.$$.fragment),ve=c(),_=r("p"),Qe=n("This model was contributed by "),L=r("a"),Xe=n("jdemouth"),Ye=n(". The original code can be found "),N=r("a"),Ze=n("here"),et=n(`. That repository contains a multi-GPU and multi-node implementation of the
Megatron Language models. In particular, it contains a hybrid model parallel approach using \u201Ctensor parallel\u201D and
\u201Cpipeline parallel\u201D techniques.`),this.h()},l(e){const o=At('[data-svelte="svelte-1phssyn"]',document.head);u=i(o,"META",{name:!0,content:!0}),o.forEach(t),R=f(e),h=i(e,"H1",{class:!0});var we=m(h);d=i(we,"A",{id:!0,class:!0,href:!0});var tt=m(d);Q=i(tt,"SPAN",{});var at=m(Q);H(b.$$.fragment,at),at.forEach(t),tt.forEach(t),Ae=f(we),X=i(we,"SPAN",{});var ot=m(X);Ee=l(ot,"MegatronGPT2"),ot.forEach(t),we.forEach(t),oe=f(e),w=i(e,"H2",{class:!0});var ye=m(w);y=i(ye,"A",{id:!0,class:!0,href:!0});var rt=m(y);Y=i(rt,"SPAN",{});var nt=m(Y);H($.$$.fragment,nt),nt.forEach(t),rt.forEach(t),ke=f(ye),Z=i(ye,"SPAN",{});var it=m(Z);Oe=l(it,"Overview"),it.forEach(t),ye.forEach(t),re=f(e),T=i(e,"P",{});var Te=m(T);Me=l(Te,"The MegatronGPT2 model was proposed in "),A=i(Te,"A",{href:!0,rel:!0});var lt=m(A);Ge=l(lt,`Megatron-LM: Training Multi-Billion Parameter Language Models Using Model
Parallelism`),lt.forEach(t),Se=l(Te,` by Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,
Jared Casper and Bryan Catanzaro.`),Te.forEach(t),ne=f(e),I=i(e,"P",{});var st=m(I);Ce=l(st,"The abstract from the paper is the following:"),st.forEach(t),ie=f(e),x=i(e,"P",{});var mt=m(x);ee=i(mt,"EM",{});var pt=m(ee);Le=l(pt,`Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance
the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9
billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in
BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we
achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA
accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy
of 89.4%).`),pt.forEach(t),mt.forEach(t),le=f(e),z=i(e,"P",{});var ct=m(z);Ne=l(ct,"Tips:"),ct.forEach(t),se=f(e),P=i(e,"P",{});var Pe=m(P);Re=l(Pe,"We have provided pretrained "),E=i(Pe,"A",{href:!0,rel:!0});var ft=m(E);Ie=l(ft,"GPT2-345M"),ft.forEach(t),xe=l(Pe,` checkpoints
for use to evaluate or finetuning downstream tasks.`),Pe.forEach(t),me=f(e),g=i(e,"P",{});var F=m(g);ze=l(F,"To access these checkpoints, first "),k=i(F,"A",{href:!0,rel:!0});var ht=m(k);Be=l(ht,"sign up"),ht.forEach(t),Ue=l(F,` for and setup the NVIDIA GPU Cloud (NGC)
Registry CLI. Further documentation for downloading models can be found in the `),O=i(F,"A",{href:!0,rel:!0});var dt=m(O);Fe=l(dt,"NGC documentation"),dt.forEach(t),We=l(F,"."),F.forEach(t),pe=f(e),B=i(e,"P",{});var ut=m(B);qe=l(ut,"Alternatively, you can directly download the checkpoints using:"),ut.forEach(t),ce=f(e),H(M.$$.fragment,e),fe=f(e),H(G.$$.fragment,e),he=f(e),U=i(e,"P",{});var gt=m(U);De=l(gt,`Once you have obtained the checkpoint from NVIDIA GPU Cloud (NGC), you have to convert it to a format that will easily
be loaded by Hugging Face Transformers GPT2 implementation.`),gt.forEach(t),de=f(e),v=i(e,"P",{});var W=m(v);He=l(W,"The following command allows you to do the conversion. We assume that the folder "),te=i(W,"CODE",{});var vt=m(te);Ve=l(vt,"models/megatron_gpt2"),vt.forEach(t),je=l(W,` contains
`),ae=i(W,"CODE",{});var _t=m(ae);Je=l(_t,"megatron_gpt2_345m_v0_0.zip"),_t.forEach(t),Ke=l(W," and that the command is run from that folder:"),W.forEach(t),ue=f(e),H(S.$$.fragment,e),ge=f(e),H(C.$$.fragment,e),ve=f(e),_=i(e,"P",{});var q=m(_);Qe=l(q,"This model was contributed by "),L=i(q,"A",{href:!0,rel:!0});var wt=m(L);Xe=l(wt,"jdemouth"),wt.forEach(t),Ye=l(q,". The original code can be found "),N=i(q,"A",{href:!0,rel:!0});var yt=m(N);Ze=l(yt,"here"),yt.forEach(t),et=l(q,`. That repository contains a multi-GPU and multi-node implementation of the
Megatron Language models. In particular, it contains a hybrid model parallel approach using \u201Ctensor parallel\u201D and
\u201Cpipeline parallel\u201D techniques.`),q.forEach(t),this.h()},h(){p(u,"name","hf:doc:metadata"),p(u,"content",JSON.stringify(Ot)),p(d,"id","megatrongpt2"),p(d,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(d,"href","#megatrongpt2"),p(h,"class","relative group"),p(y,"id","overview"),p(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(y,"href","#overview"),p(w,"class","relative group"),p(A,"href","https://arxiv.org/abs/1909.08053"),p(A,"rel","nofollow"),p(E,"href","https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m"),p(E,"rel","nofollow"),p(k,"href","https://ngc.nvidia.com/signup"),p(k,"rel","nofollow"),p(O,"href","https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1"),p(O,"rel","nofollow"),p(L,"href","https://huggingface.co/jdemouth"),p(L,"rel","nofollow"),p(N,"href","https://github.com/NVIDIA/Megatron-LM"),p(N,"rel","nofollow")},m(e,o){a(document.head,u),s(e,R,o),s(e,h,o),a(h,d),a(d,Q),V(b,Q,null),a(h,Ae),a(h,X),a(X,Ee),s(e,oe,o),s(e,w,o),a(w,y),a(y,Y),V($,Y,null),a(w,ke),a(w,Z),a(Z,Oe),s(e,re,o),s(e,T,o),a(T,Me),a(T,A),a(A,Ge),a(T,Se),s(e,ne,o),s(e,I,o),a(I,Ce),s(e,ie,o),s(e,x,o),a(x,ee),a(ee,Le),s(e,le,o),s(e,z,o),a(z,Ne),s(e,se,o),s(e,P,o),a(P,Re),a(P,E),a(E,Ie),a(P,xe),s(e,me,o),s(e,g,o),a(g,ze),a(g,k),a(k,Be),a(g,Ue),a(g,O),a(O,Fe),a(g,We),s(e,pe,o),s(e,B,o),a(B,qe),s(e,ce,o),V(M,e,o),s(e,fe,o),V(G,e,o),s(e,he,o),s(e,U,o),a(U,De),s(e,de,o),s(e,v,o),a(v,He),a(v,te),a(te,Ve),a(v,je),a(v,ae),a(ae,Je),a(v,Ke),s(e,ue,o),V(S,e,o),s(e,ge,o),V(C,e,o),s(e,ve,o),s(e,_,o),a(_,Qe),a(_,L),a(L,Xe),a(_,Ye),a(_,N),a(N,Ze),a(_,et),_e=!0},p:Et,i(e){_e||(j(b.$$.fragment,e),j($.$$.fragment,e),j(M.$$.fragment,e),j(G.$$.fragment,e),j(S.$$.fragment,e),j(C.$$.fragment,e),_e=!0)},o(e){J(b.$$.fragment,e),J($.$$.fragment,e),J(M.$$.fragment,e),J(G.$$.fragment,e),J(S.$$.fragment,e),J(C.$$.fragment,e),_e=!1},d(e){t(u),e&&t(R),e&&t(h),K(b),e&&t(oe),e&&t(w),K($),e&&t(re),e&&t(T),e&&t(ne),e&&t(I),e&&t(ie),e&&t(x),e&&t(le),e&&t(z),e&&t(se),e&&t(P),e&&t(me),e&&t(g),e&&t(pe),e&&t(B),e&&t(ce),K(M,e),e&&t(fe),K(G,e),e&&t(he),e&&t(U),e&&t(de),e&&t(v),e&&t(ue),K(S,e),e&&t(ge),K(C,e),e&&t(ve),e&&t(_)}}}const Ot={local:"megatrongpt2",sections:[{local:"overview",title:"Overview"}],title:"MegatronGPT2"};function Mt($e,u,R){let{fw:h}=u;return $e.$$set=d=>{"fw"in d&&R(0,h=d.fw)},[h]}class Nt extends Pt{constructor(u){super();bt(this,u,Mt,kt,$t,{fw:0})}}export{Nt as default,Ot as metadata};
