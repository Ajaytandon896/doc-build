import{S as vi,i as bi,s as wi,e as o,k as c,w as _,t as r,M as yi,c as a,d as n,m as d,a as s,x as v,h as i,b as l,F as e,g as m,y as b,q as w,o as y,B as S}from"../../chunks/vendor-68651a14.js";import{T as vo}from"../../chunks/Tip-3fbaa85f.js";import{D as W}from"../../chunks/Docstring-d699e906.js";import{C as Tn}from"../../chunks/CodeBlock-b98730f5.js";import{I as Te}from"../../chunks/IconCopyLink-8ff17449.js";import"../../chunks/CopyButton-e88c769b.js";function Si(A){let h,U,u,T,k;return{c(){h=o("p"),U=r(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),u=o("code"),T=r("Module"),k=r(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(f){h=a(f,"P",{});var g=s(h);U=i(g,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(n),k=i(g,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),g.forEach(n)},m(f,g){m(f,h,g),e(h,U),e(h,u),e(u,T),e(h,k)},d(f){f&&n(h)}}}function Ti(A){let h,U,u,T,k;return{c(){h=o("p"),U=r(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),u=o("code"),T=r("Module"),k=r(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(f){h=a(f,"P",{});var g=s(h);U=i(g,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(n),k=i(g,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),g.forEach(n)},m(f,g){m(f,h,g),e(h,U),e(h,u),e(u,T),e(h,k)},d(f){f&&n(h)}}}function Ui(A){let h,U,u,T,k;return{c(){h=o("p"),U=r(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),u=o("code"),T=r("Module"),k=r(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(f){h=a(f,"P",{});var g=s(h);U=i(g,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(n),k=i(g,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),g.forEach(n)},m(f,g){m(f,h,g),e(h,U),e(h,u),e(u,T),e(h,k)},d(f){f&&n(h)}}}function ki(A){let h,U,u,T,k;return{c(){h=o("p"),U=r(`Although the recipe for forward pass needs to be defined within this function, one should call the
`),u=o("code"),T=r("Module"),k=r(` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`)},l(f){h=a(f,"P",{});var g=s(h);U=i(g,`Although the recipe for forward pass needs to be defined within this function, one should call the
`),u=a(g,"CODE",{});var $=s(u);T=i($,"Module"),$.forEach(n),k=i(g,` instance afterwards instead of this since the former takes care of running the pre and post
processing steps while the latter silently ignores them.`),g.forEach(n)},m(f,g){m(f,h,g),e(h,U),e(h,u),e(u,T),e(h,k)},d(f){f&&n(h)}}}function $i(A){let h,U,u,T,k,f,g,$,bo,Un,R,de,Kt,Ue,wo,Bt,yo,kn,pe,So,ke,To,Uo,$n,yt,ko,jn,St,Yt,$o,Cn,Tt,jo,xn,he,$e,Co,Ut,xo,qo,Fo,je,Po,kt,Eo,Mo,qn,L,zo,Ce,Do,Ao,xe,Wo,Lo,Fn,Q,me,Rt,qe,Oo,Qt,No,Pn,C,Fe,Vo,X,Io,$t,Ho,Ko,Pe,Bo,Yo,Ro,Z,Qo,jt,Xo,Zo,Ct,Jo,Go,ea,Xt,ta,na,Ee,En,J,ue,Zt,Me,oa,Jt,aa,Mn,G,ze,sa,De,ra,Gt,ia,la,zn,ee,Ae,ca,We,da,en,pa,ha,Dn,te,fe,tn,Le,ma,nn,ua,An,x,Oe,fa,Ne,ga,Ve,_a,va,ba,Ie,wa,xt,ya,Sa,Ta,He,Ua,Ke,ka,$a,ja,P,Be,Ca,ne,xa,qt,qa,Fa,on,Pa,Ea,Ma,ge,za,an,Da,Aa,Ye,Wn,oe,_e,sn,Re,Wa,rn,La,Ln,q,Qe,Oa,ae,Na,ln,Va,Ia,Xe,Ha,Ka,Ba,Ze,Ya,Ft,Ra,Qa,Xa,Je,Za,Ge,Ja,Ga,es,E,et,ts,se,ns,Pt,os,as,cn,ss,rs,is,ve,ls,dn,cs,ds,tt,On,re,be,pn,nt,ps,hn,hs,Nn,j,ot,ms,mn,us,fs,at,gs,st,_s,vs,bs,rt,ws,Et,ys,Ss,Ts,it,Us,lt,ks,$s,js,M,ct,Cs,ie,xs,Mt,qs,Fs,un,Ps,Es,Ms,we,zs,fn,Ds,As,dt,Vn,le,ye,gn,pt,Ws,_n,Ls,In,F,ht,Os,mt,Ns,ut,Vs,Is,Hs,ft,Ks,zt,Bs,Ys,Rs,gt,Qs,_t,Xs,Zs,Js,z,vt,Gs,ce,er,Dt,tr,nr,vn,or,ar,sr,Se,rr,bn,ir,lr,bt,Hn;return f=new Te({}),Ue=new Te({}),qe=new Te({}),Fe=new W({props:{name:"class transformers.UniSpeechConfig",anchor:"transformers.UniSpeechConfig",parameters:[{name:"vocab_size",val:" = 32"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"feat_proj_dropout",val:" = 0.0"},{name:"feat_quantizer_dropout",val:" = 0.0"},{name:"final_dropout",val:" = 0.1"},{name:"layerdrop",val:" = 0.1"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"feat_extract_norm",val:" = 'group'"},{name:"feat_extract_activation",val:" = 'gelu'"},{name:"conv_dim",val:" = (512, 512, 512, 512, 512, 512, 512)"},{name:"conv_stride",val:" = (5, 2, 2, 2, 2, 2, 2)"},{name:"conv_kernel",val:" = (10, 3, 3, 3, 3, 2, 2)"},{name:"conv_bias",val:" = False"},{name:"num_conv_pos_embeddings",val:" = 128"},{name:"num_conv_pos_embedding_groups",val:" = 16"},{name:"do_stable_layer_norm",val:" = False"},{name:"apply_spec_augment",val:" = True"},{name:"mask_time_prob",val:" = 0.05"},{name:"mask_time_length",val:" = 10"},{name:"mask_time_min_masks",val:" = 2"},{name:"mask_feature_prob",val:" = 0.0"},{name:"mask_feature_length",val:" = 10"},{name:"mask_feature_min_masks",val:" = 0"},{name:"num_codevectors_per_group",val:" = 320"},{name:"num_codevector_groups",val:" = 2"},{name:"contrastive_logits_temperature",val:" = 0.1"},{name:"num_negatives",val:" = 100"},{name:"codevector_dim",val:" = 256"},{name:"proj_codevector_dim",val:" = 256"},{name:"diversity_loss_weight",val:" = 0.1"},{name:"ctc_loss_reduction",val:" = 'mean'"},{name:"ctc_zero_infinity",val:" = False"},{name:"use_weighted_layer_sum",val:" = False"},{name:"classifier_proj_size",val:" = 256"},{name:"num_ctc_classes",val:" = 80"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"replace_prob",val:" = 0.5"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/configuration_unispeech.py#L29",parametersDescription:[{anchor:"transformers.UniSpeechConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Vocabulary size of the UniSpeech model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a>. Vocabulary size of the
model. Defines the different tokens that can be represented by the <em>inputs_ids</em> passed to the forward
method of <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a>.`,name:"vocab_size"},{anchor:"transformers.UniSpeechConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.UniSpeechConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.UniSpeechConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.UniSpeechConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.UniSpeechConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string,
<code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.UniSpeechConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout"},{anchor:"transformers.UniSpeechConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.UniSpeechConfig.final_dropout",description:`<strong>final_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for the final projection layer of <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a>.`,name:"final_dropout"},{anchor:"transformers.UniSpeechConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.UniSpeechConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.UniSpeechConfig.feat_extract_norm",description:`<strong>feat_extract_norm</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;group&quot;</code>) &#x2014;
The norm to be applied to 1D convolutional layers in feature extractor. One of <code>&quot;group&quot;</code> for group
normalization of only the first 1D convolutional layer or <code>&quot;layer&quot;</code> for layer normalization of all 1D
convolutional layers.`,name:"feat_extract_norm"},{anchor:"transformers.UniSpeechConfig.feat_proj_dropout",description:`<strong>feat_proj_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for output of the feature extractor.`,name:"feat_proj_dropout"},{anchor:"transformers.UniSpeechConfig.feat_extract_activation",description:"<strong>feat_extract_activation</strong> (<code>str, </code>optional<code>, defaults to </code>&#x201C;gelu&#x201D;<code>) -- The non-linear activation function (function or string) in the 1D convolutional layers of the feature extractor. If string, </code>&#x201C;gelu&#x201D;<code>, </code>&#x201C;relu&#x201D;<code>, </code>&#x201C;selu&#x201D;<code>and</code>&#x201C;gelu_new&#x201D;` are supported.",name:"feat_extract_activation"},{anchor:"transformers.UniSpeechConfig.feat_quantizer_dropout",description:`<strong>feat_quantizer_dropout</strong> (obj &#x2014;<em>float</em>, <em>optional</em>, defaults to 0.0):
The dropout probabilitiy for quantized feature extractor states.`,name:"feat_quantizer_dropout"},{anchor:"transformers.UniSpeechConfig.conv_dim",description:`<strong>conv_dim</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(512, 512, 512, 512, 512, 512, 512)</code>) &#x2014;
A tuple of integers defining the number of input and output channels of each 1D convolutional layer in the
feature extractor. The length of <em>conv_dim</em> defines the number of 1D convolutional layers.`,name:"conv_dim"},{anchor:"transformers.UniSpeechConfig.conv_stride",description:`<strong>conv_stride</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(5, 2, 2, 2, 2, 2, 2)</code>) &#x2014;
A tuple of integers defining the stride of each 1D convolutional layer in the feature extractor. The length
of <em>conv_stride</em> defines the number of convolutional layers and has to match the the length of <em>conv_dim</em>.`,name:"conv_stride"},{anchor:"transformers.UniSpeechConfig.conv_kernel",description:`<strong>conv_kernel</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>(10, 3, 3, 3, 3, 3, 3)</code>) &#x2014;
A tuple of integers defining the kernel size of each 1D convolutional layer in the feature extractor. The
length of <em>conv_kernel</em> defines the number of convolutional layers and has to match the the length of
<em>conv_dim</em>.`,name:"conv_kernel"},{anchor:"transformers.UniSpeechConfig.conv_bias",description:`<strong>conv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the 1D convolutional layers have a bias.`,name:"conv_bias"},{anchor:"transformers.UniSpeechConfig.num_conv_pos_embeddings",description:`<strong>num_conv_pos_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Number of convolutional positional embeddings. Defines the kernel size of 1D convolutional positional
embeddings layer.`,name:"num_conv_pos_embeddings"},{anchor:"transformers.UniSpeechConfig.num_conv_pos_embedding_groups",description:`<strong>num_conv_pos_embedding_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of groups of 1D convolutional positional embeddings layer.`,name:"num_conv_pos_embedding_groups"},{anchor:"transformers.UniSpeechConfig.do_stable_layer_norm",description:`<strong>do_stable_layer_norm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to apply <em>stable</em> layer norm architecture of the Transformer encoder. <code>do_stable_layer_norm is True</code> corresponds to applying layer norm before the attention layer, whereas <code>do_stable_layer_norm is False</code> corresponds to applying layer norm after the attention layer.`,name:"do_stable_layer_norm"},{anchor:"transformers.UniSpeechConfig.apply_spec_augment",description:`<strong>apply_spec_augment</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to apply <em>SpecAugment</em> data augmentation to the outputs of the feature extractor. For reference see
<a href="https://arxiv.org/abs/1904.08779" rel="nofollow">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</a>.`,name:"apply_spec_augment"},{anchor:"transformers.UniSpeechConfig.mask_time_prob",description:`<strong>mask_time_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.05) &#x2014;
Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking
procecure generates &#x201D;mask<em>time_prob*len(time_axis)/mask_time_length&#x201D; independent masks over the axis. If
reasoning from the propability of each feature vector to be chosen as the start of the vector span to be
masked, _mask_time_prob</em> should be <code>prob_vector_start*mask_time_length</code>. Note that overlap may decrease
the actual percentage of masked vectors. This is only relevant if <code>apply_spec_augment is True</code>.`,name:"mask_time_prob"},{anchor:"transformers.UniSpeechConfig.mask_time_length",description:`<strong>mask_time_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Length of vector span along the time axis.`,name:"mask_time_length"},{anchor:"transformers.UniSpeechConfig.mask_time_min_masks",description:`<strong>mask_time_min_masks</strong> (<code>int</code>, <em>optional</em>, defaults to 2), &#x2014;
The minimum number of masks of length <code>mask_feature_length</code> generated along the time axis, each time
step, irrespectively of <code>mask_feature_prob</code>. Only relevant if
&#x201D;mask_time_prob*len(time_axis)/mask_time_length &lt; mask_time_min_masks&#x201D;`,name:"mask_time_min_masks"},{anchor:"transformers.UniSpeechConfig.mask_feature_prob",description:`<strong>mask_feature_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The
masking procecure generates &#x201D;mask<em>feature_prob*len(feature_axis)/mask_time_length&#x201D; independent masks over
the axis. If reasoning from the propability of each feature vector to be chosen as the start of the vector
span to be masked, _mask_feature_prob</em> should be <code>prob_vector_start*mask_feature_length</code>. Note that
overlap may decrease the actual percentage of masked vectors. This is only relevant if <code>apply_spec_augment is True</code>.`,name:"mask_feature_prob"},{anchor:"transformers.UniSpeechConfig.mask_feature_length",description:`<strong>mask_feature_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Length of vector span along the feature axis.`,name:"mask_feature_length"},{anchor:"transformers.UniSpeechConfig.mask_feature_min_masks",description:`<strong>mask_feature_min_masks</strong> (<code>int</code>, <em>optional</em>, defaults to 0), &#x2014;
The minimum number of masks of length <code>mask_feature_length</code> generated along the feature axis, each time
step, irrespectively of <code>mask_feature_prob</code>. Only relevant if
&#x201D;mask_feature_prob*len(feature_axis)/mask_feature_length &lt; mask_feature_min_masks&#x201D;`,name:"mask_feature_min_masks"},{anchor:"transformers.UniSpeechConfig.num_codevectors_per_group",description:`<strong>num_codevectors_per_group</strong> (<code>int</code>, <em>optional</em>, defaults to 320) &#x2014;
Number of entries in each quantization codebook (group).`,name:"num_codevectors_per_group"},{anchor:"transformers.UniSpeechConfig.num_codevector_groups",description:`<strong>num_codevector_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of codevector groups for product codevector quantization.`,name:"num_codevector_groups"},{anchor:"transformers.UniSpeechConfig.contrastive_logits_temperature",description:`<strong>contrastive_logits_temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The temperature <em>kappa</em> in the contrastive loss.`,name:"contrastive_logits_temperature"},{anchor:"transformers.UniSpeechConfig.feat_quantizer_dropout",description:`<strong>feat_quantizer_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probabilitiy for the output of the feature extractor that&#x2019;s used by the quantizer.`,name:"feat_quantizer_dropout"},{anchor:"transformers.UniSpeechConfig.num_negatives",description:`<strong>num_negatives</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Number of negative samples for the contrastive loss.`,name:"num_negatives"},{anchor:"transformers.UniSpeechConfig.codevector_dim",description:`<strong>codevector_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the quantized feature vectors.`,name:"codevector_dim"},{anchor:"transformers.UniSpeechConfig.proj_codevector_dim",description:`<strong>proj_codevector_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the final projection of both the quantized and the transformer features.`,name:"proj_codevector_dim"},{anchor:"transformers.UniSpeechConfig.diversity_loss_weight",description:`<strong>diversity_loss_weight</strong> (<code>int</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The weight of the codebook diversity loss component.`,name:"diversity_loss_weight"},{anchor:"transformers.UniSpeechConfig.ctc_loss_reduction",description:`<strong>ctc_loss_reduction</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Specifies the reduction to apply to the output of <code>torch.nn.CTCLoss</code>. Only relevant when training an
instance of <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a>.`,name:"ctc_loss_reduction"},{anchor:"transformers.UniSpeechConfig.ctc_zero_infinity",description:`<strong>ctc_zero_infinity</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to zero infinite losses and the associated gradients of <code>torch.nn.CTCLoss</code>. Infinite losses
mainly occur when the inputs are too short to be aligned to the targets. Only relevant when training an
instance of <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a>.`,name:"ctc_zero_infinity"},{anchor:"transformers.UniSpeechConfig.use_weighted_layer_sum",description:`<strong>use_weighted_layer_sum</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a weighted average of layer outputs with learned weights. Only relevant when using an
instance of <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a>.`,name:"use_weighted_layer_sum"},{anchor:"transformers.UniSpeechConfig.classifier_proj_size",description:`<strong>classifier_proj_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimensionality of the projection before token mean-pooling for classification.`,name:"classifier_proj_size"},{anchor:"transformers.UniSpeechConfig.replace_prob",description:`<strong>replace_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
Propability that transformer feature is replaced by quantized feature for pretraining.`,name:"replace_prob"}]}}),Ee=new Tn({props:{code:`from transformers import UniSpeechModel, UniSpeechConfig

# Initializing a UniSpeech facebook/unispeech-base-960h style configuration
configuration = UniSpeechConfig()

# Initializing a model from the facebook/unispeech-base-960h style configuration
model = UniSpeechModel(configuration)

# Accessing the model configuration
configuration = model.config,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> UniSpeechModel, UniSpeechConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a UniSpeech facebook/unispeech-base-960h style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = UniSpeechConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the facebook/unispeech-base-960h style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),Me=new Te({}),ze=new W({props:{name:"class transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput",anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"extract_features",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/modeling_unispeech.py#L61",parametersDescription:[{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.extract_features",description:`<strong>extract_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, conv_dim[-1])</code>) &#x2014;
Sequence of extracted feature vectors of the last convolutional layer of the model.`,name:"extract_features"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Ae=new W({props:{name:"class transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput",anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"projected_states",val:": FloatTensor = None"},{name:"projected_quantized_states",val:": FloatTensor = None"},{name:"codevector_perplexity",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/modeling_unispeech.py#L90",parametersDescription:[{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.loss",description:`<strong>loss</strong> (<em>optional</em>, returned when model is in train mode, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) &#x2014;
Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the <a href="https://arxiv.org/pdf/2006.11477.pdf" rel="nofollow">official
paper</a> . (classification) loss.`,name:"loss"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.projected_states",description:`<strong>projected_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) &#x2014;
Hidden-states of the model projected to <em>config.proj_codevector_dim</em> that can be used to predict the masked
projected quantized states.`,name:"projected_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.projected_quantized_states",description:`<strong>projected_quantized_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) &#x2014;
Quantized extracted feature vectors projected to <em>config.proj_codevector_dim</em> representing the positive
target vectors for contrastive loss.`,name:"projected_quantized_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}]}}),Le=new Te({}),Oe=new W({props:{name:"class transformers.UniSpeechModel",anchor:"transformers.UniSpeechModel",parameters:[{name:"config",val:": UniSpeechConfig"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/modeling_unispeech.py#L1046",parametersDescription:[{anchor:"transformers.UniSpeechModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),Be=new W({props:{name:"forward",anchor:"transformers.UniSpeechModel.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"mask_time_indices",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/modeling_unispeech.py#L1110",parametersDescription:[{anchor:"transformers.UniSpeechModel.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should
be used for padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See
<code>transformers.UniSpeechProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.UniSpeechModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"
>UniSpeechBaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>extract_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, conv_dim[-1])</code>) \u2014 Sequence of extracted feature vectors of the last convolutional layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"
>UniSpeechBaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ge=new vo({props:{$$slots:{default:[Si]},$$scope:{ctx:A}}}),Ye=new Tn({props:{code:`from transformers import Wav2Vec2Processor, UniSpeechModel
from datasets import load_dataset

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
sampling_rate = dataset.features["audio"].sampling_rate

processor = Wav2Vec2Processor.from_pretrained('microsoft/unispeech-large-1500h-cv')
model = UniSpeechModel.from_pretrained('microsoft/unispeech-large-1500h-cv')

# audio file is decoded on the fly
inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2Processor, UniSpeechModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechModel.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Re=new Te({}),Qe=new W({props:{name:"class transformers.UniSpeechForCTC",anchor:"transformers.UniSpeechForCTC",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/modeling_unispeech.py#L1339",parametersDescription:[{anchor:"transformers.UniSpeechForCTC.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),et=new W({props:{name:"forward",anchor:"transformers.UniSpeechForCTC.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/modeling_unispeech.py#L1365",parametersDescription:[{anchor:"transformers.UniSpeechForCTC.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should
be used for padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See
<code>transformers.UniSpeechProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.UniSpeechForCTC.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>CausalLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.CausalLMOutput"
>CausalLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ve=new vo({props:{$$slots:{default:[Ti]},$$scope:{ctx:A}}}),tt=new Tn({props:{code:`from transformers import Wav2Vec2Processor, UniSpeechForCTC
from datasets import load_dataset
import torch

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
sampling_rate = dataset.features["audio"].sampling_rate

processor = Wav2Vec2Processor.from_pretrained('microsoft/unispeech-large-1500h-cv')
model = UniSpeechForCTC.from_pretrained('microsoft/unispeech-large-1500h-cv')

# audio file is decoded on the fly
inputs = processor(dataset[0]["audio"]["array"], sampling_rate=sampling_rate, return_tensors="pt")
logits = model(**inputs).logits
predicted_ids = torch.argmax(logits, dim=-1)

# transcribe speech
transcription = processor.batch_decode(predicted_ids)

# compute loss
with processor.as_target_processor():
    inputs["labels"] = processor(dataset[0]["text"], return_tensors="pt").input_ids

loss = model(**inputs).loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2Processor, UniSpeechForCTC
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Wav2Vec2Processor.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechForCTC.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># transcribe speech</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(predicted_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> processor.as_target_processor():
<span class="hljs-meta">... </span>    inputs[<span class="hljs-string">&quot;labels&quot;</span>] = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;text&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss`}}),nt=new Te({}),ot=new W({props:{name:"class transformers.UniSpeechForSequenceClassification",anchor:"transformers.UniSpeechForSequenceClassification",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/modeling_unispeech.py#L1453",parametersDescription:[{anchor:"transformers.UniSpeechForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),ct=new W({props:{name:"forward",anchor:"transformers.UniSpeechForSequenceClassification.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"labels",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/modeling_unispeech.py#L1482",parametersDescription:[{anchor:"transformers.UniSpeechForSequenceClassification.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should
be used for padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See
<code>transformers.UniSpeechProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.UniSpeechForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),we=new vo({props:{$$slots:{default:[Ui]},$$scope:{ctx:A}}}),dt=new Tn({props:{code:`from transformers import Wav2Vec2FeatureExtractor, UniSpeechForSequenceClassification
from datasets import load_dataset
import torch

dataset = load_dataset("hf-internal-testing/librispeech_asr_demo", "clean", split="validation")
sampling_rate = dataset.features["audio"].sampling_rate

feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/unispeech-large-1500h-cv')
model = UniSpeechForSequenceClassification.from_pretrained('microsoft/unispeech-large-1500h-cv')

# audio file is decoded on the fly
inputs = feature_extractor(dataset[0]["audio"]["array"], return_tensors="pt")
logits = model(**inputs).logits
predicted_class_ids = torch.argmax(logits, dim=-1)
predicted_label = model.config.id2label[predicted_class_ids]

# compute loss - target_label is e.g. "down"
target_label = model.config.id2label[0]
inputs["labels"] = torch.tensor([model.config.label2id[target_label]])
loss = model(**inputs).loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2FeatureExtractor, UniSpeechForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UniSpeechForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;microsoft/unispeech-large-1500h-cv&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = model.config.id2label[predicted_class_ids]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss - target_label is e.g. &quot;down&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_label = model.config.id2label[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([model.config.label2id[target_label]])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss`}}),pt=new Te({}),ht=new W({props:{name:"class transformers.UniSpeechForPreTraining",anchor:"transformers.UniSpeechForPreTraining",parameters:[{name:"config",val:": UniSpeechConfig"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/modeling_unispeech.py#L1169",parametersDescription:[{anchor:"transformers.UniSpeechForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model
weights.`,name:"config"}]}}),vt=new W({props:{name:"forward",anchor:"transformers.UniSpeechForPreTraining.forward",parameters:[{name:"input_values",val:""},{name:"attention_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/unispeech/modeling_unispeech.py#L1219",parametersDescription:[{anchor:"transformers.UniSpeechForPreTraining.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Float values of input raw speech waveform. Values can be obtained by loading a <em>.flac</em> or <em>.wav</em> audio file
into an array of type <em>List[float]</em> or a <em>numpy.ndarray</em>, <em>e.g.</em> via the soundfile library (<em>pip install
soundfile</em>). To prepare the array into <em>input_values</em>, the <code>UniSpeechProcessor</code> should
be used for padding and conversion into a tensor of type <em>torch.FloatTensor</em>. See
<code>transformers.UniSpeechProcessor.__call__</code> for details.`,name:"input_values"},{anchor:"transformers.UniSpeechForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing convolution and attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput"
>UniSpeechForPreTrainingOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising
various elements depending on the configuration (<a
  href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"
>UniSpeechConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<em>optional</em>, returned when model is in train mode, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) \u2014 Total loss as the sum of the contrastive loss (L_m) and the diversity loss (L_d) as stated in the <a
  href="https://arxiv.org/pdf/2006.11477.pdf"
  rel="nofollow"
>official
paper</a> . (classification) loss.</p>
</li>
<li>
<p><strong>projected_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) \u2014 Hidden-states of the model projected to <em>config.proj_codevector_dim</em> that can be used to predict the masked
projected quantized states.</p>
</li>
<li>
<p><strong>projected_quantized_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.proj_codevector_dim)</code>) \u2014 Quantized extracted feature vectors projected to <em>config.proj_codevector_dim</em> representing the positive
target vectors for contrastive loss.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer)
of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/model_doc/unispeech#transformers.models.unispeech.modeling_unispeech.UniSpeechForPreTrainingOutput"
>UniSpeechForPreTrainingOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Se=new vo({props:{$$slots:{default:[ki]},$$scope:{ctx:A}}}),bt=new Tn({props:{code:`import torch
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForPreTraining
from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices
from datasets import load_dataset
import soundfile as sf

feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("patrickvonplaten/wav2vec2-base")
model = Wav2Vec2ForPreTraining.from_pretrained("patrickvonplaten/wav2vec2-base")


def map_to_array(batch):
    speech, _ = sf.read(batch["file"])
    batch["speech"] = speech
    return batch


ds = load_dataset("patrickvonplaten/librispeech_asr_dummy", "clean", split="validation")
ds = ds.map(map_to_array)

input_values = feature_extractor(ds["speech"][0], return_tensors="pt").input_values  # Batch size 1

# compute masked indices
batch_size, raw_sequence_length = input_values.shape
sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length)
mask_time_indices = _compute_mask_indices((batch_size, sequence_length), mask_prob=0.2, mask_length=2, device=model.device)

with torch.no_grad():
    outputs = model(input_values, mask_time_indices=mask_time_indices)

# compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)
cosine_sim = torch.cosine_similarity(
    outputs.projected_states, outputs.projected_quantized_states, dim=-1
)

# show that cosine similarity is much higher than random
assert cosine_sim[mask_time_indices].mean() > 0.5

# for contrastive loss training model should be put into train mode
model.train()
loss = model(input_values, mask_time_indices=mask_time_indices).loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Wav2Vec2FeatureExtractor, Wav2Vec2ForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.models.wav2vec2.modeling_wav2vec2 <span class="hljs-keyword">import</span> _compute_mask_indices
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> soundfile <span class="hljs-keyword">as</span> sf

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/wav2vec2-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Wav2Vec2ForPreTraining.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/wav2vec2-base&quot;</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">map_to_array</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    speech, _ = sf.read(batch[<span class="hljs-string">&quot;file&quot;</span>])
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;speech&quot;</span>] = speech
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch


<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;patrickvonplaten/librispeech_asr_dummy&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.<span class="hljs-built_in">map</span>(map_to_array)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_values = feature_extractor(ds[<span class="hljs-string">&quot;speech&quot;</span>][<span class="hljs-number">0</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_values  <span class="hljs-comment"># Batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute masked indices</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size, raw_sequence_length = input_values.shape
<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_length = model._get_feat_extract_output_lengths(raw_sequence_length)
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_time_indices = _compute_mask_indices((batch_size, sequence_length), mask_prob=<span class="hljs-number">0.2</span>, mask_length=<span class="hljs-number">2</span>, device=model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(input_values, mask_time_indices=mask_time_indices)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute cosine similarity between predicted (=projected_states) and target (=projected_quantized_states)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>cosine_sim = torch.cosine_similarity(
<span class="hljs-meta">... </span>    outputs.projected_states, outputs.projected_quantized_states, dim=-<span class="hljs-number">1</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># show that cosine similarity is much higher than random</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> cosine_sim[mask_time_indices].mean() &gt; <span class="hljs-number">0.5</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># for contrastive loss training model should be put into train mode</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.train()
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(input_values, mask_time_indices=mask_time_indices).loss`}}),{c(){h=o("meta"),U=c(),u=o("h1"),T=o("a"),k=o("span"),_(f.$$.fragment),g=c(),$=o("span"),bo=r("UniSpeech"),Un=c(),R=o("h2"),de=o("a"),Kt=o("span"),_(Ue.$$.fragment),wo=c(),Bt=o("span"),yo=r("Overview"),kn=c(),pe=o("p"),So=r("The UniSpeech model was proposed in "),ke=o("a"),To=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),Uo=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael
Zeng, Xuedong Huang .`),$n=c(),yt=o("p"),ko=r("The abstract from the paper is the following:"),jn=c(),St=o("p"),Yt=o("em"),$o=r(`In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both
unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive
self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture
information more correlated with phonetic structures and improve the generalization across languages and domains. We
evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The
results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech
recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all
testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task,
i.e., a relative word error rate reduction of 6% against the previous approach.`),Cn=c(),Tt=o("p"),jo=r("Tips:"),xn=c(),he=o("ul"),$e=o("li"),Co=r(`UniSpeech is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. Please
use `),Ut=o("a"),xo=r("Wav2Vec2Processor"),qo=r(" for the feature extraction."),Fo=c(),je=o("li"),Po=r(`UniSpeech model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be
decoded using `),kt=o("a"),Eo=r("Wav2Vec2CTCTokenizer"),Mo=r("."),qn=c(),L=o("p"),zo=r("This model was contributed by "),Ce=o("a"),Do=r("patrickvonplaten"),Ao=r(`. The Authors\u2019 code can be
found `),xe=o("a"),Wo=r("here"),Lo=r("."),Fn=c(),Q=o("h2"),me=o("a"),Rt=o("span"),_(qe.$$.fragment),Oo=c(),Qt=o("span"),No=r("UniSpeechConfig"),Pn=c(),C=o("div"),_(Fe.$$.fragment),Vo=c(),X=o("p"),Io=r("This is the configuration class to store the configuration of a "),$t=o("a"),Ho=r("UniSpeechModel"),Ko=r(`. It is used
to instantiate an UniSpeech model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the UniSpeech
`),Pe=o("a"),Bo=r("facebook/unispeech-base-960h"),Yo=r(" architecture."),Ro=c(),Z=o("p"),Qo=r("Configuration objects inherit from "),jt=o("a"),Xo=r("PretrainedConfig"),Zo=r(` and can be used to control the model
outputs. Read the documentation from `),Ct=o("a"),Jo=r("PretrainedConfig"),Go=r(" for more information."),ea=c(),Xt=o("p"),ta=r("Example:"),na=c(),_(Ee.$$.fragment),En=c(),J=o("h2"),ue=o("a"),Zt=o("span"),_(Me.$$.fragment),oa=c(),Jt=o("span"),aa=r("UniSpeech specific outputs"),Mn=c(),G=o("div"),_(ze.$$.fragment),sa=c(),De=o("p"),ra=r("Output type of "),Gt=o("code"),ia=r("UniSpeechBaseModelOutput"),la=r(", with potential hidden states and attentions."),zn=c(),ee=o("div"),_(Ae.$$.fragment),ca=c(),We=o("p"),da=r("Output type of "),en=o("code"),pa=r("UniSpeechForPreTrainingOutput"),ha=r(", with potential hidden states and attentions."),Dn=c(),te=o("h2"),fe=o("a"),tn=o("span"),_(Le.$$.fragment),ma=c(),nn=o("span"),ua=r("UniSpeechModel"),An=c(),x=o("div"),_(Oe.$$.fragment),fa=c(),Ne=o("p"),ga=r(`The bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.
UniSpeech was proposed in `),Ve=o("a"),_a=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),va=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),ba=c(),Ie=o("p"),wa=r("This model inherits from "),xt=o("a"),ya=r("PreTrainedModel"),Sa=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),Ta=c(),He=o("p"),Ua=r("This model is a PyTorch "),Ke=o("a"),ka=r("torch.nn.Module"),$a=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ja=c(),P=o("div"),_(Be.$$.fragment),Ca=c(),ne=o("p"),xa=r("The "),qt=o("a"),qa=r("UniSpeechModel"),Fa=r(" forward method, overrides the "),on=o("code"),Pa=r("__call__"),Ea=r(" special method."),Ma=c(),_(ge.$$.fragment),za=c(),an=o("p"),Da=r("Example:"),Aa=c(),_(Ye.$$.fragment),Wn=c(),oe=o("h2"),_e=o("a"),sn=o("span"),_(Re.$$.fragment),Wa=c(),rn=o("span"),La=r("UniSpeechForCTC"),Ln=c(),q=o("div"),_(Qe.$$.fragment),Oa=c(),ae=o("p"),Na=r("UniSpeech Model with a "),ln=o("em"),Va=r("language modeling"),Ia=r(` head on top for Connectionist Temporal Classification (CTC).
UniSpeech was proposed in `),Xe=o("a"),Ha=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),Ka=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),Ba=c(),Ze=o("p"),Ya=r("This model inherits from "),Ft=o("a"),Ra=r("PreTrainedModel"),Qa=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),Xa=c(),Je=o("p"),Za=r("This model is a PyTorch "),Ge=o("a"),Ja=r("torch.nn.Module"),Ga=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),es=c(),E=o("div"),_(et.$$.fragment),ts=c(),se=o("p"),ns=r("The "),Pt=o("a"),os=r("UniSpeechForCTC"),as=r(" forward method, overrides the "),cn=o("code"),ss=r("__call__"),rs=r(" special method."),is=c(),_(ve.$$.fragment),ls=c(),dn=o("p"),cs=r("Example:"),ds=c(),_(tt.$$.fragment),On=c(),re=o("h2"),be=o("a"),pn=o("span"),_(nt.$$.fragment),ps=c(),hn=o("span"),hs=r("UniSpeechForSequenceClassification"),Nn=c(),j=o("div"),_(ot.$$.fragment),ms=c(),mn=o("p"),us=r(`UniSpeech Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like
SUPERB Keyword Spotting.`),fs=c(),at=o("p"),gs=r("UniSpeech was proposed in "),st=o("a"),_s=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),vs=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),bs=c(),rt=o("p"),ws=r("This model inherits from "),Et=o("a"),ys=r("PreTrainedModel"),Ss=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),Ts=c(),it=o("p"),Us=r("This model is a PyTorch "),lt=o("a"),ks=r("torch.nn.Module"),$s=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),js=c(),M=o("div"),_(ct.$$.fragment),Cs=c(),ie=o("p"),xs=r("The "),Mt=o("a"),qs=r("UniSpeechForSequenceClassification"),Fs=r(" forward method, overrides the "),un=o("code"),Ps=r("__call__"),Es=r(" special method."),Ms=c(),_(we.$$.fragment),zs=c(),fn=o("p"),Ds=r("Example:"),As=c(),_(dt.$$.fragment),Vn=c(),le=o("h2"),ye=o("a"),gn=o("span"),_(pt.$$.fragment),Ws=c(),_n=o("span"),Ls=r("UniSpeechForPreTraining"),In=c(),F=o("div"),_(ht.$$.fragment),Os=c(),mt=o("p"),Ns=r(`UniSpeech Model with a vector-quantization module and ctc loss for pre-training.
UniSpeech was proposed in `),ut=o("a"),Vs=r("UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),Is=r(` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),Hs=c(),ft=o("p"),Ks=r("This model inherits from "),zt=o("a"),Bs=r("PreTrainedModel"),Ys=r(`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),Rs=c(),gt=o("p"),Qs=r("This model is a PyTorch "),_t=o("a"),Xs=r("torch.nn.Module"),Zs=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Js=c(),z=o("div"),_(vt.$$.fragment),Gs=c(),ce=o("p"),er=r("The "),Dt=o("a"),tr=r("UniSpeechForPreTraining"),nr=r(" forward method, overrides the "),vn=o("code"),or=r("__call__"),ar=r(" special method."),sr=c(),_(Se.$$.fragment),rr=c(),bn=o("p"),ir=r("Example:"),lr=c(),_(bt.$$.fragment),this.h()},l(t){const p=yi('[data-svelte="svelte-1phssyn"]',document.head);h=a(p,"META",{name:!0,content:!0}),p.forEach(n),U=d(t),u=a(t,"H1",{class:!0});var wt=s(u);T=a(wt,"A",{id:!0,class:!0,href:!0});var wn=s(T);k=a(wn,"SPAN",{});var yn=s(k);v(f.$$.fragment,yn),yn.forEach(n),wn.forEach(n),g=d(wt),$=a(wt,"SPAN",{});var Sn=s($);bo=i(Sn,"UniSpeech"),Sn.forEach(n),wt.forEach(n),Un=d(t),R=a(t,"H2",{class:!0});var Kn=s(R);de=a(Kn,"A",{id:!0,class:!0,href:!0});var cr=s(de);Kt=a(cr,"SPAN",{});var dr=s(Kt);v(Ue.$$.fragment,dr),dr.forEach(n),cr.forEach(n),wo=d(Kn),Bt=a(Kn,"SPAN",{});var pr=s(Bt);yo=i(pr,"Overview"),pr.forEach(n),Kn.forEach(n),kn=d(t),pe=a(t,"P",{});var Bn=s(pe);So=i(Bn,"The UniSpeech model was proposed in "),ke=a(Bn,"A",{href:!0,rel:!0});var hr=s(ke);To=i(hr,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),hr.forEach(n),Uo=i(Bn,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael
Zeng, Xuedong Huang .`),Bn.forEach(n),$n=d(t),yt=a(t,"P",{});var mr=s(yt);ko=i(mr,"The abstract from the paper is the following:"),mr.forEach(n),jn=d(t),St=a(t,"P",{});var ur=s(St);Yt=a(ur,"EM",{});var fr=s(Yt);$o=i(fr,`In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both
unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive
self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture
information more correlated with phonetic structures and improve the generalization across languages and domains. We
evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The
results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech
recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all
testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task,
i.e., a relative word error rate reduction of 6% against the previous approach.`),fr.forEach(n),ur.forEach(n),Cn=d(t),Tt=a(t,"P",{});var gr=s(Tt);jo=i(gr,"Tips:"),gr.forEach(n),xn=d(t),he=a(t,"UL",{});var Yn=s(he);$e=a(Yn,"LI",{});var Rn=s($e);Co=i(Rn,`UniSpeech is a speech model that accepts a float array corresponding to the raw waveform of the speech signal. Please
use `),Ut=a(Rn,"A",{href:!0});var _r=s(Ut);xo=i(_r,"Wav2Vec2Processor"),_r.forEach(n),qo=i(Rn," for the feature extraction."),Rn.forEach(n),Fo=d(Yn),je=a(Yn,"LI",{});var Qn=s(je);Po=i(Qn,`UniSpeech model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be
decoded using `),kt=a(Qn,"A",{href:!0});var vr=s(kt);Eo=i(vr,"Wav2Vec2CTCTokenizer"),vr.forEach(n),Mo=i(Qn,"."),Qn.forEach(n),Yn.forEach(n),qn=d(t),L=a(t,"P",{});var At=s(L);zo=i(At,"This model was contributed by "),Ce=a(At,"A",{href:!0,rel:!0});var br=s(Ce);Do=i(br,"patrickvonplaten"),br.forEach(n),Ao=i(At,`. The Authors\u2019 code can be
found `),xe=a(At,"A",{href:!0,rel:!0});var wr=s(xe);Wo=i(wr,"here"),wr.forEach(n),Lo=i(At,"."),At.forEach(n),Fn=d(t),Q=a(t,"H2",{class:!0});var Xn=s(Q);me=a(Xn,"A",{id:!0,class:!0,href:!0});var yr=s(me);Rt=a(yr,"SPAN",{});var Sr=s(Rt);v(qe.$$.fragment,Sr),Sr.forEach(n),yr.forEach(n),Oo=d(Xn),Qt=a(Xn,"SPAN",{});var Tr=s(Qt);No=i(Tr,"UniSpeechConfig"),Tr.forEach(n),Xn.forEach(n),Pn=d(t),C=a(t,"DIV",{class:!0});var O=s(C);v(Fe.$$.fragment,O),Vo=d(O),X=a(O,"P",{});var Wt=s(X);Io=i(Wt,"This is the configuration class to store the configuration of a "),$t=a(Wt,"A",{href:!0});var Ur=s($t);Ho=i(Ur,"UniSpeechModel"),Ur.forEach(n),Ko=i(Wt,`. It is used
to instantiate an UniSpeech model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the UniSpeech
`),Pe=a(Wt,"A",{href:!0,rel:!0});var kr=s(Pe);Bo=i(kr,"facebook/unispeech-base-960h"),kr.forEach(n),Yo=i(Wt," architecture."),Wt.forEach(n),Ro=d(O),Z=a(O,"P",{});var Lt=s(Z);Qo=i(Lt,"Configuration objects inherit from "),jt=a(Lt,"A",{href:!0});var $r=s(jt);Xo=i($r,"PretrainedConfig"),$r.forEach(n),Zo=i(Lt,` and can be used to control the model
outputs. Read the documentation from `),Ct=a(Lt,"A",{href:!0});var jr=s(Ct);Jo=i(jr,"PretrainedConfig"),jr.forEach(n),Go=i(Lt," for more information."),Lt.forEach(n),ea=d(O),Xt=a(O,"P",{});var Cr=s(Xt);ta=i(Cr,"Example:"),Cr.forEach(n),na=d(O),v(Ee.$$.fragment,O),O.forEach(n),En=d(t),J=a(t,"H2",{class:!0});var Zn=s(J);ue=a(Zn,"A",{id:!0,class:!0,href:!0});var xr=s(ue);Zt=a(xr,"SPAN",{});var qr=s(Zt);v(Me.$$.fragment,qr),qr.forEach(n),xr.forEach(n),oa=d(Zn),Jt=a(Zn,"SPAN",{});var Fr=s(Jt);aa=i(Fr,"UniSpeech specific outputs"),Fr.forEach(n),Zn.forEach(n),Mn=d(t),G=a(t,"DIV",{class:!0});var Jn=s(G);v(ze.$$.fragment,Jn),sa=d(Jn),De=a(Jn,"P",{});var Gn=s(De);ra=i(Gn,"Output type of "),Gt=a(Gn,"CODE",{});var Pr=s(Gt);ia=i(Pr,"UniSpeechBaseModelOutput"),Pr.forEach(n),la=i(Gn,", with potential hidden states and attentions."),Gn.forEach(n),Jn.forEach(n),zn=d(t),ee=a(t,"DIV",{class:!0});var eo=s(ee);v(Ae.$$.fragment,eo),ca=d(eo),We=a(eo,"P",{});var to=s(We);da=i(to,"Output type of "),en=a(to,"CODE",{});var Er=s(en);pa=i(Er,"UniSpeechForPreTrainingOutput"),Er.forEach(n),ha=i(to,", with potential hidden states and attentions."),to.forEach(n),eo.forEach(n),Dn=d(t),te=a(t,"H2",{class:!0});var no=s(te);fe=a(no,"A",{id:!0,class:!0,href:!0});var Mr=s(fe);tn=a(Mr,"SPAN",{});var zr=s(tn);v(Le.$$.fragment,zr),zr.forEach(n),Mr.forEach(n),ma=d(no),nn=a(no,"SPAN",{});var Dr=s(nn);ua=i(Dr,"UniSpeechModel"),Dr.forEach(n),no.forEach(n),An=d(t),x=a(t,"DIV",{class:!0});var N=s(x);v(Oe.$$.fragment,N),fa=d(N),Ne=a(N,"P",{});var oo=s(Ne);ga=i(oo,`The bare UniSpeech Model transformer outputting raw hidden-states without any specific head on top.
UniSpeech was proposed in `),Ve=a(oo,"A",{href:!0,rel:!0});var Ar=s(Ve);_a=i(Ar,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),Ar.forEach(n),va=i(oo,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),oo.forEach(n),ba=d(N),Ie=a(N,"P",{});var ao=s(Ie);wa=i(ao,"This model inherits from "),xt=a(ao,"A",{href:!0});var Wr=s(xt);ya=i(Wr,"PreTrainedModel"),Wr.forEach(n),Sa=i(ao,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),ao.forEach(n),Ta=d(N),He=a(N,"P",{});var so=s(He);Ua=i(so,"This model is a PyTorch "),Ke=a(so,"A",{href:!0,rel:!0});var Lr=s(Ke);ka=i(Lr,"torch.nn.Module"),Lr.forEach(n),$a=i(so,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),so.forEach(n),ja=d(N),P=a(N,"DIV",{class:!0});var V=s(P);v(Be.$$.fragment,V),Ca=d(V),ne=a(V,"P",{});var Ot=s(ne);xa=i(Ot,"The "),qt=a(Ot,"A",{href:!0});var Or=s(qt);qa=i(Or,"UniSpeechModel"),Or.forEach(n),Fa=i(Ot," forward method, overrides the "),on=a(Ot,"CODE",{});var Nr=s(on);Pa=i(Nr,"__call__"),Nr.forEach(n),Ea=i(Ot," special method."),Ot.forEach(n),Ma=d(V),v(ge.$$.fragment,V),za=d(V),an=a(V,"P",{});var Vr=s(an);Da=i(Vr,"Example:"),Vr.forEach(n),Aa=d(V),v(Ye.$$.fragment,V),V.forEach(n),N.forEach(n),Wn=d(t),oe=a(t,"H2",{class:!0});var ro=s(oe);_e=a(ro,"A",{id:!0,class:!0,href:!0});var Ir=s(_e);sn=a(Ir,"SPAN",{});var Hr=s(sn);v(Re.$$.fragment,Hr),Hr.forEach(n),Ir.forEach(n),Wa=d(ro),rn=a(ro,"SPAN",{});var Kr=s(rn);La=i(Kr,"UniSpeechForCTC"),Kr.forEach(n),ro.forEach(n),Ln=d(t),q=a(t,"DIV",{class:!0});var I=s(q);v(Qe.$$.fragment,I),Oa=d(I),ae=a(I,"P",{});var Nt=s(ae);Na=i(Nt,"UniSpeech Model with a "),ln=a(Nt,"EM",{});var Br=s(ln);Va=i(Br,"language modeling"),Br.forEach(n),Ia=i(Nt,` head on top for Connectionist Temporal Classification (CTC).
UniSpeech was proposed in `),Xe=a(Nt,"A",{href:!0,rel:!0});var Yr=s(Xe);Ha=i(Yr,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),Yr.forEach(n),Ka=i(Nt,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),Nt.forEach(n),Ba=d(I),Ze=a(I,"P",{});var io=s(Ze);Ya=i(io,"This model inherits from "),Ft=a(io,"A",{href:!0});var Rr=s(Ft);Ra=i(Rr,"PreTrainedModel"),Rr.forEach(n),Qa=i(io,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),io.forEach(n),Xa=d(I),Je=a(I,"P",{});var lo=s(Je);Za=i(lo,"This model is a PyTorch "),Ge=a(lo,"A",{href:!0,rel:!0});var Qr=s(Ge);Ja=i(Qr,"torch.nn.Module"),Qr.forEach(n),Ga=i(lo,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),lo.forEach(n),es=d(I),E=a(I,"DIV",{class:!0});var H=s(E);v(et.$$.fragment,H),ts=d(H),se=a(H,"P",{});var Vt=s(se);ns=i(Vt,"The "),Pt=a(Vt,"A",{href:!0});var Xr=s(Pt);os=i(Xr,"UniSpeechForCTC"),Xr.forEach(n),as=i(Vt," forward method, overrides the "),cn=a(Vt,"CODE",{});var Zr=s(cn);ss=i(Zr,"__call__"),Zr.forEach(n),rs=i(Vt," special method."),Vt.forEach(n),is=d(H),v(ve.$$.fragment,H),ls=d(H),dn=a(H,"P",{});var Jr=s(dn);cs=i(Jr,"Example:"),Jr.forEach(n),ds=d(H),v(tt.$$.fragment,H),H.forEach(n),I.forEach(n),On=d(t),re=a(t,"H2",{class:!0});var co=s(re);be=a(co,"A",{id:!0,class:!0,href:!0});var Gr=s(be);pn=a(Gr,"SPAN",{});var ei=s(pn);v(nt.$$.fragment,ei),ei.forEach(n),Gr.forEach(n),ps=d(co),hn=a(co,"SPAN",{});var ti=s(hn);hs=i(ti,"UniSpeechForSequenceClassification"),ti.forEach(n),co.forEach(n),Nn=d(t),j=a(t,"DIV",{class:!0});var D=s(j);v(ot.$$.fragment,D),ms=d(D),mn=a(D,"P",{});var ni=s(mn);us=i(ni,`UniSpeech Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like
SUPERB Keyword Spotting.`),ni.forEach(n),fs=d(D),at=a(D,"P",{});var po=s(at);gs=i(po,"UniSpeech was proposed in "),st=a(po,"A",{href:!0,rel:!0});var oi=s(st);_s=i(oi,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),oi.forEach(n),vs=i(po,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),po.forEach(n),bs=d(D),rt=a(D,"P",{});var ho=s(rt);ws=i(ho,"This model inherits from "),Et=a(ho,"A",{href:!0});var ai=s(Et);ys=i(ai,"PreTrainedModel"),ai.forEach(n),Ss=i(ho,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),ho.forEach(n),Ts=d(D),it=a(D,"P",{});var mo=s(it);Us=i(mo,"This model is a PyTorch "),lt=a(mo,"A",{href:!0,rel:!0});var si=s(lt);ks=i(si,"torch.nn.Module"),si.forEach(n),$s=i(mo,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),mo.forEach(n),js=d(D),M=a(D,"DIV",{class:!0});var K=s(M);v(ct.$$.fragment,K),Cs=d(K),ie=a(K,"P",{});var It=s(ie);xs=i(It,"The "),Mt=a(It,"A",{href:!0});var ri=s(Mt);qs=i(ri,"UniSpeechForSequenceClassification"),ri.forEach(n),Fs=i(It," forward method, overrides the "),un=a(It,"CODE",{});var ii=s(un);Ps=i(ii,"__call__"),ii.forEach(n),Es=i(It," special method."),It.forEach(n),Ms=d(K),v(we.$$.fragment,K),zs=d(K),fn=a(K,"P",{});var li=s(fn);Ds=i(li,"Example:"),li.forEach(n),As=d(K),v(dt.$$.fragment,K),K.forEach(n),D.forEach(n),Vn=d(t),le=a(t,"H2",{class:!0});var uo=s(le);ye=a(uo,"A",{id:!0,class:!0,href:!0});var ci=s(ye);gn=a(ci,"SPAN",{});var di=s(gn);v(pt.$$.fragment,di),di.forEach(n),ci.forEach(n),Ws=d(uo),_n=a(uo,"SPAN",{});var pi=s(_n);Ls=i(pi,"UniSpeechForPreTraining"),pi.forEach(n),uo.forEach(n),In=d(t),F=a(t,"DIV",{class:!0});var B=s(F);v(ht.$$.fragment,B),Os=d(B),mt=a(B,"P",{});var fo=s(mt);Ns=i(fo,`UniSpeech Model with a vector-quantization module and ctc loss for pre-training.
UniSpeech was proposed in `),ut=a(fo,"A",{href:!0,rel:!0});var hi=s(ut);Vs=i(hi,"UniSpeech: Unified Speech Representation Learning with Labeled and Unlabeled Data"),hi.forEach(n),Is=i(fo,` by Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei,
Michael Zeng, Xuedong Huang.`),fo.forEach(n),Hs=d(B),ft=a(B,"P",{});var go=s(ft);Ks=i(go,"This model inherits from "),zt=a(go,"A",{href:!0});var mi=s(zt);Bs=i(mi,"PreTrainedModel"),mi.forEach(n),Ys=i(go,`. Check the superclass documentation for the generic
methods the library implements for all its model (such as downloading or saving etc.).`),go.forEach(n),Rs=d(B),gt=a(B,"P",{});var _o=s(gt);Qs=i(_o,"This model is a PyTorch "),_t=a(_o,"A",{href:!0,rel:!0});var ui=s(_t);Xs=i(ui,"torch.nn.Module"),ui.forEach(n),Zs=i(_o,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),_o.forEach(n),Js=d(B),z=a(B,"DIV",{class:!0});var Y=s(z);v(vt.$$.fragment,Y),Gs=d(Y),ce=a(Y,"P",{});var Ht=s(ce);er=i(Ht,"The "),Dt=a(Ht,"A",{href:!0});var fi=s(Dt);tr=i(fi,"UniSpeechForPreTraining"),fi.forEach(n),nr=i(Ht," forward method, overrides the "),vn=a(Ht,"CODE",{});var gi=s(vn);or=i(gi,"__call__"),gi.forEach(n),ar=i(Ht," special method."),Ht.forEach(n),sr=d(Y),v(Se.$$.fragment,Y),rr=d(Y),bn=a(Y,"P",{});var _i=s(bn);ir=i(_i,"Example:"),_i.forEach(n),lr=d(Y),v(bt.$$.fragment,Y),Y.forEach(n),B.forEach(n),this.h()},h(){l(h,"name","hf:doc:metadata"),l(h,"content",JSON.stringify(ji)),l(T,"id","unispeech"),l(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(T,"href","#unispeech"),l(u,"class","relative group"),l(de,"id","overview"),l(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(de,"href","#overview"),l(R,"class","relative group"),l(ke,"href","https://arxiv.org/abs/2101.07597"),l(ke,"rel","nofollow"),l(Ut,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),l(kt,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),l(Ce,"href","https://huggingface.co/patrickvonplaten"),l(Ce,"rel","nofollow"),l(xe,"href","https://github.com/microsoft/UniSpeech/tree/main/UniSpeech"),l(xe,"rel","nofollow"),l(me,"id","transformers.UniSpeechConfig"),l(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(me,"href","#transformers.UniSpeechConfig"),l(Q,"class","relative group"),l($t,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel"),l(Pe,"href","https://huggingface.co/facebook/unispeech-base-960h"),l(Pe,"rel","nofollow"),l(jt,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),l(Ct,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),l(C,"class","docstring"),l(ue,"id","transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"),l(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ue,"href","#transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput"),l(J,"class","relative group"),l(G,"class","docstring"),l(ee,"class","docstring"),l(fe,"id","transformers.UniSpeechModel"),l(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(fe,"href","#transformers.UniSpeechModel"),l(te,"class","relative group"),l(Ve,"href","https://arxiv.org/abs/2101.07597"),l(Ve,"rel","nofollow"),l(xt,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),l(Ke,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Ke,"rel","nofollow"),l(qt,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel"),l(P,"class","docstring"),l(x,"class","docstring"),l(_e,"id","transformers.UniSpeechForCTC"),l(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(_e,"href","#transformers.UniSpeechForCTC"),l(oe,"class","relative group"),l(Xe,"href","https://arxiv.org/abs/2101.07597"),l(Xe,"rel","nofollow"),l(Ft,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),l(Ge,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(Ge,"rel","nofollow"),l(Pt,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC"),l(E,"class","docstring"),l(q,"class","docstring"),l(be,"id","transformers.UniSpeechForSequenceClassification"),l(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(be,"href","#transformers.UniSpeechForSequenceClassification"),l(re,"class","relative group"),l(st,"href","https://arxiv.org/abs/2101.07597"),l(st,"rel","nofollow"),l(Et,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),l(lt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(lt,"rel","nofollow"),l(Mt,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),l(M,"class","docstring"),l(j,"class","docstring"),l(ye,"id","transformers.UniSpeechForPreTraining"),l(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),l(ye,"href","#transformers.UniSpeechForPreTraining"),l(le,"class","relative group"),l(ut,"href","https://arxiv.org/abs/2101.07597"),l(ut,"rel","nofollow"),l(zt,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),l(_t,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),l(_t,"rel","nofollow"),l(Dt,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),l(z,"class","docstring"),l(F,"class","docstring")},m(t,p){e(document.head,h),m(t,U,p),m(t,u,p),e(u,T),e(T,k),b(f,k,null),e(u,g),e(u,$),e($,bo),m(t,Un,p),m(t,R,p),e(R,de),e(de,Kt),b(Ue,Kt,null),e(R,wo),e(R,Bt),e(Bt,yo),m(t,kn,p),m(t,pe,p),e(pe,So),e(pe,ke),e(ke,To),e(pe,Uo),m(t,$n,p),m(t,yt,p),e(yt,ko),m(t,jn,p),m(t,St,p),e(St,Yt),e(Yt,$o),m(t,Cn,p),m(t,Tt,p),e(Tt,jo),m(t,xn,p),m(t,he,p),e(he,$e),e($e,Co),e($e,Ut),e(Ut,xo),e($e,qo),e(he,Fo),e(he,je),e(je,Po),e(je,kt),e(kt,Eo),e(je,Mo),m(t,qn,p),m(t,L,p),e(L,zo),e(L,Ce),e(Ce,Do),e(L,Ao),e(L,xe),e(xe,Wo),e(L,Lo),m(t,Fn,p),m(t,Q,p),e(Q,me),e(me,Rt),b(qe,Rt,null),e(Q,Oo),e(Q,Qt),e(Qt,No),m(t,Pn,p),m(t,C,p),b(Fe,C,null),e(C,Vo),e(C,X),e(X,Io),e(X,$t),e($t,Ho),e(X,Ko),e(X,Pe),e(Pe,Bo),e(X,Yo),e(C,Ro),e(C,Z),e(Z,Qo),e(Z,jt),e(jt,Xo),e(Z,Zo),e(Z,Ct),e(Ct,Jo),e(Z,Go),e(C,ea),e(C,Xt),e(Xt,ta),e(C,na),b(Ee,C,null),m(t,En,p),m(t,J,p),e(J,ue),e(ue,Zt),b(Me,Zt,null),e(J,oa),e(J,Jt),e(Jt,aa),m(t,Mn,p),m(t,G,p),b(ze,G,null),e(G,sa),e(G,De),e(De,ra),e(De,Gt),e(Gt,ia),e(De,la),m(t,zn,p),m(t,ee,p),b(Ae,ee,null),e(ee,ca),e(ee,We),e(We,da),e(We,en),e(en,pa),e(We,ha),m(t,Dn,p),m(t,te,p),e(te,fe),e(fe,tn),b(Le,tn,null),e(te,ma),e(te,nn),e(nn,ua),m(t,An,p),m(t,x,p),b(Oe,x,null),e(x,fa),e(x,Ne),e(Ne,ga),e(Ne,Ve),e(Ve,_a),e(Ne,va),e(x,ba),e(x,Ie),e(Ie,wa),e(Ie,xt),e(xt,ya),e(Ie,Sa),e(x,Ta),e(x,He),e(He,Ua),e(He,Ke),e(Ke,ka),e(He,$a),e(x,ja),e(x,P),b(Be,P,null),e(P,Ca),e(P,ne),e(ne,xa),e(ne,qt),e(qt,qa),e(ne,Fa),e(ne,on),e(on,Pa),e(ne,Ea),e(P,Ma),b(ge,P,null),e(P,za),e(P,an),e(an,Da),e(P,Aa),b(Ye,P,null),m(t,Wn,p),m(t,oe,p),e(oe,_e),e(_e,sn),b(Re,sn,null),e(oe,Wa),e(oe,rn),e(rn,La),m(t,Ln,p),m(t,q,p),b(Qe,q,null),e(q,Oa),e(q,ae),e(ae,Na),e(ae,ln),e(ln,Va),e(ae,Ia),e(ae,Xe),e(Xe,Ha),e(ae,Ka),e(q,Ba),e(q,Ze),e(Ze,Ya),e(Ze,Ft),e(Ft,Ra),e(Ze,Qa),e(q,Xa),e(q,Je),e(Je,Za),e(Je,Ge),e(Ge,Ja),e(Je,Ga),e(q,es),e(q,E),b(et,E,null),e(E,ts),e(E,se),e(se,ns),e(se,Pt),e(Pt,os),e(se,as),e(se,cn),e(cn,ss),e(se,rs),e(E,is),b(ve,E,null),e(E,ls),e(E,dn),e(dn,cs),e(E,ds),b(tt,E,null),m(t,On,p),m(t,re,p),e(re,be),e(be,pn),b(nt,pn,null),e(re,ps),e(re,hn),e(hn,hs),m(t,Nn,p),m(t,j,p),b(ot,j,null),e(j,ms),e(j,mn),e(mn,us),e(j,fs),e(j,at),e(at,gs),e(at,st),e(st,_s),e(at,vs),e(j,bs),e(j,rt),e(rt,ws),e(rt,Et),e(Et,ys),e(rt,Ss),e(j,Ts),e(j,it),e(it,Us),e(it,lt),e(lt,ks),e(it,$s),e(j,js),e(j,M),b(ct,M,null),e(M,Cs),e(M,ie),e(ie,xs),e(ie,Mt),e(Mt,qs),e(ie,Fs),e(ie,un),e(un,Ps),e(ie,Es),e(M,Ms),b(we,M,null),e(M,zs),e(M,fn),e(fn,Ds),e(M,As),b(dt,M,null),m(t,Vn,p),m(t,le,p),e(le,ye),e(ye,gn),b(pt,gn,null),e(le,Ws),e(le,_n),e(_n,Ls),m(t,In,p),m(t,F,p),b(ht,F,null),e(F,Os),e(F,mt),e(mt,Ns),e(mt,ut),e(ut,Vs),e(mt,Is),e(F,Hs),e(F,ft),e(ft,Ks),e(ft,zt),e(zt,Bs),e(ft,Ys),e(F,Rs),e(F,gt),e(gt,Qs),e(gt,_t),e(_t,Xs),e(gt,Zs),e(F,Js),e(F,z),b(vt,z,null),e(z,Gs),e(z,ce),e(ce,er),e(ce,Dt),e(Dt,tr),e(ce,nr),e(ce,vn),e(vn,or),e(ce,ar),e(z,sr),b(Se,z,null),e(z,rr),e(z,bn),e(bn,ir),e(z,lr),b(bt,z,null),Hn=!0},p(t,[p]){const wt={};p&2&&(wt.$$scope={dirty:p,ctx:t}),ge.$set(wt);const wn={};p&2&&(wn.$$scope={dirty:p,ctx:t}),ve.$set(wn);const yn={};p&2&&(yn.$$scope={dirty:p,ctx:t}),we.$set(yn);const Sn={};p&2&&(Sn.$$scope={dirty:p,ctx:t}),Se.$set(Sn)},i(t){Hn||(w(f.$$.fragment,t),w(Ue.$$.fragment,t),w(qe.$$.fragment,t),w(Fe.$$.fragment,t),w(Ee.$$.fragment,t),w(Me.$$.fragment,t),w(ze.$$.fragment,t),w(Ae.$$.fragment,t),w(Le.$$.fragment,t),w(Oe.$$.fragment,t),w(Be.$$.fragment,t),w(ge.$$.fragment,t),w(Ye.$$.fragment,t),w(Re.$$.fragment,t),w(Qe.$$.fragment,t),w(et.$$.fragment,t),w(ve.$$.fragment,t),w(tt.$$.fragment,t),w(nt.$$.fragment,t),w(ot.$$.fragment,t),w(ct.$$.fragment,t),w(we.$$.fragment,t),w(dt.$$.fragment,t),w(pt.$$.fragment,t),w(ht.$$.fragment,t),w(vt.$$.fragment,t),w(Se.$$.fragment,t),w(bt.$$.fragment,t),Hn=!0)},o(t){y(f.$$.fragment,t),y(Ue.$$.fragment,t),y(qe.$$.fragment,t),y(Fe.$$.fragment,t),y(Ee.$$.fragment,t),y(Me.$$.fragment,t),y(ze.$$.fragment,t),y(Ae.$$.fragment,t),y(Le.$$.fragment,t),y(Oe.$$.fragment,t),y(Be.$$.fragment,t),y(ge.$$.fragment,t),y(Ye.$$.fragment,t),y(Re.$$.fragment,t),y(Qe.$$.fragment,t),y(et.$$.fragment,t),y(ve.$$.fragment,t),y(tt.$$.fragment,t),y(nt.$$.fragment,t),y(ot.$$.fragment,t),y(ct.$$.fragment,t),y(we.$$.fragment,t),y(dt.$$.fragment,t),y(pt.$$.fragment,t),y(ht.$$.fragment,t),y(vt.$$.fragment,t),y(Se.$$.fragment,t),y(bt.$$.fragment,t),Hn=!1},d(t){n(h),t&&n(U),t&&n(u),S(f),t&&n(Un),t&&n(R),S(Ue),t&&n(kn),t&&n(pe),t&&n($n),t&&n(yt),t&&n(jn),t&&n(St),t&&n(Cn),t&&n(Tt),t&&n(xn),t&&n(he),t&&n(qn),t&&n(L),t&&n(Fn),t&&n(Q),S(qe),t&&n(Pn),t&&n(C),S(Fe),S(Ee),t&&n(En),t&&n(J),S(Me),t&&n(Mn),t&&n(G),S(ze),t&&n(zn),t&&n(ee),S(Ae),t&&n(Dn),t&&n(te),S(Le),t&&n(An),t&&n(x),S(Oe),S(Be),S(ge),S(Ye),t&&n(Wn),t&&n(oe),S(Re),t&&n(Ln),t&&n(q),S(Qe),S(et),S(ve),S(tt),t&&n(On),t&&n(re),S(nt),t&&n(Nn),t&&n(j),S(ot),S(ct),S(we),S(dt),t&&n(Vn),t&&n(le),S(pt),t&&n(In),t&&n(F),S(ht),S(vt),S(Se),S(bt)}}}const ji={local:"unispeech",sections:[{local:"overview",title:"Overview"},{local:"transformers.UniSpeechConfig",title:"UniSpeechConfig"},{local:"transformers.models.unispeech.modeling_unispeech.UniSpeechBaseModelOutput",title:"UniSpeech specific outputs"},{local:"transformers.UniSpeechModel",title:"UniSpeechModel"},{local:"transformers.UniSpeechForCTC",title:"UniSpeechForCTC"},{local:"transformers.UniSpeechForSequenceClassification",title:"UniSpeechForSequenceClassification"},{local:"transformers.UniSpeechForPreTraining",title:"UniSpeechForPreTraining"}],title:"UniSpeech"};function Ci(A,h,U){let{fw:u}=h;return A.$$set=T=>{"fw"in T&&U(0,u=T.fw)},[u]}class zi extends vi{constructor(h){super();bi(this,h,Ci,$i,wi,{fw:0})}}export{zi as default,ji as metadata};
