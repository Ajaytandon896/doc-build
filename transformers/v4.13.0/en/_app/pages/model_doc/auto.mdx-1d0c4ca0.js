import{S as PKr,i as SKr,s as $Kr,e as a,k as l,w as m,t as o,L as IKr,c as s,d as t,m as i,a as n,x as f,h as r,b as c,J as e,g as b,y as h,q as g,o as u,B as p}from"../../chunks/vendor-e859c359.js";import{T as kpr}from"../../chunks/Tip-edc75249.js";import{D as y}from"../../chunks/Docstring-ade913b3.js";import{C as w}from"../../chunks/CodeBlock-ce4317c2.js";import{I as Z}from"../../chunks/IconCopyLink-5fae3b20.js";import"../../chunks/CopyButton-77addb3d.js";function DKr(ql){let re,Pe,fe,ue,ro,ge,Ee,$o,zl,Lc,Ot,Xl,Ql,pC,Bc,Be,ao,Vl,hs,_C,gs,us,bC,Wl,ps,vC,Hl,xc,_a;return{c(){re=a("p"),Pe=o("If your "),fe=a("code"),ue=o("NewModelConfig"),ro=o(" is a subclass of "),ge=a("code"),Ee=o("PretrainedConfig"),$o=o(`, make sure its
`),zl=a("code"),Lc=o("model_type"),Ot=o(" attribute is set to the same key you use when registering the config (here "),Xl=a("code"),Ql=o('"new-model"'),pC=o(")."),Bc=l(),Be=a("p"),ao=o("Likewise, if your "),Vl=a("code"),hs=o("NewModel"),_C=o(" is a subclass of "),gs=a("a"),us=o("PreTrainedModel"),bC=o(`, make sure its
`),Wl=a("code"),ps=o("config_class"),vC=o(` attribute is set to the same class you use when registering the model (here
`),Hl=a("code"),xc=o("NewModelConfig"),_a=o(")."),this.h()},l(so){re=s(so,"P",{});var pe=n(re);Pe=r(pe,"If your "),fe=s(pe,"CODE",{});var T0=n(fe);ue=r(T0,"NewModelConfig"),T0.forEach(t),ro=r(pe," is a subclass of "),ge=s(pe,"CODE",{});var Ul=n(ge);Ee=r(Ul,"PretrainedConfig"),Ul.forEach(t),$o=r(pe,`, make sure its
`),zl=s(pe,"CODE",{});var F0=n(zl);Lc=r(F0,"model_type"),F0.forEach(t),Ot=r(pe," attribute is set to the same key you use when registering the config (here "),Xl=s(pe,"CODE",{});var E0=n(Xl);Ql=r(E0,'"new-model"'),E0.forEach(t),pC=r(pe,")."),pe.forEach(t),Bc=i(so),Be=s(so,"P",{});var Io=n(Be);ao=r(Io,"Likewise, if your "),Vl=s(Io,"CODE",{});var ba=n(Vl);hs=r(ba,"NewModel"),ba.forEach(t),_C=r(Io," is a subclass of "),gs=s(Io,"A",{href:!0});var C0=n(gs);us=r(C0,"PreTrainedModel"),C0.forEach(t),bC=r(Io,`, make sure its
`),Wl=s(Io,"CODE",{});var kc=n(Wl);ps=r(kc,"config_class"),kc.forEach(t),vC=r(Io,` attribute is set to the same class you use when registering the model (here
`),Hl=s(Io,"CODE",{});var M0=n(Hl);xc=r(M0,"NewModelConfig"),M0.forEach(t),_a=r(Io,")."),Io.forEach(t),this.h()},h(){c(gs,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel")},m(so,pe){b(so,re,pe),e(re,Pe),e(re,fe),e(fe,ue),e(re,ro),e(re,ge),e(ge,Ee),e(re,$o),e(re,zl),e(zl,Lc),e(re,Ot),e(re,Xl),e(Xl,Ql),e(re,pC),b(so,Bc,pe),b(so,Be,pe),e(Be,ao),e(Be,Vl),e(Vl,hs),e(Be,_C),e(Be,gs),e(gs,us),e(Be,bC),e(Be,Wl),e(Wl,ps),e(Be,vC),e(Be,Hl),e(Hl,xc),e(Be,_a)},d(so){so&&t(re),so&&t(Bc),so&&t(Be)}}}function NKr(ql){let re,Pe,fe,ue,ro;return{c(){re=a("p"),Pe=o("Passing "),fe=a("code"),ue=o("use_auth_token=True"),ro=o(" is required when you want to use a private model.")},l(ge){re=s(ge,"P",{});var Ee=n(re);Pe=r(Ee,"Passing "),fe=s(Ee,"CODE",{});var $o=n(fe);ue=r($o,"use_auth_token=True"),$o.forEach(t),ro=r(Ee," is required when you want to use a private model."),Ee.forEach(t)},m(ge,Ee){b(ge,re,Ee),e(re,Pe),e(re,fe),e(fe,ue),e(re,ro)},d(ge){ge&&t(re)}}}function jKr(ql){let re,Pe,fe,ue,ro;return{c(){re=a("p"),Pe=o("Passing "),fe=a("code"),ue=o("use_auth_token=True"),ro=o(" is required when you want to use a private model.")},l(ge){re=s(ge,"P",{});var Ee=n(re);Pe=r(Ee,"Passing "),fe=s(Ee,"CODE",{});var $o=n(fe);ue=r($o,"use_auth_token=True"),$o.forEach(t),ro=r(Ee," is required when you want to use a private model."),Ee.forEach(t)},m(ge,Ee){b(ge,re,Ee),e(re,Pe),e(re,fe),e(fe,ue),e(re,ro)},d(ge){ge&&t(re)}}}function OKr(ql){let re,Pe,fe,ue,ro,ge,Ee,$o,zl,Lc,Ot,Xl,Ql,pC,Bc,Be,ao,Vl,hs,_C,gs,us,bC,Wl,ps,vC,Hl,xc,_a,so,pe,T0,Ul,F0,E0,Io,ba,C0,kc,M0,e0e,a5e,Jl,Rc,BO,TC,o0e,xO,r0e,s5e,_s,t0e,kO,a0e,s0e,RO,n0e,l0e,n5e,FC,l5e,y0,i0e,i5e,Pc,d5e,Kl,Sc,PO,EC,d0e,SO,c0e,c5e,Do,CC,m0e,MC,f0e,w0,h0e,g0e,u0e,yC,p0e,$O,_0e,b0e,v0e,no,wC,T0e,IO,F0e,E0e,Yl,C0e,DO,M0e,y0e,NO,w0e,A0e,L0e,v,$c,jO,B0e,x0e,A0,k0e,R0e,P0e,Ic,OO,S0e,$0e,L0,I0e,D0e,N0e,Dc,GO,j0e,O0e,B0,G0e,q0e,z0e,Nc,qO,X0e,Q0e,x0,V0e,W0e,H0e,jc,zO,U0e,J0e,k0,K0e,Y0e,Z0e,Oc,XO,eAe,oAe,R0,rAe,tAe,aAe,Gc,QO,sAe,nAe,P0,lAe,iAe,dAe,qc,VO,cAe,mAe,S0,fAe,hAe,gAe,zc,WO,uAe,pAe,$0,_Ae,bAe,vAe,Xc,HO,TAe,FAe,I0,EAe,CAe,MAe,Qc,UO,yAe,wAe,D0,AAe,LAe,BAe,Vc,JO,xAe,kAe,N0,RAe,PAe,SAe,Wc,KO,$Ae,IAe,j0,DAe,NAe,jAe,Hc,YO,OAe,GAe,O0,qAe,zAe,XAe,Uc,ZO,QAe,VAe,G0,WAe,HAe,UAe,Jc,eG,JAe,KAe,q0,YAe,ZAe,e6e,Kc,oG,o6e,r6e,z0,t6e,a6e,s6e,Yc,rG,n6e,l6e,X0,i6e,d6e,c6e,Zc,tG,m6e,f6e,Q0,h6e,g6e,u6e,em,aG,p6e,_6e,V0,b6e,v6e,T6e,om,sG,F6e,E6e,W0,C6e,M6e,y6e,rm,nG,w6e,A6e,H0,L6e,B6e,x6e,tm,lG,k6e,R6e,U0,P6e,S6e,$6e,am,iG,I6e,D6e,J0,N6e,j6e,O6e,sm,dG,G6e,q6e,K0,z6e,X6e,Q6e,nm,cG,V6e,W6e,Y0,H6e,U6e,J6e,lm,mG,K6e,Y6e,Z0,Z6e,eLe,oLe,im,fG,rLe,tLe,eA,aLe,sLe,nLe,dm,hG,lLe,iLe,oA,dLe,cLe,mLe,cm,gG,fLe,hLe,rA,gLe,uLe,pLe,mm,uG,_Le,bLe,tA,vLe,TLe,FLe,fm,pG,ELe,CLe,aA,MLe,yLe,wLe,hm,_G,ALe,LLe,sA,BLe,xLe,kLe,gm,bG,RLe,PLe,nA,SLe,$Le,ILe,um,vG,DLe,NLe,lA,jLe,OLe,GLe,pm,TG,qLe,zLe,iA,XLe,QLe,VLe,_m,FG,WLe,HLe,dA,ULe,JLe,KLe,bm,EG,YLe,ZLe,cA,e8e,o8e,r8e,vm,CG,t8e,a8e,mA,s8e,n8e,l8e,Tm,MG,i8e,d8e,fA,c8e,m8e,f8e,Fm,yG,h8e,g8e,hA,u8e,p8e,_8e,Em,wG,b8e,v8e,gA,T8e,F8e,E8e,Cm,AG,C8e,M8e,uA,y8e,w8e,A8e,Mm,LG,L8e,B8e,pA,x8e,k8e,R8e,ym,BG,P8e,S8e,_A,$8e,I8e,D8e,wm,xG,N8e,j8e,bA,O8e,G8e,q8e,Am,kG,z8e,X8e,vA,Q8e,V8e,W8e,Lm,RG,H8e,U8e,TA,J8e,K8e,Y8e,Bm,PG,Z8e,eBe,FA,oBe,rBe,tBe,xm,SG,aBe,sBe,EA,nBe,lBe,iBe,km,$G,dBe,cBe,CA,mBe,fBe,hBe,Rm,IG,gBe,uBe,MA,pBe,_Be,bBe,Pm,DG,vBe,TBe,yA,FBe,EBe,CBe,Sm,NG,MBe,yBe,wA,wBe,ABe,LBe,$m,jG,BBe,xBe,AA,kBe,RBe,PBe,Im,OG,SBe,$Be,LA,IBe,DBe,NBe,Dm,GG,jBe,OBe,BA,GBe,qBe,zBe,Nm,qG,XBe,QBe,xA,VBe,WBe,HBe,jm,zG,UBe,JBe,kA,KBe,YBe,ZBe,Om,XG,e9e,o9e,RA,r9e,t9e,a9e,Gm,QG,s9e,n9e,PA,l9e,i9e,d9e,qm,VG,c9e,m9e,SA,f9e,h9e,g9e,zm,WG,u9e,p9e,$A,_9e,b9e,v9e,Xm,HG,T9e,F9e,IA,E9e,C9e,M9e,Qm,UG,y9e,w9e,DA,A9e,L9e,B9e,Vm,JG,x9e,k9e,NA,R9e,P9e,S9e,Wm,KG,$9e,I9e,jA,D9e,N9e,j9e,Hm,YG,O9e,G9e,OA,q9e,z9e,X9e,Um,ZG,Q9e,V9e,GA,W9e,H9e,U9e,Jm,eq,J9e,K9e,qA,Y9e,Z9e,exe,Km,oq,oxe,rxe,zA,txe,axe,sxe,Ym,rq,nxe,lxe,XA,ixe,dxe,cxe,Zm,tq,mxe,fxe,QA,hxe,gxe,uxe,ef,aq,pxe,_xe,VA,bxe,vxe,Txe,of,sq,Fxe,Exe,WA,Cxe,Mxe,yxe,rf,nq,wxe,Axe,HA,Lxe,Bxe,xxe,tf,lq,kxe,Rxe,UA,Pxe,Sxe,$xe,af,iq,Ixe,Dxe,JA,Nxe,jxe,Oxe,sf,dq,Gxe,qxe,KA,zxe,Xxe,Qxe,cq,Vxe,Wxe,AC,Hxe,nf,LC,Uxe,mq,Jxe,m5e,Zl,lf,fq,BC,Kxe,hq,Yxe,f5e,No,xC,Zxe,kC,eke,YA,oke,rke,tke,RC,ake,gq,ske,nke,lke,ye,PC,ike,uq,dke,cke,va,mke,pq,fke,hke,_q,gke,uke,bq,pke,_ke,bke,C,bs,vq,vke,Tke,ZA,Fke,Eke,e6,Cke,Mke,yke,vs,Tq,wke,Ake,o6,Lke,Bke,r6,xke,kke,Rke,Ts,Fq,Pke,Ske,t6,$ke,Ike,a6,Dke,Nke,jke,df,Eq,Oke,Gke,s6,qke,zke,Xke,Fs,Cq,Qke,Vke,n6,Wke,Hke,l6,Uke,Jke,Kke,cf,Mq,Yke,Zke,i6,eRe,oRe,rRe,mf,yq,tRe,aRe,d6,sRe,nRe,lRe,ff,wq,iRe,dRe,c6,cRe,mRe,fRe,Es,Aq,hRe,gRe,m6,uRe,pRe,f6,_Re,bRe,vRe,Cs,Lq,TRe,FRe,h6,ERe,CRe,g6,MRe,yRe,wRe,Ms,Bq,ARe,LRe,u6,BRe,xRe,p6,kRe,RRe,PRe,hf,xq,SRe,$Re,_6,IRe,DRe,NRe,gf,kq,jRe,ORe,b6,GRe,qRe,zRe,ys,Rq,XRe,QRe,v6,VRe,WRe,T6,HRe,URe,JRe,uf,Pq,KRe,YRe,F6,ZRe,ePe,oPe,ws,Sq,rPe,tPe,E6,aPe,sPe,C6,nPe,lPe,iPe,As,$q,dPe,cPe,M6,mPe,fPe,y6,hPe,gPe,uPe,Ls,Iq,pPe,_Pe,w6,bPe,vPe,Dq,TPe,FPe,EPe,pf,Nq,CPe,MPe,A6,yPe,wPe,APe,Bs,jq,LPe,BPe,L6,xPe,kPe,B6,RPe,PPe,SPe,_f,Oq,$Pe,IPe,x6,DPe,NPe,jPe,xs,Gq,OPe,GPe,k6,qPe,zPe,R6,XPe,QPe,VPe,ks,qq,WPe,HPe,P6,UPe,JPe,S6,KPe,YPe,ZPe,Rs,zq,eSe,oSe,$6,rSe,tSe,I6,aSe,sSe,nSe,bf,Xq,lSe,iSe,D6,dSe,cSe,mSe,Ps,Qq,fSe,hSe,N6,gSe,uSe,j6,pSe,_Se,bSe,vf,Vq,vSe,TSe,O6,FSe,ESe,CSe,Ss,Wq,MSe,ySe,G6,wSe,ASe,q6,LSe,BSe,xSe,$s,Hq,kSe,RSe,z6,PSe,SSe,X6,$Se,ISe,DSe,Is,Uq,NSe,jSe,Q6,OSe,GSe,V6,qSe,zSe,XSe,Tf,Jq,QSe,VSe,W6,WSe,HSe,USe,Ds,Kq,JSe,KSe,H6,YSe,ZSe,U6,e$e,o$e,r$e,Ns,Yq,t$e,a$e,J6,s$e,n$e,K6,l$e,i$e,d$e,js,Zq,c$e,m$e,Y6,f$e,h$e,Z6,g$e,u$e,p$e,Os,ez,_$e,b$e,eL,v$e,T$e,oL,F$e,E$e,C$e,Gs,oz,M$e,y$e,rL,w$e,A$e,tL,L$e,B$e,x$e,Ff,rz,k$e,R$e,aL,P$e,S$e,$$e,qs,tz,I$e,D$e,sL,N$e,j$e,nL,O$e,G$e,q$e,Ef,az,z$e,X$e,lL,Q$e,V$e,W$e,Cf,sz,H$e,U$e,iL,J$e,K$e,Y$e,zs,nz,Z$e,eIe,dL,oIe,rIe,cL,tIe,aIe,sIe,Xs,lz,nIe,lIe,mL,iIe,dIe,fL,cIe,mIe,fIe,Qs,iz,hIe,gIe,hL,uIe,pIe,gL,_Ie,bIe,vIe,Vs,dz,TIe,FIe,uL,EIe,CIe,pL,MIe,yIe,wIe,Ws,cz,AIe,LIe,_L,BIe,xIe,bL,kIe,RIe,PIe,Hs,mz,SIe,$Ie,vL,IIe,DIe,TL,NIe,jIe,OIe,Us,fz,GIe,qIe,FL,zIe,XIe,EL,QIe,VIe,WIe,Mf,hz,HIe,UIe,CL,JIe,KIe,YIe,yf,gz,ZIe,eDe,ML,oDe,rDe,tDe,Js,uz,aDe,sDe,yL,nDe,lDe,wL,iDe,dDe,cDe,wf,pz,mDe,fDe,AL,hDe,gDe,uDe,Ks,_z,pDe,_De,LL,bDe,vDe,BL,TDe,FDe,EDe,Ys,bz,CDe,MDe,xL,yDe,wDe,kL,ADe,LDe,BDe,Zs,vz,xDe,kDe,RL,RDe,PDe,PL,SDe,$De,IDe,en,Tz,DDe,NDe,SL,jDe,ODe,$L,GDe,qDe,zDe,on,Fz,XDe,QDe,IL,VDe,WDe,DL,HDe,UDe,JDe,Af,Ez,KDe,YDe,NL,ZDe,eNe,oNe,Lf,Cz,rNe,tNe,jL,aNe,sNe,nNe,rn,Mz,lNe,iNe,OL,dNe,cNe,GL,mNe,fNe,hNe,tn,yz,gNe,uNe,qL,pNe,_Ne,zL,bNe,vNe,TNe,an,wz,FNe,ENe,XL,CNe,MNe,QL,yNe,wNe,ANe,Bf,Az,LNe,BNe,VL,xNe,kNe,RNe,xf,Lz,PNe,SNe,WL,$Ne,INe,DNe,kf,Bz,NNe,jNe,HL,ONe,GNe,qNe,Rf,xz,zNe,XNe,UL,QNe,VNe,WNe,Pf,kz,HNe,UNe,JL,JNe,KNe,YNe,sn,Rz,ZNe,eje,KL,oje,rje,YL,tje,aje,sje,nn,Pz,nje,lje,ZL,ije,dje,e8,cje,mje,fje,ei,hje,Sz,gje,uje,$z,pje,_je,bje,oi,Ta,vje,Iz,Tje,Fje,Dz,Eje,Cje,Nz,Mje,yje,wje,Fa,Aje,jz,Lje,Bje,o8,xje,kje,Oz,Rje,Pje,Sje,k,$je,Gz,Ije,Dje,qz,Nje,jje,zz,Oje,Gje,r8,qje,zje,Xz,Xje,Qje,ri,Vje,Qz,Wje,Hje,Vz,Uje,Jje,Kje,SC,Yje,Wz,Zje,eOe,oOe,Hz,rOe,tOe,$C,aOe,Uz,sOe,nOe,lOe,Jz,iOe,dOe,Kz,cOe,mOe,Yz,fOe,hOe,Zz,gOe,uOe,eX,pOe,_Oe,oX,bOe,vOe,rX,TOe,FOe,tX,EOe,COe,aX,MOe,yOe,sX,wOe,AOe,IC,LOe,nX,BOe,xOe,kOe,lX,ROe,POe,DC,SOe,iX,$Oe,IOe,DOe,NC,NOe,dX,jOe,OOe,GOe,cX,qOe,zOe,mX,XOe,QOe,fX,VOe,WOe,hX,HOe,UOe,gX,JOe,KOe,uX,YOe,ZOe,pX,eGe,oGe,_X,rGe,tGe,bX,aGe,sGe,vX,nGe,lGe,TX,iGe,dGe,FX,cGe,mGe,EX,fGe,hGe,gGe,CX,uGe,pGe,jC,_Ge,Sf,OC,bGe,MX,vGe,h5e,ti,$f,yX,GC,TGe,wX,FGe,g5e,Dt,qC,EGe,zC,CGe,t8,MGe,yGe,wGe,XC,AGe,AX,LGe,BGe,xGe,Te,QC,kGe,LX,RGe,PGe,Ea,SGe,BX,$Ge,IGe,xX,DGe,NGe,kX,jGe,OGe,GGe,Ce,If,RX,qGe,zGe,a8,XGe,QGe,VGe,Df,PX,WGe,HGe,s8,UGe,JGe,KGe,Nf,SX,YGe,ZGe,n8,eqe,oqe,rqe,jf,$X,tqe,aqe,l8,sqe,nqe,lqe,Of,IX,iqe,dqe,i8,cqe,mqe,fqe,Gf,DX,hqe,gqe,d8,uqe,pqe,_qe,qf,NX,bqe,vqe,c8,Tqe,Fqe,Eqe,zf,jX,Cqe,Mqe,m8,yqe,wqe,Aqe,Xf,OX,Lqe,Bqe,f8,xqe,kqe,Rqe,ai,Pqe,GX,Sqe,$qe,qX,Iqe,Dqe,Nqe,si,Ca,jqe,zX,Oqe,Gqe,XX,qqe,zqe,QX,Xqe,Qqe,Vqe,Ma,Wqe,VX,Hqe,Uqe,h8,Jqe,Kqe,WX,Yqe,Zqe,eze,N,oze,HX,rze,tze,UX,aze,sze,ni,nze,JX,lze,ize,KX,dze,cze,mze,VC,fze,YX,hze,gze,uze,ZX,pze,_ze,WC,bze,eQ,vze,Tze,Fze,oQ,Eze,Cze,rQ,Mze,yze,tQ,wze,Aze,aQ,Lze,Bze,HC,xze,sQ,kze,Rze,Pze,nQ,Sze,$ze,lQ,Ize,Dze,iQ,Nze,jze,dQ,Oze,Gze,cQ,qze,zze,mQ,Xze,Qze,fQ,Vze,Wze,hQ,Hze,Uze,UC,Jze,gQ,Kze,Yze,Zze,uQ,eXe,oXe,pQ,rXe,tXe,_Q,aXe,sXe,bQ,nXe,lXe,vQ,iXe,dXe,TQ,cXe,mXe,FQ,fXe,hXe,EQ,gXe,uXe,CQ,pXe,_Xe,MQ,bXe,vXe,yQ,TXe,FXe,EXe,Qf,CXe,wQ,MXe,yXe,JC,u5e,li,Vf,AQ,KC,wXe,LQ,AXe,p5e,Nt,YC,LXe,ZC,BXe,g8,xXe,kXe,RXe,e3,PXe,BQ,SXe,$Xe,IXe,Fe,o3,DXe,xQ,NXe,jXe,ii,OXe,kQ,GXe,qXe,RQ,zXe,XXe,QXe,to,Wf,PQ,VXe,WXe,u8,HXe,UXe,JXe,Hf,SQ,KXe,YXe,p8,ZXe,eQe,oQe,Uf,$Q,rQe,tQe,_8,aQe,sQe,nQe,Jf,IQ,lQe,iQe,b8,dQe,cQe,mQe,Kf,DQ,fQe,hQe,v8,gQe,uQe,pQe,Yf,NQ,_Qe,bQe,T8,vQe,TQe,FQe,Zf,jQ,EQe,CQe,F8,MQe,yQe,wQe,di,AQe,OQ,LQe,BQe,GQ,xQe,kQe,RQe,r3,ya,PQe,qQ,SQe,$Qe,zQ,IQe,DQe,XQ,NQe,jQe,OQe,D,GQe,QQ,qQe,zQe,VQ,XQe,QQe,WQ,VQe,WQe,ci,HQe,HQ,UQe,JQe,UQ,KQe,YQe,ZQe,t3,eVe,JQ,oVe,rVe,tVe,KQ,aVe,sVe,a3,nVe,YQ,lVe,iVe,dVe,ZQ,cVe,mVe,eV,fVe,hVe,oV,gVe,uVe,rV,pVe,_Ve,s3,bVe,tV,vVe,TVe,FVe,aV,EVe,CVe,sV,MVe,yVe,nV,wVe,AVe,lV,LVe,BVe,iV,xVe,kVe,dV,RVe,PVe,cV,SVe,$Ve,mV,IVe,DVe,n3,NVe,fV,jVe,OVe,GVe,hV,qVe,zVe,gV,XVe,QVe,uV,VVe,WVe,pV,HVe,UVe,_V,JVe,KVe,bV,YVe,ZVe,vV,eWe,oWe,TV,rWe,tWe,FV,aWe,sWe,EV,nWe,lWe,CV,iWe,dWe,cWe,eh,mWe,MV,fWe,hWe,l3,_5e,mi,oh,yV,i3,gWe,wV,uWe,b5e,jo,d3,pWe,fi,_We,AV,bWe,vWe,LV,TWe,FWe,EWe,c3,CWe,BV,MWe,yWe,wWe,kr,m3,AWe,xV,LWe,BWe,hi,xWe,kV,kWe,RWe,RV,PWe,SWe,$We,PV,IWe,DWe,f3,NWe,Se,h3,jWe,SV,OWe,GWe,wa,qWe,$V,zWe,XWe,IV,QWe,VWe,DV,WWe,HWe,UWe,F,rh,NV,JWe,KWe,E8,YWe,ZWe,eHe,th,jV,oHe,rHe,C8,tHe,aHe,sHe,ah,OV,nHe,lHe,M8,iHe,dHe,cHe,sh,GV,mHe,fHe,y8,hHe,gHe,uHe,nh,qV,pHe,_He,w8,bHe,vHe,THe,lh,zV,FHe,EHe,A8,CHe,MHe,yHe,ih,XV,wHe,AHe,L8,LHe,BHe,xHe,dh,QV,kHe,RHe,B8,PHe,SHe,$He,ch,VV,IHe,DHe,x8,NHe,jHe,OHe,mh,WV,GHe,qHe,k8,zHe,XHe,QHe,fh,HV,VHe,WHe,R8,HHe,UHe,JHe,hh,UV,KHe,YHe,P8,ZHe,eUe,oUe,gh,JV,rUe,tUe,S8,aUe,sUe,nUe,uh,KV,lUe,iUe,$8,dUe,cUe,mUe,ph,YV,fUe,hUe,I8,gUe,uUe,pUe,_h,ZV,_Ue,bUe,D8,vUe,TUe,FUe,bh,eW,EUe,CUe,N8,MUe,yUe,wUe,vh,oW,AUe,LUe,j8,BUe,xUe,kUe,Th,rW,RUe,PUe,O8,SUe,$Ue,IUe,Fh,tW,DUe,NUe,G8,jUe,OUe,GUe,Eh,aW,qUe,zUe,q8,XUe,QUe,VUe,Ch,sW,WUe,HUe,z8,UUe,JUe,KUe,Mh,nW,YUe,ZUe,X8,eJe,oJe,rJe,yh,lW,tJe,aJe,Q8,sJe,nJe,lJe,ln,iW,iJe,dJe,V8,cJe,mJe,W8,fJe,hJe,gJe,wh,dW,uJe,pJe,H8,_Je,bJe,vJe,Ah,cW,TJe,FJe,U8,EJe,CJe,MJe,Lh,mW,yJe,wJe,J8,AJe,LJe,BJe,Bh,fW,xJe,kJe,K8,RJe,PJe,SJe,xh,hW,$Je,IJe,Y8,DJe,NJe,jJe,kh,gW,OJe,GJe,Z8,qJe,zJe,XJe,Rh,uW,QJe,VJe,eB,WJe,HJe,UJe,Ph,pW,JJe,KJe,oB,YJe,ZJe,eKe,Sh,_W,oKe,rKe,rB,tKe,aKe,sKe,$h,bW,nKe,lKe,tB,iKe,dKe,cKe,Ih,vW,mKe,fKe,aB,hKe,gKe,uKe,Dh,TW,pKe,_Ke,sB,bKe,vKe,TKe,Nh,FW,FKe,EKe,nB,CKe,MKe,yKe,jh,EW,wKe,AKe,lB,LKe,BKe,xKe,Oh,CW,kKe,RKe,iB,PKe,SKe,$Ke,Gh,MW,IKe,DKe,dB,NKe,jKe,OKe,qh,yW,GKe,qKe,cB,zKe,XKe,QKe,zh,wW,VKe,WKe,mB,HKe,UKe,JKe,Xh,AW,KKe,YKe,fB,ZKe,eYe,oYe,Qh,LW,rYe,tYe,hB,aYe,sYe,nYe,Vh,BW,lYe,iYe,gB,dYe,cYe,mYe,Wh,xW,fYe,hYe,uB,gYe,uYe,pYe,Hh,kW,_Ye,bYe,pB,vYe,TYe,FYe,Uh,RW,EYe,CYe,_B,MYe,yYe,wYe,Jh,PW,AYe,LYe,bB,BYe,xYe,kYe,Kh,SW,RYe,PYe,vB,SYe,$Ye,IYe,Yh,$W,DYe,NYe,TB,jYe,OYe,GYe,Zh,IW,qYe,zYe,FB,XYe,QYe,VYe,eg,DW,WYe,HYe,EB,UYe,JYe,KYe,og,NW,YYe,ZYe,CB,eZe,oZe,rZe,rg,jW,tZe,aZe,MB,sZe,nZe,lZe,tg,OW,iZe,dZe,yB,cZe,mZe,fZe,ag,GW,hZe,gZe,wB,uZe,pZe,_Ze,sg,qW,bZe,vZe,AB,TZe,FZe,EZe,ng,zW,CZe,MZe,LB,yZe,wZe,AZe,lg,XW,LZe,BZe,BB,xZe,kZe,RZe,ig,QW,PZe,SZe,xB,$Ze,IZe,DZe,dg,VW,NZe,jZe,kB,OZe,GZe,qZe,cg,WW,zZe,XZe,RB,QZe,VZe,WZe,mg,HW,HZe,UZe,PB,JZe,KZe,YZe,fg,UW,ZZe,eeo,SB,oeo,reo,teo,hg,JW,aeo,seo,$B,neo,leo,ieo,gg,KW,deo,ceo,IB,meo,feo,heo,ug,YW,geo,ueo,DB,peo,_eo,beo,pg,ZW,veo,Teo,NB,Feo,Eeo,Ceo,_g,eH,Meo,yeo,jB,weo,Aeo,Leo,bg,oH,Beo,xeo,OB,keo,Reo,Peo,vg,rH,Seo,$eo,GB,Ieo,Deo,Neo,Tg,jeo,tH,Oeo,Geo,aH,qeo,zeo,sH,Xeo,Qeo,g3,v5e,gi,Fg,nH,u3,Veo,lH,Weo,T5e,Oo,p3,Heo,ui,Ueo,iH,Jeo,Keo,dH,Yeo,Zeo,eoo,_3,ooo,cH,roo,too,aoo,Rr,b3,soo,mH,noo,loo,pi,ioo,fH,doo,coo,hH,moo,foo,hoo,gH,goo,uoo,v3,poo,$e,T3,_oo,uH,boo,voo,Aa,Too,pH,Foo,Eoo,_H,Coo,Moo,bH,yoo,woo,Aoo,R,Eg,vH,Loo,Boo,qB,xoo,koo,Roo,Cg,TH,Poo,Soo,zB,$oo,Ioo,Doo,Mg,FH,Noo,joo,XB,Ooo,Goo,qoo,yg,EH,zoo,Xoo,QB,Qoo,Voo,Woo,wg,CH,Hoo,Uoo,VB,Joo,Koo,Yoo,Ag,MH,Zoo,ero,WB,oro,rro,tro,Lg,yH,aro,sro,HB,nro,lro,iro,Bg,wH,dro,cro,UB,mro,fro,hro,xg,AH,gro,uro,JB,pro,_ro,bro,kg,LH,vro,Tro,KB,Fro,Ero,Cro,Rg,BH,Mro,yro,YB,wro,Aro,Lro,Pg,xH,Bro,xro,ZB,kro,Rro,Pro,Sg,kH,Sro,$ro,e9,Iro,Dro,Nro,$g,RH,jro,Oro,o9,Gro,qro,zro,Ig,PH,Xro,Qro,r9,Vro,Wro,Hro,Dg,SH,Uro,Jro,t9,Kro,Yro,Zro,Ng,$H,eto,oto,a9,rto,tto,ato,jg,IH,sto,nto,s9,lto,ito,dto,Og,DH,cto,mto,n9,fto,hto,gto,Gg,NH,uto,pto,l9,_to,bto,vto,qg,jH,Tto,Fto,i9,Eto,Cto,Mto,zg,OH,yto,wto,d9,Ato,Lto,Bto,Xg,GH,xto,kto,c9,Rto,Pto,Sto,Qg,qH,$to,Ito,m9,Dto,Nto,jto,Vg,zH,Oto,Gto,f9,qto,zto,Xto,Wg,XH,Qto,Vto,h9,Wto,Hto,Uto,Hg,QH,Jto,Kto,g9,Yto,Zto,eao,Ug,VH,oao,rao,u9,tao,aao,sao,Jg,WH,nao,lao,p9,iao,dao,cao,Kg,HH,mao,fao,_9,hao,gao,uao,Yg,UH,pao,_ao,b9,bao,vao,Tao,Zg,JH,Fao,Eao,v9,Cao,Mao,yao,eu,KH,wao,Aao,T9,Lao,Bao,xao,ou,YH,kao,Rao,F9,Pao,Sao,$ao,ru,ZH,Iao,Dao,E9,Nao,jao,Oao,tu,eU,Gao,qao,C9,zao,Xao,Qao,au,Vao,oU,Wao,Hao,rU,Uao,Jao,tU,Kao,Yao,F3,F5e,_i,su,aU,E3,Zao,sU,eso,E5e,Go,C3,oso,bi,rso,nU,tso,aso,lU,sso,nso,lso,M3,iso,iU,dso,cso,mso,Pr,y3,fso,dU,hso,gso,vi,uso,cU,pso,_so,mU,bso,vso,Tso,fU,Fso,Eso,w3,Cso,Ie,A3,Mso,hU,yso,wso,La,Aso,gU,Lso,Bso,uU,xso,kso,pU,Rso,Pso,Sso,q,nu,_U,$so,Iso,M9,Dso,Nso,jso,lu,bU,Oso,Gso,y9,qso,zso,Xso,iu,vU,Qso,Vso,w9,Wso,Hso,Uso,du,TU,Jso,Kso,A9,Yso,Zso,eno,cu,FU,ono,rno,L9,tno,ano,sno,mu,EU,nno,lno,B9,ino,dno,cno,fu,CU,mno,fno,x9,hno,gno,uno,hu,MU,pno,_no,k9,bno,vno,Tno,gu,yU,Fno,Eno,R9,Cno,Mno,yno,uu,wU,wno,Ano,P9,Lno,Bno,xno,pu,AU,kno,Rno,S9,Pno,Sno,$no,_u,LU,Ino,Dno,$9,Nno,jno,Ono,bu,BU,Gno,qno,I9,zno,Xno,Qno,vu,xU,Vno,Wno,D9,Hno,Uno,Jno,Tu,kU,Kno,Yno,N9,Zno,elo,olo,Fu,RU,rlo,tlo,j9,alo,slo,nlo,Eu,PU,llo,ilo,O9,dlo,clo,mlo,Cu,SU,flo,hlo,G9,glo,ulo,plo,Mu,$U,_lo,blo,q9,vlo,Tlo,Flo,yu,IU,Elo,Clo,z9,Mlo,ylo,wlo,wu,DU,Alo,Llo,X9,Blo,xlo,klo,Au,NU,Rlo,Plo,Q9,Slo,$lo,Ilo,Lu,jU,Dlo,Nlo,V9,jlo,Olo,Glo,Bu,OU,qlo,zlo,W9,Xlo,Qlo,Vlo,xu,GU,Wlo,Hlo,H9,Ulo,Jlo,Klo,ku,qU,Ylo,Zlo,U9,eio,oio,rio,Ru,zU,tio,aio,J9,sio,nio,lio,Pu,XU,iio,dio,K9,cio,mio,fio,Su,QU,hio,gio,Y9,uio,pio,_io,$u,VU,bio,vio,Z9,Tio,Fio,Eio,Iu,Cio,WU,Mio,yio,HU,wio,Aio,UU,Lio,Bio,L3,C5e,Ti,Du,JU,B3,xio,KU,kio,M5e,qo,x3,Rio,Fi,Pio,YU,Sio,$io,ZU,Iio,Dio,Nio,k3,jio,eJ,Oio,Gio,qio,Sr,R3,zio,oJ,Xio,Qio,Ei,Vio,rJ,Wio,Hio,tJ,Uio,Jio,Kio,aJ,Yio,Zio,P3,edo,De,S3,odo,sJ,rdo,tdo,Ba,ado,nJ,sdo,ndo,lJ,ldo,ido,iJ,ddo,cdo,mdo,O,Nu,dJ,fdo,hdo,ex,gdo,udo,pdo,ju,cJ,_do,bdo,ox,vdo,Tdo,Fdo,Ou,mJ,Edo,Cdo,rx,Mdo,ydo,wdo,Gu,fJ,Ado,Ldo,tx,Bdo,xdo,kdo,qu,hJ,Rdo,Pdo,ax,Sdo,$do,Ido,zu,gJ,Ddo,Ndo,sx,jdo,Odo,Gdo,Xu,uJ,qdo,zdo,nx,Xdo,Qdo,Vdo,Qu,pJ,Wdo,Hdo,lx,Udo,Jdo,Kdo,Vu,_J,Ydo,Zdo,ix,eco,oco,rco,Wu,bJ,tco,aco,dx,sco,nco,lco,Hu,vJ,ico,dco,cx,cco,mco,fco,Uu,TJ,hco,gco,mx,uco,pco,_co,Ju,FJ,bco,vco,fx,Tco,Fco,Eco,Ku,EJ,Cco,Mco,hx,yco,wco,Aco,Yu,CJ,Lco,Bco,gx,xco,kco,Rco,Zu,MJ,Pco,Sco,ux,$co,Ico,Dco,ep,yJ,Nco,jco,px,Oco,Gco,qco,op,wJ,zco,Xco,_x,Qco,Vco,Wco,rp,AJ,Hco,Uco,bx,Jco,Kco,Yco,tp,LJ,Zco,emo,vx,omo,rmo,tmo,ap,BJ,amo,smo,Tx,nmo,lmo,imo,sp,xJ,dmo,cmo,Fx,mmo,fmo,hmo,np,kJ,gmo,umo,Ex,pmo,_mo,bmo,lp,RJ,vmo,Tmo,Cx,Fmo,Emo,Cmo,ip,PJ,Mmo,ymo,Mx,wmo,Amo,Lmo,dp,SJ,Bmo,xmo,yx,kmo,Rmo,Pmo,cp,$J,Smo,$mo,wx,Imo,Dmo,Nmo,mp,IJ,jmo,Omo,Ax,Gmo,qmo,zmo,fp,DJ,Xmo,Qmo,NJ,Vmo,Wmo,Hmo,hp,jJ,Umo,Jmo,Lx,Kmo,Ymo,Zmo,gp,OJ,efo,ofo,Bx,rfo,tfo,afo,up,sfo,GJ,nfo,lfo,qJ,ifo,dfo,zJ,cfo,mfo,$3,y5e,Ci,pp,XJ,I3,ffo,QJ,hfo,w5e,zo,D3,gfo,Mi,ufo,VJ,pfo,_fo,WJ,bfo,vfo,Tfo,N3,Ffo,HJ,Efo,Cfo,Mfo,$r,j3,yfo,UJ,wfo,Afo,yi,Lfo,JJ,Bfo,xfo,KJ,kfo,Rfo,Pfo,YJ,Sfo,$fo,O3,Ifo,Ne,G3,Dfo,ZJ,Nfo,jfo,xa,Ofo,eK,Gfo,qfo,oK,zfo,Xfo,rK,Qfo,Vfo,Wfo,me,_p,tK,Hfo,Ufo,xx,Jfo,Kfo,Yfo,bp,aK,Zfo,eho,kx,oho,rho,tho,vp,sK,aho,sho,Rx,nho,lho,iho,Tp,nK,dho,cho,Px,mho,fho,hho,Fp,lK,gho,uho,Sx,pho,_ho,bho,Ep,iK,vho,Tho,$x,Fho,Eho,Cho,Cp,dK,Mho,yho,Ix,who,Aho,Lho,Mp,cK,Bho,xho,Dx,kho,Rho,Pho,yp,mK,Sho,$ho,Nx,Iho,Dho,Nho,wp,fK,jho,Oho,jx,Gho,qho,zho,Ap,hK,Xho,Qho,Ox,Vho,Who,Hho,Lp,gK,Uho,Jho,Gx,Kho,Yho,Zho,Bp,uK,ego,ogo,qx,rgo,tgo,ago,xp,pK,sgo,ngo,zx,lgo,igo,dgo,kp,_K,cgo,mgo,Xx,fgo,hgo,ggo,Rp,ugo,bK,pgo,_go,vK,bgo,vgo,TK,Tgo,Fgo,q3,A5e,wi,Pp,FK,z3,Ego,EK,Cgo,L5e,Xo,X3,Mgo,Ai,ygo,CK,wgo,Ago,MK,Lgo,Bgo,xgo,Q3,kgo,yK,Rgo,Pgo,Sgo,Ir,V3,$go,wK,Igo,Dgo,Li,Ngo,AK,jgo,Ogo,LK,Ggo,qgo,zgo,BK,Xgo,Qgo,W3,Vgo,je,H3,Wgo,xK,Hgo,Ugo,ka,Jgo,kK,Kgo,Ygo,RK,Zgo,euo,PK,ouo,ruo,tuo,A,Sp,SK,auo,suo,Qx,nuo,luo,iuo,$p,$K,duo,cuo,Vx,muo,fuo,huo,Ip,IK,guo,uuo,Wx,puo,_uo,buo,Dp,DK,vuo,Tuo,Hx,Fuo,Euo,Cuo,Np,NK,Muo,yuo,Ux,wuo,Auo,Luo,jp,jK,Buo,xuo,Jx,kuo,Ruo,Puo,Op,OK,Suo,$uo,Kx,Iuo,Duo,Nuo,Gp,GK,juo,Ouo,Yx,Guo,quo,zuo,qp,qK,Xuo,Quo,Zx,Vuo,Wuo,Huo,zp,zK,Uuo,Juo,ek,Kuo,Yuo,Zuo,Xp,XK,epo,opo,ok,rpo,tpo,apo,Qp,QK,spo,npo,rk,lpo,ipo,dpo,Vp,VK,cpo,mpo,tk,fpo,hpo,gpo,Wp,WK,upo,ppo,ak,_po,bpo,vpo,Hp,HK,Tpo,Fpo,sk,Epo,Cpo,Mpo,Up,UK,ypo,wpo,nk,Apo,Lpo,Bpo,Jp,JK,xpo,kpo,lk,Rpo,Ppo,Spo,Kp,KK,$po,Ipo,ik,Dpo,Npo,jpo,Yp,YK,Opo,Gpo,dk,qpo,zpo,Xpo,Zp,ZK,Qpo,Vpo,ck,Wpo,Hpo,Upo,e_,eY,Jpo,Kpo,mk,Ypo,Zpo,e_o,o_,oY,o_o,r_o,fk,t_o,a_o,s_o,r_,rY,n_o,l_o,hk,i_o,d_o,c_o,t_,tY,m_o,f_o,gk,h_o,g_o,u_o,a_,aY,p_o,__o,uk,b_o,v_o,T_o,s_,sY,F_o,E_o,pk,C_o,M_o,y_o,n_,nY,w_o,A_o,_k,L_o,B_o,x_o,l_,lY,k_o,R_o,bk,P_o,S_o,$_o,i_,iY,I_o,D_o,vk,N_o,j_o,O_o,d_,dY,G_o,q_o,Tk,z_o,X_o,Q_o,c_,cY,V_o,W_o,Fk,H_o,U_o,J_o,m_,mY,K_o,Y_o,Ek,Z_o,ebo,obo,f_,fY,rbo,tbo,Ck,abo,sbo,nbo,h_,hY,lbo,ibo,Mk,dbo,cbo,mbo,g_,gY,fbo,hbo,yk,gbo,ubo,pbo,u_,uY,_bo,bbo,wk,vbo,Tbo,Fbo,p_,pY,Ebo,Cbo,Ak,Mbo,ybo,wbo,__,_Y,Abo,Lbo,Lk,Bbo,xbo,kbo,b_,bY,Rbo,Pbo,Bk,Sbo,$bo,Ibo,v_,vY,Dbo,Nbo,xk,jbo,Obo,Gbo,T_,TY,qbo,zbo,kk,Xbo,Qbo,Vbo,F_,Wbo,FY,Hbo,Ubo,EY,Jbo,Kbo,CY,Ybo,Zbo,U3,B5e,Bi,E_,MY,J3,e2o,yY,o2o,x5e,Qo,K3,r2o,xi,t2o,wY,a2o,s2o,AY,n2o,l2o,i2o,Y3,d2o,LY,c2o,m2o,f2o,Dr,Z3,h2o,BY,g2o,u2o,ki,p2o,xY,_2o,b2o,kY,v2o,T2o,F2o,RY,E2o,C2o,eM,M2o,Oe,oM,y2o,PY,w2o,A2o,Ra,L2o,SY,B2o,x2o,$Y,k2o,R2o,IY,P2o,S2o,$2o,H,C_,DY,I2o,D2o,Rk,N2o,j2o,O2o,M_,NY,G2o,q2o,Pk,z2o,X2o,Q2o,y_,jY,V2o,W2o,Sk,H2o,U2o,J2o,w_,OY,K2o,Y2o,$k,Z2o,evo,ovo,A_,GY,rvo,tvo,Ik,avo,svo,nvo,L_,qY,lvo,ivo,Dk,dvo,cvo,mvo,B_,zY,fvo,hvo,Nk,gvo,uvo,pvo,x_,XY,_vo,bvo,jk,vvo,Tvo,Fvo,k_,QY,Evo,Cvo,Ok,Mvo,yvo,wvo,R_,VY,Avo,Lvo,Gk,Bvo,xvo,kvo,P_,WY,Rvo,Pvo,qk,Svo,$vo,Ivo,S_,HY,Dvo,Nvo,zk,jvo,Ovo,Gvo,$_,UY,qvo,zvo,Xk,Xvo,Qvo,Vvo,I_,JY,Wvo,Hvo,Qk,Uvo,Jvo,Kvo,D_,KY,Yvo,Zvo,Vk,eTo,oTo,rTo,N_,YY,tTo,aTo,Wk,sTo,nTo,lTo,j_,ZY,iTo,dTo,Hk,cTo,mTo,fTo,O_,eZ,hTo,gTo,Uk,uTo,pTo,_To,G_,oZ,bTo,vTo,Jk,TTo,FTo,ETo,q_,rZ,CTo,MTo,Kk,yTo,wTo,ATo,z_,tZ,LTo,BTo,Yk,xTo,kTo,RTo,X_,aZ,PTo,STo,Zk,$To,ITo,DTo,Q_,sZ,NTo,jTo,eR,OTo,GTo,qTo,V_,nZ,zTo,XTo,oR,QTo,VTo,WTo,W_,HTo,lZ,UTo,JTo,iZ,KTo,YTo,dZ,ZTo,e1o,rM,k5e,Ri,H_,cZ,tM,o1o,mZ,r1o,R5e,Vo,aM,t1o,Pi,a1o,fZ,s1o,n1o,hZ,l1o,i1o,d1o,sM,c1o,gZ,m1o,f1o,h1o,Nr,nM,g1o,uZ,u1o,p1o,Si,_1o,pZ,b1o,v1o,_Z,T1o,F1o,E1o,bZ,C1o,M1o,lM,y1o,Ge,iM,w1o,vZ,A1o,L1o,Pa,B1o,TZ,x1o,k1o,FZ,R1o,P1o,EZ,S1o,$1o,I1o,jt,U_,CZ,D1o,N1o,rR,j1o,O1o,G1o,J_,MZ,q1o,z1o,tR,X1o,Q1o,V1o,K_,yZ,W1o,H1o,aR,U1o,J1o,K1o,Y_,wZ,Y1o,Z1o,sR,eFo,oFo,rFo,Z_,AZ,tFo,aFo,nR,sFo,nFo,lFo,eb,iFo,LZ,dFo,cFo,BZ,mFo,fFo,xZ,hFo,gFo,dM,P5e,$i,ob,kZ,cM,uFo,RZ,pFo,S5e,Wo,mM,_Fo,Ii,bFo,PZ,vFo,TFo,SZ,FFo,EFo,CFo,fM,MFo,$Z,yFo,wFo,AFo,jr,hM,LFo,IZ,BFo,xFo,Di,kFo,DZ,RFo,PFo,NZ,SFo,$Fo,IFo,jZ,DFo,NFo,gM,jFo,qe,uM,OFo,OZ,GFo,qFo,Sa,zFo,GZ,XFo,QFo,qZ,VFo,WFo,zZ,HFo,UFo,JFo,X,rb,XZ,KFo,YFo,lR,ZFo,eEo,oEo,tb,QZ,rEo,tEo,iR,aEo,sEo,nEo,ab,VZ,lEo,iEo,dR,dEo,cEo,mEo,sb,WZ,fEo,hEo,cR,gEo,uEo,pEo,nb,HZ,_Eo,bEo,mR,vEo,TEo,FEo,lb,UZ,EEo,CEo,fR,MEo,yEo,wEo,ib,JZ,AEo,LEo,hR,BEo,xEo,kEo,db,KZ,REo,PEo,gR,SEo,$Eo,IEo,cb,YZ,DEo,NEo,uR,jEo,OEo,GEo,mb,ZZ,qEo,zEo,pR,XEo,QEo,VEo,fb,eee,WEo,HEo,_R,UEo,JEo,KEo,hb,oee,YEo,ZEo,bR,e4o,o4o,r4o,gb,ree,t4o,a4o,vR,s4o,n4o,l4o,ub,tee,i4o,d4o,TR,c4o,m4o,f4o,pb,aee,h4o,g4o,FR,u4o,p4o,_4o,_b,see,b4o,v4o,ER,T4o,F4o,E4o,bb,nee,C4o,M4o,CR,y4o,w4o,A4o,vb,lee,L4o,B4o,MR,x4o,k4o,R4o,Tb,iee,P4o,S4o,yR,$4o,I4o,D4o,Fb,dee,N4o,j4o,wR,O4o,G4o,q4o,Eb,cee,z4o,X4o,AR,Q4o,V4o,W4o,Cb,mee,H4o,U4o,LR,J4o,K4o,Y4o,Mb,fee,Z4o,eCo,BR,oCo,rCo,tCo,yb,hee,aCo,sCo,xR,nCo,lCo,iCo,wb,gee,dCo,cCo,kR,mCo,fCo,hCo,Ab,uee,gCo,uCo,RR,pCo,_Co,bCo,Lb,pee,vCo,TCo,PR,FCo,ECo,CCo,Bb,_ee,MCo,yCo,SR,wCo,ACo,LCo,xb,bee,BCo,xCo,$R,kCo,RCo,PCo,kb,SCo,vee,$Co,ICo,Tee,DCo,NCo,Fee,jCo,OCo,pM,$5e,Ni,Rb,Eee,_M,GCo,Cee,qCo,I5e,Ho,bM,zCo,ji,XCo,Mee,QCo,VCo,yee,WCo,HCo,UCo,vM,JCo,wee,KCo,YCo,ZCo,Or,TM,e3o,Aee,o3o,r3o,Oi,t3o,Lee,a3o,s3o,Bee,n3o,l3o,i3o,xee,d3o,c3o,FM,m3o,ze,EM,f3o,kee,h3o,g3o,$a,u3o,Ree,p3o,_3o,Pee,b3o,v3o,See,T3o,F3o,E3o,S,Pb,$ee,C3o,M3o,IR,y3o,w3o,A3o,Sb,Iee,L3o,B3o,DR,x3o,k3o,R3o,$b,Dee,P3o,S3o,NR,$3o,I3o,D3o,Ib,Nee,N3o,j3o,jR,O3o,G3o,q3o,Db,jee,z3o,X3o,OR,Q3o,V3o,W3o,Nb,Oee,H3o,U3o,GR,J3o,K3o,Y3o,jb,Gee,Z3o,eMo,qR,oMo,rMo,tMo,Ob,qee,aMo,sMo,zR,nMo,lMo,iMo,Gb,zee,dMo,cMo,XR,mMo,fMo,hMo,qb,Xee,gMo,uMo,QR,pMo,_Mo,bMo,zb,Qee,vMo,TMo,VR,FMo,EMo,CMo,Xb,Vee,MMo,yMo,WR,wMo,AMo,LMo,Qb,Wee,BMo,xMo,HR,kMo,RMo,PMo,Vb,Hee,SMo,$Mo,UR,IMo,DMo,NMo,Wb,Uee,jMo,OMo,JR,GMo,qMo,zMo,Hb,Jee,XMo,QMo,KR,VMo,WMo,HMo,Ub,Kee,UMo,JMo,YR,KMo,YMo,ZMo,Jb,Yee,e5o,o5o,ZR,r5o,t5o,a5o,Kb,Zee,s5o,n5o,eP,l5o,i5o,d5o,Yb,eoe,c5o,m5o,oP,f5o,h5o,g5o,Zb,ooe,u5o,p5o,rP,_5o,b5o,v5o,e2,roe,T5o,F5o,tP,E5o,C5o,M5o,o2,toe,y5o,w5o,aP,A5o,L5o,B5o,r2,aoe,x5o,k5o,sP,R5o,P5o,S5o,t2,soe,$5o,I5o,nP,D5o,N5o,j5o,a2,noe,O5o,G5o,lP,q5o,z5o,X5o,s2,loe,Q5o,V5o,iP,W5o,H5o,U5o,n2,ioe,J5o,K5o,dP,Y5o,Z5o,eyo,l2,doe,oyo,ryo,cP,tyo,ayo,syo,i2,coe,nyo,lyo,mP,iyo,dyo,cyo,d2,moe,myo,fyo,fP,hyo,gyo,uyo,c2,foe,pyo,_yo,hP,byo,vyo,Tyo,m2,hoe,Fyo,Eyo,gP,Cyo,Myo,yyo,f2,goe,wyo,Ayo,uP,Lyo,Byo,xyo,h2,uoe,kyo,Ryo,pP,Pyo,Syo,$yo,g2,Iyo,poe,Dyo,Nyo,_oe,jyo,Oyo,boe,Gyo,qyo,CM,D5e,Gi,u2,voe,MM,zyo,Toe,Xyo,N5e,Uo,yM,Qyo,qi,Vyo,Foe,Wyo,Hyo,Eoe,Uyo,Jyo,Kyo,wM,Yyo,Coe,Zyo,ewo,owo,Gr,AM,rwo,Moe,two,awo,zi,swo,yoe,nwo,lwo,woe,iwo,dwo,cwo,Aoe,mwo,fwo,LM,hwo,Xe,BM,gwo,Loe,uwo,pwo,Ia,_wo,Boe,bwo,vwo,xoe,Two,Fwo,koe,Ewo,Cwo,Mwo,Roe,p2,Poe,ywo,wwo,_P,Awo,Lwo,Bwo,_2,xwo,Soe,kwo,Rwo,$oe,Pwo,Swo,Ioe,$wo,Iwo,xM,j5e,Xi,b2,Doe,kM,Dwo,Noe,Nwo,O5e,Jo,RM,jwo,Qi,Owo,joe,Gwo,qwo,Ooe,zwo,Xwo,Qwo,PM,Vwo,Goe,Wwo,Hwo,Uwo,qr,SM,Jwo,qoe,Kwo,Ywo,Vi,Zwo,zoe,e7o,o7o,Xoe,r7o,t7o,a7o,Qoe,s7o,n7o,$M,l7o,Qe,IM,i7o,Voe,d7o,c7o,Da,m7o,Woe,f7o,h7o,Hoe,g7o,u7o,Uoe,p7o,_7o,b7o,Ko,v2,Joe,v7o,T7o,bP,F7o,E7o,C7o,dn,Koe,M7o,y7o,vP,w7o,A7o,TP,L7o,B7o,x7o,T2,Yoe,k7o,R7o,FP,P7o,S7o,$7o,Gt,Zoe,I7o,D7o,EP,N7o,j7o,CP,O7o,G7o,MP,q7o,z7o,X7o,F2,ere,Q7o,V7o,yP,W7o,H7o,U7o,E2,ore,J7o,K7o,wP,Y7o,Z7o,e0o,C2,o0o,rre,r0o,t0o,tre,a0o,s0o,are,n0o,l0o,DM,G5e,Wi,M2,sre,NM,i0o,nre,d0o,q5e,Yo,jM,c0o,Hi,m0o,lre,f0o,h0o,ire,g0o,u0o,p0o,OM,_0o,dre,b0o,v0o,T0o,zr,GM,F0o,cre,E0o,C0o,Ui,M0o,mre,y0o,w0o,fre,A0o,L0o,B0o,hre,x0o,k0o,qM,R0o,Ve,zM,P0o,gre,S0o,$0o,Na,I0o,ure,D0o,N0o,pre,j0o,O0o,_re,G0o,q0o,z0o,bre,y2,vre,X0o,Q0o,AP,V0o,W0o,H0o,w2,U0o,Tre,J0o,K0o,Fre,Y0o,Z0o,Ere,eAo,oAo,XM,z5e,Ji,A2,Cre,QM,rAo,Mre,tAo,X5e,Zo,VM,aAo,Ki,sAo,yre,nAo,lAo,wre,iAo,dAo,cAo,WM,mAo,Are,fAo,hAo,gAo,Xr,HM,uAo,Lre,pAo,_Ao,Yi,bAo,Bre,vAo,TAo,xre,FAo,EAo,CAo,kre,MAo,yAo,UM,wAo,We,JM,AAo,Rre,LAo,BAo,ja,xAo,Pre,kAo,RAo,Sre,PAo,SAo,$re,$Ao,IAo,DAo,er,L2,Ire,NAo,jAo,LP,OAo,GAo,qAo,B2,Dre,zAo,XAo,BP,QAo,VAo,WAo,x2,Nre,HAo,UAo,xP,JAo,KAo,YAo,k2,jre,ZAo,e6o,kP,o6o,r6o,t6o,R2,Ore,a6o,s6o,RP,n6o,l6o,i6o,P2,Gre,d6o,c6o,PP,m6o,f6o,h6o,S2,g6o,qre,u6o,p6o,zre,_6o,b6o,Xre,v6o,T6o,KM,Q5e,Zi,$2,Qre,YM,F6o,Vre,E6o,V5e,or,ZM,C6o,ed,M6o,Wre,y6o,w6o,Hre,A6o,L6o,B6o,e5,x6o,Ure,k6o,R6o,P6o,Qr,o5,S6o,Jre,$6o,I6o,od,D6o,Kre,N6o,j6o,Yre,O6o,G6o,q6o,Zre,z6o,X6o,r5,Q6o,He,t5,V6o,ete,W6o,H6o,Oa,U6o,ote,J6o,K6o,rte,Y6o,Z6o,tte,eLo,oLo,rLo,rr,I2,ate,tLo,aLo,SP,sLo,nLo,lLo,D2,ste,iLo,dLo,$P,cLo,mLo,fLo,N2,nte,hLo,gLo,IP,uLo,pLo,_Lo,j2,lte,bLo,vLo,DP,TLo,FLo,ELo,O2,ite,CLo,MLo,NP,yLo,wLo,ALo,G2,dte,LLo,BLo,jP,xLo,kLo,RLo,q2,PLo,cte,SLo,$Lo,mte,ILo,DLo,fte,NLo,jLo,a5,W5e,rd,z2,hte,s5,OLo,gte,GLo,H5e,tr,n5,qLo,td,zLo,ute,XLo,QLo,pte,VLo,WLo,HLo,l5,ULo,_te,JLo,KLo,YLo,Vr,i5,ZLo,bte,e8o,o8o,ad,r8o,vte,t8o,a8o,Tte,s8o,n8o,l8o,Fte,i8o,d8o,d5,c8o,Ue,c5,m8o,Ete,f8o,h8o,Ga,g8o,Cte,u8o,p8o,Mte,_8o,b8o,yte,v8o,T8o,F8o,m5,X2,wte,E8o,C8o,OP,M8o,y8o,w8o,Q2,Ate,A8o,L8o,GP,B8o,x8o,k8o,V2,R8o,Lte,P8o,S8o,Bte,$8o,I8o,xte,D8o,N8o,f5,U5e,sd,W2,kte,h5,j8o,Rte,O8o,J5e,ar,g5,G8o,nd,q8o,Pte,z8o,X8o,Ste,Q8o,V8o,W8o,u5,H8o,$te,U8o,J8o,K8o,Wr,p5,Y8o,Ite,Z8o,eBo,ld,oBo,Dte,rBo,tBo,Nte,aBo,sBo,nBo,jte,lBo,iBo,_5,dBo,Je,b5,cBo,Ote,mBo,fBo,qa,hBo,Gte,gBo,uBo,qte,pBo,_Bo,zte,bBo,vBo,TBo,Xte,H2,Qte,FBo,EBo,qP,CBo,MBo,yBo,U2,wBo,Vte,ABo,LBo,Wte,BBo,xBo,Hte,kBo,RBo,v5,K5e,id,J2,Ute,T5,PBo,Jte,SBo,Y5e,sr,F5,$Bo,dd,IBo,Kte,DBo,NBo,Yte,jBo,OBo,GBo,E5,qBo,Zte,zBo,XBo,QBo,Hr,C5,VBo,eae,WBo,HBo,cd,UBo,oae,JBo,KBo,rae,YBo,ZBo,e9o,tae,o9o,r9o,M5,t9o,Ke,y5,a9o,aae,s9o,n9o,za,l9o,sae,i9o,d9o,nae,c9o,m9o,lae,f9o,h9o,g9o,iae,K2,dae,u9o,p9o,zP,_9o,b9o,v9o,Y2,T9o,cae,F9o,E9o,mae,C9o,M9o,fae,y9o,w9o,w5,Z5e,md,Z2,hae,A5,A9o,gae,L9o,eye,nr,L5,B9o,fd,x9o,uae,k9o,R9o,pae,P9o,S9o,$9o,B5,I9o,_ae,D9o,N9o,j9o,Ur,x5,O9o,bae,G9o,q9o,hd,z9o,vae,X9o,Q9o,Tae,V9o,W9o,H9o,Fae,U9o,J9o,k5,K9o,lo,R5,Y9o,Eae,Z9o,exo,Xa,oxo,Cae,rxo,txo,Mae,axo,sxo,yae,nxo,lxo,ixo,B,ev,wae,dxo,cxo,XP,mxo,fxo,hxo,ov,Aae,gxo,uxo,QP,pxo,_xo,bxo,rv,Lae,vxo,Txo,VP,Fxo,Exo,Cxo,tv,Bae,Mxo,yxo,WP,wxo,Axo,Lxo,av,xae,Bxo,xxo,HP,kxo,Rxo,Pxo,sv,kae,Sxo,$xo,UP,Ixo,Dxo,Nxo,nv,Rae,jxo,Oxo,JP,Gxo,qxo,zxo,lv,Pae,Xxo,Qxo,KP,Vxo,Wxo,Hxo,iv,Sae,Uxo,Jxo,YP,Kxo,Yxo,Zxo,dv,$ae,eko,oko,ZP,rko,tko,ako,cv,Iae,sko,nko,eS,lko,iko,dko,mv,Dae,cko,mko,oS,fko,hko,gko,fv,Nae,uko,pko,rS,_ko,bko,vko,hv,jae,Tko,Fko,tS,Eko,Cko,Mko,cn,Oae,yko,wko,aS,Ako,Lko,sS,Bko,xko,kko,gv,Gae,Rko,Pko,nS,Sko,$ko,Iko,uv,qae,Dko,Nko,lS,jko,Oko,Gko,pv,zae,qko,zko,iS,Xko,Qko,Vko,_v,Xae,Wko,Hko,dS,Uko,Jko,Kko,bv,Qae,Yko,Zko,cS,eRo,oRo,rRo,vv,Vae,tRo,aRo,mS,sRo,nRo,lRo,Tv,Wae,iRo,dRo,fS,cRo,mRo,fRo,Fv,Hae,hRo,gRo,hS,uRo,pRo,_Ro,Ev,Uae,bRo,vRo,gS,TRo,FRo,ERo,Cv,Jae,CRo,MRo,uS,yRo,wRo,ARo,Mv,Kae,LRo,BRo,pS,xRo,kRo,RRo,yv,Yae,PRo,SRo,_S,$Ro,IRo,DRo,wv,Zae,NRo,jRo,bS,ORo,GRo,qRo,Av,ese,zRo,XRo,vS,QRo,VRo,WRo,Lv,ose,HRo,URo,TS,JRo,KRo,YRo,Bv,rse,ZRo,ePo,FS,oPo,rPo,tPo,xv,tse,aPo,sPo,ES,nPo,lPo,iPo,kv,ase,dPo,cPo,CS,mPo,fPo,hPo,Rv,sse,gPo,uPo,MS,pPo,_Po,bPo,Pv,nse,vPo,TPo,yS,FPo,EPo,CPo,Sv,lse,MPo,yPo,wS,wPo,APo,LPo,$v,ise,BPo,xPo,AS,kPo,RPo,PPo,Iv,dse,SPo,$Po,LS,IPo,DPo,NPo,Dv,cse,jPo,OPo,BS,GPo,qPo,zPo,mse,XPo,QPo,P5,oye,gd,Nv,fse,S5,VPo,hse,WPo,rye,lr,$5,HPo,ud,UPo,gse,JPo,KPo,use,YPo,ZPo,eSo,I5,oSo,pse,rSo,tSo,aSo,Jr,D5,sSo,_se,nSo,lSo,pd,iSo,bse,dSo,cSo,vse,mSo,fSo,hSo,Tse,gSo,uSo,N5,pSo,io,j5,_So,Fse,bSo,vSo,Qa,TSo,Ese,FSo,ESo,Cse,CSo,MSo,Mse,ySo,wSo,ASo,K,jv,yse,LSo,BSo,xS,xSo,kSo,RSo,Ov,wse,PSo,SSo,kS,$So,ISo,DSo,Gv,Ase,NSo,jSo,RS,OSo,GSo,qSo,qv,Lse,zSo,XSo,PS,QSo,VSo,WSo,zv,Bse,HSo,USo,SS,JSo,KSo,YSo,Xv,xse,ZSo,e$o,$S,o$o,r$o,t$o,Qv,kse,a$o,s$o,IS,n$o,l$o,i$o,Vv,Rse,d$o,c$o,DS,m$o,f$o,h$o,Wv,Pse,g$o,u$o,NS,p$o,_$o,b$o,Hv,Sse,v$o,T$o,jS,F$o,E$o,C$o,Uv,$se,M$o,y$o,OS,w$o,A$o,L$o,Jv,Ise,B$o,x$o,GS,k$o,R$o,P$o,Kv,Dse,S$o,$$o,qS,I$o,D$o,N$o,Yv,Nse,j$o,O$o,zS,G$o,q$o,z$o,Zv,jse,X$o,Q$o,XS,V$o,W$o,H$o,eT,Ose,U$o,J$o,QS,K$o,Y$o,Z$o,oT,Gse,eIo,oIo,VS,rIo,tIo,aIo,rT,qse,sIo,nIo,WS,lIo,iIo,dIo,tT,zse,cIo,mIo,HS,fIo,hIo,gIo,aT,Xse,uIo,pIo,US,_Io,bIo,vIo,sT,Qse,TIo,FIo,JS,EIo,CIo,MIo,nT,Vse,yIo,wIo,KS,AIo,LIo,BIo,Wse,xIo,kIo,O5,tye,_d,lT,Hse,G5,RIo,Use,PIo,aye,ir,q5,SIo,bd,$Io,Jse,IIo,DIo,Kse,NIo,jIo,OIo,z5,GIo,Yse,qIo,zIo,XIo,Kr,X5,QIo,Zse,VIo,WIo,vd,HIo,ene,UIo,JIo,one,KIo,YIo,ZIo,rne,eDo,oDo,Q5,rDo,co,V5,tDo,tne,aDo,sDo,Va,nDo,ane,lDo,iDo,sne,dDo,cDo,nne,mDo,fDo,hDo,_e,iT,lne,gDo,uDo,YS,pDo,_Do,bDo,dT,ine,vDo,TDo,ZS,FDo,EDo,CDo,cT,dne,MDo,yDo,e$,wDo,ADo,LDo,mT,cne,BDo,xDo,o$,kDo,RDo,PDo,fT,mne,SDo,$Do,r$,IDo,DDo,NDo,hT,fne,jDo,ODo,t$,GDo,qDo,zDo,gT,hne,XDo,QDo,a$,VDo,WDo,HDo,uT,gne,UDo,JDo,s$,KDo,YDo,ZDo,pT,une,eNo,oNo,n$,rNo,tNo,aNo,_T,pne,sNo,nNo,l$,lNo,iNo,dNo,_ne,cNo,mNo,W5,sye,Td,bT,bne,H5,fNo,vne,hNo,nye,dr,U5,gNo,Fd,uNo,Tne,pNo,_No,Fne,bNo,vNo,TNo,J5,FNo,Ene,ENo,CNo,MNo,Yr,K5,yNo,Cne,wNo,ANo,Ed,LNo,Mne,BNo,xNo,yne,kNo,RNo,PNo,wne,SNo,$No,Y5,INo,mo,Z5,DNo,Ane,NNo,jNo,Wa,ONo,Lne,GNo,qNo,Bne,zNo,XNo,xne,QNo,VNo,WNo,kne,vT,Rne,HNo,UNo,i$,JNo,KNo,YNo,Pne,ZNo,ejo,ey,lye,Cd,TT,Sne,oy,ojo,$ne,rjo,iye,cr,ry,tjo,Md,ajo,Ine,sjo,njo,Dne,ljo,ijo,djo,ty,cjo,Nne,mjo,fjo,hjo,Zr,ay,gjo,jne,ujo,pjo,yd,_jo,One,bjo,vjo,Gne,Tjo,Fjo,Ejo,qne,Cjo,Mjo,sy,yjo,fo,ny,wjo,zne,Ajo,Ljo,Ha,Bjo,Xne,xjo,kjo,Qne,Rjo,Pjo,Vne,Sjo,$jo,Ijo,te,FT,Wne,Djo,Njo,d$,jjo,Ojo,Gjo,ET,Hne,qjo,zjo,c$,Xjo,Qjo,Vjo,CT,Une,Wjo,Hjo,m$,Ujo,Jjo,Kjo,MT,Jne,Yjo,Zjo,f$,eOo,oOo,rOo,yT,Kne,tOo,aOo,h$,sOo,nOo,lOo,wT,Yne,iOo,dOo,g$,cOo,mOo,fOo,AT,Zne,hOo,gOo,u$,uOo,pOo,_Oo,LT,ele,bOo,vOo,p$,TOo,FOo,EOo,BT,ole,COo,MOo,_$,yOo,wOo,AOo,xT,rle,LOo,BOo,b$,xOo,kOo,ROo,kT,tle,POo,SOo,v$,$Oo,IOo,DOo,RT,ale,NOo,jOo,T$,OOo,GOo,qOo,PT,sle,zOo,XOo,F$,QOo,VOo,WOo,ST,nle,HOo,UOo,E$,JOo,KOo,YOo,$T,lle,ZOo,eGo,C$,oGo,rGo,tGo,IT,ile,aGo,sGo,M$,nGo,lGo,iGo,DT,dle,dGo,cGo,y$,mGo,fGo,hGo,NT,cle,gGo,uGo,w$,pGo,_Go,bGo,jT,mle,vGo,TGo,A$,FGo,EGo,CGo,OT,fle,MGo,yGo,L$,wGo,AGo,LGo,hle,BGo,xGo,ly,dye,wd,GT,gle,iy,kGo,ule,RGo,cye,mr,dy,PGo,Ad,SGo,ple,$Go,IGo,_le,DGo,NGo,jGo,cy,OGo,ble,GGo,qGo,zGo,et,my,XGo,vle,QGo,VGo,Ld,WGo,Tle,HGo,UGo,Fle,JGo,KGo,YGo,Ele,ZGo,eqo,fy,oqo,ho,hy,rqo,Cle,tqo,aqo,Ua,sqo,Mle,nqo,lqo,yle,iqo,dqo,wle,cqo,mqo,fqo,be,qT,Ale,hqo,gqo,B$,uqo,pqo,_qo,zT,Lle,bqo,vqo,x$,Tqo,Fqo,Eqo,XT,Ble,Cqo,Mqo,k$,yqo,wqo,Aqo,QT,xle,Lqo,Bqo,R$,xqo,kqo,Rqo,VT,kle,Pqo,Sqo,P$,$qo,Iqo,Dqo,WT,Rle,Nqo,jqo,S$,Oqo,Gqo,qqo,HT,Ple,zqo,Xqo,$$,Qqo,Vqo,Wqo,UT,Sle,Hqo,Uqo,I$,Jqo,Kqo,Yqo,JT,$le,Zqo,ezo,D$,ozo,rzo,tzo,KT,Ile,azo,szo,N$,nzo,lzo,izo,Dle,dzo,czo,gy,mye,Bd,YT,Nle,uy,mzo,jle,fzo,fye,fr,py,hzo,xd,gzo,Ole,uzo,pzo,Gle,_zo,bzo,vzo,_y,Tzo,qle,Fzo,Ezo,Czo,ot,by,Mzo,zle,yzo,wzo,kd,Azo,Xle,Lzo,Bzo,Qle,xzo,kzo,Rzo,Vle,Pzo,Szo,vy,$zo,go,Ty,Izo,Wle,Dzo,Nzo,Ja,jzo,Hle,Ozo,Gzo,Ule,qzo,zzo,Jle,Xzo,Qzo,Vzo,W,ZT,Kle,Wzo,Hzo,j$,Uzo,Jzo,Kzo,e1,Yle,Yzo,Zzo,O$,eXo,oXo,rXo,o1,Zle,tXo,aXo,G$,sXo,nXo,lXo,r1,eie,iXo,dXo,q$,cXo,mXo,fXo,t1,oie,hXo,gXo,z$,uXo,pXo,_Xo,a1,rie,bXo,vXo,X$,TXo,FXo,EXo,s1,tie,CXo,MXo,Q$,yXo,wXo,AXo,n1,aie,LXo,BXo,V$,xXo,kXo,RXo,l1,sie,PXo,SXo,W$,$Xo,IXo,DXo,i1,nie,NXo,jXo,H$,OXo,GXo,qXo,d1,lie,zXo,XXo,U$,QXo,VXo,WXo,c1,iie,HXo,UXo,J$,JXo,KXo,YXo,m1,die,ZXo,eQo,K$,oQo,rQo,tQo,f1,cie,aQo,sQo,Y$,nQo,lQo,iQo,h1,mie,dQo,cQo,Z$,mQo,fQo,hQo,g1,fie,gQo,uQo,eI,pQo,_Qo,bQo,u1,hie,vQo,TQo,oI,FQo,EQo,CQo,p1,gie,MQo,yQo,rI,wQo,AQo,LQo,_1,uie,BQo,xQo,tI,kQo,RQo,PQo,b1,pie,SQo,$Qo,aI,IQo,DQo,NQo,v1,_ie,jQo,OQo,sI,GQo,qQo,zQo,T1,bie,XQo,QQo,nI,VQo,WQo,HQo,F1,vie,UQo,JQo,lI,KQo,YQo,ZQo,E1,Tie,eVo,oVo,iI,rVo,tVo,aVo,C1,Fie,sVo,nVo,dI,lVo,iVo,dVo,Eie,cVo,mVo,Fy,hye,Rd,M1,Cie,Ey,fVo,Mie,hVo,gye,hr,Cy,gVo,Pd,uVo,yie,pVo,_Vo,wie,bVo,vVo,TVo,My,FVo,Aie,EVo,CVo,MVo,rt,yy,yVo,Lie,wVo,AVo,Sd,LVo,Bie,BVo,xVo,xie,kVo,RVo,PVo,kie,SVo,$Vo,wy,IVo,uo,Ay,DVo,Rie,NVo,jVo,Ka,OVo,Pie,GVo,qVo,Sie,zVo,XVo,$ie,QVo,VVo,WVo,de,y1,Iie,HVo,UVo,cI,JVo,KVo,YVo,w1,Die,ZVo,eWo,mI,oWo,rWo,tWo,A1,Nie,aWo,sWo,fI,nWo,lWo,iWo,L1,jie,dWo,cWo,hI,mWo,fWo,hWo,B1,Oie,gWo,uWo,gI,pWo,_Wo,bWo,x1,Gie,vWo,TWo,uI,FWo,EWo,CWo,k1,qie,MWo,yWo,pI,wWo,AWo,LWo,R1,zie,BWo,xWo,_I,kWo,RWo,PWo,P1,Xie,SWo,$Wo,bI,IWo,DWo,NWo,S1,Qie,jWo,OWo,vI,GWo,qWo,zWo,$1,Vie,XWo,QWo,TI,VWo,WWo,HWo,I1,Wie,UWo,JWo,FI,KWo,YWo,ZWo,D1,Hie,eHo,oHo,EI,rHo,tHo,aHo,N1,Uie,sHo,nHo,CI,lHo,iHo,dHo,j1,Jie,cHo,mHo,MI,fHo,hHo,gHo,O1,Kie,uHo,pHo,yI,_Ho,bHo,vHo,G1,Yie,THo,FHo,wI,EHo,CHo,MHo,Zie,yHo,wHo,Ly,uye,$d,q1,ede,By,AHo,ode,LHo,pye,gr,xy,BHo,Id,xHo,rde,kHo,RHo,tde,PHo,SHo,$Ho,ky,IHo,ade,DHo,NHo,jHo,tt,Ry,OHo,sde,GHo,qHo,Dd,zHo,nde,XHo,QHo,lde,VHo,WHo,HHo,ide,UHo,JHo,Py,KHo,po,Sy,YHo,dde,ZHo,eUo,Ya,oUo,cde,rUo,tUo,mde,aUo,sUo,fde,nUo,lUo,iUo,hde,z1,gde,dUo,cUo,AI,mUo,fUo,hUo,ude,gUo,uUo,$y,_ye,Nd,X1,pde,Iy,pUo,_de,_Uo,bye,ur,Dy,bUo,jd,vUo,bde,TUo,FUo,vde,EUo,CUo,MUo,Ny,yUo,Tde,wUo,AUo,LUo,at,jy,BUo,Fde,xUo,kUo,Od,RUo,Ede,PUo,SUo,Cde,$Uo,IUo,DUo,Mde,NUo,jUo,Oy,OUo,_o,Gy,GUo,yde,qUo,zUo,Za,XUo,wde,QUo,VUo,Ade,WUo,HUo,Lde,UUo,JUo,KUo,ae,Q1,Bde,YUo,ZUo,LI,eJo,oJo,rJo,V1,xde,tJo,aJo,BI,sJo,nJo,lJo,W1,kde,iJo,dJo,xI,cJo,mJo,fJo,H1,Rde,hJo,gJo,kI,uJo,pJo,_Jo,U1,Pde,bJo,vJo,RI,TJo,FJo,EJo,J1,Sde,CJo,MJo,PI,yJo,wJo,AJo,K1,$de,LJo,BJo,SI,xJo,kJo,RJo,Y1,Ide,PJo,SJo,$I,$Jo,IJo,DJo,Z1,Dde,NJo,jJo,II,OJo,GJo,qJo,eF,Nde,zJo,XJo,DI,QJo,VJo,WJo,oF,jde,HJo,UJo,NI,JJo,KJo,YJo,rF,Ode,ZJo,eKo,jI,oKo,rKo,tKo,tF,Gde,aKo,sKo,OI,nKo,lKo,iKo,aF,qde,dKo,cKo,GI,mKo,fKo,hKo,sF,zde,gKo,uKo,qI,pKo,_Ko,bKo,nF,Xde,vKo,TKo,zI,FKo,EKo,CKo,lF,Qde,MKo,yKo,XI,wKo,AKo,LKo,iF,Vde,BKo,xKo,QI,kKo,RKo,PKo,dF,Wde,SKo,$Ko,VI,IKo,DKo,NKo,cF,Hde,jKo,OKo,WI,GKo,qKo,zKo,Ude,XKo,QKo,qy,vye,Gd,mF,Jde,zy,VKo,Kde,WKo,Tye,pr,Xy,HKo,qd,UKo,Yde,JKo,KKo,Zde,YKo,ZKo,eYo,Qy,oYo,ece,rYo,tYo,aYo,st,Vy,sYo,oce,nYo,lYo,zd,iYo,rce,dYo,cYo,tce,mYo,fYo,hYo,ace,gYo,uYo,Wy,pYo,bo,Hy,_Yo,sce,bYo,vYo,es,TYo,nce,FYo,EYo,lce,CYo,MYo,ice,yYo,wYo,AYo,se,fF,dce,LYo,BYo,HI,xYo,kYo,RYo,hF,cce,PYo,SYo,UI,$Yo,IYo,DYo,gF,mce,NYo,jYo,JI,OYo,GYo,qYo,uF,fce,zYo,XYo,KI,QYo,VYo,WYo,pF,hce,HYo,UYo,YI,JYo,KYo,YYo,_F,gce,ZYo,eZo,ZI,oZo,rZo,tZo,bF,uce,aZo,sZo,eD,nZo,lZo,iZo,vF,pce,dZo,cZo,oD,mZo,fZo,hZo,TF,_ce,gZo,uZo,rD,pZo,_Zo,bZo,FF,bce,vZo,TZo,tD,FZo,EZo,CZo,EF,vce,MZo,yZo,aD,wZo,AZo,LZo,CF,Tce,BZo,xZo,sD,kZo,RZo,PZo,MF,Fce,SZo,$Zo,nD,IZo,DZo,NZo,yF,Ece,jZo,OZo,lD,GZo,qZo,zZo,wF,Cce,XZo,QZo,iD,VZo,WZo,HZo,AF,Mce,UZo,JZo,dD,KZo,YZo,ZZo,LF,yce,eer,oer,cD,rer,ter,aer,BF,wce,ser,ner,mD,ler,ier,der,xF,Ace,cer,mer,fD,fer,her,ger,Lce,uer,per,Uy,Fye,Xd,kF,Bce,Jy,_er,xce,ber,Eye,_r,Ky,ver,Qd,Ter,kce,Fer,Eer,Rce,Cer,Mer,yer,Yy,wer,Pce,Aer,Ler,Ber,nt,Zy,xer,Sce,ker,Rer,Vd,Per,$ce,Ser,$er,Ice,Ier,Der,Ner,Dce,jer,Oer,ew,Ger,vo,ow,qer,Nce,zer,Xer,os,Qer,jce,Ver,Wer,Oce,Her,Uer,Gce,Jer,Ker,Yer,Y,RF,qce,Zer,eor,hD,oor,ror,tor,PF,zce,aor,sor,gD,nor,lor,ior,SF,Xce,dor,cor,uD,mor,hor,gor,$F,Qce,uor,por,pD,_or,bor,vor,IF,Vce,Tor,For,_D,Eor,Cor,Mor,DF,Wce,yor,wor,bD,Aor,Lor,Bor,NF,Hce,xor,kor,vD,Ror,Por,Sor,jF,Uce,$or,Ior,TD,Dor,Nor,jor,OF,Jce,Oor,Gor,FD,qor,zor,Xor,GF,Kce,Qor,Vor,ED,Wor,Hor,Uor,qF,Yce,Jor,Kor,CD,Yor,Zor,err,zF,Zce,orr,rrr,MD,trr,arr,srr,XF,eme,nrr,lrr,yD,irr,drr,crr,QF,ome,mrr,frr,wD,hrr,grr,urr,VF,rme,prr,_rr,AD,brr,vrr,Trr,WF,tme,Frr,Err,LD,Crr,Mrr,yrr,HF,ame,wrr,Arr,BD,Lrr,Brr,xrr,UF,sme,krr,Rrr,xD,Prr,Srr,$rr,JF,nme,Irr,Drr,kD,Nrr,jrr,Orr,KF,lme,Grr,qrr,RD,zrr,Xrr,Qrr,YF,ime,Vrr,Wrr,PD,Hrr,Urr,Jrr,ZF,dme,Krr,Yrr,SD,Zrr,etr,otr,cme,rtr,ttr,rw,Cye,Wd,eE,mme,tw,atr,fme,str,Mye,br,aw,ntr,Hd,ltr,hme,itr,dtr,gme,ctr,mtr,ftr,sw,htr,ume,gtr,utr,ptr,lt,nw,_tr,pme,btr,vtr,Ud,Ttr,_me,Ftr,Etr,bme,Ctr,Mtr,ytr,vme,wtr,Atr,lw,Ltr,To,iw,Btr,Tme,xtr,ktr,rs,Rtr,Fme,Ptr,Str,Eme,$tr,Itr,Cme,Dtr,Ntr,jtr,Jd,oE,Mme,Otr,Gtr,$D,qtr,ztr,Xtr,rE,yme,Qtr,Vtr,ID,Wtr,Htr,Utr,tE,wme,Jtr,Ktr,DD,Ytr,Ztr,ear,Ame,oar,rar,dw,yye,Kd,aE,Lme,cw,tar,Bme,aar,wye,vr,mw,sar,Yd,nar,xme,lar,iar,kme,dar,car,mar,fw,far,Rme,har,gar,uar,it,hw,par,Pme,_ar,bar,Zd,Tar,Sme,Far,Ear,$me,Car,Mar,yar,Ime,war,Aar,gw,Lar,Fo,uw,Bar,Dme,xar,kar,ts,Rar,Nme,Par,Sar,jme,$ar,Iar,Ome,Dar,Nar,jar,ve,sE,Gme,Oar,Gar,ND,qar,zar,Xar,nE,qme,Qar,Var,jD,War,Har,Uar,lE,zme,Jar,Kar,OD,Yar,Zar,esr,iE,Xme,osr,rsr,GD,tsr,asr,ssr,dE,Qme,nsr,lsr,qD,isr,dsr,csr,cE,Vme,msr,fsr,zD,hsr,gsr,usr,mE,Wme,psr,_sr,XD,bsr,vsr,Tsr,fE,Hme,Fsr,Esr,QD,Csr,Msr,ysr,hE,Ume,wsr,Asr,VD,Lsr,Bsr,xsr,gE,Jme,ksr,Rsr,WD,Psr,Ssr,$sr,Kme,Isr,Dsr,pw,Aye,ec,uE,Yme,_w,Nsr,Zme,jsr,Lye,Tr,bw,Osr,oc,Gsr,efe,qsr,zsr,ofe,Xsr,Qsr,Vsr,vw,Wsr,rfe,Hsr,Usr,Jsr,dt,Tw,Ksr,tfe,Ysr,Zsr,rc,enr,afe,onr,rnr,sfe,tnr,anr,snr,nfe,nnr,lnr,Fw,inr,Eo,Ew,dnr,lfe,cnr,mnr,as,fnr,ife,hnr,gnr,dfe,unr,pnr,cfe,_nr,bnr,vnr,xe,pE,mfe,Tnr,Fnr,HD,Enr,Cnr,Mnr,_E,ffe,ynr,wnr,UD,Anr,Lnr,Bnr,bE,hfe,xnr,knr,JD,Rnr,Pnr,Snr,vE,gfe,$nr,Inr,KD,Dnr,Nnr,jnr,TE,ufe,Onr,Gnr,YD,qnr,znr,Xnr,FE,pfe,Qnr,Vnr,ZD,Wnr,Hnr,Unr,EE,_fe,Jnr,Knr,eN,Ynr,Znr,elr,CE,bfe,olr,rlr,oN,tlr,alr,slr,vfe,nlr,llr,Cw,Bye,tc,ME,Tfe,Mw,ilr,Ffe,dlr,xye,Fr,yw,clr,ac,mlr,Efe,flr,hlr,Cfe,glr,ulr,plr,ww,_lr,Mfe,blr,vlr,Tlr,ct,Aw,Flr,yfe,Elr,Clr,sc,Mlr,wfe,ylr,wlr,Afe,Alr,Llr,Blr,Lfe,xlr,klr,Lw,Rlr,Co,Bw,Plr,Bfe,Slr,$lr,ss,Ilr,xfe,Dlr,Nlr,kfe,jlr,Olr,Rfe,Glr,qlr,zlr,Me,yE,Pfe,Xlr,Qlr,rN,Vlr,Wlr,Hlr,wE,Sfe,Ulr,Jlr,tN,Klr,Ylr,Zlr,AE,$fe,eir,oir,aN,rir,tir,air,LE,Ife,sir,nir,sN,lir,iir,dir,BE,Dfe,cir,mir,nN,fir,hir,gir,xE,Nfe,uir,pir,lN,_ir,bir,vir,kE,jfe,Tir,Fir,iN,Eir,Cir,Mir,RE,Ofe,yir,wir,dN,Air,Lir,Bir,PE,Gfe,xir,kir,cN,Rir,Pir,Sir,qfe,$ir,Iir,xw,kye,nc,SE,zfe,kw,Dir,Xfe,Nir,Rye,Er,Rw,jir,lc,Oir,Qfe,Gir,qir,Vfe,zir,Xir,Qir,Pw,Vir,Wfe,Wir,Hir,Uir,mt,Sw,Jir,Hfe,Kir,Yir,ic,Zir,Ufe,edr,odr,Jfe,rdr,tdr,adr,Kfe,sdr,ndr,$w,ldr,Mo,Iw,idr,Yfe,ddr,cdr,ns,mdr,Zfe,fdr,hdr,ehe,gdr,udr,ohe,pdr,_dr,bdr,ke,$E,rhe,vdr,Tdr,mN,Fdr,Edr,Cdr,IE,the,Mdr,ydr,fN,wdr,Adr,Ldr,DE,ahe,Bdr,xdr,hN,kdr,Rdr,Pdr,NE,she,Sdr,$dr,gN,Idr,Ddr,Ndr,jE,nhe,jdr,Odr,uN,Gdr,qdr,zdr,OE,lhe,Xdr,Qdr,pN,Vdr,Wdr,Hdr,GE,ihe,Udr,Jdr,_N,Kdr,Ydr,Zdr,qE,dhe,ecr,ocr,bN,rcr,tcr,acr,che,scr,ncr,Dw,Pye,dc,zE,mhe,Nw,lcr,fhe,icr,Sye,Cr,jw,dcr,cc,ccr,hhe,mcr,fcr,ghe,hcr,gcr,ucr,Ow,pcr,uhe,_cr,bcr,vcr,ft,Gw,Tcr,phe,Fcr,Ecr,mc,Ccr,_he,Mcr,ycr,bhe,wcr,Acr,Lcr,vhe,Bcr,xcr,qw,kcr,yo,zw,Rcr,The,Pcr,Scr,ls,$cr,Fhe,Icr,Dcr,Ehe,Ncr,jcr,Che,Ocr,Gcr,qcr,Re,XE,Mhe,zcr,Xcr,vN,Qcr,Vcr,Wcr,QE,yhe,Hcr,Ucr,TN,Jcr,Kcr,Ycr,VE,whe,Zcr,emr,FN,omr,rmr,tmr,WE,Ahe,amr,smr,EN,nmr,lmr,imr,HE,Lhe,dmr,cmr,CN,mmr,fmr,hmr,UE,Bhe,gmr,umr,MN,pmr,_mr,bmr,JE,xhe,vmr,Tmr,yN,Fmr,Emr,Cmr,KE,khe,Mmr,ymr,wN,wmr,Amr,Lmr,Rhe,Bmr,xmr,Xw,$ye,fc,YE,Phe,Qw,kmr,She,Rmr,Iye,Mr,Vw,Pmr,hc,Smr,$he,$mr,Imr,Ihe,Dmr,Nmr,jmr,Ww,Omr,Dhe,Gmr,qmr,zmr,ht,Hw,Xmr,Nhe,Qmr,Vmr,gc,Wmr,jhe,Hmr,Umr,Ohe,Jmr,Kmr,Ymr,Ghe,Zmr,efr,Uw,ofr,wo,Jw,rfr,qhe,tfr,afr,is,sfr,zhe,nfr,lfr,Xhe,ifr,dfr,Qhe,cfr,mfr,ffr,yr,ZE,Vhe,hfr,gfr,AN,ufr,pfr,_fr,e4,Whe,bfr,vfr,LN,Tfr,Ffr,Efr,o4,Hhe,Cfr,Mfr,BN,yfr,wfr,Afr,r4,Uhe,Lfr,Bfr,xN,xfr,kfr,Rfr,t4,Jhe,Pfr,Sfr,kN,$fr,Ifr,Dfr,a4,Khe,Nfr,jfr,RN,Ofr,Gfr,qfr,Yhe,zfr,Xfr,Kw,Dye,uc,s4,Zhe,Yw,Qfr,ege,Vfr,Nye,wr,Zw,Wfr,pc,Hfr,oge,Ufr,Jfr,rge,Kfr,Yfr,Zfr,e7,ehr,tge,ohr,rhr,thr,gt,o7,ahr,age,shr,nhr,_c,lhr,sge,ihr,dhr,nge,chr,mhr,fhr,lge,hhr,ghr,r7,uhr,Ao,t7,phr,ige,_hr,bhr,ds,vhr,dge,Thr,Fhr,cge,Ehr,Chr,mge,Mhr,yhr,whr,Ar,n4,fge,Ahr,Lhr,PN,Bhr,xhr,khr,l4,hge,Rhr,Phr,SN,Shr,$hr,Ihr,i4,gge,Dhr,Nhr,$N,jhr,Ohr,Ghr,d4,uge,qhr,zhr,IN,Xhr,Qhr,Vhr,c4,pge,Whr,Hhr,DN,Uhr,Jhr,Khr,m4,_ge,Yhr,Zhr,NN,egr,ogr,rgr,bge,tgr,agr,a7,jye,bc,f4,vge,s7,sgr,Tge,ngr,Oye,Lr,n7,lgr,vc,igr,Fge,dgr,cgr,Ege,mgr,fgr,hgr,l7,ggr,Cge,ugr,pgr,_gr,ut,i7,bgr,Mge,vgr,Tgr,Tc,Fgr,yge,Egr,Cgr,wge,Mgr,ygr,wgr,Age,Agr,Lgr,d7,Bgr,Lo,c7,xgr,Lge,kgr,Rgr,cs,Pgr,Bge,Sgr,$gr,xge,Igr,Dgr,kge,Ngr,jgr,Ogr,Rge,h4,Pge,Ggr,qgr,jN,zgr,Xgr,Qgr,Sge,Vgr,Wgr,m7,Gye,Fc,g4,$ge,f7,Hgr,Ige,Ugr,qye,Br,h7,Jgr,Ec,Kgr,Dge,Ygr,Zgr,Nge,eur,our,rur,g7,tur,jge,aur,sur,nur,pt,u7,lur,Oge,iur,dur,Cc,cur,Gge,mur,fur,qge,hur,gur,uur,zge,pur,_ur,p7,bur,Bo,_7,vur,Xge,Tur,Fur,ms,Eur,Qge,Cur,Mur,Vge,yur,wur,Wge,Aur,Lur,Bur,b7,u4,Hge,xur,kur,ON,Rur,Pur,Sur,p4,Uge,$ur,Iur,GN,Dur,Nur,jur,Jge,Our,Gur,v7,zye,Mc,_4,Kge,T7,qur,Yge,zur,Xye,xr,F7,Xur,yc,Qur,Zge,Vur,Wur,eue,Hur,Uur,Jur,E7,Kur,oue,Yur,Zur,epr,_t,C7,opr,rue,rpr,tpr,wc,apr,tue,spr,npr,aue,lpr,ipr,dpr,sue,cpr,mpr,M7,fpr,xo,y7,hpr,nue,gpr,upr,fs,ppr,lue,_pr,bpr,iue,vpr,Tpr,due,Fpr,Epr,Cpr,cue,b4,mue,Mpr,ypr,qN,wpr,Apr,Lpr,fue,Bpr,xpr,w7,Qye;return ge=new Z({}),_a=new w({props:{code:"model = AutoModel.from_pretrained('bert-base-cased'),",highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)'}}),TC=new Z({}),FC=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Pc=new kpr({props:{warning:!0,$$slots:{default:[DKr]},$$scope:{ctx:ql}}}),EC=new Z({}),CC=new y({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L473"}}),wC=new y({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L496",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em>
is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e.,
the part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),AC=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-uncased')

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained('dbmdz/bert-base-german-cased')

# If configuration file is in a directory (e.g., was saved using _save_pretrained('./test/saved_model/')_).
config = AutoConfig.from_pretrained('./test/bert_saved_model/')

# Load a specific configuration file.
config = AutoConfig.from_pretrained('./test/bert_saved_model/my_configuration.json')

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained('bert-base-uncased', output_attentions=True, foo=False)
config.output_attentions
config, unused_kwargs = AutoConfig.from_pretrained('bert-base-uncased', output_attentions=True, foo=False, return_unused_kwargs=True)
config.output_attentions
config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;dbmdz/bert-base-german-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using _save_pretrained(&#x27;./test/saved_model/&#x27;)_).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./test/bert_saved_model/&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./test/bert_saved_model/my_configuration.json&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),LC=new y({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L614",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),BC=new Z({}),xC=new y({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L352"}}),PC=new y({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L366"}}),jC=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained('dbmdz/bert-base-german-cased')

# If vocabulary files are in a directory (e.g. tokenizer was saved using _save_pretrained('./test/saved_model/')_)
tokenizer = AutoTokenizer.from_pretrained('./test/bert_saved_model/'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;bert-base-uncased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;dbmdz/bert-base-german-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using _save_pretrained(&#x27;./test/saved_model/&#x27;)_)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;./test/bert_saved_model/&#x27;</span>)`}}),OC=new y({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L557",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),GC=new Z({}),qC=new y({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L64"}}),QC=new y({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L78"}}),Qf=new kpr({props:{$$slots:{default:[NKr]},$$scope:{ctx:ql}}}),JC=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained('facebook/wav2vec2-base-960h')

# If feature extractor files are in a directory (e.g. feature extractor was saved using _save_pretrained('./test/saved_model/')_)
feature_extractor = AutoFeatureExtractor.from_pretrained('./test/saved_model/'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;facebook/wav2vec2-base-960h&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using _save_pretrained(&#x27;./test/saved_model/&#x27;)_)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&#x27;./test/saved_model/&#x27;</span>)`}}),KC=new Z({}),YC=new y({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L62"}}),o3=new y({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L76"}}),eh=new kpr({props:{$$slots:{default:[jKr]},$$scope:{ctx:ql}}}),l3=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained('facebook/wav2vec2-base-960h')

# If processor files are in a directory (e.g. processor was saved using _save_pretrained('./test/saved_model/')_)
processor = AutoProcessor.from_pretrained('./test/saved_model/'),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&#x27;facebook/wav2vec2-base-960h&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using _save_pretrained(&#x27;./test/saved_model/&#x27;)_)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&#x27;./test/saved_model/&#x27;</span>)`}}),i3=new Z({}),d3=new y({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L583"}}),m3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bertgeneration#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bertgeneration#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew_d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew_d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),f3=new w({props:{code:`from transformers import AutoConfig, AutoModel
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),h3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),g3=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModel.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModel.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),u3=new Z({}),p3=new y({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L590"}}),b3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),v3=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),T3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),F3=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForPreTraining.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),E3=new Z({}),C3=new y({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L605"}}),y3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bertgeneration#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bertgeneration#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),w3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),A3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),L3=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForCausalLM.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),B3=new Z({}),x3=new y({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L612"}}),R3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),P3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),S3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$3=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForMaskedLM.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),I3=new Z({}),D3=new y({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L619"}}),j3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),O3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('t5-base')
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),G3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),q3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained('t5-base')

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained('t5-base', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/t5_tf_model_config.json')
model = AutoModelForSeq2SeqLM.from_pretrained('./tf_model/t5_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/t5_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;./tf_model/t5_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),z3=new Z({}),X3=new y({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L628"}}),V3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),W3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),H3=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),U3=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForSequenceClassification.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),J3=new Z({}),K3=new y({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L662"}}),Z3=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),eM=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),oM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),rM=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForMultipleChoice.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),tM=new Z({}),aM=new y({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L669"}}),nM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),lM=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),iM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dM=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForNextSentencePrediction.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),cM=new Z({}),mM=new y({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L655"}}),hM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),gM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),uM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),pM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForTokenClassification.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),_M=new Z({}),bM=new y({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L637"}}),TM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),FM=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),EM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),CM=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForQuestionAnswering.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),MM=new Z({}),yM=new y({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L644"}}),AM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),LM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('google/tapas-base-finetuned-wtq')
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),BM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xM=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/tapas_tf_model_config.json')
model = AutoModelForTableQuestionAnswering.from_pretrained('./tf_model/tapas_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/tapas_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;./tf_model/tapas_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),kM=new Z({}),RM=new y({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L678"}}),SM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),$M=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),IM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),DM=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForImageClassification.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),NM=new Z({}),jM=new y({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L699"}}),GM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),qM=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),zM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),XM=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForVision2Seq.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),QM=new Z({}),VM=new y({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L706"}}),HM=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew_d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew_d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),UM=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),JM=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),KM=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForAudioClassification.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),YM=new Z({}),ZM=new y({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L713"}}),o5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew_d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew_d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),r5=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),t5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),a5=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForCTC.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForCTC.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),s5=new Z({}),n5=new y({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L720"}}),i5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speechencoderdecoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speechencoderdecoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),d5=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),c5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),f5=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForSpeechSeq2Seq.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),h5=new Z({}),g5=new y({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L692"}}),p5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),_5=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),b5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),v5=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForObjectDetection.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),T5=new Z({}),F5=new y({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L685"}}),C5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),M5=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),y5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in
a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),w5=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained('bert-base-cased')

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained('./tf_model/bert_tf_model_config.json')
model = AutoModelForImageSegmentation.from_pretrained('./tf_model/bert_tf_checkpoint.ckpt.index', from_tf=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&#x27;./tf_model/bert_tf_checkpoint.ckpt.index&#x27;</span>, from_tf=<span class="hljs-literal">True</span>, config=config)`}}),A5=new Z({}),L5=new y({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L353"}}),x5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),k5=new w({props:{code:`from transformers import AutoConfig, TFAutoModel
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),R5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),P5=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModel.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModel.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),S5=new Z({}),$5=new y({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L360"}}),D5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),N5=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),j5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),O5=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForPreTraining.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),G5=new Z({}),q5=new y({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L375"}}),X5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Q5=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),V5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),W5=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForCausalLM.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),H5=new Z({}),U5=new y({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L382"}}),K5=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),Y5=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),Z5=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ey=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForImageClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),oy=new Z({}),ry=new y({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L389"}}),ay=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),sy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),ny=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ly=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForMaskedLM.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),iy=new Z({}),dy=new y({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L396"}}),my=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),fy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('t5-base')
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),hy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained('t5-base')

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained('t5-base', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/t5_pt_model_config.json')
model = TFAutoModelForSeq2SeqLM.from_pretrained('./pt_model/t5_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/t5_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/t5_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),uy=new Z({}),py=new y({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L405"}}),by=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transformerxl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),vy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),Ty=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Fy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForSequenceClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Ey=new Z({}),Cy=new y({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L441"}}),yy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),wy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),Ay=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ly=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForMultipleChoice.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),By=new Z({}),xy=new y({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L421"}}),Ry=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),Py=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('google/tapas-base-finetuned-wtq')
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),Sy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),$y=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq')

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained('google/tapas-base-finetuned-wtq', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/tapas_pt_model_config.json')
model = TFAutoModelForTableQuestionAnswering.from_pretrained('./pt_model/tapas_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;google/tapas-base-finetuned-wtq&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/tapas_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;./pt_model/tapas_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Iy=new Z({}),Dy=new y({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L432"}}),jy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Oy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),Gy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),qy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForTokenClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),zy=new Z({}),Xy=new y({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L414"}}),Vy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta_v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),Wy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),Hy=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Uy=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained('bert-base-cased')

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = TFAutoModelForQuestionAnswering.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Jy=new Z({}),Ky=new y({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L211"}}),Zy=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision_text_dual_encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),ew=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),ow=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),rw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModel.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModel.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),tw=new Z({}),aw=new y({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L225"}}),nw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
</ul>`,name:"config"}]}}),lw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),iw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),dw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForCausalLM.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),cw=new Z({}),mw=new y({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L218"}}),hw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),gw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),uw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),pw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForPreTraining.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),_w=new Z({}),bw=new y({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L232"}}),Tw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Fw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),Ew=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Cw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForMaskedLM.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Mw=new Z({}),yw=new y({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L239"}}),Aw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot_small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoderdecoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),Lw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('t5-base')
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),Bw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),xw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained('t5-base')

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained('t5-base', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/t5_pt_model_config.json')
model = FlaxAutoModelForSeq2SeqLM.from_pretrained('./pt_model/t5_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;t5-base&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/t5_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&#x27;./pt_model/t5_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),kw=new Z({}),Rw=new y({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L248"}}),Sw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),$w=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),Iw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Dw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForSequenceClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Nw=new Z({}),jw=new y({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),Gw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),qw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),zw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Xw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForQuestionAnswering.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Qw=new Z({}),Vw=new y({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L264"}}),Hw=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),Uw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),Jw=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Kw=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForTokenClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),Yw=new Z({}),Zw=new y({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L273"}}),o7=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),r7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),t7=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),a7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForMultipleChoice.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),s7=new Z({}),n7=new y({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L280"}}),i7=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),d7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),c7=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),m7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForNextSentencePrediction.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),f7=new Z({}),h7=new y({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L289"}}),u7=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),p7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),_7=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),v7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForImageClassification.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),T7=new Z({}),F7=new y({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),C7=new y({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L379",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visionencoderdecoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),M7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq
# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained('bert-base-cased')
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),y7=new y({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L407",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under
a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In
this case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided
as <code>config</code> argument. This loading path is slower than converting the PyTorch model in a
TensorFlow model using the provided conversion scripts and loading the TensorFlow model
afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded
by supplying the save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of
<code>kwargs</code> that corresponds to a configuration attribute will be used to override said attribute
with the supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration
attribute will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),w7=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained('bert-base-cased')

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained('bert-base-cased', output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained('./pt_model/bert_pt_model_config.json')
model = FlaxAutoModelForVision2Seq.from_pretrained('./pt_model/bert_pytorch_model.bin', from_pt=True, config=config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;bert-base-cased&#x27;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pt_model_config.json&#x27;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&#x27;./pt_model/bert_pytorch_model.bin&#x27;</span>, from_pt=<span class="hljs-literal">True</span>, config=config)`}}),{c(){re=a("meta"),Pe=l(),fe=a("h1"),ue=a("a"),ro=a("span"),m(ge.$$.fragment),Ee=l(),$o=a("span"),zl=o("Auto Classes"),Lc=l(),Ot=a("p"),Xl=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Ql=a("code"),pC=o("from_pretrained()"),Bc=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),Be=l(),ao=a("p"),Vl=o("Instantiating one of "),hs=a("a"),_C=o("AutoConfig"),gs=o(", "),us=a("a"),bC=o("AutoModel"),Wl=o(`, and
`),ps=a("a"),vC=o("AutoTokenizer"),Hl=o(" will directly create a class of the relevant architecture. For instance"),xc=l(),m(_a.$$.fragment),so=l(),pe=a("p"),T0=o("will create a model that is an instance of "),Ul=a("a"),F0=o("BertModel"),E0=o("."),Io=l(),ba=a("p"),C0=o("There is one class of "),kc=a("code"),M0=o("AutoModel"),e0e=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),a5e=l(),Jl=a("h2"),Rc=a("a"),BO=a("span"),m(TC.$$.fragment),o0e=l(),xO=a("span"),r0e=o("Extending the Auto Classes"),s5e=l(),_s=a("p"),t0e=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),kO=a("code"),a0e=o("NewModel"),s0e=o(", make sure you have a "),RO=a("code"),n0e=o("NewModelConfig"),l0e=o(` then you can add those to the auto
classes like this:`),n5e=l(),m(FC.$$.fragment),l5e=l(),y0=a("p"),i0e=o("You will then be able to use the auto classes like you would usually do!"),i5e=l(),m(Pc.$$.fragment),d5e=l(),Kl=a("h2"),Sc=a("a"),PO=a("span"),m(EC.$$.fragment),d0e=l(),SO=a("span"),c0e=o("AutoConfig"),c5e=l(),Do=a("div"),m(CC.$$.fragment),m0e=l(),MC=a("p"),f0e=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),w0=a("a"),h0e=o("from_pretrained()"),g0e=o(" class method."),u0e=l(),yC=a("p"),p0e=o("This class cannot be instantiated directly using "),$O=a("code"),_0e=o("__init__()"),b0e=o(" (throws an error)."),v0e=l(),no=a("div"),m(wC.$$.fragment),T0e=l(),IO=a("p"),F0e=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),E0e=l(),Yl=a("p"),C0e=o("The configuration class to instantiate is selected based on the "),DO=a("code"),M0e=o("model_type"),y0e=o(` property of the config object
that is loaded, or when it\u2019s missing, by falling back to using pattern matching on
`),NO=a("code"),w0e=o("pretrained_model_name_or_path"),A0e=o(":"),L0e=l(),v=a("ul"),$c=a("li"),jO=a("strong"),B0e=o("albert"),x0e=o(" \u2014 "),A0=a("a"),k0e=o("AlbertConfig"),R0e=o(" (ALBERT model)"),P0e=l(),Ic=a("li"),OO=a("strong"),S0e=o("bart"),$0e=o(" \u2014 "),L0=a("a"),I0e=o("BartConfig"),D0e=o(" (BART model)"),N0e=l(),Dc=a("li"),GO=a("strong"),j0e=o("beit"),O0e=o(" \u2014 "),B0=a("a"),G0e=o("BeitConfig"),q0e=o(" (BEiT model)"),z0e=l(),Nc=a("li"),qO=a("strong"),X0e=o("bert"),Q0e=o(" \u2014 "),x0=a("a"),V0e=o("BertConfig"),W0e=o(" (BERT model)"),H0e=l(),jc=a("li"),zO=a("strong"),U0e=o("bert-generation"),J0e=o(" \u2014 "),k0=a("a"),K0e=o("BertGenerationConfig"),Y0e=o(" (Bert Generation model)"),Z0e=l(),Oc=a("li"),XO=a("strong"),eAe=o("big_bird"),oAe=o(" \u2014 "),R0=a("a"),rAe=o("BigBirdConfig"),tAe=o(" (BigBird model)"),aAe=l(),Gc=a("li"),QO=a("strong"),sAe=o("bigbird_pegasus"),nAe=o(" \u2014 "),P0=a("a"),lAe=o("BigBirdPegasusConfig"),iAe=o(" (BigBirdPegasus model)"),dAe=l(),qc=a("li"),VO=a("strong"),cAe=o("blenderbot"),mAe=o(" \u2014 "),S0=a("a"),fAe=o("BlenderbotConfig"),hAe=o(" (Blenderbot model)"),gAe=l(),zc=a("li"),WO=a("strong"),uAe=o("blenderbot-small"),pAe=o(" \u2014 "),$0=a("a"),_Ae=o("BlenderbotSmallConfig"),bAe=o(" (BlenderbotSmall model)"),vAe=l(),Xc=a("li"),HO=a("strong"),TAe=o("camembert"),FAe=o(" \u2014 "),I0=a("a"),EAe=o("CamembertConfig"),CAe=o(" (CamemBERT model)"),MAe=l(),Qc=a("li"),UO=a("strong"),yAe=o("canine"),wAe=o(" \u2014 "),D0=a("a"),AAe=o("CanineConfig"),LAe=o(" (Canine model)"),BAe=l(),Vc=a("li"),JO=a("strong"),xAe=o("clip"),kAe=o(" \u2014 "),N0=a("a"),RAe=o("CLIPConfig"),PAe=o(" (CLIP model)"),SAe=l(),Wc=a("li"),KO=a("strong"),$Ae=o("convbert"),IAe=o(" \u2014 "),j0=a("a"),DAe=o("ConvBertConfig"),NAe=o(" (ConvBERT model)"),jAe=l(),Hc=a("li"),YO=a("strong"),OAe=o("ctrl"),GAe=o(" \u2014 "),O0=a("a"),qAe=o("CTRLConfig"),zAe=o(" (CTRL model)"),XAe=l(),Uc=a("li"),ZO=a("strong"),QAe=o("deberta"),VAe=o(" \u2014 "),G0=a("a"),WAe=o("DebertaConfig"),HAe=o(" (DeBERTa model)"),UAe=l(),Jc=a("li"),eG=a("strong"),JAe=o("deberta-v2"),KAe=o(" \u2014 "),q0=a("a"),YAe=o("DebertaV2Config"),ZAe=o(" (DeBERTa-v2 model)"),e6e=l(),Kc=a("li"),oG=a("strong"),o6e=o("deit"),r6e=o(" \u2014 "),z0=a("a"),t6e=o("DeiTConfig"),a6e=o(" (DeiT model)"),s6e=l(),Yc=a("li"),rG=a("strong"),n6e=o("detr"),l6e=o(" \u2014 "),X0=a("a"),i6e=o("DetrConfig"),d6e=o(" (DETR model)"),c6e=l(),Zc=a("li"),tG=a("strong"),m6e=o("distilbert"),f6e=o(" \u2014 "),Q0=a("a"),h6e=o("DistilBertConfig"),g6e=o(" (DistilBERT model)"),u6e=l(),em=a("li"),aG=a("strong"),p6e=o("dpr"),_6e=o(" \u2014 "),V0=a("a"),b6e=o("DPRConfig"),v6e=o(" (DPR model)"),T6e=l(),om=a("li"),sG=a("strong"),F6e=o("electra"),E6e=o(" \u2014 "),W0=a("a"),C6e=o("ElectraConfig"),M6e=o(" (ELECTRA model)"),y6e=l(),rm=a("li"),nG=a("strong"),w6e=o("encoder-decoder"),A6e=o(" \u2014 "),H0=a("a"),L6e=o("EncoderDecoderConfig"),B6e=o(" (Encoder decoder model)"),x6e=l(),tm=a("li"),lG=a("strong"),k6e=o("flaubert"),R6e=o(" \u2014 "),U0=a("a"),P6e=o("FlaubertConfig"),S6e=o(" (FlauBERT model)"),$6e=l(),am=a("li"),iG=a("strong"),I6e=o("fnet"),D6e=o(" \u2014 "),J0=a("a"),N6e=o("FNetConfig"),j6e=o(" (FNet model)"),O6e=l(),sm=a("li"),dG=a("strong"),G6e=o("fsmt"),q6e=o(" \u2014 "),K0=a("a"),z6e=o("FSMTConfig"),X6e=o(" (FairSeq Machine-Translation model)"),Q6e=l(),nm=a("li"),cG=a("strong"),V6e=o("funnel"),W6e=o(" \u2014 "),Y0=a("a"),H6e=o("FunnelConfig"),U6e=o(" (Funnel Transformer model)"),J6e=l(),lm=a("li"),mG=a("strong"),K6e=o("gpt2"),Y6e=o(" \u2014 "),Z0=a("a"),Z6e=o("GPT2Config"),eLe=o(" (OpenAI GPT-2 model)"),oLe=l(),im=a("li"),fG=a("strong"),rLe=o("gpt_neo"),tLe=o(" \u2014 "),eA=a("a"),aLe=o("GPTNeoConfig"),sLe=o(" (GPT Neo model)"),nLe=l(),dm=a("li"),hG=a("strong"),lLe=o("gptj"),iLe=o(" \u2014 "),oA=a("a"),dLe=o("GPTJConfig"),cLe=o(" (GPT-J model)"),mLe=l(),cm=a("li"),gG=a("strong"),fLe=o("hubert"),hLe=o(" \u2014 "),rA=a("a"),gLe=o("HubertConfig"),uLe=o(" (Hubert model)"),pLe=l(),mm=a("li"),uG=a("strong"),_Le=o("ibert"),bLe=o(" \u2014 "),tA=a("a"),vLe=o("IBertConfig"),TLe=o(" (I-BERT model)"),FLe=l(),fm=a("li"),pG=a("strong"),ELe=o("imagegpt"),CLe=o(" \u2014 "),aA=a("a"),MLe=o("ImageGPTConfig"),yLe=o(" (ImageGPT model)"),wLe=l(),hm=a("li"),_G=a("strong"),ALe=o("layoutlm"),LLe=o(" \u2014 "),sA=a("a"),BLe=o("LayoutLMConfig"),xLe=o(" (LayoutLM model)"),kLe=l(),gm=a("li"),bG=a("strong"),RLe=o("layoutlmv2"),PLe=o(" \u2014 "),nA=a("a"),SLe=o("LayoutLMv2Config"),$Le=o(" (LayoutLMv2 model)"),ILe=l(),um=a("li"),vG=a("strong"),DLe=o("led"),NLe=o(" \u2014 "),lA=a("a"),jLe=o("LEDConfig"),OLe=o(" (LED model)"),GLe=l(),pm=a("li"),TG=a("strong"),qLe=o("longformer"),zLe=o(" \u2014 "),iA=a("a"),XLe=o("LongformerConfig"),QLe=o(" (Longformer model)"),VLe=l(),_m=a("li"),FG=a("strong"),WLe=o("luke"),HLe=o(" \u2014 "),dA=a("a"),ULe=o("LukeConfig"),JLe=o(" (LUKE model)"),KLe=l(),bm=a("li"),EG=a("strong"),YLe=o("lxmert"),ZLe=o(" \u2014 "),cA=a("a"),e8e=o("LxmertConfig"),o8e=o(" (LXMERT model)"),r8e=l(),vm=a("li"),CG=a("strong"),t8e=o("m2m_100"),a8e=o(" \u2014 "),mA=a("a"),s8e=o("M2M100Config"),n8e=o(" (M2M100 model)"),l8e=l(),Tm=a("li"),MG=a("strong"),i8e=o("marian"),d8e=o(" \u2014 "),fA=a("a"),c8e=o("MarianConfig"),m8e=o(" (Marian model)"),f8e=l(),Fm=a("li"),yG=a("strong"),h8e=o("mbart"),g8e=o(" \u2014 "),hA=a("a"),u8e=o("MBartConfig"),p8e=o(" (mBART model)"),_8e=l(),Em=a("li"),wG=a("strong"),b8e=o("megatron-bert"),v8e=o(" \u2014 "),gA=a("a"),T8e=o("MegatronBertConfig"),F8e=o(" (MegatronBert model)"),E8e=l(),Cm=a("li"),AG=a("strong"),C8e=o("mobilebert"),M8e=o(" \u2014 "),uA=a("a"),y8e=o("MobileBertConfig"),w8e=o(" (MobileBERT model)"),A8e=l(),Mm=a("li"),LG=a("strong"),L8e=o("mpnet"),B8e=o(" \u2014 "),pA=a("a"),x8e=o("MPNetConfig"),k8e=o(" (MPNet model)"),R8e=l(),ym=a("li"),BG=a("strong"),P8e=o("mt5"),S8e=o(" \u2014 "),_A=a("a"),$8e=o("MT5Config"),I8e=o(" (mT5 model)"),D8e=l(),wm=a("li"),xG=a("strong"),N8e=o("openai-gpt"),j8e=o(" \u2014 "),bA=a("a"),O8e=o("OpenAIGPTConfig"),G8e=o(" (OpenAI GPT model)"),q8e=l(),Am=a("li"),kG=a("strong"),z8e=o("pegasus"),X8e=o(" \u2014 "),vA=a("a"),Q8e=o("PegasusConfig"),V8e=o(" (Pegasus model)"),W8e=l(),Lm=a("li"),RG=a("strong"),H8e=o("perceiver"),U8e=o(" \u2014 "),TA=a("a"),J8e=o("PerceiverConfig"),K8e=o(" (Perceiver model)"),Y8e=l(),Bm=a("li"),PG=a("strong"),Z8e=o("prophetnet"),eBe=o(" \u2014 "),FA=a("a"),oBe=o("ProphetNetConfig"),rBe=o(" (ProphetNet model)"),tBe=l(),xm=a("li"),SG=a("strong"),aBe=o("qdqbert"),sBe=o(" \u2014 "),EA=a("a"),nBe=o("QDQBertConfig"),lBe=o(" (QDQBert model)"),iBe=l(),km=a("li"),$G=a("strong"),dBe=o("rag"),cBe=o(" \u2014 "),CA=a("a"),mBe=o("RagConfig"),fBe=o(" (RAG model)"),hBe=l(),Rm=a("li"),IG=a("strong"),gBe=o("reformer"),uBe=o(" \u2014 "),MA=a("a"),pBe=o("ReformerConfig"),_Be=o(" (Reformer model)"),bBe=l(),Pm=a("li"),DG=a("strong"),vBe=o("rembert"),TBe=o(" \u2014 "),yA=a("a"),FBe=o("RemBertConfig"),EBe=o(" (RemBERT model)"),CBe=l(),Sm=a("li"),NG=a("strong"),MBe=o("retribert"),yBe=o(" \u2014 "),wA=a("a"),wBe=o("RetriBertConfig"),ABe=o(" (RetriBERT model)"),LBe=l(),$m=a("li"),jG=a("strong"),BBe=o("roberta"),xBe=o(" \u2014 "),AA=a("a"),kBe=o("RobertaConfig"),RBe=o(" (RoBERTa model)"),PBe=l(),Im=a("li"),OG=a("strong"),SBe=o("roformer"),$Be=o(" \u2014 "),LA=a("a"),IBe=o("RoFormerConfig"),DBe=o(" (RoFormer model)"),NBe=l(),Dm=a("li"),GG=a("strong"),jBe=o("segformer"),OBe=o(" \u2014 "),BA=a("a"),GBe=o("SegformerConfig"),qBe=o(" (SegFormer model)"),zBe=l(),Nm=a("li"),qG=a("strong"),XBe=o("sew"),QBe=o(" \u2014 "),xA=a("a"),VBe=o("SEWConfig"),WBe=o(" (SEW model)"),HBe=l(),jm=a("li"),zG=a("strong"),UBe=o("sew-d"),JBe=o(" \u2014 "),kA=a("a"),KBe=o("SEWDConfig"),YBe=o(" (SEW-D model)"),ZBe=l(),Om=a("li"),XG=a("strong"),e9e=o("speech-encoder-decoder"),o9e=o(" \u2014 "),RA=a("a"),r9e=o("SpeechEncoderDecoderConfig"),t9e=o(" (Speech Encoder decoder model)"),a9e=l(),Gm=a("li"),QG=a("strong"),s9e=o("speech_to_text"),n9e=o(" \u2014 "),PA=a("a"),l9e=o("Speech2TextConfig"),i9e=o(" (Speech2Text model)"),d9e=l(),qm=a("li"),VG=a("strong"),c9e=o("speech_to_text_2"),m9e=o(" \u2014 "),SA=a("a"),f9e=o("Speech2Text2Config"),h9e=o(" (Speech2Text2 model)"),g9e=l(),zm=a("li"),WG=a("strong"),u9e=o("splinter"),p9e=o(" \u2014 "),$A=a("a"),_9e=o("SplinterConfig"),b9e=o(" (Splinter model)"),v9e=l(),Xm=a("li"),HG=a("strong"),T9e=o("squeezebert"),F9e=o(" \u2014 "),IA=a("a"),E9e=o("SqueezeBertConfig"),C9e=o(" (SqueezeBERT model)"),M9e=l(),Qm=a("li"),UG=a("strong"),y9e=o("t5"),w9e=o(" \u2014 "),DA=a("a"),A9e=o("T5Config"),L9e=o(" (T5 model)"),B9e=l(),Vm=a("li"),JG=a("strong"),x9e=o("tapas"),k9e=o(" \u2014 "),NA=a("a"),R9e=o("TapasConfig"),P9e=o(" (TAPAS model)"),S9e=l(),Wm=a("li"),KG=a("strong"),$9e=o("transfo-xl"),I9e=o(" \u2014 "),jA=a("a"),D9e=o("TransfoXLConfig"),N9e=o(" (Transformer-XL model)"),j9e=l(),Hm=a("li"),YG=a("strong"),O9e=o("trocr"),G9e=o(" \u2014 "),OA=a("a"),q9e=o("TrOCRConfig"),z9e=o(" (TrOCR model)"),X9e=l(),Um=a("li"),ZG=a("strong"),Q9e=o("unispeech"),V9e=o(" \u2014 "),GA=a("a"),W9e=o("UniSpeechConfig"),H9e=o(" (UniSpeech model)"),U9e=l(),Jm=a("li"),eq=a("strong"),J9e=o("unispeech-sat"),K9e=o(" \u2014 "),qA=a("a"),Y9e=o("UniSpeechSatConfig"),Z9e=o(" (UniSpeechSat model)"),exe=l(),Km=a("li"),oq=a("strong"),oxe=o("vision-encoder-decoder"),rxe=o(" \u2014 "),zA=a("a"),txe=o("VisionEncoderDecoderConfig"),axe=o(" (Vision Encoder decoder model)"),sxe=l(),Ym=a("li"),rq=a("strong"),nxe=o("vision-text-dual-encoder"),lxe=o(" \u2014 "),XA=a("a"),ixe=o("VisionTextDualEncoderConfig"),dxe=o(" (VisionTextDualEncoder model)"),cxe=l(),Zm=a("li"),tq=a("strong"),mxe=o("visual_bert"),fxe=o(" \u2014 "),QA=a("a"),hxe=o("VisualBertConfig"),gxe=o(" (VisualBert model)"),uxe=l(),ef=a("li"),aq=a("strong"),pxe=o("vit"),_xe=o(" \u2014 "),VA=a("a"),bxe=o("ViTConfig"),vxe=o(" (ViT model)"),Txe=l(),of=a("li"),sq=a("strong"),Fxe=o("wav2vec2"),Exe=o(" \u2014 "),WA=a("a"),Cxe=o("Wav2Vec2Config"),Mxe=o(" (Wav2Vec2 model)"),yxe=l(),rf=a("li"),nq=a("strong"),wxe=o("xlm"),Axe=o(" \u2014 "),HA=a("a"),Lxe=o("XLMConfig"),Bxe=o(" (XLM model)"),xxe=l(),tf=a("li"),lq=a("strong"),kxe=o("xlm-prophetnet"),Rxe=o(" \u2014 "),UA=a("a"),Pxe=o("XLMProphetNetConfig"),Sxe=o(" (XLMProphetNet model)"),$xe=l(),af=a("li"),iq=a("strong"),Ixe=o("xlm-roberta"),Dxe=o(" \u2014 "),JA=a("a"),Nxe=o("XLMRobertaConfig"),jxe=o(" (XLM-RoBERTa model)"),Oxe=l(),sf=a("li"),dq=a("strong"),Gxe=o("xlnet"),qxe=o(" \u2014 "),KA=a("a"),zxe=o("XLNetConfig"),Xxe=o(" (XLNet model)"),Qxe=l(),cq=a("p"),Vxe=o("Examples:"),Wxe=l(),m(AC.$$.fragment),Hxe=l(),nf=a("div"),m(LC.$$.fragment),Uxe=l(),mq=a("p"),Jxe=o("Register a new configuration for this class."),m5e=l(),Zl=a("h2"),lf=a("a"),fq=a("span"),m(BC.$$.fragment),Kxe=l(),hq=a("span"),Yxe=o("AutoTokenizer"),f5e=l(),No=a("div"),m(xC.$$.fragment),Zxe=l(),kC=a("p"),eke=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),YA=a("a"),oke=o("AutoTokenizer.from_pretrained()"),rke=o(" class method."),tke=l(),RC=a("p"),ake=o("This class cannot be instantiated directly using "),gq=a("code"),ske=o("__init__()"),nke=o(" (throws an error)."),lke=l(),ye=a("div"),m(PC.$$.fragment),ike=l(),uq=a("p"),dke=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),cke=l(),va=a("p"),mke=o("The tokenizer class to instantiate is selected based on the "),pq=a("code"),fke=o("model_type"),hke=o(` property of the config object
(either passed as an argument or loaded from `),_q=a("code"),gke=o("pretrained_model_name_or_path"),uke=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),bq=a("code"),pke=o("pretrained_model_name_or_path"),_ke=o(":"),bke=l(),C=a("ul"),bs=a("li"),vq=a("strong"),vke=o("albert"),Tke=o(" \u2014 "),ZA=a("a"),Fke=o("AlbertTokenizer"),Eke=o(" or "),e6=a("a"),Cke=o("AlbertTokenizerFast"),Mke=o(" (ALBERT model)"),yke=l(),vs=a("li"),Tq=a("strong"),wke=o("bart"),Ake=o(" \u2014 "),o6=a("a"),Lke=o("BartTokenizer"),Bke=o(" or "),r6=a("a"),xke=o("BartTokenizerFast"),kke=o(" (BART model)"),Rke=l(),Ts=a("li"),Fq=a("strong"),Pke=o("barthez"),Ske=o(" \u2014 "),t6=a("a"),$ke=o("BarthezTokenizer"),Ike=o(" or "),a6=a("a"),Dke=o("BarthezTokenizerFast"),Nke=o(" (BARThez model)"),jke=l(),df=a("li"),Eq=a("strong"),Oke=o("bartpho"),Gke=o(" \u2014 "),s6=a("a"),qke=o("BartphoTokenizer"),zke=o(" (BARTpho model)"),Xke=l(),Fs=a("li"),Cq=a("strong"),Qke=o("bert"),Vke=o(" \u2014 "),n6=a("a"),Wke=o("BertTokenizer"),Hke=o(" or "),l6=a("a"),Uke=o("BertTokenizerFast"),Jke=o(" (BERT model)"),Kke=l(),cf=a("li"),Mq=a("strong"),Yke=o("bert-generation"),Zke=o(" \u2014 "),i6=a("a"),eRe=o("BertGenerationTokenizer"),oRe=o(" (Bert Generation model)"),rRe=l(),mf=a("li"),yq=a("strong"),tRe=o("bert-japanese"),aRe=o(" \u2014 "),d6=a("a"),sRe=o("BertJapaneseTokenizer"),nRe=o(" (BertJapanese model)"),lRe=l(),ff=a("li"),wq=a("strong"),iRe=o("bertweet"),dRe=o(" \u2014 "),c6=a("a"),cRe=o("BertweetTokenizer"),mRe=o(" (Bertweet model)"),fRe=l(),Es=a("li"),Aq=a("strong"),hRe=o("big_bird"),gRe=o(" \u2014 "),m6=a("a"),uRe=o("BigBirdTokenizer"),pRe=o(" or "),f6=a("a"),_Re=o("BigBirdTokenizerFast"),bRe=o(" (BigBird model)"),vRe=l(),Cs=a("li"),Lq=a("strong"),TRe=o("bigbird_pegasus"),FRe=o(" \u2014 "),h6=a("a"),ERe=o("PegasusTokenizer"),CRe=o(" or "),g6=a("a"),MRe=o("PegasusTokenizerFast"),yRe=o(" (BigBirdPegasus model)"),wRe=l(),Ms=a("li"),Bq=a("strong"),ARe=o("blenderbot"),LRe=o(" \u2014 "),u6=a("a"),BRe=o("BlenderbotTokenizer"),xRe=o(" or "),p6=a("a"),kRe=o("BlenderbotTokenizerFast"),RRe=o(" (Blenderbot model)"),PRe=l(),hf=a("li"),xq=a("strong"),SRe=o("blenderbot-small"),$Re=o(" \u2014 "),_6=a("a"),IRe=o("BlenderbotSmallTokenizer"),DRe=o(" (BlenderbotSmall model)"),NRe=l(),gf=a("li"),kq=a("strong"),jRe=o("byt5"),ORe=o(" \u2014 "),b6=a("a"),GRe=o("ByT5Tokenizer"),qRe=o(" (ByT5 model)"),zRe=l(),ys=a("li"),Rq=a("strong"),XRe=o("camembert"),QRe=o(" \u2014 "),v6=a("a"),VRe=o("CamembertTokenizer"),WRe=o(" or "),T6=a("a"),HRe=o("CamembertTokenizerFast"),URe=o(" (CamemBERT model)"),JRe=l(),uf=a("li"),Pq=a("strong"),KRe=o("canine"),YRe=o(" \u2014 "),F6=a("a"),ZRe=o("CanineTokenizer"),ePe=o(" (Canine model)"),oPe=l(),ws=a("li"),Sq=a("strong"),rPe=o("clip"),tPe=o(" \u2014 "),E6=a("a"),aPe=o("CLIPTokenizer"),sPe=o(" or "),C6=a("a"),nPe=o("CLIPTokenizerFast"),lPe=o(" (CLIP model)"),iPe=l(),As=a("li"),$q=a("strong"),dPe=o("convbert"),cPe=o(" \u2014 "),M6=a("a"),mPe=o("ConvBertTokenizer"),fPe=o(" or "),y6=a("a"),hPe=o("ConvBertTokenizerFast"),gPe=o(" (ConvBERT model)"),uPe=l(),Ls=a("li"),Iq=a("strong"),pPe=o("cpm"),_Pe=o(" \u2014 "),w6=a("a"),bPe=o("CpmTokenizer"),vPe=o(" or "),Dq=a("code"),TPe=o("CpmTokenizerFast"),FPe=o(" (CPM model)"),EPe=l(),pf=a("li"),Nq=a("strong"),CPe=o("ctrl"),MPe=o(" \u2014 "),A6=a("a"),yPe=o("CTRLTokenizer"),wPe=o(" (CTRL model)"),APe=l(),Bs=a("li"),jq=a("strong"),LPe=o("deberta"),BPe=o(" \u2014 "),L6=a("a"),xPe=o("DebertaTokenizer"),kPe=o(" or "),B6=a("a"),RPe=o("DebertaTokenizerFast"),PPe=o(" (DeBERTa model)"),SPe=l(),_f=a("li"),Oq=a("strong"),$Pe=o("deberta-v2"),IPe=o(" \u2014 "),x6=a("a"),DPe=o("DebertaV2Tokenizer"),NPe=o(" (DeBERTa-v2 model)"),jPe=l(),xs=a("li"),Gq=a("strong"),OPe=o("distilbert"),GPe=o(" \u2014 "),k6=a("a"),qPe=o("DistilBertTokenizer"),zPe=o(" or "),R6=a("a"),XPe=o("DistilBertTokenizerFast"),QPe=o(" (DistilBERT model)"),VPe=l(),ks=a("li"),qq=a("strong"),WPe=o("dpr"),HPe=o(" \u2014 "),P6=a("a"),UPe=o("DPRQuestionEncoderTokenizer"),JPe=o(" or "),S6=a("a"),KPe=o("DPRQuestionEncoderTokenizerFast"),YPe=o(" (DPR model)"),ZPe=l(),Rs=a("li"),zq=a("strong"),eSe=o("electra"),oSe=o(" \u2014 "),$6=a("a"),rSe=o("ElectraTokenizer"),tSe=o(" or "),I6=a("a"),aSe=o("ElectraTokenizerFast"),sSe=o(" (ELECTRA model)"),nSe=l(),bf=a("li"),Xq=a("strong"),lSe=o("flaubert"),iSe=o(" \u2014 "),D6=a("a"),dSe=o("FlaubertTokenizer"),cSe=o(" (FlauBERT model)"),mSe=l(),Ps=a("li"),Qq=a("strong"),fSe=o("fnet"),hSe=o(" \u2014 "),N6=a("a"),gSe=o("FNetTokenizer"),uSe=o(" or "),j6=a("a"),pSe=o("FNetTokenizerFast"),_Se=o(" (FNet model)"),bSe=l(),vf=a("li"),Vq=a("strong"),vSe=o("fsmt"),TSe=o(" \u2014 "),O6=a("a"),FSe=o("FSMTTokenizer"),ESe=o(" (FairSeq Machine-Translation model)"),CSe=l(),Ss=a("li"),Wq=a("strong"),MSe=o("funnel"),ySe=o(" \u2014 "),G6=a("a"),wSe=o("FunnelTokenizer"),ASe=o(" or "),q6=a("a"),LSe=o("FunnelTokenizerFast"),BSe=o(" (Funnel Transformer model)"),xSe=l(),$s=a("li"),Hq=a("strong"),kSe=o("gpt2"),RSe=o(" \u2014 "),z6=a("a"),PSe=o("GPT2Tokenizer"),SSe=o(" or "),X6=a("a"),$Se=o("GPT2TokenizerFast"),ISe=o(" (OpenAI GPT-2 model)"),DSe=l(),Is=a("li"),Uq=a("strong"),NSe=o("gpt_neo"),jSe=o(" \u2014 "),Q6=a("a"),OSe=o("GPT2Tokenizer"),GSe=o(" or "),V6=a("a"),qSe=o("GPT2TokenizerFast"),zSe=o(" (GPT Neo model)"),XSe=l(),Tf=a("li"),Jq=a("strong"),QSe=o("hubert"),VSe=o(" \u2014 "),W6=a("a"),WSe=o("Wav2Vec2CTCTokenizer"),HSe=o(" (Hubert model)"),USe=l(),Ds=a("li"),Kq=a("strong"),JSe=o("ibert"),KSe=o(" \u2014 "),H6=a("a"),YSe=o("RobertaTokenizer"),ZSe=o(" or "),U6=a("a"),e$e=o("RobertaTokenizerFast"),o$e=o(" (I-BERT model)"),r$e=l(),Ns=a("li"),Yq=a("strong"),t$e=o("layoutlm"),a$e=o(" \u2014 "),J6=a("a"),s$e=o("LayoutLMTokenizer"),n$e=o(" or "),K6=a("a"),l$e=o("LayoutLMTokenizerFast"),i$e=o(" (LayoutLM model)"),d$e=l(),js=a("li"),Zq=a("strong"),c$e=o("layoutlmv2"),m$e=o(" \u2014 "),Y6=a("a"),f$e=o("LayoutLMv2Tokenizer"),h$e=o(" or "),Z6=a("a"),g$e=o("LayoutLMv2TokenizerFast"),u$e=o(" (LayoutLMv2 model)"),p$e=l(),Os=a("li"),ez=a("strong"),_$e=o("led"),b$e=o(" \u2014 "),eL=a("a"),v$e=o("LEDTokenizer"),T$e=o(" or "),oL=a("a"),F$e=o("LEDTokenizerFast"),E$e=o(" (LED model)"),C$e=l(),Gs=a("li"),oz=a("strong"),M$e=o("longformer"),y$e=o(" \u2014 "),rL=a("a"),w$e=o("LongformerTokenizer"),A$e=o(" or "),tL=a("a"),L$e=o("LongformerTokenizerFast"),B$e=o(" (Longformer model)"),x$e=l(),Ff=a("li"),rz=a("strong"),k$e=o("luke"),R$e=o(" \u2014 "),aL=a("a"),P$e=o("LukeTokenizer"),S$e=o(" (LUKE model)"),$$e=l(),qs=a("li"),tz=a("strong"),I$e=o("lxmert"),D$e=o(" \u2014 "),sL=a("a"),N$e=o("LxmertTokenizer"),j$e=o(" or "),nL=a("a"),O$e=o("LxmertTokenizerFast"),G$e=o(" (LXMERT model)"),q$e=l(),Ef=a("li"),az=a("strong"),z$e=o("m2m_100"),X$e=o(" \u2014 "),lL=a("a"),Q$e=o("M2M100Tokenizer"),V$e=o(" (M2M100 model)"),W$e=l(),Cf=a("li"),sz=a("strong"),H$e=o("marian"),U$e=o(" \u2014 "),iL=a("a"),J$e=o("MarianTokenizer"),K$e=o(" (Marian model)"),Y$e=l(),zs=a("li"),nz=a("strong"),Z$e=o("mbart"),eIe=o(" \u2014 "),dL=a("a"),oIe=o("MBartTokenizer"),rIe=o(" or "),cL=a("a"),tIe=o("MBartTokenizerFast"),aIe=o(" (mBART model)"),sIe=l(),Xs=a("li"),lz=a("strong"),nIe=o("mbart50"),lIe=o(" \u2014 "),mL=a("a"),iIe=o("MBart50Tokenizer"),dIe=o(" or "),fL=a("a"),cIe=o("MBart50TokenizerFast"),mIe=o(" (mBART-50 model)"),fIe=l(),Qs=a("li"),iz=a("strong"),hIe=o("mobilebert"),gIe=o(" \u2014 "),hL=a("a"),uIe=o("MobileBertTokenizer"),pIe=o(" or "),gL=a("a"),_Ie=o("MobileBertTokenizerFast"),bIe=o(" (MobileBERT model)"),vIe=l(),Vs=a("li"),dz=a("strong"),TIe=o("mpnet"),FIe=o(" \u2014 "),uL=a("a"),EIe=o("MPNetTokenizer"),CIe=o(" or "),pL=a("a"),MIe=o("MPNetTokenizerFast"),yIe=o(" (MPNet model)"),wIe=l(),Ws=a("li"),cz=a("strong"),AIe=o("mt5"),LIe=o(" \u2014 "),_L=a("a"),BIe=o("MT5Tokenizer"),xIe=o(" or "),bL=a("a"),kIe=o("MT5TokenizerFast"),RIe=o(" (mT5 model)"),PIe=l(),Hs=a("li"),mz=a("strong"),SIe=o("openai-gpt"),$Ie=o(" \u2014 "),vL=a("a"),IIe=o("OpenAIGPTTokenizer"),DIe=o(" or "),TL=a("a"),NIe=o("OpenAIGPTTokenizerFast"),jIe=o(" (OpenAI GPT model)"),OIe=l(),Us=a("li"),fz=a("strong"),GIe=o("pegasus"),qIe=o(" \u2014 "),FL=a("a"),zIe=o("PegasusTokenizer"),XIe=o(" or "),EL=a("a"),QIe=o("PegasusTokenizerFast"),VIe=o(" (Pegasus model)"),WIe=l(),Mf=a("li"),hz=a("strong"),HIe=o("phobert"),UIe=o(" \u2014 "),CL=a("a"),JIe=o("PhobertTokenizer"),KIe=o(" (PhoBERT model)"),YIe=l(),yf=a("li"),gz=a("strong"),ZIe=o("prophetnet"),eDe=o(" \u2014 "),ML=a("a"),oDe=o("ProphetNetTokenizer"),rDe=o(" (ProphetNet model)"),tDe=l(),Js=a("li"),uz=a("strong"),aDe=o("qdqbert"),sDe=o(" \u2014 "),yL=a("a"),nDe=o("BertTokenizer"),lDe=o(" or "),wL=a("a"),iDe=o("BertTokenizerFast"),dDe=o(" (QDQBert model)"),cDe=l(),wf=a("li"),pz=a("strong"),mDe=o("rag"),fDe=o(" \u2014 "),AL=a("a"),hDe=o("RagTokenizer"),gDe=o(" (RAG model)"),uDe=l(),Ks=a("li"),_z=a("strong"),pDe=o("reformer"),_De=o(" \u2014 "),LL=a("a"),bDe=o("ReformerTokenizer"),vDe=o(" or "),BL=a("a"),TDe=o("ReformerTokenizerFast"),FDe=o(" (Reformer model)"),EDe=l(),Ys=a("li"),bz=a("strong"),CDe=o("rembert"),MDe=o(" \u2014 "),xL=a("a"),yDe=o("RemBertTokenizer"),wDe=o(" or "),kL=a("a"),ADe=o("RemBertTokenizerFast"),LDe=o(" (RemBERT model)"),BDe=l(),Zs=a("li"),vz=a("strong"),xDe=o("retribert"),kDe=o(" \u2014 "),RL=a("a"),RDe=o("RetriBertTokenizer"),PDe=o(" or "),PL=a("a"),SDe=o("RetriBertTokenizerFast"),$De=o(" (RetriBERT model)"),IDe=l(),en=a("li"),Tz=a("strong"),DDe=o("roberta"),NDe=o(" \u2014 "),SL=a("a"),jDe=o("RobertaTokenizer"),ODe=o(" or "),$L=a("a"),GDe=o("RobertaTokenizerFast"),qDe=o(" (RoBERTa model)"),zDe=l(),on=a("li"),Fz=a("strong"),XDe=o("roformer"),QDe=o(" \u2014 "),IL=a("a"),VDe=o("RoFormerTokenizer"),WDe=o(" or "),DL=a("a"),HDe=o("RoFormerTokenizerFast"),UDe=o(" (RoFormer model)"),JDe=l(),Af=a("li"),Ez=a("strong"),KDe=o("speech_to_text"),YDe=o(" \u2014 "),NL=a("a"),ZDe=o("Speech2TextTokenizer"),eNe=o(" (Speech2Text model)"),oNe=l(),Lf=a("li"),Cz=a("strong"),rNe=o("speech_to_text_2"),tNe=o(" \u2014 "),jL=a("a"),aNe=o("Speech2Text2Tokenizer"),sNe=o(" (Speech2Text2 model)"),nNe=l(),rn=a("li"),Mz=a("strong"),lNe=o("splinter"),iNe=o(" \u2014 "),OL=a("a"),dNe=o("SplinterTokenizer"),cNe=o(" or "),GL=a("a"),mNe=o("SplinterTokenizerFast"),fNe=o(" (Splinter model)"),hNe=l(),tn=a("li"),yz=a("strong"),gNe=o("squeezebert"),uNe=o(" \u2014 "),qL=a("a"),pNe=o("SqueezeBertTokenizer"),_Ne=o(" or "),zL=a("a"),bNe=o("SqueezeBertTokenizerFast"),vNe=o(" (SqueezeBERT model)"),TNe=l(),an=a("li"),wz=a("strong"),FNe=o("t5"),ENe=o(" \u2014 "),XL=a("a"),CNe=o("T5Tokenizer"),MNe=o(" or "),QL=a("a"),yNe=o("T5TokenizerFast"),wNe=o(" (T5 model)"),ANe=l(),Bf=a("li"),Az=a("strong"),LNe=o("tapas"),BNe=o(" \u2014 "),VL=a("a"),xNe=o("TapasTokenizer"),kNe=o(" (TAPAS model)"),RNe=l(),xf=a("li"),Lz=a("strong"),PNe=o("transfo-xl"),SNe=o(" \u2014 "),WL=a("a"),$Ne=o("TransfoXLTokenizer"),INe=o(" (Transformer-XL model)"),DNe=l(),kf=a("li"),Bz=a("strong"),NNe=o("wav2vec2"),jNe=o(" \u2014 "),HL=a("a"),ONe=o("Wav2Vec2CTCTokenizer"),GNe=o(" (Wav2Vec2 model)"),qNe=l(),Rf=a("li"),xz=a("strong"),zNe=o("xlm"),XNe=o(" \u2014 "),UL=a("a"),QNe=o("XLMTokenizer"),VNe=o(" (XLM model)"),WNe=l(),Pf=a("li"),kz=a("strong"),HNe=o("xlm-prophetnet"),UNe=o(" \u2014 "),JL=a("a"),JNe=o("XLMProphetNetTokenizer"),KNe=o(" (XLMProphetNet model)"),YNe=l(),sn=a("li"),Rz=a("strong"),ZNe=o("xlm-roberta"),eje=o(" \u2014 "),KL=a("a"),oje=o("XLMRobertaTokenizer"),rje=o(" or "),YL=a("a"),tje=o("XLMRobertaTokenizerFast"),aje=o(" (XLM-RoBERTa model)"),sje=l(),nn=a("li"),Pz=a("strong"),nje=o("xlnet"),lje=o(" \u2014 "),ZL=a("a"),ije=o("XLNetTokenizer"),dje=o(" or "),e8=a("a"),cje=o("XLNetTokenizerFast"),mje=o(" (XLNet model)"),fje=l(),ei=a("p"),hje=o(`Params:
pretrained_model_name_or_path (`),Sz=a("code"),gje=o("str"),uje=o(" or "),$z=a("code"),pje=o("os.PathLike"),_je=o(`):
Can be either:`),bje=l(),oi=a("ul"),Ta=a("li"),vje=o("A string, the "),Iz=a("em"),Tje=o("model id"),Fje=o(` of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Dz=a("code"),Eje=o("bert-base-uncased"),Cje=o(`, or namespaced under
a user or organization name, like `),Nz=a("code"),Mje=o("dbmdz/bert-base-german-cased"),yje=o("."),wje=l(),Fa=a("li"),Aje=o("A path to a "),jz=a("em"),Lje=o("directory"),Bje=o(` containing vocabulary files required by the tokenizer, for instance saved
using the `),o8=a("a"),xje=o("save_pretrained()"),kje=o(` method, e.g.,
`),Oz=a("code"),Rje=o("./my_model_directory/"),Pje=o("."),Sje=l(),k=a("li"),$je=o(`A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: `),Gz=a("code"),Ije=o("./my_model_directory/vocab.txt"),Dje=o(`. (Not
applicable to all derived classes)
inputs (additional positional arguments, `),qz=a("em"),Nje=o("optional"),jje=o(`):
Will be passed along to the Tokenizer `),zz=a("code"),Oje=o("__init__()"),Gje=o(` method.
config (`),r8=a("a"),qje=o("PretrainedConfig"),zje=o(", "),Xz=a("em"),Xje=o("optional"),Qje=o(`)
The configuration object used to dertermine the tokenizer class to instantiate.
cache`),ri=a("em"),Vje=o("dir ("),Qz=a("code"),Wje=o("str"),Hje=o(" or "),Vz=a("code"),Uje=o("os.PathLike"),Jje=o(", _optional"),Kje=o(`):
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.
force`),SC=a("em"),Yje=o("download ("),Wz=a("code"),Zje=o("bool"),eOe=o(", _optional"),oOe=o(", defaults to "),Hz=a("code"),rOe=o("False"),tOe=o(`):
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.
resume`),$C=a("em"),aOe=o("download ("),Uz=a("code"),sOe=o("bool"),nOe=o(", _optional"),lOe=o(", defaults to "),Jz=a("code"),iOe=o("False"),dOe=o(`):
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.
proxies (`),Kz=a("code"),cOe=o("Dict[str, str]"),mOe=o(", "),Yz=a("em"),fOe=o("optional"),hOe=o(`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),Zz=a("code"),gOe=o("{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}"),uOe=o(`. The proxies are used on each request.
revision(`),eX=a("code"),pOe=o("str"),_Oe=o(", "),oX=a("em"),bOe=o("optional"),vOe=o(", defaults to "),rX=a("code"),TOe=o('"main"'),FOe=o(`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),tX=a("code"),EOe=o("revision"),COe=o(` can be any
identifier allowed by git.
subfolder (`),aX=a("code"),MOe=o("str"),yOe=o(", "),sX=a("em"),wOe=o("optional"),AOe=o(`):
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.
use`),IC=a("em"),LOe=o("fast ("),nX=a("code"),BOe=o("bool"),xOe=o(", _optional"),kOe=o(", defaults to "),lX=a("code"),ROe=o("True"),POe=o(`):
Whether or not to try to load the fast version of the tokenizer.
tokenizer`),DC=a("em"),SOe=o("type ("),iX=a("code"),$Oe=o("str"),IOe=o(", _optional"),DOe=o(`):
Tokenizer type to be loaded.
trust`),NC=a("em"),NOe=o("remote_code ("),dX=a("code"),jOe=o("bool"),OOe=o(", _optional"),GOe=o(", defaults to "),cX=a("code"),qOe=o("False"),zOe=o(`):
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to `),mX=a("code"),XOe=o("True"),QOe=o(` for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.
kwargs (additional keyword arguments, `),fX=a("em"),VOe=o("optional"),WOe=o(`):
Will be passed to the Tokenizer `),hX=a("code"),HOe=o("__init__()"),UOe=o(` method. Can be used to set special tokens like
`),gX=a("code"),JOe=o("bos_token"),KOe=o(", "),uX=a("code"),YOe=o("eos_token"),ZOe=o(", "),pX=a("code"),eGe=o("unk_token"),oGe=o(", "),_X=a("code"),rGe=o("sep_token"),tGe=o(", "),bX=a("code"),aGe=o("pad_token"),sGe=o(", "),vX=a("code"),nGe=o("cls_token"),lGe=o(`,
`),TX=a("code"),iGe=o("mask_token"),dGe=o(", "),FX=a("code"),cGe=o("additional_special_tokens"),mGe=o(". See parameters in the "),EX=a("code"),fGe=o("__init__()"),hGe=o(" for more details."),gGe=l(),CX=a("p"),uGe=o("Examples:"),pGe=l(),m(jC.$$.fragment),_Ge=l(),Sf=a("div"),m(OC.$$.fragment),bGe=l(),MX=a("p"),vGe=o("Register a new tokenizer in this mapping."),h5e=l(),ti=a("h2"),$f=a("a"),yX=a("span"),m(GC.$$.fragment),TGe=l(),wX=a("span"),FGe=o("AutoFeatureExtractor"),g5e=l(),Dt=a("div"),m(qC.$$.fragment),EGe=l(),zC=a("p"),CGe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),t8=a("a"),MGe=o("AutoFeatureExtractor.from_pretrained()"),yGe=o(" class method."),wGe=l(),XC=a("p"),AGe=o("This class cannot be instantiated directly using "),AX=a("code"),LGe=o("__init__()"),BGe=o(" (throws an error)."),xGe=l(),Te=a("div"),m(QC.$$.fragment),kGe=l(),LX=a("p"),RGe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),PGe=l(),Ea=a("p"),SGe=o("The feature extractor class to instantiate is selected based on the "),BX=a("code"),$Ge=o("model_type"),IGe=o(` property of the config
object (either passed as an argument or loaded from `),xX=a("code"),DGe=o("pretrained_model_name_or_path"),NGe=o(` if possible), or when
it\u2019s missing, by falling back to using pattern matching on `),kX=a("code"),jGe=o("pretrained_model_name_or_path"),OGe=o(":"),GGe=l(),Ce=a("ul"),If=a("li"),RX=a("strong"),qGe=o("beit"),zGe=o(" \u2014 "),a8=a("a"),XGe=o("BeitFeatureExtractor"),QGe=o(" (BEiT model)"),VGe=l(),Df=a("li"),PX=a("strong"),WGe=o("clip"),HGe=o(" \u2014 "),s8=a("a"),UGe=o("CLIPFeatureExtractor"),JGe=o(" (CLIP model)"),KGe=l(),Nf=a("li"),SX=a("strong"),YGe=o("deit"),ZGe=o(" \u2014 "),n8=a("a"),eqe=o("DeiTFeatureExtractor"),oqe=o(" (DeiT model)"),rqe=l(),jf=a("li"),$X=a("strong"),tqe=o("detr"),aqe=o(" \u2014 "),l8=a("a"),sqe=o("DetrFeatureExtractor"),nqe=o(" (DETR model)"),lqe=l(),Of=a("li"),IX=a("strong"),iqe=o("hubert"),dqe=o(" \u2014 "),i8=a("a"),cqe=o("Wav2Vec2FeatureExtractor"),mqe=o(" (Hubert model)"),fqe=l(),Gf=a("li"),DX=a("strong"),hqe=o("layoutlmv2"),gqe=o(" \u2014 "),d8=a("a"),uqe=o("LayoutLMv2FeatureExtractor"),pqe=o(" (LayoutLMv2 model)"),_qe=l(),qf=a("li"),NX=a("strong"),bqe=o("speech_to_text"),vqe=o(" \u2014 "),c8=a("a"),Tqe=o("Speech2TextFeatureExtractor"),Fqe=o(" (Speech2Text model)"),Eqe=l(),zf=a("li"),jX=a("strong"),Cqe=o("vit"),Mqe=o(" \u2014 "),m8=a("a"),yqe=o("ViTFeatureExtractor"),wqe=o(" (ViT model)"),Aqe=l(),Xf=a("li"),OX=a("strong"),Lqe=o("wav2vec2"),Bqe=o(" \u2014 "),f8=a("a"),xqe=o("Wav2Vec2FeatureExtractor"),kqe=o(" (Wav2Vec2 model)"),Rqe=l(),ai=a("p"),Pqe=o(`Params:
pretrained_model_name_or_path (`),GX=a("code"),Sqe=o("str"),$qe=o(" or "),qX=a("code"),Iqe=o("os.PathLike"),Dqe=o(`):
This can be either:`),Nqe=l(),si=a("ul"),Ca=a("li"),jqe=o("a string, the "),zX=a("em"),Oqe=o("model id"),Gqe=o(` of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like `),XX=a("code"),qqe=o("bert-base-uncased"),zqe=o(`, or
namespaced under a user or organization name, like `),QX=a("code"),Xqe=o("dbmdz/bert-base-german-cased"),Qqe=o("."),Vqe=l(),Ma=a("li"),Wqe=o("a path to a "),VX=a("em"),Hqe=o("directory"),Uqe=o(` containing a feature extractor file saved using the
`),h8=a("a"),Jqe=o("save_pretrained()"),Kqe=o(` method, e.g.,
`),WX=a("code"),Yqe=o("./my_model_directory/"),Zqe=o("."),eze=l(),N=a("li"),oze=o("a path or url to a saved feature extractor JSON "),HX=a("em"),rze=o("file"),tze=o(`, e.g.,
`),UX=a("code"),aze=o("./my_model_directory/preprocessor_config.json"),sze=o(`.
cache`),ni=a("em"),nze=o("dir ("),JX=a("code"),lze=o("str"),ize=o(" or "),KX=a("code"),dze=o("os.PathLike"),cze=o(", _optional"),mze=o(`):
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.
force`),VC=a("em"),fze=o("download ("),YX=a("code"),hze=o("bool"),gze=o(", _optional"),uze=o(", defaults to "),ZX=a("code"),pze=o("False"),_ze=o(`):
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.
resume`),WC=a("em"),bze=o("download ("),eQ=a("code"),vze=o("bool"),Tze=o(", _optional"),Fze=o(", defaults to "),oQ=a("code"),Eze=o("False"),Cze=o(`):
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.
proxies (`),rQ=a("code"),Mze=o("Dict[str, str]"),yze=o(", "),tQ=a("em"),wze=o("optional"),Aze=o(`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),aQ=a("code"),Lze=o("{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}."),Bze=o(` The proxies are used on each request.
use`),HC=a("em"),xze=o("auth_token ("),sQ=a("code"),kze=o("str"),Rze=o(" or _bool"),Pze=o(", "),nQ=a("em"),Sze=o("optional"),$ze=o(`):
The token to use as HTTP bearer authorization for remote files. If `),lQ=a("code"),Ize=o("True"),Dze=o(`, will use the token
generated when running `),iQ=a("code"),Nze=o("transformers-cli login"),jze=o(" (stored in "),dQ=a("code"),Oze=o("~/.huggingface"),Gze=o(`).
revision(`),cQ=a("code"),qze=o("str"),zze=o(", "),mQ=a("em"),Xze=o("optional"),Qze=o(", defaults to "),fQ=a("code"),Vze=o('"main"'),Wze=o(`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),hQ=a("code"),Hze=o("revision"),Uze=o(` can be any
identifier allowed by git.
return`),UC=a("em"),Jze=o("unused_kwargs ("),gQ=a("code"),Kze=o("bool"),Yze=o(", _optional"),Zze=o(", defaults to "),uQ=a("code"),eXe=o("False"),oXe=o(`):
If `),pQ=a("code"),rXe=o("False"),tXe=o(", then this function returns just the final feature extractor object. If "),_Q=a("code"),aXe=o("True"),sXe=o(`,
then this functions returns a `),bQ=a("code"),nXe=o("Tuple(feature_extractor, unused_kwargs)"),lXe=o(" where "),vQ=a("em"),iXe=o("unused_kwargs"),dXe=o(` is a
dictionary consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the
part of `),TQ=a("code"),cXe=o("kwargs"),mXe=o(" which has not been used to update "),FQ=a("code"),fXe=o("feature_extractor"),hXe=o(` and is otherwise ignored.
kwargs (`),EQ=a("code"),gXe=o("Dict[str, Any]"),uXe=o(", "),CQ=a("em"),pXe=o("optional"),_Xe=o(`):
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are `),MQ=a("em"),bXe=o("not"),vXe=o(` feature extractor attributes is
controlled by the `),yQ=a("code"),TXe=o("return_unused_kwargs"),FXe=o(" keyword parameter."),EXe=l(),m(Qf.$$.fragment),CXe=l(),wQ=a("p"),MXe=o("Examples:"),yXe=l(),m(JC.$$.fragment),u5e=l(),li=a("h2"),Vf=a("a"),AQ=a("span"),m(KC.$$.fragment),wXe=l(),LQ=a("span"),AXe=o("AutoProcessor"),p5e=l(),Nt=a("div"),m(YC.$$.fragment),LXe=l(),ZC=a("p"),BXe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),g8=a("a"),xXe=o("AutoProcessor.from_pretrained()"),kXe=o(" class method."),RXe=l(),e3=a("p"),PXe=o("This class cannot be instantiated directly using "),BQ=a("code"),SXe=o("__init__()"),$Xe=o(" (throws an error)."),IXe=l(),Fe=a("div"),m(o3.$$.fragment),DXe=l(),xQ=a("p"),NXe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),jXe=l(),ii=a("p"),OXe=o("The processor class to instantiate is selected based on the "),kQ=a("code"),GXe=o("model_type"),qXe=o(` property of the config object
(either passed as an argument or loaded from `),RQ=a("code"),zXe=o("pretrained_model_name_or_path"),XXe=o(" if possible):"),QXe=l(),to=a("ul"),Wf=a("li"),PQ=a("strong"),VXe=o("clip"),WXe=o(" \u2014 "),u8=a("a"),HXe=o("CLIPProcessor"),UXe=o(" (CLIP model)"),JXe=l(),Hf=a("li"),SQ=a("strong"),KXe=o("layoutlmv2"),YXe=o(" \u2014 "),p8=a("a"),ZXe=o("LayoutLMv2Processor"),eQe=o(" (LayoutLMv2 model)"),oQe=l(),Uf=a("li"),$Q=a("strong"),rQe=o("speech_to_text"),tQe=o(" \u2014 "),_8=a("a"),aQe=o("Speech2TextProcessor"),sQe=o(" (Speech2Text model)"),nQe=l(),Jf=a("li"),IQ=a("strong"),lQe=o("speech_to_text_2"),iQe=o(" \u2014 "),b8=a("a"),dQe=o("Speech2Text2Processor"),cQe=o(" (Speech2Text2 model)"),mQe=l(),Kf=a("li"),DQ=a("strong"),fQe=o("trocr"),hQe=o(" \u2014 "),v8=a("a"),gQe=o("TrOCRProcessor"),uQe=o(" (TrOCR model)"),pQe=l(),Yf=a("li"),NQ=a("strong"),_Qe=o("vision-text-dual-encoder"),bQe=o(" \u2014 "),T8=a("a"),vQe=o("VisionTextDualEncoderProcessor"),TQe=o(" (VisionTextDualEncoder model)"),FQe=l(),Zf=a("li"),jQ=a("strong"),EQe=o("wav2vec2"),CQe=o(" \u2014 "),F8=a("a"),MQe=o("Wav2Vec2Processor"),yQe=o(" (Wav2Vec2 model)"),wQe=l(),di=a("p"),AQe=o(`Params:
pretrained_model_name_or_path (`),OQ=a("code"),LQe=o("str"),BQe=o(" or "),GQ=a("code"),xQe=o("os.PathLike"),kQe=o(`):
This can be either:`),RQe=l(),r3=a("ul"),ya=a("li"),PQe=o("a string, the "),qQ=a("em"),SQe=o("model id"),$Qe=o(` of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like `),zQ=a("code"),IQe=o("bert-base-uncased"),DQe=o(`, or
namespaced under a user or organization name, like `),XQ=a("code"),NQe=o("dbmdz/bert-base-german-cased"),jQe=o("."),OQe=l(),D=a("li"),GQe=o("a path to a "),QQ=a("em"),qQe=o("directory"),zQe=o(" containing a processor files saved using the "),VQ=a("code"),XQe=o("save_pretrained()"),QQe=o(` method,
e.g., `),WQ=a("code"),VQe=o("./my_model_directory/"),WQe=o(`.
cache`),ci=a("em"),HQe=o("dir ("),HQ=a("code"),UQe=o("str"),JQe=o(" or "),UQ=a("code"),KQe=o("os.PathLike"),YQe=o(", _optional"),ZQe=o(`):
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.
force`),t3=a("em"),eVe=o("download ("),JQ=a("code"),oVe=o("bool"),rVe=o(", _optional"),tVe=o(", defaults to "),KQ=a("code"),aVe=o("False"),sVe=o(`):
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.
resume`),a3=a("em"),nVe=o("download ("),YQ=a("code"),lVe=o("bool"),iVe=o(", _optional"),dVe=o(", defaults to "),ZQ=a("code"),cVe=o("False"),mVe=o(`):
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.
proxies (`),eV=a("code"),fVe=o("Dict[str, str]"),hVe=o(", "),oV=a("em"),gVe=o("optional"),uVe=o(`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),rV=a("code"),pVe=o("{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}."),_Ve=o(` The proxies are used on each request.
use`),s3=a("em"),bVe=o("auth_token ("),tV=a("code"),vVe=o("str"),TVe=o(" or _bool"),FVe=o(", "),aV=a("em"),EVe=o("optional"),CVe=o(`):
The token to use as HTTP bearer authorization for remote files. If `),sV=a("code"),MVe=o("True"),yVe=o(`, will use the token
generated when running `),nV=a("code"),wVe=o("transformers-cli login"),AVe=o(" (stored in "),lV=a("code"),LVe=o("~/.huggingface"),BVe=o(`).
revision (`),iV=a("code"),xVe=o("str"),kVe=o(", "),dV=a("em"),RVe=o("optional"),PVe=o(", defaults to "),cV=a("code"),SVe=o('"main"'),$Ve=o(`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),mV=a("code"),IVe=o("revision"),DVe=o(` can be any
identifier allowed by git.
return`),n3=a("em"),NVe=o("unused_kwargs ("),fV=a("code"),jVe=o("bool"),OVe=o(", _optional"),GVe=o(", defaults to "),hV=a("code"),qVe=o("False"),zVe=o(`):
If `),gV=a("code"),XVe=o("False"),QVe=o(", then this function returns just the final feature extractor object. If "),uV=a("code"),VVe=o("True"),WVe=o(`,
then this functions returns a `),pV=a("code"),HVe=o("Tuple(feature_extractor, unused_kwargs)"),UVe=o(" where "),_V=a("em"),JVe=o("unused_kwargs"),KVe=o(` is a
dictionary consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the
part of `),bV=a("code"),YVe=o("kwargs"),ZVe=o(" which has not been used to update "),vV=a("code"),eWe=o("feature_extractor"),oWe=o(` and is otherwise ignored.
kwargs (`),TV=a("code"),rWe=o("Dict[str, Any]"),tWe=o(", "),FV=a("em"),aWe=o("optional"),sWe=o(`):
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are `),EV=a("em"),nWe=o("not"),lWe=o(` feature extractor attributes is
controlled by the `),CV=a("code"),iWe=o("return_unused_kwargs"),dWe=o(" keyword parameter."),cWe=l(),m(eh.$$.fragment),mWe=l(),MV=a("p"),fWe=o("Examples:"),hWe=l(),m(l3.$$.fragment),_5e=l(),mi=a("h2"),oh=a("a"),yV=a("span"),m(i3.$$.fragment),gWe=l(),wV=a("span"),uWe=o("AutoModel"),b5e=l(),jo=a("div"),m(d3.$$.fragment),pWe=l(),fi=a("p"),_We=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),AV=a("code"),bWe=o("from_pretrained()"),vWe=o(` class method or the
`),LV=a("code"),TWe=o("from_config()"),FWe=o(" class method."),EWe=l(),c3=a("p"),CWe=o("This class cannot be instantiated directly using "),BV=a("code"),MWe=o("__init__()"),yWe=o(" (throws an error)."),wWe=l(),kr=a("div"),m(m3.$$.fragment),AWe=l(),xV=a("p"),LWe=o("Instantiates one of the base model classes of the library from a configuration."),BWe=l(),hi=a("p"),xWe=o(`Note:
Loading a model from its configuration file does `),kV=a("strong"),kWe=o("not"),RWe=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),RV=a("code"),PWe=o("from_pretrained()"),SWe=o(` to load the model
weights.`),$We=l(),PV=a("p"),IWe=o("Examples:"),DWe=l(),m(f3.$$.fragment),NWe=l(),Se=a("div"),m(h3.$$.fragment),jWe=l(),SV=a("p"),OWe=o("Instantiate one of the base model classes of the library from a pretrained model."),GWe=l(),wa=a("p"),qWe=o("The model class to instantiate is selected based on the "),$V=a("code"),zWe=o("model_type"),XWe=o(` property of the config object (either
passed as an argument or loaded from `),IV=a("code"),QWe=o("pretrained_model_name_or_path"),VWe=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),DV=a("code"),WWe=o("pretrained_model_name_or_path"),HWe=o(":"),UWe=l(),F=a("ul"),rh=a("li"),NV=a("strong"),JWe=o("albert"),KWe=o(" \u2014 "),E8=a("a"),YWe=o("AlbertModel"),ZWe=o(" (ALBERT model)"),eHe=l(),th=a("li"),jV=a("strong"),oHe=o("bart"),rHe=o(" \u2014 "),C8=a("a"),tHe=o("BartModel"),aHe=o(" (BART model)"),sHe=l(),ah=a("li"),OV=a("strong"),nHe=o("beit"),lHe=o(" \u2014 "),M8=a("a"),iHe=o("BeitModel"),dHe=o(" (BEiT model)"),cHe=l(),sh=a("li"),GV=a("strong"),mHe=o("bert"),fHe=o(" \u2014 "),y8=a("a"),hHe=o("BertModel"),gHe=o(" (BERT model)"),uHe=l(),nh=a("li"),qV=a("strong"),pHe=o("bert-generation"),_He=o(" \u2014 "),w8=a("a"),bHe=o("BertGenerationEncoder"),vHe=o(" (Bert Generation model)"),THe=l(),lh=a("li"),zV=a("strong"),FHe=o("big_bird"),EHe=o(" \u2014 "),A8=a("a"),CHe=o("BigBirdModel"),MHe=o(" (BigBird model)"),yHe=l(),ih=a("li"),XV=a("strong"),wHe=o("bigbird_pegasus"),AHe=o(" \u2014 "),L8=a("a"),LHe=o("BigBirdPegasusModel"),BHe=o(" (BigBirdPegasus model)"),xHe=l(),dh=a("li"),QV=a("strong"),kHe=o("blenderbot"),RHe=o(" \u2014 "),B8=a("a"),PHe=o("BlenderbotModel"),SHe=o(" (Blenderbot model)"),$He=l(),ch=a("li"),VV=a("strong"),IHe=o("blenderbot-small"),DHe=o(" \u2014 "),x8=a("a"),NHe=o("BlenderbotSmallModel"),jHe=o(" (BlenderbotSmall model)"),OHe=l(),mh=a("li"),WV=a("strong"),GHe=o("camembert"),qHe=o(" \u2014 "),k8=a("a"),zHe=o("CamembertModel"),XHe=o(" (CamemBERT model)"),QHe=l(),fh=a("li"),HV=a("strong"),VHe=o("canine"),WHe=o(" \u2014 "),R8=a("a"),HHe=o("CanineModel"),UHe=o(" (Canine model)"),JHe=l(),hh=a("li"),UV=a("strong"),KHe=o("clip"),YHe=o(" \u2014 "),P8=a("a"),ZHe=o("CLIPModel"),eUe=o(" (CLIP model)"),oUe=l(),gh=a("li"),JV=a("strong"),rUe=o("convbert"),tUe=o(" \u2014 "),S8=a("a"),aUe=o("ConvBertModel"),sUe=o(" (ConvBERT model)"),nUe=l(),uh=a("li"),KV=a("strong"),lUe=o("ctrl"),iUe=o(" \u2014 "),$8=a("a"),dUe=o("CTRLModel"),cUe=o(" (CTRL model)"),mUe=l(),ph=a("li"),YV=a("strong"),fUe=o("deberta"),hUe=o(" \u2014 "),I8=a("a"),gUe=o("DebertaModel"),uUe=o(" (DeBERTa model)"),pUe=l(),_h=a("li"),ZV=a("strong"),_Ue=o("deberta-v2"),bUe=o(" \u2014 "),D8=a("a"),vUe=o("DebertaV2Model"),TUe=o(" (DeBERTa-v2 model)"),FUe=l(),bh=a("li"),eW=a("strong"),EUe=o("deit"),CUe=o(" \u2014 "),N8=a("a"),MUe=o("DeiTModel"),yUe=o(" (DeiT model)"),wUe=l(),vh=a("li"),oW=a("strong"),AUe=o("detr"),LUe=o(" \u2014 "),j8=a("a"),BUe=o("DetrModel"),xUe=o(" (DETR model)"),kUe=l(),Th=a("li"),rW=a("strong"),RUe=o("distilbert"),PUe=o(" \u2014 "),O8=a("a"),SUe=o("DistilBertModel"),$Ue=o(" (DistilBERT model)"),IUe=l(),Fh=a("li"),tW=a("strong"),DUe=o("dpr"),NUe=o(" \u2014 "),G8=a("a"),jUe=o("DPRQuestionEncoder"),OUe=o(" (DPR model)"),GUe=l(),Eh=a("li"),aW=a("strong"),qUe=o("electra"),zUe=o(" \u2014 "),q8=a("a"),XUe=o("ElectraModel"),QUe=o(" (ELECTRA model)"),VUe=l(),Ch=a("li"),sW=a("strong"),WUe=o("flaubert"),HUe=o(" \u2014 "),z8=a("a"),UUe=o("FlaubertModel"),JUe=o(" (FlauBERT model)"),KUe=l(),Mh=a("li"),nW=a("strong"),YUe=o("fnet"),ZUe=o(" \u2014 "),X8=a("a"),eJe=o("FNetModel"),oJe=o(" (FNet model)"),rJe=l(),yh=a("li"),lW=a("strong"),tJe=o("fsmt"),aJe=o(" \u2014 "),Q8=a("a"),sJe=o("FSMTModel"),nJe=o(" (FairSeq Machine-Translation model)"),lJe=l(),ln=a("li"),iW=a("strong"),iJe=o("funnel"),dJe=o(" \u2014 "),V8=a("a"),cJe=o("FunnelModel"),mJe=o(" or "),W8=a("a"),fJe=o("FunnelBaseModel"),hJe=o(" (Funnel Transformer model)"),gJe=l(),wh=a("li"),dW=a("strong"),uJe=o("gpt2"),pJe=o(" \u2014 "),H8=a("a"),_Je=o("GPT2Model"),bJe=o(" (OpenAI GPT-2 model)"),vJe=l(),Ah=a("li"),cW=a("strong"),TJe=o("gpt_neo"),FJe=o(" \u2014 "),U8=a("a"),EJe=o("GPTNeoModel"),CJe=o(" (GPT Neo model)"),MJe=l(),Lh=a("li"),mW=a("strong"),yJe=o("gptj"),wJe=o(" \u2014 "),J8=a("a"),AJe=o("GPTJModel"),LJe=o(" (GPT-J model)"),BJe=l(),Bh=a("li"),fW=a("strong"),xJe=o("hubert"),kJe=o(" \u2014 "),K8=a("a"),RJe=o("HubertModel"),PJe=o(" (Hubert model)"),SJe=l(),xh=a("li"),hW=a("strong"),$Je=o("ibert"),IJe=o(" \u2014 "),Y8=a("a"),DJe=o("IBertModel"),NJe=o(" (I-BERT model)"),jJe=l(),kh=a("li"),gW=a("strong"),OJe=o("imagegpt"),GJe=o(" \u2014 "),Z8=a("a"),qJe=o("ImageGPTModel"),zJe=o(" (ImageGPT model)"),XJe=l(),Rh=a("li"),uW=a("strong"),QJe=o("layoutlm"),VJe=o(" \u2014 "),eB=a("a"),WJe=o("LayoutLMModel"),HJe=o(" (LayoutLM model)"),UJe=l(),Ph=a("li"),pW=a("strong"),JJe=o("layoutlmv2"),KJe=o(" \u2014 "),oB=a("a"),YJe=o("LayoutLMv2Model"),ZJe=o(" (LayoutLMv2 model)"),eKe=l(),Sh=a("li"),_W=a("strong"),oKe=o("led"),rKe=o(" \u2014 "),rB=a("a"),tKe=o("LEDModel"),aKe=o(" (LED model)"),sKe=l(),$h=a("li"),bW=a("strong"),nKe=o("longformer"),lKe=o(" \u2014 "),tB=a("a"),iKe=o("LongformerModel"),dKe=o(" (Longformer model)"),cKe=l(),Ih=a("li"),vW=a("strong"),mKe=o("luke"),fKe=o(" \u2014 "),aB=a("a"),hKe=o("LukeModel"),gKe=o(" (LUKE model)"),uKe=l(),Dh=a("li"),TW=a("strong"),pKe=o("lxmert"),_Ke=o(" \u2014 "),sB=a("a"),bKe=o("LxmertModel"),vKe=o(" (LXMERT model)"),TKe=l(),Nh=a("li"),FW=a("strong"),FKe=o("m2m_100"),EKe=o(" \u2014 "),nB=a("a"),CKe=o("M2M100Model"),MKe=o(" (M2M100 model)"),yKe=l(),jh=a("li"),EW=a("strong"),wKe=o("marian"),AKe=o(" \u2014 "),lB=a("a"),LKe=o("MarianModel"),BKe=o(" (Marian model)"),xKe=l(),Oh=a("li"),CW=a("strong"),kKe=o("mbart"),RKe=o(" \u2014 "),iB=a("a"),PKe=o("MBartModel"),SKe=o(" (mBART model)"),$Ke=l(),Gh=a("li"),MW=a("strong"),IKe=o("megatron-bert"),DKe=o(" \u2014 "),dB=a("a"),NKe=o("MegatronBertModel"),jKe=o(" (MegatronBert model)"),OKe=l(),qh=a("li"),yW=a("strong"),GKe=o("mobilebert"),qKe=o(" \u2014 "),cB=a("a"),zKe=o("MobileBertModel"),XKe=o(" (MobileBERT model)"),QKe=l(),zh=a("li"),wW=a("strong"),VKe=o("mpnet"),WKe=o(" \u2014 "),mB=a("a"),HKe=o("MPNetModel"),UKe=o(" (MPNet model)"),JKe=l(),Xh=a("li"),AW=a("strong"),KKe=o("mt5"),YKe=o(" \u2014 "),fB=a("a"),ZKe=o("MT5Model"),eYe=o(" (mT5 model)"),oYe=l(),Qh=a("li"),LW=a("strong"),rYe=o("openai-gpt"),tYe=o(" \u2014 "),hB=a("a"),aYe=o("OpenAIGPTModel"),sYe=o(" (OpenAI GPT model)"),nYe=l(),Vh=a("li"),BW=a("strong"),lYe=o("pegasus"),iYe=o(" \u2014 "),gB=a("a"),dYe=o("PegasusModel"),cYe=o(" (Pegasus model)"),mYe=l(),Wh=a("li"),xW=a("strong"),fYe=o("perceiver"),hYe=o(" \u2014 "),uB=a("a"),gYe=o("PerceiverModel"),uYe=o(" (Perceiver model)"),pYe=l(),Hh=a("li"),kW=a("strong"),_Ye=o("prophetnet"),bYe=o(" \u2014 "),pB=a("a"),vYe=o("ProphetNetModel"),TYe=o(" (ProphetNet model)"),FYe=l(),Uh=a("li"),RW=a("strong"),EYe=o("qdqbert"),CYe=o(" \u2014 "),_B=a("a"),MYe=o("QDQBertModel"),yYe=o(" (QDQBert model)"),wYe=l(),Jh=a("li"),PW=a("strong"),AYe=o("reformer"),LYe=o(" \u2014 "),bB=a("a"),BYe=o("ReformerModel"),xYe=o(" (Reformer model)"),kYe=l(),Kh=a("li"),SW=a("strong"),RYe=o("rembert"),PYe=o(" \u2014 "),vB=a("a"),SYe=o("RemBertModel"),$Ye=o(" (RemBERT model)"),IYe=l(),Yh=a("li"),$W=a("strong"),DYe=o("retribert"),NYe=o(" \u2014 "),TB=a("a"),jYe=o("RetriBertModel"),OYe=o(" (RetriBERT model)"),GYe=l(),Zh=a("li"),IW=a("strong"),qYe=o("roberta"),zYe=o(" \u2014 "),FB=a("a"),XYe=o("RobertaModel"),QYe=o(" (RoBERTa model)"),VYe=l(),eg=a("li"),DW=a("strong"),WYe=o("roformer"),HYe=o(" \u2014 "),EB=a("a"),UYe=o("RoFormerModel"),JYe=o(" (RoFormer model)"),KYe=l(),og=a("li"),NW=a("strong"),YYe=o("segformer"),ZYe=o(" \u2014 "),CB=a("a"),eZe=o("SegformerModel"),oZe=o(" (SegFormer model)"),rZe=l(),rg=a("li"),jW=a("strong"),tZe=o("sew"),aZe=o(" \u2014 "),MB=a("a"),sZe=o("SEWModel"),nZe=o(" (SEW model)"),lZe=l(),tg=a("li"),OW=a("strong"),iZe=o("sew-d"),dZe=o(" \u2014 "),yB=a("a"),cZe=o("SEWDModel"),mZe=o(" (SEW-D model)"),fZe=l(),ag=a("li"),GW=a("strong"),hZe=o("speech_to_text"),gZe=o(" \u2014 "),wB=a("a"),uZe=o("Speech2TextModel"),pZe=o(" (Speech2Text model)"),_Ze=l(),sg=a("li"),qW=a("strong"),bZe=o("splinter"),vZe=o(" \u2014 "),AB=a("a"),TZe=o("SplinterModel"),FZe=o(" (Splinter model)"),EZe=l(),ng=a("li"),zW=a("strong"),CZe=o("squeezebert"),MZe=o(" \u2014 "),LB=a("a"),yZe=o("SqueezeBertModel"),wZe=o(" (SqueezeBERT model)"),AZe=l(),lg=a("li"),XW=a("strong"),LZe=o("t5"),BZe=o(" \u2014 "),BB=a("a"),xZe=o("T5Model"),kZe=o(" (T5 model)"),RZe=l(),ig=a("li"),QW=a("strong"),PZe=o("tapas"),SZe=o(" \u2014 "),xB=a("a"),$Ze=o("TapasModel"),IZe=o(" (TAPAS model)"),DZe=l(),dg=a("li"),VW=a("strong"),NZe=o("transfo-xl"),jZe=o(" \u2014 "),kB=a("a"),OZe=o("TransfoXLModel"),GZe=o(" (Transformer-XL model)"),qZe=l(),cg=a("li"),WW=a("strong"),zZe=o("unispeech"),XZe=o(" \u2014 "),RB=a("a"),QZe=o("UniSpeechModel"),VZe=o(" (UniSpeech model)"),WZe=l(),mg=a("li"),HW=a("strong"),HZe=o("unispeech-sat"),UZe=o(" \u2014 "),PB=a("a"),JZe=o("UniSpeechSatModel"),KZe=o(" (UniSpeechSat model)"),YZe=l(),fg=a("li"),UW=a("strong"),ZZe=o("vision-text-dual-encoder"),eeo=o(" \u2014 "),SB=a("a"),oeo=o("VisionTextDualEncoderModel"),reo=o(" (VisionTextDualEncoder model)"),teo=l(),hg=a("li"),JW=a("strong"),aeo=o("visual_bert"),seo=o(" \u2014 "),$B=a("a"),neo=o("VisualBertModel"),leo=o(" (VisualBert model)"),ieo=l(),gg=a("li"),KW=a("strong"),deo=o("vit"),ceo=o(" \u2014 "),IB=a("a"),meo=o("ViTModel"),feo=o(" (ViT model)"),heo=l(),ug=a("li"),YW=a("strong"),geo=o("wav2vec2"),ueo=o(" \u2014 "),DB=a("a"),peo=o("Wav2Vec2Model"),_eo=o(" (Wav2Vec2 model)"),beo=l(),pg=a("li"),ZW=a("strong"),veo=o("xlm"),Teo=o(" \u2014 "),NB=a("a"),Feo=o("XLMModel"),Eeo=o(" (XLM model)"),Ceo=l(),_g=a("li"),eH=a("strong"),Meo=o("xlm-prophetnet"),yeo=o(" \u2014 "),jB=a("a"),weo=o("XLMProphetNetModel"),Aeo=o(" (XLMProphetNet model)"),Leo=l(),bg=a("li"),oH=a("strong"),Beo=o("xlm-roberta"),xeo=o(" \u2014 "),OB=a("a"),keo=o("XLMRobertaModel"),Reo=o(" (XLM-RoBERTa model)"),Peo=l(),vg=a("li"),rH=a("strong"),Seo=o("xlnet"),$eo=o(" \u2014 "),GB=a("a"),Ieo=o("XLNetModel"),Deo=o(" (XLNet model)"),Neo=l(),Tg=a("p"),jeo=o("The model is set in evaluation mode by default using "),tH=a("code"),Oeo=o("model.eval()"),Geo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),aH=a("code"),qeo=o("model.train()"),zeo=l(),sH=a("p"),Xeo=o("Examples:"),Qeo=l(),m(g3.$$.fragment),v5e=l(),gi=a("h2"),Fg=a("a"),nH=a("span"),m(u3.$$.fragment),Veo=l(),lH=a("span"),Weo=o("AutoModelForPreTraining"),T5e=l(),Oo=a("div"),m(p3.$$.fragment),Heo=l(),ui=a("p"),Ueo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),iH=a("code"),Jeo=o("from_pretrained()"),Keo=o(` class method or the
`),dH=a("code"),Yeo=o("from_config()"),Zeo=o(" class method."),eoo=l(),_3=a("p"),ooo=o("This class cannot be instantiated directly using "),cH=a("code"),roo=o("__init__()"),too=o(" (throws an error)."),aoo=l(),Rr=a("div"),m(b3.$$.fragment),soo=l(),mH=a("p"),noo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),loo=l(),pi=a("p"),ioo=o(`Note:
Loading a model from its configuration file does `),fH=a("strong"),doo=o("not"),coo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),hH=a("code"),moo=o("from_pretrained()"),foo=o(` to load the model
weights.`),hoo=l(),gH=a("p"),goo=o("Examples:"),uoo=l(),m(v3.$$.fragment),poo=l(),$e=a("div"),m(T3.$$.fragment),_oo=l(),uH=a("p"),boo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),voo=l(),Aa=a("p"),Too=o("The model class to instantiate is selected based on the "),pH=a("code"),Foo=o("model_type"),Eoo=o(` property of the config object (either
passed as an argument or loaded from `),_H=a("code"),Coo=o("pretrained_model_name_or_path"),Moo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),bH=a("code"),yoo=o("pretrained_model_name_or_path"),woo=o(":"),Aoo=l(),R=a("ul"),Eg=a("li"),vH=a("strong"),Loo=o("albert"),Boo=o(" \u2014 "),qB=a("a"),xoo=o("AlbertForPreTraining"),koo=o(" (ALBERT model)"),Roo=l(),Cg=a("li"),TH=a("strong"),Poo=o("bart"),Soo=o(" \u2014 "),zB=a("a"),$oo=o("BartForConditionalGeneration"),Ioo=o(" (BART model)"),Doo=l(),Mg=a("li"),FH=a("strong"),Noo=o("bert"),joo=o(" \u2014 "),XB=a("a"),Ooo=o("BertForPreTraining"),Goo=o(" (BERT model)"),qoo=l(),yg=a("li"),EH=a("strong"),zoo=o("big_bird"),Xoo=o(" \u2014 "),QB=a("a"),Qoo=o("BigBirdForPreTraining"),Voo=o(" (BigBird model)"),Woo=l(),wg=a("li"),CH=a("strong"),Hoo=o("camembert"),Uoo=o(" \u2014 "),VB=a("a"),Joo=o("CamembertForMaskedLM"),Koo=o(" (CamemBERT model)"),Yoo=l(),Ag=a("li"),MH=a("strong"),Zoo=o("ctrl"),ero=o(" \u2014 "),WB=a("a"),oro=o("CTRLLMHeadModel"),rro=o(" (CTRL model)"),tro=l(),Lg=a("li"),yH=a("strong"),aro=o("deberta"),sro=o(" \u2014 "),HB=a("a"),nro=o("DebertaForMaskedLM"),lro=o(" (DeBERTa model)"),iro=l(),Bg=a("li"),wH=a("strong"),dro=o("deberta-v2"),cro=o(" \u2014 "),UB=a("a"),mro=o("DebertaV2ForMaskedLM"),fro=o(" (DeBERTa-v2 model)"),hro=l(),xg=a("li"),AH=a("strong"),gro=o("distilbert"),uro=o(" \u2014 "),JB=a("a"),pro=o("DistilBertForMaskedLM"),_ro=o(" (DistilBERT model)"),bro=l(),kg=a("li"),LH=a("strong"),vro=o("electra"),Tro=o(" \u2014 "),KB=a("a"),Fro=o("ElectraForPreTraining"),Ero=o(" (ELECTRA model)"),Cro=l(),Rg=a("li"),BH=a("strong"),Mro=o("flaubert"),yro=o(" \u2014 "),YB=a("a"),wro=o("FlaubertWithLMHeadModel"),Aro=o(" (FlauBERT model)"),Lro=l(),Pg=a("li"),xH=a("strong"),Bro=o("fnet"),xro=o(" \u2014 "),ZB=a("a"),kro=o("FNetForPreTraining"),Rro=o(" (FNet model)"),Pro=l(),Sg=a("li"),kH=a("strong"),Sro=o("fsmt"),$ro=o(" \u2014 "),e9=a("a"),Iro=o("FSMTForConditionalGeneration"),Dro=o(" (FairSeq Machine-Translation model)"),Nro=l(),$g=a("li"),RH=a("strong"),jro=o("funnel"),Oro=o(" \u2014 "),o9=a("a"),Gro=o("FunnelForPreTraining"),qro=o(" (Funnel Transformer model)"),zro=l(),Ig=a("li"),PH=a("strong"),Xro=o("gpt2"),Qro=o(" \u2014 "),r9=a("a"),Vro=o("GPT2LMHeadModel"),Wro=o(" (OpenAI GPT-2 model)"),Hro=l(),Dg=a("li"),SH=a("strong"),Uro=o("ibert"),Jro=o(" \u2014 "),t9=a("a"),Kro=o("IBertForMaskedLM"),Yro=o(" (I-BERT model)"),Zro=l(),Ng=a("li"),$H=a("strong"),eto=o("layoutlm"),oto=o(" \u2014 "),a9=a("a"),rto=o("LayoutLMForMaskedLM"),tto=o(" (LayoutLM model)"),ato=l(),jg=a("li"),IH=a("strong"),sto=o("longformer"),nto=o(" \u2014 "),s9=a("a"),lto=o("LongformerForMaskedLM"),ito=o(" (Longformer model)"),dto=l(),Og=a("li"),DH=a("strong"),cto=o("lxmert"),mto=o(" \u2014 "),n9=a("a"),fto=o("LxmertForPreTraining"),hto=o(" (LXMERT model)"),gto=l(),Gg=a("li"),NH=a("strong"),uto=o("megatron-bert"),pto=o(" \u2014 "),l9=a("a"),_to=o("MegatronBertForPreTraining"),bto=o(" (MegatronBert model)"),vto=l(),qg=a("li"),jH=a("strong"),Tto=o("mobilebert"),Fto=o(" \u2014 "),i9=a("a"),Eto=o("MobileBertForPreTraining"),Cto=o(" (MobileBERT model)"),Mto=l(),zg=a("li"),OH=a("strong"),yto=o("mpnet"),wto=o(" \u2014 "),d9=a("a"),Ato=o("MPNetForMaskedLM"),Lto=o(" (MPNet model)"),Bto=l(),Xg=a("li"),GH=a("strong"),xto=o("openai-gpt"),kto=o(" \u2014 "),c9=a("a"),Rto=o("OpenAIGPTLMHeadModel"),Pto=o(" (OpenAI GPT model)"),Sto=l(),Qg=a("li"),qH=a("strong"),$to=o("retribert"),Ito=o(" \u2014 "),m9=a("a"),Dto=o("RetriBertModel"),Nto=o(" (RetriBERT model)"),jto=l(),Vg=a("li"),zH=a("strong"),Oto=o("roberta"),Gto=o(" \u2014 "),f9=a("a"),qto=o("RobertaForMaskedLM"),zto=o(" (RoBERTa model)"),Xto=l(),Wg=a("li"),XH=a("strong"),Qto=o("squeezebert"),Vto=o(" \u2014 "),h9=a("a"),Wto=o("SqueezeBertForMaskedLM"),Hto=o(" (SqueezeBERT model)"),Uto=l(),Hg=a("li"),QH=a("strong"),Jto=o("t5"),Kto=o(" \u2014 "),g9=a("a"),Yto=o("T5ForConditionalGeneration"),Zto=o(" (T5 model)"),eao=l(),Ug=a("li"),VH=a("strong"),oao=o("tapas"),rao=o(" \u2014 "),u9=a("a"),tao=o("TapasForMaskedLM"),aao=o(" (TAPAS model)"),sao=l(),Jg=a("li"),WH=a("strong"),nao=o("transfo-xl"),lao=o(" \u2014 "),p9=a("a"),iao=o("TransfoXLLMHeadModel"),dao=o(" (Transformer-XL model)"),cao=l(),Kg=a("li"),HH=a("strong"),mao=o("unispeech"),fao=o(" \u2014 "),_9=a("a"),hao=o("UniSpeechForPreTraining"),gao=o(" (UniSpeech model)"),uao=l(),Yg=a("li"),UH=a("strong"),pao=o("unispeech-sat"),_ao=o(" \u2014 "),b9=a("a"),bao=o("UniSpeechSatForPreTraining"),vao=o(" (UniSpeechSat model)"),Tao=l(),Zg=a("li"),JH=a("strong"),Fao=o("visual_bert"),Eao=o(" \u2014 "),v9=a("a"),Cao=o("VisualBertForPreTraining"),Mao=o(" (VisualBert model)"),yao=l(),eu=a("li"),KH=a("strong"),wao=o("wav2vec2"),Aao=o(" \u2014 "),T9=a("a"),Lao=o("Wav2Vec2ForPreTraining"),Bao=o(" (Wav2Vec2 model)"),xao=l(),ou=a("li"),YH=a("strong"),kao=o("xlm"),Rao=o(" \u2014 "),F9=a("a"),Pao=o("XLMWithLMHeadModel"),Sao=o(" (XLM model)"),$ao=l(),ru=a("li"),ZH=a("strong"),Iao=o("xlm-roberta"),Dao=o(" \u2014 "),E9=a("a"),Nao=o("XLMRobertaForMaskedLM"),jao=o(" (XLM-RoBERTa model)"),Oao=l(),tu=a("li"),eU=a("strong"),Gao=o("xlnet"),qao=o(" \u2014 "),C9=a("a"),zao=o("XLNetLMHeadModel"),Xao=o(" (XLNet model)"),Qao=l(),au=a("p"),Vao=o("The model is set in evaluation mode by default using "),oU=a("code"),Wao=o("model.eval()"),Hao=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),rU=a("code"),Uao=o("model.train()"),Jao=l(),tU=a("p"),Kao=o("Examples:"),Yao=l(),m(F3.$$.fragment),F5e=l(),_i=a("h2"),su=a("a"),aU=a("span"),m(E3.$$.fragment),Zao=l(),sU=a("span"),eso=o("AutoModelForCausalLM"),E5e=l(),Go=a("div"),m(C3.$$.fragment),oso=l(),bi=a("p"),rso=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),nU=a("code"),tso=o("from_pretrained()"),aso=o(` class method or the
`),lU=a("code"),sso=o("from_config()"),nso=o(" class method."),lso=l(),M3=a("p"),iso=o("This class cannot be instantiated directly using "),iU=a("code"),dso=o("__init__()"),cso=o(" (throws an error)."),mso=l(),Pr=a("div"),m(y3.$$.fragment),fso=l(),dU=a("p"),hso=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),gso=l(),vi=a("p"),uso=o(`Note:
Loading a model from its configuration file does `),cU=a("strong"),pso=o("not"),_so=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),mU=a("code"),bso=o("from_pretrained()"),vso=o(` to load the model
weights.`),Tso=l(),fU=a("p"),Fso=o("Examples:"),Eso=l(),m(w3.$$.fragment),Cso=l(),Ie=a("div"),m(A3.$$.fragment),Mso=l(),hU=a("p"),yso=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),wso=l(),La=a("p"),Aso=o("The model class to instantiate is selected based on the "),gU=a("code"),Lso=o("model_type"),Bso=o(` property of the config object (either
passed as an argument or loaded from `),uU=a("code"),xso=o("pretrained_model_name_or_path"),kso=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),pU=a("code"),Rso=o("pretrained_model_name_or_path"),Pso=o(":"),Sso=l(),q=a("ul"),nu=a("li"),_U=a("strong"),$so=o("bart"),Iso=o(" \u2014 "),M9=a("a"),Dso=o("BartForCausalLM"),Nso=o(" (BART model)"),jso=l(),lu=a("li"),bU=a("strong"),Oso=o("bert"),Gso=o(" \u2014 "),y9=a("a"),qso=o("BertLMHeadModel"),zso=o(" (BERT model)"),Xso=l(),iu=a("li"),vU=a("strong"),Qso=o("bert-generation"),Vso=o(" \u2014 "),w9=a("a"),Wso=o("BertGenerationDecoder"),Hso=o(" (Bert Generation model)"),Uso=l(),du=a("li"),TU=a("strong"),Jso=o("big_bird"),Kso=o(" \u2014 "),A9=a("a"),Yso=o("BigBirdForCausalLM"),Zso=o(" (BigBird model)"),eno=l(),cu=a("li"),FU=a("strong"),ono=o("bigbird_pegasus"),rno=o(" \u2014 "),L9=a("a"),tno=o("BigBirdPegasusForCausalLM"),ano=o(" (BigBirdPegasus model)"),sno=l(),mu=a("li"),EU=a("strong"),nno=o("blenderbot"),lno=o(" \u2014 "),B9=a("a"),ino=o("BlenderbotForCausalLM"),dno=o(" (Blenderbot model)"),cno=l(),fu=a("li"),CU=a("strong"),mno=o("blenderbot-small"),fno=o(" \u2014 "),x9=a("a"),hno=o("BlenderbotSmallForCausalLM"),gno=o(" (BlenderbotSmall model)"),uno=l(),hu=a("li"),MU=a("strong"),pno=o("camembert"),_no=o(" \u2014 "),k9=a("a"),bno=o("CamembertForCausalLM"),vno=o(" (CamemBERT model)"),Tno=l(),gu=a("li"),yU=a("strong"),Fno=o("ctrl"),Eno=o(" \u2014 "),R9=a("a"),Cno=o("CTRLLMHeadModel"),Mno=o(" (CTRL model)"),yno=l(),uu=a("li"),wU=a("strong"),wno=o("gpt2"),Ano=o(" \u2014 "),P9=a("a"),Lno=o("GPT2LMHeadModel"),Bno=o(" (OpenAI GPT-2 model)"),xno=l(),pu=a("li"),AU=a("strong"),kno=o("gpt_neo"),Rno=o(" \u2014 "),S9=a("a"),Pno=o("GPTNeoForCausalLM"),Sno=o(" (GPT Neo model)"),$no=l(),_u=a("li"),LU=a("strong"),Ino=o("gptj"),Dno=o(" \u2014 "),$9=a("a"),Nno=o("GPTJForCausalLM"),jno=o(" (GPT-J model)"),Ono=l(),bu=a("li"),BU=a("strong"),Gno=o("marian"),qno=o(" \u2014 "),I9=a("a"),zno=o("MarianForCausalLM"),Xno=o(" (Marian model)"),Qno=l(),vu=a("li"),xU=a("strong"),Vno=o("mbart"),Wno=o(" \u2014 "),D9=a("a"),Hno=o("MBartForCausalLM"),Uno=o(" (mBART model)"),Jno=l(),Tu=a("li"),kU=a("strong"),Kno=o("megatron-bert"),Yno=o(" \u2014 "),N9=a("a"),Zno=o("MegatronBertForCausalLM"),elo=o(" (MegatronBert model)"),olo=l(),Fu=a("li"),RU=a("strong"),rlo=o("openai-gpt"),tlo=o(" \u2014 "),j9=a("a"),alo=o("OpenAIGPTLMHeadModel"),slo=o(" (OpenAI GPT model)"),nlo=l(),Eu=a("li"),PU=a("strong"),llo=o("pegasus"),ilo=o(" \u2014 "),O9=a("a"),dlo=o("PegasusForCausalLM"),clo=o(" (Pegasus model)"),mlo=l(),Cu=a("li"),SU=a("strong"),flo=o("prophetnet"),hlo=o(" \u2014 "),G9=a("a"),glo=o("ProphetNetForCausalLM"),ulo=o(" (ProphetNet model)"),plo=l(),Mu=a("li"),$U=a("strong"),_lo=o("qdqbert"),blo=o(" \u2014 "),q9=a("a"),vlo=o("QDQBertLMHeadModel"),Tlo=o(" (QDQBert model)"),Flo=l(),yu=a("li"),IU=a("strong"),Elo=o("reformer"),Clo=o(" \u2014 "),z9=a("a"),Mlo=o("ReformerModelWithLMHead"),ylo=o(" (Reformer model)"),wlo=l(),wu=a("li"),DU=a("strong"),Alo=o("rembert"),Llo=o(" \u2014 "),X9=a("a"),Blo=o("RemBertForCausalLM"),xlo=o(" (RemBERT model)"),klo=l(),Au=a("li"),NU=a("strong"),Rlo=o("roberta"),Plo=o(" \u2014 "),Q9=a("a"),Slo=o("RobertaForCausalLM"),$lo=o(" (RoBERTa model)"),Ilo=l(),Lu=a("li"),jU=a("strong"),Dlo=o("roformer"),Nlo=o(" \u2014 "),V9=a("a"),jlo=o("RoFormerForCausalLM"),Olo=o(" (RoFormer model)"),Glo=l(),Bu=a("li"),OU=a("strong"),qlo=o("speech_to_text_2"),zlo=o(" \u2014 "),W9=a("a"),Xlo=o("Speech2Text2ForCausalLM"),Qlo=o(" (Speech2Text2 model)"),Vlo=l(),xu=a("li"),GU=a("strong"),Wlo=o("transfo-xl"),Hlo=o(" \u2014 "),H9=a("a"),Ulo=o("TransfoXLLMHeadModel"),Jlo=o(" (Transformer-XL model)"),Klo=l(),ku=a("li"),qU=a("strong"),Ylo=o("trocr"),Zlo=o(" \u2014 "),U9=a("a"),eio=o("TrOCRForCausalLM"),oio=o(" (TrOCR model)"),rio=l(),Ru=a("li"),zU=a("strong"),tio=o("xlm"),aio=o(" \u2014 "),J9=a("a"),sio=o("XLMWithLMHeadModel"),nio=o(" (XLM model)"),lio=l(),Pu=a("li"),XU=a("strong"),iio=o("xlm-prophetnet"),dio=o(" \u2014 "),K9=a("a"),cio=o("XLMProphetNetForCausalLM"),mio=o(" (XLMProphetNet model)"),fio=l(),Su=a("li"),QU=a("strong"),hio=o("xlm-roberta"),gio=o(" \u2014 "),Y9=a("a"),uio=o("XLMRobertaForCausalLM"),pio=o(" (XLM-RoBERTa model)"),_io=l(),$u=a("li"),VU=a("strong"),bio=o("xlnet"),vio=o(" \u2014 "),Z9=a("a"),Tio=o("XLNetLMHeadModel"),Fio=o(" (XLNet model)"),Eio=l(),Iu=a("p"),Cio=o("The model is set in evaluation mode by default using "),WU=a("code"),Mio=o("model.eval()"),yio=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HU=a("code"),wio=o("model.train()"),Aio=l(),UU=a("p"),Lio=o("Examples:"),Bio=l(),m(L3.$$.fragment),C5e=l(),Ti=a("h2"),Du=a("a"),JU=a("span"),m(B3.$$.fragment),xio=l(),KU=a("span"),kio=o("AutoModelForMaskedLM"),M5e=l(),qo=a("div"),m(x3.$$.fragment),Rio=l(),Fi=a("p"),Pio=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),YU=a("code"),Sio=o("from_pretrained()"),$io=o(` class method or the
`),ZU=a("code"),Iio=o("from_config()"),Dio=o(" class method."),Nio=l(),k3=a("p"),jio=o("This class cannot be instantiated directly using "),eJ=a("code"),Oio=o("__init__()"),Gio=o(" (throws an error)."),qio=l(),Sr=a("div"),m(R3.$$.fragment),zio=l(),oJ=a("p"),Xio=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Qio=l(),Ei=a("p"),Vio=o(`Note:
Loading a model from its configuration file does `),rJ=a("strong"),Wio=o("not"),Hio=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tJ=a("code"),Uio=o("from_pretrained()"),Jio=o(` to load the model
weights.`),Kio=l(),aJ=a("p"),Yio=o("Examples:"),Zio=l(),m(P3.$$.fragment),edo=l(),De=a("div"),m(S3.$$.fragment),odo=l(),sJ=a("p"),rdo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),tdo=l(),Ba=a("p"),ado=o("The model class to instantiate is selected based on the "),nJ=a("code"),sdo=o("model_type"),ndo=o(` property of the config object (either
passed as an argument or loaded from `),lJ=a("code"),ldo=o("pretrained_model_name_or_path"),ido=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),iJ=a("code"),ddo=o("pretrained_model_name_or_path"),cdo=o(":"),mdo=l(),O=a("ul"),Nu=a("li"),dJ=a("strong"),fdo=o("albert"),hdo=o(" \u2014 "),ex=a("a"),gdo=o("AlbertForMaskedLM"),udo=o(" (ALBERT model)"),pdo=l(),ju=a("li"),cJ=a("strong"),_do=o("bart"),bdo=o(" \u2014 "),ox=a("a"),vdo=o("BartForConditionalGeneration"),Tdo=o(" (BART model)"),Fdo=l(),Ou=a("li"),mJ=a("strong"),Edo=o("bert"),Cdo=o(" \u2014 "),rx=a("a"),Mdo=o("BertForMaskedLM"),ydo=o(" (BERT model)"),wdo=l(),Gu=a("li"),fJ=a("strong"),Ado=o("big_bird"),Ldo=o(" \u2014 "),tx=a("a"),Bdo=o("BigBirdForMaskedLM"),xdo=o(" (BigBird model)"),kdo=l(),qu=a("li"),hJ=a("strong"),Rdo=o("camembert"),Pdo=o(" \u2014 "),ax=a("a"),Sdo=o("CamembertForMaskedLM"),$do=o(" (CamemBERT model)"),Ido=l(),zu=a("li"),gJ=a("strong"),Ddo=o("convbert"),Ndo=o(" \u2014 "),sx=a("a"),jdo=o("ConvBertForMaskedLM"),Odo=o(" (ConvBERT model)"),Gdo=l(),Xu=a("li"),uJ=a("strong"),qdo=o("deberta"),zdo=o(" \u2014 "),nx=a("a"),Xdo=o("DebertaForMaskedLM"),Qdo=o(" (DeBERTa model)"),Vdo=l(),Qu=a("li"),pJ=a("strong"),Wdo=o("deberta-v2"),Hdo=o(" \u2014 "),lx=a("a"),Udo=o("DebertaV2ForMaskedLM"),Jdo=o(" (DeBERTa-v2 model)"),Kdo=l(),Vu=a("li"),_J=a("strong"),Ydo=o("distilbert"),Zdo=o(" \u2014 "),ix=a("a"),eco=o("DistilBertForMaskedLM"),oco=o(" (DistilBERT model)"),rco=l(),Wu=a("li"),bJ=a("strong"),tco=o("electra"),aco=o(" \u2014 "),dx=a("a"),sco=o("ElectraForMaskedLM"),nco=o(" (ELECTRA model)"),lco=l(),Hu=a("li"),vJ=a("strong"),ico=o("flaubert"),dco=o(" \u2014 "),cx=a("a"),cco=o("FlaubertWithLMHeadModel"),mco=o(" (FlauBERT model)"),fco=l(),Uu=a("li"),TJ=a("strong"),hco=o("fnet"),gco=o(" \u2014 "),mx=a("a"),uco=o("FNetForMaskedLM"),pco=o(" (FNet model)"),_co=l(),Ju=a("li"),FJ=a("strong"),bco=o("funnel"),vco=o(" \u2014 "),fx=a("a"),Tco=o("FunnelForMaskedLM"),Fco=o(" (Funnel Transformer model)"),Eco=l(),Ku=a("li"),EJ=a("strong"),Cco=o("ibert"),Mco=o(" \u2014 "),hx=a("a"),yco=o("IBertForMaskedLM"),wco=o(" (I-BERT model)"),Aco=l(),Yu=a("li"),CJ=a("strong"),Lco=o("layoutlm"),Bco=o(" \u2014 "),gx=a("a"),xco=o("LayoutLMForMaskedLM"),kco=o(" (LayoutLM model)"),Rco=l(),Zu=a("li"),MJ=a("strong"),Pco=o("longformer"),Sco=o(" \u2014 "),ux=a("a"),$co=o("LongformerForMaskedLM"),Ico=o(" (Longformer model)"),Dco=l(),ep=a("li"),yJ=a("strong"),Nco=o("mbart"),jco=o(" \u2014 "),px=a("a"),Oco=o("MBartForConditionalGeneration"),Gco=o(" (mBART model)"),qco=l(),op=a("li"),wJ=a("strong"),zco=o("megatron-bert"),Xco=o(" \u2014 "),_x=a("a"),Qco=o("MegatronBertForMaskedLM"),Vco=o(" (MegatronBert model)"),Wco=l(),rp=a("li"),AJ=a("strong"),Hco=o("mobilebert"),Uco=o(" \u2014 "),bx=a("a"),Jco=o("MobileBertForMaskedLM"),Kco=o(" (MobileBERT model)"),Yco=l(),tp=a("li"),LJ=a("strong"),Zco=o("mpnet"),emo=o(" \u2014 "),vx=a("a"),omo=o("MPNetForMaskedLM"),rmo=o(" (MPNet model)"),tmo=l(),ap=a("li"),BJ=a("strong"),amo=o("perceiver"),smo=o(" \u2014 "),Tx=a("a"),nmo=o("PerceiverForMaskedLM"),lmo=o(" (Perceiver model)"),imo=l(),sp=a("li"),xJ=a("strong"),dmo=o("qdqbert"),cmo=o(" \u2014 "),Fx=a("a"),mmo=o("QDQBertForMaskedLM"),fmo=o(" (QDQBert model)"),hmo=l(),np=a("li"),kJ=a("strong"),gmo=o("reformer"),umo=o(" \u2014 "),Ex=a("a"),pmo=o("ReformerForMaskedLM"),_mo=o(" (Reformer model)"),bmo=l(),lp=a("li"),RJ=a("strong"),vmo=o("rembert"),Tmo=o(" \u2014 "),Cx=a("a"),Fmo=o("RemBertForMaskedLM"),Emo=o(" (RemBERT model)"),Cmo=l(),ip=a("li"),PJ=a("strong"),Mmo=o("roberta"),ymo=o(" \u2014 "),Mx=a("a"),wmo=o("RobertaForMaskedLM"),Amo=o(" (RoBERTa model)"),Lmo=l(),dp=a("li"),SJ=a("strong"),Bmo=o("roformer"),xmo=o(" \u2014 "),yx=a("a"),kmo=o("RoFormerForMaskedLM"),Rmo=o(" (RoFormer model)"),Pmo=l(),cp=a("li"),$J=a("strong"),Smo=o("squeezebert"),$mo=o(" \u2014 "),wx=a("a"),Imo=o("SqueezeBertForMaskedLM"),Dmo=o(" (SqueezeBERT model)"),Nmo=l(),mp=a("li"),IJ=a("strong"),jmo=o("tapas"),Omo=o(" \u2014 "),Ax=a("a"),Gmo=o("TapasForMaskedLM"),qmo=o(" (TAPAS model)"),zmo=l(),fp=a("li"),DJ=a("strong"),Xmo=o("wav2vec2"),Qmo=o(" \u2014 "),NJ=a("code"),Vmo=o("Wav2Vec2ForMaskedLM"),Wmo=o(" (Wav2Vec2 model)"),Hmo=l(),hp=a("li"),jJ=a("strong"),Umo=o("xlm"),Jmo=o(" \u2014 "),Lx=a("a"),Kmo=o("XLMWithLMHeadModel"),Ymo=o(" (XLM model)"),Zmo=l(),gp=a("li"),OJ=a("strong"),efo=o("xlm-roberta"),ofo=o(" \u2014 "),Bx=a("a"),rfo=o("XLMRobertaForMaskedLM"),tfo=o(" (XLM-RoBERTa model)"),afo=l(),up=a("p"),sfo=o("The model is set in evaluation mode by default using "),GJ=a("code"),nfo=o("model.eval()"),lfo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),qJ=a("code"),ifo=o("model.train()"),dfo=l(),zJ=a("p"),cfo=o("Examples:"),mfo=l(),m($3.$$.fragment),y5e=l(),Ci=a("h2"),pp=a("a"),XJ=a("span"),m(I3.$$.fragment),ffo=l(),QJ=a("span"),hfo=o("AutoModelForSeq2SeqLM"),w5e=l(),zo=a("div"),m(D3.$$.fragment),gfo=l(),Mi=a("p"),ufo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),VJ=a("code"),pfo=o("from_pretrained()"),_fo=o(` class method or the
`),WJ=a("code"),bfo=o("from_config()"),vfo=o(" class method."),Tfo=l(),N3=a("p"),Ffo=o("This class cannot be instantiated directly using "),HJ=a("code"),Efo=o("__init__()"),Cfo=o(" (throws an error)."),Mfo=l(),$r=a("div"),m(j3.$$.fragment),yfo=l(),UJ=a("p"),wfo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Afo=l(),yi=a("p"),Lfo=o(`Note:
Loading a model from its configuration file does `),JJ=a("strong"),Bfo=o("not"),xfo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),KJ=a("code"),kfo=o("from_pretrained()"),Rfo=o(` to load the model
weights.`),Pfo=l(),YJ=a("p"),Sfo=o("Examples:"),$fo=l(),m(O3.$$.fragment),Ifo=l(),Ne=a("div"),m(G3.$$.fragment),Dfo=l(),ZJ=a("p"),Nfo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),jfo=l(),xa=a("p"),Ofo=o("The model class to instantiate is selected based on the "),eK=a("code"),Gfo=o("model_type"),qfo=o(` property of the config object (either
passed as an argument or loaded from `),oK=a("code"),zfo=o("pretrained_model_name_or_path"),Xfo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),rK=a("code"),Qfo=o("pretrained_model_name_or_path"),Vfo=o(":"),Wfo=l(),me=a("ul"),_p=a("li"),tK=a("strong"),Hfo=o("bart"),Ufo=o(" \u2014 "),xx=a("a"),Jfo=o("BartForConditionalGeneration"),Kfo=o(" (BART model)"),Yfo=l(),bp=a("li"),aK=a("strong"),Zfo=o("bigbird_pegasus"),eho=o(" \u2014 "),kx=a("a"),oho=o("BigBirdPegasusForConditionalGeneration"),rho=o(" (BigBirdPegasus model)"),tho=l(),vp=a("li"),sK=a("strong"),aho=o("blenderbot"),sho=o(" \u2014 "),Rx=a("a"),nho=o("BlenderbotForConditionalGeneration"),lho=o(" (Blenderbot model)"),iho=l(),Tp=a("li"),nK=a("strong"),dho=o("blenderbot-small"),cho=o(" \u2014 "),Px=a("a"),mho=o("BlenderbotSmallForConditionalGeneration"),fho=o(" (BlenderbotSmall model)"),hho=l(),Fp=a("li"),lK=a("strong"),gho=o("encoder-decoder"),uho=o(" \u2014 "),Sx=a("a"),pho=o("EncoderDecoderModel"),_ho=o(" (Encoder decoder model)"),bho=l(),Ep=a("li"),iK=a("strong"),vho=o("fsmt"),Tho=o(" \u2014 "),$x=a("a"),Fho=o("FSMTForConditionalGeneration"),Eho=o(" (FairSeq Machine-Translation model)"),Cho=l(),Cp=a("li"),dK=a("strong"),Mho=o("led"),yho=o(" \u2014 "),Ix=a("a"),who=o("LEDForConditionalGeneration"),Aho=o(" (LED model)"),Lho=l(),Mp=a("li"),cK=a("strong"),Bho=o("m2m_100"),xho=o(" \u2014 "),Dx=a("a"),kho=o("M2M100ForConditionalGeneration"),Rho=o(" (M2M100 model)"),Pho=l(),yp=a("li"),mK=a("strong"),Sho=o("marian"),$ho=o(" \u2014 "),Nx=a("a"),Iho=o("MarianMTModel"),Dho=o(" (Marian model)"),Nho=l(),wp=a("li"),fK=a("strong"),jho=o("mbart"),Oho=o(" \u2014 "),jx=a("a"),Gho=o("MBartForConditionalGeneration"),qho=o(" (mBART model)"),zho=l(),Ap=a("li"),hK=a("strong"),Xho=o("mt5"),Qho=o(" \u2014 "),Ox=a("a"),Vho=o("MT5ForConditionalGeneration"),Who=o(" (mT5 model)"),Hho=l(),Lp=a("li"),gK=a("strong"),Uho=o("pegasus"),Jho=o(" \u2014 "),Gx=a("a"),Kho=o("PegasusForConditionalGeneration"),Yho=o(" (Pegasus model)"),Zho=l(),Bp=a("li"),uK=a("strong"),ego=o("prophetnet"),ogo=o(" \u2014 "),qx=a("a"),rgo=o("ProphetNetForConditionalGeneration"),tgo=o(" (ProphetNet model)"),ago=l(),xp=a("li"),pK=a("strong"),sgo=o("t5"),ngo=o(" \u2014 "),zx=a("a"),lgo=o("T5ForConditionalGeneration"),igo=o(" (T5 model)"),dgo=l(),kp=a("li"),_K=a("strong"),cgo=o("xlm-prophetnet"),mgo=o(" \u2014 "),Xx=a("a"),fgo=o("XLMProphetNetForConditionalGeneration"),hgo=o(" (XLMProphetNet model)"),ggo=l(),Rp=a("p"),ugo=o("The model is set in evaluation mode by default using "),bK=a("code"),pgo=o("model.eval()"),_go=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vK=a("code"),bgo=o("model.train()"),vgo=l(),TK=a("p"),Tgo=o("Examples:"),Fgo=l(),m(q3.$$.fragment),A5e=l(),wi=a("h2"),Pp=a("a"),FK=a("span"),m(z3.$$.fragment),Ego=l(),EK=a("span"),Cgo=o("AutoModelForSequenceClassification"),L5e=l(),Xo=a("div"),m(X3.$$.fragment),Mgo=l(),Ai=a("p"),ygo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),CK=a("code"),wgo=o("from_pretrained()"),Ago=o(` class method or the
`),MK=a("code"),Lgo=o("from_config()"),Bgo=o(" class method."),xgo=l(),Q3=a("p"),kgo=o("This class cannot be instantiated directly using "),yK=a("code"),Rgo=o("__init__()"),Pgo=o(" (throws an error)."),Sgo=l(),Ir=a("div"),m(V3.$$.fragment),$go=l(),wK=a("p"),Igo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Dgo=l(),Li=a("p"),Ngo=o(`Note:
Loading a model from its configuration file does `),AK=a("strong"),jgo=o("not"),Ogo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),LK=a("code"),Ggo=o("from_pretrained()"),qgo=o(` to load the model
weights.`),zgo=l(),BK=a("p"),Xgo=o("Examples:"),Qgo=l(),m(W3.$$.fragment),Vgo=l(),je=a("div"),m(H3.$$.fragment),Wgo=l(),xK=a("p"),Hgo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Ugo=l(),ka=a("p"),Jgo=o("The model class to instantiate is selected based on the "),kK=a("code"),Kgo=o("model_type"),Ygo=o(` property of the config object (either
passed as an argument or loaded from `),RK=a("code"),Zgo=o("pretrained_model_name_or_path"),euo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),PK=a("code"),ouo=o("pretrained_model_name_or_path"),ruo=o(":"),tuo=l(),A=a("ul"),Sp=a("li"),SK=a("strong"),auo=o("albert"),suo=o(" \u2014 "),Qx=a("a"),nuo=o("AlbertForSequenceClassification"),luo=o(" (ALBERT model)"),iuo=l(),$p=a("li"),$K=a("strong"),duo=o("bart"),cuo=o(" \u2014 "),Vx=a("a"),muo=o("BartForSequenceClassification"),fuo=o(" (BART model)"),huo=l(),Ip=a("li"),IK=a("strong"),guo=o("bert"),uuo=o(" \u2014 "),Wx=a("a"),puo=o("BertForSequenceClassification"),_uo=o(" (BERT model)"),buo=l(),Dp=a("li"),DK=a("strong"),vuo=o("big_bird"),Tuo=o(" \u2014 "),Hx=a("a"),Fuo=o("BigBirdForSequenceClassification"),Euo=o(" (BigBird model)"),Cuo=l(),Np=a("li"),NK=a("strong"),Muo=o("bigbird_pegasus"),yuo=o(" \u2014 "),Ux=a("a"),wuo=o("BigBirdPegasusForSequenceClassification"),Auo=o(" (BigBirdPegasus model)"),Luo=l(),jp=a("li"),jK=a("strong"),Buo=o("camembert"),xuo=o(" \u2014 "),Jx=a("a"),kuo=o("CamembertForSequenceClassification"),Ruo=o(" (CamemBERT model)"),Puo=l(),Op=a("li"),OK=a("strong"),Suo=o("canine"),$uo=o(" \u2014 "),Kx=a("a"),Iuo=o("CanineForSequenceClassification"),Duo=o(" (Canine model)"),Nuo=l(),Gp=a("li"),GK=a("strong"),juo=o("convbert"),Ouo=o(" \u2014 "),Yx=a("a"),Guo=o("ConvBertForSequenceClassification"),quo=o(" (ConvBERT model)"),zuo=l(),qp=a("li"),qK=a("strong"),Xuo=o("ctrl"),Quo=o(" \u2014 "),Zx=a("a"),Vuo=o("CTRLForSequenceClassification"),Wuo=o(" (CTRL model)"),Huo=l(),zp=a("li"),zK=a("strong"),Uuo=o("deberta"),Juo=o(" \u2014 "),ek=a("a"),Kuo=o("DebertaForSequenceClassification"),Yuo=o(" (DeBERTa model)"),Zuo=l(),Xp=a("li"),XK=a("strong"),epo=o("deberta-v2"),opo=o(" \u2014 "),ok=a("a"),rpo=o("DebertaV2ForSequenceClassification"),tpo=o(" (DeBERTa-v2 model)"),apo=l(),Qp=a("li"),QK=a("strong"),spo=o("distilbert"),npo=o(" \u2014 "),rk=a("a"),lpo=o("DistilBertForSequenceClassification"),ipo=o(" (DistilBERT model)"),dpo=l(),Vp=a("li"),VK=a("strong"),cpo=o("electra"),mpo=o(" \u2014 "),tk=a("a"),fpo=o("ElectraForSequenceClassification"),hpo=o(" (ELECTRA model)"),gpo=l(),Wp=a("li"),WK=a("strong"),upo=o("flaubert"),ppo=o(" \u2014 "),ak=a("a"),_po=o("FlaubertForSequenceClassification"),bpo=o(" (FlauBERT model)"),vpo=l(),Hp=a("li"),HK=a("strong"),Tpo=o("fnet"),Fpo=o(" \u2014 "),sk=a("a"),Epo=o("FNetForSequenceClassification"),Cpo=o(" (FNet model)"),Mpo=l(),Up=a("li"),UK=a("strong"),ypo=o("funnel"),wpo=o(" \u2014 "),nk=a("a"),Apo=o("FunnelForSequenceClassification"),Lpo=o(" (Funnel Transformer model)"),Bpo=l(),Jp=a("li"),JK=a("strong"),xpo=o("gpt2"),kpo=o(" \u2014 "),lk=a("a"),Rpo=o("GPT2ForSequenceClassification"),Ppo=o(" (OpenAI GPT-2 model)"),Spo=l(),Kp=a("li"),KK=a("strong"),$po=o("gpt_neo"),Ipo=o(" \u2014 "),ik=a("a"),Dpo=o("GPTNeoForSequenceClassification"),Npo=o(" (GPT Neo model)"),jpo=l(),Yp=a("li"),YK=a("strong"),Opo=o("gptj"),Gpo=o(" \u2014 "),dk=a("a"),qpo=o("GPTJForSequenceClassification"),zpo=o(" (GPT-J model)"),Xpo=l(),Zp=a("li"),ZK=a("strong"),Qpo=o("ibert"),Vpo=o(" \u2014 "),ck=a("a"),Wpo=o("IBertForSequenceClassification"),Hpo=o(" (I-BERT model)"),Upo=l(),e_=a("li"),eY=a("strong"),Jpo=o("layoutlm"),Kpo=o(" \u2014 "),mk=a("a"),Ypo=o("LayoutLMForSequenceClassification"),Zpo=o(" (LayoutLM model)"),e_o=l(),o_=a("li"),oY=a("strong"),o_o=o("layoutlmv2"),r_o=o(" \u2014 "),fk=a("a"),t_o=o("LayoutLMv2ForSequenceClassification"),a_o=o(" (LayoutLMv2 model)"),s_o=l(),r_=a("li"),rY=a("strong"),n_o=o("led"),l_o=o(" \u2014 "),hk=a("a"),i_o=o("LEDForSequenceClassification"),d_o=o(" (LED model)"),c_o=l(),t_=a("li"),tY=a("strong"),m_o=o("longformer"),f_o=o(" \u2014 "),gk=a("a"),h_o=o("LongformerForSequenceClassification"),g_o=o(" (Longformer model)"),u_o=l(),a_=a("li"),aY=a("strong"),p_o=o("mbart"),__o=o(" \u2014 "),uk=a("a"),b_o=o("MBartForSequenceClassification"),v_o=o(" (mBART model)"),T_o=l(),s_=a("li"),sY=a("strong"),F_o=o("megatron-bert"),E_o=o(" \u2014 "),pk=a("a"),C_o=o("MegatronBertForSequenceClassification"),M_o=o(" (MegatronBert model)"),y_o=l(),n_=a("li"),nY=a("strong"),w_o=o("mobilebert"),A_o=o(" \u2014 "),_k=a("a"),L_o=o("MobileBertForSequenceClassification"),B_o=o(" (MobileBERT model)"),x_o=l(),l_=a("li"),lY=a("strong"),k_o=o("mpnet"),R_o=o(" \u2014 "),bk=a("a"),P_o=o("MPNetForSequenceClassification"),S_o=o(" (MPNet model)"),$_o=l(),i_=a("li"),iY=a("strong"),I_o=o("openai-gpt"),D_o=o(" \u2014 "),vk=a("a"),N_o=o("OpenAIGPTForSequenceClassification"),j_o=o(" (OpenAI GPT model)"),O_o=l(),d_=a("li"),dY=a("strong"),G_o=o("perceiver"),q_o=o(" \u2014 "),Tk=a("a"),z_o=o("PerceiverForSequenceClassification"),X_o=o(" (Perceiver model)"),Q_o=l(),c_=a("li"),cY=a("strong"),V_o=o("qdqbert"),W_o=o(" \u2014 "),Fk=a("a"),H_o=o("QDQBertForSequenceClassification"),U_o=o(" (QDQBert model)"),J_o=l(),m_=a("li"),mY=a("strong"),K_o=o("reformer"),Y_o=o(" \u2014 "),Ek=a("a"),Z_o=o("ReformerForSequenceClassification"),ebo=o(" (Reformer model)"),obo=l(),f_=a("li"),fY=a("strong"),rbo=o("rembert"),tbo=o(" \u2014 "),Ck=a("a"),abo=o("RemBertForSequenceClassification"),sbo=o(" (RemBERT model)"),nbo=l(),h_=a("li"),hY=a("strong"),lbo=o("roberta"),ibo=o(" \u2014 "),Mk=a("a"),dbo=o("RobertaForSequenceClassification"),cbo=o(" (RoBERTa model)"),mbo=l(),g_=a("li"),gY=a("strong"),fbo=o("roformer"),hbo=o(" \u2014 "),yk=a("a"),gbo=o("RoFormerForSequenceClassification"),ubo=o(" (RoFormer model)"),pbo=l(),u_=a("li"),uY=a("strong"),_bo=o("squeezebert"),bbo=o(" \u2014 "),wk=a("a"),vbo=o("SqueezeBertForSequenceClassification"),Tbo=o(" (SqueezeBERT model)"),Fbo=l(),p_=a("li"),pY=a("strong"),Ebo=o("tapas"),Cbo=o(" \u2014 "),Ak=a("a"),Mbo=o("TapasForSequenceClassification"),ybo=o(" (TAPAS model)"),wbo=l(),__=a("li"),_Y=a("strong"),Abo=o("transfo-xl"),Lbo=o(" \u2014 "),Lk=a("a"),Bbo=o("TransfoXLForSequenceClassification"),xbo=o(" (Transformer-XL model)"),kbo=l(),b_=a("li"),bY=a("strong"),Rbo=o("xlm"),Pbo=o(" \u2014 "),Bk=a("a"),Sbo=o("XLMForSequenceClassification"),$bo=o(" (XLM model)"),Ibo=l(),v_=a("li"),vY=a("strong"),Dbo=o("xlm-roberta"),Nbo=o(" \u2014 "),xk=a("a"),jbo=o("XLMRobertaForSequenceClassification"),Obo=o(" (XLM-RoBERTa model)"),Gbo=l(),T_=a("li"),TY=a("strong"),qbo=o("xlnet"),zbo=o(" \u2014 "),kk=a("a"),Xbo=o("XLNetForSequenceClassification"),Qbo=o(" (XLNet model)"),Vbo=l(),F_=a("p"),Wbo=o("The model is set in evaluation mode by default using "),FY=a("code"),Hbo=o("model.eval()"),Ubo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),EY=a("code"),Jbo=o("model.train()"),Kbo=l(),CY=a("p"),Ybo=o("Examples:"),Zbo=l(),m(U3.$$.fragment),B5e=l(),Bi=a("h2"),E_=a("a"),MY=a("span"),m(J3.$$.fragment),e2o=l(),yY=a("span"),o2o=o("AutoModelForMultipleChoice"),x5e=l(),Qo=a("div"),m(K3.$$.fragment),r2o=l(),xi=a("p"),t2o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),wY=a("code"),a2o=o("from_pretrained()"),s2o=o(` class method or the
`),AY=a("code"),n2o=o("from_config()"),l2o=o(" class method."),i2o=l(),Y3=a("p"),d2o=o("This class cannot be instantiated directly using "),LY=a("code"),c2o=o("__init__()"),m2o=o(" (throws an error)."),f2o=l(),Dr=a("div"),m(Z3.$$.fragment),h2o=l(),BY=a("p"),g2o=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),u2o=l(),ki=a("p"),p2o=o(`Note:
Loading a model from its configuration file does `),xY=a("strong"),_2o=o("not"),b2o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kY=a("code"),v2o=o("from_pretrained()"),T2o=o(` to load the model
weights.`),F2o=l(),RY=a("p"),E2o=o("Examples:"),C2o=l(),m(eM.$$.fragment),M2o=l(),Oe=a("div"),m(oM.$$.fragment),y2o=l(),PY=a("p"),w2o=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),A2o=l(),Ra=a("p"),L2o=o("The model class to instantiate is selected based on the "),SY=a("code"),B2o=o("model_type"),x2o=o(` property of the config object (either
passed as an argument or loaded from `),$Y=a("code"),k2o=o("pretrained_model_name_or_path"),R2o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),IY=a("code"),P2o=o("pretrained_model_name_or_path"),S2o=o(":"),$2o=l(),H=a("ul"),C_=a("li"),DY=a("strong"),I2o=o("albert"),D2o=o(" \u2014 "),Rk=a("a"),N2o=o("AlbertForMultipleChoice"),j2o=o(" (ALBERT model)"),O2o=l(),M_=a("li"),NY=a("strong"),G2o=o("bert"),q2o=o(" \u2014 "),Pk=a("a"),z2o=o("BertForMultipleChoice"),X2o=o(" (BERT model)"),Q2o=l(),y_=a("li"),jY=a("strong"),V2o=o("big_bird"),W2o=o(" \u2014 "),Sk=a("a"),H2o=o("BigBirdForMultipleChoice"),U2o=o(" (BigBird model)"),J2o=l(),w_=a("li"),OY=a("strong"),K2o=o("camembert"),Y2o=o(" \u2014 "),$k=a("a"),Z2o=o("CamembertForMultipleChoice"),evo=o(" (CamemBERT model)"),ovo=l(),A_=a("li"),GY=a("strong"),rvo=o("canine"),tvo=o(" \u2014 "),Ik=a("a"),avo=o("CanineForMultipleChoice"),svo=o(" (Canine model)"),nvo=l(),L_=a("li"),qY=a("strong"),lvo=o("convbert"),ivo=o(" \u2014 "),Dk=a("a"),dvo=o("ConvBertForMultipleChoice"),cvo=o(" (ConvBERT model)"),mvo=l(),B_=a("li"),zY=a("strong"),fvo=o("distilbert"),hvo=o(" \u2014 "),Nk=a("a"),gvo=o("DistilBertForMultipleChoice"),uvo=o(" (DistilBERT model)"),pvo=l(),x_=a("li"),XY=a("strong"),_vo=o("electra"),bvo=o(" \u2014 "),jk=a("a"),vvo=o("ElectraForMultipleChoice"),Tvo=o(" (ELECTRA model)"),Fvo=l(),k_=a("li"),QY=a("strong"),Evo=o("flaubert"),Cvo=o(" \u2014 "),Ok=a("a"),Mvo=o("FlaubertForMultipleChoice"),yvo=o(" (FlauBERT model)"),wvo=l(),R_=a("li"),VY=a("strong"),Avo=o("fnet"),Lvo=o(" \u2014 "),Gk=a("a"),Bvo=o("FNetForMultipleChoice"),xvo=o(" (FNet model)"),kvo=l(),P_=a("li"),WY=a("strong"),Rvo=o("funnel"),Pvo=o(" \u2014 "),qk=a("a"),Svo=o("FunnelForMultipleChoice"),$vo=o(" (Funnel Transformer model)"),Ivo=l(),S_=a("li"),HY=a("strong"),Dvo=o("ibert"),Nvo=o(" \u2014 "),zk=a("a"),jvo=o("IBertForMultipleChoice"),Ovo=o(" (I-BERT model)"),Gvo=l(),$_=a("li"),UY=a("strong"),qvo=o("longformer"),zvo=o(" \u2014 "),Xk=a("a"),Xvo=o("LongformerForMultipleChoice"),Qvo=o(" (Longformer model)"),Vvo=l(),I_=a("li"),JY=a("strong"),Wvo=o("megatron-bert"),Hvo=o(" \u2014 "),Qk=a("a"),Uvo=o("MegatronBertForMultipleChoice"),Jvo=o(" (MegatronBert model)"),Kvo=l(),D_=a("li"),KY=a("strong"),Yvo=o("mobilebert"),Zvo=o(" \u2014 "),Vk=a("a"),eTo=o("MobileBertForMultipleChoice"),oTo=o(" (MobileBERT model)"),rTo=l(),N_=a("li"),YY=a("strong"),tTo=o("mpnet"),aTo=o(" \u2014 "),Wk=a("a"),sTo=o("MPNetForMultipleChoice"),nTo=o(" (MPNet model)"),lTo=l(),j_=a("li"),ZY=a("strong"),iTo=o("qdqbert"),dTo=o(" \u2014 "),Hk=a("a"),cTo=o("QDQBertForMultipleChoice"),mTo=o(" (QDQBert model)"),fTo=l(),O_=a("li"),eZ=a("strong"),hTo=o("rembert"),gTo=o(" \u2014 "),Uk=a("a"),uTo=o("RemBertForMultipleChoice"),pTo=o(" (RemBERT model)"),_To=l(),G_=a("li"),oZ=a("strong"),bTo=o("roberta"),vTo=o(" \u2014 "),Jk=a("a"),TTo=o("RobertaForMultipleChoice"),FTo=o(" (RoBERTa model)"),ETo=l(),q_=a("li"),rZ=a("strong"),CTo=o("roformer"),MTo=o(" \u2014 "),Kk=a("a"),yTo=o("RoFormerForMultipleChoice"),wTo=o(" (RoFormer model)"),ATo=l(),z_=a("li"),tZ=a("strong"),LTo=o("squeezebert"),BTo=o(" \u2014 "),Yk=a("a"),xTo=o("SqueezeBertForMultipleChoice"),kTo=o(" (SqueezeBERT model)"),RTo=l(),X_=a("li"),aZ=a("strong"),PTo=o("xlm"),STo=o(" \u2014 "),Zk=a("a"),$To=o("XLMForMultipleChoice"),ITo=o(" (XLM model)"),DTo=l(),Q_=a("li"),sZ=a("strong"),NTo=o("xlm-roberta"),jTo=o(" \u2014 "),eR=a("a"),OTo=o("XLMRobertaForMultipleChoice"),GTo=o(" (XLM-RoBERTa model)"),qTo=l(),V_=a("li"),nZ=a("strong"),zTo=o("xlnet"),XTo=o(" \u2014 "),oR=a("a"),QTo=o("XLNetForMultipleChoice"),VTo=o(" (XLNet model)"),WTo=l(),W_=a("p"),HTo=o("The model is set in evaluation mode by default using "),lZ=a("code"),UTo=o("model.eval()"),JTo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),iZ=a("code"),KTo=o("model.train()"),YTo=l(),dZ=a("p"),ZTo=o("Examples:"),e1o=l(),m(rM.$$.fragment),k5e=l(),Ri=a("h2"),H_=a("a"),cZ=a("span"),m(tM.$$.fragment),o1o=l(),mZ=a("span"),r1o=o("AutoModelForNextSentencePrediction"),R5e=l(),Vo=a("div"),m(aM.$$.fragment),t1o=l(),Pi=a("p"),a1o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),fZ=a("code"),s1o=o("from_pretrained()"),n1o=o(` class method or the
`),hZ=a("code"),l1o=o("from_config()"),i1o=o(" class method."),d1o=l(),sM=a("p"),c1o=o("This class cannot be instantiated directly using "),gZ=a("code"),m1o=o("__init__()"),f1o=o(" (throws an error)."),h1o=l(),Nr=a("div"),m(nM.$$.fragment),g1o=l(),uZ=a("p"),u1o=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),p1o=l(),Si=a("p"),_1o=o(`Note:
Loading a model from its configuration file does `),pZ=a("strong"),b1o=o("not"),v1o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_Z=a("code"),T1o=o("from_pretrained()"),F1o=o(` to load the model
weights.`),E1o=l(),bZ=a("p"),C1o=o("Examples:"),M1o=l(),m(lM.$$.fragment),y1o=l(),Ge=a("div"),m(iM.$$.fragment),w1o=l(),vZ=a("p"),A1o=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),L1o=l(),Pa=a("p"),B1o=o("The model class to instantiate is selected based on the "),TZ=a("code"),x1o=o("model_type"),k1o=o(` property of the config object (either
passed as an argument or loaded from `),FZ=a("code"),R1o=o("pretrained_model_name_or_path"),P1o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),EZ=a("code"),S1o=o("pretrained_model_name_or_path"),$1o=o(":"),I1o=l(),jt=a("ul"),U_=a("li"),CZ=a("strong"),D1o=o("bert"),N1o=o(" \u2014 "),rR=a("a"),j1o=o("BertForNextSentencePrediction"),O1o=o(" (BERT model)"),G1o=l(),J_=a("li"),MZ=a("strong"),q1o=o("fnet"),z1o=o(" \u2014 "),tR=a("a"),X1o=o("FNetForNextSentencePrediction"),Q1o=o(" (FNet model)"),V1o=l(),K_=a("li"),yZ=a("strong"),W1o=o("megatron-bert"),H1o=o(" \u2014 "),aR=a("a"),U1o=o("MegatronBertForNextSentencePrediction"),J1o=o(" (MegatronBert model)"),K1o=l(),Y_=a("li"),wZ=a("strong"),Y1o=o("mobilebert"),Z1o=o(" \u2014 "),sR=a("a"),eFo=o("MobileBertForNextSentencePrediction"),oFo=o(" (MobileBERT model)"),rFo=l(),Z_=a("li"),AZ=a("strong"),tFo=o("qdqbert"),aFo=o(" \u2014 "),nR=a("a"),sFo=o("QDQBertForNextSentencePrediction"),nFo=o(" (QDQBert model)"),lFo=l(),eb=a("p"),iFo=o("The model is set in evaluation mode by default using "),LZ=a("code"),dFo=o("model.eval()"),cFo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),BZ=a("code"),mFo=o("model.train()"),fFo=l(),xZ=a("p"),hFo=o("Examples:"),gFo=l(),m(dM.$$.fragment),P5e=l(),$i=a("h2"),ob=a("a"),kZ=a("span"),m(cM.$$.fragment),uFo=l(),RZ=a("span"),pFo=o("AutoModelForTokenClassification"),S5e=l(),Wo=a("div"),m(mM.$$.fragment),_Fo=l(),Ii=a("p"),bFo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),PZ=a("code"),vFo=o("from_pretrained()"),TFo=o(` class method or the
`),SZ=a("code"),FFo=o("from_config()"),EFo=o(" class method."),CFo=l(),fM=a("p"),MFo=o("This class cannot be instantiated directly using "),$Z=a("code"),yFo=o("__init__()"),wFo=o(" (throws an error)."),AFo=l(),jr=a("div"),m(hM.$$.fragment),LFo=l(),IZ=a("p"),BFo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),xFo=l(),Di=a("p"),kFo=o(`Note:
Loading a model from its configuration file does `),DZ=a("strong"),RFo=o("not"),PFo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),NZ=a("code"),SFo=o("from_pretrained()"),$Fo=o(` to load the model
weights.`),IFo=l(),jZ=a("p"),DFo=o("Examples:"),NFo=l(),m(gM.$$.fragment),jFo=l(),qe=a("div"),m(uM.$$.fragment),OFo=l(),OZ=a("p"),GFo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),qFo=l(),Sa=a("p"),zFo=o("The model class to instantiate is selected based on the "),GZ=a("code"),XFo=o("model_type"),QFo=o(` property of the config object (either
passed as an argument or loaded from `),qZ=a("code"),VFo=o("pretrained_model_name_or_path"),WFo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),zZ=a("code"),HFo=o("pretrained_model_name_or_path"),UFo=o(":"),JFo=l(),X=a("ul"),rb=a("li"),XZ=a("strong"),KFo=o("albert"),YFo=o(" \u2014 "),lR=a("a"),ZFo=o("AlbertForTokenClassification"),eEo=o(" (ALBERT model)"),oEo=l(),tb=a("li"),QZ=a("strong"),rEo=o("bert"),tEo=o(" \u2014 "),iR=a("a"),aEo=o("BertForTokenClassification"),sEo=o(" (BERT model)"),nEo=l(),ab=a("li"),VZ=a("strong"),lEo=o("big_bird"),iEo=o(" \u2014 "),dR=a("a"),dEo=o("BigBirdForTokenClassification"),cEo=o(" (BigBird model)"),mEo=l(),sb=a("li"),WZ=a("strong"),fEo=o("camembert"),hEo=o(" \u2014 "),cR=a("a"),gEo=o("CamembertForTokenClassification"),uEo=o(" (CamemBERT model)"),pEo=l(),nb=a("li"),HZ=a("strong"),_Eo=o("canine"),bEo=o(" \u2014 "),mR=a("a"),vEo=o("CanineForTokenClassification"),TEo=o(" (Canine model)"),FEo=l(),lb=a("li"),UZ=a("strong"),EEo=o("convbert"),CEo=o(" \u2014 "),fR=a("a"),MEo=o("ConvBertForTokenClassification"),yEo=o(" (ConvBERT model)"),wEo=l(),ib=a("li"),JZ=a("strong"),AEo=o("deberta"),LEo=o(" \u2014 "),hR=a("a"),BEo=o("DebertaForTokenClassification"),xEo=o(" (DeBERTa model)"),kEo=l(),db=a("li"),KZ=a("strong"),REo=o("deberta-v2"),PEo=o(" \u2014 "),gR=a("a"),SEo=o("DebertaV2ForTokenClassification"),$Eo=o(" (DeBERTa-v2 model)"),IEo=l(),cb=a("li"),YZ=a("strong"),DEo=o("distilbert"),NEo=o(" \u2014 "),uR=a("a"),jEo=o("DistilBertForTokenClassification"),OEo=o(" (DistilBERT model)"),GEo=l(),mb=a("li"),ZZ=a("strong"),qEo=o("electra"),zEo=o(" \u2014 "),pR=a("a"),XEo=o("ElectraForTokenClassification"),QEo=o(" (ELECTRA model)"),VEo=l(),fb=a("li"),eee=a("strong"),WEo=o("flaubert"),HEo=o(" \u2014 "),_R=a("a"),UEo=o("FlaubertForTokenClassification"),JEo=o(" (FlauBERT model)"),KEo=l(),hb=a("li"),oee=a("strong"),YEo=o("fnet"),ZEo=o(" \u2014 "),bR=a("a"),e4o=o("FNetForTokenClassification"),o4o=o(" (FNet model)"),r4o=l(),gb=a("li"),ree=a("strong"),t4o=o("funnel"),a4o=o(" \u2014 "),vR=a("a"),s4o=o("FunnelForTokenClassification"),n4o=o(" (Funnel Transformer model)"),l4o=l(),ub=a("li"),tee=a("strong"),i4o=o("gpt2"),d4o=o(" \u2014 "),TR=a("a"),c4o=o("GPT2ForTokenClassification"),m4o=o(" (OpenAI GPT-2 model)"),f4o=l(),pb=a("li"),aee=a("strong"),h4o=o("ibert"),g4o=o(" \u2014 "),FR=a("a"),u4o=o("IBertForTokenClassification"),p4o=o(" (I-BERT model)"),_4o=l(),_b=a("li"),see=a("strong"),b4o=o("layoutlm"),v4o=o(" \u2014 "),ER=a("a"),T4o=o("LayoutLMForTokenClassification"),F4o=o(" (LayoutLM model)"),E4o=l(),bb=a("li"),nee=a("strong"),C4o=o("layoutlmv2"),M4o=o(" \u2014 "),CR=a("a"),y4o=o("LayoutLMv2ForTokenClassification"),w4o=o(" (LayoutLMv2 model)"),A4o=l(),vb=a("li"),lee=a("strong"),L4o=o("longformer"),B4o=o(" \u2014 "),MR=a("a"),x4o=o("LongformerForTokenClassification"),k4o=o(" (Longformer model)"),R4o=l(),Tb=a("li"),iee=a("strong"),P4o=o("megatron-bert"),S4o=o(" \u2014 "),yR=a("a"),$4o=o("MegatronBertForTokenClassification"),I4o=o(" (MegatronBert model)"),D4o=l(),Fb=a("li"),dee=a("strong"),N4o=o("mobilebert"),j4o=o(" \u2014 "),wR=a("a"),O4o=o("MobileBertForTokenClassification"),G4o=o(" (MobileBERT model)"),q4o=l(),Eb=a("li"),cee=a("strong"),z4o=o("mpnet"),X4o=o(" \u2014 "),AR=a("a"),Q4o=o("MPNetForTokenClassification"),V4o=o(" (MPNet model)"),W4o=l(),Cb=a("li"),mee=a("strong"),H4o=o("qdqbert"),U4o=o(" \u2014 "),LR=a("a"),J4o=o("QDQBertForTokenClassification"),K4o=o(" (QDQBert model)"),Y4o=l(),Mb=a("li"),fee=a("strong"),Z4o=o("rembert"),eCo=o(" \u2014 "),BR=a("a"),oCo=o("RemBertForTokenClassification"),rCo=o(" (RemBERT model)"),tCo=l(),yb=a("li"),hee=a("strong"),aCo=o("roberta"),sCo=o(" \u2014 "),xR=a("a"),nCo=o("RobertaForTokenClassification"),lCo=o(" (RoBERTa model)"),iCo=l(),wb=a("li"),gee=a("strong"),dCo=o("roformer"),cCo=o(" \u2014 "),kR=a("a"),mCo=o("RoFormerForTokenClassification"),fCo=o(" (RoFormer model)"),hCo=l(),Ab=a("li"),uee=a("strong"),gCo=o("squeezebert"),uCo=o(" \u2014 "),RR=a("a"),pCo=o("SqueezeBertForTokenClassification"),_Co=o(" (SqueezeBERT model)"),bCo=l(),Lb=a("li"),pee=a("strong"),vCo=o("xlm"),TCo=o(" \u2014 "),PR=a("a"),FCo=o("XLMForTokenClassification"),ECo=o(" (XLM model)"),CCo=l(),Bb=a("li"),_ee=a("strong"),MCo=o("xlm-roberta"),yCo=o(" \u2014 "),SR=a("a"),wCo=o("XLMRobertaForTokenClassification"),ACo=o(" (XLM-RoBERTa model)"),LCo=l(),xb=a("li"),bee=a("strong"),BCo=o("xlnet"),xCo=o(" \u2014 "),$R=a("a"),kCo=o("XLNetForTokenClassification"),RCo=o(" (XLNet model)"),PCo=l(),kb=a("p"),SCo=o("The model is set in evaluation mode by default using "),vee=a("code"),$Co=o("model.eval()"),ICo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tee=a("code"),DCo=o("model.train()"),NCo=l(),Fee=a("p"),jCo=o("Examples:"),OCo=l(),m(pM.$$.fragment),$5e=l(),Ni=a("h2"),Rb=a("a"),Eee=a("span"),m(_M.$$.fragment),GCo=l(),Cee=a("span"),qCo=o("AutoModelForQuestionAnswering"),I5e=l(),Ho=a("div"),m(bM.$$.fragment),zCo=l(),ji=a("p"),XCo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Mee=a("code"),QCo=o("from_pretrained()"),VCo=o(` class method or the
`),yee=a("code"),WCo=o("from_config()"),HCo=o(" class method."),UCo=l(),vM=a("p"),JCo=o("This class cannot be instantiated directly using "),wee=a("code"),KCo=o("__init__()"),YCo=o(" (throws an error)."),ZCo=l(),Or=a("div"),m(TM.$$.fragment),e3o=l(),Aee=a("p"),o3o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),r3o=l(),Oi=a("p"),t3o=o(`Note:
Loading a model from its configuration file does `),Lee=a("strong"),a3o=o("not"),s3o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bee=a("code"),n3o=o("from_pretrained()"),l3o=o(` to load the model
weights.`),i3o=l(),xee=a("p"),d3o=o("Examples:"),c3o=l(),m(FM.$$.fragment),m3o=l(),ze=a("div"),m(EM.$$.fragment),f3o=l(),kee=a("p"),h3o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),g3o=l(),$a=a("p"),u3o=o("The model class to instantiate is selected based on the "),Ree=a("code"),p3o=o("model_type"),_3o=o(` property of the config object (either
passed as an argument or loaded from `),Pee=a("code"),b3o=o("pretrained_model_name_or_path"),v3o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),See=a("code"),T3o=o("pretrained_model_name_or_path"),F3o=o(":"),E3o=l(),S=a("ul"),Pb=a("li"),$ee=a("strong"),C3o=o("albert"),M3o=o(" \u2014 "),IR=a("a"),y3o=o("AlbertForQuestionAnswering"),w3o=o(" (ALBERT model)"),A3o=l(),Sb=a("li"),Iee=a("strong"),L3o=o("bart"),B3o=o(" \u2014 "),DR=a("a"),x3o=o("BartForQuestionAnswering"),k3o=o(" (BART model)"),R3o=l(),$b=a("li"),Dee=a("strong"),P3o=o("bert"),S3o=o(" \u2014 "),NR=a("a"),$3o=o("BertForQuestionAnswering"),I3o=o(" (BERT model)"),D3o=l(),Ib=a("li"),Nee=a("strong"),N3o=o("big_bird"),j3o=o(" \u2014 "),jR=a("a"),O3o=o("BigBirdForQuestionAnswering"),G3o=o(" (BigBird model)"),q3o=l(),Db=a("li"),jee=a("strong"),z3o=o("bigbird_pegasus"),X3o=o(" \u2014 "),OR=a("a"),Q3o=o("BigBirdPegasusForQuestionAnswering"),V3o=o(" (BigBirdPegasus model)"),W3o=l(),Nb=a("li"),Oee=a("strong"),H3o=o("camembert"),U3o=o(" \u2014 "),GR=a("a"),J3o=o("CamembertForQuestionAnswering"),K3o=o(" (CamemBERT model)"),Y3o=l(),jb=a("li"),Gee=a("strong"),Z3o=o("canine"),eMo=o(" \u2014 "),qR=a("a"),oMo=o("CanineForQuestionAnswering"),rMo=o(" (Canine model)"),tMo=l(),Ob=a("li"),qee=a("strong"),aMo=o("convbert"),sMo=o(" \u2014 "),zR=a("a"),nMo=o("ConvBertForQuestionAnswering"),lMo=o(" (ConvBERT model)"),iMo=l(),Gb=a("li"),zee=a("strong"),dMo=o("deberta"),cMo=o(" \u2014 "),XR=a("a"),mMo=o("DebertaForQuestionAnswering"),fMo=o(" (DeBERTa model)"),hMo=l(),qb=a("li"),Xee=a("strong"),gMo=o("deberta-v2"),uMo=o(" \u2014 "),QR=a("a"),pMo=o("DebertaV2ForQuestionAnswering"),_Mo=o(" (DeBERTa-v2 model)"),bMo=l(),zb=a("li"),Qee=a("strong"),vMo=o("distilbert"),TMo=o(" \u2014 "),VR=a("a"),FMo=o("DistilBertForQuestionAnswering"),EMo=o(" (DistilBERT model)"),CMo=l(),Xb=a("li"),Vee=a("strong"),MMo=o("electra"),yMo=o(" \u2014 "),WR=a("a"),wMo=o("ElectraForQuestionAnswering"),AMo=o(" (ELECTRA model)"),LMo=l(),Qb=a("li"),Wee=a("strong"),BMo=o("flaubert"),xMo=o(" \u2014 "),HR=a("a"),kMo=o("FlaubertForQuestionAnsweringSimple"),RMo=o(" (FlauBERT model)"),PMo=l(),Vb=a("li"),Hee=a("strong"),SMo=o("fnet"),$Mo=o(" \u2014 "),UR=a("a"),IMo=o("FNetForQuestionAnswering"),DMo=o(" (FNet model)"),NMo=l(),Wb=a("li"),Uee=a("strong"),jMo=o("funnel"),OMo=o(" \u2014 "),JR=a("a"),GMo=o("FunnelForQuestionAnswering"),qMo=o(" (Funnel Transformer model)"),zMo=l(),Hb=a("li"),Jee=a("strong"),XMo=o("gptj"),QMo=o(" \u2014 "),KR=a("a"),VMo=o("GPTJForQuestionAnswering"),WMo=o(" (GPT-J model)"),HMo=l(),Ub=a("li"),Kee=a("strong"),UMo=o("ibert"),JMo=o(" \u2014 "),YR=a("a"),KMo=o("IBertForQuestionAnswering"),YMo=o(" (I-BERT model)"),ZMo=l(),Jb=a("li"),Yee=a("strong"),e5o=o("layoutlmv2"),o5o=o(" \u2014 "),ZR=a("a"),r5o=o("LayoutLMv2ForQuestionAnswering"),t5o=o(" (LayoutLMv2 model)"),a5o=l(),Kb=a("li"),Zee=a("strong"),s5o=o("led"),n5o=o(" \u2014 "),eP=a("a"),l5o=o("LEDForQuestionAnswering"),i5o=o(" (LED model)"),d5o=l(),Yb=a("li"),eoe=a("strong"),c5o=o("longformer"),m5o=o(" \u2014 "),oP=a("a"),f5o=o("LongformerForQuestionAnswering"),h5o=o(" (Longformer model)"),g5o=l(),Zb=a("li"),ooe=a("strong"),u5o=o("lxmert"),p5o=o(" \u2014 "),rP=a("a"),_5o=o("LxmertForQuestionAnswering"),b5o=o(" (LXMERT model)"),v5o=l(),e2=a("li"),roe=a("strong"),T5o=o("mbart"),F5o=o(" \u2014 "),tP=a("a"),E5o=o("MBartForQuestionAnswering"),C5o=o(" (mBART model)"),M5o=l(),o2=a("li"),toe=a("strong"),y5o=o("megatron-bert"),w5o=o(" \u2014 "),aP=a("a"),A5o=o("MegatronBertForQuestionAnswering"),L5o=o(" (MegatronBert model)"),B5o=l(),r2=a("li"),aoe=a("strong"),x5o=o("mobilebert"),k5o=o(" \u2014 "),sP=a("a"),R5o=o("MobileBertForQuestionAnswering"),P5o=o(" (MobileBERT model)"),S5o=l(),t2=a("li"),soe=a("strong"),$5o=o("mpnet"),I5o=o(" \u2014 "),nP=a("a"),D5o=o("MPNetForQuestionAnswering"),N5o=o(" (MPNet model)"),j5o=l(),a2=a("li"),noe=a("strong"),O5o=o("qdqbert"),G5o=o(" \u2014 "),lP=a("a"),q5o=o("QDQBertForQuestionAnswering"),z5o=o(" (QDQBert model)"),X5o=l(),s2=a("li"),loe=a("strong"),Q5o=o("reformer"),V5o=o(" \u2014 "),iP=a("a"),W5o=o("ReformerForQuestionAnswering"),H5o=o(" (Reformer model)"),U5o=l(),n2=a("li"),ioe=a("strong"),J5o=o("rembert"),K5o=o(" \u2014 "),dP=a("a"),Y5o=o("RemBertForQuestionAnswering"),Z5o=o(" (RemBERT model)"),eyo=l(),l2=a("li"),doe=a("strong"),oyo=o("roberta"),ryo=o(" \u2014 "),cP=a("a"),tyo=o("RobertaForQuestionAnswering"),ayo=o(" (RoBERTa model)"),syo=l(),i2=a("li"),coe=a("strong"),nyo=o("roformer"),lyo=o(" \u2014 "),mP=a("a"),iyo=o("RoFormerForQuestionAnswering"),dyo=o(" (RoFormer model)"),cyo=l(),d2=a("li"),moe=a("strong"),myo=o("splinter"),fyo=o(" \u2014 "),fP=a("a"),hyo=o("SplinterForQuestionAnswering"),gyo=o(" (Splinter model)"),uyo=l(),c2=a("li"),foe=a("strong"),pyo=o("squeezebert"),_yo=o(" \u2014 "),hP=a("a"),byo=o("SqueezeBertForQuestionAnswering"),vyo=o(" (SqueezeBERT model)"),Tyo=l(),m2=a("li"),hoe=a("strong"),Fyo=o("xlm"),Eyo=o(" \u2014 "),gP=a("a"),Cyo=o("XLMForQuestionAnsweringSimple"),Myo=o(" (XLM model)"),yyo=l(),f2=a("li"),goe=a("strong"),wyo=o("xlm-roberta"),Ayo=o(" \u2014 "),uP=a("a"),Lyo=o("XLMRobertaForQuestionAnswering"),Byo=o(" (XLM-RoBERTa model)"),xyo=l(),h2=a("li"),uoe=a("strong"),kyo=o("xlnet"),Ryo=o(" \u2014 "),pP=a("a"),Pyo=o("XLNetForQuestionAnsweringSimple"),Syo=o(" (XLNet model)"),$yo=l(),g2=a("p"),Iyo=o("The model is set in evaluation mode by default using "),poe=a("code"),Dyo=o("model.eval()"),Nyo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),_oe=a("code"),jyo=o("model.train()"),Oyo=l(),boe=a("p"),Gyo=o("Examples:"),qyo=l(),m(CM.$$.fragment),D5e=l(),Gi=a("h2"),u2=a("a"),voe=a("span"),m(MM.$$.fragment),zyo=l(),Toe=a("span"),Xyo=o("AutoModelForTableQuestionAnswering"),N5e=l(),Uo=a("div"),m(yM.$$.fragment),Qyo=l(),qi=a("p"),Vyo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Foe=a("code"),Wyo=o("from_pretrained()"),Hyo=o(` class method or the
`),Eoe=a("code"),Uyo=o("from_config()"),Jyo=o(" class method."),Kyo=l(),wM=a("p"),Yyo=o("This class cannot be instantiated directly using "),Coe=a("code"),Zyo=o("__init__()"),ewo=o(" (throws an error)."),owo=l(),Gr=a("div"),m(AM.$$.fragment),rwo=l(),Moe=a("p"),two=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),awo=l(),zi=a("p"),swo=o(`Note:
Loading a model from its configuration file does `),yoe=a("strong"),nwo=o("not"),lwo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),woe=a("code"),iwo=o("from_pretrained()"),dwo=o(` to load the model
weights.`),cwo=l(),Aoe=a("p"),mwo=o("Examples:"),fwo=l(),m(LM.$$.fragment),hwo=l(),Xe=a("div"),m(BM.$$.fragment),gwo=l(),Loe=a("p"),uwo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),pwo=l(),Ia=a("p"),_wo=o("The model class to instantiate is selected based on the "),Boe=a("code"),bwo=o("model_type"),vwo=o(` property of the config object (either
passed as an argument or loaded from `),xoe=a("code"),Two=o("pretrained_model_name_or_path"),Fwo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),koe=a("code"),Ewo=o("pretrained_model_name_or_path"),Cwo=o(":"),Mwo=l(),Roe=a("ul"),p2=a("li"),Poe=a("strong"),ywo=o("tapas"),wwo=o(" \u2014 "),_P=a("a"),Awo=o("TapasForQuestionAnswering"),Lwo=o(" (TAPAS model)"),Bwo=l(),_2=a("p"),xwo=o("The model is set in evaluation mode by default using "),Soe=a("code"),kwo=o("model.eval()"),Rwo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$oe=a("code"),Pwo=o("model.train()"),Swo=l(),Ioe=a("p"),$wo=o("Examples:"),Iwo=l(),m(xM.$$.fragment),j5e=l(),Xi=a("h2"),b2=a("a"),Doe=a("span"),m(kM.$$.fragment),Dwo=l(),Noe=a("span"),Nwo=o("AutoModelForImageClassification"),O5e=l(),Jo=a("div"),m(RM.$$.fragment),jwo=l(),Qi=a("p"),Owo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),joe=a("code"),Gwo=o("from_pretrained()"),qwo=o(` class method or the
`),Ooe=a("code"),zwo=o("from_config()"),Xwo=o(" class method."),Qwo=l(),PM=a("p"),Vwo=o("This class cannot be instantiated directly using "),Goe=a("code"),Wwo=o("__init__()"),Hwo=o(" (throws an error)."),Uwo=l(),qr=a("div"),m(SM.$$.fragment),Jwo=l(),qoe=a("p"),Kwo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Ywo=l(),Vi=a("p"),Zwo=o(`Note:
Loading a model from its configuration file does `),zoe=a("strong"),e7o=o("not"),o7o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xoe=a("code"),r7o=o("from_pretrained()"),t7o=o(` to load the model
weights.`),a7o=l(),Qoe=a("p"),s7o=o("Examples:"),n7o=l(),m($M.$$.fragment),l7o=l(),Qe=a("div"),m(IM.$$.fragment),i7o=l(),Voe=a("p"),d7o=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),c7o=l(),Da=a("p"),m7o=o("The model class to instantiate is selected based on the "),Woe=a("code"),f7o=o("model_type"),h7o=o(` property of the config object (either
passed as an argument or loaded from `),Hoe=a("code"),g7o=o("pretrained_model_name_or_path"),u7o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Uoe=a("code"),p7o=o("pretrained_model_name_or_path"),_7o=o(":"),b7o=l(),Ko=a("ul"),v2=a("li"),Joe=a("strong"),v7o=o("beit"),T7o=o(" \u2014 "),bP=a("a"),F7o=o("BeitForImageClassification"),E7o=o(" (BEiT model)"),C7o=l(),dn=a("li"),Koe=a("strong"),M7o=o("deit"),y7o=o(" \u2014 "),vP=a("a"),w7o=o("DeiTForImageClassification"),A7o=o(" or "),TP=a("a"),L7o=o("DeiTForImageClassificationWithTeacher"),B7o=o(" (DeiT model)"),x7o=l(),T2=a("li"),Yoe=a("strong"),k7o=o("imagegpt"),R7o=o(" \u2014 "),FP=a("a"),P7o=o("ImageGPTForImageClassification"),S7o=o(" (ImageGPT model)"),$7o=l(),Gt=a("li"),Zoe=a("strong"),I7o=o("perceiver"),D7o=o(" \u2014 "),EP=a("a"),N7o=o("PerceiverForImageClassificationLearned"),j7o=o(" or "),CP=a("a"),O7o=o("PerceiverForImageClassificationFourier"),G7o=o(" or "),MP=a("a"),q7o=o("PerceiverForImageClassificationConvProcessing"),z7o=o(" (Perceiver model)"),X7o=l(),F2=a("li"),ere=a("strong"),Q7o=o("segformer"),V7o=o(" \u2014 "),yP=a("a"),W7o=o("SegformerForImageClassification"),H7o=o(" (SegFormer model)"),U7o=l(),E2=a("li"),ore=a("strong"),J7o=o("vit"),K7o=o(" \u2014 "),wP=a("a"),Y7o=o("ViTForImageClassification"),Z7o=o(" (ViT model)"),e0o=l(),C2=a("p"),o0o=o("The model is set in evaluation mode by default using "),rre=a("code"),r0o=o("model.eval()"),t0o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tre=a("code"),a0o=o("model.train()"),s0o=l(),are=a("p"),n0o=o("Examples:"),l0o=l(),m(DM.$$.fragment),G5e=l(),Wi=a("h2"),M2=a("a"),sre=a("span"),m(NM.$$.fragment),i0o=l(),nre=a("span"),d0o=o("AutoModelForVision2Seq"),q5e=l(),Yo=a("div"),m(jM.$$.fragment),c0o=l(),Hi=a("p"),m0o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),lre=a("code"),f0o=o("from_pretrained()"),h0o=o(` class method or the
`),ire=a("code"),g0o=o("from_config()"),u0o=o(" class method."),p0o=l(),OM=a("p"),_0o=o("This class cannot be instantiated directly using "),dre=a("code"),b0o=o("__init__()"),v0o=o(" (throws an error)."),T0o=l(),zr=a("div"),m(GM.$$.fragment),F0o=l(),cre=a("p"),E0o=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),C0o=l(),Ui=a("p"),M0o=o(`Note:
Loading a model from its configuration file does `),mre=a("strong"),y0o=o("not"),w0o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),fre=a("code"),A0o=o("from_pretrained()"),L0o=o(` to load the model
weights.`),B0o=l(),hre=a("p"),x0o=o("Examples:"),k0o=l(),m(qM.$$.fragment),R0o=l(),Ve=a("div"),m(zM.$$.fragment),P0o=l(),gre=a("p"),S0o=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),$0o=l(),Na=a("p"),I0o=o("The model class to instantiate is selected based on the "),ure=a("code"),D0o=o("model_type"),N0o=o(` property of the config object (either
passed as an argument or loaded from `),pre=a("code"),j0o=o("pretrained_model_name_or_path"),O0o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),_re=a("code"),G0o=o("pretrained_model_name_or_path"),q0o=o(":"),z0o=l(),bre=a("ul"),y2=a("li"),vre=a("strong"),X0o=o("vision-encoder-decoder"),Q0o=o(" \u2014 "),AP=a("a"),V0o=o("VisionEncoderDecoderModel"),W0o=o(" (Vision Encoder decoder model)"),H0o=l(),w2=a("p"),U0o=o("The model is set in evaluation mode by default using "),Tre=a("code"),J0o=o("model.eval()"),K0o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fre=a("code"),Y0o=o("model.train()"),Z0o=l(),Ere=a("p"),eAo=o("Examples:"),oAo=l(),m(XM.$$.fragment),z5e=l(),Ji=a("h2"),A2=a("a"),Cre=a("span"),m(QM.$$.fragment),rAo=l(),Mre=a("span"),tAo=o("AutoModelForAudioClassification"),X5e=l(),Zo=a("div"),m(VM.$$.fragment),aAo=l(),Ki=a("p"),sAo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),yre=a("code"),nAo=o("from_pretrained()"),lAo=o(` class method or the
`),wre=a("code"),iAo=o("from_config()"),dAo=o(" class method."),cAo=l(),WM=a("p"),mAo=o("This class cannot be instantiated directly using "),Are=a("code"),fAo=o("__init__()"),hAo=o(" (throws an error)."),gAo=l(),Xr=a("div"),m(HM.$$.fragment),uAo=l(),Lre=a("p"),pAo=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),_Ao=l(),Yi=a("p"),bAo=o(`Note:
Loading a model from its configuration file does `),Bre=a("strong"),vAo=o("not"),TAo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xre=a("code"),FAo=o("from_pretrained()"),EAo=o(` to load the model
weights.`),CAo=l(),kre=a("p"),MAo=o("Examples:"),yAo=l(),m(UM.$$.fragment),wAo=l(),We=a("div"),m(JM.$$.fragment),AAo=l(),Rre=a("p"),LAo=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),BAo=l(),ja=a("p"),xAo=o("The model class to instantiate is selected based on the "),Pre=a("code"),kAo=o("model_type"),RAo=o(` property of the config object (either
passed as an argument or loaded from `),Sre=a("code"),PAo=o("pretrained_model_name_or_path"),SAo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),$re=a("code"),$Ao=o("pretrained_model_name_or_path"),IAo=o(":"),DAo=l(),er=a("ul"),L2=a("li"),Ire=a("strong"),NAo=o("hubert"),jAo=o(" \u2014 "),LP=a("a"),OAo=o("HubertForSequenceClassification"),GAo=o(" (Hubert model)"),qAo=l(),B2=a("li"),Dre=a("strong"),zAo=o("sew"),XAo=o(" \u2014 "),BP=a("a"),QAo=o("SEWForSequenceClassification"),VAo=o(" (SEW model)"),WAo=l(),x2=a("li"),Nre=a("strong"),HAo=o("sew-d"),UAo=o(" \u2014 "),xP=a("a"),JAo=o("SEWDForSequenceClassification"),KAo=o(" (SEW-D model)"),YAo=l(),k2=a("li"),jre=a("strong"),ZAo=o("unispeech"),e6o=o(" \u2014 "),kP=a("a"),o6o=o("UniSpeechForSequenceClassification"),r6o=o(" (UniSpeech model)"),t6o=l(),R2=a("li"),Ore=a("strong"),a6o=o("unispeech-sat"),s6o=o(" \u2014 "),RP=a("a"),n6o=o("UniSpeechSatForSequenceClassification"),l6o=o(" (UniSpeechSat model)"),i6o=l(),P2=a("li"),Gre=a("strong"),d6o=o("wav2vec2"),c6o=o(" \u2014 "),PP=a("a"),m6o=o("Wav2Vec2ForSequenceClassification"),f6o=o(" (Wav2Vec2 model)"),h6o=l(),S2=a("p"),g6o=o("The model is set in evaluation mode by default using "),qre=a("code"),u6o=o("model.eval()"),p6o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zre=a("code"),_6o=o("model.train()"),b6o=l(),Xre=a("p"),v6o=o("Examples:"),T6o=l(),m(KM.$$.fragment),Q5e=l(),Zi=a("h2"),$2=a("a"),Qre=a("span"),m(YM.$$.fragment),F6o=l(),Vre=a("span"),E6o=o("AutoModelForCTC"),V5e=l(),or=a("div"),m(ZM.$$.fragment),C6o=l(),ed=a("p"),M6o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Wre=a("code"),y6o=o("from_pretrained()"),w6o=o(` class method or the
`),Hre=a("code"),A6o=o("from_config()"),L6o=o(" class method."),B6o=l(),e5=a("p"),x6o=o("This class cannot be instantiated directly using "),Ure=a("code"),k6o=o("__init__()"),R6o=o(" (throws an error)."),P6o=l(),Qr=a("div"),m(o5.$$.fragment),S6o=l(),Jre=a("p"),$6o=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),I6o=l(),od=a("p"),D6o=o(`Note:
Loading a model from its configuration file does `),Kre=a("strong"),N6o=o("not"),j6o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yre=a("code"),O6o=o("from_pretrained()"),G6o=o(` to load the model
weights.`),q6o=l(),Zre=a("p"),z6o=o("Examples:"),X6o=l(),m(r5.$$.fragment),Q6o=l(),He=a("div"),m(t5.$$.fragment),V6o=l(),ete=a("p"),W6o=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),H6o=l(),Oa=a("p"),U6o=o("The model class to instantiate is selected based on the "),ote=a("code"),J6o=o("model_type"),K6o=o(` property of the config object (either
passed as an argument or loaded from `),rte=a("code"),Y6o=o("pretrained_model_name_or_path"),Z6o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),tte=a("code"),eLo=o("pretrained_model_name_or_path"),oLo=o(":"),rLo=l(),rr=a("ul"),I2=a("li"),ate=a("strong"),tLo=o("hubert"),aLo=o(" \u2014 "),SP=a("a"),sLo=o("HubertForCTC"),nLo=o(" (Hubert model)"),lLo=l(),D2=a("li"),ste=a("strong"),iLo=o("sew"),dLo=o(" \u2014 "),$P=a("a"),cLo=o("SEWForCTC"),mLo=o(" (SEW model)"),fLo=l(),N2=a("li"),nte=a("strong"),hLo=o("sew-d"),gLo=o(" \u2014 "),IP=a("a"),uLo=o("SEWDForCTC"),pLo=o(" (SEW-D model)"),_Lo=l(),j2=a("li"),lte=a("strong"),bLo=o("unispeech"),vLo=o(" \u2014 "),DP=a("a"),TLo=o("UniSpeechForCTC"),FLo=o(" (UniSpeech model)"),ELo=l(),O2=a("li"),ite=a("strong"),CLo=o("unispeech-sat"),MLo=o(" \u2014 "),NP=a("a"),yLo=o("UniSpeechSatForCTC"),wLo=o(" (UniSpeechSat model)"),ALo=l(),G2=a("li"),dte=a("strong"),LLo=o("wav2vec2"),BLo=o(" \u2014 "),jP=a("a"),xLo=o("Wav2Vec2ForCTC"),kLo=o(" (Wav2Vec2 model)"),RLo=l(),q2=a("p"),PLo=o("The model is set in evaluation mode by default using "),cte=a("code"),SLo=o("model.eval()"),$Lo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mte=a("code"),ILo=o("model.train()"),DLo=l(),fte=a("p"),NLo=o("Examples:"),jLo=l(),m(a5.$$.fragment),W5e=l(),rd=a("h2"),z2=a("a"),hte=a("span"),m(s5.$$.fragment),OLo=l(),gte=a("span"),GLo=o("AutoModelForSpeechSeq2Seq"),H5e=l(),tr=a("div"),m(n5.$$.fragment),qLo=l(),td=a("p"),zLo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) when created
with the `),ute=a("code"),XLo=o("from_pretrained()"),QLo=o(` class method or the
`),pte=a("code"),VLo=o("from_config()"),WLo=o(" class method."),HLo=l(),l5=a("p"),ULo=o("This class cannot be instantiated directly using "),_te=a("code"),JLo=o("__init__()"),KLo=o(" (throws an error)."),YLo=l(),Vr=a("div"),m(i5.$$.fragment),ZLo=l(),bte=a("p"),e8o=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) from a configuration."),o8o=l(),ad=a("p"),r8o=o(`Note:
Loading a model from its configuration file does `),vte=a("strong"),t8o=o("not"),a8o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Tte=a("code"),s8o=o("from_pretrained()"),n8o=o(` to load the model
weights.`),l8o=l(),Fte=a("p"),i8o=o("Examples:"),d8o=l(),m(d5.$$.fragment),c8o=l(),Ue=a("div"),m(c5.$$.fragment),m8o=l(),Ete=a("p"),f8o=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) from a pretrained model."),h8o=l(),Ga=a("p"),g8o=o("The model class to instantiate is selected based on the "),Cte=a("code"),u8o=o("model_type"),p8o=o(` property of the config object (either
passed as an argument or loaded from `),Mte=a("code"),_8o=o("pretrained_model_name_or_path"),b8o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),yte=a("code"),v8o=o("pretrained_model_name_or_path"),T8o=o(":"),F8o=l(),m5=a("ul"),X2=a("li"),wte=a("strong"),E8o=o("speech-encoder-decoder"),C8o=o(" \u2014 "),OP=a("a"),M8o=o("SpeechEncoderDecoderModel"),y8o=o(" (Speech Encoder decoder model)"),w8o=l(),Q2=a("li"),Ate=a("strong"),A8o=o("speech_to_text"),L8o=o(" \u2014 "),GP=a("a"),B8o=o("Speech2TextForConditionalGeneration"),x8o=o(" (Speech2Text model)"),k8o=l(),V2=a("p"),R8o=o("The model is set in evaluation mode by default using "),Lte=a("code"),P8o=o("model.eval()"),S8o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Bte=a("code"),$8o=o("model.train()"),I8o=l(),xte=a("p"),D8o=o("Examples:"),N8o=l(),m(f5.$$.fragment),U5e=l(),sd=a("h2"),W2=a("a"),kte=a("span"),m(h5.$$.fragment),j8o=l(),Rte=a("span"),O8o=o("AutoModelForObjectDetection"),J5e=l(),ar=a("div"),m(g5.$$.fragment),G8o=l(),nd=a("p"),q8o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Pte=a("code"),z8o=o("from_pretrained()"),X8o=o(` class method or the
`),Ste=a("code"),Q8o=o("from_config()"),V8o=o(" class method."),W8o=l(),u5=a("p"),H8o=o("This class cannot be instantiated directly using "),$te=a("code"),U8o=o("__init__()"),J8o=o(" (throws an error)."),K8o=l(),Wr=a("div"),m(p5.$$.fragment),Y8o=l(),Ite=a("p"),Z8o=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),eBo=l(),ld=a("p"),oBo=o(`Note:
Loading a model from its configuration file does `),Dte=a("strong"),rBo=o("not"),tBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Nte=a("code"),aBo=o("from_pretrained()"),sBo=o(` to load the model
weights.`),nBo=l(),jte=a("p"),lBo=o("Examples:"),iBo=l(),m(_5.$$.fragment),dBo=l(),Je=a("div"),m(b5.$$.fragment),cBo=l(),Ote=a("p"),mBo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),fBo=l(),qa=a("p"),hBo=o("The model class to instantiate is selected based on the "),Gte=a("code"),gBo=o("model_type"),uBo=o(` property of the config object (either
passed as an argument or loaded from `),qte=a("code"),pBo=o("pretrained_model_name_or_path"),_Bo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),zte=a("code"),bBo=o("pretrained_model_name_or_path"),vBo=o(":"),TBo=l(),Xte=a("ul"),H2=a("li"),Qte=a("strong"),FBo=o("detr"),EBo=o(" \u2014 "),qP=a("a"),CBo=o("DetrForObjectDetection"),MBo=o(" (DETR model)"),yBo=l(),U2=a("p"),wBo=o("The model is set in evaluation mode by default using "),Vte=a("code"),ABo=o("model.eval()"),LBo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Wte=a("code"),BBo=o("model.train()"),xBo=l(),Hte=a("p"),kBo=o("Examples:"),RBo=l(),m(v5.$$.fragment),K5e=l(),id=a("h2"),J2=a("a"),Ute=a("span"),m(T5.$$.fragment),PBo=l(),Jte=a("span"),SBo=o("AutoModelForImageSegmentation"),Y5e=l(),sr=a("div"),m(F5.$$.fragment),$Bo=l(),dd=a("p"),IBo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),Kte=a("code"),DBo=o("from_pretrained()"),NBo=o(` class method or the
`),Yte=a("code"),jBo=o("from_config()"),OBo=o(" class method."),GBo=l(),E5=a("p"),qBo=o("This class cannot be instantiated directly using "),Zte=a("code"),zBo=o("__init__()"),XBo=o(" (throws an error)."),QBo=l(),Hr=a("div"),m(C5.$$.fragment),VBo=l(),eae=a("p"),WBo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),HBo=l(),cd=a("p"),UBo=o(`Note:
Loading a model from its configuration file does `),oae=a("strong"),JBo=o("not"),KBo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),rae=a("code"),YBo=o("from_pretrained()"),ZBo=o(` to load the model
weights.`),e9o=l(),tae=a("p"),o9o=o("Examples:"),r9o=l(),m(M5.$$.fragment),t9o=l(),Ke=a("div"),m(y5.$$.fragment),a9o=l(),aae=a("p"),s9o=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),n9o=l(),za=a("p"),l9o=o("The model class to instantiate is selected based on the "),sae=a("code"),i9o=o("model_type"),d9o=o(` property of the config object (either
passed as an argument or loaded from `),nae=a("code"),c9o=o("pretrained_model_name_or_path"),m9o=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),lae=a("code"),f9o=o("pretrained_model_name_or_path"),h9o=o(":"),g9o=l(),iae=a("ul"),K2=a("li"),dae=a("strong"),u9o=o("detr"),p9o=o(" \u2014 "),zP=a("a"),_9o=o("DetrForSegmentation"),b9o=o(" (DETR model)"),v9o=l(),Y2=a("p"),T9o=o("The model is set in evaluation mode by default using "),cae=a("code"),F9o=o("model.eval()"),E9o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mae=a("code"),C9o=o("model.train()"),M9o=l(),fae=a("p"),y9o=o("Examples:"),w9o=l(),m(w5.$$.fragment),Z5e=l(),md=a("h2"),Z2=a("a"),hae=a("span"),m(A5.$$.fragment),A9o=l(),gae=a("span"),L9o=o("TFAutoModel"),eye=l(),nr=a("div"),m(L5.$$.fragment),B9o=l(),fd=a("p"),x9o=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),uae=a("code"),k9o=o("from_pretrained()"),R9o=o(` class method or the
`),pae=a("code"),P9o=o("from_config()"),S9o=o(" class method."),$9o=l(),B5=a("p"),I9o=o("This class cannot be instantiated directly using "),_ae=a("code"),D9o=o("__init__()"),N9o=o(" (throws an error)."),j9o=l(),Ur=a("div"),m(x5.$$.fragment),O9o=l(),bae=a("p"),G9o=o("Instantiates one of the base model classes of the library from a configuration."),q9o=l(),hd=a("p"),z9o=o(`Note:
Loading a model from its configuration file does `),vae=a("strong"),X9o=o("not"),Q9o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Tae=a("code"),V9o=o("from_pretrained()"),W9o=o(` to load the model
weights.`),H9o=l(),Fae=a("p"),U9o=o("Examples:"),J9o=l(),m(k5.$$.fragment),K9o=l(),lo=a("div"),m(R5.$$.fragment),Y9o=l(),Eae=a("p"),Z9o=o("Instantiate one of the base model classes of the library from a pretrained model."),exo=l(),Xa=a("p"),oxo=o("The model class to instantiate is selected based on the "),Cae=a("code"),rxo=o("model_type"),txo=o(` property of the config object (either
passed as an argument or loaded from `),Mae=a("code"),axo=o("pretrained_model_name_or_path"),sxo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),yae=a("code"),nxo=o("pretrained_model_name_or_path"),lxo=o(":"),ixo=l(),B=a("ul"),ev=a("li"),wae=a("strong"),dxo=o("albert"),cxo=o(" \u2014 "),XP=a("a"),mxo=o("TFAlbertModel"),fxo=o(" (ALBERT model)"),hxo=l(),ov=a("li"),Aae=a("strong"),gxo=o("bart"),uxo=o(" \u2014 "),QP=a("a"),pxo=o("TFBartModel"),_xo=o(" (BART model)"),bxo=l(),rv=a("li"),Lae=a("strong"),vxo=o("bert"),Txo=o(" \u2014 "),VP=a("a"),Fxo=o("TFBertModel"),Exo=o(" (BERT model)"),Cxo=l(),tv=a("li"),Bae=a("strong"),Mxo=o("blenderbot"),yxo=o(" \u2014 "),WP=a("a"),wxo=o("TFBlenderbotModel"),Axo=o(" (Blenderbot model)"),Lxo=l(),av=a("li"),xae=a("strong"),Bxo=o("blenderbot-small"),xxo=o(" \u2014 "),HP=a("a"),kxo=o("TFBlenderbotSmallModel"),Rxo=o(" (BlenderbotSmall model)"),Pxo=l(),sv=a("li"),kae=a("strong"),Sxo=o("camembert"),$xo=o(" \u2014 "),UP=a("a"),Ixo=o("TFCamembertModel"),Dxo=o(" (CamemBERT model)"),Nxo=l(),nv=a("li"),Rae=a("strong"),jxo=o("convbert"),Oxo=o(" \u2014 "),JP=a("a"),Gxo=o("TFConvBertModel"),qxo=o(" (ConvBERT model)"),zxo=l(),lv=a("li"),Pae=a("strong"),Xxo=o("ctrl"),Qxo=o(" \u2014 "),KP=a("a"),Vxo=o("TFCTRLModel"),Wxo=o(" (CTRL model)"),Hxo=l(),iv=a("li"),Sae=a("strong"),Uxo=o("deberta"),Jxo=o(" \u2014 "),YP=a("a"),Kxo=o("TFDebertaModel"),Yxo=o(" (DeBERTa model)"),Zxo=l(),dv=a("li"),$ae=a("strong"),eko=o("deberta-v2"),oko=o(" \u2014 "),ZP=a("a"),rko=o("TFDebertaV2Model"),tko=o(" (DeBERTa-v2 model)"),ako=l(),cv=a("li"),Iae=a("strong"),sko=o("distilbert"),nko=o(" \u2014 "),eS=a("a"),lko=o("TFDistilBertModel"),iko=o(" (DistilBERT model)"),dko=l(),mv=a("li"),Dae=a("strong"),cko=o("dpr"),mko=o(" \u2014 "),oS=a("a"),fko=o("TFDPRQuestionEncoder"),hko=o(" (DPR model)"),gko=l(),fv=a("li"),Nae=a("strong"),uko=o("electra"),pko=o(" \u2014 "),rS=a("a"),_ko=o("TFElectraModel"),bko=o(" (ELECTRA model)"),vko=l(),hv=a("li"),jae=a("strong"),Tko=o("flaubert"),Fko=o(" \u2014 "),tS=a("a"),Eko=o("TFFlaubertModel"),Cko=o(" (FlauBERT model)"),Mko=l(),cn=a("li"),Oae=a("strong"),yko=o("funnel"),wko=o(" \u2014 "),aS=a("a"),Ako=o("TFFunnelModel"),Lko=o(" or "),sS=a("a"),Bko=o("TFFunnelBaseModel"),xko=o(" (Funnel Transformer model)"),kko=l(),gv=a("li"),Gae=a("strong"),Rko=o("gpt2"),Pko=o(" \u2014 "),nS=a("a"),Sko=o("TFGPT2Model"),$ko=o(" (OpenAI GPT-2 model)"),Iko=l(),uv=a("li"),qae=a("strong"),Dko=o("hubert"),Nko=o(" \u2014 "),lS=a("a"),jko=o("TFHubertModel"),Oko=o(" (Hubert model)"),Gko=l(),pv=a("li"),zae=a("strong"),qko=o("layoutlm"),zko=o(" \u2014 "),iS=a("a"),Xko=o("TFLayoutLMModel"),Qko=o(" (LayoutLM model)"),Vko=l(),_v=a("li"),Xae=a("strong"),Wko=o("led"),Hko=o(" \u2014 "),dS=a("a"),Uko=o("TFLEDModel"),Jko=o(" (LED model)"),Kko=l(),bv=a("li"),Qae=a("strong"),Yko=o("longformer"),Zko=o(" \u2014 "),cS=a("a"),eRo=o("TFLongformerModel"),oRo=o(" (Longformer model)"),rRo=l(),vv=a("li"),Vae=a("strong"),tRo=o("lxmert"),aRo=o(" \u2014 "),mS=a("a"),sRo=o("TFLxmertModel"),nRo=o(" (LXMERT model)"),lRo=l(),Tv=a("li"),Wae=a("strong"),iRo=o("marian"),dRo=o(" \u2014 "),fS=a("a"),cRo=o("TFMarianModel"),mRo=o(" (Marian model)"),fRo=l(),Fv=a("li"),Hae=a("strong"),hRo=o("mbart"),gRo=o(" \u2014 "),hS=a("a"),uRo=o("TFMBartModel"),pRo=o(" (mBART model)"),_Ro=l(),Ev=a("li"),Uae=a("strong"),bRo=o("mobilebert"),vRo=o(" \u2014 "),gS=a("a"),TRo=o("TFMobileBertModel"),FRo=o(" (MobileBERT model)"),ERo=l(),Cv=a("li"),Jae=a("strong"),CRo=o("mpnet"),MRo=o(" \u2014 "),uS=a("a"),yRo=o("TFMPNetModel"),wRo=o(" (MPNet model)"),ARo=l(),Mv=a("li"),Kae=a("strong"),LRo=o("mt5"),BRo=o(" \u2014 "),pS=a("a"),xRo=o("TFMT5Model"),kRo=o(" (mT5 model)"),RRo=l(),yv=a("li"),Yae=a("strong"),PRo=o("openai-gpt"),SRo=o(" \u2014 "),_S=a("a"),$Ro=o("TFOpenAIGPTModel"),IRo=o(" (OpenAI GPT model)"),DRo=l(),wv=a("li"),Zae=a("strong"),NRo=o("pegasus"),jRo=o(" \u2014 "),bS=a("a"),ORo=o("TFPegasusModel"),GRo=o(" (Pegasus model)"),qRo=l(),Av=a("li"),ese=a("strong"),zRo=o("rembert"),XRo=o(" \u2014 "),vS=a("a"),QRo=o("TFRemBertModel"),VRo=o(" (RemBERT model)"),WRo=l(),Lv=a("li"),ose=a("strong"),HRo=o("roberta"),URo=o(" \u2014 "),TS=a("a"),JRo=o("TFRobertaModel"),KRo=o(" (RoBERTa model)"),YRo=l(),Bv=a("li"),rse=a("strong"),ZRo=o("roformer"),ePo=o(" \u2014 "),FS=a("a"),oPo=o("TFRoFormerModel"),rPo=o(" (RoFormer model)"),tPo=l(),xv=a("li"),tse=a("strong"),aPo=o("t5"),sPo=o(" \u2014 "),ES=a("a"),nPo=o("TFT5Model"),lPo=o(" (T5 model)"),iPo=l(),kv=a("li"),ase=a("strong"),dPo=o("tapas"),cPo=o(" \u2014 "),CS=a("a"),mPo=o("TFTapasModel"),fPo=o(" (TAPAS model)"),hPo=l(),Rv=a("li"),sse=a("strong"),gPo=o("transfo-xl"),uPo=o(" \u2014 "),MS=a("a"),pPo=o("TFTransfoXLModel"),_Po=o(" (Transformer-XL model)"),bPo=l(),Pv=a("li"),nse=a("strong"),vPo=o("vit"),TPo=o(" \u2014 "),yS=a("a"),FPo=o("TFViTModel"),EPo=o(" (ViT model)"),CPo=l(),Sv=a("li"),lse=a("strong"),MPo=o("wav2vec2"),yPo=o(" \u2014 "),wS=a("a"),wPo=o("TFWav2Vec2Model"),APo=o(" (Wav2Vec2 model)"),LPo=l(),$v=a("li"),ise=a("strong"),BPo=o("xlm"),xPo=o(" \u2014 "),AS=a("a"),kPo=o("TFXLMModel"),RPo=o(" (XLM model)"),PPo=l(),Iv=a("li"),dse=a("strong"),SPo=o("xlm-roberta"),$Po=o(" \u2014 "),LS=a("a"),IPo=o("TFXLMRobertaModel"),DPo=o(" (XLM-RoBERTa model)"),NPo=l(),Dv=a("li"),cse=a("strong"),jPo=o("xlnet"),OPo=o(" \u2014 "),BS=a("a"),GPo=o("TFXLNetModel"),qPo=o(" (XLNet model)"),zPo=l(),mse=a("p"),XPo=o("Examples:"),QPo=l(),m(P5.$$.fragment),oye=l(),gd=a("h2"),Nv=a("a"),fse=a("span"),m(S5.$$.fragment),VPo=l(),hse=a("span"),WPo=o("TFAutoModelForPreTraining"),rye=l(),lr=a("div"),m($5.$$.fragment),HPo=l(),ud=a("p"),UPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),gse=a("code"),JPo=o("from_pretrained()"),KPo=o(` class method or the
`),use=a("code"),YPo=o("from_config()"),ZPo=o(" class method."),eSo=l(),I5=a("p"),oSo=o("This class cannot be instantiated directly using "),pse=a("code"),rSo=o("__init__()"),tSo=o(" (throws an error)."),aSo=l(),Jr=a("div"),m(D5.$$.fragment),sSo=l(),_se=a("p"),nSo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),lSo=l(),pd=a("p"),iSo=o(`Note:
Loading a model from its configuration file does `),bse=a("strong"),dSo=o("not"),cSo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),vse=a("code"),mSo=o("from_pretrained()"),fSo=o(` to load the model
weights.`),hSo=l(),Tse=a("p"),gSo=o("Examples:"),uSo=l(),m(N5.$$.fragment),pSo=l(),io=a("div"),m(j5.$$.fragment),_So=l(),Fse=a("p"),bSo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),vSo=l(),Qa=a("p"),TSo=o("The model class to instantiate is selected based on the "),Ese=a("code"),FSo=o("model_type"),ESo=o(` property of the config object (either
passed as an argument or loaded from `),Cse=a("code"),CSo=o("pretrained_model_name_or_path"),MSo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Mse=a("code"),ySo=o("pretrained_model_name_or_path"),wSo=o(":"),ASo=l(),K=a("ul"),jv=a("li"),yse=a("strong"),LSo=o("albert"),BSo=o(" \u2014 "),xS=a("a"),xSo=o("TFAlbertForPreTraining"),kSo=o(" (ALBERT model)"),RSo=l(),Ov=a("li"),wse=a("strong"),PSo=o("bart"),SSo=o(" \u2014 "),kS=a("a"),$So=o("TFBartForConditionalGeneration"),ISo=o(" (BART model)"),DSo=l(),Gv=a("li"),Ase=a("strong"),NSo=o("bert"),jSo=o(" \u2014 "),RS=a("a"),OSo=o("TFBertForPreTraining"),GSo=o(" (BERT model)"),qSo=l(),qv=a("li"),Lse=a("strong"),zSo=o("camembert"),XSo=o(" \u2014 "),PS=a("a"),QSo=o("TFCamembertForMaskedLM"),VSo=o(" (CamemBERT model)"),WSo=l(),zv=a("li"),Bse=a("strong"),HSo=o("ctrl"),USo=o(" \u2014 "),SS=a("a"),JSo=o("TFCTRLLMHeadModel"),KSo=o(" (CTRL model)"),YSo=l(),Xv=a("li"),xse=a("strong"),ZSo=o("distilbert"),e$o=o(" \u2014 "),$S=a("a"),o$o=o("TFDistilBertForMaskedLM"),r$o=o(" (DistilBERT model)"),t$o=l(),Qv=a("li"),kse=a("strong"),a$o=o("electra"),s$o=o(" \u2014 "),IS=a("a"),n$o=o("TFElectraForPreTraining"),l$o=o(" (ELECTRA model)"),i$o=l(),Vv=a("li"),Rse=a("strong"),d$o=o("flaubert"),c$o=o(" \u2014 "),DS=a("a"),m$o=o("TFFlaubertWithLMHeadModel"),f$o=o(" (FlauBERT model)"),h$o=l(),Wv=a("li"),Pse=a("strong"),g$o=o("funnel"),u$o=o(" \u2014 "),NS=a("a"),p$o=o("TFFunnelForPreTraining"),_$o=o(" (Funnel Transformer model)"),b$o=l(),Hv=a("li"),Sse=a("strong"),v$o=o("gpt2"),T$o=o(" \u2014 "),jS=a("a"),F$o=o("TFGPT2LMHeadModel"),E$o=o(" (OpenAI GPT-2 model)"),C$o=l(),Uv=a("li"),$se=a("strong"),M$o=o("layoutlm"),y$o=o(" \u2014 "),OS=a("a"),w$o=o("TFLayoutLMForMaskedLM"),A$o=o(" (LayoutLM model)"),L$o=l(),Jv=a("li"),Ise=a("strong"),B$o=o("lxmert"),x$o=o(" \u2014 "),GS=a("a"),k$o=o("TFLxmertForPreTraining"),R$o=o(" (LXMERT model)"),P$o=l(),Kv=a("li"),Dse=a("strong"),S$o=o("mobilebert"),$$o=o(" \u2014 "),qS=a("a"),I$o=o("TFMobileBertForPreTraining"),D$o=o(" (MobileBERT model)"),N$o=l(),Yv=a("li"),Nse=a("strong"),j$o=o("mpnet"),O$o=o(" \u2014 "),zS=a("a"),G$o=o("TFMPNetForMaskedLM"),q$o=o(" (MPNet model)"),z$o=l(),Zv=a("li"),jse=a("strong"),X$o=o("openai-gpt"),Q$o=o(" \u2014 "),XS=a("a"),V$o=o("TFOpenAIGPTLMHeadModel"),W$o=o(" (OpenAI GPT model)"),H$o=l(),eT=a("li"),Ose=a("strong"),U$o=o("roberta"),J$o=o(" \u2014 "),QS=a("a"),K$o=o("TFRobertaForMaskedLM"),Y$o=o(" (RoBERTa model)"),Z$o=l(),oT=a("li"),Gse=a("strong"),eIo=o("t5"),oIo=o(" \u2014 "),VS=a("a"),rIo=o("TFT5ForConditionalGeneration"),tIo=o(" (T5 model)"),aIo=l(),rT=a("li"),qse=a("strong"),sIo=o("tapas"),nIo=o(" \u2014 "),WS=a("a"),lIo=o("TFTapasForMaskedLM"),iIo=o(" (TAPAS model)"),dIo=l(),tT=a("li"),zse=a("strong"),cIo=o("transfo-xl"),mIo=o(" \u2014 "),HS=a("a"),fIo=o("TFTransfoXLLMHeadModel"),hIo=o(" (Transformer-XL model)"),gIo=l(),aT=a("li"),Xse=a("strong"),uIo=o("xlm"),pIo=o(" \u2014 "),US=a("a"),_Io=o("TFXLMWithLMHeadModel"),bIo=o(" (XLM model)"),vIo=l(),sT=a("li"),Qse=a("strong"),TIo=o("xlm-roberta"),FIo=o(" \u2014 "),JS=a("a"),EIo=o("TFXLMRobertaForMaskedLM"),CIo=o(" (XLM-RoBERTa model)"),MIo=l(),nT=a("li"),Vse=a("strong"),yIo=o("xlnet"),wIo=o(" \u2014 "),KS=a("a"),AIo=o("TFXLNetLMHeadModel"),LIo=o(" (XLNet model)"),BIo=l(),Wse=a("p"),xIo=o("Examples:"),kIo=l(),m(O5.$$.fragment),tye=l(),_d=a("h2"),lT=a("a"),Hse=a("span"),m(G5.$$.fragment),RIo=l(),Use=a("span"),PIo=o("TFAutoModelForCausalLM"),aye=l(),ir=a("div"),m(q5.$$.fragment),SIo=l(),bd=a("p"),$Io=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Jse=a("code"),IIo=o("from_pretrained()"),DIo=o(` class method or the
`),Kse=a("code"),NIo=o("from_config()"),jIo=o(" class method."),OIo=l(),z5=a("p"),GIo=o("This class cannot be instantiated directly using "),Yse=a("code"),qIo=o("__init__()"),zIo=o(" (throws an error)."),XIo=l(),Kr=a("div"),m(X5.$$.fragment),QIo=l(),Zse=a("p"),VIo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),WIo=l(),vd=a("p"),HIo=o(`Note:
Loading a model from its configuration file does `),ene=a("strong"),UIo=o("not"),JIo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),one=a("code"),KIo=o("from_pretrained()"),YIo=o(` to load the model
weights.`),ZIo=l(),rne=a("p"),eDo=o("Examples:"),oDo=l(),m(Q5.$$.fragment),rDo=l(),co=a("div"),m(V5.$$.fragment),tDo=l(),tne=a("p"),aDo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),sDo=l(),Va=a("p"),nDo=o("The model class to instantiate is selected based on the "),ane=a("code"),lDo=o("model_type"),iDo=o(` property of the config object (either
passed as an argument or loaded from `),sne=a("code"),dDo=o("pretrained_model_name_or_path"),cDo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),nne=a("code"),mDo=o("pretrained_model_name_or_path"),fDo=o(":"),hDo=l(),_e=a("ul"),iT=a("li"),lne=a("strong"),gDo=o("bert"),uDo=o(" \u2014 "),YS=a("a"),pDo=o("TFBertLMHeadModel"),_Do=o(" (BERT model)"),bDo=l(),dT=a("li"),ine=a("strong"),vDo=o("ctrl"),TDo=o(" \u2014 "),ZS=a("a"),FDo=o("TFCTRLLMHeadModel"),EDo=o(" (CTRL model)"),CDo=l(),cT=a("li"),dne=a("strong"),MDo=o("gpt2"),yDo=o(" \u2014 "),e$=a("a"),wDo=o("TFGPT2LMHeadModel"),ADo=o(" (OpenAI GPT-2 model)"),LDo=l(),mT=a("li"),cne=a("strong"),BDo=o("openai-gpt"),xDo=o(" \u2014 "),o$=a("a"),kDo=o("TFOpenAIGPTLMHeadModel"),RDo=o(" (OpenAI GPT model)"),PDo=l(),fT=a("li"),mne=a("strong"),SDo=o("rembert"),$Do=o(" \u2014 "),r$=a("a"),IDo=o("TFRemBertForCausalLM"),DDo=o(" (RemBERT model)"),NDo=l(),hT=a("li"),fne=a("strong"),jDo=o("roberta"),ODo=o(" \u2014 "),t$=a("a"),GDo=o("TFRobertaForCausalLM"),qDo=o(" (RoBERTa model)"),zDo=l(),gT=a("li"),hne=a("strong"),XDo=o("roformer"),QDo=o(" \u2014 "),a$=a("a"),VDo=o("TFRoFormerForCausalLM"),WDo=o(" (RoFormer model)"),HDo=l(),uT=a("li"),gne=a("strong"),UDo=o("transfo-xl"),JDo=o(" \u2014 "),s$=a("a"),KDo=o("TFTransfoXLLMHeadModel"),YDo=o(" (Transformer-XL model)"),ZDo=l(),pT=a("li"),une=a("strong"),eNo=o("xlm"),oNo=o(" \u2014 "),n$=a("a"),rNo=o("TFXLMWithLMHeadModel"),tNo=o(" (XLM model)"),aNo=l(),_T=a("li"),pne=a("strong"),sNo=o("xlnet"),nNo=o(" \u2014 "),l$=a("a"),lNo=o("TFXLNetLMHeadModel"),iNo=o(" (XLNet model)"),dNo=l(),_ne=a("p"),cNo=o("Examples:"),mNo=l(),m(W5.$$.fragment),sye=l(),Td=a("h2"),bT=a("a"),bne=a("span"),m(H5.$$.fragment),fNo=l(),vne=a("span"),hNo=o("TFAutoModelForImageClassification"),nye=l(),dr=a("div"),m(U5.$$.fragment),gNo=l(),Fd=a("p"),uNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Tne=a("code"),pNo=o("from_pretrained()"),_No=o(` class method or the
`),Fne=a("code"),bNo=o("from_config()"),vNo=o(" class method."),TNo=l(),J5=a("p"),FNo=o("This class cannot be instantiated directly using "),Ene=a("code"),ENo=o("__init__()"),CNo=o(" (throws an error)."),MNo=l(),Yr=a("div"),m(K5.$$.fragment),yNo=l(),Cne=a("p"),wNo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),ANo=l(),Ed=a("p"),LNo=o(`Note:
Loading a model from its configuration file does `),Mne=a("strong"),BNo=o("not"),xNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),yne=a("code"),kNo=o("from_pretrained()"),RNo=o(` to load the model
weights.`),PNo=l(),wne=a("p"),SNo=o("Examples:"),$No=l(),m(Y5.$$.fragment),INo=l(),mo=a("div"),m(Z5.$$.fragment),DNo=l(),Ane=a("p"),NNo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),jNo=l(),Wa=a("p"),ONo=o("The model class to instantiate is selected based on the "),Lne=a("code"),GNo=o("model_type"),qNo=o(` property of the config object (either
passed as an argument or loaded from `),Bne=a("code"),zNo=o("pretrained_model_name_or_path"),XNo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),xne=a("code"),QNo=o("pretrained_model_name_or_path"),VNo=o(":"),WNo=l(),kne=a("ul"),vT=a("li"),Rne=a("strong"),HNo=o("vit"),UNo=o(" \u2014 "),i$=a("a"),JNo=o("TFViTForImageClassification"),KNo=o(" (ViT model)"),YNo=l(),Pne=a("p"),ZNo=o("Examples:"),ejo=l(),m(ey.$$.fragment),lye=l(),Cd=a("h2"),TT=a("a"),Sne=a("span"),m(oy.$$.fragment),ojo=l(),$ne=a("span"),rjo=o("TFAutoModelForMaskedLM"),iye=l(),cr=a("div"),m(ry.$$.fragment),tjo=l(),Md=a("p"),ajo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Ine=a("code"),sjo=o("from_pretrained()"),njo=o(` class method or the
`),Dne=a("code"),ljo=o("from_config()"),ijo=o(" class method."),djo=l(),ty=a("p"),cjo=o("This class cannot be instantiated directly using "),Nne=a("code"),mjo=o("__init__()"),fjo=o(" (throws an error)."),hjo=l(),Zr=a("div"),m(ay.$$.fragment),gjo=l(),jne=a("p"),ujo=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),pjo=l(),yd=a("p"),_jo=o(`Note:
Loading a model from its configuration file does `),One=a("strong"),bjo=o("not"),vjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Gne=a("code"),Tjo=o("from_pretrained()"),Fjo=o(` to load the model
weights.`),Ejo=l(),qne=a("p"),Cjo=o("Examples:"),Mjo=l(),m(sy.$$.fragment),yjo=l(),fo=a("div"),m(ny.$$.fragment),wjo=l(),zne=a("p"),Ajo=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Ljo=l(),Ha=a("p"),Bjo=o("The model class to instantiate is selected based on the "),Xne=a("code"),xjo=o("model_type"),kjo=o(` property of the config object (either
passed as an argument or loaded from `),Qne=a("code"),Rjo=o("pretrained_model_name_or_path"),Pjo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Vne=a("code"),Sjo=o("pretrained_model_name_or_path"),$jo=o(":"),Ijo=l(),te=a("ul"),FT=a("li"),Wne=a("strong"),Djo=o("albert"),Njo=o(" \u2014 "),d$=a("a"),jjo=o("TFAlbertForMaskedLM"),Ojo=o(" (ALBERT model)"),Gjo=l(),ET=a("li"),Hne=a("strong"),qjo=o("bert"),zjo=o(" \u2014 "),c$=a("a"),Xjo=o("TFBertForMaskedLM"),Qjo=o(" (BERT model)"),Vjo=l(),CT=a("li"),Une=a("strong"),Wjo=o("camembert"),Hjo=o(" \u2014 "),m$=a("a"),Ujo=o("TFCamembertForMaskedLM"),Jjo=o(" (CamemBERT model)"),Kjo=l(),MT=a("li"),Jne=a("strong"),Yjo=o("convbert"),Zjo=o(" \u2014 "),f$=a("a"),eOo=o("TFConvBertForMaskedLM"),oOo=o(" (ConvBERT model)"),rOo=l(),yT=a("li"),Kne=a("strong"),tOo=o("deberta"),aOo=o(" \u2014 "),h$=a("a"),sOo=o("TFDebertaForMaskedLM"),nOo=o(" (DeBERTa model)"),lOo=l(),wT=a("li"),Yne=a("strong"),iOo=o("deberta-v2"),dOo=o(" \u2014 "),g$=a("a"),cOo=o("TFDebertaV2ForMaskedLM"),mOo=o(" (DeBERTa-v2 model)"),fOo=l(),AT=a("li"),Zne=a("strong"),hOo=o("distilbert"),gOo=o(" \u2014 "),u$=a("a"),uOo=o("TFDistilBertForMaskedLM"),pOo=o(" (DistilBERT model)"),_Oo=l(),LT=a("li"),ele=a("strong"),bOo=o("electra"),vOo=o(" \u2014 "),p$=a("a"),TOo=o("TFElectraForMaskedLM"),FOo=o(" (ELECTRA model)"),EOo=l(),BT=a("li"),ole=a("strong"),COo=o("flaubert"),MOo=o(" \u2014 "),_$=a("a"),yOo=o("TFFlaubertWithLMHeadModel"),wOo=o(" (FlauBERT model)"),AOo=l(),xT=a("li"),rle=a("strong"),LOo=o("funnel"),BOo=o(" \u2014 "),b$=a("a"),xOo=o("TFFunnelForMaskedLM"),kOo=o(" (Funnel Transformer model)"),ROo=l(),kT=a("li"),tle=a("strong"),POo=o("layoutlm"),SOo=o(" \u2014 "),v$=a("a"),$Oo=o("TFLayoutLMForMaskedLM"),IOo=o(" (LayoutLM model)"),DOo=l(),RT=a("li"),ale=a("strong"),NOo=o("longformer"),jOo=o(" \u2014 "),T$=a("a"),OOo=o("TFLongformerForMaskedLM"),GOo=o(" (Longformer model)"),qOo=l(),PT=a("li"),sle=a("strong"),zOo=o("mobilebert"),XOo=o(" \u2014 "),F$=a("a"),QOo=o("TFMobileBertForMaskedLM"),VOo=o(" (MobileBERT model)"),WOo=l(),ST=a("li"),nle=a("strong"),HOo=o("mpnet"),UOo=o(" \u2014 "),E$=a("a"),JOo=o("TFMPNetForMaskedLM"),KOo=o(" (MPNet model)"),YOo=l(),$T=a("li"),lle=a("strong"),ZOo=o("rembert"),eGo=o(" \u2014 "),C$=a("a"),oGo=o("TFRemBertForMaskedLM"),rGo=o(" (RemBERT model)"),tGo=l(),IT=a("li"),ile=a("strong"),aGo=o("roberta"),sGo=o(" \u2014 "),M$=a("a"),nGo=o("TFRobertaForMaskedLM"),lGo=o(" (RoBERTa model)"),iGo=l(),DT=a("li"),dle=a("strong"),dGo=o("roformer"),cGo=o(" \u2014 "),y$=a("a"),mGo=o("TFRoFormerForMaskedLM"),fGo=o(" (RoFormer model)"),hGo=l(),NT=a("li"),cle=a("strong"),gGo=o("tapas"),uGo=o(" \u2014 "),w$=a("a"),pGo=o("TFTapasForMaskedLM"),_Go=o(" (TAPAS model)"),bGo=l(),jT=a("li"),mle=a("strong"),vGo=o("xlm"),TGo=o(" \u2014 "),A$=a("a"),FGo=o("TFXLMWithLMHeadModel"),EGo=o(" (XLM model)"),CGo=l(),OT=a("li"),fle=a("strong"),MGo=o("xlm-roberta"),yGo=o(" \u2014 "),L$=a("a"),wGo=o("TFXLMRobertaForMaskedLM"),AGo=o(" (XLM-RoBERTa model)"),LGo=l(),hle=a("p"),BGo=o("Examples:"),xGo=l(),m(ly.$$.fragment),dye=l(),wd=a("h2"),GT=a("a"),gle=a("span"),m(iy.$$.fragment),kGo=l(),ule=a("span"),RGo=o("TFAutoModelForSeq2SeqLM"),cye=l(),mr=a("div"),m(dy.$$.fragment),PGo=l(),Ad=a("p"),SGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),ple=a("code"),$Go=o("from_pretrained()"),IGo=o(` class method or the
`),_le=a("code"),DGo=o("from_config()"),NGo=o(" class method."),jGo=l(),cy=a("p"),OGo=o("This class cannot be instantiated directly using "),ble=a("code"),GGo=o("__init__()"),qGo=o(" (throws an error)."),zGo=l(),et=a("div"),m(my.$$.fragment),XGo=l(),vle=a("p"),QGo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),VGo=l(),Ld=a("p"),WGo=o(`Note:
Loading a model from its configuration file does `),Tle=a("strong"),HGo=o("not"),UGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Fle=a("code"),JGo=o("from_pretrained()"),KGo=o(` to load the model
weights.`),YGo=l(),Ele=a("p"),ZGo=o("Examples:"),eqo=l(),m(fy.$$.fragment),oqo=l(),ho=a("div"),m(hy.$$.fragment),rqo=l(),Cle=a("p"),tqo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),aqo=l(),Ua=a("p"),sqo=o("The model class to instantiate is selected based on the "),Mle=a("code"),nqo=o("model_type"),lqo=o(` property of the config object (either
passed as an argument or loaded from `),yle=a("code"),iqo=o("pretrained_model_name_or_path"),dqo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),wle=a("code"),cqo=o("pretrained_model_name_or_path"),mqo=o(":"),fqo=l(),be=a("ul"),qT=a("li"),Ale=a("strong"),hqo=o("bart"),gqo=o(" \u2014 "),B$=a("a"),uqo=o("TFBartForConditionalGeneration"),pqo=o(" (BART model)"),_qo=l(),zT=a("li"),Lle=a("strong"),bqo=o("blenderbot"),vqo=o(" \u2014 "),x$=a("a"),Tqo=o("TFBlenderbotForConditionalGeneration"),Fqo=o(" (Blenderbot model)"),Eqo=l(),XT=a("li"),Ble=a("strong"),Cqo=o("blenderbot-small"),Mqo=o(" \u2014 "),k$=a("a"),yqo=o("TFBlenderbotSmallForConditionalGeneration"),wqo=o(" (BlenderbotSmall model)"),Aqo=l(),QT=a("li"),xle=a("strong"),Lqo=o("encoder-decoder"),Bqo=o(" \u2014 "),R$=a("a"),xqo=o("TFEncoderDecoderModel"),kqo=o(" (Encoder decoder model)"),Rqo=l(),VT=a("li"),kle=a("strong"),Pqo=o("led"),Sqo=o(" \u2014 "),P$=a("a"),$qo=o("TFLEDForConditionalGeneration"),Iqo=o(" (LED model)"),Dqo=l(),WT=a("li"),Rle=a("strong"),Nqo=o("marian"),jqo=o(" \u2014 "),S$=a("a"),Oqo=o("TFMarianMTModel"),Gqo=o(" (Marian model)"),qqo=l(),HT=a("li"),Ple=a("strong"),zqo=o("mbart"),Xqo=o(" \u2014 "),$$=a("a"),Qqo=o("TFMBartForConditionalGeneration"),Vqo=o(" (mBART model)"),Wqo=l(),UT=a("li"),Sle=a("strong"),Hqo=o("mt5"),Uqo=o(" \u2014 "),I$=a("a"),Jqo=o("TFMT5ForConditionalGeneration"),Kqo=o(" (mT5 model)"),Yqo=l(),JT=a("li"),$le=a("strong"),Zqo=o("pegasus"),ezo=o(" \u2014 "),D$=a("a"),ozo=o("TFPegasusForConditionalGeneration"),rzo=o(" (Pegasus model)"),tzo=l(),KT=a("li"),Ile=a("strong"),azo=o("t5"),szo=o(" \u2014 "),N$=a("a"),nzo=o("TFT5ForConditionalGeneration"),lzo=o(" (T5 model)"),izo=l(),Dle=a("p"),dzo=o("Examples:"),czo=l(),m(gy.$$.fragment),mye=l(),Bd=a("h2"),YT=a("a"),Nle=a("span"),m(uy.$$.fragment),mzo=l(),jle=a("span"),fzo=o("TFAutoModelForSequenceClassification"),fye=l(),fr=a("div"),m(py.$$.fragment),hzo=l(),xd=a("p"),gzo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Ole=a("code"),uzo=o("from_pretrained()"),pzo=o(` class method or the
`),Gle=a("code"),_zo=o("from_config()"),bzo=o(" class method."),vzo=l(),_y=a("p"),Tzo=o("This class cannot be instantiated directly using "),qle=a("code"),Fzo=o("__init__()"),Ezo=o(" (throws an error)."),Czo=l(),ot=a("div"),m(by.$$.fragment),Mzo=l(),zle=a("p"),yzo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),wzo=l(),kd=a("p"),Azo=o(`Note:
Loading a model from its configuration file does `),Xle=a("strong"),Lzo=o("not"),Bzo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Qle=a("code"),xzo=o("from_pretrained()"),kzo=o(` to load the model
weights.`),Rzo=l(),Vle=a("p"),Pzo=o("Examples:"),Szo=l(),m(vy.$$.fragment),$zo=l(),go=a("div"),m(Ty.$$.fragment),Izo=l(),Wle=a("p"),Dzo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Nzo=l(),Ja=a("p"),jzo=o("The model class to instantiate is selected based on the "),Hle=a("code"),Ozo=o("model_type"),Gzo=o(` property of the config object (either
passed as an argument or loaded from `),Ule=a("code"),qzo=o("pretrained_model_name_or_path"),zzo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Jle=a("code"),Xzo=o("pretrained_model_name_or_path"),Qzo=o(":"),Vzo=l(),W=a("ul"),ZT=a("li"),Kle=a("strong"),Wzo=o("albert"),Hzo=o(" \u2014 "),j$=a("a"),Uzo=o("TFAlbertForSequenceClassification"),Jzo=o(" (ALBERT model)"),Kzo=l(),e1=a("li"),Yle=a("strong"),Yzo=o("bert"),Zzo=o(" \u2014 "),O$=a("a"),eXo=o("TFBertForSequenceClassification"),oXo=o(" (BERT model)"),rXo=l(),o1=a("li"),Zle=a("strong"),tXo=o("camembert"),aXo=o(" \u2014 "),G$=a("a"),sXo=o("TFCamembertForSequenceClassification"),nXo=o(" (CamemBERT model)"),lXo=l(),r1=a("li"),eie=a("strong"),iXo=o("convbert"),dXo=o(" \u2014 "),q$=a("a"),cXo=o("TFConvBertForSequenceClassification"),mXo=o(" (ConvBERT model)"),fXo=l(),t1=a("li"),oie=a("strong"),hXo=o("ctrl"),gXo=o(" \u2014 "),z$=a("a"),uXo=o("TFCTRLForSequenceClassification"),pXo=o(" (CTRL model)"),_Xo=l(),a1=a("li"),rie=a("strong"),bXo=o("deberta"),vXo=o(" \u2014 "),X$=a("a"),TXo=o("TFDebertaForSequenceClassification"),FXo=o(" (DeBERTa model)"),EXo=l(),s1=a("li"),tie=a("strong"),CXo=o("deberta-v2"),MXo=o(" \u2014 "),Q$=a("a"),yXo=o("TFDebertaV2ForSequenceClassification"),wXo=o(" (DeBERTa-v2 model)"),AXo=l(),n1=a("li"),aie=a("strong"),LXo=o("distilbert"),BXo=o(" \u2014 "),V$=a("a"),xXo=o("TFDistilBertForSequenceClassification"),kXo=o(" (DistilBERT model)"),RXo=l(),l1=a("li"),sie=a("strong"),PXo=o("electra"),SXo=o(" \u2014 "),W$=a("a"),$Xo=o("TFElectraForSequenceClassification"),IXo=o(" (ELECTRA model)"),DXo=l(),i1=a("li"),nie=a("strong"),NXo=o("flaubert"),jXo=o(" \u2014 "),H$=a("a"),OXo=o("TFFlaubertForSequenceClassification"),GXo=o(" (FlauBERT model)"),qXo=l(),d1=a("li"),lie=a("strong"),zXo=o("funnel"),XXo=o(" \u2014 "),U$=a("a"),QXo=o("TFFunnelForSequenceClassification"),VXo=o(" (Funnel Transformer model)"),WXo=l(),c1=a("li"),iie=a("strong"),HXo=o("gpt2"),UXo=o(" \u2014 "),J$=a("a"),JXo=o("TFGPT2ForSequenceClassification"),KXo=o(" (OpenAI GPT-2 model)"),YXo=l(),m1=a("li"),die=a("strong"),ZXo=o("layoutlm"),eQo=o(" \u2014 "),K$=a("a"),oQo=o("TFLayoutLMForSequenceClassification"),rQo=o(" (LayoutLM model)"),tQo=l(),f1=a("li"),cie=a("strong"),aQo=o("longformer"),sQo=o(" \u2014 "),Y$=a("a"),nQo=o("TFLongformerForSequenceClassification"),lQo=o(" (Longformer model)"),iQo=l(),h1=a("li"),mie=a("strong"),dQo=o("mobilebert"),cQo=o(" \u2014 "),Z$=a("a"),mQo=o("TFMobileBertForSequenceClassification"),fQo=o(" (MobileBERT model)"),hQo=l(),g1=a("li"),fie=a("strong"),gQo=o("mpnet"),uQo=o(" \u2014 "),eI=a("a"),pQo=o("TFMPNetForSequenceClassification"),_Qo=o(" (MPNet model)"),bQo=l(),u1=a("li"),hie=a("strong"),vQo=o("openai-gpt"),TQo=o(" \u2014 "),oI=a("a"),FQo=o("TFOpenAIGPTForSequenceClassification"),EQo=o(" (OpenAI GPT model)"),CQo=l(),p1=a("li"),gie=a("strong"),MQo=o("rembert"),yQo=o(" \u2014 "),rI=a("a"),wQo=o("TFRemBertForSequenceClassification"),AQo=o(" (RemBERT model)"),LQo=l(),_1=a("li"),uie=a("strong"),BQo=o("roberta"),xQo=o(" \u2014 "),tI=a("a"),kQo=o("TFRobertaForSequenceClassification"),RQo=o(" (RoBERTa model)"),PQo=l(),b1=a("li"),pie=a("strong"),SQo=o("roformer"),$Qo=o(" \u2014 "),aI=a("a"),IQo=o("TFRoFormerForSequenceClassification"),DQo=o(" (RoFormer model)"),NQo=l(),v1=a("li"),_ie=a("strong"),jQo=o("tapas"),OQo=o(" \u2014 "),sI=a("a"),GQo=o("TFTapasForSequenceClassification"),qQo=o(" (TAPAS model)"),zQo=l(),T1=a("li"),bie=a("strong"),XQo=o("transfo-xl"),QQo=o(" \u2014 "),nI=a("a"),VQo=o("TFTransfoXLForSequenceClassification"),WQo=o(" (Transformer-XL model)"),HQo=l(),F1=a("li"),vie=a("strong"),UQo=o("xlm"),JQo=o(" \u2014 "),lI=a("a"),KQo=o("TFXLMForSequenceClassification"),YQo=o(" (XLM model)"),ZQo=l(),E1=a("li"),Tie=a("strong"),eVo=o("xlm-roberta"),oVo=o(" \u2014 "),iI=a("a"),rVo=o("TFXLMRobertaForSequenceClassification"),tVo=o(" (XLM-RoBERTa model)"),aVo=l(),C1=a("li"),Fie=a("strong"),sVo=o("xlnet"),nVo=o(" \u2014 "),dI=a("a"),lVo=o("TFXLNetForSequenceClassification"),iVo=o(" (XLNet model)"),dVo=l(),Eie=a("p"),cVo=o("Examples:"),mVo=l(),m(Fy.$$.fragment),hye=l(),Rd=a("h2"),M1=a("a"),Cie=a("span"),m(Ey.$$.fragment),fVo=l(),Mie=a("span"),hVo=o("TFAutoModelForMultipleChoice"),gye=l(),hr=a("div"),m(Cy.$$.fragment),gVo=l(),Pd=a("p"),uVo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),yie=a("code"),pVo=o("from_pretrained()"),_Vo=o(` class method or the
`),wie=a("code"),bVo=o("from_config()"),vVo=o(" class method."),TVo=l(),My=a("p"),FVo=o("This class cannot be instantiated directly using "),Aie=a("code"),EVo=o("__init__()"),CVo=o(" (throws an error)."),MVo=l(),rt=a("div"),m(yy.$$.fragment),yVo=l(),Lie=a("p"),wVo=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),AVo=l(),Sd=a("p"),LVo=o(`Note:
Loading a model from its configuration file does `),Bie=a("strong"),BVo=o("not"),xVo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xie=a("code"),kVo=o("from_pretrained()"),RVo=o(` to load the model
weights.`),PVo=l(),kie=a("p"),SVo=o("Examples:"),$Vo=l(),m(wy.$$.fragment),IVo=l(),uo=a("div"),m(Ay.$$.fragment),DVo=l(),Rie=a("p"),NVo=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),jVo=l(),Ka=a("p"),OVo=o("The model class to instantiate is selected based on the "),Pie=a("code"),GVo=o("model_type"),qVo=o(` property of the config object (either
passed as an argument or loaded from `),Sie=a("code"),zVo=o("pretrained_model_name_or_path"),XVo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),$ie=a("code"),QVo=o("pretrained_model_name_or_path"),VVo=o(":"),WVo=l(),de=a("ul"),y1=a("li"),Iie=a("strong"),HVo=o("albert"),UVo=o(" \u2014 "),cI=a("a"),JVo=o("TFAlbertForMultipleChoice"),KVo=o(" (ALBERT model)"),YVo=l(),w1=a("li"),Die=a("strong"),ZVo=o("bert"),eWo=o(" \u2014 "),mI=a("a"),oWo=o("TFBertForMultipleChoice"),rWo=o(" (BERT model)"),tWo=l(),A1=a("li"),Nie=a("strong"),aWo=o("camembert"),sWo=o(" \u2014 "),fI=a("a"),nWo=o("TFCamembertForMultipleChoice"),lWo=o(" (CamemBERT model)"),iWo=l(),L1=a("li"),jie=a("strong"),dWo=o("convbert"),cWo=o(" \u2014 "),hI=a("a"),mWo=o("TFConvBertForMultipleChoice"),fWo=o(" (ConvBERT model)"),hWo=l(),B1=a("li"),Oie=a("strong"),gWo=o("distilbert"),uWo=o(" \u2014 "),gI=a("a"),pWo=o("TFDistilBertForMultipleChoice"),_Wo=o(" (DistilBERT model)"),bWo=l(),x1=a("li"),Gie=a("strong"),vWo=o("electra"),TWo=o(" \u2014 "),uI=a("a"),FWo=o("TFElectraForMultipleChoice"),EWo=o(" (ELECTRA model)"),CWo=l(),k1=a("li"),qie=a("strong"),MWo=o("flaubert"),yWo=o(" \u2014 "),pI=a("a"),wWo=o("TFFlaubertForMultipleChoice"),AWo=o(" (FlauBERT model)"),LWo=l(),R1=a("li"),zie=a("strong"),BWo=o("funnel"),xWo=o(" \u2014 "),_I=a("a"),kWo=o("TFFunnelForMultipleChoice"),RWo=o(" (Funnel Transformer model)"),PWo=l(),P1=a("li"),Xie=a("strong"),SWo=o("longformer"),$Wo=o(" \u2014 "),bI=a("a"),IWo=o("TFLongformerForMultipleChoice"),DWo=o(" (Longformer model)"),NWo=l(),S1=a("li"),Qie=a("strong"),jWo=o("mobilebert"),OWo=o(" \u2014 "),vI=a("a"),GWo=o("TFMobileBertForMultipleChoice"),qWo=o(" (MobileBERT model)"),zWo=l(),$1=a("li"),Vie=a("strong"),XWo=o("mpnet"),QWo=o(" \u2014 "),TI=a("a"),VWo=o("TFMPNetForMultipleChoice"),WWo=o(" (MPNet model)"),HWo=l(),I1=a("li"),Wie=a("strong"),UWo=o("rembert"),JWo=o(" \u2014 "),FI=a("a"),KWo=o("TFRemBertForMultipleChoice"),YWo=o(" (RemBERT model)"),ZWo=l(),D1=a("li"),Hie=a("strong"),eHo=o("roberta"),oHo=o(" \u2014 "),EI=a("a"),rHo=o("TFRobertaForMultipleChoice"),tHo=o(" (RoBERTa model)"),aHo=l(),N1=a("li"),Uie=a("strong"),sHo=o("roformer"),nHo=o(" \u2014 "),CI=a("a"),lHo=o("TFRoFormerForMultipleChoice"),iHo=o(" (RoFormer model)"),dHo=l(),j1=a("li"),Jie=a("strong"),cHo=o("xlm"),mHo=o(" \u2014 "),MI=a("a"),fHo=o("TFXLMForMultipleChoice"),hHo=o(" (XLM model)"),gHo=l(),O1=a("li"),Kie=a("strong"),uHo=o("xlm-roberta"),pHo=o(" \u2014 "),yI=a("a"),_Ho=o("TFXLMRobertaForMultipleChoice"),bHo=o(" (XLM-RoBERTa model)"),vHo=l(),G1=a("li"),Yie=a("strong"),THo=o("xlnet"),FHo=o(" \u2014 "),wI=a("a"),EHo=o("TFXLNetForMultipleChoice"),CHo=o(" (XLNet model)"),MHo=l(),Zie=a("p"),yHo=o("Examples:"),wHo=l(),m(Ly.$$.fragment),uye=l(),$d=a("h2"),q1=a("a"),ede=a("span"),m(By.$$.fragment),AHo=l(),ode=a("span"),LHo=o("TFAutoModelForTableQuestionAnswering"),pye=l(),gr=a("div"),m(xy.$$.fragment),BHo=l(),Id=a("p"),xHo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),rde=a("code"),kHo=o("from_pretrained()"),RHo=o(` class method or the
`),tde=a("code"),PHo=o("from_config()"),SHo=o(" class method."),$Ho=l(),ky=a("p"),IHo=o("This class cannot be instantiated directly using "),ade=a("code"),DHo=o("__init__()"),NHo=o(" (throws an error)."),jHo=l(),tt=a("div"),m(Ry.$$.fragment),OHo=l(),sde=a("p"),GHo=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),qHo=l(),Dd=a("p"),zHo=o(`Note:
Loading a model from its configuration file does `),nde=a("strong"),XHo=o("not"),QHo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),lde=a("code"),VHo=o("from_pretrained()"),WHo=o(` to load the model
weights.`),HHo=l(),ide=a("p"),UHo=o("Examples:"),JHo=l(),m(Py.$$.fragment),KHo=l(),po=a("div"),m(Sy.$$.fragment),YHo=l(),dde=a("p"),ZHo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),eUo=l(),Ya=a("p"),oUo=o("The model class to instantiate is selected based on the "),cde=a("code"),rUo=o("model_type"),tUo=o(` property of the config object (either
passed as an argument or loaded from `),mde=a("code"),aUo=o("pretrained_model_name_or_path"),sUo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),fde=a("code"),nUo=o("pretrained_model_name_or_path"),lUo=o(":"),iUo=l(),hde=a("ul"),z1=a("li"),gde=a("strong"),dUo=o("tapas"),cUo=o(" \u2014 "),AI=a("a"),mUo=o("TFTapasForQuestionAnswering"),fUo=o(" (TAPAS model)"),hUo=l(),ude=a("p"),gUo=o("Examples:"),uUo=l(),m($y.$$.fragment),_ye=l(),Nd=a("h2"),X1=a("a"),pde=a("span"),m(Iy.$$.fragment),pUo=l(),_de=a("span"),_Uo=o("TFAutoModelForTokenClassification"),bye=l(),ur=a("div"),m(Dy.$$.fragment),bUo=l(),jd=a("p"),vUo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),bde=a("code"),TUo=o("from_pretrained()"),FUo=o(` class method or the
`),vde=a("code"),EUo=o("from_config()"),CUo=o(" class method."),MUo=l(),Ny=a("p"),yUo=o("This class cannot be instantiated directly using "),Tde=a("code"),wUo=o("__init__()"),AUo=o(" (throws an error)."),LUo=l(),at=a("div"),m(jy.$$.fragment),BUo=l(),Fde=a("p"),xUo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),kUo=l(),Od=a("p"),RUo=o(`Note:
Loading a model from its configuration file does `),Ede=a("strong"),PUo=o("not"),SUo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Cde=a("code"),$Uo=o("from_pretrained()"),IUo=o(` to load the model
weights.`),DUo=l(),Mde=a("p"),NUo=o("Examples:"),jUo=l(),m(Oy.$$.fragment),OUo=l(),_o=a("div"),m(Gy.$$.fragment),GUo=l(),yde=a("p"),qUo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),zUo=l(),Za=a("p"),XUo=o("The model class to instantiate is selected based on the "),wde=a("code"),QUo=o("model_type"),VUo=o(` property of the config object (either
passed as an argument or loaded from `),Ade=a("code"),WUo=o("pretrained_model_name_or_path"),HUo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Lde=a("code"),UUo=o("pretrained_model_name_or_path"),JUo=o(":"),KUo=l(),ae=a("ul"),Q1=a("li"),Bde=a("strong"),YUo=o("albert"),ZUo=o(" \u2014 "),LI=a("a"),eJo=o("TFAlbertForTokenClassification"),oJo=o(" (ALBERT model)"),rJo=l(),V1=a("li"),xde=a("strong"),tJo=o("bert"),aJo=o(" \u2014 "),BI=a("a"),sJo=o("TFBertForTokenClassification"),nJo=o(" (BERT model)"),lJo=l(),W1=a("li"),kde=a("strong"),iJo=o("camembert"),dJo=o(" \u2014 "),xI=a("a"),cJo=o("TFCamembertForTokenClassification"),mJo=o(" (CamemBERT model)"),fJo=l(),H1=a("li"),Rde=a("strong"),hJo=o("convbert"),gJo=o(" \u2014 "),kI=a("a"),uJo=o("TFConvBertForTokenClassification"),pJo=o(" (ConvBERT model)"),_Jo=l(),U1=a("li"),Pde=a("strong"),bJo=o("deberta"),vJo=o(" \u2014 "),RI=a("a"),TJo=o("TFDebertaForTokenClassification"),FJo=o(" (DeBERTa model)"),EJo=l(),J1=a("li"),Sde=a("strong"),CJo=o("deberta-v2"),MJo=o(" \u2014 "),PI=a("a"),yJo=o("TFDebertaV2ForTokenClassification"),wJo=o(" (DeBERTa-v2 model)"),AJo=l(),K1=a("li"),$de=a("strong"),LJo=o("distilbert"),BJo=o(" \u2014 "),SI=a("a"),xJo=o("TFDistilBertForTokenClassification"),kJo=o(" (DistilBERT model)"),RJo=l(),Y1=a("li"),Ide=a("strong"),PJo=o("electra"),SJo=o(" \u2014 "),$I=a("a"),$Jo=o("TFElectraForTokenClassification"),IJo=o(" (ELECTRA model)"),DJo=l(),Z1=a("li"),Dde=a("strong"),NJo=o("flaubert"),jJo=o(" \u2014 "),II=a("a"),OJo=o("TFFlaubertForTokenClassification"),GJo=o(" (FlauBERT model)"),qJo=l(),eF=a("li"),Nde=a("strong"),zJo=o("funnel"),XJo=o(" \u2014 "),DI=a("a"),QJo=o("TFFunnelForTokenClassification"),VJo=o(" (Funnel Transformer model)"),WJo=l(),oF=a("li"),jde=a("strong"),HJo=o("layoutlm"),UJo=o(" \u2014 "),NI=a("a"),JJo=o("TFLayoutLMForTokenClassification"),KJo=o(" (LayoutLM model)"),YJo=l(),rF=a("li"),Ode=a("strong"),ZJo=o("longformer"),eKo=o(" \u2014 "),jI=a("a"),oKo=o("TFLongformerForTokenClassification"),rKo=o(" (Longformer model)"),tKo=l(),tF=a("li"),Gde=a("strong"),aKo=o("mobilebert"),sKo=o(" \u2014 "),OI=a("a"),nKo=o("TFMobileBertForTokenClassification"),lKo=o(" (MobileBERT model)"),iKo=l(),aF=a("li"),qde=a("strong"),dKo=o("mpnet"),cKo=o(" \u2014 "),GI=a("a"),mKo=o("TFMPNetForTokenClassification"),fKo=o(" (MPNet model)"),hKo=l(),sF=a("li"),zde=a("strong"),gKo=o("rembert"),uKo=o(" \u2014 "),qI=a("a"),pKo=o("TFRemBertForTokenClassification"),_Ko=o(" (RemBERT model)"),bKo=l(),nF=a("li"),Xde=a("strong"),vKo=o("roberta"),TKo=o(" \u2014 "),zI=a("a"),FKo=o("TFRobertaForTokenClassification"),EKo=o(" (RoBERTa model)"),CKo=l(),lF=a("li"),Qde=a("strong"),MKo=o("roformer"),yKo=o(" \u2014 "),XI=a("a"),wKo=o("TFRoFormerForTokenClassification"),AKo=o(" (RoFormer model)"),LKo=l(),iF=a("li"),Vde=a("strong"),BKo=o("xlm"),xKo=o(" \u2014 "),QI=a("a"),kKo=o("TFXLMForTokenClassification"),RKo=o(" (XLM model)"),PKo=l(),dF=a("li"),Wde=a("strong"),SKo=o("xlm-roberta"),$Ko=o(" \u2014 "),VI=a("a"),IKo=o("TFXLMRobertaForTokenClassification"),DKo=o(" (XLM-RoBERTa model)"),NKo=l(),cF=a("li"),Hde=a("strong"),jKo=o("xlnet"),OKo=o(" \u2014 "),WI=a("a"),GKo=o("TFXLNetForTokenClassification"),qKo=o(" (XLNet model)"),zKo=l(),Ude=a("p"),XKo=o("Examples:"),QKo=l(),m(qy.$$.fragment),vye=l(),Gd=a("h2"),mF=a("a"),Jde=a("span"),m(zy.$$.fragment),VKo=l(),Kde=a("span"),WKo=o("TFAutoModelForQuestionAnswering"),Tye=l(),pr=a("div"),m(Xy.$$.fragment),HKo=l(),qd=a("p"),UKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Yde=a("code"),JKo=o("from_pretrained()"),KKo=o(` class method or the
`),Zde=a("code"),YKo=o("from_config()"),ZKo=o(" class method."),eYo=l(),Qy=a("p"),oYo=o("This class cannot be instantiated directly using "),ece=a("code"),rYo=o("__init__()"),tYo=o(" (throws an error)."),aYo=l(),st=a("div"),m(Vy.$$.fragment),sYo=l(),oce=a("p"),nYo=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),lYo=l(),zd=a("p"),iYo=o(`Note:
Loading a model from its configuration file does `),rce=a("strong"),dYo=o("not"),cYo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tce=a("code"),mYo=o("from_pretrained()"),fYo=o(` to load the model
weights.`),hYo=l(),ace=a("p"),gYo=o("Examples:"),uYo=l(),m(Wy.$$.fragment),pYo=l(),bo=a("div"),m(Hy.$$.fragment),_Yo=l(),sce=a("p"),bYo=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),vYo=l(),es=a("p"),TYo=o("The model class to instantiate is selected based on the "),nce=a("code"),FYo=o("model_type"),EYo=o(` property of the config object (either
passed as an argument or loaded from `),lce=a("code"),CYo=o("pretrained_model_name_or_path"),MYo=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),ice=a("code"),yYo=o("pretrained_model_name_or_path"),wYo=o(":"),AYo=l(),se=a("ul"),fF=a("li"),dce=a("strong"),LYo=o("albert"),BYo=o(" \u2014 "),HI=a("a"),xYo=o("TFAlbertForQuestionAnswering"),kYo=o(" (ALBERT model)"),RYo=l(),hF=a("li"),cce=a("strong"),PYo=o("bert"),SYo=o(" \u2014 "),UI=a("a"),$Yo=o("TFBertForQuestionAnswering"),IYo=o(" (BERT model)"),DYo=l(),gF=a("li"),mce=a("strong"),NYo=o("camembert"),jYo=o(" \u2014 "),JI=a("a"),OYo=o("TFCamembertForQuestionAnswering"),GYo=o(" (CamemBERT model)"),qYo=l(),uF=a("li"),fce=a("strong"),zYo=o("convbert"),XYo=o(" \u2014 "),KI=a("a"),QYo=o("TFConvBertForQuestionAnswering"),VYo=o(" (ConvBERT model)"),WYo=l(),pF=a("li"),hce=a("strong"),HYo=o("deberta"),UYo=o(" \u2014 "),YI=a("a"),JYo=o("TFDebertaForQuestionAnswering"),KYo=o(" (DeBERTa model)"),YYo=l(),_F=a("li"),gce=a("strong"),ZYo=o("deberta-v2"),eZo=o(" \u2014 "),ZI=a("a"),oZo=o("TFDebertaV2ForQuestionAnswering"),rZo=o(" (DeBERTa-v2 model)"),tZo=l(),bF=a("li"),uce=a("strong"),aZo=o("distilbert"),sZo=o(" \u2014 "),eD=a("a"),nZo=o("TFDistilBertForQuestionAnswering"),lZo=o(" (DistilBERT model)"),iZo=l(),vF=a("li"),pce=a("strong"),dZo=o("electra"),cZo=o(" \u2014 "),oD=a("a"),mZo=o("TFElectraForQuestionAnswering"),fZo=o(" (ELECTRA model)"),hZo=l(),TF=a("li"),_ce=a("strong"),gZo=o("flaubert"),uZo=o(" \u2014 "),rD=a("a"),pZo=o("TFFlaubertForQuestionAnsweringSimple"),_Zo=o(" (FlauBERT model)"),bZo=l(),FF=a("li"),bce=a("strong"),vZo=o("funnel"),TZo=o(" \u2014 "),tD=a("a"),FZo=o("TFFunnelForQuestionAnswering"),EZo=o(" (Funnel Transformer model)"),CZo=l(),EF=a("li"),vce=a("strong"),MZo=o("longformer"),yZo=o(" \u2014 "),aD=a("a"),wZo=o("TFLongformerForQuestionAnswering"),AZo=o(" (Longformer model)"),LZo=l(),CF=a("li"),Tce=a("strong"),BZo=o("mobilebert"),xZo=o(" \u2014 "),sD=a("a"),kZo=o("TFMobileBertForQuestionAnswering"),RZo=o(" (MobileBERT model)"),PZo=l(),MF=a("li"),Fce=a("strong"),SZo=o("mpnet"),$Zo=o(" \u2014 "),nD=a("a"),IZo=o("TFMPNetForQuestionAnswering"),DZo=o(" (MPNet model)"),NZo=l(),yF=a("li"),Ece=a("strong"),jZo=o("rembert"),OZo=o(" \u2014 "),lD=a("a"),GZo=o("TFRemBertForQuestionAnswering"),qZo=o(" (RemBERT model)"),zZo=l(),wF=a("li"),Cce=a("strong"),XZo=o("roberta"),QZo=o(" \u2014 "),iD=a("a"),VZo=o("TFRobertaForQuestionAnswering"),WZo=o(" (RoBERTa model)"),HZo=l(),AF=a("li"),Mce=a("strong"),UZo=o("roformer"),JZo=o(" \u2014 "),dD=a("a"),KZo=o("TFRoFormerForQuestionAnswering"),YZo=o(" (RoFormer model)"),ZZo=l(),LF=a("li"),yce=a("strong"),eer=o("xlm"),oer=o(" \u2014 "),cD=a("a"),rer=o("TFXLMForQuestionAnsweringSimple"),ter=o(" (XLM model)"),aer=l(),BF=a("li"),wce=a("strong"),ser=o("xlm-roberta"),ner=o(" \u2014 "),mD=a("a"),ler=o("TFXLMRobertaForQuestionAnswering"),ier=o(" (XLM-RoBERTa model)"),der=l(),xF=a("li"),Ace=a("strong"),cer=o("xlnet"),mer=o(" \u2014 "),fD=a("a"),fer=o("TFXLNetForQuestionAnsweringSimple"),her=o(" (XLNet model)"),ger=l(),Lce=a("p"),uer=o("Examples:"),per=l(),m(Uy.$$.fragment),Fye=l(),Xd=a("h2"),kF=a("a"),Bce=a("span"),m(Jy.$$.fragment),_er=l(),xce=a("span"),ber=o("FlaxAutoModel"),Eye=l(),_r=a("div"),m(Ky.$$.fragment),ver=l(),Qd=a("p"),Ter=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),kce=a("code"),Fer=o("from_pretrained()"),Eer=o(` class method or the
`),Rce=a("code"),Cer=o("from_config()"),Mer=o(" class method."),yer=l(),Yy=a("p"),wer=o("This class cannot be instantiated directly using "),Pce=a("code"),Aer=o("__init__()"),Ler=o(" (throws an error)."),Ber=l(),nt=a("div"),m(Zy.$$.fragment),xer=l(),Sce=a("p"),ker=o("Instantiates one of the base model classes of the library from a configuration."),Rer=l(),Vd=a("p"),Per=o(`Note:
Loading a model from its configuration file does `),$ce=a("strong"),Ser=o("not"),$er=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ice=a("code"),Ier=o("from_pretrained()"),Der=o(` to load the model
weights.`),Ner=l(),Dce=a("p"),jer=o("Examples:"),Oer=l(),m(ew.$$.fragment),Ger=l(),vo=a("div"),m(ow.$$.fragment),qer=l(),Nce=a("p"),zer=o("Instantiate one of the base model classes of the library from a pretrained model."),Xer=l(),os=a("p"),Qer=o("The model class to instantiate is selected based on the "),jce=a("code"),Ver=o("model_type"),Wer=o(` property of the config object (either
passed as an argument or loaded from `),Oce=a("code"),Her=o("pretrained_model_name_or_path"),Uer=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Gce=a("code"),Jer=o("pretrained_model_name_or_path"),Ker=o(":"),Yer=l(),Y=a("ul"),RF=a("li"),qce=a("strong"),Zer=o("albert"),eor=o(" \u2014 "),hD=a("a"),oor=o("FlaxAlbertModel"),ror=o(" (ALBERT model)"),tor=l(),PF=a("li"),zce=a("strong"),aor=o("bart"),sor=o(" \u2014 "),gD=a("a"),nor=o("FlaxBartModel"),lor=o(" (BART model)"),ior=l(),SF=a("li"),Xce=a("strong"),dor=o("beit"),cor=o(" \u2014 "),uD=a("a"),mor=o("FlaxBeitModel"),hor=o(" (BEiT model)"),gor=l(),$F=a("li"),Qce=a("strong"),uor=o("bert"),por=o(" \u2014 "),pD=a("a"),_or=o("FlaxBertModel"),bor=o(" (BERT model)"),vor=l(),IF=a("li"),Vce=a("strong"),Tor=o("big_bird"),For=o(" \u2014 "),_D=a("a"),Eor=o("FlaxBigBirdModel"),Cor=o(" (BigBird model)"),Mor=l(),DF=a("li"),Wce=a("strong"),yor=o("blenderbot"),wor=o(" \u2014 "),bD=a("a"),Aor=o("FlaxBlenderbotModel"),Lor=o(" (Blenderbot model)"),Bor=l(),NF=a("li"),Hce=a("strong"),xor=o("blenderbot-small"),kor=o(" \u2014 "),vD=a("a"),Ror=o("FlaxBlenderbotSmallModel"),Por=o(" (BlenderbotSmall model)"),Sor=l(),jF=a("li"),Uce=a("strong"),$or=o("clip"),Ior=o(" \u2014 "),TD=a("a"),Dor=o("FlaxCLIPModel"),Nor=o(" (CLIP model)"),jor=l(),OF=a("li"),Jce=a("strong"),Oor=o("distilbert"),Gor=o(" \u2014 "),FD=a("a"),qor=o("FlaxDistilBertModel"),zor=o(" (DistilBERT model)"),Xor=l(),GF=a("li"),Kce=a("strong"),Qor=o("electra"),Vor=o(" \u2014 "),ED=a("a"),Wor=o("FlaxElectraModel"),Hor=o(" (ELECTRA model)"),Uor=l(),qF=a("li"),Yce=a("strong"),Jor=o("gpt2"),Kor=o(" \u2014 "),CD=a("a"),Yor=o("FlaxGPT2Model"),Zor=o(" (OpenAI GPT-2 model)"),err=l(),zF=a("li"),Zce=a("strong"),orr=o("gpt_neo"),rrr=o(" \u2014 "),MD=a("a"),trr=o("FlaxGPTNeoModel"),arr=o(" (GPT Neo model)"),srr=l(),XF=a("li"),eme=a("strong"),nrr=o("gptj"),lrr=o(" \u2014 "),yD=a("a"),irr=o("FlaxGPTJModel"),drr=o(" (GPT-J model)"),crr=l(),QF=a("li"),ome=a("strong"),mrr=o("marian"),frr=o(" \u2014 "),wD=a("a"),hrr=o("FlaxMarianModel"),grr=o(" (Marian model)"),urr=l(),VF=a("li"),rme=a("strong"),prr=o("mbart"),_rr=o(" \u2014 "),AD=a("a"),brr=o("FlaxMBartModel"),vrr=o(" (mBART model)"),Trr=l(),WF=a("li"),tme=a("strong"),Frr=o("mt5"),Err=o(" \u2014 "),LD=a("a"),Crr=o("FlaxMT5Model"),Mrr=o(" (mT5 model)"),yrr=l(),HF=a("li"),ame=a("strong"),wrr=o("pegasus"),Arr=o(" \u2014 "),BD=a("a"),Lrr=o("FlaxPegasusModel"),Brr=o(" (Pegasus model)"),xrr=l(),UF=a("li"),sme=a("strong"),krr=o("roberta"),Rrr=o(" \u2014 "),xD=a("a"),Prr=o("FlaxRobertaModel"),Srr=o(" (RoBERTa model)"),$rr=l(),JF=a("li"),nme=a("strong"),Irr=o("t5"),Drr=o(" \u2014 "),kD=a("a"),Nrr=o("FlaxT5Model"),jrr=o(" (T5 model)"),Orr=l(),KF=a("li"),lme=a("strong"),Grr=o("vision-text-dual-encoder"),qrr=o(" \u2014 "),RD=a("a"),zrr=o("FlaxVisionTextDualEncoderModel"),Xrr=o(" (VisionTextDualEncoder model)"),Qrr=l(),YF=a("li"),ime=a("strong"),Vrr=o("vit"),Wrr=o(" \u2014 "),PD=a("a"),Hrr=o("FlaxViTModel"),Urr=o(" (ViT model)"),Jrr=l(),ZF=a("li"),dme=a("strong"),Krr=o("wav2vec2"),Yrr=o(" \u2014 "),SD=a("a"),Zrr=o("FlaxWav2Vec2Model"),etr=o(" (Wav2Vec2 model)"),otr=l(),cme=a("p"),rtr=o("Examples:"),ttr=l(),m(rw.$$.fragment),Cye=l(),Wd=a("h2"),eE=a("a"),mme=a("span"),m(tw.$$.fragment),atr=l(),fme=a("span"),str=o("FlaxAutoModelForCausalLM"),Mye=l(),br=a("div"),m(aw.$$.fragment),ntr=l(),Hd=a("p"),ltr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),hme=a("code"),itr=o("from_pretrained()"),dtr=o(` class method or the
`),gme=a("code"),ctr=o("from_config()"),mtr=o(" class method."),ftr=l(),sw=a("p"),htr=o("This class cannot be instantiated directly using "),ume=a("code"),gtr=o("__init__()"),utr=o(" (throws an error)."),ptr=l(),lt=a("div"),m(nw.$$.fragment),_tr=l(),pme=a("p"),btr=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),vtr=l(),Ud=a("p"),Ttr=o(`Note:
Loading a model from its configuration file does `),_me=a("strong"),Ftr=o("not"),Etr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bme=a("code"),Ctr=o("from_pretrained()"),Mtr=o(` to load the model
weights.`),ytr=l(),vme=a("p"),wtr=o("Examples:"),Atr=l(),m(lw.$$.fragment),Ltr=l(),To=a("div"),m(iw.$$.fragment),Btr=l(),Tme=a("p"),xtr=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),ktr=l(),rs=a("p"),Rtr=o("The model class to instantiate is selected based on the "),Fme=a("code"),Ptr=o("model_type"),Str=o(` property of the config object (either
passed as an argument or loaded from `),Eme=a("code"),$tr=o("pretrained_model_name_or_path"),Itr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Cme=a("code"),Dtr=o("pretrained_model_name_or_path"),Ntr=o(":"),jtr=l(),Jd=a("ul"),oE=a("li"),Mme=a("strong"),Otr=o("gpt2"),Gtr=o(" \u2014 "),$D=a("a"),qtr=o("FlaxGPT2LMHeadModel"),ztr=o(" (OpenAI GPT-2 model)"),Xtr=l(),rE=a("li"),yme=a("strong"),Qtr=o("gpt_neo"),Vtr=o(" \u2014 "),ID=a("a"),Wtr=o("FlaxGPTNeoForCausalLM"),Htr=o(" (GPT Neo model)"),Utr=l(),tE=a("li"),wme=a("strong"),Jtr=o("gptj"),Ktr=o(" \u2014 "),DD=a("a"),Ytr=o("FlaxGPTJForCausalLM"),Ztr=o(" (GPT-J model)"),ear=l(),Ame=a("p"),oar=o("Examples:"),rar=l(),m(dw.$$.fragment),yye=l(),Kd=a("h2"),aE=a("a"),Lme=a("span"),m(cw.$$.fragment),tar=l(),Bme=a("span"),aar=o("FlaxAutoModelForPreTraining"),wye=l(),vr=a("div"),m(mw.$$.fragment),sar=l(),Yd=a("p"),nar=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),xme=a("code"),lar=o("from_pretrained()"),iar=o(` class method or the
`),kme=a("code"),dar=o("from_config()"),car=o(" class method."),mar=l(),fw=a("p"),far=o("This class cannot be instantiated directly using "),Rme=a("code"),har=o("__init__()"),gar=o(" (throws an error)."),uar=l(),it=a("div"),m(hw.$$.fragment),par=l(),Pme=a("p"),_ar=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),bar=l(),Zd=a("p"),Tar=o(`Note:
Loading a model from its configuration file does `),Sme=a("strong"),Far=o("not"),Ear=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),$me=a("code"),Car=o("from_pretrained()"),Mar=o(` to load the model
weights.`),yar=l(),Ime=a("p"),war=o("Examples:"),Aar=l(),m(gw.$$.fragment),Lar=l(),Fo=a("div"),m(uw.$$.fragment),Bar=l(),Dme=a("p"),xar=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),kar=l(),ts=a("p"),Rar=o("The model class to instantiate is selected based on the "),Nme=a("code"),Par=o("model_type"),Sar=o(` property of the config object (either
passed as an argument or loaded from `),jme=a("code"),$ar=o("pretrained_model_name_or_path"),Iar=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Ome=a("code"),Dar=o("pretrained_model_name_or_path"),Nar=o(":"),jar=l(),ve=a("ul"),sE=a("li"),Gme=a("strong"),Oar=o("albert"),Gar=o(" \u2014 "),ND=a("a"),qar=o("FlaxAlbertForPreTraining"),zar=o(" (ALBERT model)"),Xar=l(),nE=a("li"),qme=a("strong"),Qar=o("bart"),Var=o(" \u2014 "),jD=a("a"),War=o("FlaxBartForConditionalGeneration"),Har=o(" (BART model)"),Uar=l(),lE=a("li"),zme=a("strong"),Jar=o("bert"),Kar=o(" \u2014 "),OD=a("a"),Yar=o("FlaxBertForPreTraining"),Zar=o(" (BERT model)"),esr=l(),iE=a("li"),Xme=a("strong"),osr=o("big_bird"),rsr=o(" \u2014 "),GD=a("a"),tsr=o("FlaxBigBirdForPreTraining"),asr=o(" (BigBird model)"),ssr=l(),dE=a("li"),Qme=a("strong"),nsr=o("electra"),lsr=o(" \u2014 "),qD=a("a"),isr=o("FlaxElectraForPreTraining"),dsr=o(" (ELECTRA model)"),csr=l(),cE=a("li"),Vme=a("strong"),msr=o("mbart"),fsr=o(" \u2014 "),zD=a("a"),hsr=o("FlaxMBartForConditionalGeneration"),gsr=o(" (mBART model)"),usr=l(),mE=a("li"),Wme=a("strong"),psr=o("mt5"),_sr=o(" \u2014 "),XD=a("a"),bsr=o("FlaxMT5ForConditionalGeneration"),vsr=o(" (mT5 model)"),Tsr=l(),fE=a("li"),Hme=a("strong"),Fsr=o("roberta"),Esr=o(" \u2014 "),QD=a("a"),Csr=o("FlaxRobertaForMaskedLM"),Msr=o(" (RoBERTa model)"),ysr=l(),hE=a("li"),Ume=a("strong"),wsr=o("t5"),Asr=o(" \u2014 "),VD=a("a"),Lsr=o("FlaxT5ForConditionalGeneration"),Bsr=o(" (T5 model)"),xsr=l(),gE=a("li"),Jme=a("strong"),ksr=o("wav2vec2"),Rsr=o(" \u2014 "),WD=a("a"),Psr=o("FlaxWav2Vec2ForPreTraining"),Ssr=o(" (Wav2Vec2 model)"),$sr=l(),Kme=a("p"),Isr=o("Examples:"),Dsr=l(),m(pw.$$.fragment),Aye=l(),ec=a("h2"),uE=a("a"),Yme=a("span"),m(_w.$$.fragment),Nsr=l(),Zme=a("span"),jsr=o("FlaxAutoModelForMaskedLM"),Lye=l(),Tr=a("div"),m(bw.$$.fragment),Osr=l(),oc=a("p"),Gsr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),efe=a("code"),qsr=o("from_pretrained()"),zsr=o(` class method or the
`),ofe=a("code"),Xsr=o("from_config()"),Qsr=o(" class method."),Vsr=l(),vw=a("p"),Wsr=o("This class cannot be instantiated directly using "),rfe=a("code"),Hsr=o("__init__()"),Usr=o(" (throws an error)."),Jsr=l(),dt=a("div"),m(Tw.$$.fragment),Ksr=l(),tfe=a("p"),Ysr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),Zsr=l(),rc=a("p"),enr=o(`Note:
Loading a model from its configuration file does `),afe=a("strong"),onr=o("not"),rnr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sfe=a("code"),tnr=o("from_pretrained()"),anr=o(` to load the model
weights.`),snr=l(),nfe=a("p"),nnr=o("Examples:"),lnr=l(),m(Fw.$$.fragment),inr=l(),Eo=a("div"),m(Ew.$$.fragment),dnr=l(),lfe=a("p"),cnr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),mnr=l(),as=a("p"),fnr=o("The model class to instantiate is selected based on the "),ife=a("code"),hnr=o("model_type"),gnr=o(` property of the config object (either
passed as an argument or loaded from `),dfe=a("code"),unr=o("pretrained_model_name_or_path"),pnr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),cfe=a("code"),_nr=o("pretrained_model_name_or_path"),bnr=o(":"),vnr=l(),xe=a("ul"),pE=a("li"),mfe=a("strong"),Tnr=o("albert"),Fnr=o(" \u2014 "),HD=a("a"),Enr=o("FlaxAlbertForMaskedLM"),Cnr=o(" (ALBERT model)"),Mnr=l(),_E=a("li"),ffe=a("strong"),ynr=o("bart"),wnr=o(" \u2014 "),UD=a("a"),Anr=o("FlaxBartForConditionalGeneration"),Lnr=o(" (BART model)"),Bnr=l(),bE=a("li"),hfe=a("strong"),xnr=o("bert"),knr=o(" \u2014 "),JD=a("a"),Rnr=o("FlaxBertForMaskedLM"),Pnr=o(" (BERT model)"),Snr=l(),vE=a("li"),gfe=a("strong"),$nr=o("big_bird"),Inr=o(" \u2014 "),KD=a("a"),Dnr=o("FlaxBigBirdForMaskedLM"),Nnr=o(" (BigBird model)"),jnr=l(),TE=a("li"),ufe=a("strong"),Onr=o("distilbert"),Gnr=o(" \u2014 "),YD=a("a"),qnr=o("FlaxDistilBertForMaskedLM"),znr=o(" (DistilBERT model)"),Xnr=l(),FE=a("li"),pfe=a("strong"),Qnr=o("electra"),Vnr=o(" \u2014 "),ZD=a("a"),Wnr=o("FlaxElectraForMaskedLM"),Hnr=o(" (ELECTRA model)"),Unr=l(),EE=a("li"),_fe=a("strong"),Jnr=o("mbart"),Knr=o(" \u2014 "),eN=a("a"),Ynr=o("FlaxMBartForConditionalGeneration"),Znr=o(" (mBART model)"),elr=l(),CE=a("li"),bfe=a("strong"),olr=o("roberta"),rlr=o(" \u2014 "),oN=a("a"),tlr=o("FlaxRobertaForMaskedLM"),alr=o(" (RoBERTa model)"),slr=l(),vfe=a("p"),nlr=o("Examples:"),llr=l(),m(Cw.$$.fragment),Bye=l(),tc=a("h2"),ME=a("a"),Tfe=a("span"),m(Mw.$$.fragment),ilr=l(),Ffe=a("span"),dlr=o("FlaxAutoModelForSeq2SeqLM"),xye=l(),Fr=a("div"),m(yw.$$.fragment),clr=l(),ac=a("p"),mlr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Efe=a("code"),flr=o("from_pretrained()"),hlr=o(` class method or the
`),Cfe=a("code"),glr=o("from_config()"),ulr=o(" class method."),plr=l(),ww=a("p"),_lr=o("This class cannot be instantiated directly using "),Mfe=a("code"),blr=o("__init__()"),vlr=o(" (throws an error)."),Tlr=l(),ct=a("div"),m(Aw.$$.fragment),Flr=l(),yfe=a("p"),Elr=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Clr=l(),sc=a("p"),Mlr=o(`Note:
Loading a model from its configuration file does `),wfe=a("strong"),ylr=o("not"),wlr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Afe=a("code"),Alr=o("from_pretrained()"),Llr=o(` to load the model
weights.`),Blr=l(),Lfe=a("p"),xlr=o("Examples:"),klr=l(),m(Lw.$$.fragment),Rlr=l(),Co=a("div"),m(Bw.$$.fragment),Plr=l(),Bfe=a("p"),Slr=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),$lr=l(),ss=a("p"),Ilr=o("The model class to instantiate is selected based on the "),xfe=a("code"),Dlr=o("model_type"),Nlr=o(` property of the config object (either
passed as an argument or loaded from `),kfe=a("code"),jlr=o("pretrained_model_name_or_path"),Olr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Rfe=a("code"),Glr=o("pretrained_model_name_or_path"),qlr=o(":"),zlr=l(),Me=a("ul"),yE=a("li"),Pfe=a("strong"),Xlr=o("bart"),Qlr=o(" \u2014 "),rN=a("a"),Vlr=o("FlaxBartForConditionalGeneration"),Wlr=o(" (BART model)"),Hlr=l(),wE=a("li"),Sfe=a("strong"),Ulr=o("blenderbot"),Jlr=o(" \u2014 "),tN=a("a"),Klr=o("FlaxBlenderbotForConditionalGeneration"),Ylr=o(" (Blenderbot model)"),Zlr=l(),AE=a("li"),$fe=a("strong"),eir=o("blenderbot-small"),oir=o(" \u2014 "),aN=a("a"),rir=o("FlaxBlenderbotSmallForConditionalGeneration"),tir=o(" (BlenderbotSmall model)"),air=l(),LE=a("li"),Ife=a("strong"),sir=o("encoder-decoder"),nir=o(" \u2014 "),sN=a("a"),lir=o("FlaxEncoderDecoderModel"),iir=o(" (Encoder decoder model)"),dir=l(),BE=a("li"),Dfe=a("strong"),cir=o("marian"),mir=o(" \u2014 "),nN=a("a"),fir=o("FlaxMarianMTModel"),hir=o(" (Marian model)"),gir=l(),xE=a("li"),Nfe=a("strong"),uir=o("mbart"),pir=o(" \u2014 "),lN=a("a"),_ir=o("FlaxMBartForConditionalGeneration"),bir=o(" (mBART model)"),vir=l(),kE=a("li"),jfe=a("strong"),Tir=o("mt5"),Fir=o(" \u2014 "),iN=a("a"),Eir=o("FlaxMT5ForConditionalGeneration"),Cir=o(" (mT5 model)"),Mir=l(),RE=a("li"),Ofe=a("strong"),yir=o("pegasus"),wir=o(" \u2014 "),dN=a("a"),Air=o("FlaxPegasusForConditionalGeneration"),Lir=o(" (Pegasus model)"),Bir=l(),PE=a("li"),Gfe=a("strong"),xir=o("t5"),kir=o(" \u2014 "),cN=a("a"),Rir=o("FlaxT5ForConditionalGeneration"),Pir=o(" (T5 model)"),Sir=l(),qfe=a("p"),$ir=o("Examples:"),Iir=l(),m(xw.$$.fragment),kye=l(),nc=a("h2"),SE=a("a"),zfe=a("span"),m(kw.$$.fragment),Dir=l(),Xfe=a("span"),Nir=o("FlaxAutoModelForSequenceClassification"),Rye=l(),Er=a("div"),m(Rw.$$.fragment),jir=l(),lc=a("p"),Oir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Qfe=a("code"),Gir=o("from_pretrained()"),qir=o(` class method or the
`),Vfe=a("code"),zir=o("from_config()"),Xir=o(" class method."),Qir=l(),Pw=a("p"),Vir=o("This class cannot be instantiated directly using "),Wfe=a("code"),Wir=o("__init__()"),Hir=o(" (throws an error)."),Uir=l(),mt=a("div"),m(Sw.$$.fragment),Jir=l(),Hfe=a("p"),Kir=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Yir=l(),ic=a("p"),Zir=o(`Note:
Loading a model from its configuration file does `),Ufe=a("strong"),edr=o("not"),odr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Jfe=a("code"),rdr=o("from_pretrained()"),tdr=o(` to load the model
weights.`),adr=l(),Kfe=a("p"),sdr=o("Examples:"),ndr=l(),m($w.$$.fragment),ldr=l(),Mo=a("div"),m(Iw.$$.fragment),idr=l(),Yfe=a("p"),ddr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),cdr=l(),ns=a("p"),mdr=o("The model class to instantiate is selected based on the "),Zfe=a("code"),fdr=o("model_type"),hdr=o(` property of the config object (either
passed as an argument or loaded from `),ehe=a("code"),gdr=o("pretrained_model_name_or_path"),udr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),ohe=a("code"),pdr=o("pretrained_model_name_or_path"),_dr=o(":"),bdr=l(),ke=a("ul"),$E=a("li"),rhe=a("strong"),vdr=o("albert"),Tdr=o(" \u2014 "),mN=a("a"),Fdr=o("FlaxAlbertForSequenceClassification"),Edr=o(" (ALBERT model)"),Cdr=l(),IE=a("li"),the=a("strong"),Mdr=o("bart"),ydr=o(" \u2014 "),fN=a("a"),wdr=o("FlaxBartForSequenceClassification"),Adr=o(" (BART model)"),Ldr=l(),DE=a("li"),ahe=a("strong"),Bdr=o("bert"),xdr=o(" \u2014 "),hN=a("a"),kdr=o("FlaxBertForSequenceClassification"),Rdr=o(" (BERT model)"),Pdr=l(),NE=a("li"),she=a("strong"),Sdr=o("big_bird"),$dr=o(" \u2014 "),gN=a("a"),Idr=o("FlaxBigBirdForSequenceClassification"),Ddr=o(" (BigBird model)"),Ndr=l(),jE=a("li"),nhe=a("strong"),jdr=o("distilbert"),Odr=o(" \u2014 "),uN=a("a"),Gdr=o("FlaxDistilBertForSequenceClassification"),qdr=o(" (DistilBERT model)"),zdr=l(),OE=a("li"),lhe=a("strong"),Xdr=o("electra"),Qdr=o(" \u2014 "),pN=a("a"),Vdr=o("FlaxElectraForSequenceClassification"),Wdr=o(" (ELECTRA model)"),Hdr=l(),GE=a("li"),ihe=a("strong"),Udr=o("mbart"),Jdr=o(" \u2014 "),_N=a("a"),Kdr=o("FlaxMBartForSequenceClassification"),Ydr=o(" (mBART model)"),Zdr=l(),qE=a("li"),dhe=a("strong"),ecr=o("roberta"),ocr=o(" \u2014 "),bN=a("a"),rcr=o("FlaxRobertaForSequenceClassification"),tcr=o(" (RoBERTa model)"),acr=l(),che=a("p"),scr=o("Examples:"),ncr=l(),m(Dw.$$.fragment),Pye=l(),dc=a("h2"),zE=a("a"),mhe=a("span"),m(Nw.$$.fragment),lcr=l(),fhe=a("span"),icr=o("FlaxAutoModelForQuestionAnswering"),Sye=l(),Cr=a("div"),m(jw.$$.fragment),dcr=l(),cc=a("p"),ccr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),hhe=a("code"),mcr=o("from_pretrained()"),fcr=o(` class method or the
`),ghe=a("code"),hcr=o("from_config()"),gcr=o(" class method."),ucr=l(),Ow=a("p"),pcr=o("This class cannot be instantiated directly using "),uhe=a("code"),_cr=o("__init__()"),bcr=o(" (throws an error)."),vcr=l(),ft=a("div"),m(Gw.$$.fragment),Tcr=l(),phe=a("p"),Fcr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Ecr=l(),mc=a("p"),Ccr=o(`Note:
Loading a model from its configuration file does `),_he=a("strong"),Mcr=o("not"),ycr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bhe=a("code"),wcr=o("from_pretrained()"),Acr=o(` to load the model
weights.`),Lcr=l(),vhe=a("p"),Bcr=o("Examples:"),xcr=l(),m(qw.$$.fragment),kcr=l(),yo=a("div"),m(zw.$$.fragment),Rcr=l(),The=a("p"),Pcr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Scr=l(),ls=a("p"),$cr=o("The model class to instantiate is selected based on the "),Fhe=a("code"),Icr=o("model_type"),Dcr=o(` property of the config object (either
passed as an argument or loaded from `),Ehe=a("code"),Ncr=o("pretrained_model_name_or_path"),jcr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Che=a("code"),Ocr=o("pretrained_model_name_or_path"),Gcr=o(":"),qcr=l(),Re=a("ul"),XE=a("li"),Mhe=a("strong"),zcr=o("albert"),Xcr=o(" \u2014 "),vN=a("a"),Qcr=o("FlaxAlbertForQuestionAnswering"),Vcr=o(" (ALBERT model)"),Wcr=l(),QE=a("li"),yhe=a("strong"),Hcr=o("bart"),Ucr=o(" \u2014 "),TN=a("a"),Jcr=o("FlaxBartForQuestionAnswering"),Kcr=o(" (BART model)"),Ycr=l(),VE=a("li"),whe=a("strong"),Zcr=o("bert"),emr=o(" \u2014 "),FN=a("a"),omr=o("FlaxBertForQuestionAnswering"),rmr=o(" (BERT model)"),tmr=l(),WE=a("li"),Ahe=a("strong"),amr=o("big_bird"),smr=o(" \u2014 "),EN=a("a"),nmr=o("FlaxBigBirdForQuestionAnswering"),lmr=o(" (BigBird model)"),imr=l(),HE=a("li"),Lhe=a("strong"),dmr=o("distilbert"),cmr=o(" \u2014 "),CN=a("a"),mmr=o("FlaxDistilBertForQuestionAnswering"),fmr=o(" (DistilBERT model)"),hmr=l(),UE=a("li"),Bhe=a("strong"),gmr=o("electra"),umr=o(" \u2014 "),MN=a("a"),pmr=o("FlaxElectraForQuestionAnswering"),_mr=o(" (ELECTRA model)"),bmr=l(),JE=a("li"),xhe=a("strong"),vmr=o("mbart"),Tmr=o(" \u2014 "),yN=a("a"),Fmr=o("FlaxMBartForQuestionAnswering"),Emr=o(" (mBART model)"),Cmr=l(),KE=a("li"),khe=a("strong"),Mmr=o("roberta"),ymr=o(" \u2014 "),wN=a("a"),wmr=o("FlaxRobertaForQuestionAnswering"),Amr=o(" (RoBERTa model)"),Lmr=l(),Rhe=a("p"),Bmr=o("Examples:"),xmr=l(),m(Xw.$$.fragment),$ye=l(),fc=a("h2"),YE=a("a"),Phe=a("span"),m(Qw.$$.fragment),kmr=l(),She=a("span"),Rmr=o("FlaxAutoModelForTokenClassification"),Iye=l(),Mr=a("div"),m(Vw.$$.fragment),Pmr=l(),hc=a("p"),Smr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),$he=a("code"),$mr=o("from_pretrained()"),Imr=o(` class method or the
`),Ihe=a("code"),Dmr=o("from_config()"),Nmr=o(" class method."),jmr=l(),Ww=a("p"),Omr=o("This class cannot be instantiated directly using "),Dhe=a("code"),Gmr=o("__init__()"),qmr=o(" (throws an error)."),zmr=l(),ht=a("div"),m(Hw.$$.fragment),Xmr=l(),Nhe=a("p"),Qmr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Vmr=l(),gc=a("p"),Wmr=o(`Note:
Loading a model from its configuration file does `),jhe=a("strong"),Hmr=o("not"),Umr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ohe=a("code"),Jmr=o("from_pretrained()"),Kmr=o(` to load the model
weights.`),Ymr=l(),Ghe=a("p"),Zmr=o("Examples:"),efr=l(),m(Uw.$$.fragment),ofr=l(),wo=a("div"),m(Jw.$$.fragment),rfr=l(),qhe=a("p"),tfr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),afr=l(),is=a("p"),sfr=o("The model class to instantiate is selected based on the "),zhe=a("code"),nfr=o("model_type"),lfr=o(` property of the config object (either
passed as an argument or loaded from `),Xhe=a("code"),ifr=o("pretrained_model_name_or_path"),dfr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Qhe=a("code"),cfr=o("pretrained_model_name_or_path"),mfr=o(":"),ffr=l(),yr=a("ul"),ZE=a("li"),Vhe=a("strong"),hfr=o("albert"),gfr=o(" \u2014 "),AN=a("a"),ufr=o("FlaxAlbertForTokenClassification"),pfr=o(" (ALBERT model)"),_fr=l(),e4=a("li"),Whe=a("strong"),bfr=o("bert"),vfr=o(" \u2014 "),LN=a("a"),Tfr=o("FlaxBertForTokenClassification"),Ffr=o(" (BERT model)"),Efr=l(),o4=a("li"),Hhe=a("strong"),Cfr=o("big_bird"),Mfr=o(" \u2014 "),BN=a("a"),yfr=o("FlaxBigBirdForTokenClassification"),wfr=o(" (BigBird model)"),Afr=l(),r4=a("li"),Uhe=a("strong"),Lfr=o("distilbert"),Bfr=o(" \u2014 "),xN=a("a"),xfr=o("FlaxDistilBertForTokenClassification"),kfr=o(" (DistilBERT model)"),Rfr=l(),t4=a("li"),Jhe=a("strong"),Pfr=o("electra"),Sfr=o(" \u2014 "),kN=a("a"),$fr=o("FlaxElectraForTokenClassification"),Ifr=o(" (ELECTRA model)"),Dfr=l(),a4=a("li"),Khe=a("strong"),Nfr=o("roberta"),jfr=o(" \u2014 "),RN=a("a"),Ofr=o("FlaxRobertaForTokenClassification"),Gfr=o(" (RoBERTa model)"),qfr=l(),Yhe=a("p"),zfr=o("Examples:"),Xfr=l(),m(Kw.$$.fragment),Dye=l(),uc=a("h2"),s4=a("a"),Zhe=a("span"),m(Yw.$$.fragment),Qfr=l(),ege=a("span"),Vfr=o("FlaxAutoModelForMultipleChoice"),Nye=l(),wr=a("div"),m(Zw.$$.fragment),Wfr=l(),pc=a("p"),Hfr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),oge=a("code"),Ufr=o("from_pretrained()"),Jfr=o(` class method or the
`),rge=a("code"),Kfr=o("from_config()"),Yfr=o(" class method."),Zfr=l(),e7=a("p"),ehr=o("This class cannot be instantiated directly using "),tge=a("code"),ohr=o("__init__()"),rhr=o(" (throws an error)."),thr=l(),gt=a("div"),m(o7.$$.fragment),ahr=l(),age=a("p"),shr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),nhr=l(),_c=a("p"),lhr=o(`Note:
Loading a model from its configuration file does `),sge=a("strong"),ihr=o("not"),dhr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nge=a("code"),chr=o("from_pretrained()"),mhr=o(` to load the model
weights.`),fhr=l(),lge=a("p"),hhr=o("Examples:"),ghr=l(),m(r7.$$.fragment),uhr=l(),Ao=a("div"),m(t7.$$.fragment),phr=l(),ige=a("p"),_hr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),bhr=l(),ds=a("p"),vhr=o("The model class to instantiate is selected based on the "),dge=a("code"),Thr=o("model_type"),Fhr=o(` property of the config object (either
passed as an argument or loaded from `),cge=a("code"),Ehr=o("pretrained_model_name_or_path"),Chr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),mge=a("code"),Mhr=o("pretrained_model_name_or_path"),yhr=o(":"),whr=l(),Ar=a("ul"),n4=a("li"),fge=a("strong"),Ahr=o("albert"),Lhr=o(" \u2014 "),PN=a("a"),Bhr=o("FlaxAlbertForMultipleChoice"),xhr=o(" (ALBERT model)"),khr=l(),l4=a("li"),hge=a("strong"),Rhr=o("bert"),Phr=o(" \u2014 "),SN=a("a"),Shr=o("FlaxBertForMultipleChoice"),$hr=o(" (BERT model)"),Ihr=l(),i4=a("li"),gge=a("strong"),Dhr=o("big_bird"),Nhr=o(" \u2014 "),$N=a("a"),jhr=o("FlaxBigBirdForMultipleChoice"),Ohr=o(" (BigBird model)"),Ghr=l(),d4=a("li"),uge=a("strong"),qhr=o("distilbert"),zhr=o(" \u2014 "),IN=a("a"),Xhr=o("FlaxDistilBertForMultipleChoice"),Qhr=o(" (DistilBERT model)"),Vhr=l(),c4=a("li"),pge=a("strong"),Whr=o("electra"),Hhr=o(" \u2014 "),DN=a("a"),Uhr=o("FlaxElectraForMultipleChoice"),Jhr=o(" (ELECTRA model)"),Khr=l(),m4=a("li"),_ge=a("strong"),Yhr=o("roberta"),Zhr=o(" \u2014 "),NN=a("a"),egr=o("FlaxRobertaForMultipleChoice"),ogr=o(" (RoBERTa model)"),rgr=l(),bge=a("p"),tgr=o("Examples:"),agr=l(),m(a7.$$.fragment),jye=l(),bc=a("h2"),f4=a("a"),vge=a("span"),m(s7.$$.fragment),sgr=l(),Tge=a("span"),ngr=o("FlaxAutoModelForNextSentencePrediction"),Oye=l(),Lr=a("div"),m(n7.$$.fragment),lgr=l(),vc=a("p"),igr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Fge=a("code"),dgr=o("from_pretrained()"),cgr=o(` class method or the
`),Ege=a("code"),mgr=o("from_config()"),fgr=o(" class method."),hgr=l(),l7=a("p"),ggr=o("This class cannot be instantiated directly using "),Cge=a("code"),ugr=o("__init__()"),pgr=o(" (throws an error)."),_gr=l(),ut=a("div"),m(i7.$$.fragment),bgr=l(),Mge=a("p"),vgr=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Tgr=l(),Tc=a("p"),Fgr=o(`Note:
Loading a model from its configuration file does `),yge=a("strong"),Egr=o("not"),Cgr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),wge=a("code"),Mgr=o("from_pretrained()"),ygr=o(` to load the model
weights.`),wgr=l(),Age=a("p"),Agr=o("Examples:"),Lgr=l(),m(d7.$$.fragment),Bgr=l(),Lo=a("div"),m(c7.$$.fragment),xgr=l(),Lge=a("p"),kgr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Rgr=l(),cs=a("p"),Pgr=o("The model class to instantiate is selected based on the "),Bge=a("code"),Sgr=o("model_type"),$gr=o(` property of the config object (either
passed as an argument or loaded from `),xge=a("code"),Igr=o("pretrained_model_name_or_path"),Dgr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),kge=a("code"),Ngr=o("pretrained_model_name_or_path"),jgr=o(":"),Ogr=l(),Rge=a("ul"),h4=a("li"),Pge=a("strong"),Ggr=o("bert"),qgr=o(" \u2014 "),jN=a("a"),zgr=o("FlaxBertForNextSentencePrediction"),Xgr=o(" (BERT model)"),Qgr=l(),Sge=a("p"),Vgr=o("Examples:"),Wgr=l(),m(m7.$$.fragment),Gye=l(),Fc=a("h2"),g4=a("a"),$ge=a("span"),m(f7.$$.fragment),Hgr=l(),Ige=a("span"),Ugr=o("FlaxAutoModelForImageClassification"),qye=l(),Br=a("div"),m(h7.$$.fragment),Jgr=l(),Ec=a("p"),Kgr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Dge=a("code"),Ygr=o("from_pretrained()"),Zgr=o(` class method or the
`),Nge=a("code"),eur=o("from_config()"),our=o(" class method."),rur=l(),g7=a("p"),tur=o("This class cannot be instantiated directly using "),jge=a("code"),aur=o("__init__()"),sur=o(" (throws an error)."),nur=l(),pt=a("div"),m(u7.$$.fragment),lur=l(),Oge=a("p"),iur=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),dur=l(),Cc=a("p"),cur=o(`Note:
Loading a model from its configuration file does `),Gge=a("strong"),mur=o("not"),fur=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qge=a("code"),hur=o("from_pretrained()"),gur=o(` to load the model
weights.`),uur=l(),zge=a("p"),pur=o("Examples:"),_ur=l(),m(p7.$$.fragment),bur=l(),Bo=a("div"),m(_7.$$.fragment),vur=l(),Xge=a("p"),Tur=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Fur=l(),ms=a("p"),Eur=o("The model class to instantiate is selected based on the "),Qge=a("code"),Cur=o("model_type"),Mur=o(` property of the config object (either
passed as an argument or loaded from `),Vge=a("code"),yur=o("pretrained_model_name_or_path"),wur=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Wge=a("code"),Aur=o("pretrained_model_name_or_path"),Lur=o(":"),Bur=l(),b7=a("ul"),u4=a("li"),Hge=a("strong"),xur=o("beit"),kur=o(" \u2014 "),ON=a("a"),Rur=o("FlaxBeitForImageClassification"),Pur=o(" (BEiT model)"),Sur=l(),p4=a("li"),Uge=a("strong"),$ur=o("vit"),Iur=o(" \u2014 "),GN=a("a"),Dur=o("FlaxViTForImageClassification"),Nur=o(" (ViT model)"),jur=l(),Jge=a("p"),Our=o("Examples:"),Gur=l(),m(v7.$$.fragment),zye=l(),Mc=a("h2"),_4=a("a"),Kge=a("span"),m(T7.$$.fragment),qur=l(),Yge=a("span"),zur=o("FlaxAutoModelForVision2Seq"),Xye=l(),xr=a("div"),m(F7.$$.fragment),Xur=l(),yc=a("p"),Qur=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Zge=a("code"),Vur=o("from_pretrained()"),Wur=o(` class method or the
`),eue=a("code"),Hur=o("from_config()"),Uur=o(" class method."),Jur=l(),E7=a("p"),Kur=o("This class cannot be instantiated directly using "),oue=a("code"),Yur=o("__init__()"),Zur=o(" (throws an error)."),epr=l(),_t=a("div"),m(C7.$$.fragment),opr=l(),rue=a("p"),rpr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),tpr=l(),wc=a("p"),apr=o(`Note:
Loading a model from its configuration file does `),tue=a("strong"),spr=o("not"),npr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),aue=a("code"),lpr=o("from_pretrained()"),ipr=o(` to load the model
weights.`),dpr=l(),sue=a("p"),cpr=o("Examples:"),mpr=l(),m(M7.$$.fragment),fpr=l(),xo=a("div"),m(y7.$$.fragment),hpr=l(),nue=a("p"),gpr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),upr=l(),fs=a("p"),ppr=o("The model class to instantiate is selected based on the "),lue=a("code"),_pr=o("model_type"),bpr=o(` property of the config object (either
passed as an argument or loaded from `),iue=a("code"),vpr=o("pretrained_model_name_or_path"),Tpr=o(` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),due=a("code"),Fpr=o("pretrained_model_name_or_path"),Epr=o(":"),Cpr=l(),cue=a("ul"),b4=a("li"),mue=a("strong"),Mpr=o("vision-encoder-decoder"),ypr=o(" \u2014 "),qN=a("a"),wpr=o("FlaxVisionEncoderDecoderModel"),Apr=o(" (Vision Encoder decoder model)"),Lpr=l(),fue=a("p"),Bpr=o("Examples:"),xpr=l(),m(w7.$$.fragment),this.h()},l(d){const _=IKr('[data-svelte="svelte-1phssyn"]',document.head);re=s(_,"META",{name:!0,content:!0}),_.forEach(t),Pe=i(d),fe=s(d,"H1",{class:!0});var A7=n(fe);ue=s(A7,"A",{id:!0,class:!0,href:!0});var hue=n(ue);ro=s(hue,"SPAN",{});var gue=n(ro);f(ge.$$.fragment,gue),gue.forEach(t),hue.forEach(t),Ee=i(A7),$o=s(A7,"SPAN",{});var Rpr=n($o);zl=r(Rpr,"Auto Classes"),Rpr.forEach(t),A7.forEach(t),Lc=i(d),Ot=s(d,"P",{});var Vye=n(Ot);Xl=r(Vye,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),Ql=s(Vye,"CODE",{});var Ppr=n(Ql);pC=r(Ppr,"from_pretrained()"),Ppr.forEach(t),Bc=r(Vye,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),Vye.forEach(t),Be=i(d),ao=s(d,"P",{});var v4=n(ao);Vl=r(v4,"Instantiating one of "),hs=s(v4,"A",{href:!0});var Spr=n(hs);_C=r(Spr,"AutoConfig"),Spr.forEach(t),gs=r(v4,", "),us=s(v4,"A",{href:!0});var $pr=n(us);bC=r($pr,"AutoModel"),$pr.forEach(t),Wl=r(v4,`, and
`),ps=s(v4,"A",{href:!0});var Ipr=n(ps);vC=r(Ipr,"AutoTokenizer"),Ipr.forEach(t),Hl=r(v4," will directly create a class of the relevant architecture. For instance"),v4.forEach(t),xc=i(d),f(_a.$$.fragment,d),so=i(d),pe=s(d,"P",{});var Wye=n(pe);T0=r(Wye,"will create a model that is an instance of "),Ul=s(Wye,"A",{href:!0});var Dpr=n(Ul);F0=r(Dpr,"BertModel"),Dpr.forEach(t),E0=r(Wye,"."),Wye.forEach(t),Io=i(d),ba=s(d,"P",{});var Hye=n(ba);C0=r(Hye,"There is one class of "),kc=s(Hye,"CODE",{});var Npr=n(kc);M0=r(Npr,"AutoModel"),Npr.forEach(t),e0e=r(Hye," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),Hye.forEach(t),a5e=i(d),Jl=s(d,"H2",{class:!0});var Uye=n(Jl);Rc=s(Uye,"A",{id:!0,class:!0,href:!0});var jpr=n(Rc);BO=s(jpr,"SPAN",{});var Opr=n(BO);f(TC.$$.fragment,Opr),Opr.forEach(t),jpr.forEach(t),o0e=i(Uye),xO=s(Uye,"SPAN",{});var Gpr=n(xO);r0e=r(Gpr,"Extending the Auto Classes"),Gpr.forEach(t),Uye.forEach(t),s5e=i(d),_s=s(d,"P",{});var zN=n(_s);t0e=r(zN,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),kO=s(zN,"CODE",{});var qpr=n(kO);a0e=r(qpr,"NewModel"),qpr.forEach(t),s0e=r(zN,", make sure you have a "),RO=s(zN,"CODE",{});var zpr=n(RO);n0e=r(zpr,"NewModelConfig"),zpr.forEach(t),l0e=r(zN,` then you can add those to the auto
classes like this:`),zN.forEach(t),n5e=i(d),f(FC.$$.fragment,d),l5e=i(d),y0=s(d,"P",{});var Xpr=n(y0);i0e=r(Xpr,"You will then be able to use the auto classes like you would usually do!"),Xpr.forEach(t),i5e=i(d),f(Pc.$$.fragment,d),d5e=i(d),Kl=s(d,"H2",{class:!0});var Jye=n(Kl);Sc=s(Jye,"A",{id:!0,class:!0,href:!0});var Qpr=n(Sc);PO=s(Qpr,"SPAN",{});var Vpr=n(PO);f(EC.$$.fragment,Vpr),Vpr.forEach(t),Qpr.forEach(t),d0e=i(Jye),SO=s(Jye,"SPAN",{});var Wpr=n(SO);c0e=r(Wpr,"AutoConfig"),Wpr.forEach(t),Jye.forEach(t),c5e=i(d),Do=s(d,"DIV",{class:!0});var mn=n(Do);f(CC.$$.fragment,mn),m0e=i(mn),MC=s(mn,"P",{});var Kye=n(MC);f0e=r(Kye,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),w0=s(Kye,"A",{href:!0});var Hpr=n(w0);h0e=r(Hpr,"from_pretrained()"),Hpr.forEach(t),g0e=r(Kye," class method."),Kye.forEach(t),u0e=i(mn),yC=s(mn,"P",{});var Yye=n(yC);p0e=r(Yye,"This class cannot be instantiated directly using "),$O=s(Yye,"CODE",{});var Upr=n($O);_0e=r(Upr,"__init__()"),Upr.forEach(t),b0e=r(Yye," (throws an error)."),Yye.forEach(t),v0e=i(mn),no=s(mn,"DIV",{class:!0});var qt=n(no);f(wC.$$.fragment,qt),T0e=i(qt),IO=s(qt,"P",{});var Jpr=n(IO);F0e=r(Jpr,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),Jpr.forEach(t),E0e=i(qt),Yl=s(qt,"P",{});var XN=n(Yl);C0e=r(XN,"The configuration class to instantiate is selected based on the "),DO=s(XN,"CODE",{});var Kpr=n(DO);M0e=r(Kpr,"model_type"),Kpr.forEach(t),y0e=r(XN,` property of the config object
that is loaded, or when it\u2019s missing, by falling back to using pattern matching on
`),NO=s(XN,"CODE",{});var Ypr=n(NO);w0e=r(Ypr,"pretrained_model_name_or_path"),Ypr.forEach(t),A0e=r(XN,":"),XN.forEach(t),L0e=i(qt),v=s(qt,"UL",{});var T=n(v);$c=s(T,"LI",{});var uue=n($c);jO=s(uue,"STRONG",{});var Zpr=n(jO);B0e=r(Zpr,"albert"),Zpr.forEach(t),x0e=r(uue," \u2014 "),A0=s(uue,"A",{href:!0});var e_r=n(A0);k0e=r(e_r,"AlbertConfig"),e_r.forEach(t),R0e=r(uue," (ALBERT model)"),uue.forEach(t),P0e=i(T),Ic=s(T,"LI",{});var pue=n(Ic);OO=s(pue,"STRONG",{});var o_r=n(OO);S0e=r(o_r,"bart"),o_r.forEach(t),$0e=r(pue," \u2014 "),L0=s(pue,"A",{href:!0});var r_r=n(L0);I0e=r(r_r,"BartConfig"),r_r.forEach(t),D0e=r(pue," (BART model)"),pue.forEach(t),N0e=i(T),Dc=s(T,"LI",{});var _ue=n(Dc);GO=s(_ue,"STRONG",{});var t_r=n(GO);j0e=r(t_r,"beit"),t_r.forEach(t),O0e=r(_ue," \u2014 "),B0=s(_ue,"A",{href:!0});var a_r=n(B0);G0e=r(a_r,"BeitConfig"),a_r.forEach(t),q0e=r(_ue," (BEiT model)"),_ue.forEach(t),z0e=i(T),Nc=s(T,"LI",{});var bue=n(Nc);qO=s(bue,"STRONG",{});var s_r=n(qO);X0e=r(s_r,"bert"),s_r.forEach(t),Q0e=r(bue," \u2014 "),x0=s(bue,"A",{href:!0});var n_r=n(x0);V0e=r(n_r,"BertConfig"),n_r.forEach(t),W0e=r(bue," (BERT model)"),bue.forEach(t),H0e=i(T),jc=s(T,"LI",{});var vue=n(jc);zO=s(vue,"STRONG",{});var l_r=n(zO);U0e=r(l_r,"bert-generation"),l_r.forEach(t),J0e=r(vue," \u2014 "),k0=s(vue,"A",{href:!0});var i_r=n(k0);K0e=r(i_r,"BertGenerationConfig"),i_r.forEach(t),Y0e=r(vue," (Bert Generation model)"),vue.forEach(t),Z0e=i(T),Oc=s(T,"LI",{});var Tue=n(Oc);XO=s(Tue,"STRONG",{});var d_r=n(XO);eAe=r(d_r,"big_bird"),d_r.forEach(t),oAe=r(Tue," \u2014 "),R0=s(Tue,"A",{href:!0});var c_r=n(R0);rAe=r(c_r,"BigBirdConfig"),c_r.forEach(t),tAe=r(Tue," (BigBird model)"),Tue.forEach(t),aAe=i(T),Gc=s(T,"LI",{});var Fue=n(Gc);QO=s(Fue,"STRONG",{});var m_r=n(QO);sAe=r(m_r,"bigbird_pegasus"),m_r.forEach(t),nAe=r(Fue," \u2014 "),P0=s(Fue,"A",{href:!0});var f_r=n(P0);lAe=r(f_r,"BigBirdPegasusConfig"),f_r.forEach(t),iAe=r(Fue," (BigBirdPegasus model)"),Fue.forEach(t),dAe=i(T),qc=s(T,"LI",{});var Eue=n(qc);VO=s(Eue,"STRONG",{});var h_r=n(VO);cAe=r(h_r,"blenderbot"),h_r.forEach(t),mAe=r(Eue," \u2014 "),S0=s(Eue,"A",{href:!0});var g_r=n(S0);fAe=r(g_r,"BlenderbotConfig"),g_r.forEach(t),hAe=r(Eue," (Blenderbot model)"),Eue.forEach(t),gAe=i(T),zc=s(T,"LI",{});var Cue=n(zc);WO=s(Cue,"STRONG",{});var u_r=n(WO);uAe=r(u_r,"blenderbot-small"),u_r.forEach(t),pAe=r(Cue," \u2014 "),$0=s(Cue,"A",{href:!0});var p_r=n($0);_Ae=r(p_r,"BlenderbotSmallConfig"),p_r.forEach(t),bAe=r(Cue," (BlenderbotSmall model)"),Cue.forEach(t),vAe=i(T),Xc=s(T,"LI",{});var Mue=n(Xc);HO=s(Mue,"STRONG",{});var __r=n(HO);TAe=r(__r,"camembert"),__r.forEach(t),FAe=r(Mue," \u2014 "),I0=s(Mue,"A",{href:!0});var b_r=n(I0);EAe=r(b_r,"CamembertConfig"),b_r.forEach(t),CAe=r(Mue," (CamemBERT model)"),Mue.forEach(t),MAe=i(T),Qc=s(T,"LI",{});var yue=n(Qc);UO=s(yue,"STRONG",{});var v_r=n(UO);yAe=r(v_r,"canine"),v_r.forEach(t),wAe=r(yue," \u2014 "),D0=s(yue,"A",{href:!0});var T_r=n(D0);AAe=r(T_r,"CanineConfig"),T_r.forEach(t),LAe=r(yue," (Canine model)"),yue.forEach(t),BAe=i(T),Vc=s(T,"LI",{});var wue=n(Vc);JO=s(wue,"STRONG",{});var F_r=n(JO);xAe=r(F_r,"clip"),F_r.forEach(t),kAe=r(wue," \u2014 "),N0=s(wue,"A",{href:!0});var E_r=n(N0);RAe=r(E_r,"CLIPConfig"),E_r.forEach(t),PAe=r(wue," (CLIP model)"),wue.forEach(t),SAe=i(T),Wc=s(T,"LI",{});var Aue=n(Wc);KO=s(Aue,"STRONG",{});var C_r=n(KO);$Ae=r(C_r,"convbert"),C_r.forEach(t),IAe=r(Aue," \u2014 "),j0=s(Aue,"A",{href:!0});var M_r=n(j0);DAe=r(M_r,"ConvBertConfig"),M_r.forEach(t),NAe=r(Aue," (ConvBERT model)"),Aue.forEach(t),jAe=i(T),Hc=s(T,"LI",{});var Lue=n(Hc);YO=s(Lue,"STRONG",{});var y_r=n(YO);OAe=r(y_r,"ctrl"),y_r.forEach(t),GAe=r(Lue," \u2014 "),O0=s(Lue,"A",{href:!0});var w_r=n(O0);qAe=r(w_r,"CTRLConfig"),w_r.forEach(t),zAe=r(Lue," (CTRL model)"),Lue.forEach(t),XAe=i(T),Uc=s(T,"LI",{});var Bue=n(Uc);ZO=s(Bue,"STRONG",{});var A_r=n(ZO);QAe=r(A_r,"deberta"),A_r.forEach(t),VAe=r(Bue," \u2014 "),G0=s(Bue,"A",{href:!0});var L_r=n(G0);WAe=r(L_r,"DebertaConfig"),L_r.forEach(t),HAe=r(Bue," (DeBERTa model)"),Bue.forEach(t),UAe=i(T),Jc=s(T,"LI",{});var xue=n(Jc);eG=s(xue,"STRONG",{});var B_r=n(eG);JAe=r(B_r,"deberta-v2"),B_r.forEach(t),KAe=r(xue," \u2014 "),q0=s(xue,"A",{href:!0});var x_r=n(q0);YAe=r(x_r,"DebertaV2Config"),x_r.forEach(t),ZAe=r(xue," (DeBERTa-v2 model)"),xue.forEach(t),e6e=i(T),Kc=s(T,"LI",{});var kue=n(Kc);oG=s(kue,"STRONG",{});var k_r=n(oG);o6e=r(k_r,"deit"),k_r.forEach(t),r6e=r(kue," \u2014 "),z0=s(kue,"A",{href:!0});var R_r=n(z0);t6e=r(R_r,"DeiTConfig"),R_r.forEach(t),a6e=r(kue," (DeiT model)"),kue.forEach(t),s6e=i(T),Yc=s(T,"LI",{});var Rue=n(Yc);rG=s(Rue,"STRONG",{});var P_r=n(rG);n6e=r(P_r,"detr"),P_r.forEach(t),l6e=r(Rue," \u2014 "),X0=s(Rue,"A",{href:!0});var S_r=n(X0);i6e=r(S_r,"DetrConfig"),S_r.forEach(t),d6e=r(Rue," (DETR model)"),Rue.forEach(t),c6e=i(T),Zc=s(T,"LI",{});var Pue=n(Zc);tG=s(Pue,"STRONG",{});var $_r=n(tG);m6e=r($_r,"distilbert"),$_r.forEach(t),f6e=r(Pue," \u2014 "),Q0=s(Pue,"A",{href:!0});var I_r=n(Q0);h6e=r(I_r,"DistilBertConfig"),I_r.forEach(t),g6e=r(Pue," (DistilBERT model)"),Pue.forEach(t),u6e=i(T),em=s(T,"LI",{});var Sue=n(em);aG=s(Sue,"STRONG",{});var D_r=n(aG);p6e=r(D_r,"dpr"),D_r.forEach(t),_6e=r(Sue," \u2014 "),V0=s(Sue,"A",{href:!0});var N_r=n(V0);b6e=r(N_r,"DPRConfig"),N_r.forEach(t),v6e=r(Sue," (DPR model)"),Sue.forEach(t),T6e=i(T),om=s(T,"LI",{});var $ue=n(om);sG=s($ue,"STRONG",{});var j_r=n(sG);F6e=r(j_r,"electra"),j_r.forEach(t),E6e=r($ue," \u2014 "),W0=s($ue,"A",{href:!0});var O_r=n(W0);C6e=r(O_r,"ElectraConfig"),O_r.forEach(t),M6e=r($ue," (ELECTRA model)"),$ue.forEach(t),y6e=i(T),rm=s(T,"LI",{});var Iue=n(rm);nG=s(Iue,"STRONG",{});var G_r=n(nG);w6e=r(G_r,"encoder-decoder"),G_r.forEach(t),A6e=r(Iue," \u2014 "),H0=s(Iue,"A",{href:!0});var q_r=n(H0);L6e=r(q_r,"EncoderDecoderConfig"),q_r.forEach(t),B6e=r(Iue," (Encoder decoder model)"),Iue.forEach(t),x6e=i(T),tm=s(T,"LI",{});var Due=n(tm);lG=s(Due,"STRONG",{});var z_r=n(lG);k6e=r(z_r,"flaubert"),z_r.forEach(t),R6e=r(Due," \u2014 "),U0=s(Due,"A",{href:!0});var X_r=n(U0);P6e=r(X_r,"FlaubertConfig"),X_r.forEach(t),S6e=r(Due," (FlauBERT model)"),Due.forEach(t),$6e=i(T),am=s(T,"LI",{});var Nue=n(am);iG=s(Nue,"STRONG",{});var Q_r=n(iG);I6e=r(Q_r,"fnet"),Q_r.forEach(t),D6e=r(Nue," \u2014 "),J0=s(Nue,"A",{href:!0});var V_r=n(J0);N6e=r(V_r,"FNetConfig"),V_r.forEach(t),j6e=r(Nue," (FNet model)"),Nue.forEach(t),O6e=i(T),sm=s(T,"LI",{});var jue=n(sm);dG=s(jue,"STRONG",{});var W_r=n(dG);G6e=r(W_r,"fsmt"),W_r.forEach(t),q6e=r(jue," \u2014 "),K0=s(jue,"A",{href:!0});var H_r=n(K0);z6e=r(H_r,"FSMTConfig"),H_r.forEach(t),X6e=r(jue," (FairSeq Machine-Translation model)"),jue.forEach(t),Q6e=i(T),nm=s(T,"LI",{});var Oue=n(nm);cG=s(Oue,"STRONG",{});var U_r=n(cG);V6e=r(U_r,"funnel"),U_r.forEach(t),W6e=r(Oue," \u2014 "),Y0=s(Oue,"A",{href:!0});var J_r=n(Y0);H6e=r(J_r,"FunnelConfig"),J_r.forEach(t),U6e=r(Oue," (Funnel Transformer model)"),Oue.forEach(t),J6e=i(T),lm=s(T,"LI",{});var Gue=n(lm);mG=s(Gue,"STRONG",{});var K_r=n(mG);K6e=r(K_r,"gpt2"),K_r.forEach(t),Y6e=r(Gue," \u2014 "),Z0=s(Gue,"A",{href:!0});var Y_r=n(Z0);Z6e=r(Y_r,"GPT2Config"),Y_r.forEach(t),eLe=r(Gue," (OpenAI GPT-2 model)"),Gue.forEach(t),oLe=i(T),im=s(T,"LI",{});var que=n(im);fG=s(que,"STRONG",{});var Z_r=n(fG);rLe=r(Z_r,"gpt_neo"),Z_r.forEach(t),tLe=r(que," \u2014 "),eA=s(que,"A",{href:!0});var ebr=n(eA);aLe=r(ebr,"GPTNeoConfig"),ebr.forEach(t),sLe=r(que," (GPT Neo model)"),que.forEach(t),nLe=i(T),dm=s(T,"LI",{});var zue=n(dm);hG=s(zue,"STRONG",{});var obr=n(hG);lLe=r(obr,"gptj"),obr.forEach(t),iLe=r(zue," \u2014 "),oA=s(zue,"A",{href:!0});var rbr=n(oA);dLe=r(rbr,"GPTJConfig"),rbr.forEach(t),cLe=r(zue," (GPT-J model)"),zue.forEach(t),mLe=i(T),cm=s(T,"LI",{});var Xue=n(cm);gG=s(Xue,"STRONG",{});var tbr=n(gG);fLe=r(tbr,"hubert"),tbr.forEach(t),hLe=r(Xue," \u2014 "),rA=s(Xue,"A",{href:!0});var abr=n(rA);gLe=r(abr,"HubertConfig"),abr.forEach(t),uLe=r(Xue," (Hubert model)"),Xue.forEach(t),pLe=i(T),mm=s(T,"LI",{});var Que=n(mm);uG=s(Que,"STRONG",{});var sbr=n(uG);_Le=r(sbr,"ibert"),sbr.forEach(t),bLe=r(Que," \u2014 "),tA=s(Que,"A",{href:!0});var nbr=n(tA);vLe=r(nbr,"IBertConfig"),nbr.forEach(t),TLe=r(Que," (I-BERT model)"),Que.forEach(t),FLe=i(T),fm=s(T,"LI",{});var Vue=n(fm);pG=s(Vue,"STRONG",{});var lbr=n(pG);ELe=r(lbr,"imagegpt"),lbr.forEach(t),CLe=r(Vue," \u2014 "),aA=s(Vue,"A",{href:!0});var ibr=n(aA);MLe=r(ibr,"ImageGPTConfig"),ibr.forEach(t),yLe=r(Vue," (ImageGPT model)"),Vue.forEach(t),wLe=i(T),hm=s(T,"LI",{});var Wue=n(hm);_G=s(Wue,"STRONG",{});var dbr=n(_G);ALe=r(dbr,"layoutlm"),dbr.forEach(t),LLe=r(Wue," \u2014 "),sA=s(Wue,"A",{href:!0});var cbr=n(sA);BLe=r(cbr,"LayoutLMConfig"),cbr.forEach(t),xLe=r(Wue," (LayoutLM model)"),Wue.forEach(t),kLe=i(T),gm=s(T,"LI",{});var Hue=n(gm);bG=s(Hue,"STRONG",{});var mbr=n(bG);RLe=r(mbr,"layoutlmv2"),mbr.forEach(t),PLe=r(Hue," \u2014 "),nA=s(Hue,"A",{href:!0});var fbr=n(nA);SLe=r(fbr,"LayoutLMv2Config"),fbr.forEach(t),$Le=r(Hue," (LayoutLMv2 model)"),Hue.forEach(t),ILe=i(T),um=s(T,"LI",{});var Uue=n(um);vG=s(Uue,"STRONG",{});var hbr=n(vG);DLe=r(hbr,"led"),hbr.forEach(t),NLe=r(Uue," \u2014 "),lA=s(Uue,"A",{href:!0});var gbr=n(lA);jLe=r(gbr,"LEDConfig"),gbr.forEach(t),OLe=r(Uue," (LED model)"),Uue.forEach(t),GLe=i(T),pm=s(T,"LI",{});var Jue=n(pm);TG=s(Jue,"STRONG",{});var ubr=n(TG);qLe=r(ubr,"longformer"),ubr.forEach(t),zLe=r(Jue," \u2014 "),iA=s(Jue,"A",{href:!0});var pbr=n(iA);XLe=r(pbr,"LongformerConfig"),pbr.forEach(t),QLe=r(Jue," (Longformer model)"),Jue.forEach(t),VLe=i(T),_m=s(T,"LI",{});var Kue=n(_m);FG=s(Kue,"STRONG",{});var _br=n(FG);WLe=r(_br,"luke"),_br.forEach(t),HLe=r(Kue," \u2014 "),dA=s(Kue,"A",{href:!0});var bbr=n(dA);ULe=r(bbr,"LukeConfig"),bbr.forEach(t),JLe=r(Kue," (LUKE model)"),Kue.forEach(t),KLe=i(T),bm=s(T,"LI",{});var Yue=n(bm);EG=s(Yue,"STRONG",{});var vbr=n(EG);YLe=r(vbr,"lxmert"),vbr.forEach(t),ZLe=r(Yue," \u2014 "),cA=s(Yue,"A",{href:!0});var Tbr=n(cA);e8e=r(Tbr,"LxmertConfig"),Tbr.forEach(t),o8e=r(Yue," (LXMERT model)"),Yue.forEach(t),r8e=i(T),vm=s(T,"LI",{});var Zue=n(vm);CG=s(Zue,"STRONG",{});var Fbr=n(CG);t8e=r(Fbr,"m2m_100"),Fbr.forEach(t),a8e=r(Zue," \u2014 "),mA=s(Zue,"A",{href:!0});var Ebr=n(mA);s8e=r(Ebr,"M2M100Config"),Ebr.forEach(t),n8e=r(Zue," (M2M100 model)"),Zue.forEach(t),l8e=i(T),Tm=s(T,"LI",{});var epe=n(Tm);MG=s(epe,"STRONG",{});var Cbr=n(MG);i8e=r(Cbr,"marian"),Cbr.forEach(t),d8e=r(epe," \u2014 "),fA=s(epe,"A",{href:!0});var Mbr=n(fA);c8e=r(Mbr,"MarianConfig"),Mbr.forEach(t),m8e=r(epe," (Marian model)"),epe.forEach(t),f8e=i(T),Fm=s(T,"LI",{});var ope=n(Fm);yG=s(ope,"STRONG",{});var ybr=n(yG);h8e=r(ybr,"mbart"),ybr.forEach(t),g8e=r(ope," \u2014 "),hA=s(ope,"A",{href:!0});var wbr=n(hA);u8e=r(wbr,"MBartConfig"),wbr.forEach(t),p8e=r(ope," (mBART model)"),ope.forEach(t),_8e=i(T),Em=s(T,"LI",{});var rpe=n(Em);wG=s(rpe,"STRONG",{});var Abr=n(wG);b8e=r(Abr,"megatron-bert"),Abr.forEach(t),v8e=r(rpe," \u2014 "),gA=s(rpe,"A",{href:!0});var Lbr=n(gA);T8e=r(Lbr,"MegatronBertConfig"),Lbr.forEach(t),F8e=r(rpe," (MegatronBert model)"),rpe.forEach(t),E8e=i(T),Cm=s(T,"LI",{});var tpe=n(Cm);AG=s(tpe,"STRONG",{});var Bbr=n(AG);C8e=r(Bbr,"mobilebert"),Bbr.forEach(t),M8e=r(tpe," \u2014 "),uA=s(tpe,"A",{href:!0});var xbr=n(uA);y8e=r(xbr,"MobileBertConfig"),xbr.forEach(t),w8e=r(tpe," (MobileBERT model)"),tpe.forEach(t),A8e=i(T),Mm=s(T,"LI",{});var ape=n(Mm);LG=s(ape,"STRONG",{});var kbr=n(LG);L8e=r(kbr,"mpnet"),kbr.forEach(t),B8e=r(ape," \u2014 "),pA=s(ape,"A",{href:!0});var Rbr=n(pA);x8e=r(Rbr,"MPNetConfig"),Rbr.forEach(t),k8e=r(ape," (MPNet model)"),ape.forEach(t),R8e=i(T),ym=s(T,"LI",{});var spe=n(ym);BG=s(spe,"STRONG",{});var Pbr=n(BG);P8e=r(Pbr,"mt5"),Pbr.forEach(t),S8e=r(spe," \u2014 "),_A=s(spe,"A",{href:!0});var Sbr=n(_A);$8e=r(Sbr,"MT5Config"),Sbr.forEach(t),I8e=r(spe," (mT5 model)"),spe.forEach(t),D8e=i(T),wm=s(T,"LI",{});var npe=n(wm);xG=s(npe,"STRONG",{});var $br=n(xG);N8e=r($br,"openai-gpt"),$br.forEach(t),j8e=r(npe," \u2014 "),bA=s(npe,"A",{href:!0});var Ibr=n(bA);O8e=r(Ibr,"OpenAIGPTConfig"),Ibr.forEach(t),G8e=r(npe," (OpenAI GPT model)"),npe.forEach(t),q8e=i(T),Am=s(T,"LI",{});var lpe=n(Am);kG=s(lpe,"STRONG",{});var Dbr=n(kG);z8e=r(Dbr,"pegasus"),Dbr.forEach(t),X8e=r(lpe," \u2014 "),vA=s(lpe,"A",{href:!0});var Nbr=n(vA);Q8e=r(Nbr,"PegasusConfig"),Nbr.forEach(t),V8e=r(lpe," (Pegasus model)"),lpe.forEach(t),W8e=i(T),Lm=s(T,"LI",{});var ipe=n(Lm);RG=s(ipe,"STRONG",{});var jbr=n(RG);H8e=r(jbr,"perceiver"),jbr.forEach(t),U8e=r(ipe," \u2014 "),TA=s(ipe,"A",{href:!0});var Obr=n(TA);J8e=r(Obr,"PerceiverConfig"),Obr.forEach(t),K8e=r(ipe," (Perceiver model)"),ipe.forEach(t),Y8e=i(T),Bm=s(T,"LI",{});var dpe=n(Bm);PG=s(dpe,"STRONG",{});var Gbr=n(PG);Z8e=r(Gbr,"prophetnet"),Gbr.forEach(t),eBe=r(dpe," \u2014 "),FA=s(dpe,"A",{href:!0});var qbr=n(FA);oBe=r(qbr,"ProphetNetConfig"),qbr.forEach(t),rBe=r(dpe," (ProphetNet model)"),dpe.forEach(t),tBe=i(T),xm=s(T,"LI",{});var cpe=n(xm);SG=s(cpe,"STRONG",{});var zbr=n(SG);aBe=r(zbr,"qdqbert"),zbr.forEach(t),sBe=r(cpe," \u2014 "),EA=s(cpe,"A",{href:!0});var Xbr=n(EA);nBe=r(Xbr,"QDQBertConfig"),Xbr.forEach(t),lBe=r(cpe," (QDQBert model)"),cpe.forEach(t),iBe=i(T),km=s(T,"LI",{});var mpe=n(km);$G=s(mpe,"STRONG",{});var Qbr=n($G);dBe=r(Qbr,"rag"),Qbr.forEach(t),cBe=r(mpe," \u2014 "),CA=s(mpe,"A",{href:!0});var Vbr=n(CA);mBe=r(Vbr,"RagConfig"),Vbr.forEach(t),fBe=r(mpe," (RAG model)"),mpe.forEach(t),hBe=i(T),Rm=s(T,"LI",{});var fpe=n(Rm);IG=s(fpe,"STRONG",{});var Wbr=n(IG);gBe=r(Wbr,"reformer"),Wbr.forEach(t),uBe=r(fpe," \u2014 "),MA=s(fpe,"A",{href:!0});var Hbr=n(MA);pBe=r(Hbr,"ReformerConfig"),Hbr.forEach(t),_Be=r(fpe," (Reformer model)"),fpe.forEach(t),bBe=i(T),Pm=s(T,"LI",{});var hpe=n(Pm);DG=s(hpe,"STRONG",{});var Ubr=n(DG);vBe=r(Ubr,"rembert"),Ubr.forEach(t),TBe=r(hpe," \u2014 "),yA=s(hpe,"A",{href:!0});var Jbr=n(yA);FBe=r(Jbr,"RemBertConfig"),Jbr.forEach(t),EBe=r(hpe," (RemBERT model)"),hpe.forEach(t),CBe=i(T),Sm=s(T,"LI",{});var gpe=n(Sm);NG=s(gpe,"STRONG",{});var Kbr=n(NG);MBe=r(Kbr,"retribert"),Kbr.forEach(t),yBe=r(gpe," \u2014 "),wA=s(gpe,"A",{href:!0});var Ybr=n(wA);wBe=r(Ybr,"RetriBertConfig"),Ybr.forEach(t),ABe=r(gpe," (RetriBERT model)"),gpe.forEach(t),LBe=i(T),$m=s(T,"LI",{});var upe=n($m);jG=s(upe,"STRONG",{});var Zbr=n(jG);BBe=r(Zbr,"roberta"),Zbr.forEach(t),xBe=r(upe," \u2014 "),AA=s(upe,"A",{href:!0});var e2r=n(AA);kBe=r(e2r,"RobertaConfig"),e2r.forEach(t),RBe=r(upe," (RoBERTa model)"),upe.forEach(t),PBe=i(T),Im=s(T,"LI",{});var ppe=n(Im);OG=s(ppe,"STRONG",{});var o2r=n(OG);SBe=r(o2r,"roformer"),o2r.forEach(t),$Be=r(ppe," \u2014 "),LA=s(ppe,"A",{href:!0});var r2r=n(LA);IBe=r(r2r,"RoFormerConfig"),r2r.forEach(t),DBe=r(ppe," (RoFormer model)"),ppe.forEach(t),NBe=i(T),Dm=s(T,"LI",{});var _pe=n(Dm);GG=s(_pe,"STRONG",{});var t2r=n(GG);jBe=r(t2r,"segformer"),t2r.forEach(t),OBe=r(_pe," \u2014 "),BA=s(_pe,"A",{href:!0});var a2r=n(BA);GBe=r(a2r,"SegformerConfig"),a2r.forEach(t),qBe=r(_pe," (SegFormer model)"),_pe.forEach(t),zBe=i(T),Nm=s(T,"LI",{});var bpe=n(Nm);qG=s(bpe,"STRONG",{});var s2r=n(qG);XBe=r(s2r,"sew"),s2r.forEach(t),QBe=r(bpe," \u2014 "),xA=s(bpe,"A",{href:!0});var n2r=n(xA);VBe=r(n2r,"SEWConfig"),n2r.forEach(t),WBe=r(bpe," (SEW model)"),bpe.forEach(t),HBe=i(T),jm=s(T,"LI",{});var vpe=n(jm);zG=s(vpe,"STRONG",{});var l2r=n(zG);UBe=r(l2r,"sew-d"),l2r.forEach(t),JBe=r(vpe," \u2014 "),kA=s(vpe,"A",{href:!0});var i2r=n(kA);KBe=r(i2r,"SEWDConfig"),i2r.forEach(t),YBe=r(vpe," (SEW-D model)"),vpe.forEach(t),ZBe=i(T),Om=s(T,"LI",{});var Tpe=n(Om);XG=s(Tpe,"STRONG",{});var d2r=n(XG);e9e=r(d2r,"speech-encoder-decoder"),d2r.forEach(t),o9e=r(Tpe," \u2014 "),RA=s(Tpe,"A",{href:!0});var c2r=n(RA);r9e=r(c2r,"SpeechEncoderDecoderConfig"),c2r.forEach(t),t9e=r(Tpe," (Speech Encoder decoder model)"),Tpe.forEach(t),a9e=i(T),Gm=s(T,"LI",{});var Fpe=n(Gm);QG=s(Fpe,"STRONG",{});var m2r=n(QG);s9e=r(m2r,"speech_to_text"),m2r.forEach(t),n9e=r(Fpe," \u2014 "),PA=s(Fpe,"A",{href:!0});var f2r=n(PA);l9e=r(f2r,"Speech2TextConfig"),f2r.forEach(t),i9e=r(Fpe," (Speech2Text model)"),Fpe.forEach(t),d9e=i(T),qm=s(T,"LI",{});var Epe=n(qm);VG=s(Epe,"STRONG",{});var h2r=n(VG);c9e=r(h2r,"speech_to_text_2"),h2r.forEach(t),m9e=r(Epe," \u2014 "),SA=s(Epe,"A",{href:!0});var g2r=n(SA);f9e=r(g2r,"Speech2Text2Config"),g2r.forEach(t),h9e=r(Epe," (Speech2Text2 model)"),Epe.forEach(t),g9e=i(T),zm=s(T,"LI",{});var Cpe=n(zm);WG=s(Cpe,"STRONG",{});var u2r=n(WG);u9e=r(u2r,"splinter"),u2r.forEach(t),p9e=r(Cpe," \u2014 "),$A=s(Cpe,"A",{href:!0});var p2r=n($A);_9e=r(p2r,"SplinterConfig"),p2r.forEach(t),b9e=r(Cpe," (Splinter model)"),Cpe.forEach(t),v9e=i(T),Xm=s(T,"LI",{});var Mpe=n(Xm);HG=s(Mpe,"STRONG",{});var _2r=n(HG);T9e=r(_2r,"squeezebert"),_2r.forEach(t),F9e=r(Mpe," \u2014 "),IA=s(Mpe,"A",{href:!0});var b2r=n(IA);E9e=r(b2r,"SqueezeBertConfig"),b2r.forEach(t),C9e=r(Mpe," (SqueezeBERT model)"),Mpe.forEach(t),M9e=i(T),Qm=s(T,"LI",{});var ype=n(Qm);UG=s(ype,"STRONG",{});var v2r=n(UG);y9e=r(v2r,"t5"),v2r.forEach(t),w9e=r(ype," \u2014 "),DA=s(ype,"A",{href:!0});var T2r=n(DA);A9e=r(T2r,"T5Config"),T2r.forEach(t),L9e=r(ype," (T5 model)"),ype.forEach(t),B9e=i(T),Vm=s(T,"LI",{});var wpe=n(Vm);JG=s(wpe,"STRONG",{});var F2r=n(JG);x9e=r(F2r,"tapas"),F2r.forEach(t),k9e=r(wpe," \u2014 "),NA=s(wpe,"A",{href:!0});var E2r=n(NA);R9e=r(E2r,"TapasConfig"),E2r.forEach(t),P9e=r(wpe," (TAPAS model)"),wpe.forEach(t),S9e=i(T),Wm=s(T,"LI",{});var Ape=n(Wm);KG=s(Ape,"STRONG",{});var C2r=n(KG);$9e=r(C2r,"transfo-xl"),C2r.forEach(t),I9e=r(Ape," \u2014 "),jA=s(Ape,"A",{href:!0});var M2r=n(jA);D9e=r(M2r,"TransfoXLConfig"),M2r.forEach(t),N9e=r(Ape," (Transformer-XL model)"),Ape.forEach(t),j9e=i(T),Hm=s(T,"LI",{});var Lpe=n(Hm);YG=s(Lpe,"STRONG",{});var y2r=n(YG);O9e=r(y2r,"trocr"),y2r.forEach(t),G9e=r(Lpe," \u2014 "),OA=s(Lpe,"A",{href:!0});var w2r=n(OA);q9e=r(w2r,"TrOCRConfig"),w2r.forEach(t),z9e=r(Lpe," (TrOCR model)"),Lpe.forEach(t),X9e=i(T),Um=s(T,"LI",{});var Bpe=n(Um);ZG=s(Bpe,"STRONG",{});var A2r=n(ZG);Q9e=r(A2r,"unispeech"),A2r.forEach(t),V9e=r(Bpe," \u2014 "),GA=s(Bpe,"A",{href:!0});var L2r=n(GA);W9e=r(L2r,"UniSpeechConfig"),L2r.forEach(t),H9e=r(Bpe," (UniSpeech model)"),Bpe.forEach(t),U9e=i(T),Jm=s(T,"LI",{});var xpe=n(Jm);eq=s(xpe,"STRONG",{});var B2r=n(eq);J9e=r(B2r,"unispeech-sat"),B2r.forEach(t),K9e=r(xpe," \u2014 "),qA=s(xpe,"A",{href:!0});var x2r=n(qA);Y9e=r(x2r,"UniSpeechSatConfig"),x2r.forEach(t),Z9e=r(xpe," (UniSpeechSat model)"),xpe.forEach(t),exe=i(T),Km=s(T,"LI",{});var kpe=n(Km);oq=s(kpe,"STRONG",{});var k2r=n(oq);oxe=r(k2r,"vision-encoder-decoder"),k2r.forEach(t),rxe=r(kpe," \u2014 "),zA=s(kpe,"A",{href:!0});var R2r=n(zA);txe=r(R2r,"VisionEncoderDecoderConfig"),R2r.forEach(t),axe=r(kpe," (Vision Encoder decoder model)"),kpe.forEach(t),sxe=i(T),Ym=s(T,"LI",{});var Rpe=n(Ym);rq=s(Rpe,"STRONG",{});var P2r=n(rq);nxe=r(P2r,"vision-text-dual-encoder"),P2r.forEach(t),lxe=r(Rpe," \u2014 "),XA=s(Rpe,"A",{href:!0});var S2r=n(XA);ixe=r(S2r,"VisionTextDualEncoderConfig"),S2r.forEach(t),dxe=r(Rpe," (VisionTextDualEncoder model)"),Rpe.forEach(t),cxe=i(T),Zm=s(T,"LI",{});var Ppe=n(Zm);tq=s(Ppe,"STRONG",{});var $2r=n(tq);mxe=r($2r,"visual_bert"),$2r.forEach(t),fxe=r(Ppe," \u2014 "),QA=s(Ppe,"A",{href:!0});var I2r=n(QA);hxe=r(I2r,"VisualBertConfig"),I2r.forEach(t),gxe=r(Ppe," (VisualBert model)"),Ppe.forEach(t),uxe=i(T),ef=s(T,"LI",{});var Spe=n(ef);aq=s(Spe,"STRONG",{});var D2r=n(aq);pxe=r(D2r,"vit"),D2r.forEach(t),_xe=r(Spe," \u2014 "),VA=s(Spe,"A",{href:!0});var N2r=n(VA);bxe=r(N2r,"ViTConfig"),N2r.forEach(t),vxe=r(Spe," (ViT model)"),Spe.forEach(t),Txe=i(T),of=s(T,"LI",{});var $pe=n(of);sq=s($pe,"STRONG",{});var j2r=n(sq);Fxe=r(j2r,"wav2vec2"),j2r.forEach(t),Exe=r($pe," \u2014 "),WA=s($pe,"A",{href:!0});var O2r=n(WA);Cxe=r(O2r,"Wav2Vec2Config"),O2r.forEach(t),Mxe=r($pe," (Wav2Vec2 model)"),$pe.forEach(t),yxe=i(T),rf=s(T,"LI",{});var Ipe=n(rf);nq=s(Ipe,"STRONG",{});var G2r=n(nq);wxe=r(G2r,"xlm"),G2r.forEach(t),Axe=r(Ipe," \u2014 "),HA=s(Ipe,"A",{href:!0});var q2r=n(HA);Lxe=r(q2r,"XLMConfig"),q2r.forEach(t),Bxe=r(Ipe," (XLM model)"),Ipe.forEach(t),xxe=i(T),tf=s(T,"LI",{});var Dpe=n(tf);lq=s(Dpe,"STRONG",{});var z2r=n(lq);kxe=r(z2r,"xlm-prophetnet"),z2r.forEach(t),Rxe=r(Dpe," \u2014 "),UA=s(Dpe,"A",{href:!0});var X2r=n(UA);Pxe=r(X2r,"XLMProphetNetConfig"),X2r.forEach(t),Sxe=r(Dpe," (XLMProphetNet model)"),Dpe.forEach(t),$xe=i(T),af=s(T,"LI",{});var Npe=n(af);iq=s(Npe,"STRONG",{});var Q2r=n(iq);Ixe=r(Q2r,"xlm-roberta"),Q2r.forEach(t),Dxe=r(Npe," \u2014 "),JA=s(Npe,"A",{href:!0});var V2r=n(JA);Nxe=r(V2r,"XLMRobertaConfig"),V2r.forEach(t),jxe=r(Npe," (XLM-RoBERTa model)"),Npe.forEach(t),Oxe=i(T),sf=s(T,"LI",{});var jpe=n(sf);dq=s(jpe,"STRONG",{});var W2r=n(dq);Gxe=r(W2r,"xlnet"),W2r.forEach(t),qxe=r(jpe," \u2014 "),KA=s(jpe,"A",{href:!0});var H2r=n(KA);zxe=r(H2r,"XLNetConfig"),H2r.forEach(t),Xxe=r(jpe," (XLNet model)"),jpe.forEach(t),T.forEach(t),Qxe=i(qt),cq=s(qt,"P",{});var U2r=n(cq);Vxe=r(U2r,"Examples:"),U2r.forEach(t),Wxe=i(qt),f(AC.$$.fragment,qt),qt.forEach(t),Hxe=i(mn),nf=s(mn,"DIV",{class:!0});var Zye=n(nf);f(LC.$$.fragment,Zye),Uxe=i(Zye),mq=s(Zye,"P",{});var J2r=n(mq);Jxe=r(J2r,"Register a new configuration for this class."),J2r.forEach(t),Zye.forEach(t),mn.forEach(t),m5e=i(d),Zl=s(d,"H2",{class:!0});var ewe=n(Zl);lf=s(ewe,"A",{id:!0,class:!0,href:!0});var K2r=n(lf);fq=s(K2r,"SPAN",{});var Y2r=n(fq);f(BC.$$.fragment,Y2r),Y2r.forEach(t),K2r.forEach(t),Kxe=i(ewe),hq=s(ewe,"SPAN",{});var Z2r=n(hq);Yxe=r(Z2r,"AutoTokenizer"),Z2r.forEach(t),ewe.forEach(t),f5e=i(d),No=s(d,"DIV",{class:!0});var fn=n(No);f(xC.$$.fragment,fn),Zxe=i(fn),kC=s(fn,"P",{});var owe=n(kC);eke=r(owe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),YA=s(owe,"A",{href:!0});var evr=n(YA);oke=r(evr,"AutoTokenizer.from_pretrained()"),evr.forEach(t),rke=r(owe," class method."),owe.forEach(t),tke=i(fn),RC=s(fn,"P",{});var rwe=n(RC);ake=r(rwe,"This class cannot be instantiated directly using "),gq=s(rwe,"CODE",{});var ovr=n(gq);ske=r(ovr,"__init__()"),ovr.forEach(t),nke=r(rwe," (throws an error)."),rwe.forEach(t),lke=i(fn),ye=s(fn,"DIV",{class:!0});var ko=n(ye);f(PC.$$.fragment,ko),ike=i(ko),uq=s(ko,"P",{});var rvr=n(uq);dke=r(rvr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),rvr.forEach(t),cke=i(ko),va=s(ko,"P",{});var T4=n(va);mke=r(T4,"The tokenizer class to instantiate is selected based on the "),pq=s(T4,"CODE",{});var tvr=n(pq);fke=r(tvr,"model_type"),tvr.forEach(t),hke=r(T4,` property of the config object
(either passed as an argument or loaded from `),_q=s(T4,"CODE",{});var avr=n(_q);gke=r(avr,"pretrained_model_name_or_path"),avr.forEach(t),uke=r(T4,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),bq=s(T4,"CODE",{});var svr=n(bq);pke=r(svr,"pretrained_model_name_or_path"),svr.forEach(t),_ke=r(T4,":"),T4.forEach(t),bke=i(ko),C=s(ko,"UL",{});var M=n(C);bs=s(M,"LI",{});var L7=n(bs);vq=s(L7,"STRONG",{});var nvr=n(vq);vke=r(nvr,"albert"),nvr.forEach(t),Tke=r(L7," \u2014 "),ZA=s(L7,"A",{href:!0});var lvr=n(ZA);Fke=r(lvr,"AlbertTokenizer"),lvr.forEach(t),Eke=r(L7," or "),e6=s(L7,"A",{href:!0});var ivr=n(e6);Cke=r(ivr,"AlbertTokenizerFast"),ivr.forEach(t),Mke=r(L7," (ALBERT model)"),L7.forEach(t),yke=i(M),vs=s(M,"LI",{});var B7=n(vs);Tq=s(B7,"STRONG",{});var dvr=n(Tq);wke=r(dvr,"bart"),dvr.forEach(t),Ake=r(B7," \u2014 "),o6=s(B7,"A",{href:!0});var cvr=n(o6);Lke=r(cvr,"BartTokenizer"),cvr.forEach(t),Bke=r(B7," or "),r6=s(B7,"A",{href:!0});var mvr=n(r6);xke=r(mvr,"BartTokenizerFast"),mvr.forEach(t),kke=r(B7," (BART model)"),B7.forEach(t),Rke=i(M),Ts=s(M,"LI",{});var x7=n(Ts);Fq=s(x7,"STRONG",{});var fvr=n(Fq);Pke=r(fvr,"barthez"),fvr.forEach(t),Ske=r(x7," \u2014 "),t6=s(x7,"A",{href:!0});var hvr=n(t6);$ke=r(hvr,"BarthezTokenizer"),hvr.forEach(t),Ike=r(x7," or "),a6=s(x7,"A",{href:!0});var gvr=n(a6);Dke=r(gvr,"BarthezTokenizerFast"),gvr.forEach(t),Nke=r(x7," (BARThez model)"),x7.forEach(t),jke=i(M),df=s(M,"LI",{});var Ope=n(df);Eq=s(Ope,"STRONG",{});var uvr=n(Eq);Oke=r(uvr,"bartpho"),uvr.forEach(t),Gke=r(Ope," \u2014 "),s6=s(Ope,"A",{href:!0});var pvr=n(s6);qke=r(pvr,"BartphoTokenizer"),pvr.forEach(t),zke=r(Ope," (BARTpho model)"),Ope.forEach(t),Xke=i(M),Fs=s(M,"LI",{});var k7=n(Fs);Cq=s(k7,"STRONG",{});var _vr=n(Cq);Qke=r(_vr,"bert"),_vr.forEach(t),Vke=r(k7," \u2014 "),n6=s(k7,"A",{href:!0});var bvr=n(n6);Wke=r(bvr,"BertTokenizer"),bvr.forEach(t),Hke=r(k7," or "),l6=s(k7,"A",{href:!0});var vvr=n(l6);Uke=r(vvr,"BertTokenizerFast"),vvr.forEach(t),Jke=r(k7," (BERT model)"),k7.forEach(t),Kke=i(M),cf=s(M,"LI",{});var Gpe=n(cf);Mq=s(Gpe,"STRONG",{});var Tvr=n(Mq);Yke=r(Tvr,"bert-generation"),Tvr.forEach(t),Zke=r(Gpe," \u2014 "),i6=s(Gpe,"A",{href:!0});var Fvr=n(i6);eRe=r(Fvr,"BertGenerationTokenizer"),Fvr.forEach(t),oRe=r(Gpe," (Bert Generation model)"),Gpe.forEach(t),rRe=i(M),mf=s(M,"LI",{});var qpe=n(mf);yq=s(qpe,"STRONG",{});var Evr=n(yq);tRe=r(Evr,"bert-japanese"),Evr.forEach(t),aRe=r(qpe," \u2014 "),d6=s(qpe,"A",{href:!0});var Cvr=n(d6);sRe=r(Cvr,"BertJapaneseTokenizer"),Cvr.forEach(t),nRe=r(qpe," (BertJapanese model)"),qpe.forEach(t),lRe=i(M),ff=s(M,"LI",{});var zpe=n(ff);wq=s(zpe,"STRONG",{});var Mvr=n(wq);iRe=r(Mvr,"bertweet"),Mvr.forEach(t),dRe=r(zpe," \u2014 "),c6=s(zpe,"A",{href:!0});var yvr=n(c6);cRe=r(yvr,"BertweetTokenizer"),yvr.forEach(t),mRe=r(zpe," (Bertweet model)"),zpe.forEach(t),fRe=i(M),Es=s(M,"LI",{});var R7=n(Es);Aq=s(R7,"STRONG",{});var wvr=n(Aq);hRe=r(wvr,"big_bird"),wvr.forEach(t),gRe=r(R7," \u2014 "),m6=s(R7,"A",{href:!0});var Avr=n(m6);uRe=r(Avr,"BigBirdTokenizer"),Avr.forEach(t),pRe=r(R7," or "),f6=s(R7,"A",{href:!0});var Lvr=n(f6);_Re=r(Lvr,"BigBirdTokenizerFast"),Lvr.forEach(t),bRe=r(R7," (BigBird model)"),R7.forEach(t),vRe=i(M),Cs=s(M,"LI",{});var P7=n(Cs);Lq=s(P7,"STRONG",{});var Bvr=n(Lq);TRe=r(Bvr,"bigbird_pegasus"),Bvr.forEach(t),FRe=r(P7," \u2014 "),h6=s(P7,"A",{href:!0});var xvr=n(h6);ERe=r(xvr,"PegasusTokenizer"),xvr.forEach(t),CRe=r(P7," or "),g6=s(P7,"A",{href:!0});var kvr=n(g6);MRe=r(kvr,"PegasusTokenizerFast"),kvr.forEach(t),yRe=r(P7," (BigBirdPegasus model)"),P7.forEach(t),wRe=i(M),Ms=s(M,"LI",{});var S7=n(Ms);Bq=s(S7,"STRONG",{});var Rvr=n(Bq);ARe=r(Rvr,"blenderbot"),Rvr.forEach(t),LRe=r(S7," \u2014 "),u6=s(S7,"A",{href:!0});var Pvr=n(u6);BRe=r(Pvr,"BlenderbotTokenizer"),Pvr.forEach(t),xRe=r(S7," or "),p6=s(S7,"A",{href:!0});var Svr=n(p6);kRe=r(Svr,"BlenderbotTokenizerFast"),Svr.forEach(t),RRe=r(S7," (Blenderbot model)"),S7.forEach(t),PRe=i(M),hf=s(M,"LI",{});var Xpe=n(hf);xq=s(Xpe,"STRONG",{});var $vr=n(xq);SRe=r($vr,"blenderbot-small"),$vr.forEach(t),$Re=r(Xpe," \u2014 "),_6=s(Xpe,"A",{href:!0});var Ivr=n(_6);IRe=r(Ivr,"BlenderbotSmallTokenizer"),Ivr.forEach(t),DRe=r(Xpe," (BlenderbotSmall model)"),Xpe.forEach(t),NRe=i(M),gf=s(M,"LI",{});var Qpe=n(gf);kq=s(Qpe,"STRONG",{});var Dvr=n(kq);jRe=r(Dvr,"byt5"),Dvr.forEach(t),ORe=r(Qpe," \u2014 "),b6=s(Qpe,"A",{href:!0});var Nvr=n(b6);GRe=r(Nvr,"ByT5Tokenizer"),Nvr.forEach(t),qRe=r(Qpe," (ByT5 model)"),Qpe.forEach(t),zRe=i(M),ys=s(M,"LI",{});var $7=n(ys);Rq=s($7,"STRONG",{});var jvr=n(Rq);XRe=r(jvr,"camembert"),jvr.forEach(t),QRe=r($7," \u2014 "),v6=s($7,"A",{href:!0});var Ovr=n(v6);VRe=r(Ovr,"CamembertTokenizer"),Ovr.forEach(t),WRe=r($7," or "),T6=s($7,"A",{href:!0});var Gvr=n(T6);HRe=r(Gvr,"CamembertTokenizerFast"),Gvr.forEach(t),URe=r($7," (CamemBERT model)"),$7.forEach(t),JRe=i(M),uf=s(M,"LI",{});var Vpe=n(uf);Pq=s(Vpe,"STRONG",{});var qvr=n(Pq);KRe=r(qvr,"canine"),qvr.forEach(t),YRe=r(Vpe," \u2014 "),F6=s(Vpe,"A",{href:!0});var zvr=n(F6);ZRe=r(zvr,"CanineTokenizer"),zvr.forEach(t),ePe=r(Vpe," (Canine model)"),Vpe.forEach(t),oPe=i(M),ws=s(M,"LI",{});var I7=n(ws);Sq=s(I7,"STRONG",{});var Xvr=n(Sq);rPe=r(Xvr,"clip"),Xvr.forEach(t),tPe=r(I7," \u2014 "),E6=s(I7,"A",{href:!0});var Qvr=n(E6);aPe=r(Qvr,"CLIPTokenizer"),Qvr.forEach(t),sPe=r(I7," or "),C6=s(I7,"A",{href:!0});var Vvr=n(C6);nPe=r(Vvr,"CLIPTokenizerFast"),Vvr.forEach(t),lPe=r(I7," (CLIP model)"),I7.forEach(t),iPe=i(M),As=s(M,"LI",{});var D7=n(As);$q=s(D7,"STRONG",{});var Wvr=n($q);dPe=r(Wvr,"convbert"),Wvr.forEach(t),cPe=r(D7," \u2014 "),M6=s(D7,"A",{href:!0});var Hvr=n(M6);mPe=r(Hvr,"ConvBertTokenizer"),Hvr.forEach(t),fPe=r(D7," or "),y6=s(D7,"A",{href:!0});var Uvr=n(y6);hPe=r(Uvr,"ConvBertTokenizerFast"),Uvr.forEach(t),gPe=r(D7," (ConvBERT model)"),D7.forEach(t),uPe=i(M),Ls=s(M,"LI",{});var N7=n(Ls);Iq=s(N7,"STRONG",{});var Jvr=n(Iq);pPe=r(Jvr,"cpm"),Jvr.forEach(t),_Pe=r(N7," \u2014 "),w6=s(N7,"A",{href:!0});var Kvr=n(w6);bPe=r(Kvr,"CpmTokenizer"),Kvr.forEach(t),vPe=r(N7," or "),Dq=s(N7,"CODE",{});var Yvr=n(Dq);TPe=r(Yvr,"CpmTokenizerFast"),Yvr.forEach(t),FPe=r(N7," (CPM model)"),N7.forEach(t),EPe=i(M),pf=s(M,"LI",{});var Wpe=n(pf);Nq=s(Wpe,"STRONG",{});var Zvr=n(Nq);CPe=r(Zvr,"ctrl"),Zvr.forEach(t),MPe=r(Wpe," \u2014 "),A6=s(Wpe,"A",{href:!0});var eTr=n(A6);yPe=r(eTr,"CTRLTokenizer"),eTr.forEach(t),wPe=r(Wpe," (CTRL model)"),Wpe.forEach(t),APe=i(M),Bs=s(M,"LI",{});var j7=n(Bs);jq=s(j7,"STRONG",{});var oTr=n(jq);LPe=r(oTr,"deberta"),oTr.forEach(t),BPe=r(j7," \u2014 "),L6=s(j7,"A",{href:!0});var rTr=n(L6);xPe=r(rTr,"DebertaTokenizer"),rTr.forEach(t),kPe=r(j7," or "),B6=s(j7,"A",{href:!0});var tTr=n(B6);RPe=r(tTr,"DebertaTokenizerFast"),tTr.forEach(t),PPe=r(j7," (DeBERTa model)"),j7.forEach(t),SPe=i(M),_f=s(M,"LI",{});var Hpe=n(_f);Oq=s(Hpe,"STRONG",{});var aTr=n(Oq);$Pe=r(aTr,"deberta-v2"),aTr.forEach(t),IPe=r(Hpe," \u2014 "),x6=s(Hpe,"A",{href:!0});var sTr=n(x6);DPe=r(sTr,"DebertaV2Tokenizer"),sTr.forEach(t),NPe=r(Hpe," (DeBERTa-v2 model)"),Hpe.forEach(t),jPe=i(M),xs=s(M,"LI",{});var O7=n(xs);Gq=s(O7,"STRONG",{});var nTr=n(Gq);OPe=r(nTr,"distilbert"),nTr.forEach(t),GPe=r(O7," \u2014 "),k6=s(O7,"A",{href:!0});var lTr=n(k6);qPe=r(lTr,"DistilBertTokenizer"),lTr.forEach(t),zPe=r(O7," or "),R6=s(O7,"A",{href:!0});var iTr=n(R6);XPe=r(iTr,"DistilBertTokenizerFast"),iTr.forEach(t),QPe=r(O7," (DistilBERT model)"),O7.forEach(t),VPe=i(M),ks=s(M,"LI",{});var G7=n(ks);qq=s(G7,"STRONG",{});var dTr=n(qq);WPe=r(dTr,"dpr"),dTr.forEach(t),HPe=r(G7," \u2014 "),P6=s(G7,"A",{href:!0});var cTr=n(P6);UPe=r(cTr,"DPRQuestionEncoderTokenizer"),cTr.forEach(t),JPe=r(G7," or "),S6=s(G7,"A",{href:!0});var mTr=n(S6);KPe=r(mTr,"DPRQuestionEncoderTokenizerFast"),mTr.forEach(t),YPe=r(G7," (DPR model)"),G7.forEach(t),ZPe=i(M),Rs=s(M,"LI",{});var q7=n(Rs);zq=s(q7,"STRONG",{});var fTr=n(zq);eSe=r(fTr,"electra"),fTr.forEach(t),oSe=r(q7," \u2014 "),$6=s(q7,"A",{href:!0});var hTr=n($6);rSe=r(hTr,"ElectraTokenizer"),hTr.forEach(t),tSe=r(q7," or "),I6=s(q7,"A",{href:!0});var gTr=n(I6);aSe=r(gTr,"ElectraTokenizerFast"),gTr.forEach(t),sSe=r(q7," (ELECTRA model)"),q7.forEach(t),nSe=i(M),bf=s(M,"LI",{});var Upe=n(bf);Xq=s(Upe,"STRONG",{});var uTr=n(Xq);lSe=r(uTr,"flaubert"),uTr.forEach(t),iSe=r(Upe," \u2014 "),D6=s(Upe,"A",{href:!0});var pTr=n(D6);dSe=r(pTr,"FlaubertTokenizer"),pTr.forEach(t),cSe=r(Upe," (FlauBERT model)"),Upe.forEach(t),mSe=i(M),Ps=s(M,"LI",{});var z7=n(Ps);Qq=s(z7,"STRONG",{});var _Tr=n(Qq);fSe=r(_Tr,"fnet"),_Tr.forEach(t),hSe=r(z7," \u2014 "),N6=s(z7,"A",{href:!0});var bTr=n(N6);gSe=r(bTr,"FNetTokenizer"),bTr.forEach(t),uSe=r(z7," or "),j6=s(z7,"A",{href:!0});var vTr=n(j6);pSe=r(vTr,"FNetTokenizerFast"),vTr.forEach(t),_Se=r(z7," (FNet model)"),z7.forEach(t),bSe=i(M),vf=s(M,"LI",{});var Jpe=n(vf);Vq=s(Jpe,"STRONG",{});var TTr=n(Vq);vSe=r(TTr,"fsmt"),TTr.forEach(t),TSe=r(Jpe," \u2014 "),O6=s(Jpe,"A",{href:!0});var FTr=n(O6);FSe=r(FTr,"FSMTTokenizer"),FTr.forEach(t),ESe=r(Jpe," (FairSeq Machine-Translation model)"),Jpe.forEach(t),CSe=i(M),Ss=s(M,"LI",{});var X7=n(Ss);Wq=s(X7,"STRONG",{});var ETr=n(Wq);MSe=r(ETr,"funnel"),ETr.forEach(t),ySe=r(X7," \u2014 "),G6=s(X7,"A",{href:!0});var CTr=n(G6);wSe=r(CTr,"FunnelTokenizer"),CTr.forEach(t),ASe=r(X7," or "),q6=s(X7,"A",{href:!0});var MTr=n(q6);LSe=r(MTr,"FunnelTokenizerFast"),MTr.forEach(t),BSe=r(X7," (Funnel Transformer model)"),X7.forEach(t),xSe=i(M),$s=s(M,"LI",{});var Q7=n($s);Hq=s(Q7,"STRONG",{});var yTr=n(Hq);kSe=r(yTr,"gpt2"),yTr.forEach(t),RSe=r(Q7," \u2014 "),z6=s(Q7,"A",{href:!0});var wTr=n(z6);PSe=r(wTr,"GPT2Tokenizer"),wTr.forEach(t),SSe=r(Q7," or "),X6=s(Q7,"A",{href:!0});var ATr=n(X6);$Se=r(ATr,"GPT2TokenizerFast"),ATr.forEach(t),ISe=r(Q7," (OpenAI GPT-2 model)"),Q7.forEach(t),DSe=i(M),Is=s(M,"LI",{});var V7=n(Is);Uq=s(V7,"STRONG",{});var LTr=n(Uq);NSe=r(LTr,"gpt_neo"),LTr.forEach(t),jSe=r(V7," \u2014 "),Q6=s(V7,"A",{href:!0});var BTr=n(Q6);OSe=r(BTr,"GPT2Tokenizer"),BTr.forEach(t),GSe=r(V7," or "),V6=s(V7,"A",{href:!0});var xTr=n(V6);qSe=r(xTr,"GPT2TokenizerFast"),xTr.forEach(t),zSe=r(V7," (GPT Neo model)"),V7.forEach(t),XSe=i(M),Tf=s(M,"LI",{});var Kpe=n(Tf);Jq=s(Kpe,"STRONG",{});var kTr=n(Jq);QSe=r(kTr,"hubert"),kTr.forEach(t),VSe=r(Kpe," \u2014 "),W6=s(Kpe,"A",{href:!0});var RTr=n(W6);WSe=r(RTr,"Wav2Vec2CTCTokenizer"),RTr.forEach(t),HSe=r(Kpe," (Hubert model)"),Kpe.forEach(t),USe=i(M),Ds=s(M,"LI",{});var W7=n(Ds);Kq=s(W7,"STRONG",{});var PTr=n(Kq);JSe=r(PTr,"ibert"),PTr.forEach(t),KSe=r(W7," \u2014 "),H6=s(W7,"A",{href:!0});var STr=n(H6);YSe=r(STr,"RobertaTokenizer"),STr.forEach(t),ZSe=r(W7," or "),U6=s(W7,"A",{href:!0});var $Tr=n(U6);e$e=r($Tr,"RobertaTokenizerFast"),$Tr.forEach(t),o$e=r(W7," (I-BERT model)"),W7.forEach(t),r$e=i(M),Ns=s(M,"LI",{});var H7=n(Ns);Yq=s(H7,"STRONG",{});var ITr=n(Yq);t$e=r(ITr,"layoutlm"),ITr.forEach(t),a$e=r(H7," \u2014 "),J6=s(H7,"A",{href:!0});var DTr=n(J6);s$e=r(DTr,"LayoutLMTokenizer"),DTr.forEach(t),n$e=r(H7," or "),K6=s(H7,"A",{href:!0});var NTr=n(K6);l$e=r(NTr,"LayoutLMTokenizerFast"),NTr.forEach(t),i$e=r(H7," (LayoutLM model)"),H7.forEach(t),d$e=i(M),js=s(M,"LI",{});var U7=n(js);Zq=s(U7,"STRONG",{});var jTr=n(Zq);c$e=r(jTr,"layoutlmv2"),jTr.forEach(t),m$e=r(U7," \u2014 "),Y6=s(U7,"A",{href:!0});var OTr=n(Y6);f$e=r(OTr,"LayoutLMv2Tokenizer"),OTr.forEach(t),h$e=r(U7," or "),Z6=s(U7,"A",{href:!0});var GTr=n(Z6);g$e=r(GTr,"LayoutLMv2TokenizerFast"),GTr.forEach(t),u$e=r(U7," (LayoutLMv2 model)"),U7.forEach(t),p$e=i(M),Os=s(M,"LI",{});var J7=n(Os);ez=s(J7,"STRONG",{});var qTr=n(ez);_$e=r(qTr,"led"),qTr.forEach(t),b$e=r(J7," \u2014 "),eL=s(J7,"A",{href:!0});var zTr=n(eL);v$e=r(zTr,"LEDTokenizer"),zTr.forEach(t),T$e=r(J7," or "),oL=s(J7,"A",{href:!0});var XTr=n(oL);F$e=r(XTr,"LEDTokenizerFast"),XTr.forEach(t),E$e=r(J7," (LED model)"),J7.forEach(t),C$e=i(M),Gs=s(M,"LI",{});var K7=n(Gs);oz=s(K7,"STRONG",{});var QTr=n(oz);M$e=r(QTr,"longformer"),QTr.forEach(t),y$e=r(K7," \u2014 "),rL=s(K7,"A",{href:!0});var VTr=n(rL);w$e=r(VTr,"LongformerTokenizer"),VTr.forEach(t),A$e=r(K7," or "),tL=s(K7,"A",{href:!0});var WTr=n(tL);L$e=r(WTr,"LongformerTokenizerFast"),WTr.forEach(t),B$e=r(K7," (Longformer model)"),K7.forEach(t),x$e=i(M),Ff=s(M,"LI",{});var Ype=n(Ff);rz=s(Ype,"STRONG",{});var HTr=n(rz);k$e=r(HTr,"luke"),HTr.forEach(t),R$e=r(Ype," \u2014 "),aL=s(Ype,"A",{href:!0});var UTr=n(aL);P$e=r(UTr,"LukeTokenizer"),UTr.forEach(t),S$e=r(Ype," (LUKE model)"),Ype.forEach(t),$$e=i(M),qs=s(M,"LI",{});var Y7=n(qs);tz=s(Y7,"STRONG",{});var JTr=n(tz);I$e=r(JTr,"lxmert"),JTr.forEach(t),D$e=r(Y7," \u2014 "),sL=s(Y7,"A",{href:!0});var KTr=n(sL);N$e=r(KTr,"LxmertTokenizer"),KTr.forEach(t),j$e=r(Y7," or "),nL=s(Y7,"A",{href:!0});var YTr=n(nL);O$e=r(YTr,"LxmertTokenizerFast"),YTr.forEach(t),G$e=r(Y7," (LXMERT model)"),Y7.forEach(t),q$e=i(M),Ef=s(M,"LI",{});var Zpe=n(Ef);az=s(Zpe,"STRONG",{});var ZTr=n(az);z$e=r(ZTr,"m2m_100"),ZTr.forEach(t),X$e=r(Zpe," \u2014 "),lL=s(Zpe,"A",{href:!0});var e1r=n(lL);Q$e=r(e1r,"M2M100Tokenizer"),e1r.forEach(t),V$e=r(Zpe," (M2M100 model)"),Zpe.forEach(t),W$e=i(M),Cf=s(M,"LI",{});var e_e=n(Cf);sz=s(e_e,"STRONG",{});var o1r=n(sz);H$e=r(o1r,"marian"),o1r.forEach(t),U$e=r(e_e," \u2014 "),iL=s(e_e,"A",{href:!0});var r1r=n(iL);J$e=r(r1r,"MarianTokenizer"),r1r.forEach(t),K$e=r(e_e," (Marian model)"),e_e.forEach(t),Y$e=i(M),zs=s(M,"LI",{});var Z7=n(zs);nz=s(Z7,"STRONG",{});var t1r=n(nz);Z$e=r(t1r,"mbart"),t1r.forEach(t),eIe=r(Z7," \u2014 "),dL=s(Z7,"A",{href:!0});var a1r=n(dL);oIe=r(a1r,"MBartTokenizer"),a1r.forEach(t),rIe=r(Z7," or "),cL=s(Z7,"A",{href:!0});var s1r=n(cL);tIe=r(s1r,"MBartTokenizerFast"),s1r.forEach(t),aIe=r(Z7," (mBART model)"),Z7.forEach(t),sIe=i(M),Xs=s(M,"LI",{});var e0=n(Xs);lz=s(e0,"STRONG",{});var n1r=n(lz);nIe=r(n1r,"mbart50"),n1r.forEach(t),lIe=r(e0," \u2014 "),mL=s(e0,"A",{href:!0});var l1r=n(mL);iIe=r(l1r,"MBart50Tokenizer"),l1r.forEach(t),dIe=r(e0," or "),fL=s(e0,"A",{href:!0});var i1r=n(fL);cIe=r(i1r,"MBart50TokenizerFast"),i1r.forEach(t),mIe=r(e0," (mBART-50 model)"),e0.forEach(t),fIe=i(M),Qs=s(M,"LI",{});var o0=n(Qs);iz=s(o0,"STRONG",{});var d1r=n(iz);hIe=r(d1r,"mobilebert"),d1r.forEach(t),gIe=r(o0," \u2014 "),hL=s(o0,"A",{href:!0});var c1r=n(hL);uIe=r(c1r,"MobileBertTokenizer"),c1r.forEach(t),pIe=r(o0," or "),gL=s(o0,"A",{href:!0});var m1r=n(gL);_Ie=r(m1r,"MobileBertTokenizerFast"),m1r.forEach(t),bIe=r(o0," (MobileBERT model)"),o0.forEach(t),vIe=i(M),Vs=s(M,"LI",{});var r0=n(Vs);dz=s(r0,"STRONG",{});var f1r=n(dz);TIe=r(f1r,"mpnet"),f1r.forEach(t),FIe=r(r0," \u2014 "),uL=s(r0,"A",{href:!0});var h1r=n(uL);EIe=r(h1r,"MPNetTokenizer"),h1r.forEach(t),CIe=r(r0," or "),pL=s(r0,"A",{href:!0});var g1r=n(pL);MIe=r(g1r,"MPNetTokenizerFast"),g1r.forEach(t),yIe=r(r0," (MPNet model)"),r0.forEach(t),wIe=i(M),Ws=s(M,"LI",{});var t0=n(Ws);cz=s(t0,"STRONG",{});var u1r=n(cz);AIe=r(u1r,"mt5"),u1r.forEach(t),LIe=r(t0," \u2014 "),_L=s(t0,"A",{href:!0});var p1r=n(_L);BIe=r(p1r,"MT5Tokenizer"),p1r.forEach(t),xIe=r(t0," or "),bL=s(t0,"A",{href:!0});var _1r=n(bL);kIe=r(_1r,"MT5TokenizerFast"),_1r.forEach(t),RIe=r(t0," (mT5 model)"),t0.forEach(t),PIe=i(M),Hs=s(M,"LI",{});var a0=n(Hs);mz=s(a0,"STRONG",{});var b1r=n(mz);SIe=r(b1r,"openai-gpt"),b1r.forEach(t),$Ie=r(a0," \u2014 "),vL=s(a0,"A",{href:!0});var v1r=n(vL);IIe=r(v1r,"OpenAIGPTTokenizer"),v1r.forEach(t),DIe=r(a0," or "),TL=s(a0,"A",{href:!0});var T1r=n(TL);NIe=r(T1r,"OpenAIGPTTokenizerFast"),T1r.forEach(t),jIe=r(a0," (OpenAI GPT model)"),a0.forEach(t),OIe=i(M),Us=s(M,"LI",{});var s0=n(Us);fz=s(s0,"STRONG",{});var F1r=n(fz);GIe=r(F1r,"pegasus"),F1r.forEach(t),qIe=r(s0," \u2014 "),FL=s(s0,"A",{href:!0});var E1r=n(FL);zIe=r(E1r,"PegasusTokenizer"),E1r.forEach(t),XIe=r(s0," or "),EL=s(s0,"A",{href:!0});var C1r=n(EL);QIe=r(C1r,"PegasusTokenizerFast"),C1r.forEach(t),VIe=r(s0," (Pegasus model)"),s0.forEach(t),WIe=i(M),Mf=s(M,"LI",{});var o_e=n(Mf);hz=s(o_e,"STRONG",{});var M1r=n(hz);HIe=r(M1r,"phobert"),M1r.forEach(t),UIe=r(o_e," \u2014 "),CL=s(o_e,"A",{href:!0});var y1r=n(CL);JIe=r(y1r,"PhobertTokenizer"),y1r.forEach(t),KIe=r(o_e," (PhoBERT model)"),o_e.forEach(t),YIe=i(M),yf=s(M,"LI",{});var r_e=n(yf);gz=s(r_e,"STRONG",{});var w1r=n(gz);ZIe=r(w1r,"prophetnet"),w1r.forEach(t),eDe=r(r_e," \u2014 "),ML=s(r_e,"A",{href:!0});var A1r=n(ML);oDe=r(A1r,"ProphetNetTokenizer"),A1r.forEach(t),rDe=r(r_e," (ProphetNet model)"),r_e.forEach(t),tDe=i(M),Js=s(M,"LI",{});var n0=n(Js);uz=s(n0,"STRONG",{});var L1r=n(uz);aDe=r(L1r,"qdqbert"),L1r.forEach(t),sDe=r(n0," \u2014 "),yL=s(n0,"A",{href:!0});var B1r=n(yL);nDe=r(B1r,"BertTokenizer"),B1r.forEach(t),lDe=r(n0," or "),wL=s(n0,"A",{href:!0});var x1r=n(wL);iDe=r(x1r,"BertTokenizerFast"),x1r.forEach(t),dDe=r(n0," (QDQBert model)"),n0.forEach(t),cDe=i(M),wf=s(M,"LI",{});var t_e=n(wf);pz=s(t_e,"STRONG",{});var k1r=n(pz);mDe=r(k1r,"rag"),k1r.forEach(t),fDe=r(t_e," \u2014 "),AL=s(t_e,"A",{href:!0});var R1r=n(AL);hDe=r(R1r,"RagTokenizer"),R1r.forEach(t),gDe=r(t_e," (RAG model)"),t_e.forEach(t),uDe=i(M),Ks=s(M,"LI",{});var l0=n(Ks);_z=s(l0,"STRONG",{});var P1r=n(_z);pDe=r(P1r,"reformer"),P1r.forEach(t),_De=r(l0," \u2014 "),LL=s(l0,"A",{href:!0});var S1r=n(LL);bDe=r(S1r,"ReformerTokenizer"),S1r.forEach(t),vDe=r(l0," or "),BL=s(l0,"A",{href:!0});var $1r=n(BL);TDe=r($1r,"ReformerTokenizerFast"),$1r.forEach(t),FDe=r(l0," (Reformer model)"),l0.forEach(t),EDe=i(M),Ys=s(M,"LI",{});var i0=n(Ys);bz=s(i0,"STRONG",{});var I1r=n(bz);CDe=r(I1r,"rembert"),I1r.forEach(t),MDe=r(i0," \u2014 "),xL=s(i0,"A",{href:!0});var D1r=n(xL);yDe=r(D1r,"RemBertTokenizer"),D1r.forEach(t),wDe=r(i0," or "),kL=s(i0,"A",{href:!0});var N1r=n(kL);ADe=r(N1r,"RemBertTokenizerFast"),N1r.forEach(t),LDe=r(i0," (RemBERT model)"),i0.forEach(t),BDe=i(M),Zs=s(M,"LI",{});var d0=n(Zs);vz=s(d0,"STRONG",{});var j1r=n(vz);xDe=r(j1r,"retribert"),j1r.forEach(t),kDe=r(d0," \u2014 "),RL=s(d0,"A",{href:!0});var O1r=n(RL);RDe=r(O1r,"RetriBertTokenizer"),O1r.forEach(t),PDe=r(d0," or "),PL=s(d0,"A",{href:!0});var G1r=n(PL);SDe=r(G1r,"RetriBertTokenizerFast"),G1r.forEach(t),$De=r(d0," (RetriBERT model)"),d0.forEach(t),IDe=i(M),en=s(M,"LI",{});var c0=n(en);Tz=s(c0,"STRONG",{});var q1r=n(Tz);DDe=r(q1r,"roberta"),q1r.forEach(t),NDe=r(c0," \u2014 "),SL=s(c0,"A",{href:!0});var z1r=n(SL);jDe=r(z1r,"RobertaTokenizer"),z1r.forEach(t),ODe=r(c0," or "),$L=s(c0,"A",{href:!0});var X1r=n($L);GDe=r(X1r,"RobertaTokenizerFast"),X1r.forEach(t),qDe=r(c0," (RoBERTa model)"),c0.forEach(t),zDe=i(M),on=s(M,"LI",{});var m0=n(on);Fz=s(m0,"STRONG",{});var Q1r=n(Fz);XDe=r(Q1r,"roformer"),Q1r.forEach(t),QDe=r(m0," \u2014 "),IL=s(m0,"A",{href:!0});var V1r=n(IL);VDe=r(V1r,"RoFormerTokenizer"),V1r.forEach(t),WDe=r(m0," or "),DL=s(m0,"A",{href:!0});var W1r=n(DL);HDe=r(W1r,"RoFormerTokenizerFast"),W1r.forEach(t),UDe=r(m0," (RoFormer model)"),m0.forEach(t),JDe=i(M),Af=s(M,"LI",{});var a_e=n(Af);Ez=s(a_e,"STRONG",{});var H1r=n(Ez);KDe=r(H1r,"speech_to_text"),H1r.forEach(t),YDe=r(a_e," \u2014 "),NL=s(a_e,"A",{href:!0});var U1r=n(NL);ZDe=r(U1r,"Speech2TextTokenizer"),U1r.forEach(t),eNe=r(a_e," (Speech2Text model)"),a_e.forEach(t),oNe=i(M),Lf=s(M,"LI",{});var s_e=n(Lf);Cz=s(s_e,"STRONG",{});var J1r=n(Cz);rNe=r(J1r,"speech_to_text_2"),J1r.forEach(t),tNe=r(s_e," \u2014 "),jL=s(s_e,"A",{href:!0});var K1r=n(jL);aNe=r(K1r,"Speech2Text2Tokenizer"),K1r.forEach(t),sNe=r(s_e," (Speech2Text2 model)"),s_e.forEach(t),nNe=i(M),rn=s(M,"LI",{});var f0=n(rn);Mz=s(f0,"STRONG",{});var Y1r=n(Mz);lNe=r(Y1r,"splinter"),Y1r.forEach(t),iNe=r(f0," \u2014 "),OL=s(f0,"A",{href:!0});var Z1r=n(OL);dNe=r(Z1r,"SplinterTokenizer"),Z1r.forEach(t),cNe=r(f0," or "),GL=s(f0,"A",{href:!0});var eFr=n(GL);mNe=r(eFr,"SplinterTokenizerFast"),eFr.forEach(t),fNe=r(f0," (Splinter model)"),f0.forEach(t),hNe=i(M),tn=s(M,"LI",{});var h0=n(tn);yz=s(h0,"STRONG",{});var oFr=n(yz);gNe=r(oFr,"squeezebert"),oFr.forEach(t),uNe=r(h0," \u2014 "),qL=s(h0,"A",{href:!0});var rFr=n(qL);pNe=r(rFr,"SqueezeBertTokenizer"),rFr.forEach(t),_Ne=r(h0," or "),zL=s(h0,"A",{href:!0});var tFr=n(zL);bNe=r(tFr,"SqueezeBertTokenizerFast"),tFr.forEach(t),vNe=r(h0," (SqueezeBERT model)"),h0.forEach(t),TNe=i(M),an=s(M,"LI",{});var g0=n(an);wz=s(g0,"STRONG",{});var aFr=n(wz);FNe=r(aFr,"t5"),aFr.forEach(t),ENe=r(g0," \u2014 "),XL=s(g0,"A",{href:!0});var sFr=n(XL);CNe=r(sFr,"T5Tokenizer"),sFr.forEach(t),MNe=r(g0," or "),QL=s(g0,"A",{href:!0});var nFr=n(QL);yNe=r(nFr,"T5TokenizerFast"),nFr.forEach(t),wNe=r(g0," (T5 model)"),g0.forEach(t),ANe=i(M),Bf=s(M,"LI",{});var n_e=n(Bf);Az=s(n_e,"STRONG",{});var lFr=n(Az);LNe=r(lFr,"tapas"),lFr.forEach(t),BNe=r(n_e," \u2014 "),VL=s(n_e,"A",{href:!0});var iFr=n(VL);xNe=r(iFr,"TapasTokenizer"),iFr.forEach(t),kNe=r(n_e," (TAPAS model)"),n_e.forEach(t),RNe=i(M),xf=s(M,"LI",{});var l_e=n(xf);Lz=s(l_e,"STRONG",{});var dFr=n(Lz);PNe=r(dFr,"transfo-xl"),dFr.forEach(t),SNe=r(l_e," \u2014 "),WL=s(l_e,"A",{href:!0});var cFr=n(WL);$Ne=r(cFr,"TransfoXLTokenizer"),cFr.forEach(t),INe=r(l_e," (Transformer-XL model)"),l_e.forEach(t),DNe=i(M),kf=s(M,"LI",{});var i_e=n(kf);Bz=s(i_e,"STRONG",{});var mFr=n(Bz);NNe=r(mFr,"wav2vec2"),mFr.forEach(t),jNe=r(i_e," \u2014 "),HL=s(i_e,"A",{href:!0});var fFr=n(HL);ONe=r(fFr,"Wav2Vec2CTCTokenizer"),fFr.forEach(t),GNe=r(i_e," (Wav2Vec2 model)"),i_e.forEach(t),qNe=i(M),Rf=s(M,"LI",{});var d_e=n(Rf);xz=s(d_e,"STRONG",{});var hFr=n(xz);zNe=r(hFr,"xlm"),hFr.forEach(t),XNe=r(d_e," \u2014 "),UL=s(d_e,"A",{href:!0});var gFr=n(UL);QNe=r(gFr,"XLMTokenizer"),gFr.forEach(t),VNe=r(d_e," (XLM model)"),d_e.forEach(t),WNe=i(M),Pf=s(M,"LI",{});var c_e=n(Pf);kz=s(c_e,"STRONG",{});var uFr=n(kz);HNe=r(uFr,"xlm-prophetnet"),uFr.forEach(t),UNe=r(c_e," \u2014 "),JL=s(c_e,"A",{href:!0});var pFr=n(JL);JNe=r(pFr,"XLMProphetNetTokenizer"),pFr.forEach(t),KNe=r(c_e," (XLMProphetNet model)"),c_e.forEach(t),YNe=i(M),sn=s(M,"LI",{});var u0=n(sn);Rz=s(u0,"STRONG",{});var _Fr=n(Rz);ZNe=r(_Fr,"xlm-roberta"),_Fr.forEach(t),eje=r(u0," \u2014 "),KL=s(u0,"A",{href:!0});var bFr=n(KL);oje=r(bFr,"XLMRobertaTokenizer"),bFr.forEach(t),rje=r(u0," or "),YL=s(u0,"A",{href:!0});var vFr=n(YL);tje=r(vFr,"XLMRobertaTokenizerFast"),vFr.forEach(t),aje=r(u0," (XLM-RoBERTa model)"),u0.forEach(t),sje=i(M),nn=s(M,"LI",{});var p0=n(nn);Pz=s(p0,"STRONG",{});var TFr=n(Pz);nje=r(TFr,"xlnet"),TFr.forEach(t),lje=r(p0," \u2014 "),ZL=s(p0,"A",{href:!0});var FFr=n(ZL);ije=r(FFr,"XLNetTokenizer"),FFr.forEach(t),dje=r(p0," or "),e8=s(p0,"A",{href:!0});var EFr=n(e8);cje=r(EFr,"XLNetTokenizerFast"),EFr.forEach(t),mje=r(p0," (XLNet model)"),p0.forEach(t),M.forEach(t),fje=i(ko),ei=s(ko,"P",{});var QN=n(ei);hje=r(QN,`Params:
pretrained_model_name_or_path (`),Sz=s(QN,"CODE",{});var CFr=n(Sz);gje=r(CFr,"str"),CFr.forEach(t),uje=r(QN," or "),$z=s(QN,"CODE",{});var MFr=n($z);pje=r(MFr,"os.PathLike"),MFr.forEach(t),_je=r(QN,`):
Can be either:`),QN.forEach(t),bje=i(ko),oi=s(ko,"UL",{});var VN=n(oi);Ta=s(VN,"LI",{});var F4=n(Ta);vje=r(F4,"A string, the "),Iz=s(F4,"EM",{});var yFr=n(Iz);Tje=r(yFr,"model id"),yFr.forEach(t),Fje=r(F4,` of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like `),Dz=s(F4,"CODE",{});var wFr=n(Dz);Eje=r(wFr,"bert-base-uncased"),wFr.forEach(t),Cje=r(F4,`, or namespaced under
a user or organization name, like `),Nz=s(F4,"CODE",{});var AFr=n(Nz);Mje=r(AFr,"dbmdz/bert-base-german-cased"),AFr.forEach(t),yje=r(F4,"."),F4.forEach(t),wje=i(VN),Fa=s(VN,"LI",{});var E4=n(Fa);Aje=r(E4,"A path to a "),jz=s(E4,"EM",{});var LFr=n(jz);Lje=r(LFr,"directory"),LFr.forEach(t),Bje=r(E4,` containing vocabulary files required by the tokenizer, for instance saved
using the `),o8=s(E4,"A",{href:!0});var BFr=n(o8);xje=r(BFr,"save_pretrained()"),BFr.forEach(t),kje=r(E4,` method, e.g.,
`),Oz=s(E4,"CODE",{});var xFr=n(Oz);Rje=r(xFr,"./my_model_directory/"),xFr.forEach(t),Pje=r(E4,"."),E4.forEach(t),Sje=i(VN),k=s(VN,"LI",{});var P=n(k);$je=r(P,`A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: `),Gz=s(P,"CODE",{});var kFr=n(Gz);Ije=r(kFr,"./my_model_directory/vocab.txt"),kFr.forEach(t),Dje=r(P,`. (Not
applicable to all derived classes)
inputs (additional positional arguments, `),qz=s(P,"EM",{});var RFr=n(qz);Nje=r(RFr,"optional"),RFr.forEach(t),jje=r(P,`):
Will be passed along to the Tokenizer `),zz=s(P,"CODE",{});var PFr=n(zz);Oje=r(PFr,"__init__()"),PFr.forEach(t),Gje=r(P,` method.
config (`),r8=s(P,"A",{href:!0});var SFr=n(r8);qje=r(SFr,"PretrainedConfig"),SFr.forEach(t),zje=r(P,", "),Xz=s(P,"EM",{});var $Fr=n(Xz);Xje=r($Fr,"optional"),$Fr.forEach(t),Qje=r(P,`)
The configuration object used to dertermine the tokenizer class to instantiate.
cache`),ri=s(P,"EM",{});var WN=n(ri);Vje=r(WN,"dir ("),Qz=s(WN,"CODE",{});var IFr=n(Qz);Wje=r(IFr,"str"),IFr.forEach(t),Hje=r(WN," or "),Vz=s(WN,"CODE",{});var DFr=n(Vz);Uje=r(DFr,"os.PathLike"),DFr.forEach(t),Jje=r(WN,", _optional"),WN.forEach(t),Kje=r(P,`):
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.
force`),SC=s(P,"EM",{});var twe=n(SC);Yje=r(twe,"download ("),Wz=s(twe,"CODE",{});var NFr=n(Wz);Zje=r(NFr,"bool"),NFr.forEach(t),eOe=r(twe,", _optional"),twe.forEach(t),oOe=r(P,", defaults to "),Hz=s(P,"CODE",{});var jFr=n(Hz);rOe=r(jFr,"False"),jFr.forEach(t),tOe=r(P,`):
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.
resume`),$C=s(P,"EM",{});var awe=n($C);aOe=r(awe,"download ("),Uz=s(awe,"CODE",{});var OFr=n(Uz);sOe=r(OFr,"bool"),OFr.forEach(t),nOe=r(awe,", _optional"),awe.forEach(t),lOe=r(P,", defaults to "),Jz=s(P,"CODE",{});var GFr=n(Jz);iOe=r(GFr,"False"),GFr.forEach(t),dOe=r(P,`):
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.
proxies (`),Kz=s(P,"CODE",{});var qFr=n(Kz);cOe=r(qFr,"Dict[str, str]"),qFr.forEach(t),mOe=r(P,", "),Yz=s(P,"EM",{});var zFr=n(Yz);fOe=r(zFr,"optional"),zFr.forEach(t),hOe=r(P,`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),Zz=s(P,"CODE",{});var XFr=n(Zz);gOe=r(XFr,"{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}"),XFr.forEach(t),uOe=r(P,`. The proxies are used on each request.
revision(`),eX=s(P,"CODE",{});var QFr=n(eX);pOe=r(QFr,"str"),QFr.forEach(t),_Oe=r(P,", "),oX=s(P,"EM",{});var VFr=n(oX);bOe=r(VFr,"optional"),VFr.forEach(t),vOe=r(P,", defaults to "),rX=s(P,"CODE",{});var WFr=n(rX);TOe=r(WFr,'"main"'),WFr.forEach(t),FOe=r(P,`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),tX=s(P,"CODE",{});var HFr=n(tX);EOe=r(HFr,"revision"),HFr.forEach(t),COe=r(P,` can be any
identifier allowed by git.
subfolder (`),aX=s(P,"CODE",{});var UFr=n(aX);MOe=r(UFr,"str"),UFr.forEach(t),yOe=r(P,", "),sX=s(P,"EM",{});var JFr=n(sX);wOe=r(JFr,"optional"),JFr.forEach(t),AOe=r(P,`):
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.
use`),IC=s(P,"EM",{});var swe=n(IC);LOe=r(swe,"fast ("),nX=s(swe,"CODE",{});var KFr=n(nX);BOe=r(KFr,"bool"),KFr.forEach(t),xOe=r(swe,", _optional"),swe.forEach(t),kOe=r(P,", defaults to "),lX=s(P,"CODE",{});var YFr=n(lX);ROe=r(YFr,"True"),YFr.forEach(t),POe=r(P,`):
Whether or not to try to load the fast version of the tokenizer.
tokenizer`),DC=s(P,"EM",{});var nwe=n(DC);SOe=r(nwe,"type ("),iX=s(nwe,"CODE",{});var ZFr=n(iX);$Oe=r(ZFr,"str"),ZFr.forEach(t),IOe=r(nwe,", _optional"),nwe.forEach(t),DOe=r(P,`):
Tokenizer type to be loaded.
trust`),NC=s(P,"EM",{});var lwe=n(NC);NOe=r(lwe,"remote_code ("),dX=s(lwe,"CODE",{});var eEr=n(dX);jOe=r(eEr,"bool"),eEr.forEach(t),OOe=r(lwe,", _optional"),lwe.forEach(t),GOe=r(P,", defaults to "),cX=s(P,"CODE",{});var oEr=n(cX);qOe=r(oEr,"False"),oEr.forEach(t),zOe=r(P,`):
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to `),mX=s(P,"CODE",{});var rEr=n(mX);XOe=r(rEr,"True"),rEr.forEach(t),QOe=r(P,` for repositories you trust and in which you have read the code, as it
will execute code present on the Hub on your local machine.
kwargs (additional keyword arguments, `),fX=s(P,"EM",{});var tEr=n(fX);VOe=r(tEr,"optional"),tEr.forEach(t),WOe=r(P,`):
Will be passed to the Tokenizer `),hX=s(P,"CODE",{});var aEr=n(hX);HOe=r(aEr,"__init__()"),aEr.forEach(t),UOe=r(P,` method. Can be used to set special tokens like
`),gX=s(P,"CODE",{});var sEr=n(gX);JOe=r(sEr,"bos_token"),sEr.forEach(t),KOe=r(P,", "),uX=s(P,"CODE",{});var nEr=n(uX);YOe=r(nEr,"eos_token"),nEr.forEach(t),ZOe=r(P,", "),pX=s(P,"CODE",{});var lEr=n(pX);eGe=r(lEr,"unk_token"),lEr.forEach(t),oGe=r(P,", "),_X=s(P,"CODE",{});var iEr=n(_X);rGe=r(iEr,"sep_token"),iEr.forEach(t),tGe=r(P,", "),bX=s(P,"CODE",{});var dEr=n(bX);aGe=r(dEr,"pad_token"),dEr.forEach(t),sGe=r(P,", "),vX=s(P,"CODE",{});var cEr=n(vX);nGe=r(cEr,"cls_token"),cEr.forEach(t),lGe=r(P,`,
`),TX=s(P,"CODE",{});var mEr=n(TX);iGe=r(mEr,"mask_token"),mEr.forEach(t),dGe=r(P,", "),FX=s(P,"CODE",{});var fEr=n(FX);cGe=r(fEr,"additional_special_tokens"),fEr.forEach(t),mGe=r(P,". See parameters in the "),EX=s(P,"CODE",{});var hEr=n(EX);fGe=r(hEr,"__init__()"),hEr.forEach(t),hGe=r(P," for more details."),P.forEach(t),VN.forEach(t),gGe=i(ko),CX=s(ko,"P",{});var gEr=n(CX);uGe=r(gEr,"Examples:"),gEr.forEach(t),pGe=i(ko),f(jC.$$.fragment,ko),ko.forEach(t),_Ge=i(fn),Sf=s(fn,"DIV",{class:!0});var iwe=n(Sf);f(OC.$$.fragment,iwe),bGe=i(iwe),MX=s(iwe,"P",{});var uEr=n(MX);vGe=r(uEr,"Register a new tokenizer in this mapping."),uEr.forEach(t),iwe.forEach(t),fn.forEach(t),h5e=i(d),ti=s(d,"H2",{class:!0});var dwe=n(ti);$f=s(dwe,"A",{id:!0,class:!0,href:!0});var pEr=n($f);yX=s(pEr,"SPAN",{});var _Er=n(yX);f(GC.$$.fragment,_Er),_Er.forEach(t),pEr.forEach(t),TGe=i(dwe),wX=s(dwe,"SPAN",{});var bEr=n(wX);FGe=r(bEr,"AutoFeatureExtractor"),bEr.forEach(t),dwe.forEach(t),g5e=i(d),Dt=s(d,"DIV",{class:!0});var C4=n(Dt);f(qC.$$.fragment,C4),EGe=i(C4),zC=s(C4,"P",{});var cwe=n(zC);CGe=r(cwe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),t8=s(cwe,"A",{href:!0});var vEr=n(t8);MGe=r(vEr,"AutoFeatureExtractor.from_pretrained()"),vEr.forEach(t),yGe=r(cwe," class method."),cwe.forEach(t),wGe=i(C4),XC=s(C4,"P",{});var mwe=n(XC);AGe=r(mwe,"This class cannot be instantiated directly using "),AX=s(mwe,"CODE",{});var TEr=n(AX);LGe=r(TEr,"__init__()"),TEr.forEach(t),BGe=r(mwe," (throws an error)."),mwe.forEach(t),xGe=i(C4),Te=s(C4,"DIV",{class:!0});var Ye=n(Te);f(QC.$$.fragment,Ye),kGe=i(Ye),LX=s(Ye,"P",{});var FEr=n(LX);RGe=r(FEr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),FEr.forEach(t),PGe=i(Ye),Ea=s(Ye,"P",{});var M4=n(Ea);SGe=r(M4,"The feature extractor class to instantiate is selected based on the "),BX=s(M4,"CODE",{});var EEr=n(BX);$Ge=r(EEr,"model_type"),EEr.forEach(t),IGe=r(M4,` property of the config
object (either passed as an argument or loaded from `),xX=s(M4,"CODE",{});var CEr=n(xX);DGe=r(CEr,"pretrained_model_name_or_path"),CEr.forEach(t),NGe=r(M4,` if possible), or when
it\u2019s missing, by falling back to using pattern matching on `),kX=s(M4,"CODE",{});var MEr=n(kX);jGe=r(MEr,"pretrained_model_name_or_path"),MEr.forEach(t),OGe=r(M4,":"),M4.forEach(t),GGe=i(Ye),Ce=s(Ye,"UL",{});var Ze=n(Ce);If=s(Ze,"LI",{});var m_e=n(If);RX=s(m_e,"STRONG",{});var yEr=n(RX);qGe=r(yEr,"beit"),yEr.forEach(t),zGe=r(m_e," \u2014 "),a8=s(m_e,"A",{href:!0});var wEr=n(a8);XGe=r(wEr,"BeitFeatureExtractor"),wEr.forEach(t),QGe=r(m_e," (BEiT model)"),m_e.forEach(t),VGe=i(Ze),Df=s(Ze,"LI",{});var f_e=n(Df);PX=s(f_e,"STRONG",{});var AEr=n(PX);WGe=r(AEr,"clip"),AEr.forEach(t),HGe=r(f_e," \u2014 "),s8=s(f_e,"A",{href:!0});var LEr=n(s8);UGe=r(LEr,"CLIPFeatureExtractor"),LEr.forEach(t),JGe=r(f_e," (CLIP model)"),f_e.forEach(t),KGe=i(Ze),Nf=s(Ze,"LI",{});var h_e=n(Nf);SX=s(h_e,"STRONG",{});var BEr=n(SX);YGe=r(BEr,"deit"),BEr.forEach(t),ZGe=r(h_e," \u2014 "),n8=s(h_e,"A",{href:!0});var xEr=n(n8);eqe=r(xEr,"DeiTFeatureExtractor"),xEr.forEach(t),oqe=r(h_e," (DeiT model)"),h_e.forEach(t),rqe=i(Ze),jf=s(Ze,"LI",{});var g_e=n(jf);$X=s(g_e,"STRONG",{});var kEr=n($X);tqe=r(kEr,"detr"),kEr.forEach(t),aqe=r(g_e," \u2014 "),l8=s(g_e,"A",{href:!0});var REr=n(l8);sqe=r(REr,"DetrFeatureExtractor"),REr.forEach(t),nqe=r(g_e," (DETR model)"),g_e.forEach(t),lqe=i(Ze),Of=s(Ze,"LI",{});var u_e=n(Of);IX=s(u_e,"STRONG",{});var PEr=n(IX);iqe=r(PEr,"hubert"),PEr.forEach(t),dqe=r(u_e," \u2014 "),i8=s(u_e,"A",{href:!0});var SEr=n(i8);cqe=r(SEr,"Wav2Vec2FeatureExtractor"),SEr.forEach(t),mqe=r(u_e," (Hubert model)"),u_e.forEach(t),fqe=i(Ze),Gf=s(Ze,"LI",{});var p_e=n(Gf);DX=s(p_e,"STRONG",{});var $Er=n(DX);hqe=r($Er,"layoutlmv2"),$Er.forEach(t),gqe=r(p_e," \u2014 "),d8=s(p_e,"A",{href:!0});var IEr=n(d8);uqe=r(IEr,"LayoutLMv2FeatureExtractor"),IEr.forEach(t),pqe=r(p_e," (LayoutLMv2 model)"),p_e.forEach(t),_qe=i(Ze),qf=s(Ze,"LI",{});var __e=n(qf);NX=s(__e,"STRONG",{});var DEr=n(NX);bqe=r(DEr,"speech_to_text"),DEr.forEach(t),vqe=r(__e," \u2014 "),c8=s(__e,"A",{href:!0});var NEr=n(c8);Tqe=r(NEr,"Speech2TextFeatureExtractor"),NEr.forEach(t),Fqe=r(__e," (Speech2Text model)"),__e.forEach(t),Eqe=i(Ze),zf=s(Ze,"LI",{});var b_e=n(zf);jX=s(b_e,"STRONG",{});var jEr=n(jX);Cqe=r(jEr,"vit"),jEr.forEach(t),Mqe=r(b_e," \u2014 "),m8=s(b_e,"A",{href:!0});var OEr=n(m8);yqe=r(OEr,"ViTFeatureExtractor"),OEr.forEach(t),wqe=r(b_e," (ViT model)"),b_e.forEach(t),Aqe=i(Ze),Xf=s(Ze,"LI",{});var v_e=n(Xf);OX=s(v_e,"STRONG",{});var GEr=n(OX);Lqe=r(GEr,"wav2vec2"),GEr.forEach(t),Bqe=r(v_e," \u2014 "),f8=s(v_e,"A",{href:!0});var qEr=n(f8);xqe=r(qEr,"Wav2Vec2FeatureExtractor"),qEr.forEach(t),kqe=r(v_e," (Wav2Vec2 model)"),v_e.forEach(t),Ze.forEach(t),Rqe=i(Ye),ai=s(Ye,"P",{});var HN=n(ai);Pqe=r(HN,`Params:
pretrained_model_name_or_path (`),GX=s(HN,"CODE",{});var zEr=n(GX);Sqe=r(zEr,"str"),zEr.forEach(t),$qe=r(HN," or "),qX=s(HN,"CODE",{});var XEr=n(qX);Iqe=r(XEr,"os.PathLike"),XEr.forEach(t),Dqe=r(HN,`):
This can be either:`),HN.forEach(t),Nqe=i(Ye),si=s(Ye,"UL",{});var UN=n(si);Ca=s(UN,"LI",{});var y4=n(Ca);jqe=r(y4,"a string, the "),zX=s(y4,"EM",{});var QEr=n(zX);Oqe=r(QEr,"model id"),QEr.forEach(t),Gqe=r(y4,` of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like `),XX=s(y4,"CODE",{});var VEr=n(XX);qqe=r(VEr,"bert-base-uncased"),VEr.forEach(t),zqe=r(y4,`, or
namespaced under a user or organization name, like `),QX=s(y4,"CODE",{});var WEr=n(QX);Xqe=r(WEr,"dbmdz/bert-base-german-cased"),WEr.forEach(t),Qqe=r(y4,"."),y4.forEach(t),Vqe=i(UN),Ma=s(UN,"LI",{});var w4=n(Ma);Wqe=r(w4,"a path to a "),VX=s(w4,"EM",{});var HEr=n(VX);Hqe=r(HEr,"directory"),HEr.forEach(t),Uqe=r(w4,` containing a feature extractor file saved using the
`),h8=s(w4,"A",{href:!0});var UEr=n(h8);Jqe=r(UEr,"save_pretrained()"),UEr.forEach(t),Kqe=r(w4,` method, e.g.,
`),WX=s(w4,"CODE",{});var JEr=n(WX);Yqe=r(JEr,"./my_model_directory/"),JEr.forEach(t),Zqe=r(w4,"."),w4.forEach(t),eze=i(UN),N=s(UN,"LI",{});var G=n(N);oze=r(G,"a path or url to a saved feature extractor JSON "),HX=s(G,"EM",{});var KEr=n(HX);rze=r(KEr,"file"),KEr.forEach(t),tze=r(G,`, e.g.,
`),UX=s(G,"CODE",{});var YEr=n(UX);aze=r(YEr,"./my_model_directory/preprocessor_config.json"),YEr.forEach(t),sze=r(G,`.
cache`),ni=s(G,"EM",{});var JN=n(ni);nze=r(JN,"dir ("),JX=s(JN,"CODE",{});var ZEr=n(JX);lze=r(ZEr,"str"),ZEr.forEach(t),ize=r(JN," or "),KX=s(JN,"CODE",{});var e4r=n(KX);dze=r(e4r,"os.PathLike"),e4r.forEach(t),cze=r(JN,", _optional"),JN.forEach(t),mze=r(G,`):
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.
force`),VC=s(G,"EM",{});var fwe=n(VC);fze=r(fwe,"download ("),YX=s(fwe,"CODE",{});var o4r=n(YX);hze=r(o4r,"bool"),o4r.forEach(t),gze=r(fwe,", _optional"),fwe.forEach(t),uze=r(G,", defaults to "),ZX=s(G,"CODE",{});var r4r=n(ZX);pze=r(r4r,"False"),r4r.forEach(t),_ze=r(G,`):
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.
resume`),WC=s(G,"EM",{});var hwe=n(WC);bze=r(hwe,"download ("),eQ=s(hwe,"CODE",{});var t4r=n(eQ);vze=r(t4r,"bool"),t4r.forEach(t),Tze=r(hwe,", _optional"),hwe.forEach(t),Fze=r(G,", defaults to "),oQ=s(G,"CODE",{});var a4r=n(oQ);Eze=r(a4r,"False"),a4r.forEach(t),Cze=r(G,`):
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.
proxies (`),rQ=s(G,"CODE",{});var s4r=n(rQ);Mze=r(s4r,"Dict[str, str]"),s4r.forEach(t),yze=r(G,", "),tQ=s(G,"EM",{});var n4r=n(tQ);wze=r(n4r,"optional"),n4r.forEach(t),Aze=r(G,`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),aQ=s(G,"CODE",{});var l4r=n(aQ);Lze=r(l4r,"{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}."),l4r.forEach(t),Bze=r(G,` The proxies are used on each request.
use`),HC=s(G,"EM",{});var gwe=n(HC);xze=r(gwe,"auth_token ("),sQ=s(gwe,"CODE",{});var i4r=n(sQ);kze=r(i4r,"str"),i4r.forEach(t),Rze=r(gwe," or _bool"),gwe.forEach(t),Pze=r(G,", "),nQ=s(G,"EM",{});var d4r=n(nQ);Sze=r(d4r,"optional"),d4r.forEach(t),$ze=r(G,`):
The token to use as HTTP bearer authorization for remote files. If `),lQ=s(G,"CODE",{});var c4r=n(lQ);Ize=r(c4r,"True"),c4r.forEach(t),Dze=r(G,`, will use the token
generated when running `),iQ=s(G,"CODE",{});var m4r=n(iQ);Nze=r(m4r,"transformers-cli login"),m4r.forEach(t),jze=r(G," (stored in "),dQ=s(G,"CODE",{});var f4r=n(dQ);Oze=r(f4r,"~/.huggingface"),f4r.forEach(t),Gze=r(G,`).
revision(`),cQ=s(G,"CODE",{});var h4r=n(cQ);qze=r(h4r,"str"),h4r.forEach(t),zze=r(G,", "),mQ=s(G,"EM",{});var g4r=n(mQ);Xze=r(g4r,"optional"),g4r.forEach(t),Qze=r(G,", defaults to "),fQ=s(G,"CODE",{});var u4r=n(fQ);Vze=r(u4r,'"main"'),u4r.forEach(t),Wze=r(G,`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),hQ=s(G,"CODE",{});var p4r=n(hQ);Hze=r(p4r,"revision"),p4r.forEach(t),Uze=r(G,` can be any
identifier allowed by git.
return`),UC=s(G,"EM",{});var uwe=n(UC);Jze=r(uwe,"unused_kwargs ("),gQ=s(uwe,"CODE",{});var _4r=n(gQ);Kze=r(_4r,"bool"),_4r.forEach(t),Yze=r(uwe,", _optional"),uwe.forEach(t),Zze=r(G,", defaults to "),uQ=s(G,"CODE",{});var b4r=n(uQ);eXe=r(b4r,"False"),b4r.forEach(t),oXe=r(G,`):
If `),pQ=s(G,"CODE",{});var v4r=n(pQ);rXe=r(v4r,"False"),v4r.forEach(t),tXe=r(G,", then this function returns just the final feature extractor object. If "),_Q=s(G,"CODE",{});var T4r=n(_Q);aXe=r(T4r,"True"),T4r.forEach(t),sXe=r(G,`,
then this functions returns a `),bQ=s(G,"CODE",{});var F4r=n(bQ);nXe=r(F4r,"Tuple(feature_extractor, unused_kwargs)"),F4r.forEach(t),lXe=r(G," where "),vQ=s(G,"EM",{});var E4r=n(vQ);iXe=r(E4r,"unused_kwargs"),E4r.forEach(t),dXe=r(G,` is a
dictionary consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the
part of `),TQ=s(G,"CODE",{});var C4r=n(TQ);cXe=r(C4r,"kwargs"),C4r.forEach(t),mXe=r(G," which has not been used to update "),FQ=s(G,"CODE",{});var M4r=n(FQ);fXe=r(M4r,"feature_extractor"),M4r.forEach(t),hXe=r(G,` and is otherwise ignored.
kwargs (`),EQ=s(G,"CODE",{});var y4r=n(EQ);gXe=r(y4r,"Dict[str, Any]"),y4r.forEach(t),uXe=r(G,", "),CQ=s(G,"EM",{});var w4r=n(CQ);pXe=r(w4r,"optional"),w4r.forEach(t),_Xe=r(G,`):
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are `),MQ=s(G,"EM",{});var A4r=n(MQ);bXe=r(A4r,"not"),A4r.forEach(t),vXe=r(G,` feature extractor attributes is
controlled by the `),yQ=s(G,"CODE",{});var L4r=n(yQ);TXe=r(L4r,"return_unused_kwargs"),L4r.forEach(t),FXe=r(G," keyword parameter."),G.forEach(t),UN.forEach(t),EXe=i(Ye),f(Qf.$$.fragment,Ye),CXe=i(Ye),wQ=s(Ye,"P",{});var B4r=n(wQ);MXe=r(B4r,"Examples:"),B4r.forEach(t),yXe=i(Ye),f(JC.$$.fragment,Ye),Ye.forEach(t),C4.forEach(t),u5e=i(d),li=s(d,"H2",{class:!0});var pwe=n(li);Vf=s(pwe,"A",{id:!0,class:!0,href:!0});var x4r=n(Vf);AQ=s(x4r,"SPAN",{});var k4r=n(AQ);f(KC.$$.fragment,k4r),k4r.forEach(t),x4r.forEach(t),wXe=i(pwe),LQ=s(pwe,"SPAN",{});var R4r=n(LQ);AXe=r(R4r,"AutoProcessor"),R4r.forEach(t),pwe.forEach(t),p5e=i(d),Nt=s(d,"DIV",{class:!0});var A4=n(Nt);f(YC.$$.fragment,A4),LXe=i(A4),ZC=s(A4,"P",{});var _we=n(ZC);BXe=r(_we,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),g8=s(_we,"A",{href:!0});var P4r=n(g8);xXe=r(P4r,"AutoProcessor.from_pretrained()"),P4r.forEach(t),kXe=r(_we," class method."),_we.forEach(t),RXe=i(A4),e3=s(A4,"P",{});var bwe=n(e3);PXe=r(bwe,"This class cannot be instantiated directly using "),BQ=s(bwe,"CODE",{});var S4r=n(BQ);SXe=r(S4r,"__init__()"),S4r.forEach(t),$Xe=r(bwe," (throws an error)."),bwe.forEach(t),IXe=i(A4),Fe=s(A4,"DIV",{class:!0});var eo=n(Fe);f(o3.$$.fragment,eo),DXe=i(eo),xQ=s(eo,"P",{});var $4r=n(xQ);NXe=r($4r,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),$4r.forEach(t),jXe=i(eo),ii=s(eo,"P",{});var KN=n(ii);OXe=r(KN,"The processor class to instantiate is selected based on the "),kQ=s(KN,"CODE",{});var I4r=n(kQ);GXe=r(I4r,"model_type"),I4r.forEach(t),qXe=r(KN,` property of the config object
(either passed as an argument or loaded from `),RQ=s(KN,"CODE",{});var D4r=n(RQ);zXe=r(D4r,"pretrained_model_name_or_path"),D4r.forEach(t),XXe=r(KN," if possible):"),KN.forEach(t),QXe=i(eo),to=s(eo,"UL",{});var bt=n(to);Wf=s(bt,"LI",{});var T_e=n(Wf);PQ=s(T_e,"STRONG",{});var N4r=n(PQ);VXe=r(N4r,"clip"),N4r.forEach(t),WXe=r(T_e," \u2014 "),u8=s(T_e,"A",{href:!0});var j4r=n(u8);HXe=r(j4r,"CLIPProcessor"),j4r.forEach(t),UXe=r(T_e," (CLIP model)"),T_e.forEach(t),JXe=i(bt),Hf=s(bt,"LI",{});var F_e=n(Hf);SQ=s(F_e,"STRONG",{});var O4r=n(SQ);KXe=r(O4r,"layoutlmv2"),O4r.forEach(t),YXe=r(F_e," \u2014 "),p8=s(F_e,"A",{href:!0});var G4r=n(p8);ZXe=r(G4r,"LayoutLMv2Processor"),G4r.forEach(t),eQe=r(F_e," (LayoutLMv2 model)"),F_e.forEach(t),oQe=i(bt),Uf=s(bt,"LI",{});var E_e=n(Uf);$Q=s(E_e,"STRONG",{});var q4r=n($Q);rQe=r(q4r,"speech_to_text"),q4r.forEach(t),tQe=r(E_e," \u2014 "),_8=s(E_e,"A",{href:!0});var z4r=n(_8);aQe=r(z4r,"Speech2TextProcessor"),z4r.forEach(t),sQe=r(E_e," (Speech2Text model)"),E_e.forEach(t),nQe=i(bt),Jf=s(bt,"LI",{});var C_e=n(Jf);IQ=s(C_e,"STRONG",{});var X4r=n(IQ);lQe=r(X4r,"speech_to_text_2"),X4r.forEach(t),iQe=r(C_e," \u2014 "),b8=s(C_e,"A",{href:!0});var Q4r=n(b8);dQe=r(Q4r,"Speech2Text2Processor"),Q4r.forEach(t),cQe=r(C_e," (Speech2Text2 model)"),C_e.forEach(t),mQe=i(bt),Kf=s(bt,"LI",{});var M_e=n(Kf);DQ=s(M_e,"STRONG",{});var V4r=n(DQ);fQe=r(V4r,"trocr"),V4r.forEach(t),hQe=r(M_e," \u2014 "),v8=s(M_e,"A",{href:!0});var W4r=n(v8);gQe=r(W4r,"TrOCRProcessor"),W4r.forEach(t),uQe=r(M_e," (TrOCR model)"),M_e.forEach(t),pQe=i(bt),Yf=s(bt,"LI",{});var y_e=n(Yf);NQ=s(y_e,"STRONG",{});var H4r=n(NQ);_Qe=r(H4r,"vision-text-dual-encoder"),H4r.forEach(t),bQe=r(y_e," \u2014 "),T8=s(y_e,"A",{href:!0});var U4r=n(T8);vQe=r(U4r,"VisionTextDualEncoderProcessor"),U4r.forEach(t),TQe=r(y_e," (VisionTextDualEncoder model)"),y_e.forEach(t),FQe=i(bt),Zf=s(bt,"LI",{});var w_e=n(Zf);jQ=s(w_e,"STRONG",{});var J4r=n(jQ);EQe=r(J4r,"wav2vec2"),J4r.forEach(t),CQe=r(w_e," \u2014 "),F8=s(w_e,"A",{href:!0});var K4r=n(F8);MQe=r(K4r,"Wav2Vec2Processor"),K4r.forEach(t),yQe=r(w_e," (Wav2Vec2 model)"),w_e.forEach(t),bt.forEach(t),wQe=i(eo),di=s(eo,"P",{});var YN=n(di);AQe=r(YN,`Params:
pretrained_model_name_or_path (`),OQ=s(YN,"CODE",{});var Y4r=n(OQ);LQe=r(Y4r,"str"),Y4r.forEach(t),BQe=r(YN," or "),GQ=s(YN,"CODE",{});var Z4r=n(GQ);xQe=r(Z4r,"os.PathLike"),Z4r.forEach(t),kQe=r(YN,`):
This can be either:`),YN.forEach(t),RQe=i(eo),r3=s(eo,"UL",{});var vwe=n(r3);ya=s(vwe,"LI",{});var L4=n(ya);PQe=r(L4,"a string, the "),qQ=s(L4,"EM",{});var eCr=n(qQ);SQe=r(eCr,"model id"),eCr.forEach(t),$Qe=r(L4,` of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like `),zQ=s(L4,"CODE",{});var oCr=n(zQ);IQe=r(oCr,"bert-base-uncased"),oCr.forEach(t),DQe=r(L4,`, or
namespaced under a user or organization name, like `),XQ=s(L4,"CODE",{});var rCr=n(XQ);NQe=r(rCr,"dbmdz/bert-base-german-cased"),rCr.forEach(t),jQe=r(L4,"."),L4.forEach(t),OQe=i(vwe),D=s(vwe,"LI",{});var j=n(D);GQe=r(j,"a path to a "),QQ=s(j,"EM",{});var tCr=n(QQ);qQe=r(tCr,"directory"),tCr.forEach(t),zQe=r(j," containing a processor files saved using the "),VQ=s(j,"CODE",{});var aCr=n(VQ);XQe=r(aCr,"save_pretrained()"),aCr.forEach(t),QQe=r(j,` method,
e.g., `),WQ=s(j,"CODE",{});var sCr=n(WQ);VQe=r(sCr,"./my_model_directory/"),sCr.forEach(t),WQe=r(j,`.
cache`),ci=s(j,"EM",{});var ZN=n(ci);HQe=r(ZN,"dir ("),HQ=s(ZN,"CODE",{});var nCr=n(HQ);UQe=r(nCr,"str"),nCr.forEach(t),JQe=r(ZN," or "),UQ=s(ZN,"CODE",{});var lCr=n(UQ);KQe=r(lCr,"os.PathLike"),lCr.forEach(t),YQe=r(ZN,", _optional"),ZN.forEach(t),ZQe=r(j,`):
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.
force`),t3=s(j,"EM",{});var Twe=n(t3);eVe=r(Twe,"download ("),JQ=s(Twe,"CODE",{});var iCr=n(JQ);oVe=r(iCr,"bool"),iCr.forEach(t),rVe=r(Twe,", _optional"),Twe.forEach(t),tVe=r(j,", defaults to "),KQ=s(j,"CODE",{});var dCr=n(KQ);aVe=r(dCr,"False"),dCr.forEach(t),sVe=r(j,`):
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.
resume`),a3=s(j,"EM",{});var Fwe=n(a3);nVe=r(Fwe,"download ("),YQ=s(Fwe,"CODE",{});var cCr=n(YQ);lVe=r(cCr,"bool"),cCr.forEach(t),iVe=r(Fwe,", _optional"),Fwe.forEach(t),dVe=r(j,", defaults to "),ZQ=s(j,"CODE",{});var mCr=n(ZQ);cVe=r(mCr,"False"),mCr.forEach(t),mVe=r(j,`):
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.
proxies (`),eV=s(j,"CODE",{});var fCr=n(eV);fVe=r(fCr,"Dict[str, str]"),fCr.forEach(t),hVe=r(j,", "),oV=s(j,"EM",{});var hCr=n(oV);gVe=r(hCr,"optional"),hCr.forEach(t),uVe=r(j,`):
A dictionary of proxy servers to use by protocol or endpoint, e.g., `),rV=s(j,"CODE",{});var gCr=n(rV);pVe=r(gCr,"{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}."),gCr.forEach(t),_Ve=r(j,` The proxies are used on each request.
use`),s3=s(j,"EM",{});var Ewe=n(s3);bVe=r(Ewe,"auth_token ("),tV=s(Ewe,"CODE",{});var uCr=n(tV);vVe=r(uCr,"str"),uCr.forEach(t),TVe=r(Ewe," or _bool"),Ewe.forEach(t),FVe=r(j,", "),aV=s(j,"EM",{});var pCr=n(aV);EVe=r(pCr,"optional"),pCr.forEach(t),CVe=r(j,`):
The token to use as HTTP bearer authorization for remote files. If `),sV=s(j,"CODE",{});var _Cr=n(sV);MVe=r(_Cr,"True"),_Cr.forEach(t),yVe=r(j,`, will use the token
generated when running `),nV=s(j,"CODE",{});var bCr=n(nV);wVe=r(bCr,"transformers-cli login"),bCr.forEach(t),AVe=r(j," (stored in "),lV=s(j,"CODE",{});var vCr=n(lV);LVe=r(vCr,"~/.huggingface"),vCr.forEach(t),BVe=r(j,`).
revision (`),iV=s(j,"CODE",{});var TCr=n(iV);xVe=r(TCr,"str"),TCr.forEach(t),kVe=r(j,", "),dV=s(j,"EM",{});var FCr=n(dV);RVe=r(FCr,"optional"),FCr.forEach(t),PVe=r(j,", defaults to "),cV=s(j,"CODE",{});var ECr=n(cV);SVe=r(ECr,'"main"'),ECr.forEach(t),$Ve=r(j,`):
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so `),mV=s(j,"CODE",{});var CCr=n(mV);IVe=r(CCr,"revision"),CCr.forEach(t),DVe=r(j,` can be any
identifier allowed by git.
return`),n3=s(j,"EM",{});var Cwe=n(n3);NVe=r(Cwe,"unused_kwargs ("),fV=s(Cwe,"CODE",{});var MCr=n(fV);jVe=r(MCr,"bool"),MCr.forEach(t),OVe=r(Cwe,", _optional"),Cwe.forEach(t),GVe=r(j,", defaults to "),hV=s(j,"CODE",{});var yCr=n(hV);qVe=r(yCr,"False"),yCr.forEach(t),zVe=r(j,`):
If `),gV=s(j,"CODE",{});var wCr=n(gV);XVe=r(wCr,"False"),wCr.forEach(t),QVe=r(j,", then this function returns just the final feature extractor object. If "),uV=s(j,"CODE",{});var ACr=n(uV);VVe=r(ACr,"True"),ACr.forEach(t),WVe=r(j,`,
then this functions returns a `),pV=s(j,"CODE",{});var LCr=n(pV);HVe=r(LCr,"Tuple(feature_extractor, unused_kwargs)"),LCr.forEach(t),UVe=r(j," where "),_V=s(j,"EM",{});var BCr=n(_V);JVe=r(BCr,"unused_kwargs"),BCr.forEach(t),KVe=r(j,` is a
dictionary consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the
part of `),bV=s(j,"CODE",{});var xCr=n(bV);YVe=r(xCr,"kwargs"),xCr.forEach(t),ZVe=r(j," which has not been used to update "),vV=s(j,"CODE",{});var kCr=n(vV);eWe=r(kCr,"feature_extractor"),kCr.forEach(t),oWe=r(j,` and is otherwise ignored.
kwargs (`),TV=s(j,"CODE",{});var RCr=n(TV);rWe=r(RCr,"Dict[str, Any]"),RCr.forEach(t),tWe=r(j,", "),FV=s(j,"EM",{});var PCr=n(FV);aWe=r(PCr,"optional"),PCr.forEach(t),sWe=r(j,`):
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are `),EV=s(j,"EM",{});var SCr=n(EV);nWe=r(SCr,"not"),SCr.forEach(t),lWe=r(j,` feature extractor attributes is
controlled by the `),CV=s(j,"CODE",{});var $Cr=n(CV);iWe=r($Cr,"return_unused_kwargs"),$Cr.forEach(t),dWe=r(j," keyword parameter."),j.forEach(t),vwe.forEach(t),cWe=i(eo),f(eh.$$.fragment,eo),mWe=i(eo),MV=s(eo,"P",{});var ICr=n(MV);fWe=r(ICr,"Examples:"),ICr.forEach(t),hWe=i(eo),f(l3.$$.fragment,eo),eo.forEach(t),A4.forEach(t),_5e=i(d),mi=s(d,"H2",{class:!0});var Mwe=n(mi);oh=s(Mwe,"A",{id:!0,class:!0,href:!0});var DCr=n(oh);yV=s(DCr,"SPAN",{});var NCr=n(yV);f(i3.$$.fragment,NCr),NCr.forEach(t),DCr.forEach(t),gWe=i(Mwe),wV=s(Mwe,"SPAN",{});var jCr=n(wV);uWe=r(jCr,"AutoModel"),jCr.forEach(t),Mwe.forEach(t),b5e=i(d),jo=s(d,"DIV",{class:!0});var hn=n(jo);f(d3.$$.fragment,hn),pWe=i(hn),fi=s(hn,"P",{});var ej=n(fi);_We=r(ej,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),AV=s(ej,"CODE",{});var OCr=n(AV);bWe=r(OCr,"from_pretrained()"),OCr.forEach(t),vWe=r(ej,` class method or the
`),LV=s(ej,"CODE",{});var GCr=n(LV);TWe=r(GCr,"from_config()"),GCr.forEach(t),FWe=r(ej," class method."),ej.forEach(t),EWe=i(hn),c3=s(hn,"P",{});var ywe=n(c3);CWe=r(ywe,"This class cannot be instantiated directly using "),BV=s(ywe,"CODE",{});var qCr=n(BV);MWe=r(qCr,"__init__()"),qCr.forEach(t),yWe=r(ywe," (throws an error)."),ywe.forEach(t),wWe=i(hn),kr=s(hn,"DIV",{class:!0});var gn=n(kr);f(m3.$$.fragment,gn),AWe=i(gn),xV=s(gn,"P",{});var zCr=n(xV);LWe=r(zCr,"Instantiates one of the base model classes of the library from a configuration."),zCr.forEach(t),BWe=i(gn),hi=s(gn,"P",{});var oj=n(hi);xWe=r(oj,`Note:
Loading a model from its configuration file does `),kV=s(oj,"STRONG",{});var XCr=n(kV);kWe=r(XCr,"not"),XCr.forEach(t),RWe=r(oj,` load the model weights. It only affects the
model\u2019s configuration. Use `),RV=s(oj,"CODE",{});var QCr=n(RV);PWe=r(QCr,"from_pretrained()"),QCr.forEach(t),SWe=r(oj,` to load the model
weights.`),oj.forEach(t),$We=i(gn),PV=s(gn,"P",{});var VCr=n(PV);IWe=r(VCr,"Examples:"),VCr.forEach(t),DWe=i(gn),f(f3.$$.fragment,gn),gn.forEach(t),NWe=i(hn),Se=s(hn,"DIV",{class:!0});var vt=n(Se);f(h3.$$.fragment,vt),jWe=i(vt),SV=s(vt,"P",{});var WCr=n(SV);OWe=r(WCr,"Instantiate one of the base model classes of the library from a pretrained model."),WCr.forEach(t),GWe=i(vt),wa=s(vt,"P",{});var B4=n(wa);qWe=r(B4,"The model class to instantiate is selected based on the "),$V=s(B4,"CODE",{});var HCr=n($V);zWe=r(HCr,"model_type"),HCr.forEach(t),XWe=r(B4,` property of the config object (either
passed as an argument or loaded from `),IV=s(B4,"CODE",{});var UCr=n(IV);QWe=r(UCr,"pretrained_model_name_or_path"),UCr.forEach(t),VWe=r(B4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),DV=s(B4,"CODE",{});var JCr=n(DV);WWe=r(JCr,"pretrained_model_name_or_path"),JCr.forEach(t),HWe=r(B4,":"),B4.forEach(t),UWe=i(vt),F=s(vt,"UL",{});var E=n(F);rh=s(E,"LI",{});var A_e=n(rh);NV=s(A_e,"STRONG",{});var KCr=n(NV);JWe=r(KCr,"albert"),KCr.forEach(t),KWe=r(A_e," \u2014 "),E8=s(A_e,"A",{href:!0});var YCr=n(E8);YWe=r(YCr,"AlbertModel"),YCr.forEach(t),ZWe=r(A_e," (ALBERT model)"),A_e.forEach(t),eHe=i(E),th=s(E,"LI",{});var L_e=n(th);jV=s(L_e,"STRONG",{});var ZCr=n(jV);oHe=r(ZCr,"bart"),ZCr.forEach(t),rHe=r(L_e," \u2014 "),C8=s(L_e,"A",{href:!0});var e3r=n(C8);tHe=r(e3r,"BartModel"),e3r.forEach(t),aHe=r(L_e," (BART model)"),L_e.forEach(t),sHe=i(E),ah=s(E,"LI",{});var B_e=n(ah);OV=s(B_e,"STRONG",{});var o3r=n(OV);nHe=r(o3r,"beit"),o3r.forEach(t),lHe=r(B_e," \u2014 "),M8=s(B_e,"A",{href:!0});var r3r=n(M8);iHe=r(r3r,"BeitModel"),r3r.forEach(t),dHe=r(B_e," (BEiT model)"),B_e.forEach(t),cHe=i(E),sh=s(E,"LI",{});var x_e=n(sh);GV=s(x_e,"STRONG",{});var t3r=n(GV);mHe=r(t3r,"bert"),t3r.forEach(t),fHe=r(x_e," \u2014 "),y8=s(x_e,"A",{href:!0});var a3r=n(y8);hHe=r(a3r,"BertModel"),a3r.forEach(t),gHe=r(x_e," (BERT model)"),x_e.forEach(t),uHe=i(E),nh=s(E,"LI",{});var k_e=n(nh);qV=s(k_e,"STRONG",{});var s3r=n(qV);pHe=r(s3r,"bert-generation"),s3r.forEach(t),_He=r(k_e," \u2014 "),w8=s(k_e,"A",{href:!0});var n3r=n(w8);bHe=r(n3r,"BertGenerationEncoder"),n3r.forEach(t),vHe=r(k_e," (Bert Generation model)"),k_e.forEach(t),THe=i(E),lh=s(E,"LI",{});var R_e=n(lh);zV=s(R_e,"STRONG",{});var l3r=n(zV);FHe=r(l3r,"big_bird"),l3r.forEach(t),EHe=r(R_e," \u2014 "),A8=s(R_e,"A",{href:!0});var i3r=n(A8);CHe=r(i3r,"BigBirdModel"),i3r.forEach(t),MHe=r(R_e," (BigBird model)"),R_e.forEach(t),yHe=i(E),ih=s(E,"LI",{});var P_e=n(ih);XV=s(P_e,"STRONG",{});var d3r=n(XV);wHe=r(d3r,"bigbird_pegasus"),d3r.forEach(t),AHe=r(P_e," \u2014 "),L8=s(P_e,"A",{href:!0});var c3r=n(L8);LHe=r(c3r,"BigBirdPegasusModel"),c3r.forEach(t),BHe=r(P_e," (BigBirdPegasus model)"),P_e.forEach(t),xHe=i(E),dh=s(E,"LI",{});var S_e=n(dh);QV=s(S_e,"STRONG",{});var m3r=n(QV);kHe=r(m3r,"blenderbot"),m3r.forEach(t),RHe=r(S_e," \u2014 "),B8=s(S_e,"A",{href:!0});var f3r=n(B8);PHe=r(f3r,"BlenderbotModel"),f3r.forEach(t),SHe=r(S_e," (Blenderbot model)"),S_e.forEach(t),$He=i(E),ch=s(E,"LI",{});var $_e=n(ch);VV=s($_e,"STRONG",{});var h3r=n(VV);IHe=r(h3r,"blenderbot-small"),h3r.forEach(t),DHe=r($_e," \u2014 "),x8=s($_e,"A",{href:!0});var g3r=n(x8);NHe=r(g3r,"BlenderbotSmallModel"),g3r.forEach(t),jHe=r($_e," (BlenderbotSmall model)"),$_e.forEach(t),OHe=i(E),mh=s(E,"LI",{});var I_e=n(mh);WV=s(I_e,"STRONG",{});var u3r=n(WV);GHe=r(u3r,"camembert"),u3r.forEach(t),qHe=r(I_e," \u2014 "),k8=s(I_e,"A",{href:!0});var p3r=n(k8);zHe=r(p3r,"CamembertModel"),p3r.forEach(t),XHe=r(I_e," (CamemBERT model)"),I_e.forEach(t),QHe=i(E),fh=s(E,"LI",{});var D_e=n(fh);HV=s(D_e,"STRONG",{});var _3r=n(HV);VHe=r(_3r,"canine"),_3r.forEach(t),WHe=r(D_e," \u2014 "),R8=s(D_e,"A",{href:!0});var b3r=n(R8);HHe=r(b3r,"CanineModel"),b3r.forEach(t),UHe=r(D_e," (Canine model)"),D_e.forEach(t),JHe=i(E),hh=s(E,"LI",{});var N_e=n(hh);UV=s(N_e,"STRONG",{});var v3r=n(UV);KHe=r(v3r,"clip"),v3r.forEach(t),YHe=r(N_e," \u2014 "),P8=s(N_e,"A",{href:!0});var T3r=n(P8);ZHe=r(T3r,"CLIPModel"),T3r.forEach(t),eUe=r(N_e," (CLIP model)"),N_e.forEach(t),oUe=i(E),gh=s(E,"LI",{});var j_e=n(gh);JV=s(j_e,"STRONG",{});var F3r=n(JV);rUe=r(F3r,"convbert"),F3r.forEach(t),tUe=r(j_e," \u2014 "),S8=s(j_e,"A",{href:!0});var E3r=n(S8);aUe=r(E3r,"ConvBertModel"),E3r.forEach(t),sUe=r(j_e," (ConvBERT model)"),j_e.forEach(t),nUe=i(E),uh=s(E,"LI",{});var O_e=n(uh);KV=s(O_e,"STRONG",{});var C3r=n(KV);lUe=r(C3r,"ctrl"),C3r.forEach(t),iUe=r(O_e," \u2014 "),$8=s(O_e,"A",{href:!0});var M3r=n($8);dUe=r(M3r,"CTRLModel"),M3r.forEach(t),cUe=r(O_e," (CTRL model)"),O_e.forEach(t),mUe=i(E),ph=s(E,"LI",{});var G_e=n(ph);YV=s(G_e,"STRONG",{});var y3r=n(YV);fUe=r(y3r,"deberta"),y3r.forEach(t),hUe=r(G_e," \u2014 "),I8=s(G_e,"A",{href:!0});var w3r=n(I8);gUe=r(w3r,"DebertaModel"),w3r.forEach(t),uUe=r(G_e," (DeBERTa model)"),G_e.forEach(t),pUe=i(E),_h=s(E,"LI",{});var q_e=n(_h);ZV=s(q_e,"STRONG",{});var A3r=n(ZV);_Ue=r(A3r,"deberta-v2"),A3r.forEach(t),bUe=r(q_e," \u2014 "),D8=s(q_e,"A",{href:!0});var L3r=n(D8);vUe=r(L3r,"DebertaV2Model"),L3r.forEach(t),TUe=r(q_e," (DeBERTa-v2 model)"),q_e.forEach(t),FUe=i(E),bh=s(E,"LI",{});var z_e=n(bh);eW=s(z_e,"STRONG",{});var B3r=n(eW);EUe=r(B3r,"deit"),B3r.forEach(t),CUe=r(z_e," \u2014 "),N8=s(z_e,"A",{href:!0});var x3r=n(N8);MUe=r(x3r,"DeiTModel"),x3r.forEach(t),yUe=r(z_e," (DeiT model)"),z_e.forEach(t),wUe=i(E),vh=s(E,"LI",{});var X_e=n(vh);oW=s(X_e,"STRONG",{});var k3r=n(oW);AUe=r(k3r,"detr"),k3r.forEach(t),LUe=r(X_e," \u2014 "),j8=s(X_e,"A",{href:!0});var R3r=n(j8);BUe=r(R3r,"DetrModel"),R3r.forEach(t),xUe=r(X_e," (DETR model)"),X_e.forEach(t),kUe=i(E),Th=s(E,"LI",{});var Q_e=n(Th);rW=s(Q_e,"STRONG",{});var P3r=n(rW);RUe=r(P3r,"distilbert"),P3r.forEach(t),PUe=r(Q_e," \u2014 "),O8=s(Q_e,"A",{href:!0});var S3r=n(O8);SUe=r(S3r,"DistilBertModel"),S3r.forEach(t),$Ue=r(Q_e," (DistilBERT model)"),Q_e.forEach(t),IUe=i(E),Fh=s(E,"LI",{});var V_e=n(Fh);tW=s(V_e,"STRONG",{});var $3r=n(tW);DUe=r($3r,"dpr"),$3r.forEach(t),NUe=r(V_e," \u2014 "),G8=s(V_e,"A",{href:!0});var I3r=n(G8);jUe=r(I3r,"DPRQuestionEncoder"),I3r.forEach(t),OUe=r(V_e," (DPR model)"),V_e.forEach(t),GUe=i(E),Eh=s(E,"LI",{});var W_e=n(Eh);aW=s(W_e,"STRONG",{});var D3r=n(aW);qUe=r(D3r,"electra"),D3r.forEach(t),zUe=r(W_e," \u2014 "),q8=s(W_e,"A",{href:!0});var N3r=n(q8);XUe=r(N3r,"ElectraModel"),N3r.forEach(t),QUe=r(W_e," (ELECTRA model)"),W_e.forEach(t),VUe=i(E),Ch=s(E,"LI",{});var H_e=n(Ch);sW=s(H_e,"STRONG",{});var j3r=n(sW);WUe=r(j3r,"flaubert"),j3r.forEach(t),HUe=r(H_e," \u2014 "),z8=s(H_e,"A",{href:!0});var O3r=n(z8);UUe=r(O3r,"FlaubertModel"),O3r.forEach(t),JUe=r(H_e," (FlauBERT model)"),H_e.forEach(t),KUe=i(E),Mh=s(E,"LI",{});var U_e=n(Mh);nW=s(U_e,"STRONG",{});var G3r=n(nW);YUe=r(G3r,"fnet"),G3r.forEach(t),ZUe=r(U_e," \u2014 "),X8=s(U_e,"A",{href:!0});var q3r=n(X8);eJe=r(q3r,"FNetModel"),q3r.forEach(t),oJe=r(U_e," (FNet model)"),U_e.forEach(t),rJe=i(E),yh=s(E,"LI",{});var J_e=n(yh);lW=s(J_e,"STRONG",{});var z3r=n(lW);tJe=r(z3r,"fsmt"),z3r.forEach(t),aJe=r(J_e," \u2014 "),Q8=s(J_e,"A",{href:!0});var X3r=n(Q8);sJe=r(X3r,"FSMTModel"),X3r.forEach(t),nJe=r(J_e," (FairSeq Machine-Translation model)"),J_e.forEach(t),lJe=i(E),ln=s(E,"LI",{});var _0=n(ln);iW=s(_0,"STRONG",{});var Q3r=n(iW);iJe=r(Q3r,"funnel"),Q3r.forEach(t),dJe=r(_0," \u2014 "),V8=s(_0,"A",{href:!0});var V3r=n(V8);cJe=r(V3r,"FunnelModel"),V3r.forEach(t),mJe=r(_0," or "),W8=s(_0,"A",{href:!0});var W3r=n(W8);fJe=r(W3r,"FunnelBaseModel"),W3r.forEach(t),hJe=r(_0," (Funnel Transformer model)"),_0.forEach(t),gJe=i(E),wh=s(E,"LI",{});var K_e=n(wh);dW=s(K_e,"STRONG",{});var H3r=n(dW);uJe=r(H3r,"gpt2"),H3r.forEach(t),pJe=r(K_e," \u2014 "),H8=s(K_e,"A",{href:!0});var U3r=n(H8);_Je=r(U3r,"GPT2Model"),U3r.forEach(t),bJe=r(K_e," (OpenAI GPT-2 model)"),K_e.forEach(t),vJe=i(E),Ah=s(E,"LI",{});var Y_e=n(Ah);cW=s(Y_e,"STRONG",{});var J3r=n(cW);TJe=r(J3r,"gpt_neo"),J3r.forEach(t),FJe=r(Y_e," \u2014 "),U8=s(Y_e,"A",{href:!0});var K3r=n(U8);EJe=r(K3r,"GPTNeoModel"),K3r.forEach(t),CJe=r(Y_e," (GPT Neo model)"),Y_e.forEach(t),MJe=i(E),Lh=s(E,"LI",{});var Z_e=n(Lh);mW=s(Z_e,"STRONG",{});var Y3r=n(mW);yJe=r(Y3r,"gptj"),Y3r.forEach(t),wJe=r(Z_e," \u2014 "),J8=s(Z_e,"A",{href:!0});var Z3r=n(J8);AJe=r(Z3r,"GPTJModel"),Z3r.forEach(t),LJe=r(Z_e," (GPT-J model)"),Z_e.forEach(t),BJe=i(E),Bh=s(E,"LI",{});var ebe=n(Bh);fW=s(ebe,"STRONG",{});var eMr=n(fW);xJe=r(eMr,"hubert"),eMr.forEach(t),kJe=r(ebe," \u2014 "),K8=s(ebe,"A",{href:!0});var oMr=n(K8);RJe=r(oMr,"HubertModel"),oMr.forEach(t),PJe=r(ebe," (Hubert model)"),ebe.forEach(t),SJe=i(E),xh=s(E,"LI",{});var obe=n(xh);hW=s(obe,"STRONG",{});var rMr=n(hW);$Je=r(rMr,"ibert"),rMr.forEach(t),IJe=r(obe," \u2014 "),Y8=s(obe,"A",{href:!0});var tMr=n(Y8);DJe=r(tMr,"IBertModel"),tMr.forEach(t),NJe=r(obe," (I-BERT model)"),obe.forEach(t),jJe=i(E),kh=s(E,"LI",{});var rbe=n(kh);gW=s(rbe,"STRONG",{});var aMr=n(gW);OJe=r(aMr,"imagegpt"),aMr.forEach(t),GJe=r(rbe," \u2014 "),Z8=s(rbe,"A",{href:!0});var sMr=n(Z8);qJe=r(sMr,"ImageGPTModel"),sMr.forEach(t),zJe=r(rbe," (ImageGPT model)"),rbe.forEach(t),XJe=i(E),Rh=s(E,"LI",{});var tbe=n(Rh);uW=s(tbe,"STRONG",{});var nMr=n(uW);QJe=r(nMr,"layoutlm"),nMr.forEach(t),VJe=r(tbe," \u2014 "),eB=s(tbe,"A",{href:!0});var lMr=n(eB);WJe=r(lMr,"LayoutLMModel"),lMr.forEach(t),HJe=r(tbe," (LayoutLM model)"),tbe.forEach(t),UJe=i(E),Ph=s(E,"LI",{});var abe=n(Ph);pW=s(abe,"STRONG",{});var iMr=n(pW);JJe=r(iMr,"layoutlmv2"),iMr.forEach(t),KJe=r(abe," \u2014 "),oB=s(abe,"A",{href:!0});var dMr=n(oB);YJe=r(dMr,"LayoutLMv2Model"),dMr.forEach(t),ZJe=r(abe," (LayoutLMv2 model)"),abe.forEach(t),eKe=i(E),Sh=s(E,"LI",{});var sbe=n(Sh);_W=s(sbe,"STRONG",{});var cMr=n(_W);oKe=r(cMr,"led"),cMr.forEach(t),rKe=r(sbe," \u2014 "),rB=s(sbe,"A",{href:!0});var mMr=n(rB);tKe=r(mMr,"LEDModel"),mMr.forEach(t),aKe=r(sbe," (LED model)"),sbe.forEach(t),sKe=i(E),$h=s(E,"LI",{});var nbe=n($h);bW=s(nbe,"STRONG",{});var fMr=n(bW);nKe=r(fMr,"longformer"),fMr.forEach(t),lKe=r(nbe," \u2014 "),tB=s(nbe,"A",{href:!0});var hMr=n(tB);iKe=r(hMr,"LongformerModel"),hMr.forEach(t),dKe=r(nbe," (Longformer model)"),nbe.forEach(t),cKe=i(E),Ih=s(E,"LI",{});var lbe=n(Ih);vW=s(lbe,"STRONG",{});var gMr=n(vW);mKe=r(gMr,"luke"),gMr.forEach(t),fKe=r(lbe," \u2014 "),aB=s(lbe,"A",{href:!0});var uMr=n(aB);hKe=r(uMr,"LukeModel"),uMr.forEach(t),gKe=r(lbe," (LUKE model)"),lbe.forEach(t),uKe=i(E),Dh=s(E,"LI",{});var ibe=n(Dh);TW=s(ibe,"STRONG",{});var pMr=n(TW);pKe=r(pMr,"lxmert"),pMr.forEach(t),_Ke=r(ibe," \u2014 "),sB=s(ibe,"A",{href:!0});var _Mr=n(sB);bKe=r(_Mr,"LxmertModel"),_Mr.forEach(t),vKe=r(ibe," (LXMERT model)"),ibe.forEach(t),TKe=i(E),Nh=s(E,"LI",{});var dbe=n(Nh);FW=s(dbe,"STRONG",{});var bMr=n(FW);FKe=r(bMr,"m2m_100"),bMr.forEach(t),EKe=r(dbe," \u2014 "),nB=s(dbe,"A",{href:!0});var vMr=n(nB);CKe=r(vMr,"M2M100Model"),vMr.forEach(t),MKe=r(dbe," (M2M100 model)"),dbe.forEach(t),yKe=i(E),jh=s(E,"LI",{});var cbe=n(jh);EW=s(cbe,"STRONG",{});var TMr=n(EW);wKe=r(TMr,"marian"),TMr.forEach(t),AKe=r(cbe," \u2014 "),lB=s(cbe,"A",{href:!0});var FMr=n(lB);LKe=r(FMr,"MarianModel"),FMr.forEach(t),BKe=r(cbe," (Marian model)"),cbe.forEach(t),xKe=i(E),Oh=s(E,"LI",{});var mbe=n(Oh);CW=s(mbe,"STRONG",{});var EMr=n(CW);kKe=r(EMr,"mbart"),EMr.forEach(t),RKe=r(mbe," \u2014 "),iB=s(mbe,"A",{href:!0});var CMr=n(iB);PKe=r(CMr,"MBartModel"),CMr.forEach(t),SKe=r(mbe," (mBART model)"),mbe.forEach(t),$Ke=i(E),Gh=s(E,"LI",{});var fbe=n(Gh);MW=s(fbe,"STRONG",{});var MMr=n(MW);IKe=r(MMr,"megatron-bert"),MMr.forEach(t),DKe=r(fbe," \u2014 "),dB=s(fbe,"A",{href:!0});var yMr=n(dB);NKe=r(yMr,"MegatronBertModel"),yMr.forEach(t),jKe=r(fbe," (MegatronBert model)"),fbe.forEach(t),OKe=i(E),qh=s(E,"LI",{});var hbe=n(qh);yW=s(hbe,"STRONG",{});var wMr=n(yW);GKe=r(wMr,"mobilebert"),wMr.forEach(t),qKe=r(hbe," \u2014 "),cB=s(hbe,"A",{href:!0});var AMr=n(cB);zKe=r(AMr,"MobileBertModel"),AMr.forEach(t),XKe=r(hbe," (MobileBERT model)"),hbe.forEach(t),QKe=i(E),zh=s(E,"LI",{});var gbe=n(zh);wW=s(gbe,"STRONG",{});var LMr=n(wW);VKe=r(LMr,"mpnet"),LMr.forEach(t),WKe=r(gbe," \u2014 "),mB=s(gbe,"A",{href:!0});var BMr=n(mB);HKe=r(BMr,"MPNetModel"),BMr.forEach(t),UKe=r(gbe," (MPNet model)"),gbe.forEach(t),JKe=i(E),Xh=s(E,"LI",{});var ube=n(Xh);AW=s(ube,"STRONG",{});var xMr=n(AW);KKe=r(xMr,"mt5"),xMr.forEach(t),YKe=r(ube," \u2014 "),fB=s(ube,"A",{href:!0});var kMr=n(fB);ZKe=r(kMr,"MT5Model"),kMr.forEach(t),eYe=r(ube," (mT5 model)"),ube.forEach(t),oYe=i(E),Qh=s(E,"LI",{});var pbe=n(Qh);LW=s(pbe,"STRONG",{});var RMr=n(LW);rYe=r(RMr,"openai-gpt"),RMr.forEach(t),tYe=r(pbe," \u2014 "),hB=s(pbe,"A",{href:!0});var PMr=n(hB);aYe=r(PMr,"OpenAIGPTModel"),PMr.forEach(t),sYe=r(pbe," (OpenAI GPT model)"),pbe.forEach(t),nYe=i(E),Vh=s(E,"LI",{});var _be=n(Vh);BW=s(_be,"STRONG",{});var SMr=n(BW);lYe=r(SMr,"pegasus"),SMr.forEach(t),iYe=r(_be," \u2014 "),gB=s(_be,"A",{href:!0});var $Mr=n(gB);dYe=r($Mr,"PegasusModel"),$Mr.forEach(t),cYe=r(_be," (Pegasus model)"),_be.forEach(t),mYe=i(E),Wh=s(E,"LI",{});var bbe=n(Wh);xW=s(bbe,"STRONG",{});var IMr=n(xW);fYe=r(IMr,"perceiver"),IMr.forEach(t),hYe=r(bbe," \u2014 "),uB=s(bbe,"A",{href:!0});var DMr=n(uB);gYe=r(DMr,"PerceiverModel"),DMr.forEach(t),uYe=r(bbe," (Perceiver model)"),bbe.forEach(t),pYe=i(E),Hh=s(E,"LI",{});var vbe=n(Hh);kW=s(vbe,"STRONG",{});var NMr=n(kW);_Ye=r(NMr,"prophetnet"),NMr.forEach(t),bYe=r(vbe," \u2014 "),pB=s(vbe,"A",{href:!0});var jMr=n(pB);vYe=r(jMr,"ProphetNetModel"),jMr.forEach(t),TYe=r(vbe," (ProphetNet model)"),vbe.forEach(t),FYe=i(E),Uh=s(E,"LI",{});var Tbe=n(Uh);RW=s(Tbe,"STRONG",{});var OMr=n(RW);EYe=r(OMr,"qdqbert"),OMr.forEach(t),CYe=r(Tbe," \u2014 "),_B=s(Tbe,"A",{href:!0});var GMr=n(_B);MYe=r(GMr,"QDQBertModel"),GMr.forEach(t),yYe=r(Tbe," (QDQBert model)"),Tbe.forEach(t),wYe=i(E),Jh=s(E,"LI",{});var Fbe=n(Jh);PW=s(Fbe,"STRONG",{});var qMr=n(PW);AYe=r(qMr,"reformer"),qMr.forEach(t),LYe=r(Fbe," \u2014 "),bB=s(Fbe,"A",{href:!0});var zMr=n(bB);BYe=r(zMr,"ReformerModel"),zMr.forEach(t),xYe=r(Fbe," (Reformer model)"),Fbe.forEach(t),kYe=i(E),Kh=s(E,"LI",{});var Ebe=n(Kh);SW=s(Ebe,"STRONG",{});var XMr=n(SW);RYe=r(XMr,"rembert"),XMr.forEach(t),PYe=r(Ebe," \u2014 "),vB=s(Ebe,"A",{href:!0});var QMr=n(vB);SYe=r(QMr,"RemBertModel"),QMr.forEach(t),$Ye=r(Ebe," (RemBERT model)"),Ebe.forEach(t),IYe=i(E),Yh=s(E,"LI",{});var Cbe=n(Yh);$W=s(Cbe,"STRONG",{});var VMr=n($W);DYe=r(VMr,"retribert"),VMr.forEach(t),NYe=r(Cbe," \u2014 "),TB=s(Cbe,"A",{href:!0});var WMr=n(TB);jYe=r(WMr,"RetriBertModel"),WMr.forEach(t),OYe=r(Cbe," (RetriBERT model)"),Cbe.forEach(t),GYe=i(E),Zh=s(E,"LI",{});var Mbe=n(Zh);IW=s(Mbe,"STRONG",{});var HMr=n(IW);qYe=r(HMr,"roberta"),HMr.forEach(t),zYe=r(Mbe," \u2014 "),FB=s(Mbe,"A",{href:!0});var UMr=n(FB);XYe=r(UMr,"RobertaModel"),UMr.forEach(t),QYe=r(Mbe," (RoBERTa model)"),Mbe.forEach(t),VYe=i(E),eg=s(E,"LI",{});var ybe=n(eg);DW=s(ybe,"STRONG",{});var JMr=n(DW);WYe=r(JMr,"roformer"),JMr.forEach(t),HYe=r(ybe," \u2014 "),EB=s(ybe,"A",{href:!0});var KMr=n(EB);UYe=r(KMr,"RoFormerModel"),KMr.forEach(t),JYe=r(ybe," (RoFormer model)"),ybe.forEach(t),KYe=i(E),og=s(E,"LI",{});var wbe=n(og);NW=s(wbe,"STRONG",{});var YMr=n(NW);YYe=r(YMr,"segformer"),YMr.forEach(t),ZYe=r(wbe," \u2014 "),CB=s(wbe,"A",{href:!0});var ZMr=n(CB);eZe=r(ZMr,"SegformerModel"),ZMr.forEach(t),oZe=r(wbe," (SegFormer model)"),wbe.forEach(t),rZe=i(E),rg=s(E,"LI",{});var Abe=n(rg);jW=s(Abe,"STRONG",{});var e5r=n(jW);tZe=r(e5r,"sew"),e5r.forEach(t),aZe=r(Abe," \u2014 "),MB=s(Abe,"A",{href:!0});var o5r=n(MB);sZe=r(o5r,"SEWModel"),o5r.forEach(t),nZe=r(Abe," (SEW model)"),Abe.forEach(t),lZe=i(E),tg=s(E,"LI",{});var Lbe=n(tg);OW=s(Lbe,"STRONG",{});var r5r=n(OW);iZe=r(r5r,"sew-d"),r5r.forEach(t),dZe=r(Lbe," \u2014 "),yB=s(Lbe,"A",{href:!0});var t5r=n(yB);cZe=r(t5r,"SEWDModel"),t5r.forEach(t),mZe=r(Lbe," (SEW-D model)"),Lbe.forEach(t),fZe=i(E),ag=s(E,"LI",{});var Bbe=n(ag);GW=s(Bbe,"STRONG",{});var a5r=n(GW);hZe=r(a5r,"speech_to_text"),a5r.forEach(t),gZe=r(Bbe," \u2014 "),wB=s(Bbe,"A",{href:!0});var s5r=n(wB);uZe=r(s5r,"Speech2TextModel"),s5r.forEach(t),pZe=r(Bbe," (Speech2Text model)"),Bbe.forEach(t),_Ze=i(E),sg=s(E,"LI",{});var xbe=n(sg);qW=s(xbe,"STRONG",{});var n5r=n(qW);bZe=r(n5r,"splinter"),n5r.forEach(t),vZe=r(xbe," \u2014 "),AB=s(xbe,"A",{href:!0});var l5r=n(AB);TZe=r(l5r,"SplinterModel"),l5r.forEach(t),FZe=r(xbe," (Splinter model)"),xbe.forEach(t),EZe=i(E),ng=s(E,"LI",{});var kbe=n(ng);zW=s(kbe,"STRONG",{});var i5r=n(zW);CZe=r(i5r,"squeezebert"),i5r.forEach(t),MZe=r(kbe," \u2014 "),LB=s(kbe,"A",{href:!0});var d5r=n(LB);yZe=r(d5r,"SqueezeBertModel"),d5r.forEach(t),wZe=r(kbe," (SqueezeBERT model)"),kbe.forEach(t),AZe=i(E),lg=s(E,"LI",{});var Rbe=n(lg);XW=s(Rbe,"STRONG",{});var c5r=n(XW);LZe=r(c5r,"t5"),c5r.forEach(t),BZe=r(Rbe," \u2014 "),BB=s(Rbe,"A",{href:!0});var m5r=n(BB);xZe=r(m5r,"T5Model"),m5r.forEach(t),kZe=r(Rbe," (T5 model)"),Rbe.forEach(t),RZe=i(E),ig=s(E,"LI",{});var Pbe=n(ig);QW=s(Pbe,"STRONG",{});var f5r=n(QW);PZe=r(f5r,"tapas"),f5r.forEach(t),SZe=r(Pbe," \u2014 "),xB=s(Pbe,"A",{href:!0});var h5r=n(xB);$Ze=r(h5r,"TapasModel"),h5r.forEach(t),IZe=r(Pbe," (TAPAS model)"),Pbe.forEach(t),DZe=i(E),dg=s(E,"LI",{});var Sbe=n(dg);VW=s(Sbe,"STRONG",{});var g5r=n(VW);NZe=r(g5r,"transfo-xl"),g5r.forEach(t),jZe=r(Sbe," \u2014 "),kB=s(Sbe,"A",{href:!0});var u5r=n(kB);OZe=r(u5r,"TransfoXLModel"),u5r.forEach(t),GZe=r(Sbe," (Transformer-XL model)"),Sbe.forEach(t),qZe=i(E),cg=s(E,"LI",{});var $be=n(cg);WW=s($be,"STRONG",{});var p5r=n(WW);zZe=r(p5r,"unispeech"),p5r.forEach(t),XZe=r($be," \u2014 "),RB=s($be,"A",{href:!0});var _5r=n(RB);QZe=r(_5r,"UniSpeechModel"),_5r.forEach(t),VZe=r($be," (UniSpeech model)"),$be.forEach(t),WZe=i(E),mg=s(E,"LI",{});var Ibe=n(mg);HW=s(Ibe,"STRONG",{});var b5r=n(HW);HZe=r(b5r,"unispeech-sat"),b5r.forEach(t),UZe=r(Ibe," \u2014 "),PB=s(Ibe,"A",{href:!0});var v5r=n(PB);JZe=r(v5r,"UniSpeechSatModel"),v5r.forEach(t),KZe=r(Ibe," (UniSpeechSat model)"),Ibe.forEach(t),YZe=i(E),fg=s(E,"LI",{});var Dbe=n(fg);UW=s(Dbe,"STRONG",{});var T5r=n(UW);ZZe=r(T5r,"vision-text-dual-encoder"),T5r.forEach(t),eeo=r(Dbe," \u2014 "),SB=s(Dbe,"A",{href:!0});var F5r=n(SB);oeo=r(F5r,"VisionTextDualEncoderModel"),F5r.forEach(t),reo=r(Dbe," (VisionTextDualEncoder model)"),Dbe.forEach(t),teo=i(E),hg=s(E,"LI",{});var Nbe=n(hg);JW=s(Nbe,"STRONG",{});var E5r=n(JW);aeo=r(E5r,"visual_bert"),E5r.forEach(t),seo=r(Nbe," \u2014 "),$B=s(Nbe,"A",{href:!0});var C5r=n($B);neo=r(C5r,"VisualBertModel"),C5r.forEach(t),leo=r(Nbe," (VisualBert model)"),Nbe.forEach(t),ieo=i(E),gg=s(E,"LI",{});var jbe=n(gg);KW=s(jbe,"STRONG",{});var M5r=n(KW);deo=r(M5r,"vit"),M5r.forEach(t),ceo=r(jbe," \u2014 "),IB=s(jbe,"A",{href:!0});var y5r=n(IB);meo=r(y5r,"ViTModel"),y5r.forEach(t),feo=r(jbe," (ViT model)"),jbe.forEach(t),heo=i(E),ug=s(E,"LI",{});var Obe=n(ug);YW=s(Obe,"STRONG",{});var w5r=n(YW);geo=r(w5r,"wav2vec2"),w5r.forEach(t),ueo=r(Obe," \u2014 "),DB=s(Obe,"A",{href:!0});var A5r=n(DB);peo=r(A5r,"Wav2Vec2Model"),A5r.forEach(t),_eo=r(Obe," (Wav2Vec2 model)"),Obe.forEach(t),beo=i(E),pg=s(E,"LI",{});var Gbe=n(pg);ZW=s(Gbe,"STRONG",{});var L5r=n(ZW);veo=r(L5r,"xlm"),L5r.forEach(t),Teo=r(Gbe," \u2014 "),NB=s(Gbe,"A",{href:!0});var B5r=n(NB);Feo=r(B5r,"XLMModel"),B5r.forEach(t),Eeo=r(Gbe," (XLM model)"),Gbe.forEach(t),Ceo=i(E),_g=s(E,"LI",{});var qbe=n(_g);eH=s(qbe,"STRONG",{});var x5r=n(eH);Meo=r(x5r,"xlm-prophetnet"),x5r.forEach(t),yeo=r(qbe," \u2014 "),jB=s(qbe,"A",{href:!0});var k5r=n(jB);weo=r(k5r,"XLMProphetNetModel"),k5r.forEach(t),Aeo=r(qbe," (XLMProphetNet model)"),qbe.forEach(t),Leo=i(E),bg=s(E,"LI",{});var zbe=n(bg);oH=s(zbe,"STRONG",{});var R5r=n(oH);Beo=r(R5r,"xlm-roberta"),R5r.forEach(t),xeo=r(zbe," \u2014 "),OB=s(zbe,"A",{href:!0});var P5r=n(OB);keo=r(P5r,"XLMRobertaModel"),P5r.forEach(t),Reo=r(zbe," (XLM-RoBERTa model)"),zbe.forEach(t),Peo=i(E),vg=s(E,"LI",{});var Xbe=n(vg);rH=s(Xbe,"STRONG",{});var S5r=n(rH);Seo=r(S5r,"xlnet"),S5r.forEach(t),$eo=r(Xbe," \u2014 "),GB=s(Xbe,"A",{href:!0});var $5r=n(GB);Ieo=r($5r,"XLNetModel"),$5r.forEach(t),Deo=r(Xbe," (XLNet model)"),Xbe.forEach(t),E.forEach(t),Neo=i(vt),Tg=s(vt,"P",{});var Qbe=n(Tg);jeo=r(Qbe,"The model is set in evaluation mode by default using "),tH=s(Qbe,"CODE",{});var I5r=n(tH);Oeo=r(I5r,"model.eval()"),I5r.forEach(t),Geo=r(Qbe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),aH=s(Qbe,"CODE",{});var D5r=n(aH);qeo=r(D5r,"model.train()"),D5r.forEach(t),Qbe.forEach(t),zeo=i(vt),sH=s(vt,"P",{});var N5r=n(sH);Xeo=r(N5r,"Examples:"),N5r.forEach(t),Qeo=i(vt),f(g3.$$.fragment,vt),vt.forEach(t),hn.forEach(t),v5e=i(d),gi=s(d,"H2",{class:!0});var wwe=n(gi);Fg=s(wwe,"A",{id:!0,class:!0,href:!0});var j5r=n(Fg);nH=s(j5r,"SPAN",{});var O5r=n(nH);f(u3.$$.fragment,O5r),O5r.forEach(t),j5r.forEach(t),Veo=i(wwe),lH=s(wwe,"SPAN",{});var G5r=n(lH);Weo=r(G5r,"AutoModelForPreTraining"),G5r.forEach(t),wwe.forEach(t),T5e=i(d),Oo=s(d,"DIV",{class:!0});var un=n(Oo);f(p3.$$.fragment,un),Heo=i(un),ui=s(un,"P",{});var rj=n(ui);Ueo=r(rj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),iH=s(rj,"CODE",{});var q5r=n(iH);Jeo=r(q5r,"from_pretrained()"),q5r.forEach(t),Keo=r(rj,` class method or the
`),dH=s(rj,"CODE",{});var z5r=n(dH);Yeo=r(z5r,"from_config()"),z5r.forEach(t),Zeo=r(rj," class method."),rj.forEach(t),eoo=i(un),_3=s(un,"P",{});var Awe=n(_3);ooo=r(Awe,"This class cannot be instantiated directly using "),cH=s(Awe,"CODE",{});var X5r=n(cH);roo=r(X5r,"__init__()"),X5r.forEach(t),too=r(Awe," (throws an error)."),Awe.forEach(t),aoo=i(un),Rr=s(un,"DIV",{class:!0});var pn=n(Rr);f(b3.$$.fragment,pn),soo=i(pn),mH=s(pn,"P",{});var Q5r=n(mH);noo=r(Q5r,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Q5r.forEach(t),loo=i(pn),pi=s(pn,"P",{});var tj=n(pi);ioo=r(tj,`Note:
Loading a model from its configuration file does `),fH=s(tj,"STRONG",{});var V5r=n(fH);doo=r(V5r,"not"),V5r.forEach(t),coo=r(tj,` load the model weights. It only affects the
model\u2019s configuration. Use `),hH=s(tj,"CODE",{});var W5r=n(hH);moo=r(W5r,"from_pretrained()"),W5r.forEach(t),foo=r(tj,` to load the model
weights.`),tj.forEach(t),hoo=i(pn),gH=s(pn,"P",{});var H5r=n(gH);goo=r(H5r,"Examples:"),H5r.forEach(t),uoo=i(pn),f(v3.$$.fragment,pn),pn.forEach(t),poo=i(un),$e=s(un,"DIV",{class:!0});var Tt=n($e);f(T3.$$.fragment,Tt),_oo=i(Tt),uH=s(Tt,"P",{});var U5r=n(uH);boo=r(U5r,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),U5r.forEach(t),voo=i(Tt),Aa=s(Tt,"P",{});var x4=n(Aa);Too=r(x4,"The model class to instantiate is selected based on the "),pH=s(x4,"CODE",{});var J5r=n(pH);Foo=r(J5r,"model_type"),J5r.forEach(t),Eoo=r(x4,` property of the config object (either
passed as an argument or loaded from `),_H=s(x4,"CODE",{});var K5r=n(_H);Coo=r(K5r,"pretrained_model_name_or_path"),K5r.forEach(t),Moo=r(x4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),bH=s(x4,"CODE",{});var Y5r=n(bH);yoo=r(Y5r,"pretrained_model_name_or_path"),Y5r.forEach(t),woo=r(x4,":"),x4.forEach(t),Aoo=i(Tt),R=s(Tt,"UL",{});var $=n(R);Eg=s($,"LI",{});var Vbe=n(Eg);vH=s(Vbe,"STRONG",{});var Z5r=n(vH);Loo=r(Z5r,"albert"),Z5r.forEach(t),Boo=r(Vbe," \u2014 "),qB=s(Vbe,"A",{href:!0});var eyr=n(qB);xoo=r(eyr,"AlbertForPreTraining"),eyr.forEach(t),koo=r(Vbe," (ALBERT model)"),Vbe.forEach(t),Roo=i($),Cg=s($,"LI",{});var Wbe=n(Cg);TH=s(Wbe,"STRONG",{});var oyr=n(TH);Poo=r(oyr,"bart"),oyr.forEach(t),Soo=r(Wbe," \u2014 "),zB=s(Wbe,"A",{href:!0});var ryr=n(zB);$oo=r(ryr,"BartForConditionalGeneration"),ryr.forEach(t),Ioo=r(Wbe," (BART model)"),Wbe.forEach(t),Doo=i($),Mg=s($,"LI",{});var Hbe=n(Mg);FH=s(Hbe,"STRONG",{});var tyr=n(FH);Noo=r(tyr,"bert"),tyr.forEach(t),joo=r(Hbe," \u2014 "),XB=s(Hbe,"A",{href:!0});var ayr=n(XB);Ooo=r(ayr,"BertForPreTraining"),ayr.forEach(t),Goo=r(Hbe," (BERT model)"),Hbe.forEach(t),qoo=i($),yg=s($,"LI",{});var Ube=n(yg);EH=s(Ube,"STRONG",{});var syr=n(EH);zoo=r(syr,"big_bird"),syr.forEach(t),Xoo=r(Ube," \u2014 "),QB=s(Ube,"A",{href:!0});var nyr=n(QB);Qoo=r(nyr,"BigBirdForPreTraining"),nyr.forEach(t),Voo=r(Ube," (BigBird model)"),Ube.forEach(t),Woo=i($),wg=s($,"LI",{});var Jbe=n(wg);CH=s(Jbe,"STRONG",{});var lyr=n(CH);Hoo=r(lyr,"camembert"),lyr.forEach(t),Uoo=r(Jbe," \u2014 "),VB=s(Jbe,"A",{href:!0});var iyr=n(VB);Joo=r(iyr,"CamembertForMaskedLM"),iyr.forEach(t),Koo=r(Jbe," (CamemBERT model)"),Jbe.forEach(t),Yoo=i($),Ag=s($,"LI",{});var Kbe=n(Ag);MH=s(Kbe,"STRONG",{});var dyr=n(MH);Zoo=r(dyr,"ctrl"),dyr.forEach(t),ero=r(Kbe," \u2014 "),WB=s(Kbe,"A",{href:!0});var cyr=n(WB);oro=r(cyr,"CTRLLMHeadModel"),cyr.forEach(t),rro=r(Kbe," (CTRL model)"),Kbe.forEach(t),tro=i($),Lg=s($,"LI",{});var Ybe=n(Lg);yH=s(Ybe,"STRONG",{});var myr=n(yH);aro=r(myr,"deberta"),myr.forEach(t),sro=r(Ybe," \u2014 "),HB=s(Ybe,"A",{href:!0});var fyr=n(HB);nro=r(fyr,"DebertaForMaskedLM"),fyr.forEach(t),lro=r(Ybe," (DeBERTa model)"),Ybe.forEach(t),iro=i($),Bg=s($,"LI",{});var Zbe=n(Bg);wH=s(Zbe,"STRONG",{});var hyr=n(wH);dro=r(hyr,"deberta-v2"),hyr.forEach(t),cro=r(Zbe," \u2014 "),UB=s(Zbe,"A",{href:!0});var gyr=n(UB);mro=r(gyr,"DebertaV2ForMaskedLM"),gyr.forEach(t),fro=r(Zbe," (DeBERTa-v2 model)"),Zbe.forEach(t),hro=i($),xg=s($,"LI",{});var e2e=n(xg);AH=s(e2e,"STRONG",{});var uyr=n(AH);gro=r(uyr,"distilbert"),uyr.forEach(t),uro=r(e2e," \u2014 "),JB=s(e2e,"A",{href:!0});var pyr=n(JB);pro=r(pyr,"DistilBertForMaskedLM"),pyr.forEach(t),_ro=r(e2e," (DistilBERT model)"),e2e.forEach(t),bro=i($),kg=s($,"LI",{});var o2e=n(kg);LH=s(o2e,"STRONG",{});var _yr=n(LH);vro=r(_yr,"electra"),_yr.forEach(t),Tro=r(o2e," \u2014 "),KB=s(o2e,"A",{href:!0});var byr=n(KB);Fro=r(byr,"ElectraForPreTraining"),byr.forEach(t),Ero=r(o2e," (ELECTRA model)"),o2e.forEach(t),Cro=i($),Rg=s($,"LI",{});var r2e=n(Rg);BH=s(r2e,"STRONG",{});var vyr=n(BH);Mro=r(vyr,"flaubert"),vyr.forEach(t),yro=r(r2e," \u2014 "),YB=s(r2e,"A",{href:!0});var Tyr=n(YB);wro=r(Tyr,"FlaubertWithLMHeadModel"),Tyr.forEach(t),Aro=r(r2e," (FlauBERT model)"),r2e.forEach(t),Lro=i($),Pg=s($,"LI",{});var t2e=n(Pg);xH=s(t2e,"STRONG",{});var Fyr=n(xH);Bro=r(Fyr,"fnet"),Fyr.forEach(t),xro=r(t2e," \u2014 "),ZB=s(t2e,"A",{href:!0});var Eyr=n(ZB);kro=r(Eyr,"FNetForPreTraining"),Eyr.forEach(t),Rro=r(t2e," (FNet model)"),t2e.forEach(t),Pro=i($),Sg=s($,"LI",{});var a2e=n(Sg);kH=s(a2e,"STRONG",{});var Cyr=n(kH);Sro=r(Cyr,"fsmt"),Cyr.forEach(t),$ro=r(a2e," \u2014 "),e9=s(a2e,"A",{href:!0});var Myr=n(e9);Iro=r(Myr,"FSMTForConditionalGeneration"),Myr.forEach(t),Dro=r(a2e," (FairSeq Machine-Translation model)"),a2e.forEach(t),Nro=i($),$g=s($,"LI",{});var s2e=n($g);RH=s(s2e,"STRONG",{});var yyr=n(RH);jro=r(yyr,"funnel"),yyr.forEach(t),Oro=r(s2e," \u2014 "),o9=s(s2e,"A",{href:!0});var wyr=n(o9);Gro=r(wyr,"FunnelForPreTraining"),wyr.forEach(t),qro=r(s2e," (Funnel Transformer model)"),s2e.forEach(t),zro=i($),Ig=s($,"LI",{});var n2e=n(Ig);PH=s(n2e,"STRONG",{});var Ayr=n(PH);Xro=r(Ayr,"gpt2"),Ayr.forEach(t),Qro=r(n2e," \u2014 "),r9=s(n2e,"A",{href:!0});var Lyr=n(r9);Vro=r(Lyr,"GPT2LMHeadModel"),Lyr.forEach(t),Wro=r(n2e," (OpenAI GPT-2 model)"),n2e.forEach(t),Hro=i($),Dg=s($,"LI",{});var l2e=n(Dg);SH=s(l2e,"STRONG",{});var Byr=n(SH);Uro=r(Byr,"ibert"),Byr.forEach(t),Jro=r(l2e," \u2014 "),t9=s(l2e,"A",{href:!0});var xyr=n(t9);Kro=r(xyr,"IBertForMaskedLM"),xyr.forEach(t),Yro=r(l2e," (I-BERT model)"),l2e.forEach(t),Zro=i($),Ng=s($,"LI",{});var i2e=n(Ng);$H=s(i2e,"STRONG",{});var kyr=n($H);eto=r(kyr,"layoutlm"),kyr.forEach(t),oto=r(i2e," \u2014 "),a9=s(i2e,"A",{href:!0});var Ryr=n(a9);rto=r(Ryr,"LayoutLMForMaskedLM"),Ryr.forEach(t),tto=r(i2e," (LayoutLM model)"),i2e.forEach(t),ato=i($),jg=s($,"LI",{});var d2e=n(jg);IH=s(d2e,"STRONG",{});var Pyr=n(IH);sto=r(Pyr,"longformer"),Pyr.forEach(t),nto=r(d2e," \u2014 "),s9=s(d2e,"A",{href:!0});var Syr=n(s9);lto=r(Syr,"LongformerForMaskedLM"),Syr.forEach(t),ito=r(d2e," (Longformer model)"),d2e.forEach(t),dto=i($),Og=s($,"LI",{});var c2e=n(Og);DH=s(c2e,"STRONG",{});var $yr=n(DH);cto=r($yr,"lxmert"),$yr.forEach(t),mto=r(c2e," \u2014 "),n9=s(c2e,"A",{href:!0});var Iyr=n(n9);fto=r(Iyr,"LxmertForPreTraining"),Iyr.forEach(t),hto=r(c2e," (LXMERT model)"),c2e.forEach(t),gto=i($),Gg=s($,"LI",{});var m2e=n(Gg);NH=s(m2e,"STRONG",{});var Dyr=n(NH);uto=r(Dyr,"megatron-bert"),Dyr.forEach(t),pto=r(m2e," \u2014 "),l9=s(m2e,"A",{href:!0});var Nyr=n(l9);_to=r(Nyr,"MegatronBertForPreTraining"),Nyr.forEach(t),bto=r(m2e," (MegatronBert model)"),m2e.forEach(t),vto=i($),qg=s($,"LI",{});var f2e=n(qg);jH=s(f2e,"STRONG",{});var jyr=n(jH);Tto=r(jyr,"mobilebert"),jyr.forEach(t),Fto=r(f2e," \u2014 "),i9=s(f2e,"A",{href:!0});var Oyr=n(i9);Eto=r(Oyr,"MobileBertForPreTraining"),Oyr.forEach(t),Cto=r(f2e," (MobileBERT model)"),f2e.forEach(t),Mto=i($),zg=s($,"LI",{});var h2e=n(zg);OH=s(h2e,"STRONG",{});var Gyr=n(OH);yto=r(Gyr,"mpnet"),Gyr.forEach(t),wto=r(h2e," \u2014 "),d9=s(h2e,"A",{href:!0});var qyr=n(d9);Ato=r(qyr,"MPNetForMaskedLM"),qyr.forEach(t),Lto=r(h2e," (MPNet model)"),h2e.forEach(t),Bto=i($),Xg=s($,"LI",{});var g2e=n(Xg);GH=s(g2e,"STRONG",{});var zyr=n(GH);xto=r(zyr,"openai-gpt"),zyr.forEach(t),kto=r(g2e," \u2014 "),c9=s(g2e,"A",{href:!0});var Xyr=n(c9);Rto=r(Xyr,"OpenAIGPTLMHeadModel"),Xyr.forEach(t),Pto=r(g2e," (OpenAI GPT model)"),g2e.forEach(t),Sto=i($),Qg=s($,"LI",{});var u2e=n(Qg);qH=s(u2e,"STRONG",{});var Qyr=n(qH);$to=r(Qyr,"retribert"),Qyr.forEach(t),Ito=r(u2e," \u2014 "),m9=s(u2e,"A",{href:!0});var Vyr=n(m9);Dto=r(Vyr,"RetriBertModel"),Vyr.forEach(t),Nto=r(u2e," (RetriBERT model)"),u2e.forEach(t),jto=i($),Vg=s($,"LI",{});var p2e=n(Vg);zH=s(p2e,"STRONG",{});var Wyr=n(zH);Oto=r(Wyr,"roberta"),Wyr.forEach(t),Gto=r(p2e," \u2014 "),f9=s(p2e,"A",{href:!0});var Hyr=n(f9);qto=r(Hyr,"RobertaForMaskedLM"),Hyr.forEach(t),zto=r(p2e," (RoBERTa model)"),p2e.forEach(t),Xto=i($),Wg=s($,"LI",{});var _2e=n(Wg);XH=s(_2e,"STRONG",{});var Uyr=n(XH);Qto=r(Uyr,"squeezebert"),Uyr.forEach(t),Vto=r(_2e," \u2014 "),h9=s(_2e,"A",{href:!0});var Jyr=n(h9);Wto=r(Jyr,"SqueezeBertForMaskedLM"),Jyr.forEach(t),Hto=r(_2e," (SqueezeBERT model)"),_2e.forEach(t),Uto=i($),Hg=s($,"LI",{});var b2e=n(Hg);QH=s(b2e,"STRONG",{});var Kyr=n(QH);Jto=r(Kyr,"t5"),Kyr.forEach(t),Kto=r(b2e," \u2014 "),g9=s(b2e,"A",{href:!0});var Yyr=n(g9);Yto=r(Yyr,"T5ForConditionalGeneration"),Yyr.forEach(t),Zto=r(b2e," (T5 model)"),b2e.forEach(t),eao=i($),Ug=s($,"LI",{});var v2e=n(Ug);VH=s(v2e,"STRONG",{});var Zyr=n(VH);oao=r(Zyr,"tapas"),Zyr.forEach(t),rao=r(v2e," \u2014 "),u9=s(v2e,"A",{href:!0});var ewr=n(u9);tao=r(ewr,"TapasForMaskedLM"),ewr.forEach(t),aao=r(v2e," (TAPAS model)"),v2e.forEach(t),sao=i($),Jg=s($,"LI",{});var T2e=n(Jg);WH=s(T2e,"STRONG",{});var owr=n(WH);nao=r(owr,"transfo-xl"),owr.forEach(t),lao=r(T2e," \u2014 "),p9=s(T2e,"A",{href:!0});var rwr=n(p9);iao=r(rwr,"TransfoXLLMHeadModel"),rwr.forEach(t),dao=r(T2e," (Transformer-XL model)"),T2e.forEach(t),cao=i($),Kg=s($,"LI",{});var F2e=n(Kg);HH=s(F2e,"STRONG",{});var twr=n(HH);mao=r(twr,"unispeech"),twr.forEach(t),fao=r(F2e," \u2014 "),_9=s(F2e,"A",{href:!0});var awr=n(_9);hao=r(awr,"UniSpeechForPreTraining"),awr.forEach(t),gao=r(F2e," (UniSpeech model)"),F2e.forEach(t),uao=i($),Yg=s($,"LI",{});var E2e=n(Yg);UH=s(E2e,"STRONG",{});var swr=n(UH);pao=r(swr,"unispeech-sat"),swr.forEach(t),_ao=r(E2e," \u2014 "),b9=s(E2e,"A",{href:!0});var nwr=n(b9);bao=r(nwr,"UniSpeechSatForPreTraining"),nwr.forEach(t),vao=r(E2e," (UniSpeechSat model)"),E2e.forEach(t),Tao=i($),Zg=s($,"LI",{});var C2e=n(Zg);JH=s(C2e,"STRONG",{});var lwr=n(JH);Fao=r(lwr,"visual_bert"),lwr.forEach(t),Eao=r(C2e," \u2014 "),v9=s(C2e,"A",{href:!0});var iwr=n(v9);Cao=r(iwr,"VisualBertForPreTraining"),iwr.forEach(t),Mao=r(C2e," (VisualBert model)"),C2e.forEach(t),yao=i($),eu=s($,"LI",{});var M2e=n(eu);KH=s(M2e,"STRONG",{});var dwr=n(KH);wao=r(dwr,"wav2vec2"),dwr.forEach(t),Aao=r(M2e," \u2014 "),T9=s(M2e,"A",{href:!0});var cwr=n(T9);Lao=r(cwr,"Wav2Vec2ForPreTraining"),cwr.forEach(t),Bao=r(M2e," (Wav2Vec2 model)"),M2e.forEach(t),xao=i($),ou=s($,"LI",{});var y2e=n(ou);YH=s(y2e,"STRONG",{});var mwr=n(YH);kao=r(mwr,"xlm"),mwr.forEach(t),Rao=r(y2e," \u2014 "),F9=s(y2e,"A",{href:!0});var fwr=n(F9);Pao=r(fwr,"XLMWithLMHeadModel"),fwr.forEach(t),Sao=r(y2e," (XLM model)"),y2e.forEach(t),$ao=i($),ru=s($,"LI",{});var w2e=n(ru);ZH=s(w2e,"STRONG",{});var hwr=n(ZH);Iao=r(hwr,"xlm-roberta"),hwr.forEach(t),Dao=r(w2e," \u2014 "),E9=s(w2e,"A",{href:!0});var gwr=n(E9);Nao=r(gwr,"XLMRobertaForMaskedLM"),gwr.forEach(t),jao=r(w2e," (XLM-RoBERTa model)"),w2e.forEach(t),Oao=i($),tu=s($,"LI",{});var A2e=n(tu);eU=s(A2e,"STRONG",{});var uwr=n(eU);Gao=r(uwr,"xlnet"),uwr.forEach(t),qao=r(A2e," \u2014 "),C9=s(A2e,"A",{href:!0});var pwr=n(C9);zao=r(pwr,"XLNetLMHeadModel"),pwr.forEach(t),Xao=r(A2e," (XLNet model)"),A2e.forEach(t),$.forEach(t),Qao=i(Tt),au=s(Tt,"P",{});var L2e=n(au);Vao=r(L2e,"The model is set in evaluation mode by default using "),oU=s(L2e,"CODE",{});var _wr=n(oU);Wao=r(_wr,"model.eval()"),_wr.forEach(t),Hao=r(L2e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),rU=s(L2e,"CODE",{});var bwr=n(rU);Uao=r(bwr,"model.train()"),bwr.forEach(t),L2e.forEach(t),Jao=i(Tt),tU=s(Tt,"P",{});var vwr=n(tU);Kao=r(vwr,"Examples:"),vwr.forEach(t),Yao=i(Tt),f(F3.$$.fragment,Tt),Tt.forEach(t),un.forEach(t),F5e=i(d),_i=s(d,"H2",{class:!0});var Lwe=n(_i);su=s(Lwe,"A",{id:!0,class:!0,href:!0});var Twr=n(su);aU=s(Twr,"SPAN",{});var Fwr=n(aU);f(E3.$$.fragment,Fwr),Fwr.forEach(t),Twr.forEach(t),Zao=i(Lwe),sU=s(Lwe,"SPAN",{});var Ewr=n(sU);eso=r(Ewr,"AutoModelForCausalLM"),Ewr.forEach(t),Lwe.forEach(t),E5e=i(d),Go=s(d,"DIV",{class:!0});var _n=n(Go);f(C3.$$.fragment,_n),oso=i(_n),bi=s(_n,"P",{});var aj=n(bi);rso=r(aj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),nU=s(aj,"CODE",{});var Cwr=n(nU);tso=r(Cwr,"from_pretrained()"),Cwr.forEach(t),aso=r(aj,` class method or the
`),lU=s(aj,"CODE",{});var Mwr=n(lU);sso=r(Mwr,"from_config()"),Mwr.forEach(t),nso=r(aj," class method."),aj.forEach(t),lso=i(_n),M3=s(_n,"P",{});var Bwe=n(M3);iso=r(Bwe,"This class cannot be instantiated directly using "),iU=s(Bwe,"CODE",{});var ywr=n(iU);dso=r(ywr,"__init__()"),ywr.forEach(t),cso=r(Bwe," (throws an error)."),Bwe.forEach(t),mso=i(_n),Pr=s(_n,"DIV",{class:!0});var bn=n(Pr);f(y3.$$.fragment,bn),fso=i(bn),dU=s(bn,"P",{});var wwr=n(dU);hso=r(wwr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),wwr.forEach(t),gso=i(bn),vi=s(bn,"P",{});var sj=n(vi);uso=r(sj,`Note:
Loading a model from its configuration file does `),cU=s(sj,"STRONG",{});var Awr=n(cU);pso=r(Awr,"not"),Awr.forEach(t),_so=r(sj,` load the model weights. It only affects the
model\u2019s configuration. Use `),mU=s(sj,"CODE",{});var Lwr=n(mU);bso=r(Lwr,"from_pretrained()"),Lwr.forEach(t),vso=r(sj,` to load the model
weights.`),sj.forEach(t),Tso=i(bn),fU=s(bn,"P",{});var Bwr=n(fU);Fso=r(Bwr,"Examples:"),Bwr.forEach(t),Eso=i(bn),f(w3.$$.fragment,bn),bn.forEach(t),Cso=i(_n),Ie=s(_n,"DIV",{class:!0});var Ft=n(Ie);f(A3.$$.fragment,Ft),Mso=i(Ft),hU=s(Ft,"P",{});var xwr=n(hU);yso=r(xwr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),xwr.forEach(t),wso=i(Ft),La=s(Ft,"P",{});var k4=n(La);Aso=r(k4,"The model class to instantiate is selected based on the "),gU=s(k4,"CODE",{});var kwr=n(gU);Lso=r(kwr,"model_type"),kwr.forEach(t),Bso=r(k4,` property of the config object (either
passed as an argument or loaded from `),uU=s(k4,"CODE",{});var Rwr=n(uU);xso=r(Rwr,"pretrained_model_name_or_path"),Rwr.forEach(t),kso=r(k4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),pU=s(k4,"CODE",{});var Pwr=n(pU);Rso=r(Pwr,"pretrained_model_name_or_path"),Pwr.forEach(t),Pso=r(k4,":"),k4.forEach(t),Sso=i(Ft),q=s(Ft,"UL",{});var Q=n(q);nu=s(Q,"LI",{});var B2e=n(nu);_U=s(B2e,"STRONG",{});var Swr=n(_U);$so=r(Swr,"bart"),Swr.forEach(t),Iso=r(B2e," \u2014 "),M9=s(B2e,"A",{href:!0});var $wr=n(M9);Dso=r($wr,"BartForCausalLM"),$wr.forEach(t),Nso=r(B2e," (BART model)"),B2e.forEach(t),jso=i(Q),lu=s(Q,"LI",{});var x2e=n(lu);bU=s(x2e,"STRONG",{});var Iwr=n(bU);Oso=r(Iwr,"bert"),Iwr.forEach(t),Gso=r(x2e," \u2014 "),y9=s(x2e,"A",{href:!0});var Dwr=n(y9);qso=r(Dwr,"BertLMHeadModel"),Dwr.forEach(t),zso=r(x2e," (BERT model)"),x2e.forEach(t),Xso=i(Q),iu=s(Q,"LI",{});var k2e=n(iu);vU=s(k2e,"STRONG",{});var Nwr=n(vU);Qso=r(Nwr,"bert-generation"),Nwr.forEach(t),Vso=r(k2e," \u2014 "),w9=s(k2e,"A",{href:!0});var jwr=n(w9);Wso=r(jwr,"BertGenerationDecoder"),jwr.forEach(t),Hso=r(k2e," (Bert Generation model)"),k2e.forEach(t),Uso=i(Q),du=s(Q,"LI",{});var R2e=n(du);TU=s(R2e,"STRONG",{});var Owr=n(TU);Jso=r(Owr,"big_bird"),Owr.forEach(t),Kso=r(R2e," \u2014 "),A9=s(R2e,"A",{href:!0});var Gwr=n(A9);Yso=r(Gwr,"BigBirdForCausalLM"),Gwr.forEach(t),Zso=r(R2e," (BigBird model)"),R2e.forEach(t),eno=i(Q),cu=s(Q,"LI",{});var P2e=n(cu);FU=s(P2e,"STRONG",{});var qwr=n(FU);ono=r(qwr,"bigbird_pegasus"),qwr.forEach(t),rno=r(P2e," \u2014 "),L9=s(P2e,"A",{href:!0});var zwr=n(L9);tno=r(zwr,"BigBirdPegasusForCausalLM"),zwr.forEach(t),ano=r(P2e," (BigBirdPegasus model)"),P2e.forEach(t),sno=i(Q),mu=s(Q,"LI",{});var S2e=n(mu);EU=s(S2e,"STRONG",{});var Xwr=n(EU);nno=r(Xwr,"blenderbot"),Xwr.forEach(t),lno=r(S2e," \u2014 "),B9=s(S2e,"A",{href:!0});var Qwr=n(B9);ino=r(Qwr,"BlenderbotForCausalLM"),Qwr.forEach(t),dno=r(S2e," (Blenderbot model)"),S2e.forEach(t),cno=i(Q),fu=s(Q,"LI",{});var $2e=n(fu);CU=s($2e,"STRONG",{});var Vwr=n(CU);mno=r(Vwr,"blenderbot-small"),Vwr.forEach(t),fno=r($2e," \u2014 "),x9=s($2e,"A",{href:!0});var Wwr=n(x9);hno=r(Wwr,"BlenderbotSmallForCausalLM"),Wwr.forEach(t),gno=r($2e," (BlenderbotSmall model)"),$2e.forEach(t),uno=i(Q),hu=s(Q,"LI",{});var I2e=n(hu);MU=s(I2e,"STRONG",{});var Hwr=n(MU);pno=r(Hwr,"camembert"),Hwr.forEach(t),_no=r(I2e," \u2014 "),k9=s(I2e,"A",{href:!0});var Uwr=n(k9);bno=r(Uwr,"CamembertForCausalLM"),Uwr.forEach(t),vno=r(I2e," (CamemBERT model)"),I2e.forEach(t),Tno=i(Q),gu=s(Q,"LI",{});var D2e=n(gu);yU=s(D2e,"STRONG",{});var Jwr=n(yU);Fno=r(Jwr,"ctrl"),Jwr.forEach(t),Eno=r(D2e," \u2014 "),R9=s(D2e,"A",{href:!0});var Kwr=n(R9);Cno=r(Kwr,"CTRLLMHeadModel"),Kwr.forEach(t),Mno=r(D2e," (CTRL model)"),D2e.forEach(t),yno=i(Q),uu=s(Q,"LI",{});var N2e=n(uu);wU=s(N2e,"STRONG",{});var Ywr=n(wU);wno=r(Ywr,"gpt2"),Ywr.forEach(t),Ano=r(N2e," \u2014 "),P9=s(N2e,"A",{href:!0});var Zwr=n(P9);Lno=r(Zwr,"GPT2LMHeadModel"),Zwr.forEach(t),Bno=r(N2e," (OpenAI GPT-2 model)"),N2e.forEach(t),xno=i(Q),pu=s(Q,"LI",{});var j2e=n(pu);AU=s(j2e,"STRONG",{});var e7r=n(AU);kno=r(e7r,"gpt_neo"),e7r.forEach(t),Rno=r(j2e," \u2014 "),S9=s(j2e,"A",{href:!0});var o7r=n(S9);Pno=r(o7r,"GPTNeoForCausalLM"),o7r.forEach(t),Sno=r(j2e," (GPT Neo model)"),j2e.forEach(t),$no=i(Q),_u=s(Q,"LI",{});var O2e=n(_u);LU=s(O2e,"STRONG",{});var r7r=n(LU);Ino=r(r7r,"gptj"),r7r.forEach(t),Dno=r(O2e," \u2014 "),$9=s(O2e,"A",{href:!0});var t7r=n($9);Nno=r(t7r,"GPTJForCausalLM"),t7r.forEach(t),jno=r(O2e," (GPT-J model)"),O2e.forEach(t),Ono=i(Q),bu=s(Q,"LI",{});var G2e=n(bu);BU=s(G2e,"STRONG",{});var a7r=n(BU);Gno=r(a7r,"marian"),a7r.forEach(t),qno=r(G2e," \u2014 "),I9=s(G2e,"A",{href:!0});var s7r=n(I9);zno=r(s7r,"MarianForCausalLM"),s7r.forEach(t),Xno=r(G2e," (Marian model)"),G2e.forEach(t),Qno=i(Q),vu=s(Q,"LI",{});var q2e=n(vu);xU=s(q2e,"STRONG",{});var n7r=n(xU);Vno=r(n7r,"mbart"),n7r.forEach(t),Wno=r(q2e," \u2014 "),D9=s(q2e,"A",{href:!0});var l7r=n(D9);Hno=r(l7r,"MBartForCausalLM"),l7r.forEach(t),Uno=r(q2e," (mBART model)"),q2e.forEach(t),Jno=i(Q),Tu=s(Q,"LI",{});var z2e=n(Tu);kU=s(z2e,"STRONG",{});var i7r=n(kU);Kno=r(i7r,"megatron-bert"),i7r.forEach(t),Yno=r(z2e," \u2014 "),N9=s(z2e,"A",{href:!0});var d7r=n(N9);Zno=r(d7r,"MegatronBertForCausalLM"),d7r.forEach(t),elo=r(z2e," (MegatronBert model)"),z2e.forEach(t),olo=i(Q),Fu=s(Q,"LI",{});var X2e=n(Fu);RU=s(X2e,"STRONG",{});var c7r=n(RU);rlo=r(c7r,"openai-gpt"),c7r.forEach(t),tlo=r(X2e," \u2014 "),j9=s(X2e,"A",{href:!0});var m7r=n(j9);alo=r(m7r,"OpenAIGPTLMHeadModel"),m7r.forEach(t),slo=r(X2e," (OpenAI GPT model)"),X2e.forEach(t),nlo=i(Q),Eu=s(Q,"LI",{});var Q2e=n(Eu);PU=s(Q2e,"STRONG",{});var f7r=n(PU);llo=r(f7r,"pegasus"),f7r.forEach(t),ilo=r(Q2e," \u2014 "),O9=s(Q2e,"A",{href:!0});var h7r=n(O9);dlo=r(h7r,"PegasusForCausalLM"),h7r.forEach(t),clo=r(Q2e," (Pegasus model)"),Q2e.forEach(t),mlo=i(Q),Cu=s(Q,"LI",{});var V2e=n(Cu);SU=s(V2e,"STRONG",{});var g7r=n(SU);flo=r(g7r,"prophetnet"),g7r.forEach(t),hlo=r(V2e," \u2014 "),G9=s(V2e,"A",{href:!0});var u7r=n(G9);glo=r(u7r,"ProphetNetForCausalLM"),u7r.forEach(t),ulo=r(V2e," (ProphetNet model)"),V2e.forEach(t),plo=i(Q),Mu=s(Q,"LI",{});var W2e=n(Mu);$U=s(W2e,"STRONG",{});var p7r=n($U);_lo=r(p7r,"qdqbert"),p7r.forEach(t),blo=r(W2e," \u2014 "),q9=s(W2e,"A",{href:!0});var _7r=n(q9);vlo=r(_7r,"QDQBertLMHeadModel"),_7r.forEach(t),Tlo=r(W2e," (QDQBert model)"),W2e.forEach(t),Flo=i(Q),yu=s(Q,"LI",{});var H2e=n(yu);IU=s(H2e,"STRONG",{});var b7r=n(IU);Elo=r(b7r,"reformer"),b7r.forEach(t),Clo=r(H2e," \u2014 "),z9=s(H2e,"A",{href:!0});var v7r=n(z9);Mlo=r(v7r,"ReformerModelWithLMHead"),v7r.forEach(t),ylo=r(H2e," (Reformer model)"),H2e.forEach(t),wlo=i(Q),wu=s(Q,"LI",{});var U2e=n(wu);DU=s(U2e,"STRONG",{});var T7r=n(DU);Alo=r(T7r,"rembert"),T7r.forEach(t),Llo=r(U2e," \u2014 "),X9=s(U2e,"A",{href:!0});var F7r=n(X9);Blo=r(F7r,"RemBertForCausalLM"),F7r.forEach(t),xlo=r(U2e," (RemBERT model)"),U2e.forEach(t),klo=i(Q),Au=s(Q,"LI",{});var J2e=n(Au);NU=s(J2e,"STRONG",{});var E7r=n(NU);Rlo=r(E7r,"roberta"),E7r.forEach(t),Plo=r(J2e," \u2014 "),Q9=s(J2e,"A",{href:!0});var C7r=n(Q9);Slo=r(C7r,"RobertaForCausalLM"),C7r.forEach(t),$lo=r(J2e," (RoBERTa model)"),J2e.forEach(t),Ilo=i(Q),Lu=s(Q,"LI",{});var K2e=n(Lu);jU=s(K2e,"STRONG",{});var M7r=n(jU);Dlo=r(M7r,"roformer"),M7r.forEach(t),Nlo=r(K2e," \u2014 "),V9=s(K2e,"A",{href:!0});var y7r=n(V9);jlo=r(y7r,"RoFormerForCausalLM"),y7r.forEach(t),Olo=r(K2e," (RoFormer model)"),K2e.forEach(t),Glo=i(Q),Bu=s(Q,"LI",{});var Y2e=n(Bu);OU=s(Y2e,"STRONG",{});var w7r=n(OU);qlo=r(w7r,"speech_to_text_2"),w7r.forEach(t),zlo=r(Y2e," \u2014 "),W9=s(Y2e,"A",{href:!0});var A7r=n(W9);Xlo=r(A7r,"Speech2Text2ForCausalLM"),A7r.forEach(t),Qlo=r(Y2e," (Speech2Text2 model)"),Y2e.forEach(t),Vlo=i(Q),xu=s(Q,"LI",{});var Z2e=n(xu);GU=s(Z2e,"STRONG",{});var L7r=n(GU);Wlo=r(L7r,"transfo-xl"),L7r.forEach(t),Hlo=r(Z2e," \u2014 "),H9=s(Z2e,"A",{href:!0});var B7r=n(H9);Ulo=r(B7r,"TransfoXLLMHeadModel"),B7r.forEach(t),Jlo=r(Z2e," (Transformer-XL model)"),Z2e.forEach(t),Klo=i(Q),ku=s(Q,"LI",{});var eve=n(ku);qU=s(eve,"STRONG",{});var x7r=n(qU);Ylo=r(x7r,"trocr"),x7r.forEach(t),Zlo=r(eve," \u2014 "),U9=s(eve,"A",{href:!0});var k7r=n(U9);eio=r(k7r,"TrOCRForCausalLM"),k7r.forEach(t),oio=r(eve," (TrOCR model)"),eve.forEach(t),rio=i(Q),Ru=s(Q,"LI",{});var ove=n(Ru);zU=s(ove,"STRONG",{});var R7r=n(zU);tio=r(R7r,"xlm"),R7r.forEach(t),aio=r(ove," \u2014 "),J9=s(ove,"A",{href:!0});var P7r=n(J9);sio=r(P7r,"XLMWithLMHeadModel"),P7r.forEach(t),nio=r(ove," (XLM model)"),ove.forEach(t),lio=i(Q),Pu=s(Q,"LI",{});var rve=n(Pu);XU=s(rve,"STRONG",{});var S7r=n(XU);iio=r(S7r,"xlm-prophetnet"),S7r.forEach(t),dio=r(rve," \u2014 "),K9=s(rve,"A",{href:!0});var $7r=n(K9);cio=r($7r,"XLMProphetNetForCausalLM"),$7r.forEach(t),mio=r(rve," (XLMProphetNet model)"),rve.forEach(t),fio=i(Q),Su=s(Q,"LI",{});var tve=n(Su);QU=s(tve,"STRONG",{});var I7r=n(QU);hio=r(I7r,"xlm-roberta"),I7r.forEach(t),gio=r(tve," \u2014 "),Y9=s(tve,"A",{href:!0});var D7r=n(Y9);uio=r(D7r,"XLMRobertaForCausalLM"),D7r.forEach(t),pio=r(tve," (XLM-RoBERTa model)"),tve.forEach(t),_io=i(Q),$u=s(Q,"LI",{});var ave=n($u);VU=s(ave,"STRONG",{});var N7r=n(VU);bio=r(N7r,"xlnet"),N7r.forEach(t),vio=r(ave," \u2014 "),Z9=s(ave,"A",{href:!0});var j7r=n(Z9);Tio=r(j7r,"XLNetLMHeadModel"),j7r.forEach(t),Fio=r(ave," (XLNet model)"),ave.forEach(t),Q.forEach(t),Eio=i(Ft),Iu=s(Ft,"P",{});var sve=n(Iu);Cio=r(sve,"The model is set in evaluation mode by default using "),WU=s(sve,"CODE",{});var O7r=n(WU);Mio=r(O7r,"model.eval()"),O7r.forEach(t),yio=r(sve,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),HU=s(sve,"CODE",{});var G7r=n(HU);wio=r(G7r,"model.train()"),G7r.forEach(t),sve.forEach(t),Aio=i(Ft),UU=s(Ft,"P",{});var q7r=n(UU);Lio=r(q7r,"Examples:"),q7r.forEach(t),Bio=i(Ft),f(L3.$$.fragment,Ft),Ft.forEach(t),_n.forEach(t),C5e=i(d),Ti=s(d,"H2",{class:!0});var xwe=n(Ti);Du=s(xwe,"A",{id:!0,class:!0,href:!0});var z7r=n(Du);JU=s(z7r,"SPAN",{});var X7r=n(JU);f(B3.$$.fragment,X7r),X7r.forEach(t),z7r.forEach(t),xio=i(xwe),KU=s(xwe,"SPAN",{});var Q7r=n(KU);kio=r(Q7r,"AutoModelForMaskedLM"),Q7r.forEach(t),xwe.forEach(t),M5e=i(d),qo=s(d,"DIV",{class:!0});var vn=n(qo);f(x3.$$.fragment,vn),Rio=i(vn),Fi=s(vn,"P",{});var nj=n(Fi);Pio=r(nj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),YU=s(nj,"CODE",{});var V7r=n(YU);Sio=r(V7r,"from_pretrained()"),V7r.forEach(t),$io=r(nj,` class method or the
`),ZU=s(nj,"CODE",{});var W7r=n(ZU);Iio=r(W7r,"from_config()"),W7r.forEach(t),Dio=r(nj," class method."),nj.forEach(t),Nio=i(vn),k3=s(vn,"P",{});var kwe=n(k3);jio=r(kwe,"This class cannot be instantiated directly using "),eJ=s(kwe,"CODE",{});var H7r=n(eJ);Oio=r(H7r,"__init__()"),H7r.forEach(t),Gio=r(kwe," (throws an error)."),kwe.forEach(t),qio=i(vn),Sr=s(vn,"DIV",{class:!0});var Tn=n(Sr);f(R3.$$.fragment,Tn),zio=i(Tn),oJ=s(Tn,"P",{});var U7r=n(oJ);Xio=r(U7r,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),U7r.forEach(t),Qio=i(Tn),Ei=s(Tn,"P",{});var lj=n(Ei);Vio=r(lj,`Note:
Loading a model from its configuration file does `),rJ=s(lj,"STRONG",{});var J7r=n(rJ);Wio=r(J7r,"not"),J7r.forEach(t),Hio=r(lj,` load the model weights. It only affects the
model\u2019s configuration. Use `),tJ=s(lj,"CODE",{});var K7r=n(tJ);Uio=r(K7r,"from_pretrained()"),K7r.forEach(t),Jio=r(lj,` to load the model
weights.`),lj.forEach(t),Kio=i(Tn),aJ=s(Tn,"P",{});var Y7r=n(aJ);Yio=r(Y7r,"Examples:"),Y7r.forEach(t),Zio=i(Tn),f(P3.$$.fragment,Tn),Tn.forEach(t),edo=i(vn),De=s(vn,"DIV",{class:!0});var Et=n(De);f(S3.$$.fragment,Et),odo=i(Et),sJ=s(Et,"P",{});var Z7r=n(sJ);rdo=r(Z7r,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Z7r.forEach(t),tdo=i(Et),Ba=s(Et,"P",{});var R4=n(Ba);ado=r(R4,"The model class to instantiate is selected based on the "),nJ=s(R4,"CODE",{});var e0r=n(nJ);sdo=r(e0r,"model_type"),e0r.forEach(t),ndo=r(R4,` property of the config object (either
passed as an argument or loaded from `),lJ=s(R4,"CODE",{});var o0r=n(lJ);ldo=r(o0r,"pretrained_model_name_or_path"),o0r.forEach(t),ido=r(R4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),iJ=s(R4,"CODE",{});var r0r=n(iJ);ddo=r(r0r,"pretrained_model_name_or_path"),r0r.forEach(t),cdo=r(R4,":"),R4.forEach(t),mdo=i(Et),O=s(Et,"UL",{});var z=n(O);Nu=s(z,"LI",{});var nve=n(Nu);dJ=s(nve,"STRONG",{});var t0r=n(dJ);fdo=r(t0r,"albert"),t0r.forEach(t),hdo=r(nve," \u2014 "),ex=s(nve,"A",{href:!0});var a0r=n(ex);gdo=r(a0r,"AlbertForMaskedLM"),a0r.forEach(t),udo=r(nve," (ALBERT model)"),nve.forEach(t),pdo=i(z),ju=s(z,"LI",{});var lve=n(ju);cJ=s(lve,"STRONG",{});var s0r=n(cJ);_do=r(s0r,"bart"),s0r.forEach(t),bdo=r(lve," \u2014 "),ox=s(lve,"A",{href:!0});var n0r=n(ox);vdo=r(n0r,"BartForConditionalGeneration"),n0r.forEach(t),Tdo=r(lve," (BART model)"),lve.forEach(t),Fdo=i(z),Ou=s(z,"LI",{});var ive=n(Ou);mJ=s(ive,"STRONG",{});var l0r=n(mJ);Edo=r(l0r,"bert"),l0r.forEach(t),Cdo=r(ive," \u2014 "),rx=s(ive,"A",{href:!0});var i0r=n(rx);Mdo=r(i0r,"BertForMaskedLM"),i0r.forEach(t),ydo=r(ive," (BERT model)"),ive.forEach(t),wdo=i(z),Gu=s(z,"LI",{});var dve=n(Gu);fJ=s(dve,"STRONG",{});var d0r=n(fJ);Ado=r(d0r,"big_bird"),d0r.forEach(t),Ldo=r(dve," \u2014 "),tx=s(dve,"A",{href:!0});var c0r=n(tx);Bdo=r(c0r,"BigBirdForMaskedLM"),c0r.forEach(t),xdo=r(dve," (BigBird model)"),dve.forEach(t),kdo=i(z),qu=s(z,"LI",{});var cve=n(qu);hJ=s(cve,"STRONG",{});var m0r=n(hJ);Rdo=r(m0r,"camembert"),m0r.forEach(t),Pdo=r(cve," \u2014 "),ax=s(cve,"A",{href:!0});var f0r=n(ax);Sdo=r(f0r,"CamembertForMaskedLM"),f0r.forEach(t),$do=r(cve," (CamemBERT model)"),cve.forEach(t),Ido=i(z),zu=s(z,"LI",{});var mve=n(zu);gJ=s(mve,"STRONG",{});var h0r=n(gJ);Ddo=r(h0r,"convbert"),h0r.forEach(t),Ndo=r(mve," \u2014 "),sx=s(mve,"A",{href:!0});var g0r=n(sx);jdo=r(g0r,"ConvBertForMaskedLM"),g0r.forEach(t),Odo=r(mve," (ConvBERT model)"),mve.forEach(t),Gdo=i(z),Xu=s(z,"LI",{});var fve=n(Xu);uJ=s(fve,"STRONG",{});var u0r=n(uJ);qdo=r(u0r,"deberta"),u0r.forEach(t),zdo=r(fve," \u2014 "),nx=s(fve,"A",{href:!0});var p0r=n(nx);Xdo=r(p0r,"DebertaForMaskedLM"),p0r.forEach(t),Qdo=r(fve," (DeBERTa model)"),fve.forEach(t),Vdo=i(z),Qu=s(z,"LI",{});var hve=n(Qu);pJ=s(hve,"STRONG",{});var _0r=n(pJ);Wdo=r(_0r,"deberta-v2"),_0r.forEach(t),Hdo=r(hve," \u2014 "),lx=s(hve,"A",{href:!0});var b0r=n(lx);Udo=r(b0r,"DebertaV2ForMaskedLM"),b0r.forEach(t),Jdo=r(hve," (DeBERTa-v2 model)"),hve.forEach(t),Kdo=i(z),Vu=s(z,"LI",{});var gve=n(Vu);_J=s(gve,"STRONG",{});var v0r=n(_J);Ydo=r(v0r,"distilbert"),v0r.forEach(t),Zdo=r(gve," \u2014 "),ix=s(gve,"A",{href:!0});var T0r=n(ix);eco=r(T0r,"DistilBertForMaskedLM"),T0r.forEach(t),oco=r(gve," (DistilBERT model)"),gve.forEach(t),rco=i(z),Wu=s(z,"LI",{});var uve=n(Wu);bJ=s(uve,"STRONG",{});var F0r=n(bJ);tco=r(F0r,"electra"),F0r.forEach(t),aco=r(uve," \u2014 "),dx=s(uve,"A",{href:!0});var E0r=n(dx);sco=r(E0r,"ElectraForMaskedLM"),E0r.forEach(t),nco=r(uve," (ELECTRA model)"),uve.forEach(t),lco=i(z),Hu=s(z,"LI",{});var pve=n(Hu);vJ=s(pve,"STRONG",{});var C0r=n(vJ);ico=r(C0r,"flaubert"),C0r.forEach(t),dco=r(pve," \u2014 "),cx=s(pve,"A",{href:!0});var M0r=n(cx);cco=r(M0r,"FlaubertWithLMHeadModel"),M0r.forEach(t),mco=r(pve," (FlauBERT model)"),pve.forEach(t),fco=i(z),Uu=s(z,"LI",{});var _ve=n(Uu);TJ=s(_ve,"STRONG",{});var y0r=n(TJ);hco=r(y0r,"fnet"),y0r.forEach(t),gco=r(_ve," \u2014 "),mx=s(_ve,"A",{href:!0});var w0r=n(mx);uco=r(w0r,"FNetForMaskedLM"),w0r.forEach(t),pco=r(_ve," (FNet model)"),_ve.forEach(t),_co=i(z),Ju=s(z,"LI",{});var bve=n(Ju);FJ=s(bve,"STRONG",{});var A0r=n(FJ);bco=r(A0r,"funnel"),A0r.forEach(t),vco=r(bve," \u2014 "),fx=s(bve,"A",{href:!0});var L0r=n(fx);Tco=r(L0r,"FunnelForMaskedLM"),L0r.forEach(t),Fco=r(bve," (Funnel Transformer model)"),bve.forEach(t),Eco=i(z),Ku=s(z,"LI",{});var vve=n(Ku);EJ=s(vve,"STRONG",{});var B0r=n(EJ);Cco=r(B0r,"ibert"),B0r.forEach(t),Mco=r(vve," \u2014 "),hx=s(vve,"A",{href:!0});var x0r=n(hx);yco=r(x0r,"IBertForMaskedLM"),x0r.forEach(t),wco=r(vve," (I-BERT model)"),vve.forEach(t),Aco=i(z),Yu=s(z,"LI",{});var Tve=n(Yu);CJ=s(Tve,"STRONG",{});var k0r=n(CJ);Lco=r(k0r,"layoutlm"),k0r.forEach(t),Bco=r(Tve," \u2014 "),gx=s(Tve,"A",{href:!0});var R0r=n(gx);xco=r(R0r,"LayoutLMForMaskedLM"),R0r.forEach(t),kco=r(Tve," (LayoutLM model)"),Tve.forEach(t),Rco=i(z),Zu=s(z,"LI",{});var Fve=n(Zu);MJ=s(Fve,"STRONG",{});var P0r=n(MJ);Pco=r(P0r,"longformer"),P0r.forEach(t),Sco=r(Fve," \u2014 "),ux=s(Fve,"A",{href:!0});var S0r=n(ux);$co=r(S0r,"LongformerForMaskedLM"),S0r.forEach(t),Ico=r(Fve," (Longformer model)"),Fve.forEach(t),Dco=i(z),ep=s(z,"LI",{});var Eve=n(ep);yJ=s(Eve,"STRONG",{});var $0r=n(yJ);Nco=r($0r,"mbart"),$0r.forEach(t),jco=r(Eve," \u2014 "),px=s(Eve,"A",{href:!0});var I0r=n(px);Oco=r(I0r,"MBartForConditionalGeneration"),I0r.forEach(t),Gco=r(Eve," (mBART model)"),Eve.forEach(t),qco=i(z),op=s(z,"LI",{});var Cve=n(op);wJ=s(Cve,"STRONG",{});var D0r=n(wJ);zco=r(D0r,"megatron-bert"),D0r.forEach(t),Xco=r(Cve," \u2014 "),_x=s(Cve,"A",{href:!0});var N0r=n(_x);Qco=r(N0r,"MegatronBertForMaskedLM"),N0r.forEach(t),Vco=r(Cve," (MegatronBert model)"),Cve.forEach(t),Wco=i(z),rp=s(z,"LI",{});var Mve=n(rp);AJ=s(Mve,"STRONG",{});var j0r=n(AJ);Hco=r(j0r,"mobilebert"),j0r.forEach(t),Uco=r(Mve," \u2014 "),bx=s(Mve,"A",{href:!0});var O0r=n(bx);Jco=r(O0r,"MobileBertForMaskedLM"),O0r.forEach(t),Kco=r(Mve," (MobileBERT model)"),Mve.forEach(t),Yco=i(z),tp=s(z,"LI",{});var yve=n(tp);LJ=s(yve,"STRONG",{});var G0r=n(LJ);Zco=r(G0r,"mpnet"),G0r.forEach(t),emo=r(yve," \u2014 "),vx=s(yve,"A",{href:!0});var q0r=n(vx);omo=r(q0r,"MPNetForMaskedLM"),q0r.forEach(t),rmo=r(yve," (MPNet model)"),yve.forEach(t),tmo=i(z),ap=s(z,"LI",{});var wve=n(ap);BJ=s(wve,"STRONG",{});var z0r=n(BJ);amo=r(z0r,"perceiver"),z0r.forEach(t),smo=r(wve," \u2014 "),Tx=s(wve,"A",{href:!0});var X0r=n(Tx);nmo=r(X0r,"PerceiverForMaskedLM"),X0r.forEach(t),lmo=r(wve," (Perceiver model)"),wve.forEach(t),imo=i(z),sp=s(z,"LI",{});var Ave=n(sp);xJ=s(Ave,"STRONG",{});var Q0r=n(xJ);dmo=r(Q0r,"qdqbert"),Q0r.forEach(t),cmo=r(Ave," \u2014 "),Fx=s(Ave,"A",{href:!0});var V0r=n(Fx);mmo=r(V0r,"QDQBertForMaskedLM"),V0r.forEach(t),fmo=r(Ave," (QDQBert model)"),Ave.forEach(t),hmo=i(z),np=s(z,"LI",{});var Lve=n(np);kJ=s(Lve,"STRONG",{});var W0r=n(kJ);gmo=r(W0r,"reformer"),W0r.forEach(t),umo=r(Lve," \u2014 "),Ex=s(Lve,"A",{href:!0});var H0r=n(Ex);pmo=r(H0r,"ReformerForMaskedLM"),H0r.forEach(t),_mo=r(Lve," (Reformer model)"),Lve.forEach(t),bmo=i(z),lp=s(z,"LI",{});var Bve=n(lp);RJ=s(Bve,"STRONG",{});var U0r=n(RJ);vmo=r(U0r,"rembert"),U0r.forEach(t),Tmo=r(Bve," \u2014 "),Cx=s(Bve,"A",{href:!0});var J0r=n(Cx);Fmo=r(J0r,"RemBertForMaskedLM"),J0r.forEach(t),Emo=r(Bve," (RemBERT model)"),Bve.forEach(t),Cmo=i(z),ip=s(z,"LI",{});var xve=n(ip);PJ=s(xve,"STRONG",{});var K0r=n(PJ);Mmo=r(K0r,"roberta"),K0r.forEach(t),ymo=r(xve," \u2014 "),Mx=s(xve,"A",{href:!0});var Y0r=n(Mx);wmo=r(Y0r,"RobertaForMaskedLM"),Y0r.forEach(t),Amo=r(xve," (RoBERTa model)"),xve.forEach(t),Lmo=i(z),dp=s(z,"LI",{});var kve=n(dp);SJ=s(kve,"STRONG",{});var Z0r=n(SJ);Bmo=r(Z0r,"roformer"),Z0r.forEach(t),xmo=r(kve," \u2014 "),yx=s(kve,"A",{href:!0});var eAr=n(yx);kmo=r(eAr,"RoFormerForMaskedLM"),eAr.forEach(t),Rmo=r(kve," (RoFormer model)"),kve.forEach(t),Pmo=i(z),cp=s(z,"LI",{});var Rve=n(cp);$J=s(Rve,"STRONG",{});var oAr=n($J);Smo=r(oAr,"squeezebert"),oAr.forEach(t),$mo=r(Rve," \u2014 "),wx=s(Rve,"A",{href:!0});var rAr=n(wx);Imo=r(rAr,"SqueezeBertForMaskedLM"),rAr.forEach(t),Dmo=r(Rve," (SqueezeBERT model)"),Rve.forEach(t),Nmo=i(z),mp=s(z,"LI",{});var Pve=n(mp);IJ=s(Pve,"STRONG",{});var tAr=n(IJ);jmo=r(tAr,"tapas"),tAr.forEach(t),Omo=r(Pve," \u2014 "),Ax=s(Pve,"A",{href:!0});var aAr=n(Ax);Gmo=r(aAr,"TapasForMaskedLM"),aAr.forEach(t),qmo=r(Pve," (TAPAS model)"),Pve.forEach(t),zmo=i(z),fp=s(z,"LI",{});var Sve=n(fp);DJ=s(Sve,"STRONG",{});var sAr=n(DJ);Xmo=r(sAr,"wav2vec2"),sAr.forEach(t),Qmo=r(Sve," \u2014 "),NJ=s(Sve,"CODE",{});var nAr=n(NJ);Vmo=r(nAr,"Wav2Vec2ForMaskedLM"),nAr.forEach(t),Wmo=r(Sve," (Wav2Vec2 model)"),Sve.forEach(t),Hmo=i(z),hp=s(z,"LI",{});var $ve=n(hp);jJ=s($ve,"STRONG",{});var lAr=n(jJ);Umo=r(lAr,"xlm"),lAr.forEach(t),Jmo=r($ve," \u2014 "),Lx=s($ve,"A",{href:!0});var iAr=n(Lx);Kmo=r(iAr,"XLMWithLMHeadModel"),iAr.forEach(t),Ymo=r($ve," (XLM model)"),$ve.forEach(t),Zmo=i(z),gp=s(z,"LI",{});var Ive=n(gp);OJ=s(Ive,"STRONG",{});var dAr=n(OJ);efo=r(dAr,"xlm-roberta"),dAr.forEach(t),ofo=r(Ive," \u2014 "),Bx=s(Ive,"A",{href:!0});var cAr=n(Bx);rfo=r(cAr,"XLMRobertaForMaskedLM"),cAr.forEach(t),tfo=r(Ive," (XLM-RoBERTa model)"),Ive.forEach(t),z.forEach(t),afo=i(Et),up=s(Et,"P",{});var Dve=n(up);sfo=r(Dve,"The model is set in evaluation mode by default using "),GJ=s(Dve,"CODE",{});var mAr=n(GJ);nfo=r(mAr,"model.eval()"),mAr.forEach(t),lfo=r(Dve,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),qJ=s(Dve,"CODE",{});var fAr=n(qJ);ifo=r(fAr,"model.train()"),fAr.forEach(t),Dve.forEach(t),dfo=i(Et),zJ=s(Et,"P",{});var hAr=n(zJ);cfo=r(hAr,"Examples:"),hAr.forEach(t),mfo=i(Et),f($3.$$.fragment,Et),Et.forEach(t),vn.forEach(t),y5e=i(d),Ci=s(d,"H2",{class:!0});var Rwe=n(Ci);pp=s(Rwe,"A",{id:!0,class:!0,href:!0});var gAr=n(pp);XJ=s(gAr,"SPAN",{});var uAr=n(XJ);f(I3.$$.fragment,uAr),uAr.forEach(t),gAr.forEach(t),ffo=i(Rwe),QJ=s(Rwe,"SPAN",{});var pAr=n(QJ);hfo=r(pAr,"AutoModelForSeq2SeqLM"),pAr.forEach(t),Rwe.forEach(t),w5e=i(d),zo=s(d,"DIV",{class:!0});var Fn=n(zo);f(D3.$$.fragment,Fn),gfo=i(Fn),Mi=s(Fn,"P",{});var ij=n(Mi);ufo=r(ij,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),VJ=s(ij,"CODE",{});var _Ar=n(VJ);pfo=r(_Ar,"from_pretrained()"),_Ar.forEach(t),_fo=r(ij,` class method or the
`),WJ=s(ij,"CODE",{});var bAr=n(WJ);bfo=r(bAr,"from_config()"),bAr.forEach(t),vfo=r(ij," class method."),ij.forEach(t),Tfo=i(Fn),N3=s(Fn,"P",{});var Pwe=n(N3);Ffo=r(Pwe,"This class cannot be instantiated directly using "),HJ=s(Pwe,"CODE",{});var vAr=n(HJ);Efo=r(vAr,"__init__()"),vAr.forEach(t),Cfo=r(Pwe," (throws an error)."),Pwe.forEach(t),Mfo=i(Fn),$r=s(Fn,"DIV",{class:!0});var En=n($r);f(j3.$$.fragment,En),yfo=i(En),UJ=s(En,"P",{});var TAr=n(UJ);wfo=r(TAr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),TAr.forEach(t),Afo=i(En),yi=s(En,"P",{});var dj=n(yi);Lfo=r(dj,`Note:
Loading a model from its configuration file does `),JJ=s(dj,"STRONG",{});var FAr=n(JJ);Bfo=r(FAr,"not"),FAr.forEach(t),xfo=r(dj,` load the model weights. It only affects the
model\u2019s configuration. Use `),KJ=s(dj,"CODE",{});var EAr=n(KJ);kfo=r(EAr,"from_pretrained()"),EAr.forEach(t),Rfo=r(dj,` to load the model
weights.`),dj.forEach(t),Pfo=i(En),YJ=s(En,"P",{});var CAr=n(YJ);Sfo=r(CAr,"Examples:"),CAr.forEach(t),$fo=i(En),f(O3.$$.fragment,En),En.forEach(t),Ifo=i(Fn),Ne=s(Fn,"DIV",{class:!0});var Ct=n(Ne);f(G3.$$.fragment,Ct),Dfo=i(Ct),ZJ=s(Ct,"P",{});var MAr=n(ZJ);Nfo=r(MAr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),MAr.forEach(t),jfo=i(Ct),xa=s(Ct,"P",{});var P4=n(xa);Ofo=r(P4,"The model class to instantiate is selected based on the "),eK=s(P4,"CODE",{});var yAr=n(eK);Gfo=r(yAr,"model_type"),yAr.forEach(t),qfo=r(P4,` property of the config object (either
passed as an argument or loaded from `),oK=s(P4,"CODE",{});var wAr=n(oK);zfo=r(wAr,"pretrained_model_name_or_path"),wAr.forEach(t),Xfo=r(P4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),rK=s(P4,"CODE",{});var AAr=n(rK);Qfo=r(AAr,"pretrained_model_name_or_path"),AAr.forEach(t),Vfo=r(P4,":"),P4.forEach(t),Wfo=i(Ct),me=s(Ct,"UL",{});var he=n(me);_p=s(he,"LI",{});var Nve=n(_p);tK=s(Nve,"STRONG",{});var LAr=n(tK);Hfo=r(LAr,"bart"),LAr.forEach(t),Ufo=r(Nve," \u2014 "),xx=s(Nve,"A",{href:!0});var BAr=n(xx);Jfo=r(BAr,"BartForConditionalGeneration"),BAr.forEach(t),Kfo=r(Nve," (BART model)"),Nve.forEach(t),Yfo=i(he),bp=s(he,"LI",{});var jve=n(bp);aK=s(jve,"STRONG",{});var xAr=n(aK);Zfo=r(xAr,"bigbird_pegasus"),xAr.forEach(t),eho=r(jve," \u2014 "),kx=s(jve,"A",{href:!0});var kAr=n(kx);oho=r(kAr,"BigBirdPegasusForConditionalGeneration"),kAr.forEach(t),rho=r(jve," (BigBirdPegasus model)"),jve.forEach(t),tho=i(he),vp=s(he,"LI",{});var Ove=n(vp);sK=s(Ove,"STRONG",{});var RAr=n(sK);aho=r(RAr,"blenderbot"),RAr.forEach(t),sho=r(Ove," \u2014 "),Rx=s(Ove,"A",{href:!0});var PAr=n(Rx);nho=r(PAr,"BlenderbotForConditionalGeneration"),PAr.forEach(t),lho=r(Ove," (Blenderbot model)"),Ove.forEach(t),iho=i(he),Tp=s(he,"LI",{});var Gve=n(Tp);nK=s(Gve,"STRONG",{});var SAr=n(nK);dho=r(SAr,"blenderbot-small"),SAr.forEach(t),cho=r(Gve," \u2014 "),Px=s(Gve,"A",{href:!0});var $Ar=n(Px);mho=r($Ar,"BlenderbotSmallForConditionalGeneration"),$Ar.forEach(t),fho=r(Gve," (BlenderbotSmall model)"),Gve.forEach(t),hho=i(he),Fp=s(he,"LI",{});var qve=n(Fp);lK=s(qve,"STRONG",{});var IAr=n(lK);gho=r(IAr,"encoder-decoder"),IAr.forEach(t),uho=r(qve," \u2014 "),Sx=s(qve,"A",{href:!0});var DAr=n(Sx);pho=r(DAr,"EncoderDecoderModel"),DAr.forEach(t),_ho=r(qve," (Encoder decoder model)"),qve.forEach(t),bho=i(he),Ep=s(he,"LI",{});var zve=n(Ep);iK=s(zve,"STRONG",{});var NAr=n(iK);vho=r(NAr,"fsmt"),NAr.forEach(t),Tho=r(zve," \u2014 "),$x=s(zve,"A",{href:!0});var jAr=n($x);Fho=r(jAr,"FSMTForConditionalGeneration"),jAr.forEach(t),Eho=r(zve," (FairSeq Machine-Translation model)"),zve.forEach(t),Cho=i(he),Cp=s(he,"LI",{});var Xve=n(Cp);dK=s(Xve,"STRONG",{});var OAr=n(dK);Mho=r(OAr,"led"),OAr.forEach(t),yho=r(Xve," \u2014 "),Ix=s(Xve,"A",{href:!0});var GAr=n(Ix);who=r(GAr,"LEDForConditionalGeneration"),GAr.forEach(t),Aho=r(Xve," (LED model)"),Xve.forEach(t),Lho=i(he),Mp=s(he,"LI",{});var Qve=n(Mp);cK=s(Qve,"STRONG",{});var qAr=n(cK);Bho=r(qAr,"m2m_100"),qAr.forEach(t),xho=r(Qve," \u2014 "),Dx=s(Qve,"A",{href:!0});var zAr=n(Dx);kho=r(zAr,"M2M100ForConditionalGeneration"),zAr.forEach(t),Rho=r(Qve," (M2M100 model)"),Qve.forEach(t),Pho=i(he),yp=s(he,"LI",{});var Vve=n(yp);mK=s(Vve,"STRONG",{});var XAr=n(mK);Sho=r(XAr,"marian"),XAr.forEach(t),$ho=r(Vve," \u2014 "),Nx=s(Vve,"A",{href:!0});var QAr=n(Nx);Iho=r(QAr,"MarianMTModel"),QAr.forEach(t),Dho=r(Vve," (Marian model)"),Vve.forEach(t),Nho=i(he),wp=s(he,"LI",{});var Wve=n(wp);fK=s(Wve,"STRONG",{});var VAr=n(fK);jho=r(VAr,"mbart"),VAr.forEach(t),Oho=r(Wve," \u2014 "),jx=s(Wve,"A",{href:!0});var WAr=n(jx);Gho=r(WAr,"MBartForConditionalGeneration"),WAr.forEach(t),qho=r(Wve," (mBART model)"),Wve.forEach(t),zho=i(he),Ap=s(he,"LI",{});var Hve=n(Ap);hK=s(Hve,"STRONG",{});var HAr=n(hK);Xho=r(HAr,"mt5"),HAr.forEach(t),Qho=r(Hve," \u2014 "),Ox=s(Hve,"A",{href:!0});var UAr=n(Ox);Vho=r(UAr,"MT5ForConditionalGeneration"),UAr.forEach(t),Who=r(Hve," (mT5 model)"),Hve.forEach(t),Hho=i(he),Lp=s(he,"LI",{});var Uve=n(Lp);gK=s(Uve,"STRONG",{});var JAr=n(gK);Uho=r(JAr,"pegasus"),JAr.forEach(t),Jho=r(Uve," \u2014 "),Gx=s(Uve,"A",{href:!0});var KAr=n(Gx);Kho=r(KAr,"PegasusForConditionalGeneration"),KAr.forEach(t),Yho=r(Uve," (Pegasus model)"),Uve.forEach(t),Zho=i(he),Bp=s(he,"LI",{});var Jve=n(Bp);uK=s(Jve,"STRONG",{});var YAr=n(uK);ego=r(YAr,"prophetnet"),YAr.forEach(t),ogo=r(Jve," \u2014 "),qx=s(Jve,"A",{href:!0});var ZAr=n(qx);rgo=r(ZAr,"ProphetNetForConditionalGeneration"),ZAr.forEach(t),tgo=r(Jve," (ProphetNet model)"),Jve.forEach(t),ago=i(he),xp=s(he,"LI",{});var Kve=n(xp);pK=s(Kve,"STRONG",{});var e6r=n(pK);sgo=r(e6r,"t5"),e6r.forEach(t),ngo=r(Kve," \u2014 "),zx=s(Kve,"A",{href:!0});var o6r=n(zx);lgo=r(o6r,"T5ForConditionalGeneration"),o6r.forEach(t),igo=r(Kve," (T5 model)"),Kve.forEach(t),dgo=i(he),kp=s(he,"LI",{});var Yve=n(kp);_K=s(Yve,"STRONG",{});var r6r=n(_K);cgo=r(r6r,"xlm-prophetnet"),r6r.forEach(t),mgo=r(Yve," \u2014 "),Xx=s(Yve,"A",{href:!0});var t6r=n(Xx);fgo=r(t6r,"XLMProphetNetForConditionalGeneration"),t6r.forEach(t),hgo=r(Yve," (XLMProphetNet model)"),Yve.forEach(t),he.forEach(t),ggo=i(Ct),Rp=s(Ct,"P",{});var Zve=n(Rp);ugo=r(Zve,"The model is set in evaluation mode by default using "),bK=s(Zve,"CODE",{});var a6r=n(bK);pgo=r(a6r,"model.eval()"),a6r.forEach(t),_go=r(Zve,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),vK=s(Zve,"CODE",{});var s6r=n(vK);bgo=r(s6r,"model.train()"),s6r.forEach(t),Zve.forEach(t),vgo=i(Ct),TK=s(Ct,"P",{});var n6r=n(TK);Tgo=r(n6r,"Examples:"),n6r.forEach(t),Fgo=i(Ct),f(q3.$$.fragment,Ct),Ct.forEach(t),Fn.forEach(t),A5e=i(d),wi=s(d,"H2",{class:!0});var Swe=n(wi);Pp=s(Swe,"A",{id:!0,class:!0,href:!0});var l6r=n(Pp);FK=s(l6r,"SPAN",{});var i6r=n(FK);f(z3.$$.fragment,i6r),i6r.forEach(t),l6r.forEach(t),Ego=i(Swe),EK=s(Swe,"SPAN",{});var d6r=n(EK);Cgo=r(d6r,"AutoModelForSequenceClassification"),d6r.forEach(t),Swe.forEach(t),L5e=i(d),Xo=s(d,"DIV",{class:!0});var Cn=n(Xo);f(X3.$$.fragment,Cn),Mgo=i(Cn),Ai=s(Cn,"P",{});var cj=n(Ai);ygo=r(cj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),CK=s(cj,"CODE",{});var c6r=n(CK);wgo=r(c6r,"from_pretrained()"),c6r.forEach(t),Ago=r(cj,` class method or the
`),MK=s(cj,"CODE",{});var m6r=n(MK);Lgo=r(m6r,"from_config()"),m6r.forEach(t),Bgo=r(cj," class method."),cj.forEach(t),xgo=i(Cn),Q3=s(Cn,"P",{});var $we=n(Q3);kgo=r($we,"This class cannot be instantiated directly using "),yK=s($we,"CODE",{});var f6r=n(yK);Rgo=r(f6r,"__init__()"),f6r.forEach(t),Pgo=r($we," (throws an error)."),$we.forEach(t),Sgo=i(Cn),Ir=s(Cn,"DIV",{class:!0});var Mn=n(Ir);f(V3.$$.fragment,Mn),$go=i(Mn),wK=s(Mn,"P",{});var h6r=n(wK);Igo=r(h6r,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),h6r.forEach(t),Dgo=i(Mn),Li=s(Mn,"P",{});var mj=n(Li);Ngo=r(mj,`Note:
Loading a model from its configuration file does `),AK=s(mj,"STRONG",{});var g6r=n(AK);jgo=r(g6r,"not"),g6r.forEach(t),Ogo=r(mj,` load the model weights. It only affects the
model\u2019s configuration. Use `),LK=s(mj,"CODE",{});var u6r=n(LK);Ggo=r(u6r,"from_pretrained()"),u6r.forEach(t),qgo=r(mj,` to load the model
weights.`),mj.forEach(t),zgo=i(Mn),BK=s(Mn,"P",{});var p6r=n(BK);Xgo=r(p6r,"Examples:"),p6r.forEach(t),Qgo=i(Mn),f(W3.$$.fragment,Mn),Mn.forEach(t),Vgo=i(Cn),je=s(Cn,"DIV",{class:!0});var Mt=n(je);f(H3.$$.fragment,Mt),Wgo=i(Mt),xK=s(Mt,"P",{});var _6r=n(xK);Hgo=r(_6r,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),_6r.forEach(t),Ugo=i(Mt),ka=s(Mt,"P",{});var S4=n(ka);Jgo=r(S4,"The model class to instantiate is selected based on the "),kK=s(S4,"CODE",{});var b6r=n(kK);Kgo=r(b6r,"model_type"),b6r.forEach(t),Ygo=r(S4,` property of the config object (either
passed as an argument or loaded from `),RK=s(S4,"CODE",{});var v6r=n(RK);Zgo=r(v6r,"pretrained_model_name_or_path"),v6r.forEach(t),euo=r(S4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),PK=s(S4,"CODE",{});var T6r=n(PK);ouo=r(T6r,"pretrained_model_name_or_path"),T6r.forEach(t),ruo=r(S4,":"),S4.forEach(t),tuo=i(Mt),A=s(Mt,"UL",{});var L=n(A);Sp=s(L,"LI",{});var eTe=n(Sp);SK=s(eTe,"STRONG",{});var F6r=n(SK);auo=r(F6r,"albert"),F6r.forEach(t),suo=r(eTe," \u2014 "),Qx=s(eTe,"A",{href:!0});var E6r=n(Qx);nuo=r(E6r,"AlbertForSequenceClassification"),E6r.forEach(t),luo=r(eTe," (ALBERT model)"),eTe.forEach(t),iuo=i(L),$p=s(L,"LI",{});var oTe=n($p);$K=s(oTe,"STRONG",{});var C6r=n($K);duo=r(C6r,"bart"),C6r.forEach(t),cuo=r(oTe," \u2014 "),Vx=s(oTe,"A",{href:!0});var M6r=n(Vx);muo=r(M6r,"BartForSequenceClassification"),M6r.forEach(t),fuo=r(oTe," (BART model)"),oTe.forEach(t),huo=i(L),Ip=s(L,"LI",{});var rTe=n(Ip);IK=s(rTe,"STRONG",{});var y6r=n(IK);guo=r(y6r,"bert"),y6r.forEach(t),uuo=r(rTe," \u2014 "),Wx=s(rTe,"A",{href:!0});var w6r=n(Wx);puo=r(w6r,"BertForSequenceClassification"),w6r.forEach(t),_uo=r(rTe," (BERT model)"),rTe.forEach(t),buo=i(L),Dp=s(L,"LI",{});var tTe=n(Dp);DK=s(tTe,"STRONG",{});var A6r=n(DK);vuo=r(A6r,"big_bird"),A6r.forEach(t),Tuo=r(tTe," \u2014 "),Hx=s(tTe,"A",{href:!0});var L6r=n(Hx);Fuo=r(L6r,"BigBirdForSequenceClassification"),L6r.forEach(t),Euo=r(tTe," (BigBird model)"),tTe.forEach(t),Cuo=i(L),Np=s(L,"LI",{});var aTe=n(Np);NK=s(aTe,"STRONG",{});var B6r=n(NK);Muo=r(B6r,"bigbird_pegasus"),B6r.forEach(t),yuo=r(aTe," \u2014 "),Ux=s(aTe,"A",{href:!0});var x6r=n(Ux);wuo=r(x6r,"BigBirdPegasusForSequenceClassification"),x6r.forEach(t),Auo=r(aTe," (BigBirdPegasus model)"),aTe.forEach(t),Luo=i(L),jp=s(L,"LI",{});var sTe=n(jp);jK=s(sTe,"STRONG",{});var k6r=n(jK);Buo=r(k6r,"camembert"),k6r.forEach(t),xuo=r(sTe," \u2014 "),Jx=s(sTe,"A",{href:!0});var R6r=n(Jx);kuo=r(R6r,"CamembertForSequenceClassification"),R6r.forEach(t),Ruo=r(sTe," (CamemBERT model)"),sTe.forEach(t),Puo=i(L),Op=s(L,"LI",{});var nTe=n(Op);OK=s(nTe,"STRONG",{});var P6r=n(OK);Suo=r(P6r,"canine"),P6r.forEach(t),$uo=r(nTe," \u2014 "),Kx=s(nTe,"A",{href:!0});var S6r=n(Kx);Iuo=r(S6r,"CanineForSequenceClassification"),S6r.forEach(t),Duo=r(nTe," (Canine model)"),nTe.forEach(t),Nuo=i(L),Gp=s(L,"LI",{});var lTe=n(Gp);GK=s(lTe,"STRONG",{});var $6r=n(GK);juo=r($6r,"convbert"),$6r.forEach(t),Ouo=r(lTe," \u2014 "),Yx=s(lTe,"A",{href:!0});var I6r=n(Yx);Guo=r(I6r,"ConvBertForSequenceClassification"),I6r.forEach(t),quo=r(lTe," (ConvBERT model)"),lTe.forEach(t),zuo=i(L),qp=s(L,"LI",{});var iTe=n(qp);qK=s(iTe,"STRONG",{});var D6r=n(qK);Xuo=r(D6r,"ctrl"),D6r.forEach(t),Quo=r(iTe," \u2014 "),Zx=s(iTe,"A",{href:!0});var N6r=n(Zx);Vuo=r(N6r,"CTRLForSequenceClassification"),N6r.forEach(t),Wuo=r(iTe," (CTRL model)"),iTe.forEach(t),Huo=i(L),zp=s(L,"LI",{});var dTe=n(zp);zK=s(dTe,"STRONG",{});var j6r=n(zK);Uuo=r(j6r,"deberta"),j6r.forEach(t),Juo=r(dTe," \u2014 "),ek=s(dTe,"A",{href:!0});var O6r=n(ek);Kuo=r(O6r,"DebertaForSequenceClassification"),O6r.forEach(t),Yuo=r(dTe," (DeBERTa model)"),dTe.forEach(t),Zuo=i(L),Xp=s(L,"LI",{});var cTe=n(Xp);XK=s(cTe,"STRONG",{});var G6r=n(XK);epo=r(G6r,"deberta-v2"),G6r.forEach(t),opo=r(cTe," \u2014 "),ok=s(cTe,"A",{href:!0});var q6r=n(ok);rpo=r(q6r,"DebertaV2ForSequenceClassification"),q6r.forEach(t),tpo=r(cTe," (DeBERTa-v2 model)"),cTe.forEach(t),apo=i(L),Qp=s(L,"LI",{});var mTe=n(Qp);QK=s(mTe,"STRONG",{});var z6r=n(QK);spo=r(z6r,"distilbert"),z6r.forEach(t),npo=r(mTe," \u2014 "),rk=s(mTe,"A",{href:!0});var X6r=n(rk);lpo=r(X6r,"DistilBertForSequenceClassification"),X6r.forEach(t),ipo=r(mTe," (DistilBERT model)"),mTe.forEach(t),dpo=i(L),Vp=s(L,"LI",{});var fTe=n(Vp);VK=s(fTe,"STRONG",{});var Q6r=n(VK);cpo=r(Q6r,"electra"),Q6r.forEach(t),mpo=r(fTe," \u2014 "),tk=s(fTe,"A",{href:!0});var V6r=n(tk);fpo=r(V6r,"ElectraForSequenceClassification"),V6r.forEach(t),hpo=r(fTe," (ELECTRA model)"),fTe.forEach(t),gpo=i(L),Wp=s(L,"LI",{});var hTe=n(Wp);WK=s(hTe,"STRONG",{});var W6r=n(WK);upo=r(W6r,"flaubert"),W6r.forEach(t),ppo=r(hTe," \u2014 "),ak=s(hTe,"A",{href:!0});var H6r=n(ak);_po=r(H6r,"FlaubertForSequenceClassification"),H6r.forEach(t),bpo=r(hTe," (FlauBERT model)"),hTe.forEach(t),vpo=i(L),Hp=s(L,"LI",{});var gTe=n(Hp);HK=s(gTe,"STRONG",{});var U6r=n(HK);Tpo=r(U6r,"fnet"),U6r.forEach(t),Fpo=r(gTe," \u2014 "),sk=s(gTe,"A",{href:!0});var J6r=n(sk);Epo=r(J6r,"FNetForSequenceClassification"),J6r.forEach(t),Cpo=r(gTe," (FNet model)"),gTe.forEach(t),Mpo=i(L),Up=s(L,"LI",{});var uTe=n(Up);UK=s(uTe,"STRONG",{});var K6r=n(UK);ypo=r(K6r,"funnel"),K6r.forEach(t),wpo=r(uTe," \u2014 "),nk=s(uTe,"A",{href:!0});var Y6r=n(nk);Apo=r(Y6r,"FunnelForSequenceClassification"),Y6r.forEach(t),Lpo=r(uTe," (Funnel Transformer model)"),uTe.forEach(t),Bpo=i(L),Jp=s(L,"LI",{});var pTe=n(Jp);JK=s(pTe,"STRONG",{});var Z6r=n(JK);xpo=r(Z6r,"gpt2"),Z6r.forEach(t),kpo=r(pTe," \u2014 "),lk=s(pTe,"A",{href:!0});var eLr=n(lk);Rpo=r(eLr,"GPT2ForSequenceClassification"),eLr.forEach(t),Ppo=r(pTe," (OpenAI GPT-2 model)"),pTe.forEach(t),Spo=i(L),Kp=s(L,"LI",{});var _Te=n(Kp);KK=s(_Te,"STRONG",{});var oLr=n(KK);$po=r(oLr,"gpt_neo"),oLr.forEach(t),Ipo=r(_Te," \u2014 "),ik=s(_Te,"A",{href:!0});var rLr=n(ik);Dpo=r(rLr,"GPTNeoForSequenceClassification"),rLr.forEach(t),Npo=r(_Te," (GPT Neo model)"),_Te.forEach(t),jpo=i(L),Yp=s(L,"LI",{});var bTe=n(Yp);YK=s(bTe,"STRONG",{});var tLr=n(YK);Opo=r(tLr,"gptj"),tLr.forEach(t),Gpo=r(bTe," \u2014 "),dk=s(bTe,"A",{href:!0});var aLr=n(dk);qpo=r(aLr,"GPTJForSequenceClassification"),aLr.forEach(t),zpo=r(bTe," (GPT-J model)"),bTe.forEach(t),Xpo=i(L),Zp=s(L,"LI",{});var vTe=n(Zp);ZK=s(vTe,"STRONG",{});var sLr=n(ZK);Qpo=r(sLr,"ibert"),sLr.forEach(t),Vpo=r(vTe," \u2014 "),ck=s(vTe,"A",{href:!0});var nLr=n(ck);Wpo=r(nLr,"IBertForSequenceClassification"),nLr.forEach(t),Hpo=r(vTe," (I-BERT model)"),vTe.forEach(t),Upo=i(L),e_=s(L,"LI",{});var TTe=n(e_);eY=s(TTe,"STRONG",{});var lLr=n(eY);Jpo=r(lLr,"layoutlm"),lLr.forEach(t),Kpo=r(TTe," \u2014 "),mk=s(TTe,"A",{href:!0});var iLr=n(mk);Ypo=r(iLr,"LayoutLMForSequenceClassification"),iLr.forEach(t),Zpo=r(TTe," (LayoutLM model)"),TTe.forEach(t),e_o=i(L),o_=s(L,"LI",{});var FTe=n(o_);oY=s(FTe,"STRONG",{});var dLr=n(oY);o_o=r(dLr,"layoutlmv2"),dLr.forEach(t),r_o=r(FTe," \u2014 "),fk=s(FTe,"A",{href:!0});var cLr=n(fk);t_o=r(cLr,"LayoutLMv2ForSequenceClassification"),cLr.forEach(t),a_o=r(FTe," (LayoutLMv2 model)"),FTe.forEach(t),s_o=i(L),r_=s(L,"LI",{});var ETe=n(r_);rY=s(ETe,"STRONG",{});var mLr=n(rY);n_o=r(mLr,"led"),mLr.forEach(t),l_o=r(ETe," \u2014 "),hk=s(ETe,"A",{href:!0});var fLr=n(hk);i_o=r(fLr,"LEDForSequenceClassification"),fLr.forEach(t),d_o=r(ETe," (LED model)"),ETe.forEach(t),c_o=i(L),t_=s(L,"LI",{});var CTe=n(t_);tY=s(CTe,"STRONG",{});var hLr=n(tY);m_o=r(hLr,"longformer"),hLr.forEach(t),f_o=r(CTe," \u2014 "),gk=s(CTe,"A",{href:!0});var gLr=n(gk);h_o=r(gLr,"LongformerForSequenceClassification"),gLr.forEach(t),g_o=r(CTe," (Longformer model)"),CTe.forEach(t),u_o=i(L),a_=s(L,"LI",{});var MTe=n(a_);aY=s(MTe,"STRONG",{});var uLr=n(aY);p_o=r(uLr,"mbart"),uLr.forEach(t),__o=r(MTe," \u2014 "),uk=s(MTe,"A",{href:!0});var pLr=n(uk);b_o=r(pLr,"MBartForSequenceClassification"),pLr.forEach(t),v_o=r(MTe," (mBART model)"),MTe.forEach(t),T_o=i(L),s_=s(L,"LI",{});var yTe=n(s_);sY=s(yTe,"STRONG",{});var _Lr=n(sY);F_o=r(_Lr,"megatron-bert"),_Lr.forEach(t),E_o=r(yTe," \u2014 "),pk=s(yTe,"A",{href:!0});var bLr=n(pk);C_o=r(bLr,"MegatronBertForSequenceClassification"),bLr.forEach(t),M_o=r(yTe," (MegatronBert model)"),yTe.forEach(t),y_o=i(L),n_=s(L,"LI",{});var wTe=n(n_);nY=s(wTe,"STRONG",{});var vLr=n(nY);w_o=r(vLr,"mobilebert"),vLr.forEach(t),A_o=r(wTe," \u2014 "),_k=s(wTe,"A",{href:!0});var TLr=n(_k);L_o=r(TLr,"MobileBertForSequenceClassification"),TLr.forEach(t),B_o=r(wTe," (MobileBERT model)"),wTe.forEach(t),x_o=i(L),l_=s(L,"LI",{});var ATe=n(l_);lY=s(ATe,"STRONG",{});var FLr=n(lY);k_o=r(FLr,"mpnet"),FLr.forEach(t),R_o=r(ATe," \u2014 "),bk=s(ATe,"A",{href:!0});var ELr=n(bk);P_o=r(ELr,"MPNetForSequenceClassification"),ELr.forEach(t),S_o=r(ATe," (MPNet model)"),ATe.forEach(t),$_o=i(L),i_=s(L,"LI",{});var LTe=n(i_);iY=s(LTe,"STRONG",{});var CLr=n(iY);I_o=r(CLr,"openai-gpt"),CLr.forEach(t),D_o=r(LTe," \u2014 "),vk=s(LTe,"A",{href:!0});var MLr=n(vk);N_o=r(MLr,"OpenAIGPTForSequenceClassification"),MLr.forEach(t),j_o=r(LTe," (OpenAI GPT model)"),LTe.forEach(t),O_o=i(L),d_=s(L,"LI",{});var BTe=n(d_);dY=s(BTe,"STRONG",{});var yLr=n(dY);G_o=r(yLr,"perceiver"),yLr.forEach(t),q_o=r(BTe," \u2014 "),Tk=s(BTe,"A",{href:!0});var wLr=n(Tk);z_o=r(wLr,"PerceiverForSequenceClassification"),wLr.forEach(t),X_o=r(BTe," (Perceiver model)"),BTe.forEach(t),Q_o=i(L),c_=s(L,"LI",{});var xTe=n(c_);cY=s(xTe,"STRONG",{});var ALr=n(cY);V_o=r(ALr,"qdqbert"),ALr.forEach(t),W_o=r(xTe," \u2014 "),Fk=s(xTe,"A",{href:!0});var LLr=n(Fk);H_o=r(LLr,"QDQBertForSequenceClassification"),LLr.forEach(t),U_o=r(xTe," (QDQBert model)"),xTe.forEach(t),J_o=i(L),m_=s(L,"LI",{});var kTe=n(m_);mY=s(kTe,"STRONG",{});var BLr=n(mY);K_o=r(BLr,"reformer"),BLr.forEach(t),Y_o=r(kTe," \u2014 "),Ek=s(kTe,"A",{href:!0});var xLr=n(Ek);Z_o=r(xLr,"ReformerForSequenceClassification"),xLr.forEach(t),ebo=r(kTe," (Reformer model)"),kTe.forEach(t),obo=i(L),f_=s(L,"LI",{});var RTe=n(f_);fY=s(RTe,"STRONG",{});var kLr=n(fY);rbo=r(kLr,"rembert"),kLr.forEach(t),tbo=r(RTe," \u2014 "),Ck=s(RTe,"A",{href:!0});var RLr=n(Ck);abo=r(RLr,"RemBertForSequenceClassification"),RLr.forEach(t),sbo=r(RTe," (RemBERT model)"),RTe.forEach(t),nbo=i(L),h_=s(L,"LI",{});var PTe=n(h_);hY=s(PTe,"STRONG",{});var PLr=n(hY);lbo=r(PLr,"roberta"),PLr.forEach(t),ibo=r(PTe," \u2014 "),Mk=s(PTe,"A",{href:!0});var SLr=n(Mk);dbo=r(SLr,"RobertaForSequenceClassification"),SLr.forEach(t),cbo=r(PTe," (RoBERTa model)"),PTe.forEach(t),mbo=i(L),g_=s(L,"LI",{});var STe=n(g_);gY=s(STe,"STRONG",{});var $Lr=n(gY);fbo=r($Lr,"roformer"),$Lr.forEach(t),hbo=r(STe," \u2014 "),yk=s(STe,"A",{href:!0});var ILr=n(yk);gbo=r(ILr,"RoFormerForSequenceClassification"),ILr.forEach(t),ubo=r(STe," (RoFormer model)"),STe.forEach(t),pbo=i(L),u_=s(L,"LI",{});var $Te=n(u_);uY=s($Te,"STRONG",{});var DLr=n(uY);_bo=r(DLr,"squeezebert"),DLr.forEach(t),bbo=r($Te," \u2014 "),wk=s($Te,"A",{href:!0});var NLr=n(wk);vbo=r(NLr,"SqueezeBertForSequenceClassification"),NLr.forEach(t),Tbo=r($Te," (SqueezeBERT model)"),$Te.forEach(t),Fbo=i(L),p_=s(L,"LI",{});var ITe=n(p_);pY=s(ITe,"STRONG",{});var jLr=n(pY);Ebo=r(jLr,"tapas"),jLr.forEach(t),Cbo=r(ITe," \u2014 "),Ak=s(ITe,"A",{href:!0});var OLr=n(Ak);Mbo=r(OLr,"TapasForSequenceClassification"),OLr.forEach(t),ybo=r(ITe," (TAPAS model)"),ITe.forEach(t),wbo=i(L),__=s(L,"LI",{});var DTe=n(__);_Y=s(DTe,"STRONG",{});var GLr=n(_Y);Abo=r(GLr,"transfo-xl"),GLr.forEach(t),Lbo=r(DTe," \u2014 "),Lk=s(DTe,"A",{href:!0});var qLr=n(Lk);Bbo=r(qLr,"TransfoXLForSequenceClassification"),qLr.forEach(t),xbo=r(DTe," (Transformer-XL model)"),DTe.forEach(t),kbo=i(L),b_=s(L,"LI",{});var NTe=n(b_);bY=s(NTe,"STRONG",{});var zLr=n(bY);Rbo=r(zLr,"xlm"),zLr.forEach(t),Pbo=r(NTe," \u2014 "),Bk=s(NTe,"A",{href:!0});var XLr=n(Bk);Sbo=r(XLr,"XLMForSequenceClassification"),XLr.forEach(t),$bo=r(NTe," (XLM model)"),NTe.forEach(t),Ibo=i(L),v_=s(L,"LI",{});var jTe=n(v_);vY=s(jTe,"STRONG",{});var QLr=n(vY);Dbo=r(QLr,"xlm-roberta"),QLr.forEach(t),Nbo=r(jTe," \u2014 "),xk=s(jTe,"A",{href:!0});var VLr=n(xk);jbo=r(VLr,"XLMRobertaForSequenceClassification"),VLr.forEach(t),Obo=r(jTe," (XLM-RoBERTa model)"),jTe.forEach(t),Gbo=i(L),T_=s(L,"LI",{});var OTe=n(T_);TY=s(OTe,"STRONG",{});var WLr=n(TY);qbo=r(WLr,"xlnet"),WLr.forEach(t),zbo=r(OTe," \u2014 "),kk=s(OTe,"A",{href:!0});var HLr=n(kk);Xbo=r(HLr,"XLNetForSequenceClassification"),HLr.forEach(t),Qbo=r(OTe," (XLNet model)"),OTe.forEach(t),L.forEach(t),Vbo=i(Mt),F_=s(Mt,"P",{});var GTe=n(F_);Wbo=r(GTe,"The model is set in evaluation mode by default using "),FY=s(GTe,"CODE",{});var ULr=n(FY);Hbo=r(ULr,"model.eval()"),ULr.forEach(t),Ubo=r(GTe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),EY=s(GTe,"CODE",{});var JLr=n(EY);Jbo=r(JLr,"model.train()"),JLr.forEach(t),GTe.forEach(t),Kbo=i(Mt),CY=s(Mt,"P",{});var KLr=n(CY);Ybo=r(KLr,"Examples:"),KLr.forEach(t),Zbo=i(Mt),f(U3.$$.fragment,Mt),Mt.forEach(t),Cn.forEach(t),B5e=i(d),Bi=s(d,"H2",{class:!0});var Iwe=n(Bi);E_=s(Iwe,"A",{id:!0,class:!0,href:!0});var YLr=n(E_);MY=s(YLr,"SPAN",{});var ZLr=n(MY);f(J3.$$.fragment,ZLr),ZLr.forEach(t),YLr.forEach(t),e2o=i(Iwe),yY=s(Iwe,"SPAN",{});var e8r=n(yY);o2o=r(e8r,"AutoModelForMultipleChoice"),e8r.forEach(t),Iwe.forEach(t),x5e=i(d),Qo=s(d,"DIV",{class:!0});var yn=n(Qo);f(K3.$$.fragment,yn),r2o=i(yn),xi=s(yn,"P",{});var fj=n(xi);t2o=r(fj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),wY=s(fj,"CODE",{});var o8r=n(wY);a2o=r(o8r,"from_pretrained()"),o8r.forEach(t),s2o=r(fj,` class method or the
`),AY=s(fj,"CODE",{});var r8r=n(AY);n2o=r(r8r,"from_config()"),r8r.forEach(t),l2o=r(fj," class method."),fj.forEach(t),i2o=i(yn),Y3=s(yn,"P",{});var Dwe=n(Y3);d2o=r(Dwe,"This class cannot be instantiated directly using "),LY=s(Dwe,"CODE",{});var t8r=n(LY);c2o=r(t8r,"__init__()"),t8r.forEach(t),m2o=r(Dwe," (throws an error)."),Dwe.forEach(t),f2o=i(yn),Dr=s(yn,"DIV",{class:!0});var wn=n(Dr);f(Z3.$$.fragment,wn),h2o=i(wn),BY=s(wn,"P",{});var a8r=n(BY);g2o=r(a8r,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),a8r.forEach(t),u2o=i(wn),ki=s(wn,"P",{});var hj=n(ki);p2o=r(hj,`Note:
Loading a model from its configuration file does `),xY=s(hj,"STRONG",{});var s8r=n(xY);_2o=r(s8r,"not"),s8r.forEach(t),b2o=r(hj,` load the model weights. It only affects the
model\u2019s configuration. Use `),kY=s(hj,"CODE",{});var n8r=n(kY);v2o=r(n8r,"from_pretrained()"),n8r.forEach(t),T2o=r(hj,` to load the model
weights.`),hj.forEach(t),F2o=i(wn),RY=s(wn,"P",{});var l8r=n(RY);E2o=r(l8r,"Examples:"),l8r.forEach(t),C2o=i(wn),f(eM.$$.fragment,wn),wn.forEach(t),M2o=i(yn),Oe=s(yn,"DIV",{class:!0});var yt=n(Oe);f(oM.$$.fragment,yt),y2o=i(yt),PY=s(yt,"P",{});var i8r=n(PY);w2o=r(i8r,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),i8r.forEach(t),A2o=i(yt),Ra=s(yt,"P",{});var $4=n(Ra);L2o=r($4,"The model class to instantiate is selected based on the "),SY=s($4,"CODE",{});var d8r=n(SY);B2o=r(d8r,"model_type"),d8r.forEach(t),x2o=r($4,` property of the config object (either
passed as an argument or loaded from `),$Y=s($4,"CODE",{});var c8r=n($Y);k2o=r(c8r,"pretrained_model_name_or_path"),c8r.forEach(t),R2o=r($4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),IY=s($4,"CODE",{});var m8r=n(IY);P2o=r(m8r,"pretrained_model_name_or_path"),m8r.forEach(t),S2o=r($4,":"),$4.forEach(t),$2o=i(yt),H=s(yt,"UL",{});var J=n(H);C_=s(J,"LI",{});var qTe=n(C_);DY=s(qTe,"STRONG",{});var f8r=n(DY);I2o=r(f8r,"albert"),f8r.forEach(t),D2o=r(qTe," \u2014 "),Rk=s(qTe,"A",{href:!0});var h8r=n(Rk);N2o=r(h8r,"AlbertForMultipleChoice"),h8r.forEach(t),j2o=r(qTe," (ALBERT model)"),qTe.forEach(t),O2o=i(J),M_=s(J,"LI",{});var zTe=n(M_);NY=s(zTe,"STRONG",{});var g8r=n(NY);G2o=r(g8r,"bert"),g8r.forEach(t),q2o=r(zTe," \u2014 "),Pk=s(zTe,"A",{href:!0});var u8r=n(Pk);z2o=r(u8r,"BertForMultipleChoice"),u8r.forEach(t),X2o=r(zTe," (BERT model)"),zTe.forEach(t),Q2o=i(J),y_=s(J,"LI",{});var XTe=n(y_);jY=s(XTe,"STRONG",{});var p8r=n(jY);V2o=r(p8r,"big_bird"),p8r.forEach(t),W2o=r(XTe," \u2014 "),Sk=s(XTe,"A",{href:!0});var _8r=n(Sk);H2o=r(_8r,"BigBirdForMultipleChoice"),_8r.forEach(t),U2o=r(XTe," (BigBird model)"),XTe.forEach(t),J2o=i(J),w_=s(J,"LI",{});var QTe=n(w_);OY=s(QTe,"STRONG",{});var b8r=n(OY);K2o=r(b8r,"camembert"),b8r.forEach(t),Y2o=r(QTe," \u2014 "),$k=s(QTe,"A",{href:!0});var v8r=n($k);Z2o=r(v8r,"CamembertForMultipleChoice"),v8r.forEach(t),evo=r(QTe," (CamemBERT model)"),QTe.forEach(t),ovo=i(J),A_=s(J,"LI",{});var VTe=n(A_);GY=s(VTe,"STRONG",{});var T8r=n(GY);rvo=r(T8r,"canine"),T8r.forEach(t),tvo=r(VTe," \u2014 "),Ik=s(VTe,"A",{href:!0});var F8r=n(Ik);avo=r(F8r,"CanineForMultipleChoice"),F8r.forEach(t),svo=r(VTe," (Canine model)"),VTe.forEach(t),nvo=i(J),L_=s(J,"LI",{});var WTe=n(L_);qY=s(WTe,"STRONG",{});var E8r=n(qY);lvo=r(E8r,"convbert"),E8r.forEach(t),ivo=r(WTe," \u2014 "),Dk=s(WTe,"A",{href:!0});var C8r=n(Dk);dvo=r(C8r,"ConvBertForMultipleChoice"),C8r.forEach(t),cvo=r(WTe," (ConvBERT model)"),WTe.forEach(t),mvo=i(J),B_=s(J,"LI",{});var HTe=n(B_);zY=s(HTe,"STRONG",{});var M8r=n(zY);fvo=r(M8r,"distilbert"),M8r.forEach(t),hvo=r(HTe," \u2014 "),Nk=s(HTe,"A",{href:!0});var y8r=n(Nk);gvo=r(y8r,"DistilBertForMultipleChoice"),y8r.forEach(t),uvo=r(HTe," (DistilBERT model)"),HTe.forEach(t),pvo=i(J),x_=s(J,"LI",{});var UTe=n(x_);XY=s(UTe,"STRONG",{});var w8r=n(XY);_vo=r(w8r,"electra"),w8r.forEach(t),bvo=r(UTe," \u2014 "),jk=s(UTe,"A",{href:!0});var A8r=n(jk);vvo=r(A8r,"ElectraForMultipleChoice"),A8r.forEach(t),Tvo=r(UTe," (ELECTRA model)"),UTe.forEach(t),Fvo=i(J),k_=s(J,"LI",{});var JTe=n(k_);QY=s(JTe,"STRONG",{});var L8r=n(QY);Evo=r(L8r,"flaubert"),L8r.forEach(t),Cvo=r(JTe," \u2014 "),Ok=s(JTe,"A",{href:!0});var B8r=n(Ok);Mvo=r(B8r,"FlaubertForMultipleChoice"),B8r.forEach(t),yvo=r(JTe," (FlauBERT model)"),JTe.forEach(t),wvo=i(J),R_=s(J,"LI",{});var KTe=n(R_);VY=s(KTe,"STRONG",{});var x8r=n(VY);Avo=r(x8r,"fnet"),x8r.forEach(t),Lvo=r(KTe," \u2014 "),Gk=s(KTe,"A",{href:!0});var k8r=n(Gk);Bvo=r(k8r,"FNetForMultipleChoice"),k8r.forEach(t),xvo=r(KTe," (FNet model)"),KTe.forEach(t),kvo=i(J),P_=s(J,"LI",{});var YTe=n(P_);WY=s(YTe,"STRONG",{});var R8r=n(WY);Rvo=r(R8r,"funnel"),R8r.forEach(t),Pvo=r(YTe," \u2014 "),qk=s(YTe,"A",{href:!0});var P8r=n(qk);Svo=r(P8r,"FunnelForMultipleChoice"),P8r.forEach(t),$vo=r(YTe," (Funnel Transformer model)"),YTe.forEach(t),Ivo=i(J),S_=s(J,"LI",{});var ZTe=n(S_);HY=s(ZTe,"STRONG",{});var S8r=n(HY);Dvo=r(S8r,"ibert"),S8r.forEach(t),Nvo=r(ZTe," \u2014 "),zk=s(ZTe,"A",{href:!0});var $8r=n(zk);jvo=r($8r,"IBertForMultipleChoice"),$8r.forEach(t),Ovo=r(ZTe," (I-BERT model)"),ZTe.forEach(t),Gvo=i(J),$_=s(J,"LI",{});var e1e=n($_);UY=s(e1e,"STRONG",{});var I8r=n(UY);qvo=r(I8r,"longformer"),I8r.forEach(t),zvo=r(e1e," \u2014 "),Xk=s(e1e,"A",{href:!0});var D8r=n(Xk);Xvo=r(D8r,"LongformerForMultipleChoice"),D8r.forEach(t),Qvo=r(e1e," (Longformer model)"),e1e.forEach(t),Vvo=i(J),I_=s(J,"LI",{});var o1e=n(I_);JY=s(o1e,"STRONG",{});var N8r=n(JY);Wvo=r(N8r,"megatron-bert"),N8r.forEach(t),Hvo=r(o1e," \u2014 "),Qk=s(o1e,"A",{href:!0});var j8r=n(Qk);Uvo=r(j8r,"MegatronBertForMultipleChoice"),j8r.forEach(t),Jvo=r(o1e," (MegatronBert model)"),o1e.forEach(t),Kvo=i(J),D_=s(J,"LI",{});var r1e=n(D_);KY=s(r1e,"STRONG",{});var O8r=n(KY);Yvo=r(O8r,"mobilebert"),O8r.forEach(t),Zvo=r(r1e," \u2014 "),Vk=s(r1e,"A",{href:!0});var G8r=n(Vk);eTo=r(G8r,"MobileBertForMultipleChoice"),G8r.forEach(t),oTo=r(r1e," (MobileBERT model)"),r1e.forEach(t),rTo=i(J),N_=s(J,"LI",{});var t1e=n(N_);YY=s(t1e,"STRONG",{});var q8r=n(YY);tTo=r(q8r,"mpnet"),q8r.forEach(t),aTo=r(t1e," \u2014 "),Wk=s(t1e,"A",{href:!0});var z8r=n(Wk);sTo=r(z8r,"MPNetForMultipleChoice"),z8r.forEach(t),nTo=r(t1e," (MPNet model)"),t1e.forEach(t),lTo=i(J),j_=s(J,"LI",{});var a1e=n(j_);ZY=s(a1e,"STRONG",{});var X8r=n(ZY);iTo=r(X8r,"qdqbert"),X8r.forEach(t),dTo=r(a1e," \u2014 "),Hk=s(a1e,"A",{href:!0});var Q8r=n(Hk);cTo=r(Q8r,"QDQBertForMultipleChoice"),Q8r.forEach(t),mTo=r(a1e," (QDQBert model)"),a1e.forEach(t),fTo=i(J),O_=s(J,"LI",{});var s1e=n(O_);eZ=s(s1e,"STRONG",{});var V8r=n(eZ);hTo=r(V8r,"rembert"),V8r.forEach(t),gTo=r(s1e," \u2014 "),Uk=s(s1e,"A",{href:!0});var W8r=n(Uk);uTo=r(W8r,"RemBertForMultipleChoice"),W8r.forEach(t),pTo=r(s1e," (RemBERT model)"),s1e.forEach(t),_To=i(J),G_=s(J,"LI",{});var n1e=n(G_);oZ=s(n1e,"STRONG",{});var H8r=n(oZ);bTo=r(H8r,"roberta"),H8r.forEach(t),vTo=r(n1e," \u2014 "),Jk=s(n1e,"A",{href:!0});var U8r=n(Jk);TTo=r(U8r,"RobertaForMultipleChoice"),U8r.forEach(t),FTo=r(n1e," (RoBERTa model)"),n1e.forEach(t),ETo=i(J),q_=s(J,"LI",{});var l1e=n(q_);rZ=s(l1e,"STRONG",{});var J8r=n(rZ);CTo=r(J8r,"roformer"),J8r.forEach(t),MTo=r(l1e," \u2014 "),Kk=s(l1e,"A",{href:!0});var K8r=n(Kk);yTo=r(K8r,"RoFormerForMultipleChoice"),K8r.forEach(t),wTo=r(l1e," (RoFormer model)"),l1e.forEach(t),ATo=i(J),z_=s(J,"LI",{});var i1e=n(z_);tZ=s(i1e,"STRONG",{});var Y8r=n(tZ);LTo=r(Y8r,"squeezebert"),Y8r.forEach(t),BTo=r(i1e," \u2014 "),Yk=s(i1e,"A",{href:!0});var Z8r=n(Yk);xTo=r(Z8r,"SqueezeBertForMultipleChoice"),Z8r.forEach(t),kTo=r(i1e," (SqueezeBERT model)"),i1e.forEach(t),RTo=i(J),X_=s(J,"LI",{});var d1e=n(X_);aZ=s(d1e,"STRONG",{});var eBr=n(aZ);PTo=r(eBr,"xlm"),eBr.forEach(t),STo=r(d1e," \u2014 "),Zk=s(d1e,"A",{href:!0});var oBr=n(Zk);$To=r(oBr,"XLMForMultipleChoice"),oBr.forEach(t),ITo=r(d1e," (XLM model)"),d1e.forEach(t),DTo=i(J),Q_=s(J,"LI",{});var c1e=n(Q_);sZ=s(c1e,"STRONG",{});var rBr=n(sZ);NTo=r(rBr,"xlm-roberta"),rBr.forEach(t),jTo=r(c1e," \u2014 "),eR=s(c1e,"A",{href:!0});var tBr=n(eR);OTo=r(tBr,"XLMRobertaForMultipleChoice"),tBr.forEach(t),GTo=r(c1e," (XLM-RoBERTa model)"),c1e.forEach(t),qTo=i(J),V_=s(J,"LI",{});var m1e=n(V_);nZ=s(m1e,"STRONG",{});var aBr=n(nZ);zTo=r(aBr,"xlnet"),aBr.forEach(t),XTo=r(m1e," \u2014 "),oR=s(m1e,"A",{href:!0});var sBr=n(oR);QTo=r(sBr,"XLNetForMultipleChoice"),sBr.forEach(t),VTo=r(m1e," (XLNet model)"),m1e.forEach(t),J.forEach(t),WTo=i(yt),W_=s(yt,"P",{});var f1e=n(W_);HTo=r(f1e,"The model is set in evaluation mode by default using "),lZ=s(f1e,"CODE",{});var nBr=n(lZ);UTo=r(nBr,"model.eval()"),nBr.forEach(t),JTo=r(f1e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),iZ=s(f1e,"CODE",{});var lBr=n(iZ);KTo=r(lBr,"model.train()"),lBr.forEach(t),f1e.forEach(t),YTo=i(yt),dZ=s(yt,"P",{});var iBr=n(dZ);ZTo=r(iBr,"Examples:"),iBr.forEach(t),e1o=i(yt),f(rM.$$.fragment,yt),yt.forEach(t),yn.forEach(t),k5e=i(d),Ri=s(d,"H2",{class:!0});var Nwe=n(Ri);H_=s(Nwe,"A",{id:!0,class:!0,href:!0});var dBr=n(H_);cZ=s(dBr,"SPAN",{});var cBr=n(cZ);f(tM.$$.fragment,cBr),cBr.forEach(t),dBr.forEach(t),o1o=i(Nwe),mZ=s(Nwe,"SPAN",{});var mBr=n(mZ);r1o=r(mBr,"AutoModelForNextSentencePrediction"),mBr.forEach(t),Nwe.forEach(t),R5e=i(d),Vo=s(d,"DIV",{class:!0});var An=n(Vo);f(aM.$$.fragment,An),t1o=i(An),Pi=s(An,"P",{});var gj=n(Pi);a1o=r(gj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),fZ=s(gj,"CODE",{});var fBr=n(fZ);s1o=r(fBr,"from_pretrained()"),fBr.forEach(t),n1o=r(gj,` class method or the
`),hZ=s(gj,"CODE",{});var hBr=n(hZ);l1o=r(hBr,"from_config()"),hBr.forEach(t),i1o=r(gj," class method."),gj.forEach(t),d1o=i(An),sM=s(An,"P",{});var jwe=n(sM);c1o=r(jwe,"This class cannot be instantiated directly using "),gZ=s(jwe,"CODE",{});var gBr=n(gZ);m1o=r(gBr,"__init__()"),gBr.forEach(t),f1o=r(jwe," (throws an error)."),jwe.forEach(t),h1o=i(An),Nr=s(An,"DIV",{class:!0});var Ln=n(Nr);f(nM.$$.fragment,Ln),g1o=i(Ln),uZ=s(Ln,"P",{});var uBr=n(uZ);u1o=r(uBr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),uBr.forEach(t),p1o=i(Ln),Si=s(Ln,"P",{});var uj=n(Si);_1o=r(uj,`Note:
Loading a model from its configuration file does `),pZ=s(uj,"STRONG",{});var pBr=n(pZ);b1o=r(pBr,"not"),pBr.forEach(t),v1o=r(uj,` load the model weights. It only affects the
model\u2019s configuration. Use `),_Z=s(uj,"CODE",{});var _Br=n(_Z);T1o=r(_Br,"from_pretrained()"),_Br.forEach(t),F1o=r(uj,` to load the model
weights.`),uj.forEach(t),E1o=i(Ln),bZ=s(Ln,"P",{});var bBr=n(bZ);C1o=r(bBr,"Examples:"),bBr.forEach(t),M1o=i(Ln),f(lM.$$.fragment,Ln),Ln.forEach(t),y1o=i(An),Ge=s(An,"DIV",{class:!0});var wt=n(Ge);f(iM.$$.fragment,wt),w1o=i(wt),vZ=s(wt,"P",{});var vBr=n(vZ);A1o=r(vBr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),vBr.forEach(t),L1o=i(wt),Pa=s(wt,"P",{});var I4=n(Pa);B1o=r(I4,"The model class to instantiate is selected based on the "),TZ=s(I4,"CODE",{});var TBr=n(TZ);x1o=r(TBr,"model_type"),TBr.forEach(t),k1o=r(I4,` property of the config object (either
passed as an argument or loaded from `),FZ=s(I4,"CODE",{});var FBr=n(FZ);R1o=r(FBr,"pretrained_model_name_or_path"),FBr.forEach(t),P1o=r(I4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),EZ=s(I4,"CODE",{});var EBr=n(EZ);S1o=r(EBr,"pretrained_model_name_or_path"),EBr.forEach(t),$1o=r(I4,":"),I4.forEach(t),I1o=i(wt),jt=s(wt,"UL",{});var Bn=n(jt);U_=s(Bn,"LI",{});var h1e=n(U_);CZ=s(h1e,"STRONG",{});var CBr=n(CZ);D1o=r(CBr,"bert"),CBr.forEach(t),N1o=r(h1e," \u2014 "),rR=s(h1e,"A",{href:!0});var MBr=n(rR);j1o=r(MBr,"BertForNextSentencePrediction"),MBr.forEach(t),O1o=r(h1e," (BERT model)"),h1e.forEach(t),G1o=i(Bn),J_=s(Bn,"LI",{});var g1e=n(J_);MZ=s(g1e,"STRONG",{});var yBr=n(MZ);q1o=r(yBr,"fnet"),yBr.forEach(t),z1o=r(g1e," \u2014 "),tR=s(g1e,"A",{href:!0});var wBr=n(tR);X1o=r(wBr,"FNetForNextSentencePrediction"),wBr.forEach(t),Q1o=r(g1e," (FNet model)"),g1e.forEach(t),V1o=i(Bn),K_=s(Bn,"LI",{});var u1e=n(K_);yZ=s(u1e,"STRONG",{});var ABr=n(yZ);W1o=r(ABr,"megatron-bert"),ABr.forEach(t),H1o=r(u1e," \u2014 "),aR=s(u1e,"A",{href:!0});var LBr=n(aR);U1o=r(LBr,"MegatronBertForNextSentencePrediction"),LBr.forEach(t),J1o=r(u1e," (MegatronBert model)"),u1e.forEach(t),K1o=i(Bn),Y_=s(Bn,"LI",{});var p1e=n(Y_);wZ=s(p1e,"STRONG",{});var BBr=n(wZ);Y1o=r(BBr,"mobilebert"),BBr.forEach(t),Z1o=r(p1e," \u2014 "),sR=s(p1e,"A",{href:!0});var xBr=n(sR);eFo=r(xBr,"MobileBertForNextSentencePrediction"),xBr.forEach(t),oFo=r(p1e," (MobileBERT model)"),p1e.forEach(t),rFo=i(Bn),Z_=s(Bn,"LI",{});var _1e=n(Z_);AZ=s(_1e,"STRONG",{});var kBr=n(AZ);tFo=r(kBr,"qdqbert"),kBr.forEach(t),aFo=r(_1e," \u2014 "),nR=s(_1e,"A",{href:!0});var RBr=n(nR);sFo=r(RBr,"QDQBertForNextSentencePrediction"),RBr.forEach(t),nFo=r(_1e," (QDQBert model)"),_1e.forEach(t),Bn.forEach(t),lFo=i(wt),eb=s(wt,"P",{});var b1e=n(eb);iFo=r(b1e,"The model is set in evaluation mode by default using "),LZ=s(b1e,"CODE",{});var PBr=n(LZ);dFo=r(PBr,"model.eval()"),PBr.forEach(t),cFo=r(b1e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),BZ=s(b1e,"CODE",{});var SBr=n(BZ);mFo=r(SBr,"model.train()"),SBr.forEach(t),b1e.forEach(t),fFo=i(wt),xZ=s(wt,"P",{});var $Br=n(xZ);hFo=r($Br,"Examples:"),$Br.forEach(t),gFo=i(wt),f(dM.$$.fragment,wt),wt.forEach(t),An.forEach(t),P5e=i(d),$i=s(d,"H2",{class:!0});var Owe=n($i);ob=s(Owe,"A",{id:!0,class:!0,href:!0});var IBr=n(ob);kZ=s(IBr,"SPAN",{});var DBr=n(kZ);f(cM.$$.fragment,DBr),DBr.forEach(t),IBr.forEach(t),uFo=i(Owe),RZ=s(Owe,"SPAN",{});var NBr=n(RZ);pFo=r(NBr,"AutoModelForTokenClassification"),NBr.forEach(t),Owe.forEach(t),S5e=i(d),Wo=s(d,"DIV",{class:!0});var xn=n(Wo);f(mM.$$.fragment,xn),_Fo=i(xn),Ii=s(xn,"P",{});var pj=n(Ii);bFo=r(pj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),PZ=s(pj,"CODE",{});var jBr=n(PZ);vFo=r(jBr,"from_pretrained()"),jBr.forEach(t),TFo=r(pj,` class method or the
`),SZ=s(pj,"CODE",{});var OBr=n(SZ);FFo=r(OBr,"from_config()"),OBr.forEach(t),EFo=r(pj," class method."),pj.forEach(t),CFo=i(xn),fM=s(xn,"P",{});var Gwe=n(fM);MFo=r(Gwe,"This class cannot be instantiated directly using "),$Z=s(Gwe,"CODE",{});var GBr=n($Z);yFo=r(GBr,"__init__()"),GBr.forEach(t),wFo=r(Gwe," (throws an error)."),Gwe.forEach(t),AFo=i(xn),jr=s(xn,"DIV",{class:!0});var kn=n(jr);f(hM.$$.fragment,kn),LFo=i(kn),IZ=s(kn,"P",{});var qBr=n(IZ);BFo=r(qBr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),qBr.forEach(t),xFo=i(kn),Di=s(kn,"P",{});var _j=n(Di);kFo=r(_j,`Note:
Loading a model from its configuration file does `),DZ=s(_j,"STRONG",{});var zBr=n(DZ);RFo=r(zBr,"not"),zBr.forEach(t),PFo=r(_j,` load the model weights. It only affects the
model\u2019s configuration. Use `),NZ=s(_j,"CODE",{});var XBr=n(NZ);SFo=r(XBr,"from_pretrained()"),XBr.forEach(t),$Fo=r(_j,` to load the model
weights.`),_j.forEach(t),IFo=i(kn),jZ=s(kn,"P",{});var QBr=n(jZ);DFo=r(QBr,"Examples:"),QBr.forEach(t),NFo=i(kn),f(gM.$$.fragment,kn),kn.forEach(t),jFo=i(xn),qe=s(xn,"DIV",{class:!0});var At=n(qe);f(uM.$$.fragment,At),OFo=i(At),OZ=s(At,"P",{});var VBr=n(OZ);GFo=r(VBr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),VBr.forEach(t),qFo=i(At),Sa=s(At,"P",{});var D4=n(Sa);zFo=r(D4,"The model class to instantiate is selected based on the "),GZ=s(D4,"CODE",{});var WBr=n(GZ);XFo=r(WBr,"model_type"),WBr.forEach(t),QFo=r(D4,` property of the config object (either
passed as an argument or loaded from `),qZ=s(D4,"CODE",{});var HBr=n(qZ);VFo=r(HBr,"pretrained_model_name_or_path"),HBr.forEach(t),WFo=r(D4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),zZ=s(D4,"CODE",{});var UBr=n(zZ);HFo=r(UBr,"pretrained_model_name_or_path"),UBr.forEach(t),UFo=r(D4,":"),D4.forEach(t),JFo=i(At),X=s(At,"UL",{});var V=n(X);rb=s(V,"LI",{});var v1e=n(rb);XZ=s(v1e,"STRONG",{});var JBr=n(XZ);KFo=r(JBr,"albert"),JBr.forEach(t),YFo=r(v1e," \u2014 "),lR=s(v1e,"A",{href:!0});var KBr=n(lR);ZFo=r(KBr,"AlbertForTokenClassification"),KBr.forEach(t),eEo=r(v1e," (ALBERT model)"),v1e.forEach(t),oEo=i(V),tb=s(V,"LI",{});var T1e=n(tb);QZ=s(T1e,"STRONG",{});var YBr=n(QZ);rEo=r(YBr,"bert"),YBr.forEach(t),tEo=r(T1e," \u2014 "),iR=s(T1e,"A",{href:!0});var ZBr=n(iR);aEo=r(ZBr,"BertForTokenClassification"),ZBr.forEach(t),sEo=r(T1e," (BERT model)"),T1e.forEach(t),nEo=i(V),ab=s(V,"LI",{});var F1e=n(ab);VZ=s(F1e,"STRONG",{});var e9r=n(VZ);lEo=r(e9r,"big_bird"),e9r.forEach(t),iEo=r(F1e," \u2014 "),dR=s(F1e,"A",{href:!0});var o9r=n(dR);dEo=r(o9r,"BigBirdForTokenClassification"),o9r.forEach(t),cEo=r(F1e," (BigBird model)"),F1e.forEach(t),mEo=i(V),sb=s(V,"LI",{});var E1e=n(sb);WZ=s(E1e,"STRONG",{});var r9r=n(WZ);fEo=r(r9r,"camembert"),r9r.forEach(t),hEo=r(E1e," \u2014 "),cR=s(E1e,"A",{href:!0});var t9r=n(cR);gEo=r(t9r,"CamembertForTokenClassification"),t9r.forEach(t),uEo=r(E1e," (CamemBERT model)"),E1e.forEach(t),pEo=i(V),nb=s(V,"LI",{});var C1e=n(nb);HZ=s(C1e,"STRONG",{});var a9r=n(HZ);_Eo=r(a9r,"canine"),a9r.forEach(t),bEo=r(C1e," \u2014 "),mR=s(C1e,"A",{href:!0});var s9r=n(mR);vEo=r(s9r,"CanineForTokenClassification"),s9r.forEach(t),TEo=r(C1e," (Canine model)"),C1e.forEach(t),FEo=i(V),lb=s(V,"LI",{});var M1e=n(lb);UZ=s(M1e,"STRONG",{});var n9r=n(UZ);EEo=r(n9r,"convbert"),n9r.forEach(t),CEo=r(M1e," \u2014 "),fR=s(M1e,"A",{href:!0});var l9r=n(fR);MEo=r(l9r,"ConvBertForTokenClassification"),l9r.forEach(t),yEo=r(M1e," (ConvBERT model)"),M1e.forEach(t),wEo=i(V),ib=s(V,"LI",{});var y1e=n(ib);JZ=s(y1e,"STRONG",{});var i9r=n(JZ);AEo=r(i9r,"deberta"),i9r.forEach(t),LEo=r(y1e," \u2014 "),hR=s(y1e,"A",{href:!0});var d9r=n(hR);BEo=r(d9r,"DebertaForTokenClassification"),d9r.forEach(t),xEo=r(y1e," (DeBERTa model)"),y1e.forEach(t),kEo=i(V),db=s(V,"LI",{});var w1e=n(db);KZ=s(w1e,"STRONG",{});var c9r=n(KZ);REo=r(c9r,"deberta-v2"),c9r.forEach(t),PEo=r(w1e," \u2014 "),gR=s(w1e,"A",{href:!0});var m9r=n(gR);SEo=r(m9r,"DebertaV2ForTokenClassification"),m9r.forEach(t),$Eo=r(w1e," (DeBERTa-v2 model)"),w1e.forEach(t),IEo=i(V),cb=s(V,"LI",{});var A1e=n(cb);YZ=s(A1e,"STRONG",{});var f9r=n(YZ);DEo=r(f9r,"distilbert"),f9r.forEach(t),NEo=r(A1e," \u2014 "),uR=s(A1e,"A",{href:!0});var h9r=n(uR);jEo=r(h9r,"DistilBertForTokenClassification"),h9r.forEach(t),OEo=r(A1e," (DistilBERT model)"),A1e.forEach(t),GEo=i(V),mb=s(V,"LI",{});var L1e=n(mb);ZZ=s(L1e,"STRONG",{});var g9r=n(ZZ);qEo=r(g9r,"electra"),g9r.forEach(t),zEo=r(L1e," \u2014 "),pR=s(L1e,"A",{href:!0});var u9r=n(pR);XEo=r(u9r,"ElectraForTokenClassification"),u9r.forEach(t),QEo=r(L1e," (ELECTRA model)"),L1e.forEach(t),VEo=i(V),fb=s(V,"LI",{});var B1e=n(fb);eee=s(B1e,"STRONG",{});var p9r=n(eee);WEo=r(p9r,"flaubert"),p9r.forEach(t),HEo=r(B1e," \u2014 "),_R=s(B1e,"A",{href:!0});var _9r=n(_R);UEo=r(_9r,"FlaubertForTokenClassification"),_9r.forEach(t),JEo=r(B1e," (FlauBERT model)"),B1e.forEach(t),KEo=i(V),hb=s(V,"LI",{});var x1e=n(hb);oee=s(x1e,"STRONG",{});var b9r=n(oee);YEo=r(b9r,"fnet"),b9r.forEach(t),ZEo=r(x1e," \u2014 "),bR=s(x1e,"A",{href:!0});var v9r=n(bR);e4o=r(v9r,"FNetForTokenClassification"),v9r.forEach(t),o4o=r(x1e," (FNet model)"),x1e.forEach(t),r4o=i(V),gb=s(V,"LI",{});var k1e=n(gb);ree=s(k1e,"STRONG",{});var T9r=n(ree);t4o=r(T9r,"funnel"),T9r.forEach(t),a4o=r(k1e," \u2014 "),vR=s(k1e,"A",{href:!0});var F9r=n(vR);s4o=r(F9r,"FunnelForTokenClassification"),F9r.forEach(t),n4o=r(k1e," (Funnel Transformer model)"),k1e.forEach(t),l4o=i(V),ub=s(V,"LI",{});var R1e=n(ub);tee=s(R1e,"STRONG",{});var E9r=n(tee);i4o=r(E9r,"gpt2"),E9r.forEach(t),d4o=r(R1e," \u2014 "),TR=s(R1e,"A",{href:!0});var C9r=n(TR);c4o=r(C9r,"GPT2ForTokenClassification"),C9r.forEach(t),m4o=r(R1e," (OpenAI GPT-2 model)"),R1e.forEach(t),f4o=i(V),pb=s(V,"LI",{});var P1e=n(pb);aee=s(P1e,"STRONG",{});var M9r=n(aee);h4o=r(M9r,"ibert"),M9r.forEach(t),g4o=r(P1e," \u2014 "),FR=s(P1e,"A",{href:!0});var y9r=n(FR);u4o=r(y9r,"IBertForTokenClassification"),y9r.forEach(t),p4o=r(P1e," (I-BERT model)"),P1e.forEach(t),_4o=i(V),_b=s(V,"LI",{});var S1e=n(_b);see=s(S1e,"STRONG",{});var w9r=n(see);b4o=r(w9r,"layoutlm"),w9r.forEach(t),v4o=r(S1e," \u2014 "),ER=s(S1e,"A",{href:!0});var A9r=n(ER);T4o=r(A9r,"LayoutLMForTokenClassification"),A9r.forEach(t),F4o=r(S1e," (LayoutLM model)"),S1e.forEach(t),E4o=i(V),bb=s(V,"LI",{});var $1e=n(bb);nee=s($1e,"STRONG",{});var L9r=n(nee);C4o=r(L9r,"layoutlmv2"),L9r.forEach(t),M4o=r($1e," \u2014 "),CR=s($1e,"A",{href:!0});var B9r=n(CR);y4o=r(B9r,"LayoutLMv2ForTokenClassification"),B9r.forEach(t),w4o=r($1e," (LayoutLMv2 model)"),$1e.forEach(t),A4o=i(V),vb=s(V,"LI",{});var I1e=n(vb);lee=s(I1e,"STRONG",{});var x9r=n(lee);L4o=r(x9r,"longformer"),x9r.forEach(t),B4o=r(I1e," \u2014 "),MR=s(I1e,"A",{href:!0});var k9r=n(MR);x4o=r(k9r,"LongformerForTokenClassification"),k9r.forEach(t),k4o=r(I1e," (Longformer model)"),I1e.forEach(t),R4o=i(V),Tb=s(V,"LI",{});var D1e=n(Tb);iee=s(D1e,"STRONG",{});var R9r=n(iee);P4o=r(R9r,"megatron-bert"),R9r.forEach(t),S4o=r(D1e," \u2014 "),yR=s(D1e,"A",{href:!0});var P9r=n(yR);$4o=r(P9r,"MegatronBertForTokenClassification"),P9r.forEach(t),I4o=r(D1e," (MegatronBert model)"),D1e.forEach(t),D4o=i(V),Fb=s(V,"LI",{});var N1e=n(Fb);dee=s(N1e,"STRONG",{});var S9r=n(dee);N4o=r(S9r,"mobilebert"),S9r.forEach(t),j4o=r(N1e," \u2014 "),wR=s(N1e,"A",{href:!0});var $9r=n(wR);O4o=r($9r,"MobileBertForTokenClassification"),$9r.forEach(t),G4o=r(N1e," (MobileBERT model)"),N1e.forEach(t),q4o=i(V),Eb=s(V,"LI",{});var j1e=n(Eb);cee=s(j1e,"STRONG",{});var I9r=n(cee);z4o=r(I9r,"mpnet"),I9r.forEach(t),X4o=r(j1e," \u2014 "),AR=s(j1e,"A",{href:!0});var D9r=n(AR);Q4o=r(D9r,"MPNetForTokenClassification"),D9r.forEach(t),V4o=r(j1e," (MPNet model)"),j1e.forEach(t),W4o=i(V),Cb=s(V,"LI",{});var O1e=n(Cb);mee=s(O1e,"STRONG",{});var N9r=n(mee);H4o=r(N9r,"qdqbert"),N9r.forEach(t),U4o=r(O1e," \u2014 "),LR=s(O1e,"A",{href:!0});var j9r=n(LR);J4o=r(j9r,"QDQBertForTokenClassification"),j9r.forEach(t),K4o=r(O1e," (QDQBert model)"),O1e.forEach(t),Y4o=i(V),Mb=s(V,"LI",{});var G1e=n(Mb);fee=s(G1e,"STRONG",{});var O9r=n(fee);Z4o=r(O9r,"rembert"),O9r.forEach(t),eCo=r(G1e," \u2014 "),BR=s(G1e,"A",{href:!0});var G9r=n(BR);oCo=r(G9r,"RemBertForTokenClassification"),G9r.forEach(t),rCo=r(G1e," (RemBERT model)"),G1e.forEach(t),tCo=i(V),yb=s(V,"LI",{});var q1e=n(yb);hee=s(q1e,"STRONG",{});var q9r=n(hee);aCo=r(q9r,"roberta"),q9r.forEach(t),sCo=r(q1e," \u2014 "),xR=s(q1e,"A",{href:!0});var z9r=n(xR);nCo=r(z9r,"RobertaForTokenClassification"),z9r.forEach(t),lCo=r(q1e," (RoBERTa model)"),q1e.forEach(t),iCo=i(V),wb=s(V,"LI",{});var z1e=n(wb);gee=s(z1e,"STRONG",{});var X9r=n(gee);dCo=r(X9r,"roformer"),X9r.forEach(t),cCo=r(z1e," \u2014 "),kR=s(z1e,"A",{href:!0});var Q9r=n(kR);mCo=r(Q9r,"RoFormerForTokenClassification"),Q9r.forEach(t),fCo=r(z1e," (RoFormer model)"),z1e.forEach(t),hCo=i(V),Ab=s(V,"LI",{});var X1e=n(Ab);uee=s(X1e,"STRONG",{});var V9r=n(uee);gCo=r(V9r,"squeezebert"),V9r.forEach(t),uCo=r(X1e," \u2014 "),RR=s(X1e,"A",{href:!0});var W9r=n(RR);pCo=r(W9r,"SqueezeBertForTokenClassification"),W9r.forEach(t),_Co=r(X1e," (SqueezeBERT model)"),X1e.forEach(t),bCo=i(V),Lb=s(V,"LI",{});var Q1e=n(Lb);pee=s(Q1e,"STRONG",{});var H9r=n(pee);vCo=r(H9r,"xlm"),H9r.forEach(t),TCo=r(Q1e," \u2014 "),PR=s(Q1e,"A",{href:!0});var U9r=n(PR);FCo=r(U9r,"XLMForTokenClassification"),U9r.forEach(t),ECo=r(Q1e," (XLM model)"),Q1e.forEach(t),CCo=i(V),Bb=s(V,"LI",{});var V1e=n(Bb);_ee=s(V1e,"STRONG",{});var J9r=n(_ee);MCo=r(J9r,"xlm-roberta"),J9r.forEach(t),yCo=r(V1e," \u2014 "),SR=s(V1e,"A",{href:!0});var K9r=n(SR);wCo=r(K9r,"XLMRobertaForTokenClassification"),K9r.forEach(t),ACo=r(V1e," (XLM-RoBERTa model)"),V1e.forEach(t),LCo=i(V),xb=s(V,"LI",{});var W1e=n(xb);bee=s(W1e,"STRONG",{});var Y9r=n(bee);BCo=r(Y9r,"xlnet"),Y9r.forEach(t),xCo=r(W1e," \u2014 "),$R=s(W1e,"A",{href:!0});var Z9r=n($R);kCo=r(Z9r,"XLNetForTokenClassification"),Z9r.forEach(t),RCo=r(W1e," (XLNet model)"),W1e.forEach(t),V.forEach(t),PCo=i(At),kb=s(At,"P",{});var H1e=n(kb);SCo=r(H1e,"The model is set in evaluation mode by default using "),vee=s(H1e,"CODE",{});var exr=n(vee);$Co=r(exr,"model.eval()"),exr.forEach(t),ICo=r(H1e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tee=s(H1e,"CODE",{});var oxr=n(Tee);DCo=r(oxr,"model.train()"),oxr.forEach(t),H1e.forEach(t),NCo=i(At),Fee=s(At,"P",{});var rxr=n(Fee);jCo=r(rxr,"Examples:"),rxr.forEach(t),OCo=i(At),f(pM.$$.fragment,At),At.forEach(t),xn.forEach(t),$5e=i(d),Ni=s(d,"H2",{class:!0});var qwe=n(Ni);Rb=s(qwe,"A",{id:!0,class:!0,href:!0});var txr=n(Rb);Eee=s(txr,"SPAN",{});var axr=n(Eee);f(_M.$$.fragment,axr),axr.forEach(t),txr.forEach(t),GCo=i(qwe),Cee=s(qwe,"SPAN",{});var sxr=n(Cee);qCo=r(sxr,"AutoModelForQuestionAnswering"),sxr.forEach(t),qwe.forEach(t),I5e=i(d),Ho=s(d,"DIV",{class:!0});var Rn=n(Ho);f(bM.$$.fragment,Rn),zCo=i(Rn),ji=s(Rn,"P",{});var bj=n(ji);XCo=r(bj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Mee=s(bj,"CODE",{});var nxr=n(Mee);QCo=r(nxr,"from_pretrained()"),nxr.forEach(t),VCo=r(bj,` class method or the
`),yee=s(bj,"CODE",{});var lxr=n(yee);WCo=r(lxr,"from_config()"),lxr.forEach(t),HCo=r(bj," class method."),bj.forEach(t),UCo=i(Rn),vM=s(Rn,"P",{});var zwe=n(vM);JCo=r(zwe,"This class cannot be instantiated directly using "),wee=s(zwe,"CODE",{});var ixr=n(wee);KCo=r(ixr,"__init__()"),ixr.forEach(t),YCo=r(zwe," (throws an error)."),zwe.forEach(t),ZCo=i(Rn),Or=s(Rn,"DIV",{class:!0});var Pn=n(Or);f(TM.$$.fragment,Pn),e3o=i(Pn),Aee=s(Pn,"P",{});var dxr=n(Aee);o3o=r(dxr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),dxr.forEach(t),r3o=i(Pn),Oi=s(Pn,"P",{});var vj=n(Oi);t3o=r(vj,`Note:
Loading a model from its configuration file does `),Lee=s(vj,"STRONG",{});var cxr=n(Lee);a3o=r(cxr,"not"),cxr.forEach(t),s3o=r(vj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bee=s(vj,"CODE",{});var mxr=n(Bee);n3o=r(mxr,"from_pretrained()"),mxr.forEach(t),l3o=r(vj,` to load the model
weights.`),vj.forEach(t),i3o=i(Pn),xee=s(Pn,"P",{});var fxr=n(xee);d3o=r(fxr,"Examples:"),fxr.forEach(t),c3o=i(Pn),f(FM.$$.fragment,Pn),Pn.forEach(t),m3o=i(Rn),ze=s(Rn,"DIV",{class:!0});var Lt=n(ze);f(EM.$$.fragment,Lt),f3o=i(Lt),kee=s(Lt,"P",{});var hxr=n(kee);h3o=r(hxr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),hxr.forEach(t),g3o=i(Lt),$a=s(Lt,"P",{});var N4=n($a);u3o=r(N4,"The model class to instantiate is selected based on the "),Ree=s(N4,"CODE",{});var gxr=n(Ree);p3o=r(gxr,"model_type"),gxr.forEach(t),_3o=r(N4,` property of the config object (either
passed as an argument or loaded from `),Pee=s(N4,"CODE",{});var uxr=n(Pee);b3o=r(uxr,"pretrained_model_name_or_path"),uxr.forEach(t),v3o=r(N4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),See=s(N4,"CODE",{});var pxr=n(See);T3o=r(pxr,"pretrained_model_name_or_path"),pxr.forEach(t),F3o=r(N4,":"),N4.forEach(t),E3o=i(Lt),S=s(Lt,"UL",{});var I=n(S);Pb=s(I,"LI",{});var U1e=n(Pb);$ee=s(U1e,"STRONG",{});var _xr=n($ee);C3o=r(_xr,"albert"),_xr.forEach(t),M3o=r(U1e," \u2014 "),IR=s(U1e,"A",{href:!0});var bxr=n(IR);y3o=r(bxr,"AlbertForQuestionAnswering"),bxr.forEach(t),w3o=r(U1e," (ALBERT model)"),U1e.forEach(t),A3o=i(I),Sb=s(I,"LI",{});var J1e=n(Sb);Iee=s(J1e,"STRONG",{});var vxr=n(Iee);L3o=r(vxr,"bart"),vxr.forEach(t),B3o=r(J1e," \u2014 "),DR=s(J1e,"A",{href:!0});var Txr=n(DR);x3o=r(Txr,"BartForQuestionAnswering"),Txr.forEach(t),k3o=r(J1e," (BART model)"),J1e.forEach(t),R3o=i(I),$b=s(I,"LI",{});var K1e=n($b);Dee=s(K1e,"STRONG",{});var Fxr=n(Dee);P3o=r(Fxr,"bert"),Fxr.forEach(t),S3o=r(K1e," \u2014 "),NR=s(K1e,"A",{href:!0});var Exr=n(NR);$3o=r(Exr,"BertForQuestionAnswering"),Exr.forEach(t),I3o=r(K1e," (BERT model)"),K1e.forEach(t),D3o=i(I),Ib=s(I,"LI",{});var Y1e=n(Ib);Nee=s(Y1e,"STRONG",{});var Cxr=n(Nee);N3o=r(Cxr,"big_bird"),Cxr.forEach(t),j3o=r(Y1e," \u2014 "),jR=s(Y1e,"A",{href:!0});var Mxr=n(jR);O3o=r(Mxr,"BigBirdForQuestionAnswering"),Mxr.forEach(t),G3o=r(Y1e," (BigBird model)"),Y1e.forEach(t),q3o=i(I),Db=s(I,"LI",{});var Z1e=n(Db);jee=s(Z1e,"STRONG",{});var yxr=n(jee);z3o=r(yxr,"bigbird_pegasus"),yxr.forEach(t),X3o=r(Z1e," \u2014 "),OR=s(Z1e,"A",{href:!0});var wxr=n(OR);Q3o=r(wxr,"BigBirdPegasusForQuestionAnswering"),wxr.forEach(t),V3o=r(Z1e," (BigBirdPegasus model)"),Z1e.forEach(t),W3o=i(I),Nb=s(I,"LI",{});var eFe=n(Nb);Oee=s(eFe,"STRONG",{});var Axr=n(Oee);H3o=r(Axr,"camembert"),Axr.forEach(t),U3o=r(eFe," \u2014 "),GR=s(eFe,"A",{href:!0});var Lxr=n(GR);J3o=r(Lxr,"CamembertForQuestionAnswering"),Lxr.forEach(t),K3o=r(eFe," (CamemBERT model)"),eFe.forEach(t),Y3o=i(I),jb=s(I,"LI",{});var oFe=n(jb);Gee=s(oFe,"STRONG",{});var Bxr=n(Gee);Z3o=r(Bxr,"canine"),Bxr.forEach(t),eMo=r(oFe," \u2014 "),qR=s(oFe,"A",{href:!0});var xxr=n(qR);oMo=r(xxr,"CanineForQuestionAnswering"),xxr.forEach(t),rMo=r(oFe," (Canine model)"),oFe.forEach(t),tMo=i(I),Ob=s(I,"LI",{});var rFe=n(Ob);qee=s(rFe,"STRONG",{});var kxr=n(qee);aMo=r(kxr,"convbert"),kxr.forEach(t),sMo=r(rFe," \u2014 "),zR=s(rFe,"A",{href:!0});var Rxr=n(zR);nMo=r(Rxr,"ConvBertForQuestionAnswering"),Rxr.forEach(t),lMo=r(rFe," (ConvBERT model)"),rFe.forEach(t),iMo=i(I),Gb=s(I,"LI",{});var tFe=n(Gb);zee=s(tFe,"STRONG",{});var Pxr=n(zee);dMo=r(Pxr,"deberta"),Pxr.forEach(t),cMo=r(tFe," \u2014 "),XR=s(tFe,"A",{href:!0});var Sxr=n(XR);mMo=r(Sxr,"DebertaForQuestionAnswering"),Sxr.forEach(t),fMo=r(tFe," (DeBERTa model)"),tFe.forEach(t),hMo=i(I),qb=s(I,"LI",{});var aFe=n(qb);Xee=s(aFe,"STRONG",{});var $xr=n(Xee);gMo=r($xr,"deberta-v2"),$xr.forEach(t),uMo=r(aFe," \u2014 "),QR=s(aFe,"A",{href:!0});var Ixr=n(QR);pMo=r(Ixr,"DebertaV2ForQuestionAnswering"),Ixr.forEach(t),_Mo=r(aFe," (DeBERTa-v2 model)"),aFe.forEach(t),bMo=i(I),zb=s(I,"LI",{});var sFe=n(zb);Qee=s(sFe,"STRONG",{});var Dxr=n(Qee);vMo=r(Dxr,"distilbert"),Dxr.forEach(t),TMo=r(sFe," \u2014 "),VR=s(sFe,"A",{href:!0});var Nxr=n(VR);FMo=r(Nxr,"DistilBertForQuestionAnswering"),Nxr.forEach(t),EMo=r(sFe," (DistilBERT model)"),sFe.forEach(t),CMo=i(I),Xb=s(I,"LI",{});var nFe=n(Xb);Vee=s(nFe,"STRONG",{});var jxr=n(Vee);MMo=r(jxr,"electra"),jxr.forEach(t),yMo=r(nFe," \u2014 "),WR=s(nFe,"A",{href:!0});var Oxr=n(WR);wMo=r(Oxr,"ElectraForQuestionAnswering"),Oxr.forEach(t),AMo=r(nFe," (ELECTRA model)"),nFe.forEach(t),LMo=i(I),Qb=s(I,"LI",{});var lFe=n(Qb);Wee=s(lFe,"STRONG",{});var Gxr=n(Wee);BMo=r(Gxr,"flaubert"),Gxr.forEach(t),xMo=r(lFe," \u2014 "),HR=s(lFe,"A",{href:!0});var qxr=n(HR);kMo=r(qxr,"FlaubertForQuestionAnsweringSimple"),qxr.forEach(t),RMo=r(lFe," (FlauBERT model)"),lFe.forEach(t),PMo=i(I),Vb=s(I,"LI",{});var iFe=n(Vb);Hee=s(iFe,"STRONG",{});var zxr=n(Hee);SMo=r(zxr,"fnet"),zxr.forEach(t),$Mo=r(iFe," \u2014 "),UR=s(iFe,"A",{href:!0});var Xxr=n(UR);IMo=r(Xxr,"FNetForQuestionAnswering"),Xxr.forEach(t),DMo=r(iFe," (FNet model)"),iFe.forEach(t),NMo=i(I),Wb=s(I,"LI",{});var dFe=n(Wb);Uee=s(dFe,"STRONG",{});var Qxr=n(Uee);jMo=r(Qxr,"funnel"),Qxr.forEach(t),OMo=r(dFe," \u2014 "),JR=s(dFe,"A",{href:!0});var Vxr=n(JR);GMo=r(Vxr,"FunnelForQuestionAnswering"),Vxr.forEach(t),qMo=r(dFe," (Funnel Transformer model)"),dFe.forEach(t),zMo=i(I),Hb=s(I,"LI",{});var cFe=n(Hb);Jee=s(cFe,"STRONG",{});var Wxr=n(Jee);XMo=r(Wxr,"gptj"),Wxr.forEach(t),QMo=r(cFe," \u2014 "),KR=s(cFe,"A",{href:!0});var Hxr=n(KR);VMo=r(Hxr,"GPTJForQuestionAnswering"),Hxr.forEach(t),WMo=r(cFe," (GPT-J model)"),cFe.forEach(t),HMo=i(I),Ub=s(I,"LI",{});var mFe=n(Ub);Kee=s(mFe,"STRONG",{});var Uxr=n(Kee);UMo=r(Uxr,"ibert"),Uxr.forEach(t),JMo=r(mFe," \u2014 "),YR=s(mFe,"A",{href:!0});var Jxr=n(YR);KMo=r(Jxr,"IBertForQuestionAnswering"),Jxr.forEach(t),YMo=r(mFe," (I-BERT model)"),mFe.forEach(t),ZMo=i(I),Jb=s(I,"LI",{});var fFe=n(Jb);Yee=s(fFe,"STRONG",{});var Kxr=n(Yee);e5o=r(Kxr,"layoutlmv2"),Kxr.forEach(t),o5o=r(fFe," \u2014 "),ZR=s(fFe,"A",{href:!0});var Yxr=n(ZR);r5o=r(Yxr,"LayoutLMv2ForQuestionAnswering"),Yxr.forEach(t),t5o=r(fFe," (LayoutLMv2 model)"),fFe.forEach(t),a5o=i(I),Kb=s(I,"LI",{});var hFe=n(Kb);Zee=s(hFe,"STRONG",{});var Zxr=n(Zee);s5o=r(Zxr,"led"),Zxr.forEach(t),n5o=r(hFe," \u2014 "),eP=s(hFe,"A",{href:!0});var ekr=n(eP);l5o=r(ekr,"LEDForQuestionAnswering"),ekr.forEach(t),i5o=r(hFe," (LED model)"),hFe.forEach(t),d5o=i(I),Yb=s(I,"LI",{});var gFe=n(Yb);eoe=s(gFe,"STRONG",{});var okr=n(eoe);c5o=r(okr,"longformer"),okr.forEach(t),m5o=r(gFe," \u2014 "),oP=s(gFe,"A",{href:!0});var rkr=n(oP);f5o=r(rkr,"LongformerForQuestionAnswering"),rkr.forEach(t),h5o=r(gFe," (Longformer model)"),gFe.forEach(t),g5o=i(I),Zb=s(I,"LI",{});var uFe=n(Zb);ooe=s(uFe,"STRONG",{});var tkr=n(ooe);u5o=r(tkr,"lxmert"),tkr.forEach(t),p5o=r(uFe," \u2014 "),rP=s(uFe,"A",{href:!0});var akr=n(rP);_5o=r(akr,"LxmertForQuestionAnswering"),akr.forEach(t),b5o=r(uFe," (LXMERT model)"),uFe.forEach(t),v5o=i(I),e2=s(I,"LI",{});var pFe=n(e2);roe=s(pFe,"STRONG",{});var skr=n(roe);T5o=r(skr,"mbart"),skr.forEach(t),F5o=r(pFe," \u2014 "),tP=s(pFe,"A",{href:!0});var nkr=n(tP);E5o=r(nkr,"MBartForQuestionAnswering"),nkr.forEach(t),C5o=r(pFe," (mBART model)"),pFe.forEach(t),M5o=i(I),o2=s(I,"LI",{});var _Fe=n(o2);toe=s(_Fe,"STRONG",{});var lkr=n(toe);y5o=r(lkr,"megatron-bert"),lkr.forEach(t),w5o=r(_Fe," \u2014 "),aP=s(_Fe,"A",{href:!0});var ikr=n(aP);A5o=r(ikr,"MegatronBertForQuestionAnswering"),ikr.forEach(t),L5o=r(_Fe," (MegatronBert model)"),_Fe.forEach(t),B5o=i(I),r2=s(I,"LI",{});var bFe=n(r2);aoe=s(bFe,"STRONG",{});var dkr=n(aoe);x5o=r(dkr,"mobilebert"),dkr.forEach(t),k5o=r(bFe," \u2014 "),sP=s(bFe,"A",{href:!0});var ckr=n(sP);R5o=r(ckr,"MobileBertForQuestionAnswering"),ckr.forEach(t),P5o=r(bFe," (MobileBERT model)"),bFe.forEach(t),S5o=i(I),t2=s(I,"LI",{});var vFe=n(t2);soe=s(vFe,"STRONG",{});var mkr=n(soe);$5o=r(mkr,"mpnet"),mkr.forEach(t),I5o=r(vFe," \u2014 "),nP=s(vFe,"A",{href:!0});var fkr=n(nP);D5o=r(fkr,"MPNetForQuestionAnswering"),fkr.forEach(t),N5o=r(vFe," (MPNet model)"),vFe.forEach(t),j5o=i(I),a2=s(I,"LI",{});var TFe=n(a2);noe=s(TFe,"STRONG",{});var hkr=n(noe);O5o=r(hkr,"qdqbert"),hkr.forEach(t),G5o=r(TFe," \u2014 "),lP=s(TFe,"A",{href:!0});var gkr=n(lP);q5o=r(gkr,"QDQBertForQuestionAnswering"),gkr.forEach(t),z5o=r(TFe," (QDQBert model)"),TFe.forEach(t),X5o=i(I),s2=s(I,"LI",{});var FFe=n(s2);loe=s(FFe,"STRONG",{});var ukr=n(loe);Q5o=r(ukr,"reformer"),ukr.forEach(t),V5o=r(FFe," \u2014 "),iP=s(FFe,"A",{href:!0});var pkr=n(iP);W5o=r(pkr,"ReformerForQuestionAnswering"),pkr.forEach(t),H5o=r(FFe," (Reformer model)"),FFe.forEach(t),U5o=i(I),n2=s(I,"LI",{});var EFe=n(n2);ioe=s(EFe,"STRONG",{});var _kr=n(ioe);J5o=r(_kr,"rembert"),_kr.forEach(t),K5o=r(EFe," \u2014 "),dP=s(EFe,"A",{href:!0});var bkr=n(dP);Y5o=r(bkr,"RemBertForQuestionAnswering"),bkr.forEach(t),Z5o=r(EFe," (RemBERT model)"),EFe.forEach(t),eyo=i(I),l2=s(I,"LI",{});var CFe=n(l2);doe=s(CFe,"STRONG",{});var vkr=n(doe);oyo=r(vkr,"roberta"),vkr.forEach(t),ryo=r(CFe," \u2014 "),cP=s(CFe,"A",{href:!0});var Tkr=n(cP);tyo=r(Tkr,"RobertaForQuestionAnswering"),Tkr.forEach(t),ayo=r(CFe," (RoBERTa model)"),CFe.forEach(t),syo=i(I),i2=s(I,"LI",{});var MFe=n(i2);coe=s(MFe,"STRONG",{});var Fkr=n(coe);nyo=r(Fkr,"roformer"),Fkr.forEach(t),lyo=r(MFe," \u2014 "),mP=s(MFe,"A",{href:!0});var Ekr=n(mP);iyo=r(Ekr,"RoFormerForQuestionAnswering"),Ekr.forEach(t),dyo=r(MFe," (RoFormer model)"),MFe.forEach(t),cyo=i(I),d2=s(I,"LI",{});var yFe=n(d2);moe=s(yFe,"STRONG",{});var Ckr=n(moe);myo=r(Ckr,"splinter"),Ckr.forEach(t),fyo=r(yFe," \u2014 "),fP=s(yFe,"A",{href:!0});var Mkr=n(fP);hyo=r(Mkr,"SplinterForQuestionAnswering"),Mkr.forEach(t),gyo=r(yFe," (Splinter model)"),yFe.forEach(t),uyo=i(I),c2=s(I,"LI",{});var wFe=n(c2);foe=s(wFe,"STRONG",{});var ykr=n(foe);pyo=r(ykr,"squeezebert"),ykr.forEach(t),_yo=r(wFe," \u2014 "),hP=s(wFe,"A",{href:!0});var wkr=n(hP);byo=r(wkr,"SqueezeBertForQuestionAnswering"),wkr.forEach(t),vyo=r(wFe," (SqueezeBERT model)"),wFe.forEach(t),Tyo=i(I),m2=s(I,"LI",{});var AFe=n(m2);hoe=s(AFe,"STRONG",{});var Akr=n(hoe);Fyo=r(Akr,"xlm"),Akr.forEach(t),Eyo=r(AFe," \u2014 "),gP=s(AFe,"A",{href:!0});var Lkr=n(gP);Cyo=r(Lkr,"XLMForQuestionAnsweringSimple"),Lkr.forEach(t),Myo=r(AFe," (XLM model)"),AFe.forEach(t),yyo=i(I),f2=s(I,"LI",{});var LFe=n(f2);goe=s(LFe,"STRONG",{});var Bkr=n(goe);wyo=r(Bkr,"xlm-roberta"),Bkr.forEach(t),Ayo=r(LFe," \u2014 "),uP=s(LFe,"A",{href:!0});var xkr=n(uP);Lyo=r(xkr,"XLMRobertaForQuestionAnswering"),xkr.forEach(t),Byo=r(LFe," (XLM-RoBERTa model)"),LFe.forEach(t),xyo=i(I),h2=s(I,"LI",{});var BFe=n(h2);uoe=s(BFe,"STRONG",{});var kkr=n(uoe);kyo=r(kkr,"xlnet"),kkr.forEach(t),Ryo=r(BFe," \u2014 "),pP=s(BFe,"A",{href:!0});var Rkr=n(pP);Pyo=r(Rkr,"XLNetForQuestionAnsweringSimple"),Rkr.forEach(t),Syo=r(BFe," (XLNet model)"),BFe.forEach(t),I.forEach(t),$yo=i(Lt),g2=s(Lt,"P",{});var xFe=n(g2);Iyo=r(xFe,"The model is set in evaluation mode by default using "),poe=s(xFe,"CODE",{});var Pkr=n(poe);Dyo=r(Pkr,"model.eval()"),Pkr.forEach(t),Nyo=r(xFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),_oe=s(xFe,"CODE",{});var Skr=n(_oe);jyo=r(Skr,"model.train()"),Skr.forEach(t),xFe.forEach(t),Oyo=i(Lt),boe=s(Lt,"P",{});var $kr=n(boe);Gyo=r($kr,"Examples:"),$kr.forEach(t),qyo=i(Lt),f(CM.$$.fragment,Lt),Lt.forEach(t),Rn.forEach(t),D5e=i(d),Gi=s(d,"H2",{class:!0});var Xwe=n(Gi);u2=s(Xwe,"A",{id:!0,class:!0,href:!0});var Ikr=n(u2);voe=s(Ikr,"SPAN",{});var Dkr=n(voe);f(MM.$$.fragment,Dkr),Dkr.forEach(t),Ikr.forEach(t),zyo=i(Xwe),Toe=s(Xwe,"SPAN",{});var Nkr=n(Toe);Xyo=r(Nkr,"AutoModelForTableQuestionAnswering"),Nkr.forEach(t),Xwe.forEach(t),N5e=i(d),Uo=s(d,"DIV",{class:!0});var Sn=n(Uo);f(yM.$$.fragment,Sn),Qyo=i(Sn),qi=s(Sn,"P",{});var Tj=n(qi);Vyo=r(Tj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Foe=s(Tj,"CODE",{});var jkr=n(Foe);Wyo=r(jkr,"from_pretrained()"),jkr.forEach(t),Hyo=r(Tj,` class method or the
`),Eoe=s(Tj,"CODE",{});var Okr=n(Eoe);Uyo=r(Okr,"from_config()"),Okr.forEach(t),Jyo=r(Tj," class method."),Tj.forEach(t),Kyo=i(Sn),wM=s(Sn,"P",{});var Qwe=n(wM);Yyo=r(Qwe,"This class cannot be instantiated directly using "),Coe=s(Qwe,"CODE",{});var Gkr=n(Coe);Zyo=r(Gkr,"__init__()"),Gkr.forEach(t),ewo=r(Qwe," (throws an error)."),Qwe.forEach(t),owo=i(Sn),Gr=s(Sn,"DIV",{class:!0});var $n=n(Gr);f(AM.$$.fragment,$n),rwo=i($n),Moe=s($n,"P",{});var qkr=n(Moe);two=r(qkr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),qkr.forEach(t),awo=i($n),zi=s($n,"P",{});var Fj=n(zi);swo=r(Fj,`Note:
Loading a model from its configuration file does `),yoe=s(Fj,"STRONG",{});var zkr=n(yoe);nwo=r(zkr,"not"),zkr.forEach(t),lwo=r(Fj,` load the model weights. It only affects the
model\u2019s configuration. Use `),woe=s(Fj,"CODE",{});var Xkr=n(woe);iwo=r(Xkr,"from_pretrained()"),Xkr.forEach(t),dwo=r(Fj,` to load the model
weights.`),Fj.forEach(t),cwo=i($n),Aoe=s($n,"P",{});var Qkr=n(Aoe);mwo=r(Qkr,"Examples:"),Qkr.forEach(t),fwo=i($n),f(LM.$$.fragment,$n),$n.forEach(t),hwo=i(Sn),Xe=s(Sn,"DIV",{class:!0});var Bt=n(Xe);f(BM.$$.fragment,Bt),gwo=i(Bt),Loe=s(Bt,"P",{});var Vkr=n(Loe);uwo=r(Vkr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Vkr.forEach(t),pwo=i(Bt),Ia=s(Bt,"P",{});var j4=n(Ia);_wo=r(j4,"The model class to instantiate is selected based on the "),Boe=s(j4,"CODE",{});var Wkr=n(Boe);bwo=r(Wkr,"model_type"),Wkr.forEach(t),vwo=r(j4,` property of the config object (either
passed as an argument or loaded from `),xoe=s(j4,"CODE",{});var Hkr=n(xoe);Two=r(Hkr,"pretrained_model_name_or_path"),Hkr.forEach(t),Fwo=r(j4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),koe=s(j4,"CODE",{});var Ukr=n(koe);Ewo=r(Ukr,"pretrained_model_name_or_path"),Ukr.forEach(t),Cwo=r(j4,":"),j4.forEach(t),Mwo=i(Bt),Roe=s(Bt,"UL",{});var Jkr=n(Roe);p2=s(Jkr,"LI",{});var kFe=n(p2);Poe=s(kFe,"STRONG",{});var Kkr=n(Poe);ywo=r(Kkr,"tapas"),Kkr.forEach(t),wwo=r(kFe," \u2014 "),_P=s(kFe,"A",{href:!0});var Ykr=n(_P);Awo=r(Ykr,"TapasForQuestionAnswering"),Ykr.forEach(t),Lwo=r(kFe," (TAPAS model)"),kFe.forEach(t),Jkr.forEach(t),Bwo=i(Bt),_2=s(Bt,"P",{});var RFe=n(_2);xwo=r(RFe,"The model is set in evaluation mode by default using "),Soe=s(RFe,"CODE",{});var Zkr=n(Soe);kwo=r(Zkr,"model.eval()"),Zkr.forEach(t),Rwo=r(RFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$oe=s(RFe,"CODE",{});var eRr=n($oe);Pwo=r(eRr,"model.train()"),eRr.forEach(t),RFe.forEach(t),Swo=i(Bt),Ioe=s(Bt,"P",{});var oRr=n(Ioe);$wo=r(oRr,"Examples:"),oRr.forEach(t),Iwo=i(Bt),f(xM.$$.fragment,Bt),Bt.forEach(t),Sn.forEach(t),j5e=i(d),Xi=s(d,"H2",{class:!0});var Vwe=n(Xi);b2=s(Vwe,"A",{id:!0,class:!0,href:!0});var rRr=n(b2);Doe=s(rRr,"SPAN",{});var tRr=n(Doe);f(kM.$$.fragment,tRr),tRr.forEach(t),rRr.forEach(t),Dwo=i(Vwe),Noe=s(Vwe,"SPAN",{});var aRr=n(Noe);Nwo=r(aRr,"AutoModelForImageClassification"),aRr.forEach(t),Vwe.forEach(t),O5e=i(d),Jo=s(d,"DIV",{class:!0});var In=n(Jo);f(RM.$$.fragment,In),jwo=i(In),Qi=s(In,"P",{});var Ej=n(Qi);Owo=r(Ej,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),joe=s(Ej,"CODE",{});var sRr=n(joe);Gwo=r(sRr,"from_pretrained()"),sRr.forEach(t),qwo=r(Ej,` class method or the
`),Ooe=s(Ej,"CODE",{});var nRr=n(Ooe);zwo=r(nRr,"from_config()"),nRr.forEach(t),Xwo=r(Ej," class method."),Ej.forEach(t),Qwo=i(In),PM=s(In,"P",{});var Wwe=n(PM);Vwo=r(Wwe,"This class cannot be instantiated directly using "),Goe=s(Wwe,"CODE",{});var lRr=n(Goe);Wwo=r(lRr,"__init__()"),lRr.forEach(t),Hwo=r(Wwe," (throws an error)."),Wwe.forEach(t),Uwo=i(In),qr=s(In,"DIV",{class:!0});var Dn=n(qr);f(SM.$$.fragment,Dn),Jwo=i(Dn),qoe=s(Dn,"P",{});var iRr=n(qoe);Kwo=r(iRr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),iRr.forEach(t),Ywo=i(Dn),Vi=s(Dn,"P",{});var Cj=n(Vi);Zwo=r(Cj,`Note:
Loading a model from its configuration file does `),zoe=s(Cj,"STRONG",{});var dRr=n(zoe);e7o=r(dRr,"not"),dRr.forEach(t),o7o=r(Cj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xoe=s(Cj,"CODE",{});var cRr=n(Xoe);r7o=r(cRr,"from_pretrained()"),cRr.forEach(t),t7o=r(Cj,` to load the model
weights.`),Cj.forEach(t),a7o=i(Dn),Qoe=s(Dn,"P",{});var mRr=n(Qoe);s7o=r(mRr,"Examples:"),mRr.forEach(t),n7o=i(Dn),f($M.$$.fragment,Dn),Dn.forEach(t),l7o=i(In),Qe=s(In,"DIV",{class:!0});var xt=n(Qe);f(IM.$$.fragment,xt),i7o=i(xt),Voe=s(xt,"P",{});var fRr=n(Voe);d7o=r(fRr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),fRr.forEach(t),c7o=i(xt),Da=s(xt,"P",{});var O4=n(Da);m7o=r(O4,"The model class to instantiate is selected based on the "),Woe=s(O4,"CODE",{});var hRr=n(Woe);f7o=r(hRr,"model_type"),hRr.forEach(t),h7o=r(O4,` property of the config object (either
passed as an argument or loaded from `),Hoe=s(O4,"CODE",{});var gRr=n(Hoe);g7o=r(gRr,"pretrained_model_name_or_path"),gRr.forEach(t),u7o=r(O4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Uoe=s(O4,"CODE",{});var uRr=n(Uoe);p7o=r(uRr,"pretrained_model_name_or_path"),uRr.forEach(t),_7o=r(O4,":"),O4.forEach(t),b7o=i(xt),Ko=s(xt,"UL",{});var zt=n(Ko);v2=s(zt,"LI",{});var PFe=n(v2);Joe=s(PFe,"STRONG",{});var pRr=n(Joe);v7o=r(pRr,"beit"),pRr.forEach(t),T7o=r(PFe," \u2014 "),bP=s(PFe,"A",{href:!0});var _Rr=n(bP);F7o=r(_Rr,"BeitForImageClassification"),_Rr.forEach(t),E7o=r(PFe," (BEiT model)"),PFe.forEach(t),C7o=i(zt),dn=s(zt,"LI",{});var b0=n(dn);Koe=s(b0,"STRONG",{});var bRr=n(Koe);M7o=r(bRr,"deit"),bRr.forEach(t),y7o=r(b0," \u2014 "),vP=s(b0,"A",{href:!0});var vRr=n(vP);w7o=r(vRr,"DeiTForImageClassification"),vRr.forEach(t),A7o=r(b0," or "),TP=s(b0,"A",{href:!0});var TRr=n(TP);L7o=r(TRr,"DeiTForImageClassificationWithTeacher"),TRr.forEach(t),B7o=r(b0," (DeiT model)"),b0.forEach(t),x7o=i(zt),T2=s(zt,"LI",{});var SFe=n(T2);Yoe=s(SFe,"STRONG",{});var FRr=n(Yoe);k7o=r(FRr,"imagegpt"),FRr.forEach(t),R7o=r(SFe," \u2014 "),FP=s(SFe,"A",{href:!0});var ERr=n(FP);P7o=r(ERr,"ImageGPTForImageClassification"),ERr.forEach(t),S7o=r(SFe," (ImageGPT model)"),SFe.forEach(t),$7o=i(zt),Gt=s(zt,"LI",{});var Ac=n(Gt);Zoe=s(Ac,"STRONG",{});var CRr=n(Zoe);I7o=r(CRr,"perceiver"),CRr.forEach(t),D7o=r(Ac," \u2014 "),EP=s(Ac,"A",{href:!0});var MRr=n(EP);N7o=r(MRr,"PerceiverForImageClassificationLearned"),MRr.forEach(t),j7o=r(Ac," or "),CP=s(Ac,"A",{href:!0});var yRr=n(CP);O7o=r(yRr,"PerceiverForImageClassificationFourier"),yRr.forEach(t),G7o=r(Ac," or "),MP=s(Ac,"A",{href:!0});var wRr=n(MP);q7o=r(wRr,"PerceiverForImageClassificationConvProcessing"),wRr.forEach(t),z7o=r(Ac," (Perceiver model)"),Ac.forEach(t),X7o=i(zt),F2=s(zt,"LI",{});var $Fe=n(F2);ere=s($Fe,"STRONG",{});var ARr=n(ere);Q7o=r(ARr,"segformer"),ARr.forEach(t),V7o=r($Fe," \u2014 "),yP=s($Fe,"A",{href:!0});var LRr=n(yP);W7o=r(LRr,"SegformerForImageClassification"),LRr.forEach(t),H7o=r($Fe," (SegFormer model)"),$Fe.forEach(t),U7o=i(zt),E2=s(zt,"LI",{});var IFe=n(E2);ore=s(IFe,"STRONG",{});var BRr=n(ore);J7o=r(BRr,"vit"),BRr.forEach(t),K7o=r(IFe," \u2014 "),wP=s(IFe,"A",{href:!0});var xRr=n(wP);Y7o=r(xRr,"ViTForImageClassification"),xRr.forEach(t),Z7o=r(IFe," (ViT model)"),IFe.forEach(t),zt.forEach(t),e0o=i(xt),C2=s(xt,"P",{});var DFe=n(C2);o0o=r(DFe,"The model is set in evaluation mode by default using "),rre=s(DFe,"CODE",{});var kRr=n(rre);r0o=r(kRr,"model.eval()"),kRr.forEach(t),t0o=r(DFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),tre=s(DFe,"CODE",{});var RRr=n(tre);a0o=r(RRr,"model.train()"),RRr.forEach(t),DFe.forEach(t),s0o=i(xt),are=s(xt,"P",{});var PRr=n(are);n0o=r(PRr,"Examples:"),PRr.forEach(t),l0o=i(xt),f(DM.$$.fragment,xt),xt.forEach(t),In.forEach(t),G5e=i(d),Wi=s(d,"H2",{class:!0});var Hwe=n(Wi);M2=s(Hwe,"A",{id:!0,class:!0,href:!0});var SRr=n(M2);sre=s(SRr,"SPAN",{});var $Rr=n(sre);f(NM.$$.fragment,$Rr),$Rr.forEach(t),SRr.forEach(t),i0o=i(Hwe),nre=s(Hwe,"SPAN",{});var IRr=n(nre);d0o=r(IRr,"AutoModelForVision2Seq"),IRr.forEach(t),Hwe.forEach(t),q5e=i(d),Yo=s(d,"DIV",{class:!0});var Nn=n(Yo);f(jM.$$.fragment,Nn),c0o=i(Nn),Hi=s(Nn,"P",{});var Mj=n(Hi);m0o=r(Mj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),lre=s(Mj,"CODE",{});var DRr=n(lre);f0o=r(DRr,"from_pretrained()"),DRr.forEach(t),h0o=r(Mj,` class method or the
`),ire=s(Mj,"CODE",{});var NRr=n(ire);g0o=r(NRr,"from_config()"),NRr.forEach(t),u0o=r(Mj," class method."),Mj.forEach(t),p0o=i(Nn),OM=s(Nn,"P",{});var Uwe=n(OM);_0o=r(Uwe,"This class cannot be instantiated directly using "),dre=s(Uwe,"CODE",{});var jRr=n(dre);b0o=r(jRr,"__init__()"),jRr.forEach(t),v0o=r(Uwe," (throws an error)."),Uwe.forEach(t),T0o=i(Nn),zr=s(Nn,"DIV",{class:!0});var jn=n(zr);f(GM.$$.fragment,jn),F0o=i(jn),cre=s(jn,"P",{});var ORr=n(cre);E0o=r(ORr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),ORr.forEach(t),C0o=i(jn),Ui=s(jn,"P",{});var yj=n(Ui);M0o=r(yj,`Note:
Loading a model from its configuration file does `),mre=s(yj,"STRONG",{});var GRr=n(mre);y0o=r(GRr,"not"),GRr.forEach(t),w0o=r(yj,` load the model weights. It only affects the
model\u2019s configuration. Use `),fre=s(yj,"CODE",{});var qRr=n(fre);A0o=r(qRr,"from_pretrained()"),qRr.forEach(t),L0o=r(yj,` to load the model
weights.`),yj.forEach(t),B0o=i(jn),hre=s(jn,"P",{});var zRr=n(hre);x0o=r(zRr,"Examples:"),zRr.forEach(t),k0o=i(jn),f(qM.$$.fragment,jn),jn.forEach(t),R0o=i(Nn),Ve=s(Nn,"DIV",{class:!0});var kt=n(Ve);f(zM.$$.fragment,kt),P0o=i(kt),gre=s(kt,"P",{});var XRr=n(gre);S0o=r(XRr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),XRr.forEach(t),$0o=i(kt),Na=s(kt,"P",{});var G4=n(Na);I0o=r(G4,"The model class to instantiate is selected based on the "),ure=s(G4,"CODE",{});var QRr=n(ure);D0o=r(QRr,"model_type"),QRr.forEach(t),N0o=r(G4,` property of the config object (either
passed as an argument or loaded from `),pre=s(G4,"CODE",{});var VRr=n(pre);j0o=r(VRr,"pretrained_model_name_or_path"),VRr.forEach(t),O0o=r(G4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),_re=s(G4,"CODE",{});var WRr=n(_re);G0o=r(WRr,"pretrained_model_name_or_path"),WRr.forEach(t),q0o=r(G4,":"),G4.forEach(t),z0o=i(kt),bre=s(kt,"UL",{});var HRr=n(bre);y2=s(HRr,"LI",{});var NFe=n(y2);vre=s(NFe,"STRONG",{});var URr=n(vre);X0o=r(URr,"vision-encoder-decoder"),URr.forEach(t),Q0o=r(NFe," \u2014 "),AP=s(NFe,"A",{href:!0});var JRr=n(AP);V0o=r(JRr,"VisionEncoderDecoderModel"),JRr.forEach(t),W0o=r(NFe," (Vision Encoder decoder model)"),NFe.forEach(t),HRr.forEach(t),H0o=i(kt),w2=s(kt,"P",{});var jFe=n(w2);U0o=r(jFe,"The model is set in evaluation mode by default using "),Tre=s(jFe,"CODE",{});var KRr=n(Tre);J0o=r(KRr,"model.eval()"),KRr.forEach(t),K0o=r(jFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Fre=s(jFe,"CODE",{});var YRr=n(Fre);Y0o=r(YRr,"model.train()"),YRr.forEach(t),jFe.forEach(t),Z0o=i(kt),Ere=s(kt,"P",{});var ZRr=n(Ere);eAo=r(ZRr,"Examples:"),ZRr.forEach(t),oAo=i(kt),f(XM.$$.fragment,kt),kt.forEach(t),Nn.forEach(t),z5e=i(d),Ji=s(d,"H2",{class:!0});var Jwe=n(Ji);A2=s(Jwe,"A",{id:!0,class:!0,href:!0});var ePr=n(A2);Cre=s(ePr,"SPAN",{});var oPr=n(Cre);f(QM.$$.fragment,oPr),oPr.forEach(t),ePr.forEach(t),rAo=i(Jwe),Mre=s(Jwe,"SPAN",{});var rPr=n(Mre);tAo=r(rPr,"AutoModelForAudioClassification"),rPr.forEach(t),Jwe.forEach(t),X5e=i(d),Zo=s(d,"DIV",{class:!0});var On=n(Zo);f(VM.$$.fragment,On),aAo=i(On),Ki=s(On,"P",{});var wj=n(Ki);sAo=r(wj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),yre=s(wj,"CODE",{});var tPr=n(yre);nAo=r(tPr,"from_pretrained()"),tPr.forEach(t),lAo=r(wj,` class method or the
`),wre=s(wj,"CODE",{});var aPr=n(wre);iAo=r(aPr,"from_config()"),aPr.forEach(t),dAo=r(wj," class method."),wj.forEach(t),cAo=i(On),WM=s(On,"P",{});var Kwe=n(WM);mAo=r(Kwe,"This class cannot be instantiated directly using "),Are=s(Kwe,"CODE",{});var sPr=n(Are);fAo=r(sPr,"__init__()"),sPr.forEach(t),hAo=r(Kwe," (throws an error)."),Kwe.forEach(t),gAo=i(On),Xr=s(On,"DIV",{class:!0});var Gn=n(Xr);f(HM.$$.fragment,Gn),uAo=i(Gn),Lre=s(Gn,"P",{});var nPr=n(Lre);pAo=r(nPr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),nPr.forEach(t),_Ao=i(Gn),Yi=s(Gn,"P",{});var Aj=n(Yi);bAo=r(Aj,`Note:
Loading a model from its configuration file does `),Bre=s(Aj,"STRONG",{});var lPr=n(Bre);vAo=r(lPr,"not"),lPr.forEach(t),TAo=r(Aj,` load the model weights. It only affects the
model\u2019s configuration. Use `),xre=s(Aj,"CODE",{});var iPr=n(xre);FAo=r(iPr,"from_pretrained()"),iPr.forEach(t),EAo=r(Aj,` to load the model
weights.`),Aj.forEach(t),CAo=i(Gn),kre=s(Gn,"P",{});var dPr=n(kre);MAo=r(dPr,"Examples:"),dPr.forEach(t),yAo=i(Gn),f(UM.$$.fragment,Gn),Gn.forEach(t),wAo=i(On),We=s(On,"DIV",{class:!0});var Rt=n(We);f(JM.$$.fragment,Rt),AAo=i(Rt),Rre=s(Rt,"P",{});var cPr=n(Rre);LAo=r(cPr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),cPr.forEach(t),BAo=i(Rt),ja=s(Rt,"P",{});var q4=n(ja);xAo=r(q4,"The model class to instantiate is selected based on the "),Pre=s(q4,"CODE",{});var mPr=n(Pre);kAo=r(mPr,"model_type"),mPr.forEach(t),RAo=r(q4,` property of the config object (either
passed as an argument or loaded from `),Sre=s(q4,"CODE",{});var fPr=n(Sre);PAo=r(fPr,"pretrained_model_name_or_path"),fPr.forEach(t),SAo=r(q4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),$re=s(q4,"CODE",{});var hPr=n($re);$Ao=r(hPr,"pretrained_model_name_or_path"),hPr.forEach(t),IAo=r(q4,":"),q4.forEach(t),DAo=i(Rt),er=s(Rt,"UL",{});var Xt=n(er);L2=s(Xt,"LI",{});var OFe=n(L2);Ire=s(OFe,"STRONG",{});var gPr=n(Ire);NAo=r(gPr,"hubert"),gPr.forEach(t),jAo=r(OFe," \u2014 "),LP=s(OFe,"A",{href:!0});var uPr=n(LP);OAo=r(uPr,"HubertForSequenceClassification"),uPr.forEach(t),GAo=r(OFe," (Hubert model)"),OFe.forEach(t),qAo=i(Xt),B2=s(Xt,"LI",{});var GFe=n(B2);Dre=s(GFe,"STRONG",{});var pPr=n(Dre);zAo=r(pPr,"sew"),pPr.forEach(t),XAo=r(GFe," \u2014 "),BP=s(GFe,"A",{href:!0});var _Pr=n(BP);QAo=r(_Pr,"SEWForSequenceClassification"),_Pr.forEach(t),VAo=r(GFe," (SEW model)"),GFe.forEach(t),WAo=i(Xt),x2=s(Xt,"LI",{});var qFe=n(x2);Nre=s(qFe,"STRONG",{});var bPr=n(Nre);HAo=r(bPr,"sew-d"),bPr.forEach(t),UAo=r(qFe," \u2014 "),xP=s(qFe,"A",{href:!0});var vPr=n(xP);JAo=r(vPr,"SEWDForSequenceClassification"),vPr.forEach(t),KAo=r(qFe," (SEW-D model)"),qFe.forEach(t),YAo=i(Xt),k2=s(Xt,"LI",{});var zFe=n(k2);jre=s(zFe,"STRONG",{});var TPr=n(jre);ZAo=r(TPr,"unispeech"),TPr.forEach(t),e6o=r(zFe," \u2014 "),kP=s(zFe,"A",{href:!0});var FPr=n(kP);o6o=r(FPr,"UniSpeechForSequenceClassification"),FPr.forEach(t),r6o=r(zFe," (UniSpeech model)"),zFe.forEach(t),t6o=i(Xt),R2=s(Xt,"LI",{});var XFe=n(R2);Ore=s(XFe,"STRONG",{});var EPr=n(Ore);a6o=r(EPr,"unispeech-sat"),EPr.forEach(t),s6o=r(XFe," \u2014 "),RP=s(XFe,"A",{href:!0});var CPr=n(RP);n6o=r(CPr,"UniSpeechSatForSequenceClassification"),CPr.forEach(t),l6o=r(XFe," (UniSpeechSat model)"),XFe.forEach(t),i6o=i(Xt),P2=s(Xt,"LI",{});var QFe=n(P2);Gre=s(QFe,"STRONG",{});var MPr=n(Gre);d6o=r(MPr,"wav2vec2"),MPr.forEach(t),c6o=r(QFe," \u2014 "),PP=s(QFe,"A",{href:!0});var yPr=n(PP);m6o=r(yPr,"Wav2Vec2ForSequenceClassification"),yPr.forEach(t),f6o=r(QFe," (Wav2Vec2 model)"),QFe.forEach(t),Xt.forEach(t),h6o=i(Rt),S2=s(Rt,"P",{});var VFe=n(S2);g6o=r(VFe,"The model is set in evaluation mode by default using "),qre=s(VFe,"CODE",{});var wPr=n(qre);u6o=r(wPr,"model.eval()"),wPr.forEach(t),p6o=r(VFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),zre=s(VFe,"CODE",{});var APr=n(zre);_6o=r(APr,"model.train()"),APr.forEach(t),VFe.forEach(t),b6o=i(Rt),Xre=s(Rt,"P",{});var LPr=n(Xre);v6o=r(LPr,"Examples:"),LPr.forEach(t),T6o=i(Rt),f(KM.$$.fragment,Rt),Rt.forEach(t),On.forEach(t),Q5e=i(d),Zi=s(d,"H2",{class:!0});var Ywe=n(Zi);$2=s(Ywe,"A",{id:!0,class:!0,href:!0});var BPr=n($2);Qre=s(BPr,"SPAN",{});var xPr=n(Qre);f(YM.$$.fragment,xPr),xPr.forEach(t),BPr.forEach(t),F6o=i(Ywe),Vre=s(Ywe,"SPAN",{});var kPr=n(Vre);E6o=r(kPr,"AutoModelForCTC"),kPr.forEach(t),Ywe.forEach(t),V5e=i(d),or=s(d,"DIV",{class:!0});var qn=n(or);f(ZM.$$.fragment,qn),C6o=i(qn),ed=s(qn,"P",{});var Lj=n(ed);M6o=r(Lj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),Wre=s(Lj,"CODE",{});var RPr=n(Wre);y6o=r(RPr,"from_pretrained()"),RPr.forEach(t),w6o=r(Lj,` class method or the
`),Hre=s(Lj,"CODE",{});var PPr=n(Hre);A6o=r(PPr,"from_config()"),PPr.forEach(t),L6o=r(Lj," class method."),Lj.forEach(t),B6o=i(qn),e5=s(qn,"P",{});var Zwe=n(e5);x6o=r(Zwe,"This class cannot be instantiated directly using "),Ure=s(Zwe,"CODE",{});var SPr=n(Ure);k6o=r(SPr,"__init__()"),SPr.forEach(t),R6o=r(Zwe," (throws an error)."),Zwe.forEach(t),P6o=i(qn),Qr=s(qn,"DIV",{class:!0});var zn=n(Qr);f(o5.$$.fragment,zn),S6o=i(zn),Jre=s(zn,"P",{});var $Pr=n(Jre);$6o=r($Pr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),$Pr.forEach(t),I6o=i(zn),od=s(zn,"P",{});var Bj=n(od);D6o=r(Bj,`Note:
Loading a model from its configuration file does `),Kre=s(Bj,"STRONG",{});var IPr=n(Kre);N6o=r(IPr,"not"),IPr.forEach(t),j6o=r(Bj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yre=s(Bj,"CODE",{});var DPr=n(Yre);O6o=r(DPr,"from_pretrained()"),DPr.forEach(t),G6o=r(Bj,` to load the model
weights.`),Bj.forEach(t),q6o=i(zn),Zre=s(zn,"P",{});var NPr=n(Zre);z6o=r(NPr,"Examples:"),NPr.forEach(t),X6o=i(zn),f(r5.$$.fragment,zn),zn.forEach(t),Q6o=i(qn),He=s(qn,"DIV",{class:!0});var Pt=n(He);f(t5.$$.fragment,Pt),V6o=i(Pt),ete=s(Pt,"P",{});var jPr=n(ete);W6o=r(jPr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),jPr.forEach(t),H6o=i(Pt),Oa=s(Pt,"P",{});var z4=n(Oa);U6o=r(z4,"The model class to instantiate is selected based on the "),ote=s(z4,"CODE",{});var OPr=n(ote);J6o=r(OPr,"model_type"),OPr.forEach(t),K6o=r(z4,` property of the config object (either
passed as an argument or loaded from `),rte=s(z4,"CODE",{});var GPr=n(rte);Y6o=r(GPr,"pretrained_model_name_or_path"),GPr.forEach(t),Z6o=r(z4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),tte=s(z4,"CODE",{});var qPr=n(tte);eLo=r(qPr,"pretrained_model_name_or_path"),qPr.forEach(t),oLo=r(z4,":"),z4.forEach(t),rLo=i(Pt),rr=s(Pt,"UL",{});var Qt=n(rr);I2=s(Qt,"LI",{});var WFe=n(I2);ate=s(WFe,"STRONG",{});var zPr=n(ate);tLo=r(zPr,"hubert"),zPr.forEach(t),aLo=r(WFe," \u2014 "),SP=s(WFe,"A",{href:!0});var XPr=n(SP);sLo=r(XPr,"HubertForCTC"),XPr.forEach(t),nLo=r(WFe," (Hubert model)"),WFe.forEach(t),lLo=i(Qt),D2=s(Qt,"LI",{});var HFe=n(D2);ste=s(HFe,"STRONG",{});var QPr=n(ste);iLo=r(QPr,"sew"),QPr.forEach(t),dLo=r(HFe," \u2014 "),$P=s(HFe,"A",{href:!0});var VPr=n($P);cLo=r(VPr,"SEWForCTC"),VPr.forEach(t),mLo=r(HFe," (SEW model)"),HFe.forEach(t),fLo=i(Qt),N2=s(Qt,"LI",{});var UFe=n(N2);nte=s(UFe,"STRONG",{});var WPr=n(nte);hLo=r(WPr,"sew-d"),WPr.forEach(t),gLo=r(UFe," \u2014 "),IP=s(UFe,"A",{href:!0});var HPr=n(IP);uLo=r(HPr,"SEWDForCTC"),HPr.forEach(t),pLo=r(UFe," (SEW-D model)"),UFe.forEach(t),_Lo=i(Qt),j2=s(Qt,"LI",{});var JFe=n(j2);lte=s(JFe,"STRONG",{});var UPr=n(lte);bLo=r(UPr,"unispeech"),UPr.forEach(t),vLo=r(JFe," \u2014 "),DP=s(JFe,"A",{href:!0});var JPr=n(DP);TLo=r(JPr,"UniSpeechForCTC"),JPr.forEach(t),FLo=r(JFe," (UniSpeech model)"),JFe.forEach(t),ELo=i(Qt),O2=s(Qt,"LI",{});var KFe=n(O2);ite=s(KFe,"STRONG",{});var KPr=n(ite);CLo=r(KPr,"unispeech-sat"),KPr.forEach(t),MLo=r(KFe," \u2014 "),NP=s(KFe,"A",{href:!0});var YPr=n(NP);yLo=r(YPr,"UniSpeechSatForCTC"),YPr.forEach(t),wLo=r(KFe," (UniSpeechSat model)"),KFe.forEach(t),ALo=i(Qt),G2=s(Qt,"LI",{});var YFe=n(G2);dte=s(YFe,"STRONG",{});var ZPr=n(dte);LLo=r(ZPr,"wav2vec2"),ZPr.forEach(t),BLo=r(YFe," \u2014 "),jP=s(YFe,"A",{href:!0});var eSr=n(jP);xLo=r(eSr,"Wav2Vec2ForCTC"),eSr.forEach(t),kLo=r(YFe," (Wav2Vec2 model)"),YFe.forEach(t),Qt.forEach(t),RLo=i(Pt),q2=s(Pt,"P",{});var ZFe=n(q2);PLo=r(ZFe,"The model is set in evaluation mode by default using "),cte=s(ZFe,"CODE",{});var oSr=n(cte);SLo=r(oSr,"model.eval()"),oSr.forEach(t),$Lo=r(ZFe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mte=s(ZFe,"CODE",{});var rSr=n(mte);ILo=r(rSr,"model.train()"),rSr.forEach(t),ZFe.forEach(t),DLo=i(Pt),fte=s(Pt,"P",{});var tSr=n(fte);NLo=r(tSr,"Examples:"),tSr.forEach(t),jLo=i(Pt),f(a5.$$.fragment,Pt),Pt.forEach(t),qn.forEach(t),W5e=i(d),rd=s(d,"H2",{class:!0});var e7e=n(rd);z2=s(e7e,"A",{id:!0,class:!0,href:!0});var aSr=n(z2);hte=s(aSr,"SPAN",{});var sSr=n(hte);f(s5.$$.fragment,sSr),sSr.forEach(t),aSr.forEach(t),OLo=i(e7e),gte=s(e7e,"SPAN",{});var nSr=n(gte);GLo=r(nSr,"AutoModelForSpeechSeq2Seq"),nSr.forEach(t),e7e.forEach(t),H5e=i(d),tr=s(d,"DIV",{class:!0});var Xn=n(tr);f(n5.$$.fragment,Xn),qLo=i(Xn),td=s(Xn,"P",{});var xj=n(td);zLo=r(xj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) when created
with the `),ute=s(xj,"CODE",{});var lSr=n(ute);XLo=r(lSr,"from_pretrained()"),lSr.forEach(t),QLo=r(xj,` class method or the
`),pte=s(xj,"CODE",{});var iSr=n(pte);VLo=r(iSr,"from_config()"),iSr.forEach(t),WLo=r(xj," class method."),xj.forEach(t),HLo=i(Xn),l5=s(Xn,"P",{});var o7e=n(l5);ULo=r(o7e,"This class cannot be instantiated directly using "),_te=s(o7e,"CODE",{});var dSr=n(_te);JLo=r(dSr,"__init__()"),dSr.forEach(t),KLo=r(o7e," (throws an error)."),o7e.forEach(t),YLo=i(Xn),Vr=s(Xn,"DIV",{class:!0});var Qn=n(Vr);f(i5.$$.fragment,Qn),ZLo=i(Qn),bte=s(Qn,"P",{});var cSr=n(bte);e8o=r(cSr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) from a configuration."),cSr.forEach(t),o8o=i(Qn),ad=s(Qn,"P",{});var kj=n(ad);r8o=r(kj,`Note:
Loading a model from its configuration file does `),vte=s(kj,"STRONG",{});var mSr=n(vte);t8o=r(mSr,"not"),mSr.forEach(t),a8o=r(kj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Tte=s(kj,"CODE",{});var fSr=n(Tte);s8o=r(fSr,"from_pretrained()"),fSr.forEach(t),n8o=r(kj,` to load the model
weights.`),kj.forEach(t),l8o=i(Qn),Fte=s(Qn,"P",{});var hSr=n(Fte);i8o=r(hSr,"Examples:"),hSr.forEach(t),d8o=i(Qn),f(d5.$$.fragment,Qn),Qn.forEach(t),c8o=i(Xn),Ue=s(Xn,"DIV",{class:!0});var St=n(Ue);f(c5.$$.fragment,St),m8o=i(St),Ete=s(St,"P",{});var gSr=n(Ete);f8o=r(gSr,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeing head) from a pretrained model."),gSr.forEach(t),h8o=i(St),Ga=s(St,"P",{});var X4=n(Ga);g8o=r(X4,"The model class to instantiate is selected based on the "),Cte=s(X4,"CODE",{});var uSr=n(Cte);u8o=r(uSr,"model_type"),uSr.forEach(t),p8o=r(X4,` property of the config object (either
passed as an argument or loaded from `),Mte=s(X4,"CODE",{});var pSr=n(Mte);_8o=r(pSr,"pretrained_model_name_or_path"),pSr.forEach(t),b8o=r(X4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),yte=s(X4,"CODE",{});var _Sr=n(yte);v8o=r(_Sr,"pretrained_model_name_or_path"),_Sr.forEach(t),T8o=r(X4,":"),X4.forEach(t),F8o=i(St),m5=s(St,"UL",{});var r7e=n(m5);X2=s(r7e,"LI",{});var eEe=n(X2);wte=s(eEe,"STRONG",{});var bSr=n(wte);E8o=r(bSr,"speech-encoder-decoder"),bSr.forEach(t),C8o=r(eEe," \u2014 "),OP=s(eEe,"A",{href:!0});var vSr=n(OP);M8o=r(vSr,"SpeechEncoderDecoderModel"),vSr.forEach(t),y8o=r(eEe," (Speech Encoder decoder model)"),eEe.forEach(t),w8o=i(r7e),Q2=s(r7e,"LI",{});var oEe=n(Q2);Ate=s(oEe,"STRONG",{});var TSr=n(Ate);A8o=r(TSr,"speech_to_text"),TSr.forEach(t),L8o=r(oEe," \u2014 "),GP=s(oEe,"A",{href:!0});var FSr=n(GP);B8o=r(FSr,"Speech2TextForConditionalGeneration"),FSr.forEach(t),x8o=r(oEe," (Speech2Text model)"),oEe.forEach(t),r7e.forEach(t),k8o=i(St),V2=s(St,"P",{});var rEe=n(V2);R8o=r(rEe,"The model is set in evaluation mode by default using "),Lte=s(rEe,"CODE",{});var ESr=n(Lte);P8o=r(ESr,"model.eval()"),ESr.forEach(t),S8o=r(rEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Bte=s(rEe,"CODE",{});var CSr=n(Bte);$8o=r(CSr,"model.train()"),CSr.forEach(t),rEe.forEach(t),I8o=i(St),xte=s(St,"P",{});var MSr=n(xte);D8o=r(MSr,"Examples:"),MSr.forEach(t),N8o=i(St),f(f5.$$.fragment,St),St.forEach(t),Xn.forEach(t),U5e=i(d),sd=s(d,"H2",{class:!0});var t7e=n(sd);W2=s(t7e,"A",{id:!0,class:!0,href:!0});var ySr=n(W2);kte=s(ySr,"SPAN",{});var wSr=n(kte);f(h5.$$.fragment,wSr),wSr.forEach(t),ySr.forEach(t),j8o=i(t7e),Rte=s(t7e,"SPAN",{});var ASr=n(Rte);O8o=r(ASr,"AutoModelForObjectDetection"),ASr.forEach(t),t7e.forEach(t),J5e=i(d),ar=s(d,"DIV",{class:!0});var Vn=n(ar);f(g5.$$.fragment,Vn),G8o=i(Vn),nd=s(Vn,"P",{});var Rj=n(nd);q8o=r(Rj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Pte=s(Rj,"CODE",{});var LSr=n(Pte);z8o=r(LSr,"from_pretrained()"),LSr.forEach(t),X8o=r(Rj,` class method or the
`),Ste=s(Rj,"CODE",{});var BSr=n(Ste);Q8o=r(BSr,"from_config()"),BSr.forEach(t),V8o=r(Rj," class method."),Rj.forEach(t),W8o=i(Vn),u5=s(Vn,"P",{});var a7e=n(u5);H8o=r(a7e,"This class cannot be instantiated directly using "),$te=s(a7e,"CODE",{});var xSr=n($te);U8o=r(xSr,"__init__()"),xSr.forEach(t),J8o=r(a7e," (throws an error)."),a7e.forEach(t),K8o=i(Vn),Wr=s(Vn,"DIV",{class:!0});var Wn=n(Wr);f(p5.$$.fragment,Wn),Y8o=i(Wn),Ite=s(Wn,"P",{});var kSr=n(Ite);Z8o=r(kSr,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),kSr.forEach(t),eBo=i(Wn),ld=s(Wn,"P",{});var Pj=n(ld);oBo=r(Pj,`Note:
Loading a model from its configuration file does `),Dte=s(Pj,"STRONG",{});var RSr=n(Dte);rBo=r(RSr,"not"),RSr.forEach(t),tBo=r(Pj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Nte=s(Pj,"CODE",{});var PSr=n(Nte);aBo=r(PSr,"from_pretrained()"),PSr.forEach(t),sBo=r(Pj,` to load the model
weights.`),Pj.forEach(t),nBo=i(Wn),jte=s(Wn,"P",{});var SSr=n(jte);lBo=r(SSr,"Examples:"),SSr.forEach(t),iBo=i(Wn),f(_5.$$.fragment,Wn),Wn.forEach(t),dBo=i(Vn),Je=s(Vn,"DIV",{class:!0});var $t=n(Je);f(b5.$$.fragment,$t),cBo=i($t),Ote=s($t,"P",{});var $Sr=n(Ote);mBo=r($Sr,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),$Sr.forEach(t),fBo=i($t),qa=s($t,"P",{});var Q4=n(qa);hBo=r(Q4,"The model class to instantiate is selected based on the "),Gte=s(Q4,"CODE",{});var ISr=n(Gte);gBo=r(ISr,"model_type"),ISr.forEach(t),uBo=r(Q4,` property of the config object (either
passed as an argument or loaded from `),qte=s(Q4,"CODE",{});var DSr=n(qte);pBo=r(DSr,"pretrained_model_name_or_path"),DSr.forEach(t),_Bo=r(Q4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),zte=s(Q4,"CODE",{});var NSr=n(zte);bBo=r(NSr,"pretrained_model_name_or_path"),NSr.forEach(t),vBo=r(Q4,":"),Q4.forEach(t),TBo=i($t),Xte=s($t,"UL",{});var jSr=n(Xte);H2=s(jSr,"LI",{});var tEe=n(H2);Qte=s(tEe,"STRONG",{});var OSr=n(Qte);FBo=r(OSr,"detr"),OSr.forEach(t),EBo=r(tEe," \u2014 "),qP=s(tEe,"A",{href:!0});var GSr=n(qP);CBo=r(GSr,"DetrForObjectDetection"),GSr.forEach(t),MBo=r(tEe," (DETR model)"),tEe.forEach(t),jSr.forEach(t),yBo=i($t),U2=s($t,"P",{});var aEe=n(U2);wBo=r(aEe,"The model is set in evaluation mode by default using "),Vte=s(aEe,"CODE",{});var qSr=n(Vte);ABo=r(qSr,"model.eval()"),qSr.forEach(t),LBo=r(aEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Wte=s(aEe,"CODE",{});var zSr=n(Wte);BBo=r(zSr,"model.train()"),zSr.forEach(t),aEe.forEach(t),xBo=i($t),Hte=s($t,"P",{});var XSr=n(Hte);kBo=r(XSr,"Examples:"),XSr.forEach(t),RBo=i($t),f(v5.$$.fragment,$t),$t.forEach(t),Vn.forEach(t),K5e=i(d),id=s(d,"H2",{class:!0});var s7e=n(id);J2=s(s7e,"A",{id:!0,class:!0,href:!0});var QSr=n(J2);Ute=s(QSr,"SPAN",{});var VSr=n(Ute);f(T5.$$.fragment,VSr),VSr.forEach(t),QSr.forEach(t),PBo=i(s7e),Jte=s(s7e,"SPAN",{});var WSr=n(Jte);SBo=r(WSr,"AutoModelForImageSegmentation"),WSr.forEach(t),s7e.forEach(t),Y5e=i(d),sr=s(d,"DIV",{class:!0});var Hn=n(sr);f(F5.$$.fragment,Hn),$Bo=i(Hn),dd=s(Hn,"P",{});var Sj=n(dd);IBo=r(Sj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),Kte=s(Sj,"CODE",{});var HSr=n(Kte);DBo=r(HSr,"from_pretrained()"),HSr.forEach(t),NBo=r(Sj,` class method or the
`),Yte=s(Sj,"CODE",{});var USr=n(Yte);jBo=r(USr,"from_config()"),USr.forEach(t),OBo=r(Sj," class method."),Sj.forEach(t),GBo=i(Hn),E5=s(Hn,"P",{});var n7e=n(E5);qBo=r(n7e,"This class cannot be instantiated directly using "),Zte=s(n7e,"CODE",{});var JSr=n(Zte);zBo=r(JSr,"__init__()"),JSr.forEach(t),XBo=r(n7e," (throws an error)."),n7e.forEach(t),QBo=i(Hn),Hr=s(Hn,"DIV",{class:!0});var Un=n(Hr);f(C5.$$.fragment,Un),VBo=i(Un),eae=s(Un,"P",{});var KSr=n(eae);WBo=r(KSr,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),KSr.forEach(t),HBo=i(Un),cd=s(Un,"P",{});var $j=n(cd);UBo=r($j,`Note:
Loading a model from its configuration file does `),oae=s($j,"STRONG",{});var YSr=n(oae);JBo=r(YSr,"not"),YSr.forEach(t),KBo=r($j,` load the model weights. It only affects the
model\u2019s configuration. Use `),rae=s($j,"CODE",{});var ZSr=n(rae);YBo=r(ZSr,"from_pretrained()"),ZSr.forEach(t),ZBo=r($j,` to load the model
weights.`),$j.forEach(t),e9o=i(Un),tae=s(Un,"P",{});var e$r=n(tae);o9o=r(e$r,"Examples:"),e$r.forEach(t),r9o=i(Un),f(M5.$$.fragment,Un),Un.forEach(t),t9o=i(Hn),Ke=s(Hn,"DIV",{class:!0});var It=n(Ke);f(y5.$$.fragment,It),a9o=i(It),aae=s(It,"P",{});var o$r=n(aae);s9o=r(o$r,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),o$r.forEach(t),n9o=i(It),za=s(It,"P",{});var V4=n(za);l9o=r(V4,"The model class to instantiate is selected based on the "),sae=s(V4,"CODE",{});var r$r=n(sae);i9o=r(r$r,"model_type"),r$r.forEach(t),d9o=r(V4,` property of the config object (either
passed as an argument or loaded from `),nae=s(V4,"CODE",{});var t$r=n(nae);c9o=r(t$r,"pretrained_model_name_or_path"),t$r.forEach(t),m9o=r(V4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),lae=s(V4,"CODE",{});var a$r=n(lae);f9o=r(a$r,"pretrained_model_name_or_path"),a$r.forEach(t),h9o=r(V4,":"),V4.forEach(t),g9o=i(It),iae=s(It,"UL",{});var s$r=n(iae);K2=s(s$r,"LI",{});var sEe=n(K2);dae=s(sEe,"STRONG",{});var n$r=n(dae);u9o=r(n$r,"detr"),n$r.forEach(t),p9o=r(sEe," \u2014 "),zP=s(sEe,"A",{href:!0});var l$r=n(zP);_9o=r(l$r,"DetrForSegmentation"),l$r.forEach(t),b9o=r(sEe," (DETR model)"),sEe.forEach(t),s$r.forEach(t),v9o=i(It),Y2=s(It,"P",{});var nEe=n(Y2);T9o=r(nEe,"The model is set in evaluation mode by default using "),cae=s(nEe,"CODE",{});var i$r=n(cae);F9o=r(i$r,"model.eval()"),i$r.forEach(t),E9o=r(nEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mae=s(nEe,"CODE",{});var d$r=n(mae);C9o=r(d$r,"model.train()"),d$r.forEach(t),nEe.forEach(t),M9o=i(It),fae=s(It,"P",{});var c$r=n(fae);y9o=r(c$r,"Examples:"),c$r.forEach(t),w9o=i(It),f(w5.$$.fragment,It),It.forEach(t),Hn.forEach(t),Z5e=i(d),md=s(d,"H2",{class:!0});var l7e=n(md);Z2=s(l7e,"A",{id:!0,class:!0,href:!0});var m$r=n(Z2);hae=s(m$r,"SPAN",{});var f$r=n(hae);f(A5.$$.fragment,f$r),f$r.forEach(t),m$r.forEach(t),A9o=i(l7e),gae=s(l7e,"SPAN",{});var h$r=n(gae);L9o=r(h$r,"TFAutoModel"),h$r.forEach(t),l7e.forEach(t),eye=i(d),nr=s(d,"DIV",{class:!0});var Jn=n(nr);f(L5.$$.fragment,Jn),B9o=i(Jn),fd=s(Jn,"P",{});var Ij=n(fd);x9o=r(Ij,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),uae=s(Ij,"CODE",{});var g$r=n(uae);k9o=r(g$r,"from_pretrained()"),g$r.forEach(t),R9o=r(Ij,` class method or the
`),pae=s(Ij,"CODE",{});var u$r=n(pae);P9o=r(u$r,"from_config()"),u$r.forEach(t),S9o=r(Ij," class method."),Ij.forEach(t),$9o=i(Jn),B5=s(Jn,"P",{});var i7e=n(B5);I9o=r(i7e,"This class cannot be instantiated directly using "),_ae=s(i7e,"CODE",{});var p$r=n(_ae);D9o=r(p$r,"__init__()"),p$r.forEach(t),N9o=r(i7e," (throws an error)."),i7e.forEach(t),j9o=i(Jn),Ur=s(Jn,"DIV",{class:!0});var Kn=n(Ur);f(x5.$$.fragment,Kn),O9o=i(Kn),bae=s(Kn,"P",{});var _$r=n(bae);G9o=r(_$r,"Instantiates one of the base model classes of the library from a configuration."),_$r.forEach(t),q9o=i(Kn),hd=s(Kn,"P",{});var Dj=n(hd);z9o=r(Dj,`Note:
Loading a model from its configuration file does `),vae=s(Dj,"STRONG",{});var b$r=n(vae);X9o=r(b$r,"not"),b$r.forEach(t),Q9o=r(Dj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Tae=s(Dj,"CODE",{});var v$r=n(Tae);V9o=r(v$r,"from_pretrained()"),v$r.forEach(t),W9o=r(Dj,` to load the model
weights.`),Dj.forEach(t),H9o=i(Kn),Fae=s(Kn,"P",{});var T$r=n(Fae);U9o=r(T$r,"Examples:"),T$r.forEach(t),J9o=i(Kn),f(k5.$$.fragment,Kn),Kn.forEach(t),K9o=i(Jn),lo=s(Jn,"DIV",{class:!0});var Vt=n(lo);f(R5.$$.fragment,Vt),Y9o=i(Vt),Eae=s(Vt,"P",{});var F$r=n(Eae);Z9o=r(F$r,"Instantiate one of the base model classes of the library from a pretrained model."),F$r.forEach(t),exo=i(Vt),Xa=s(Vt,"P",{});var W4=n(Xa);oxo=r(W4,"The model class to instantiate is selected based on the "),Cae=s(W4,"CODE",{});var E$r=n(Cae);rxo=r(E$r,"model_type"),E$r.forEach(t),txo=r(W4,` property of the config object (either
passed as an argument or loaded from `),Mae=s(W4,"CODE",{});var C$r=n(Mae);axo=r(C$r,"pretrained_model_name_or_path"),C$r.forEach(t),sxo=r(W4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),yae=s(W4,"CODE",{});var M$r=n(yae);nxo=r(M$r,"pretrained_model_name_or_path"),M$r.forEach(t),lxo=r(W4,":"),W4.forEach(t),ixo=i(Vt),B=s(Vt,"UL",{});var x=n(B);ev=s(x,"LI",{});var lEe=n(ev);wae=s(lEe,"STRONG",{});var y$r=n(wae);dxo=r(y$r,"albert"),y$r.forEach(t),cxo=r(lEe," \u2014 "),XP=s(lEe,"A",{href:!0});var w$r=n(XP);mxo=r(w$r,"TFAlbertModel"),w$r.forEach(t),fxo=r(lEe," (ALBERT model)"),lEe.forEach(t),hxo=i(x),ov=s(x,"LI",{});var iEe=n(ov);Aae=s(iEe,"STRONG",{});var A$r=n(Aae);gxo=r(A$r,"bart"),A$r.forEach(t),uxo=r(iEe," \u2014 "),QP=s(iEe,"A",{href:!0});var L$r=n(QP);pxo=r(L$r,"TFBartModel"),L$r.forEach(t),_xo=r(iEe," (BART model)"),iEe.forEach(t),bxo=i(x),rv=s(x,"LI",{});var dEe=n(rv);Lae=s(dEe,"STRONG",{});var B$r=n(Lae);vxo=r(B$r,"bert"),B$r.forEach(t),Txo=r(dEe," \u2014 "),VP=s(dEe,"A",{href:!0});var x$r=n(VP);Fxo=r(x$r,"TFBertModel"),x$r.forEach(t),Exo=r(dEe," (BERT model)"),dEe.forEach(t),Cxo=i(x),tv=s(x,"LI",{});var cEe=n(tv);Bae=s(cEe,"STRONG",{});var k$r=n(Bae);Mxo=r(k$r,"blenderbot"),k$r.forEach(t),yxo=r(cEe," \u2014 "),WP=s(cEe,"A",{href:!0});var R$r=n(WP);wxo=r(R$r,"TFBlenderbotModel"),R$r.forEach(t),Axo=r(cEe," (Blenderbot model)"),cEe.forEach(t),Lxo=i(x),av=s(x,"LI",{});var mEe=n(av);xae=s(mEe,"STRONG",{});var P$r=n(xae);Bxo=r(P$r,"blenderbot-small"),P$r.forEach(t),xxo=r(mEe," \u2014 "),HP=s(mEe,"A",{href:!0});var S$r=n(HP);kxo=r(S$r,"TFBlenderbotSmallModel"),S$r.forEach(t),Rxo=r(mEe," (BlenderbotSmall model)"),mEe.forEach(t),Pxo=i(x),sv=s(x,"LI",{});var fEe=n(sv);kae=s(fEe,"STRONG",{});var $$r=n(kae);Sxo=r($$r,"camembert"),$$r.forEach(t),$xo=r(fEe," \u2014 "),UP=s(fEe,"A",{href:!0});var I$r=n(UP);Ixo=r(I$r,"TFCamembertModel"),I$r.forEach(t),Dxo=r(fEe," (CamemBERT model)"),fEe.forEach(t),Nxo=i(x),nv=s(x,"LI",{});var hEe=n(nv);Rae=s(hEe,"STRONG",{});var D$r=n(Rae);jxo=r(D$r,"convbert"),D$r.forEach(t),Oxo=r(hEe," \u2014 "),JP=s(hEe,"A",{href:!0});var N$r=n(JP);Gxo=r(N$r,"TFConvBertModel"),N$r.forEach(t),qxo=r(hEe," (ConvBERT model)"),hEe.forEach(t),zxo=i(x),lv=s(x,"LI",{});var gEe=n(lv);Pae=s(gEe,"STRONG",{});var j$r=n(Pae);Xxo=r(j$r,"ctrl"),j$r.forEach(t),Qxo=r(gEe," \u2014 "),KP=s(gEe,"A",{href:!0});var O$r=n(KP);Vxo=r(O$r,"TFCTRLModel"),O$r.forEach(t),Wxo=r(gEe," (CTRL model)"),gEe.forEach(t),Hxo=i(x),iv=s(x,"LI",{});var uEe=n(iv);Sae=s(uEe,"STRONG",{});var G$r=n(Sae);Uxo=r(G$r,"deberta"),G$r.forEach(t),Jxo=r(uEe," \u2014 "),YP=s(uEe,"A",{href:!0});var q$r=n(YP);Kxo=r(q$r,"TFDebertaModel"),q$r.forEach(t),Yxo=r(uEe," (DeBERTa model)"),uEe.forEach(t),Zxo=i(x),dv=s(x,"LI",{});var pEe=n(dv);$ae=s(pEe,"STRONG",{});var z$r=n($ae);eko=r(z$r,"deberta-v2"),z$r.forEach(t),oko=r(pEe," \u2014 "),ZP=s(pEe,"A",{href:!0});var X$r=n(ZP);rko=r(X$r,"TFDebertaV2Model"),X$r.forEach(t),tko=r(pEe," (DeBERTa-v2 model)"),pEe.forEach(t),ako=i(x),cv=s(x,"LI",{});var _Ee=n(cv);Iae=s(_Ee,"STRONG",{});var Q$r=n(Iae);sko=r(Q$r,"distilbert"),Q$r.forEach(t),nko=r(_Ee," \u2014 "),eS=s(_Ee,"A",{href:!0});var V$r=n(eS);lko=r(V$r,"TFDistilBertModel"),V$r.forEach(t),iko=r(_Ee," (DistilBERT model)"),_Ee.forEach(t),dko=i(x),mv=s(x,"LI",{});var bEe=n(mv);Dae=s(bEe,"STRONG",{});var W$r=n(Dae);cko=r(W$r,"dpr"),W$r.forEach(t),mko=r(bEe," \u2014 "),oS=s(bEe,"A",{href:!0});var H$r=n(oS);fko=r(H$r,"TFDPRQuestionEncoder"),H$r.forEach(t),hko=r(bEe," (DPR model)"),bEe.forEach(t),gko=i(x),fv=s(x,"LI",{});var vEe=n(fv);Nae=s(vEe,"STRONG",{});var U$r=n(Nae);uko=r(U$r,"electra"),U$r.forEach(t),pko=r(vEe," \u2014 "),rS=s(vEe,"A",{href:!0});var J$r=n(rS);_ko=r(J$r,"TFElectraModel"),J$r.forEach(t),bko=r(vEe," (ELECTRA model)"),vEe.forEach(t),vko=i(x),hv=s(x,"LI",{});var TEe=n(hv);jae=s(TEe,"STRONG",{});var K$r=n(jae);Tko=r(K$r,"flaubert"),K$r.forEach(t),Fko=r(TEe," \u2014 "),tS=s(TEe,"A",{href:!0});var Y$r=n(tS);Eko=r(Y$r,"TFFlaubertModel"),Y$r.forEach(t),Cko=r(TEe," (FlauBERT model)"),TEe.forEach(t),Mko=i(x),cn=s(x,"LI",{});var v0=n(cn);Oae=s(v0,"STRONG",{});var Z$r=n(Oae);yko=r(Z$r,"funnel"),Z$r.forEach(t),wko=r(v0," \u2014 "),aS=s(v0,"A",{href:!0});var eIr=n(aS);Ako=r(eIr,"TFFunnelModel"),eIr.forEach(t),Lko=r(v0," or "),sS=s(v0,"A",{href:!0});var oIr=n(sS);Bko=r(oIr,"TFFunnelBaseModel"),oIr.forEach(t),xko=r(v0," (Funnel Transformer model)"),v0.forEach(t),kko=i(x),gv=s(x,"LI",{});var FEe=n(gv);Gae=s(FEe,"STRONG",{});var rIr=n(Gae);Rko=r(rIr,"gpt2"),rIr.forEach(t),Pko=r(FEe," \u2014 "),nS=s(FEe,"A",{href:!0});var tIr=n(nS);Sko=r(tIr,"TFGPT2Model"),tIr.forEach(t),$ko=r(FEe," (OpenAI GPT-2 model)"),FEe.forEach(t),Iko=i(x),uv=s(x,"LI",{});var EEe=n(uv);qae=s(EEe,"STRONG",{});var aIr=n(qae);Dko=r(aIr,"hubert"),aIr.forEach(t),Nko=r(EEe," \u2014 "),lS=s(EEe,"A",{href:!0});var sIr=n(lS);jko=r(sIr,"TFHubertModel"),sIr.forEach(t),Oko=r(EEe," (Hubert model)"),EEe.forEach(t),Gko=i(x),pv=s(x,"LI",{});var CEe=n(pv);zae=s(CEe,"STRONG",{});var nIr=n(zae);qko=r(nIr,"layoutlm"),nIr.forEach(t),zko=r(CEe," \u2014 "),iS=s(CEe,"A",{href:!0});var lIr=n(iS);Xko=r(lIr,"TFLayoutLMModel"),lIr.forEach(t),Qko=r(CEe," (LayoutLM model)"),CEe.forEach(t),Vko=i(x),_v=s(x,"LI",{});var MEe=n(_v);Xae=s(MEe,"STRONG",{});var iIr=n(Xae);Wko=r(iIr,"led"),iIr.forEach(t),Hko=r(MEe," \u2014 "),dS=s(MEe,"A",{href:!0});var dIr=n(dS);Uko=r(dIr,"TFLEDModel"),dIr.forEach(t),Jko=r(MEe," (LED model)"),MEe.forEach(t),Kko=i(x),bv=s(x,"LI",{});var yEe=n(bv);Qae=s(yEe,"STRONG",{});var cIr=n(Qae);Yko=r(cIr,"longformer"),cIr.forEach(t),Zko=r(yEe," \u2014 "),cS=s(yEe,"A",{href:!0});var mIr=n(cS);eRo=r(mIr,"TFLongformerModel"),mIr.forEach(t),oRo=r(yEe," (Longformer model)"),yEe.forEach(t),rRo=i(x),vv=s(x,"LI",{});var wEe=n(vv);Vae=s(wEe,"STRONG",{});var fIr=n(Vae);tRo=r(fIr,"lxmert"),fIr.forEach(t),aRo=r(wEe," \u2014 "),mS=s(wEe,"A",{href:!0});var hIr=n(mS);sRo=r(hIr,"TFLxmertModel"),hIr.forEach(t),nRo=r(wEe," (LXMERT model)"),wEe.forEach(t),lRo=i(x),Tv=s(x,"LI",{});var AEe=n(Tv);Wae=s(AEe,"STRONG",{});var gIr=n(Wae);iRo=r(gIr,"marian"),gIr.forEach(t),dRo=r(AEe," \u2014 "),fS=s(AEe,"A",{href:!0});var uIr=n(fS);cRo=r(uIr,"TFMarianModel"),uIr.forEach(t),mRo=r(AEe," (Marian model)"),AEe.forEach(t),fRo=i(x),Fv=s(x,"LI",{});var LEe=n(Fv);Hae=s(LEe,"STRONG",{});var pIr=n(Hae);hRo=r(pIr,"mbart"),pIr.forEach(t),gRo=r(LEe," \u2014 "),hS=s(LEe,"A",{href:!0});var _Ir=n(hS);uRo=r(_Ir,"TFMBartModel"),_Ir.forEach(t),pRo=r(LEe," (mBART model)"),LEe.forEach(t),_Ro=i(x),Ev=s(x,"LI",{});var BEe=n(Ev);Uae=s(BEe,"STRONG",{});var bIr=n(Uae);bRo=r(bIr,"mobilebert"),bIr.forEach(t),vRo=r(BEe," \u2014 "),gS=s(BEe,"A",{href:!0});var vIr=n(gS);TRo=r(vIr,"TFMobileBertModel"),vIr.forEach(t),FRo=r(BEe," (MobileBERT model)"),BEe.forEach(t),ERo=i(x),Cv=s(x,"LI",{});var xEe=n(Cv);Jae=s(xEe,"STRONG",{});var TIr=n(Jae);CRo=r(TIr,"mpnet"),TIr.forEach(t),MRo=r(xEe," \u2014 "),uS=s(xEe,"A",{href:!0});var FIr=n(uS);yRo=r(FIr,"TFMPNetModel"),FIr.forEach(t),wRo=r(xEe," (MPNet model)"),xEe.forEach(t),ARo=i(x),Mv=s(x,"LI",{});var kEe=n(Mv);Kae=s(kEe,"STRONG",{});var EIr=n(Kae);LRo=r(EIr,"mt5"),EIr.forEach(t),BRo=r(kEe," \u2014 "),pS=s(kEe,"A",{href:!0});var CIr=n(pS);xRo=r(CIr,"TFMT5Model"),CIr.forEach(t),kRo=r(kEe," (mT5 model)"),kEe.forEach(t),RRo=i(x),yv=s(x,"LI",{});var REe=n(yv);Yae=s(REe,"STRONG",{});var MIr=n(Yae);PRo=r(MIr,"openai-gpt"),MIr.forEach(t),SRo=r(REe," \u2014 "),_S=s(REe,"A",{href:!0});var yIr=n(_S);$Ro=r(yIr,"TFOpenAIGPTModel"),yIr.forEach(t),IRo=r(REe," (OpenAI GPT model)"),REe.forEach(t),DRo=i(x),wv=s(x,"LI",{});var PEe=n(wv);Zae=s(PEe,"STRONG",{});var wIr=n(Zae);NRo=r(wIr,"pegasus"),wIr.forEach(t),jRo=r(PEe," \u2014 "),bS=s(PEe,"A",{href:!0});var AIr=n(bS);ORo=r(AIr,"TFPegasusModel"),AIr.forEach(t),GRo=r(PEe," (Pegasus model)"),PEe.forEach(t),qRo=i(x),Av=s(x,"LI",{});var SEe=n(Av);ese=s(SEe,"STRONG",{});var LIr=n(ese);zRo=r(LIr,"rembert"),LIr.forEach(t),XRo=r(SEe," \u2014 "),vS=s(SEe,"A",{href:!0});var BIr=n(vS);QRo=r(BIr,"TFRemBertModel"),BIr.forEach(t),VRo=r(SEe," (RemBERT model)"),SEe.forEach(t),WRo=i(x),Lv=s(x,"LI",{});var $Ee=n(Lv);ose=s($Ee,"STRONG",{});var xIr=n(ose);HRo=r(xIr,"roberta"),xIr.forEach(t),URo=r($Ee," \u2014 "),TS=s($Ee,"A",{href:!0});var kIr=n(TS);JRo=r(kIr,"TFRobertaModel"),kIr.forEach(t),KRo=r($Ee," (RoBERTa model)"),$Ee.forEach(t),YRo=i(x),Bv=s(x,"LI",{});var IEe=n(Bv);rse=s(IEe,"STRONG",{});var RIr=n(rse);ZRo=r(RIr,"roformer"),RIr.forEach(t),ePo=r(IEe," \u2014 "),FS=s(IEe,"A",{href:!0});var PIr=n(FS);oPo=r(PIr,"TFRoFormerModel"),PIr.forEach(t),rPo=r(IEe," (RoFormer model)"),IEe.forEach(t),tPo=i(x),xv=s(x,"LI",{});var DEe=n(xv);tse=s(DEe,"STRONG",{});var SIr=n(tse);aPo=r(SIr,"t5"),SIr.forEach(t),sPo=r(DEe," \u2014 "),ES=s(DEe,"A",{href:!0});var $Ir=n(ES);nPo=r($Ir,"TFT5Model"),$Ir.forEach(t),lPo=r(DEe," (T5 model)"),DEe.forEach(t),iPo=i(x),kv=s(x,"LI",{});var NEe=n(kv);ase=s(NEe,"STRONG",{});var IIr=n(ase);dPo=r(IIr,"tapas"),IIr.forEach(t),cPo=r(NEe," \u2014 "),CS=s(NEe,"A",{href:!0});var DIr=n(CS);mPo=r(DIr,"TFTapasModel"),DIr.forEach(t),fPo=r(NEe," (TAPAS model)"),NEe.forEach(t),hPo=i(x),Rv=s(x,"LI",{});var jEe=n(Rv);sse=s(jEe,"STRONG",{});var NIr=n(sse);gPo=r(NIr,"transfo-xl"),NIr.forEach(t),uPo=r(jEe," \u2014 "),MS=s(jEe,"A",{href:!0});var jIr=n(MS);pPo=r(jIr,"TFTransfoXLModel"),jIr.forEach(t),_Po=r(jEe," (Transformer-XL model)"),jEe.forEach(t),bPo=i(x),Pv=s(x,"LI",{});var OEe=n(Pv);nse=s(OEe,"STRONG",{});var OIr=n(nse);vPo=r(OIr,"vit"),OIr.forEach(t),TPo=r(OEe," \u2014 "),yS=s(OEe,"A",{href:!0});var GIr=n(yS);FPo=r(GIr,"TFViTModel"),GIr.forEach(t),EPo=r(OEe," (ViT model)"),OEe.forEach(t),CPo=i(x),Sv=s(x,"LI",{});var GEe=n(Sv);lse=s(GEe,"STRONG",{});var qIr=n(lse);MPo=r(qIr,"wav2vec2"),qIr.forEach(t),yPo=r(GEe," \u2014 "),wS=s(GEe,"A",{href:!0});var zIr=n(wS);wPo=r(zIr,"TFWav2Vec2Model"),zIr.forEach(t),APo=r(GEe," (Wav2Vec2 model)"),GEe.forEach(t),LPo=i(x),$v=s(x,"LI",{});var qEe=n($v);ise=s(qEe,"STRONG",{});var XIr=n(ise);BPo=r(XIr,"xlm"),XIr.forEach(t),xPo=r(qEe," \u2014 "),AS=s(qEe,"A",{href:!0});var QIr=n(AS);kPo=r(QIr,"TFXLMModel"),QIr.forEach(t),RPo=r(qEe," (XLM model)"),qEe.forEach(t),PPo=i(x),Iv=s(x,"LI",{});var zEe=n(Iv);dse=s(zEe,"STRONG",{});var VIr=n(dse);SPo=r(VIr,"xlm-roberta"),VIr.forEach(t),$Po=r(zEe," \u2014 "),LS=s(zEe,"A",{href:!0});var WIr=n(LS);IPo=r(WIr,"TFXLMRobertaModel"),WIr.forEach(t),DPo=r(zEe," (XLM-RoBERTa model)"),zEe.forEach(t),NPo=i(x),Dv=s(x,"LI",{});var XEe=n(Dv);cse=s(XEe,"STRONG",{});var HIr=n(cse);jPo=r(HIr,"xlnet"),HIr.forEach(t),OPo=r(XEe," \u2014 "),BS=s(XEe,"A",{href:!0});var UIr=n(BS);GPo=r(UIr,"TFXLNetModel"),UIr.forEach(t),qPo=r(XEe," (XLNet model)"),XEe.forEach(t),x.forEach(t),zPo=i(Vt),mse=s(Vt,"P",{});var JIr=n(mse);XPo=r(JIr,"Examples:"),JIr.forEach(t),QPo=i(Vt),f(P5.$$.fragment,Vt),Vt.forEach(t),Jn.forEach(t),oye=i(d),gd=s(d,"H2",{class:!0});var d7e=n(gd);Nv=s(d7e,"A",{id:!0,class:!0,href:!0});var KIr=n(Nv);fse=s(KIr,"SPAN",{});var YIr=n(fse);f(S5.$$.fragment,YIr),YIr.forEach(t),KIr.forEach(t),VPo=i(d7e),hse=s(d7e,"SPAN",{});var ZIr=n(hse);WPo=r(ZIr,"TFAutoModelForPreTraining"),ZIr.forEach(t),d7e.forEach(t),rye=i(d),lr=s(d,"DIV",{class:!0});var Yn=n(lr);f($5.$$.fragment,Yn),HPo=i(Yn),ud=s(Yn,"P",{});var Nj=n(ud);UPo=r(Nj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),gse=s(Nj,"CODE",{});var eDr=n(gse);JPo=r(eDr,"from_pretrained()"),eDr.forEach(t),KPo=r(Nj,` class method or the
`),use=s(Nj,"CODE",{});var oDr=n(use);YPo=r(oDr,"from_config()"),oDr.forEach(t),ZPo=r(Nj," class method."),Nj.forEach(t),eSo=i(Yn),I5=s(Yn,"P",{});var c7e=n(I5);oSo=r(c7e,"This class cannot be instantiated directly using "),pse=s(c7e,"CODE",{});var rDr=n(pse);rSo=r(rDr,"__init__()"),rDr.forEach(t),tSo=r(c7e," (throws an error)."),c7e.forEach(t),aSo=i(Yn),Jr=s(Yn,"DIV",{class:!0});var Zn=n(Jr);f(D5.$$.fragment,Zn),sSo=i(Zn),_se=s(Zn,"P",{});var tDr=n(_se);nSo=r(tDr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),tDr.forEach(t),lSo=i(Zn),pd=s(Zn,"P",{});var jj=n(pd);iSo=r(jj,`Note:
Loading a model from its configuration file does `),bse=s(jj,"STRONG",{});var aDr=n(bse);dSo=r(aDr,"not"),aDr.forEach(t),cSo=r(jj,` load the model weights. It only affects the
model\u2019s configuration. Use `),vse=s(jj,"CODE",{});var sDr=n(vse);mSo=r(sDr,"from_pretrained()"),sDr.forEach(t),fSo=r(jj,` to load the model
weights.`),jj.forEach(t),hSo=i(Zn),Tse=s(Zn,"P",{});var nDr=n(Tse);gSo=r(nDr,"Examples:"),nDr.forEach(t),uSo=i(Zn),f(N5.$$.fragment,Zn),Zn.forEach(t),pSo=i(Yn),io=s(Yn,"DIV",{class:!0});var Wt=n(io);f(j5.$$.fragment,Wt),_So=i(Wt),Fse=s(Wt,"P",{});var lDr=n(Fse);bSo=r(lDr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),lDr.forEach(t),vSo=i(Wt),Qa=s(Wt,"P",{});var H4=n(Qa);TSo=r(H4,"The model class to instantiate is selected based on the "),Ese=s(H4,"CODE",{});var iDr=n(Ese);FSo=r(iDr,"model_type"),iDr.forEach(t),ESo=r(H4,` property of the config object (either
passed as an argument or loaded from `),Cse=s(H4,"CODE",{});var dDr=n(Cse);CSo=r(dDr,"pretrained_model_name_or_path"),dDr.forEach(t),MSo=r(H4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Mse=s(H4,"CODE",{});var cDr=n(Mse);ySo=r(cDr,"pretrained_model_name_or_path"),cDr.forEach(t),wSo=r(H4,":"),H4.forEach(t),ASo=i(Wt),K=s(Wt,"UL",{});var ee=n(K);jv=s(ee,"LI",{});var QEe=n(jv);yse=s(QEe,"STRONG",{});var mDr=n(yse);LSo=r(mDr,"albert"),mDr.forEach(t),BSo=r(QEe," \u2014 "),xS=s(QEe,"A",{href:!0});var fDr=n(xS);xSo=r(fDr,"TFAlbertForPreTraining"),fDr.forEach(t),kSo=r(QEe," (ALBERT model)"),QEe.forEach(t),RSo=i(ee),Ov=s(ee,"LI",{});var VEe=n(Ov);wse=s(VEe,"STRONG",{});var hDr=n(wse);PSo=r(hDr,"bart"),hDr.forEach(t),SSo=r(VEe," \u2014 "),kS=s(VEe,"A",{href:!0});var gDr=n(kS);$So=r(gDr,"TFBartForConditionalGeneration"),gDr.forEach(t),ISo=r(VEe," (BART model)"),VEe.forEach(t),DSo=i(ee),Gv=s(ee,"LI",{});var WEe=n(Gv);Ase=s(WEe,"STRONG",{});var uDr=n(Ase);NSo=r(uDr,"bert"),uDr.forEach(t),jSo=r(WEe," \u2014 "),RS=s(WEe,"A",{href:!0});var pDr=n(RS);OSo=r(pDr,"TFBertForPreTraining"),pDr.forEach(t),GSo=r(WEe," (BERT model)"),WEe.forEach(t),qSo=i(ee),qv=s(ee,"LI",{});var HEe=n(qv);Lse=s(HEe,"STRONG",{});var _Dr=n(Lse);zSo=r(_Dr,"camembert"),_Dr.forEach(t),XSo=r(HEe," \u2014 "),PS=s(HEe,"A",{href:!0});var bDr=n(PS);QSo=r(bDr,"TFCamembertForMaskedLM"),bDr.forEach(t),VSo=r(HEe," (CamemBERT model)"),HEe.forEach(t),WSo=i(ee),zv=s(ee,"LI",{});var UEe=n(zv);Bse=s(UEe,"STRONG",{});var vDr=n(Bse);HSo=r(vDr,"ctrl"),vDr.forEach(t),USo=r(UEe," \u2014 "),SS=s(UEe,"A",{href:!0});var TDr=n(SS);JSo=r(TDr,"TFCTRLLMHeadModel"),TDr.forEach(t),KSo=r(UEe," (CTRL model)"),UEe.forEach(t),YSo=i(ee),Xv=s(ee,"LI",{});var JEe=n(Xv);xse=s(JEe,"STRONG",{});var FDr=n(xse);ZSo=r(FDr,"distilbert"),FDr.forEach(t),e$o=r(JEe," \u2014 "),$S=s(JEe,"A",{href:!0});var EDr=n($S);o$o=r(EDr,"TFDistilBertForMaskedLM"),EDr.forEach(t),r$o=r(JEe," (DistilBERT model)"),JEe.forEach(t),t$o=i(ee),Qv=s(ee,"LI",{});var KEe=n(Qv);kse=s(KEe,"STRONG",{});var CDr=n(kse);a$o=r(CDr,"electra"),CDr.forEach(t),s$o=r(KEe," \u2014 "),IS=s(KEe,"A",{href:!0});var MDr=n(IS);n$o=r(MDr,"TFElectraForPreTraining"),MDr.forEach(t),l$o=r(KEe," (ELECTRA model)"),KEe.forEach(t),i$o=i(ee),Vv=s(ee,"LI",{});var YEe=n(Vv);Rse=s(YEe,"STRONG",{});var yDr=n(Rse);d$o=r(yDr,"flaubert"),yDr.forEach(t),c$o=r(YEe," \u2014 "),DS=s(YEe,"A",{href:!0});var wDr=n(DS);m$o=r(wDr,"TFFlaubertWithLMHeadModel"),wDr.forEach(t),f$o=r(YEe," (FlauBERT model)"),YEe.forEach(t),h$o=i(ee),Wv=s(ee,"LI",{});var ZEe=n(Wv);Pse=s(ZEe,"STRONG",{});var ADr=n(Pse);g$o=r(ADr,"funnel"),ADr.forEach(t),u$o=r(ZEe," \u2014 "),NS=s(ZEe,"A",{href:!0});var LDr=n(NS);p$o=r(LDr,"TFFunnelForPreTraining"),LDr.forEach(t),_$o=r(ZEe," (Funnel Transformer model)"),ZEe.forEach(t),b$o=i(ee),Hv=s(ee,"LI",{});var e4e=n(Hv);Sse=s(e4e,"STRONG",{});var BDr=n(Sse);v$o=r(BDr,"gpt2"),BDr.forEach(t),T$o=r(e4e," \u2014 "),jS=s(e4e,"A",{href:!0});var xDr=n(jS);F$o=r(xDr,"TFGPT2LMHeadModel"),xDr.forEach(t),E$o=r(e4e," (OpenAI GPT-2 model)"),e4e.forEach(t),C$o=i(ee),Uv=s(ee,"LI",{});var o4e=n(Uv);$se=s(o4e,"STRONG",{});var kDr=n($se);M$o=r(kDr,"layoutlm"),kDr.forEach(t),y$o=r(o4e," \u2014 "),OS=s(o4e,"A",{href:!0});var RDr=n(OS);w$o=r(RDr,"TFLayoutLMForMaskedLM"),RDr.forEach(t),A$o=r(o4e," (LayoutLM model)"),o4e.forEach(t),L$o=i(ee),Jv=s(ee,"LI",{});var r4e=n(Jv);Ise=s(r4e,"STRONG",{});var PDr=n(Ise);B$o=r(PDr,"lxmert"),PDr.forEach(t),x$o=r(r4e," \u2014 "),GS=s(r4e,"A",{href:!0});var SDr=n(GS);k$o=r(SDr,"TFLxmertForPreTraining"),SDr.forEach(t),R$o=r(r4e," (LXMERT model)"),r4e.forEach(t),P$o=i(ee),Kv=s(ee,"LI",{});var t4e=n(Kv);Dse=s(t4e,"STRONG",{});var $Dr=n(Dse);S$o=r($Dr,"mobilebert"),$Dr.forEach(t),$$o=r(t4e," \u2014 "),qS=s(t4e,"A",{href:!0});var IDr=n(qS);I$o=r(IDr,"TFMobileBertForPreTraining"),IDr.forEach(t),D$o=r(t4e," (MobileBERT model)"),t4e.forEach(t),N$o=i(ee),Yv=s(ee,"LI",{});var a4e=n(Yv);Nse=s(a4e,"STRONG",{});var DDr=n(Nse);j$o=r(DDr,"mpnet"),DDr.forEach(t),O$o=r(a4e," \u2014 "),zS=s(a4e,"A",{href:!0});var NDr=n(zS);G$o=r(NDr,"TFMPNetForMaskedLM"),NDr.forEach(t),q$o=r(a4e," (MPNet model)"),a4e.forEach(t),z$o=i(ee),Zv=s(ee,"LI",{});var s4e=n(Zv);jse=s(s4e,"STRONG",{});var jDr=n(jse);X$o=r(jDr,"openai-gpt"),jDr.forEach(t),Q$o=r(s4e," \u2014 "),XS=s(s4e,"A",{href:!0});var ODr=n(XS);V$o=r(ODr,"TFOpenAIGPTLMHeadModel"),ODr.forEach(t),W$o=r(s4e," (OpenAI GPT model)"),s4e.forEach(t),H$o=i(ee),eT=s(ee,"LI",{});var n4e=n(eT);Ose=s(n4e,"STRONG",{});var GDr=n(Ose);U$o=r(GDr,"roberta"),GDr.forEach(t),J$o=r(n4e," \u2014 "),QS=s(n4e,"A",{href:!0});var qDr=n(QS);K$o=r(qDr,"TFRobertaForMaskedLM"),qDr.forEach(t),Y$o=r(n4e," (RoBERTa model)"),n4e.forEach(t),Z$o=i(ee),oT=s(ee,"LI",{});var l4e=n(oT);Gse=s(l4e,"STRONG",{});var zDr=n(Gse);eIo=r(zDr,"t5"),zDr.forEach(t),oIo=r(l4e," \u2014 "),VS=s(l4e,"A",{href:!0});var XDr=n(VS);rIo=r(XDr,"TFT5ForConditionalGeneration"),XDr.forEach(t),tIo=r(l4e," (T5 model)"),l4e.forEach(t),aIo=i(ee),rT=s(ee,"LI",{});var i4e=n(rT);qse=s(i4e,"STRONG",{});var QDr=n(qse);sIo=r(QDr,"tapas"),QDr.forEach(t),nIo=r(i4e," \u2014 "),WS=s(i4e,"A",{href:!0});var VDr=n(WS);lIo=r(VDr,"TFTapasForMaskedLM"),VDr.forEach(t),iIo=r(i4e," (TAPAS model)"),i4e.forEach(t),dIo=i(ee),tT=s(ee,"LI",{});var d4e=n(tT);zse=s(d4e,"STRONG",{});var WDr=n(zse);cIo=r(WDr,"transfo-xl"),WDr.forEach(t),mIo=r(d4e," \u2014 "),HS=s(d4e,"A",{href:!0});var HDr=n(HS);fIo=r(HDr,"TFTransfoXLLMHeadModel"),HDr.forEach(t),hIo=r(d4e," (Transformer-XL model)"),d4e.forEach(t),gIo=i(ee),aT=s(ee,"LI",{});var c4e=n(aT);Xse=s(c4e,"STRONG",{});var UDr=n(Xse);uIo=r(UDr,"xlm"),UDr.forEach(t),pIo=r(c4e," \u2014 "),US=s(c4e,"A",{href:!0});var JDr=n(US);_Io=r(JDr,"TFXLMWithLMHeadModel"),JDr.forEach(t),bIo=r(c4e," (XLM model)"),c4e.forEach(t),vIo=i(ee),sT=s(ee,"LI",{});var m4e=n(sT);Qse=s(m4e,"STRONG",{});var KDr=n(Qse);TIo=r(KDr,"xlm-roberta"),KDr.forEach(t),FIo=r(m4e," \u2014 "),JS=s(m4e,"A",{href:!0});var YDr=n(JS);EIo=r(YDr,"TFXLMRobertaForMaskedLM"),YDr.forEach(t),CIo=r(m4e," (XLM-RoBERTa model)"),m4e.forEach(t),MIo=i(ee),nT=s(ee,"LI",{});var f4e=n(nT);Vse=s(f4e,"STRONG",{});var ZDr=n(Vse);yIo=r(ZDr,"xlnet"),ZDr.forEach(t),wIo=r(f4e," \u2014 "),KS=s(f4e,"A",{href:!0});var eNr=n(KS);AIo=r(eNr,"TFXLNetLMHeadModel"),eNr.forEach(t),LIo=r(f4e," (XLNet model)"),f4e.forEach(t),ee.forEach(t),BIo=i(Wt),Wse=s(Wt,"P",{});var oNr=n(Wse);xIo=r(oNr,"Examples:"),oNr.forEach(t),kIo=i(Wt),f(O5.$$.fragment,Wt),Wt.forEach(t),Yn.forEach(t),tye=i(d),_d=s(d,"H2",{class:!0});var m7e=n(_d);lT=s(m7e,"A",{id:!0,class:!0,href:!0});var rNr=n(lT);Hse=s(rNr,"SPAN",{});var tNr=n(Hse);f(G5.$$.fragment,tNr),tNr.forEach(t),rNr.forEach(t),RIo=i(m7e),Use=s(m7e,"SPAN",{});var aNr=n(Use);PIo=r(aNr,"TFAutoModelForCausalLM"),aNr.forEach(t),m7e.forEach(t),aye=i(d),ir=s(d,"DIV",{class:!0});var el=n(ir);f(q5.$$.fragment,el),SIo=i(el),bd=s(el,"P",{});var Oj=n(bd);$Io=r(Oj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Jse=s(Oj,"CODE",{});var sNr=n(Jse);IIo=r(sNr,"from_pretrained()"),sNr.forEach(t),DIo=r(Oj,` class method or the
`),Kse=s(Oj,"CODE",{});var nNr=n(Kse);NIo=r(nNr,"from_config()"),nNr.forEach(t),jIo=r(Oj," class method."),Oj.forEach(t),OIo=i(el),z5=s(el,"P",{});var f7e=n(z5);GIo=r(f7e,"This class cannot be instantiated directly using "),Yse=s(f7e,"CODE",{});var lNr=n(Yse);qIo=r(lNr,"__init__()"),lNr.forEach(t),zIo=r(f7e," (throws an error)."),f7e.forEach(t),XIo=i(el),Kr=s(el,"DIV",{class:!0});var ol=n(Kr);f(X5.$$.fragment,ol),QIo=i(ol),Zse=s(ol,"P",{});var iNr=n(Zse);VIo=r(iNr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),iNr.forEach(t),WIo=i(ol),vd=s(ol,"P",{});var Gj=n(vd);HIo=r(Gj,`Note:
Loading a model from its configuration file does `),ene=s(Gj,"STRONG",{});var dNr=n(ene);UIo=r(dNr,"not"),dNr.forEach(t),JIo=r(Gj,` load the model weights. It only affects the
model\u2019s configuration. Use `),one=s(Gj,"CODE",{});var cNr=n(one);KIo=r(cNr,"from_pretrained()"),cNr.forEach(t),YIo=r(Gj,` to load the model
weights.`),Gj.forEach(t),ZIo=i(ol),rne=s(ol,"P",{});var mNr=n(rne);eDo=r(mNr,"Examples:"),mNr.forEach(t),oDo=i(ol),f(Q5.$$.fragment,ol),ol.forEach(t),rDo=i(el),co=s(el,"DIV",{class:!0});var Ht=n(co);f(V5.$$.fragment,Ht),tDo=i(Ht),tne=s(Ht,"P",{});var fNr=n(tne);aDo=r(fNr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),fNr.forEach(t),sDo=i(Ht),Va=s(Ht,"P",{});var U4=n(Va);nDo=r(U4,"The model class to instantiate is selected based on the "),ane=s(U4,"CODE",{});var hNr=n(ane);lDo=r(hNr,"model_type"),hNr.forEach(t),iDo=r(U4,` property of the config object (either
passed as an argument or loaded from `),sne=s(U4,"CODE",{});var gNr=n(sne);dDo=r(gNr,"pretrained_model_name_or_path"),gNr.forEach(t),cDo=r(U4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),nne=s(U4,"CODE",{});var uNr=n(nne);mDo=r(uNr,"pretrained_model_name_or_path"),uNr.forEach(t),fDo=r(U4,":"),U4.forEach(t),hDo=i(Ht),_e=s(Ht,"UL",{});var we=n(_e);iT=s(we,"LI",{});var h4e=n(iT);lne=s(h4e,"STRONG",{});var pNr=n(lne);gDo=r(pNr,"bert"),pNr.forEach(t),uDo=r(h4e," \u2014 "),YS=s(h4e,"A",{href:!0});var _Nr=n(YS);pDo=r(_Nr,"TFBertLMHeadModel"),_Nr.forEach(t),_Do=r(h4e," (BERT model)"),h4e.forEach(t),bDo=i(we),dT=s(we,"LI",{});var g4e=n(dT);ine=s(g4e,"STRONG",{});var bNr=n(ine);vDo=r(bNr,"ctrl"),bNr.forEach(t),TDo=r(g4e," \u2014 "),ZS=s(g4e,"A",{href:!0});var vNr=n(ZS);FDo=r(vNr,"TFCTRLLMHeadModel"),vNr.forEach(t),EDo=r(g4e," (CTRL model)"),g4e.forEach(t),CDo=i(we),cT=s(we,"LI",{});var u4e=n(cT);dne=s(u4e,"STRONG",{});var TNr=n(dne);MDo=r(TNr,"gpt2"),TNr.forEach(t),yDo=r(u4e," \u2014 "),e$=s(u4e,"A",{href:!0});var FNr=n(e$);wDo=r(FNr,"TFGPT2LMHeadModel"),FNr.forEach(t),ADo=r(u4e," (OpenAI GPT-2 model)"),u4e.forEach(t),LDo=i(we),mT=s(we,"LI",{});var p4e=n(mT);cne=s(p4e,"STRONG",{});var ENr=n(cne);BDo=r(ENr,"openai-gpt"),ENr.forEach(t),xDo=r(p4e," \u2014 "),o$=s(p4e,"A",{href:!0});var CNr=n(o$);kDo=r(CNr,"TFOpenAIGPTLMHeadModel"),CNr.forEach(t),RDo=r(p4e," (OpenAI GPT model)"),p4e.forEach(t),PDo=i(we),fT=s(we,"LI",{});var _4e=n(fT);mne=s(_4e,"STRONG",{});var MNr=n(mne);SDo=r(MNr,"rembert"),MNr.forEach(t),$Do=r(_4e," \u2014 "),r$=s(_4e,"A",{href:!0});var yNr=n(r$);IDo=r(yNr,"TFRemBertForCausalLM"),yNr.forEach(t),DDo=r(_4e," (RemBERT model)"),_4e.forEach(t),NDo=i(we),hT=s(we,"LI",{});var b4e=n(hT);fne=s(b4e,"STRONG",{});var wNr=n(fne);jDo=r(wNr,"roberta"),wNr.forEach(t),ODo=r(b4e," \u2014 "),t$=s(b4e,"A",{href:!0});var ANr=n(t$);GDo=r(ANr,"TFRobertaForCausalLM"),ANr.forEach(t),qDo=r(b4e," (RoBERTa model)"),b4e.forEach(t),zDo=i(we),gT=s(we,"LI",{});var v4e=n(gT);hne=s(v4e,"STRONG",{});var LNr=n(hne);XDo=r(LNr,"roformer"),LNr.forEach(t),QDo=r(v4e," \u2014 "),a$=s(v4e,"A",{href:!0});var BNr=n(a$);VDo=r(BNr,"TFRoFormerForCausalLM"),BNr.forEach(t),WDo=r(v4e," (RoFormer model)"),v4e.forEach(t),HDo=i(we),uT=s(we,"LI",{});var T4e=n(uT);gne=s(T4e,"STRONG",{});var xNr=n(gne);UDo=r(xNr,"transfo-xl"),xNr.forEach(t),JDo=r(T4e," \u2014 "),s$=s(T4e,"A",{href:!0});var kNr=n(s$);KDo=r(kNr,"TFTransfoXLLMHeadModel"),kNr.forEach(t),YDo=r(T4e," (Transformer-XL model)"),T4e.forEach(t),ZDo=i(we),pT=s(we,"LI",{});var F4e=n(pT);une=s(F4e,"STRONG",{});var RNr=n(une);eNo=r(RNr,"xlm"),RNr.forEach(t),oNo=r(F4e," \u2014 "),n$=s(F4e,"A",{href:!0});var PNr=n(n$);rNo=r(PNr,"TFXLMWithLMHeadModel"),PNr.forEach(t),tNo=r(F4e," (XLM model)"),F4e.forEach(t),aNo=i(we),_T=s(we,"LI",{});var E4e=n(_T);pne=s(E4e,"STRONG",{});var SNr=n(pne);sNo=r(SNr,"xlnet"),SNr.forEach(t),nNo=r(E4e," \u2014 "),l$=s(E4e,"A",{href:!0});var $Nr=n(l$);lNo=r($Nr,"TFXLNetLMHeadModel"),$Nr.forEach(t),iNo=r(E4e," (XLNet model)"),E4e.forEach(t),we.forEach(t),dNo=i(Ht),_ne=s(Ht,"P",{});var INr=n(_ne);cNo=r(INr,"Examples:"),INr.forEach(t),mNo=i(Ht),f(W5.$$.fragment,Ht),Ht.forEach(t),el.forEach(t),sye=i(d),Td=s(d,"H2",{class:!0});var h7e=n(Td);bT=s(h7e,"A",{id:!0,class:!0,href:!0});var DNr=n(bT);bne=s(DNr,"SPAN",{});var NNr=n(bne);f(H5.$$.fragment,NNr),NNr.forEach(t),DNr.forEach(t),fNo=i(h7e),vne=s(h7e,"SPAN",{});var jNr=n(vne);hNo=r(jNr,"TFAutoModelForImageClassification"),jNr.forEach(t),h7e.forEach(t),nye=i(d),dr=s(d,"DIV",{class:!0});var rl=n(dr);f(U5.$$.fragment,rl),gNo=i(rl),Fd=s(rl,"P",{});var qj=n(Fd);uNo=r(qj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Tne=s(qj,"CODE",{});var ONr=n(Tne);pNo=r(ONr,"from_pretrained()"),ONr.forEach(t),_No=r(qj,` class method or the
`),Fne=s(qj,"CODE",{});var GNr=n(Fne);bNo=r(GNr,"from_config()"),GNr.forEach(t),vNo=r(qj," class method."),qj.forEach(t),TNo=i(rl),J5=s(rl,"P",{});var g7e=n(J5);FNo=r(g7e,"This class cannot be instantiated directly using "),Ene=s(g7e,"CODE",{});var qNr=n(Ene);ENo=r(qNr,"__init__()"),qNr.forEach(t),CNo=r(g7e," (throws an error)."),g7e.forEach(t),MNo=i(rl),Yr=s(rl,"DIV",{class:!0});var tl=n(Yr);f(K5.$$.fragment,tl),yNo=i(tl),Cne=s(tl,"P",{});var zNr=n(Cne);wNo=r(zNr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),zNr.forEach(t),ANo=i(tl),Ed=s(tl,"P",{});var zj=n(Ed);LNo=r(zj,`Note:
Loading a model from its configuration file does `),Mne=s(zj,"STRONG",{});var XNr=n(Mne);BNo=r(XNr,"not"),XNr.forEach(t),xNo=r(zj,` load the model weights. It only affects the
model\u2019s configuration. Use `),yne=s(zj,"CODE",{});var QNr=n(yne);kNo=r(QNr,"from_pretrained()"),QNr.forEach(t),RNo=r(zj,` to load the model
weights.`),zj.forEach(t),PNo=i(tl),wne=s(tl,"P",{});var VNr=n(wne);SNo=r(VNr,"Examples:"),VNr.forEach(t),$No=i(tl),f(Y5.$$.fragment,tl),tl.forEach(t),INo=i(rl),mo=s(rl,"DIV",{class:!0});var Ut=n(mo);f(Z5.$$.fragment,Ut),DNo=i(Ut),Ane=s(Ut,"P",{});var WNr=n(Ane);NNo=r(WNr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),WNr.forEach(t),jNo=i(Ut),Wa=s(Ut,"P",{});var J4=n(Wa);ONo=r(J4,"The model class to instantiate is selected based on the "),Lne=s(J4,"CODE",{});var HNr=n(Lne);GNo=r(HNr,"model_type"),HNr.forEach(t),qNo=r(J4,` property of the config object (either
passed as an argument or loaded from `),Bne=s(J4,"CODE",{});var UNr=n(Bne);zNo=r(UNr,"pretrained_model_name_or_path"),UNr.forEach(t),XNo=r(J4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),xne=s(J4,"CODE",{});var JNr=n(xne);QNo=r(JNr,"pretrained_model_name_or_path"),JNr.forEach(t),VNo=r(J4,":"),J4.forEach(t),WNo=i(Ut),kne=s(Ut,"UL",{});var KNr=n(kne);vT=s(KNr,"LI",{});var C4e=n(vT);Rne=s(C4e,"STRONG",{});var YNr=n(Rne);HNo=r(YNr,"vit"),YNr.forEach(t),UNo=r(C4e," \u2014 "),i$=s(C4e,"A",{href:!0});var ZNr=n(i$);JNo=r(ZNr,"TFViTForImageClassification"),ZNr.forEach(t),KNo=r(C4e," (ViT model)"),C4e.forEach(t),KNr.forEach(t),YNo=i(Ut),Pne=s(Ut,"P",{});var ejr=n(Pne);ZNo=r(ejr,"Examples:"),ejr.forEach(t),ejo=i(Ut),f(ey.$$.fragment,Ut),Ut.forEach(t),rl.forEach(t),lye=i(d),Cd=s(d,"H2",{class:!0});var u7e=n(Cd);TT=s(u7e,"A",{id:!0,class:!0,href:!0});var ojr=n(TT);Sne=s(ojr,"SPAN",{});var rjr=n(Sne);f(oy.$$.fragment,rjr),rjr.forEach(t),ojr.forEach(t),ojo=i(u7e),$ne=s(u7e,"SPAN",{});var tjr=n($ne);rjo=r(tjr,"TFAutoModelForMaskedLM"),tjr.forEach(t),u7e.forEach(t),iye=i(d),cr=s(d,"DIV",{class:!0});var al=n(cr);f(ry.$$.fragment,al),tjo=i(al),Md=s(al,"P",{});var Xj=n(Md);ajo=r(Xj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Ine=s(Xj,"CODE",{});var ajr=n(Ine);sjo=r(ajr,"from_pretrained()"),ajr.forEach(t),njo=r(Xj,` class method or the
`),Dne=s(Xj,"CODE",{});var sjr=n(Dne);ljo=r(sjr,"from_config()"),sjr.forEach(t),ijo=r(Xj," class method."),Xj.forEach(t),djo=i(al),ty=s(al,"P",{});var p7e=n(ty);cjo=r(p7e,"This class cannot be instantiated directly using "),Nne=s(p7e,"CODE",{});var njr=n(Nne);mjo=r(njr,"__init__()"),njr.forEach(t),fjo=r(p7e," (throws an error)."),p7e.forEach(t),hjo=i(al),Zr=s(al,"DIV",{class:!0});var sl=n(Zr);f(ay.$$.fragment,sl),gjo=i(sl),jne=s(sl,"P",{});var ljr=n(jne);ujo=r(ljr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),ljr.forEach(t),pjo=i(sl),yd=s(sl,"P",{});var Qj=n(yd);_jo=r(Qj,`Note:
Loading a model from its configuration file does `),One=s(Qj,"STRONG",{});var ijr=n(One);bjo=r(ijr,"not"),ijr.forEach(t),vjo=r(Qj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Gne=s(Qj,"CODE",{});var djr=n(Gne);Tjo=r(djr,"from_pretrained()"),djr.forEach(t),Fjo=r(Qj,` to load the model
weights.`),Qj.forEach(t),Ejo=i(sl),qne=s(sl,"P",{});var cjr=n(qne);Cjo=r(cjr,"Examples:"),cjr.forEach(t),Mjo=i(sl),f(sy.$$.fragment,sl),sl.forEach(t),yjo=i(al),fo=s(al,"DIV",{class:!0});var Jt=n(fo);f(ny.$$.fragment,Jt),wjo=i(Jt),zne=s(Jt,"P",{});var mjr=n(zne);Ajo=r(mjr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),mjr.forEach(t),Ljo=i(Jt),Ha=s(Jt,"P",{});var K4=n(Ha);Bjo=r(K4,"The model class to instantiate is selected based on the "),Xne=s(K4,"CODE",{});var fjr=n(Xne);xjo=r(fjr,"model_type"),fjr.forEach(t),kjo=r(K4,` property of the config object (either
passed as an argument or loaded from `),Qne=s(K4,"CODE",{});var hjr=n(Qne);Rjo=r(hjr,"pretrained_model_name_or_path"),hjr.forEach(t),Pjo=r(K4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Vne=s(K4,"CODE",{});var gjr=n(Vne);Sjo=r(gjr,"pretrained_model_name_or_path"),gjr.forEach(t),$jo=r(K4,":"),K4.forEach(t),Ijo=i(Jt),te=s(Jt,"UL",{});var ne=n(te);FT=s(ne,"LI",{});var M4e=n(FT);Wne=s(M4e,"STRONG",{});var ujr=n(Wne);Djo=r(ujr,"albert"),ujr.forEach(t),Njo=r(M4e," \u2014 "),d$=s(M4e,"A",{href:!0});var pjr=n(d$);jjo=r(pjr,"TFAlbertForMaskedLM"),pjr.forEach(t),Ojo=r(M4e," (ALBERT model)"),M4e.forEach(t),Gjo=i(ne),ET=s(ne,"LI",{});var y4e=n(ET);Hne=s(y4e,"STRONG",{});var _jr=n(Hne);qjo=r(_jr,"bert"),_jr.forEach(t),zjo=r(y4e," \u2014 "),c$=s(y4e,"A",{href:!0});var bjr=n(c$);Xjo=r(bjr,"TFBertForMaskedLM"),bjr.forEach(t),Qjo=r(y4e," (BERT model)"),y4e.forEach(t),Vjo=i(ne),CT=s(ne,"LI",{});var w4e=n(CT);Une=s(w4e,"STRONG",{});var vjr=n(Une);Wjo=r(vjr,"camembert"),vjr.forEach(t),Hjo=r(w4e," \u2014 "),m$=s(w4e,"A",{href:!0});var Tjr=n(m$);Ujo=r(Tjr,"TFCamembertForMaskedLM"),Tjr.forEach(t),Jjo=r(w4e," (CamemBERT model)"),w4e.forEach(t),Kjo=i(ne),MT=s(ne,"LI",{});var A4e=n(MT);Jne=s(A4e,"STRONG",{});var Fjr=n(Jne);Yjo=r(Fjr,"convbert"),Fjr.forEach(t),Zjo=r(A4e," \u2014 "),f$=s(A4e,"A",{href:!0});var Ejr=n(f$);eOo=r(Ejr,"TFConvBertForMaskedLM"),Ejr.forEach(t),oOo=r(A4e," (ConvBERT model)"),A4e.forEach(t),rOo=i(ne),yT=s(ne,"LI",{});var L4e=n(yT);Kne=s(L4e,"STRONG",{});var Cjr=n(Kne);tOo=r(Cjr,"deberta"),Cjr.forEach(t),aOo=r(L4e," \u2014 "),h$=s(L4e,"A",{href:!0});var Mjr=n(h$);sOo=r(Mjr,"TFDebertaForMaskedLM"),Mjr.forEach(t),nOo=r(L4e," (DeBERTa model)"),L4e.forEach(t),lOo=i(ne),wT=s(ne,"LI",{});var B4e=n(wT);Yne=s(B4e,"STRONG",{});var yjr=n(Yne);iOo=r(yjr,"deberta-v2"),yjr.forEach(t),dOo=r(B4e," \u2014 "),g$=s(B4e,"A",{href:!0});var wjr=n(g$);cOo=r(wjr,"TFDebertaV2ForMaskedLM"),wjr.forEach(t),mOo=r(B4e," (DeBERTa-v2 model)"),B4e.forEach(t),fOo=i(ne),AT=s(ne,"LI",{});var x4e=n(AT);Zne=s(x4e,"STRONG",{});var Ajr=n(Zne);hOo=r(Ajr,"distilbert"),Ajr.forEach(t),gOo=r(x4e," \u2014 "),u$=s(x4e,"A",{href:!0});var Ljr=n(u$);uOo=r(Ljr,"TFDistilBertForMaskedLM"),Ljr.forEach(t),pOo=r(x4e," (DistilBERT model)"),x4e.forEach(t),_Oo=i(ne),LT=s(ne,"LI",{});var k4e=n(LT);ele=s(k4e,"STRONG",{});var Bjr=n(ele);bOo=r(Bjr,"electra"),Bjr.forEach(t),vOo=r(k4e," \u2014 "),p$=s(k4e,"A",{href:!0});var xjr=n(p$);TOo=r(xjr,"TFElectraForMaskedLM"),xjr.forEach(t),FOo=r(k4e," (ELECTRA model)"),k4e.forEach(t),EOo=i(ne),BT=s(ne,"LI",{});var R4e=n(BT);ole=s(R4e,"STRONG",{});var kjr=n(ole);COo=r(kjr,"flaubert"),kjr.forEach(t),MOo=r(R4e," \u2014 "),_$=s(R4e,"A",{href:!0});var Rjr=n(_$);yOo=r(Rjr,"TFFlaubertWithLMHeadModel"),Rjr.forEach(t),wOo=r(R4e," (FlauBERT model)"),R4e.forEach(t),AOo=i(ne),xT=s(ne,"LI",{});var P4e=n(xT);rle=s(P4e,"STRONG",{});var Pjr=n(rle);LOo=r(Pjr,"funnel"),Pjr.forEach(t),BOo=r(P4e," \u2014 "),b$=s(P4e,"A",{href:!0});var Sjr=n(b$);xOo=r(Sjr,"TFFunnelForMaskedLM"),Sjr.forEach(t),kOo=r(P4e," (Funnel Transformer model)"),P4e.forEach(t),ROo=i(ne),kT=s(ne,"LI",{});var S4e=n(kT);tle=s(S4e,"STRONG",{});var $jr=n(tle);POo=r($jr,"layoutlm"),$jr.forEach(t),SOo=r(S4e," \u2014 "),v$=s(S4e,"A",{href:!0});var Ijr=n(v$);$Oo=r(Ijr,"TFLayoutLMForMaskedLM"),Ijr.forEach(t),IOo=r(S4e," (LayoutLM model)"),S4e.forEach(t),DOo=i(ne),RT=s(ne,"LI",{});var $4e=n(RT);ale=s($4e,"STRONG",{});var Djr=n(ale);NOo=r(Djr,"longformer"),Djr.forEach(t),jOo=r($4e," \u2014 "),T$=s($4e,"A",{href:!0});var Njr=n(T$);OOo=r(Njr,"TFLongformerForMaskedLM"),Njr.forEach(t),GOo=r($4e," (Longformer model)"),$4e.forEach(t),qOo=i(ne),PT=s(ne,"LI",{});var I4e=n(PT);sle=s(I4e,"STRONG",{});var jjr=n(sle);zOo=r(jjr,"mobilebert"),jjr.forEach(t),XOo=r(I4e," \u2014 "),F$=s(I4e,"A",{href:!0});var Ojr=n(F$);QOo=r(Ojr,"TFMobileBertForMaskedLM"),Ojr.forEach(t),VOo=r(I4e," (MobileBERT model)"),I4e.forEach(t),WOo=i(ne),ST=s(ne,"LI",{});var D4e=n(ST);nle=s(D4e,"STRONG",{});var Gjr=n(nle);HOo=r(Gjr,"mpnet"),Gjr.forEach(t),UOo=r(D4e," \u2014 "),E$=s(D4e,"A",{href:!0});var qjr=n(E$);JOo=r(qjr,"TFMPNetForMaskedLM"),qjr.forEach(t),KOo=r(D4e," (MPNet model)"),D4e.forEach(t),YOo=i(ne),$T=s(ne,"LI",{});var N4e=n($T);lle=s(N4e,"STRONG",{});var zjr=n(lle);ZOo=r(zjr,"rembert"),zjr.forEach(t),eGo=r(N4e," \u2014 "),C$=s(N4e,"A",{href:!0});var Xjr=n(C$);oGo=r(Xjr,"TFRemBertForMaskedLM"),Xjr.forEach(t),rGo=r(N4e," (RemBERT model)"),N4e.forEach(t),tGo=i(ne),IT=s(ne,"LI",{});var j4e=n(IT);ile=s(j4e,"STRONG",{});var Qjr=n(ile);aGo=r(Qjr,"roberta"),Qjr.forEach(t),sGo=r(j4e," \u2014 "),M$=s(j4e,"A",{href:!0});var Vjr=n(M$);nGo=r(Vjr,"TFRobertaForMaskedLM"),Vjr.forEach(t),lGo=r(j4e," (RoBERTa model)"),j4e.forEach(t),iGo=i(ne),DT=s(ne,"LI",{});var O4e=n(DT);dle=s(O4e,"STRONG",{});var Wjr=n(dle);dGo=r(Wjr,"roformer"),Wjr.forEach(t),cGo=r(O4e," \u2014 "),y$=s(O4e,"A",{href:!0});var Hjr=n(y$);mGo=r(Hjr,"TFRoFormerForMaskedLM"),Hjr.forEach(t),fGo=r(O4e," (RoFormer model)"),O4e.forEach(t),hGo=i(ne),NT=s(ne,"LI",{});var G4e=n(NT);cle=s(G4e,"STRONG",{});var Ujr=n(cle);gGo=r(Ujr,"tapas"),Ujr.forEach(t),uGo=r(G4e," \u2014 "),w$=s(G4e,"A",{href:!0});var Jjr=n(w$);pGo=r(Jjr,"TFTapasForMaskedLM"),Jjr.forEach(t),_Go=r(G4e," (TAPAS model)"),G4e.forEach(t),bGo=i(ne),jT=s(ne,"LI",{});var q4e=n(jT);mle=s(q4e,"STRONG",{});var Kjr=n(mle);vGo=r(Kjr,"xlm"),Kjr.forEach(t),TGo=r(q4e," \u2014 "),A$=s(q4e,"A",{href:!0});var Yjr=n(A$);FGo=r(Yjr,"TFXLMWithLMHeadModel"),Yjr.forEach(t),EGo=r(q4e," (XLM model)"),q4e.forEach(t),CGo=i(ne),OT=s(ne,"LI",{});var z4e=n(OT);fle=s(z4e,"STRONG",{});var Zjr=n(fle);MGo=r(Zjr,"xlm-roberta"),Zjr.forEach(t),yGo=r(z4e," \u2014 "),L$=s(z4e,"A",{href:!0});var eOr=n(L$);wGo=r(eOr,"TFXLMRobertaForMaskedLM"),eOr.forEach(t),AGo=r(z4e," (XLM-RoBERTa model)"),z4e.forEach(t),ne.forEach(t),LGo=i(Jt),hle=s(Jt,"P",{});var oOr=n(hle);BGo=r(oOr,"Examples:"),oOr.forEach(t),xGo=i(Jt),f(ly.$$.fragment,Jt),Jt.forEach(t),al.forEach(t),dye=i(d),wd=s(d,"H2",{class:!0});var _7e=n(wd);GT=s(_7e,"A",{id:!0,class:!0,href:!0});var rOr=n(GT);gle=s(rOr,"SPAN",{});var tOr=n(gle);f(iy.$$.fragment,tOr),tOr.forEach(t),rOr.forEach(t),kGo=i(_7e),ule=s(_7e,"SPAN",{});var aOr=n(ule);RGo=r(aOr,"TFAutoModelForSeq2SeqLM"),aOr.forEach(t),_7e.forEach(t),cye=i(d),mr=s(d,"DIV",{class:!0});var nl=n(mr);f(dy.$$.fragment,nl),PGo=i(nl),Ad=s(nl,"P",{});var Vj=n(Ad);SGo=r(Vj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),ple=s(Vj,"CODE",{});var sOr=n(ple);$Go=r(sOr,"from_pretrained()"),sOr.forEach(t),IGo=r(Vj,` class method or the
`),_le=s(Vj,"CODE",{});var nOr=n(_le);DGo=r(nOr,"from_config()"),nOr.forEach(t),NGo=r(Vj," class method."),Vj.forEach(t),jGo=i(nl),cy=s(nl,"P",{});var b7e=n(cy);OGo=r(b7e,"This class cannot be instantiated directly using "),ble=s(b7e,"CODE",{});var lOr=n(ble);GGo=r(lOr,"__init__()"),lOr.forEach(t),qGo=r(b7e," (throws an error)."),b7e.forEach(t),zGo=i(nl),et=s(nl,"DIV",{class:!0});var ll=n(et);f(my.$$.fragment,ll),XGo=i(ll),vle=s(ll,"P",{});var iOr=n(vle);QGo=r(iOr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),iOr.forEach(t),VGo=i(ll),Ld=s(ll,"P",{});var Wj=n(Ld);WGo=r(Wj,`Note:
Loading a model from its configuration file does `),Tle=s(Wj,"STRONG",{});var dOr=n(Tle);HGo=r(dOr,"not"),dOr.forEach(t),UGo=r(Wj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Fle=s(Wj,"CODE",{});var cOr=n(Fle);JGo=r(cOr,"from_pretrained()"),cOr.forEach(t),KGo=r(Wj,` to load the model
weights.`),Wj.forEach(t),YGo=i(ll),Ele=s(ll,"P",{});var mOr=n(Ele);ZGo=r(mOr,"Examples:"),mOr.forEach(t),eqo=i(ll),f(fy.$$.fragment,ll),ll.forEach(t),oqo=i(nl),ho=s(nl,"DIV",{class:!0});var Kt=n(ho);f(hy.$$.fragment,Kt),rqo=i(Kt),Cle=s(Kt,"P",{});var fOr=n(Cle);tqo=r(fOr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),fOr.forEach(t),aqo=i(Kt),Ua=s(Kt,"P",{});var Y4=n(Ua);sqo=r(Y4,"The model class to instantiate is selected based on the "),Mle=s(Y4,"CODE",{});var hOr=n(Mle);nqo=r(hOr,"model_type"),hOr.forEach(t),lqo=r(Y4,` property of the config object (either
passed as an argument or loaded from `),yle=s(Y4,"CODE",{});var gOr=n(yle);iqo=r(gOr,"pretrained_model_name_or_path"),gOr.forEach(t),dqo=r(Y4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),wle=s(Y4,"CODE",{});var uOr=n(wle);cqo=r(uOr,"pretrained_model_name_or_path"),uOr.forEach(t),mqo=r(Y4,":"),Y4.forEach(t),fqo=i(Kt),be=s(Kt,"UL",{});var Ae=n(be);qT=s(Ae,"LI",{});var X4e=n(qT);Ale=s(X4e,"STRONG",{});var pOr=n(Ale);hqo=r(pOr,"bart"),pOr.forEach(t),gqo=r(X4e," \u2014 "),B$=s(X4e,"A",{href:!0});var _Or=n(B$);uqo=r(_Or,"TFBartForConditionalGeneration"),_Or.forEach(t),pqo=r(X4e," (BART model)"),X4e.forEach(t),_qo=i(Ae),zT=s(Ae,"LI",{});var Q4e=n(zT);Lle=s(Q4e,"STRONG",{});var bOr=n(Lle);bqo=r(bOr,"blenderbot"),bOr.forEach(t),vqo=r(Q4e," \u2014 "),x$=s(Q4e,"A",{href:!0});var vOr=n(x$);Tqo=r(vOr,"TFBlenderbotForConditionalGeneration"),vOr.forEach(t),Fqo=r(Q4e," (Blenderbot model)"),Q4e.forEach(t),Eqo=i(Ae),XT=s(Ae,"LI",{});var V4e=n(XT);Ble=s(V4e,"STRONG",{});var TOr=n(Ble);Cqo=r(TOr,"blenderbot-small"),TOr.forEach(t),Mqo=r(V4e," \u2014 "),k$=s(V4e,"A",{href:!0});var FOr=n(k$);yqo=r(FOr,"TFBlenderbotSmallForConditionalGeneration"),FOr.forEach(t),wqo=r(V4e," (BlenderbotSmall model)"),V4e.forEach(t),Aqo=i(Ae),QT=s(Ae,"LI",{});var W4e=n(QT);xle=s(W4e,"STRONG",{});var EOr=n(xle);Lqo=r(EOr,"encoder-decoder"),EOr.forEach(t),Bqo=r(W4e," \u2014 "),R$=s(W4e,"A",{href:!0});var COr=n(R$);xqo=r(COr,"TFEncoderDecoderModel"),COr.forEach(t),kqo=r(W4e," (Encoder decoder model)"),W4e.forEach(t),Rqo=i(Ae),VT=s(Ae,"LI",{});var H4e=n(VT);kle=s(H4e,"STRONG",{});var MOr=n(kle);Pqo=r(MOr,"led"),MOr.forEach(t),Sqo=r(H4e," \u2014 "),P$=s(H4e,"A",{href:!0});var yOr=n(P$);$qo=r(yOr,"TFLEDForConditionalGeneration"),yOr.forEach(t),Iqo=r(H4e," (LED model)"),H4e.forEach(t),Dqo=i(Ae),WT=s(Ae,"LI",{});var U4e=n(WT);Rle=s(U4e,"STRONG",{});var wOr=n(Rle);Nqo=r(wOr,"marian"),wOr.forEach(t),jqo=r(U4e," \u2014 "),S$=s(U4e,"A",{href:!0});var AOr=n(S$);Oqo=r(AOr,"TFMarianMTModel"),AOr.forEach(t),Gqo=r(U4e," (Marian model)"),U4e.forEach(t),qqo=i(Ae),HT=s(Ae,"LI",{});var J4e=n(HT);Ple=s(J4e,"STRONG",{});var LOr=n(Ple);zqo=r(LOr,"mbart"),LOr.forEach(t),Xqo=r(J4e," \u2014 "),$$=s(J4e,"A",{href:!0});var BOr=n($$);Qqo=r(BOr,"TFMBartForConditionalGeneration"),BOr.forEach(t),Vqo=r(J4e," (mBART model)"),J4e.forEach(t),Wqo=i(Ae),UT=s(Ae,"LI",{});var K4e=n(UT);Sle=s(K4e,"STRONG",{});var xOr=n(Sle);Hqo=r(xOr,"mt5"),xOr.forEach(t),Uqo=r(K4e," \u2014 "),I$=s(K4e,"A",{href:!0});var kOr=n(I$);Jqo=r(kOr,"TFMT5ForConditionalGeneration"),kOr.forEach(t),Kqo=r(K4e," (mT5 model)"),K4e.forEach(t),Yqo=i(Ae),JT=s(Ae,"LI",{});var Y4e=n(JT);$le=s(Y4e,"STRONG",{});var ROr=n($le);Zqo=r(ROr,"pegasus"),ROr.forEach(t),ezo=r(Y4e," \u2014 "),D$=s(Y4e,"A",{href:!0});var POr=n(D$);ozo=r(POr,"TFPegasusForConditionalGeneration"),POr.forEach(t),rzo=r(Y4e," (Pegasus model)"),Y4e.forEach(t),tzo=i(Ae),KT=s(Ae,"LI",{});var Z4e=n(KT);Ile=s(Z4e,"STRONG",{});var SOr=n(Ile);azo=r(SOr,"t5"),SOr.forEach(t),szo=r(Z4e," \u2014 "),N$=s(Z4e,"A",{href:!0});var $Or=n(N$);nzo=r($Or,"TFT5ForConditionalGeneration"),$Or.forEach(t),lzo=r(Z4e," (T5 model)"),Z4e.forEach(t),Ae.forEach(t),izo=i(Kt),Dle=s(Kt,"P",{});var IOr=n(Dle);dzo=r(IOr,"Examples:"),IOr.forEach(t),czo=i(Kt),f(gy.$$.fragment,Kt),Kt.forEach(t),nl.forEach(t),mye=i(d),Bd=s(d,"H2",{class:!0});var v7e=n(Bd);YT=s(v7e,"A",{id:!0,class:!0,href:!0});var DOr=n(YT);Nle=s(DOr,"SPAN",{});var NOr=n(Nle);f(uy.$$.fragment,NOr),NOr.forEach(t),DOr.forEach(t),mzo=i(v7e),jle=s(v7e,"SPAN",{});var jOr=n(jle);fzo=r(jOr,"TFAutoModelForSequenceClassification"),jOr.forEach(t),v7e.forEach(t),fye=i(d),fr=s(d,"DIV",{class:!0});var il=n(fr);f(py.$$.fragment,il),hzo=i(il),xd=s(il,"P",{});var Hj=n(xd);gzo=r(Hj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Ole=s(Hj,"CODE",{});var OOr=n(Ole);uzo=r(OOr,"from_pretrained()"),OOr.forEach(t),pzo=r(Hj,` class method or the
`),Gle=s(Hj,"CODE",{});var GOr=n(Gle);_zo=r(GOr,"from_config()"),GOr.forEach(t),bzo=r(Hj," class method."),Hj.forEach(t),vzo=i(il),_y=s(il,"P",{});var T7e=n(_y);Tzo=r(T7e,"This class cannot be instantiated directly using "),qle=s(T7e,"CODE",{});var qOr=n(qle);Fzo=r(qOr,"__init__()"),qOr.forEach(t),Ezo=r(T7e," (throws an error)."),T7e.forEach(t),Czo=i(il),ot=s(il,"DIV",{class:!0});var dl=n(ot);f(by.$$.fragment,dl),Mzo=i(dl),zle=s(dl,"P",{});var zOr=n(zle);yzo=r(zOr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),zOr.forEach(t),wzo=i(dl),kd=s(dl,"P",{});var Uj=n(kd);Azo=r(Uj,`Note:
Loading a model from its configuration file does `),Xle=s(Uj,"STRONG",{});var XOr=n(Xle);Lzo=r(XOr,"not"),XOr.forEach(t),Bzo=r(Uj,` load the model weights. It only affects the
model\u2019s configuration. Use `),Qle=s(Uj,"CODE",{});var QOr=n(Qle);xzo=r(QOr,"from_pretrained()"),QOr.forEach(t),kzo=r(Uj,` to load the model
weights.`),Uj.forEach(t),Rzo=i(dl),Vle=s(dl,"P",{});var VOr=n(Vle);Pzo=r(VOr,"Examples:"),VOr.forEach(t),Szo=i(dl),f(vy.$$.fragment,dl),dl.forEach(t),$zo=i(il),go=s(il,"DIV",{class:!0});var Yt=n(go);f(Ty.$$.fragment,Yt),Izo=i(Yt),Wle=s(Yt,"P",{});var WOr=n(Wle);Dzo=r(WOr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),WOr.forEach(t),Nzo=i(Yt),Ja=s(Yt,"P",{});var Z4=n(Ja);jzo=r(Z4,"The model class to instantiate is selected based on the "),Hle=s(Z4,"CODE",{});var HOr=n(Hle);Ozo=r(HOr,"model_type"),HOr.forEach(t),Gzo=r(Z4,` property of the config object (either
passed as an argument or loaded from `),Ule=s(Z4,"CODE",{});var UOr=n(Ule);qzo=r(UOr,"pretrained_model_name_or_path"),UOr.forEach(t),zzo=r(Z4,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Jle=s(Z4,"CODE",{});var JOr=n(Jle);Xzo=r(JOr,"pretrained_model_name_or_path"),JOr.forEach(t),Qzo=r(Z4,":"),Z4.forEach(t),Vzo=i(Yt),W=s(Yt,"UL",{});var U=n(W);ZT=s(U,"LI",{});var eCe=n(ZT);Kle=s(eCe,"STRONG",{});var KOr=n(Kle);Wzo=r(KOr,"albert"),KOr.forEach(t),Hzo=r(eCe," \u2014 "),j$=s(eCe,"A",{href:!0});var YOr=n(j$);Uzo=r(YOr,"TFAlbertForSequenceClassification"),YOr.forEach(t),Jzo=r(eCe," (ALBERT model)"),eCe.forEach(t),Kzo=i(U),e1=s(U,"LI",{});var oCe=n(e1);Yle=s(oCe,"STRONG",{});var ZOr=n(Yle);Yzo=r(ZOr,"bert"),ZOr.forEach(t),Zzo=r(oCe," \u2014 "),O$=s(oCe,"A",{href:!0});var eGr=n(O$);eXo=r(eGr,"TFBertForSequenceClassification"),eGr.forEach(t),oXo=r(oCe," (BERT model)"),oCe.forEach(t),rXo=i(U),o1=s(U,"LI",{});var rCe=n(o1);Zle=s(rCe,"STRONG",{});var oGr=n(Zle);tXo=r(oGr,"camembert"),oGr.forEach(t),aXo=r(rCe," \u2014 "),G$=s(rCe,"A",{href:!0});var rGr=n(G$);sXo=r(rGr,"TFCamembertForSequenceClassification"),rGr.forEach(t),nXo=r(rCe," (CamemBERT model)"),rCe.forEach(t),lXo=i(U),r1=s(U,"LI",{});var tCe=n(r1);eie=s(tCe,"STRONG",{});var tGr=n(eie);iXo=r(tGr,"convbert"),tGr.forEach(t),dXo=r(tCe," \u2014 "),q$=s(tCe,"A",{href:!0});var aGr=n(q$);cXo=r(aGr,"TFConvBertForSequenceClassification"),aGr.forEach(t),mXo=r(tCe," (ConvBERT model)"),tCe.forEach(t),fXo=i(U),t1=s(U,"LI",{});var aCe=n(t1);oie=s(aCe,"STRONG",{});var sGr=n(oie);hXo=r(sGr,"ctrl"),sGr.forEach(t),gXo=r(aCe," \u2014 "),z$=s(aCe,"A",{href:!0});var nGr=n(z$);uXo=r(nGr,"TFCTRLForSequenceClassification"),nGr.forEach(t),pXo=r(aCe," (CTRL model)"),aCe.forEach(t),_Xo=i(U),a1=s(U,"LI",{});var sCe=n(a1);rie=s(sCe,"STRONG",{});var lGr=n(rie);bXo=r(lGr,"deberta"),lGr.forEach(t),vXo=r(sCe," \u2014 "),X$=s(sCe,"A",{href:!0});var iGr=n(X$);TXo=r(iGr,"TFDebertaForSequenceClassification"),iGr.forEach(t),FXo=r(sCe," (DeBERTa model)"),sCe.forEach(t),EXo=i(U),s1=s(U,"LI",{});var nCe=n(s1);tie=s(nCe,"STRONG",{});var dGr=n(tie);CXo=r(dGr,"deberta-v2"),dGr.forEach(t),MXo=r(nCe," \u2014 "),Q$=s(nCe,"A",{href:!0});var cGr=n(Q$);yXo=r(cGr,"TFDebertaV2ForSequenceClassification"),cGr.forEach(t),wXo=r(nCe," (DeBERTa-v2 model)"),nCe.forEach(t),AXo=i(U),n1=s(U,"LI",{});var lCe=n(n1);aie=s(lCe,"STRONG",{});var mGr=n(aie);LXo=r(mGr,"distilbert"),mGr.forEach(t),BXo=r(lCe," \u2014 "),V$=s(lCe,"A",{href:!0});var fGr=n(V$);xXo=r(fGr,"TFDistilBertForSequenceClassification"),fGr.forEach(t),kXo=r(lCe," (DistilBERT model)"),lCe.forEach(t),RXo=i(U),l1=s(U,"LI",{});var iCe=n(l1);sie=s(iCe,"STRONG",{});var hGr=n(sie);PXo=r(hGr,"electra"),hGr.forEach(t),SXo=r(iCe," \u2014 "),W$=s(iCe,"A",{href:!0});var gGr=n(W$);$Xo=r(gGr,"TFElectraForSequenceClassification"),gGr.forEach(t),IXo=r(iCe," (ELECTRA model)"),iCe.forEach(t),DXo=i(U),i1=s(U,"LI",{});var dCe=n(i1);nie=s(dCe,"STRONG",{});var uGr=n(nie);NXo=r(uGr,"flaubert"),uGr.forEach(t),jXo=r(dCe," \u2014 "),H$=s(dCe,"A",{href:!0});var pGr=n(H$);OXo=r(pGr,"TFFlaubertForSequenceClassification"),pGr.forEach(t),GXo=r(dCe," (FlauBERT model)"),dCe.forEach(t),qXo=i(U),d1=s(U,"LI",{});var cCe=n(d1);lie=s(cCe,"STRONG",{});var _Gr=n(lie);zXo=r(_Gr,"funnel"),_Gr.forEach(t),XXo=r(cCe," \u2014 "),U$=s(cCe,"A",{href:!0});var bGr=n(U$);QXo=r(bGr,"TFFunnelForSequenceClassification"),bGr.forEach(t),VXo=r(cCe," (Funnel Transformer model)"),cCe.forEach(t),WXo=i(U),c1=s(U,"LI",{});var mCe=n(c1);iie=s(mCe,"STRONG",{});var vGr=n(iie);HXo=r(vGr,"gpt2"),vGr.forEach(t),UXo=r(mCe," \u2014 "),J$=s(mCe,"A",{href:!0});var TGr=n(J$);JXo=r(TGr,"TFGPT2ForSequenceClassification"),TGr.forEach(t),KXo=r(mCe," (OpenAI GPT-2 model)"),mCe.forEach(t),YXo=i(U),m1=s(U,"LI",{});var fCe=n(m1);die=s(fCe,"STRONG",{});var FGr=n(die);ZXo=r(FGr,"layoutlm"),FGr.forEach(t),eQo=r(fCe," \u2014 "),K$=s(fCe,"A",{href:!0});var EGr=n(K$);oQo=r(EGr,"TFLayoutLMForSequenceClassification"),EGr.forEach(t),rQo=r(fCe," (LayoutLM model)"),fCe.forEach(t),tQo=i(U),f1=s(U,"LI",{});var hCe=n(f1);cie=s(hCe,"STRONG",{});var CGr=n(cie);aQo=r(CGr,"longformer"),CGr.forEach(t),sQo=r(hCe," \u2014 "),Y$=s(hCe,"A",{href:!0});var MGr=n(Y$);nQo=r(MGr,"TFLongformerForSequenceClassification"),MGr.forEach(t),lQo=r(hCe," (Longformer model)"),hCe.forEach(t),iQo=i(U),h1=s(U,"LI",{});var gCe=n(h1);mie=s(gCe,"STRONG",{});var yGr=n(mie);dQo=r(yGr,"mobilebert"),yGr.forEach(t),cQo=r(gCe," \u2014 "),Z$=s(gCe,"A",{href:!0});var wGr=n(Z$);mQo=r(wGr,"TFMobileBertForSequenceClassification"),wGr.forEach(t),fQo=r(gCe," (MobileBERT model)"),gCe.forEach(t),hQo=i(U),g1=s(U,"LI",{});var uCe=n(g1);fie=s(uCe,"STRONG",{});var AGr=n(fie);gQo=r(AGr,"mpnet"),AGr.forEach(t),uQo=r(uCe," \u2014 "),eI=s(uCe,"A",{href:!0});var LGr=n(eI);pQo=r(LGr,"TFMPNetForSequenceClassification"),LGr.forEach(t),_Qo=r(uCe," (MPNet model)"),uCe.forEach(t),bQo=i(U),u1=s(U,"LI",{});var pCe=n(u1);hie=s(pCe,"STRONG",{});var BGr=n(hie);vQo=r(BGr,"openai-gpt"),BGr.forEach(t),TQo=r(pCe," \u2014 "),oI=s(pCe,"A",{href:!0});var xGr=n(oI);FQo=r(xGr,"TFOpenAIGPTForSequenceClassification"),xGr.forEach(t),EQo=r(pCe," (OpenAI GPT model)"),pCe.forEach(t),CQo=i(U),p1=s(U,"LI",{});var _Ce=n(p1);gie=s(_Ce,"STRONG",{});var kGr=n(gie);MQo=r(kGr,"rembert"),kGr.forEach(t),yQo=r(_Ce," \u2014 "),rI=s(_Ce,"A",{href:!0});var RGr=n(rI);wQo=r(RGr,"TFRemBertForSequenceClassification"),RGr.forEach(t),AQo=r(_Ce," (RemBERT model)"),_Ce.forEach(t),LQo=i(U),_1=s(U,"LI",{});var bCe=n(_1);uie=s(bCe,"STRONG",{});var PGr=n(uie);BQo=r(PGr,"roberta"),PGr.forEach(t),xQo=r(bCe," \u2014 "),tI=s(bCe,"A",{href:!0});var SGr=n(tI);kQo=r(SGr,"TFRobertaForSequenceClassification"),SGr.forEach(t),RQo=r(bCe," (RoBERTa model)"),bCe.forEach(t),PQo=i(U),b1=s(U,"LI",{});var vCe=n(b1);pie=s(vCe,"STRONG",{});var $Gr=n(pie);SQo=r($Gr,"roformer"),$Gr.forEach(t),$Qo=r(vCe," \u2014 "),aI=s(vCe,"A",{href:!0});var IGr=n(aI);IQo=r(IGr,"TFRoFormerForSequenceClassification"),IGr.forEach(t),DQo=r(vCe," (RoFormer model)"),vCe.forEach(t),NQo=i(U),v1=s(U,"LI",{});var TCe=n(v1);_ie=s(TCe,"STRONG",{});var DGr=n(_ie);jQo=r(DGr,"tapas"),DGr.forEach(t),OQo=r(TCe," \u2014 "),sI=s(TCe,"A",{href:!0});var NGr=n(sI);GQo=r(NGr,"TFTapasForSequenceClassification"),NGr.forEach(t),qQo=r(TCe," (TAPAS model)"),TCe.forEach(t),zQo=i(U),T1=s(U,"LI",{});var FCe=n(T1);bie=s(FCe,"STRONG",{});var jGr=n(bie);XQo=r(jGr,"transfo-xl"),jGr.forEach(t),QQo=r(FCe," \u2014 "),nI=s(FCe,"A",{href:!0});var OGr=n(nI);VQo=r(OGr,"TFTransfoXLForSequenceClassification"),OGr.forEach(t),WQo=r(FCe," (Transformer-XL model)"),FCe.forEach(t),HQo=i(U),F1=s(U,"LI",{});var ECe=n(F1);vie=s(ECe,"STRONG",{});var GGr=n(vie);UQo=r(GGr,"xlm"),GGr.forEach(t),JQo=r(ECe," \u2014 "),lI=s(ECe,"A",{href:!0});var qGr=n(lI);KQo=r(qGr,"TFXLMForSequenceClassification"),qGr.forEach(t),YQo=r(ECe," (XLM model)"),ECe.forEach(t),ZQo=i(U),E1=s(U,"LI",{});var CCe=n(E1);Tie=s(CCe,"STRONG",{});var zGr=n(Tie);eVo=r(zGr,"xlm-roberta"),zGr.forEach(t),oVo=r(CCe," \u2014 "),iI=s(CCe,"A",{href:!0});var XGr=n(iI);rVo=r(XGr,"TFXLMRobertaForSequenceClassification"),XGr.forEach(t),tVo=r(CCe," (XLM-RoBERTa model)"),CCe.forEach(t),aVo=i(U),C1=s(U,"LI",{});var MCe=n(C1);Fie=s(MCe,"STRONG",{});var QGr=n(Fie);sVo=r(QGr,"xlnet"),QGr.forEach(t),nVo=r(MCe," \u2014 "),dI=s(MCe,"A",{href:!0});var VGr=n(dI);lVo=r(VGr,"TFXLNetForSequenceClassification"),VGr.forEach(t),iVo=r(MCe," (XLNet model)"),MCe.forEach(t),U.forEach(t),dVo=i(Yt),Eie=s(Yt,"P",{});var WGr=n(Eie);cVo=r(WGr,"Examples:"),WGr.forEach(t),mVo=i(Yt),f(Fy.$$.fragment,Yt),Yt.forEach(t),il.forEach(t),hye=i(d),Rd=s(d,"H2",{class:!0});var F7e=n(Rd);M1=s(F7e,"A",{id:!0,class:!0,href:!0});var HGr=n(M1);Cie=s(HGr,"SPAN",{});var UGr=n(Cie);f(Ey.$$.fragment,UGr),UGr.forEach(t),HGr.forEach(t),fVo=i(F7e),Mie=s(F7e,"SPAN",{});var JGr=n(Mie);hVo=r(JGr,"TFAutoModelForMultipleChoice"),JGr.forEach(t),F7e.forEach(t),gye=i(d),hr=s(d,"DIV",{class:!0});var cl=n(hr);f(Cy.$$.fragment,cl),gVo=i(cl),Pd=s(cl,"P",{});var Jj=n(Pd);uVo=r(Jj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),yie=s(Jj,"CODE",{});var KGr=n(yie);pVo=r(KGr,"from_pretrained()"),KGr.forEach(t),_Vo=r(Jj,` class method or the
`),wie=s(Jj,"CODE",{});var YGr=n(wie);bVo=r(YGr,"from_config()"),YGr.forEach(t),vVo=r(Jj," class method."),Jj.forEach(t),TVo=i(cl),My=s(cl,"P",{});var E7e=n(My);FVo=r(E7e,"This class cannot be instantiated directly using "),Aie=s(E7e,"CODE",{});var ZGr=n(Aie);EVo=r(ZGr,"__init__()"),ZGr.forEach(t),CVo=r(E7e," (throws an error)."),E7e.forEach(t),MVo=i(cl),rt=s(cl,"DIV",{class:!0});var ml=n(rt);f(yy.$$.fragment,ml),yVo=i(ml),Lie=s(ml,"P",{});var eqr=n(Lie);wVo=r(eqr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),eqr.forEach(t),AVo=i(ml),Sd=s(ml,"P",{});var Kj=n(Sd);LVo=r(Kj,`Note:
Loading a model from its configuration file does `),Bie=s(Kj,"STRONG",{});var oqr=n(Bie);BVo=r(oqr,"not"),oqr.forEach(t),xVo=r(Kj,` load the model weights. It only affects the
model\u2019s configuration. Use `),xie=s(Kj,"CODE",{});var rqr=n(xie);kVo=r(rqr,"from_pretrained()"),rqr.forEach(t),RVo=r(Kj,` to load the model
weights.`),Kj.forEach(t),PVo=i(ml),kie=s(ml,"P",{});var tqr=n(kie);SVo=r(tqr,"Examples:"),tqr.forEach(t),$Vo=i(ml),f(wy.$$.fragment,ml),ml.forEach(t),IVo=i(cl),uo=s(cl,"DIV",{class:!0});var Zt=n(uo);f(Ay.$$.fragment,Zt),DVo=i(Zt),Rie=s(Zt,"P",{});var aqr=n(Rie);NVo=r(aqr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),aqr.forEach(t),jVo=i(Zt),Ka=s(Zt,"P",{});var eC=n(Ka);OVo=r(eC,"The model class to instantiate is selected based on the "),Pie=s(eC,"CODE",{});var sqr=n(Pie);GVo=r(sqr,"model_type"),sqr.forEach(t),qVo=r(eC,` property of the config object (either
passed as an argument or loaded from `),Sie=s(eC,"CODE",{});var nqr=n(Sie);zVo=r(nqr,"pretrained_model_name_or_path"),nqr.forEach(t),XVo=r(eC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),$ie=s(eC,"CODE",{});var lqr=n($ie);QVo=r(lqr,"pretrained_model_name_or_path"),lqr.forEach(t),VVo=r(eC,":"),eC.forEach(t),WVo=i(Zt),de=s(Zt,"UL",{});var ce=n(de);y1=s(ce,"LI",{});var yCe=n(y1);Iie=s(yCe,"STRONG",{});var iqr=n(Iie);HVo=r(iqr,"albert"),iqr.forEach(t),UVo=r(yCe," \u2014 "),cI=s(yCe,"A",{href:!0});var dqr=n(cI);JVo=r(dqr,"TFAlbertForMultipleChoice"),dqr.forEach(t),KVo=r(yCe," (ALBERT model)"),yCe.forEach(t),YVo=i(ce),w1=s(ce,"LI",{});var wCe=n(w1);Die=s(wCe,"STRONG",{});var cqr=n(Die);ZVo=r(cqr,"bert"),cqr.forEach(t),eWo=r(wCe," \u2014 "),mI=s(wCe,"A",{href:!0});var mqr=n(mI);oWo=r(mqr,"TFBertForMultipleChoice"),mqr.forEach(t),rWo=r(wCe," (BERT model)"),wCe.forEach(t),tWo=i(ce),A1=s(ce,"LI",{});var ACe=n(A1);Nie=s(ACe,"STRONG",{});var fqr=n(Nie);aWo=r(fqr,"camembert"),fqr.forEach(t),sWo=r(ACe," \u2014 "),fI=s(ACe,"A",{href:!0});var hqr=n(fI);nWo=r(hqr,"TFCamembertForMultipleChoice"),hqr.forEach(t),lWo=r(ACe," (CamemBERT model)"),ACe.forEach(t),iWo=i(ce),L1=s(ce,"LI",{});var LCe=n(L1);jie=s(LCe,"STRONG",{});var gqr=n(jie);dWo=r(gqr,"convbert"),gqr.forEach(t),cWo=r(LCe," \u2014 "),hI=s(LCe,"A",{href:!0});var uqr=n(hI);mWo=r(uqr,"TFConvBertForMultipleChoice"),uqr.forEach(t),fWo=r(LCe," (ConvBERT model)"),LCe.forEach(t),hWo=i(ce),B1=s(ce,"LI",{});var BCe=n(B1);Oie=s(BCe,"STRONG",{});var pqr=n(Oie);gWo=r(pqr,"distilbert"),pqr.forEach(t),uWo=r(BCe," \u2014 "),gI=s(BCe,"A",{href:!0});var _qr=n(gI);pWo=r(_qr,"TFDistilBertForMultipleChoice"),_qr.forEach(t),_Wo=r(BCe," (DistilBERT model)"),BCe.forEach(t),bWo=i(ce),x1=s(ce,"LI",{});var xCe=n(x1);Gie=s(xCe,"STRONG",{});var bqr=n(Gie);vWo=r(bqr,"electra"),bqr.forEach(t),TWo=r(xCe," \u2014 "),uI=s(xCe,"A",{href:!0});var vqr=n(uI);FWo=r(vqr,"TFElectraForMultipleChoice"),vqr.forEach(t),EWo=r(xCe," (ELECTRA model)"),xCe.forEach(t),CWo=i(ce),k1=s(ce,"LI",{});var kCe=n(k1);qie=s(kCe,"STRONG",{});var Tqr=n(qie);MWo=r(Tqr,"flaubert"),Tqr.forEach(t),yWo=r(kCe," \u2014 "),pI=s(kCe,"A",{href:!0});var Fqr=n(pI);wWo=r(Fqr,"TFFlaubertForMultipleChoice"),Fqr.forEach(t),AWo=r(kCe," (FlauBERT model)"),kCe.forEach(t),LWo=i(ce),R1=s(ce,"LI",{});var RCe=n(R1);zie=s(RCe,"STRONG",{});var Eqr=n(zie);BWo=r(Eqr,"funnel"),Eqr.forEach(t),xWo=r(RCe," \u2014 "),_I=s(RCe,"A",{href:!0});var Cqr=n(_I);kWo=r(Cqr,"TFFunnelForMultipleChoice"),Cqr.forEach(t),RWo=r(RCe," (Funnel Transformer model)"),RCe.forEach(t),PWo=i(ce),P1=s(ce,"LI",{});var PCe=n(P1);Xie=s(PCe,"STRONG",{});var Mqr=n(Xie);SWo=r(Mqr,"longformer"),Mqr.forEach(t),$Wo=r(PCe," \u2014 "),bI=s(PCe,"A",{href:!0});var yqr=n(bI);IWo=r(yqr,"TFLongformerForMultipleChoice"),yqr.forEach(t),DWo=r(PCe," (Longformer model)"),PCe.forEach(t),NWo=i(ce),S1=s(ce,"LI",{});var SCe=n(S1);Qie=s(SCe,"STRONG",{});var wqr=n(Qie);jWo=r(wqr,"mobilebert"),wqr.forEach(t),OWo=r(SCe," \u2014 "),vI=s(SCe,"A",{href:!0});var Aqr=n(vI);GWo=r(Aqr,"TFMobileBertForMultipleChoice"),Aqr.forEach(t),qWo=r(SCe," (MobileBERT model)"),SCe.forEach(t),zWo=i(ce),$1=s(ce,"LI",{});var $Ce=n($1);Vie=s($Ce,"STRONG",{});var Lqr=n(Vie);XWo=r(Lqr,"mpnet"),Lqr.forEach(t),QWo=r($Ce," \u2014 "),TI=s($Ce,"A",{href:!0});var Bqr=n(TI);VWo=r(Bqr,"TFMPNetForMultipleChoice"),Bqr.forEach(t),WWo=r($Ce," (MPNet model)"),$Ce.forEach(t),HWo=i(ce),I1=s(ce,"LI",{});var ICe=n(I1);Wie=s(ICe,"STRONG",{});var xqr=n(Wie);UWo=r(xqr,"rembert"),xqr.forEach(t),JWo=r(ICe," \u2014 "),FI=s(ICe,"A",{href:!0});var kqr=n(FI);KWo=r(kqr,"TFRemBertForMultipleChoice"),kqr.forEach(t),YWo=r(ICe," (RemBERT model)"),ICe.forEach(t),ZWo=i(ce),D1=s(ce,"LI",{});var DCe=n(D1);Hie=s(DCe,"STRONG",{});var Rqr=n(Hie);eHo=r(Rqr,"roberta"),Rqr.forEach(t),oHo=r(DCe," \u2014 "),EI=s(DCe,"A",{href:!0});var Pqr=n(EI);rHo=r(Pqr,"TFRobertaForMultipleChoice"),Pqr.forEach(t),tHo=r(DCe," (RoBERTa model)"),DCe.forEach(t),aHo=i(ce),N1=s(ce,"LI",{});var NCe=n(N1);Uie=s(NCe,"STRONG",{});var Sqr=n(Uie);sHo=r(Sqr,"roformer"),Sqr.forEach(t),nHo=r(NCe," \u2014 "),CI=s(NCe,"A",{href:!0});var $qr=n(CI);lHo=r($qr,"TFRoFormerForMultipleChoice"),$qr.forEach(t),iHo=r(NCe," (RoFormer model)"),NCe.forEach(t),dHo=i(ce),j1=s(ce,"LI",{});var jCe=n(j1);Jie=s(jCe,"STRONG",{});var Iqr=n(Jie);cHo=r(Iqr,"xlm"),Iqr.forEach(t),mHo=r(jCe," \u2014 "),MI=s(jCe,"A",{href:!0});var Dqr=n(MI);fHo=r(Dqr,"TFXLMForMultipleChoice"),Dqr.forEach(t),hHo=r(jCe," (XLM model)"),jCe.forEach(t),gHo=i(ce),O1=s(ce,"LI",{});var OCe=n(O1);Kie=s(OCe,"STRONG",{});var Nqr=n(Kie);uHo=r(Nqr,"xlm-roberta"),Nqr.forEach(t),pHo=r(OCe," \u2014 "),yI=s(OCe,"A",{href:!0});var jqr=n(yI);_Ho=r(jqr,"TFXLMRobertaForMultipleChoice"),jqr.forEach(t),bHo=r(OCe," (XLM-RoBERTa model)"),OCe.forEach(t),vHo=i(ce),G1=s(ce,"LI",{});var GCe=n(G1);Yie=s(GCe,"STRONG",{});var Oqr=n(Yie);THo=r(Oqr,"xlnet"),Oqr.forEach(t),FHo=r(GCe," \u2014 "),wI=s(GCe,"A",{href:!0});var Gqr=n(wI);EHo=r(Gqr,"TFXLNetForMultipleChoice"),Gqr.forEach(t),CHo=r(GCe," (XLNet model)"),GCe.forEach(t),ce.forEach(t),MHo=i(Zt),Zie=s(Zt,"P",{});var qqr=n(Zie);yHo=r(qqr,"Examples:"),qqr.forEach(t),wHo=i(Zt),f(Ly.$$.fragment,Zt),Zt.forEach(t),cl.forEach(t),uye=i(d),$d=s(d,"H2",{class:!0});var C7e=n($d);q1=s(C7e,"A",{id:!0,class:!0,href:!0});var zqr=n(q1);ede=s(zqr,"SPAN",{});var Xqr=n(ede);f(By.$$.fragment,Xqr),Xqr.forEach(t),zqr.forEach(t),AHo=i(C7e),ode=s(C7e,"SPAN",{});var Qqr=n(ode);LHo=r(Qqr,"TFAutoModelForTableQuestionAnswering"),Qqr.forEach(t),C7e.forEach(t),pye=i(d),gr=s(d,"DIV",{class:!0});var fl=n(gr);f(xy.$$.fragment,fl),BHo=i(fl),Id=s(fl,"P",{});var Yj=n(Id);xHo=r(Yj,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),rde=s(Yj,"CODE",{});var Vqr=n(rde);kHo=r(Vqr,"from_pretrained()"),Vqr.forEach(t),RHo=r(Yj,` class method or the
`),tde=s(Yj,"CODE",{});var Wqr=n(tde);PHo=r(Wqr,"from_config()"),Wqr.forEach(t),SHo=r(Yj," class method."),Yj.forEach(t),$Ho=i(fl),ky=s(fl,"P",{});var M7e=n(ky);IHo=r(M7e,"This class cannot be instantiated directly using "),ade=s(M7e,"CODE",{});var Hqr=n(ade);DHo=r(Hqr,"__init__()"),Hqr.forEach(t),NHo=r(M7e," (throws an error)."),M7e.forEach(t),jHo=i(fl),tt=s(fl,"DIV",{class:!0});var hl=n(tt);f(Ry.$$.fragment,hl),OHo=i(hl),sde=s(hl,"P",{});var Uqr=n(sde);GHo=r(Uqr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Uqr.forEach(t),qHo=i(hl),Dd=s(hl,"P",{});var Zj=n(Dd);zHo=r(Zj,`Note:
Loading a model from its configuration file does `),nde=s(Zj,"STRONG",{});var Jqr=n(nde);XHo=r(Jqr,"not"),Jqr.forEach(t),QHo=r(Zj,` load the model weights. It only affects the
model\u2019s configuration. Use `),lde=s(Zj,"CODE",{});var Kqr=n(lde);VHo=r(Kqr,"from_pretrained()"),Kqr.forEach(t),WHo=r(Zj,` to load the model
weights.`),Zj.forEach(t),HHo=i(hl),ide=s(hl,"P",{});var Yqr=n(ide);UHo=r(Yqr,"Examples:"),Yqr.forEach(t),JHo=i(hl),f(Py.$$.fragment,hl),hl.forEach(t),KHo=i(fl),po=s(fl,"DIV",{class:!0});var ea=n(po);f(Sy.$$.fragment,ea),YHo=i(ea),dde=s(ea,"P",{});var Zqr=n(dde);ZHo=r(Zqr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),Zqr.forEach(t),eUo=i(ea),Ya=s(ea,"P",{});var oC=n(Ya);oUo=r(oC,"The model class to instantiate is selected based on the "),cde=s(oC,"CODE",{});var ezr=n(cde);rUo=r(ezr,"model_type"),ezr.forEach(t),tUo=r(oC,` property of the config object (either
passed as an argument or loaded from `),mde=s(oC,"CODE",{});var ozr=n(mde);aUo=r(ozr,"pretrained_model_name_or_path"),ozr.forEach(t),sUo=r(oC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),fde=s(oC,"CODE",{});var rzr=n(fde);nUo=r(rzr,"pretrained_model_name_or_path"),rzr.forEach(t),lUo=r(oC,":"),oC.forEach(t),iUo=i(ea),hde=s(ea,"UL",{});var tzr=n(hde);z1=s(tzr,"LI",{});var qCe=n(z1);gde=s(qCe,"STRONG",{});var azr=n(gde);dUo=r(azr,"tapas"),azr.forEach(t),cUo=r(qCe," \u2014 "),AI=s(qCe,"A",{href:!0});var szr=n(AI);mUo=r(szr,"TFTapasForQuestionAnswering"),szr.forEach(t),fUo=r(qCe," (TAPAS model)"),qCe.forEach(t),tzr.forEach(t),hUo=i(ea),ude=s(ea,"P",{});var nzr=n(ude);gUo=r(nzr,"Examples:"),nzr.forEach(t),uUo=i(ea),f($y.$$.fragment,ea),ea.forEach(t),fl.forEach(t),_ye=i(d),Nd=s(d,"H2",{class:!0});var y7e=n(Nd);X1=s(y7e,"A",{id:!0,class:!0,href:!0});var lzr=n(X1);pde=s(lzr,"SPAN",{});var izr=n(pde);f(Iy.$$.fragment,izr),izr.forEach(t),lzr.forEach(t),pUo=i(y7e),_de=s(y7e,"SPAN",{});var dzr=n(_de);_Uo=r(dzr,"TFAutoModelForTokenClassification"),dzr.forEach(t),y7e.forEach(t),bye=i(d),ur=s(d,"DIV",{class:!0});var gl=n(ur);f(Dy.$$.fragment,gl),bUo=i(gl),jd=s(gl,"P",{});var eO=n(jd);vUo=r(eO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),bde=s(eO,"CODE",{});var czr=n(bde);TUo=r(czr,"from_pretrained()"),czr.forEach(t),FUo=r(eO,` class method or the
`),vde=s(eO,"CODE",{});var mzr=n(vde);EUo=r(mzr,"from_config()"),mzr.forEach(t),CUo=r(eO," class method."),eO.forEach(t),MUo=i(gl),Ny=s(gl,"P",{});var w7e=n(Ny);yUo=r(w7e,"This class cannot be instantiated directly using "),Tde=s(w7e,"CODE",{});var fzr=n(Tde);wUo=r(fzr,"__init__()"),fzr.forEach(t),AUo=r(w7e," (throws an error)."),w7e.forEach(t),LUo=i(gl),at=s(gl,"DIV",{class:!0});var ul=n(at);f(jy.$$.fragment,ul),BUo=i(ul),Fde=s(ul,"P",{});var hzr=n(Fde);xUo=r(hzr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),hzr.forEach(t),kUo=i(ul),Od=s(ul,"P",{});var oO=n(Od);RUo=r(oO,`Note:
Loading a model from its configuration file does `),Ede=s(oO,"STRONG",{});var gzr=n(Ede);PUo=r(gzr,"not"),gzr.forEach(t),SUo=r(oO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Cde=s(oO,"CODE",{});var uzr=n(Cde);$Uo=r(uzr,"from_pretrained()"),uzr.forEach(t),IUo=r(oO,` to load the model
weights.`),oO.forEach(t),DUo=i(ul),Mde=s(ul,"P",{});var pzr=n(Mde);NUo=r(pzr,"Examples:"),pzr.forEach(t),jUo=i(ul),f(Oy.$$.fragment,ul),ul.forEach(t),OUo=i(gl),_o=s(gl,"DIV",{class:!0});var oa=n(_o);f(Gy.$$.fragment,oa),GUo=i(oa),yde=s(oa,"P",{});var _zr=n(yde);qUo=r(_zr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),_zr.forEach(t),zUo=i(oa),Za=s(oa,"P",{});var rC=n(Za);XUo=r(rC,"The model class to instantiate is selected based on the "),wde=s(rC,"CODE",{});var bzr=n(wde);QUo=r(bzr,"model_type"),bzr.forEach(t),VUo=r(rC,` property of the config object (either
passed as an argument or loaded from `),Ade=s(rC,"CODE",{});var vzr=n(Ade);WUo=r(vzr,"pretrained_model_name_or_path"),vzr.forEach(t),HUo=r(rC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Lde=s(rC,"CODE",{});var Tzr=n(Lde);UUo=r(Tzr,"pretrained_model_name_or_path"),Tzr.forEach(t),JUo=r(rC,":"),rC.forEach(t),KUo=i(oa),ae=s(oa,"UL",{});var le=n(ae);Q1=s(le,"LI",{});var zCe=n(Q1);Bde=s(zCe,"STRONG",{});var Fzr=n(Bde);YUo=r(Fzr,"albert"),Fzr.forEach(t),ZUo=r(zCe," \u2014 "),LI=s(zCe,"A",{href:!0});var Ezr=n(LI);eJo=r(Ezr,"TFAlbertForTokenClassification"),Ezr.forEach(t),oJo=r(zCe," (ALBERT model)"),zCe.forEach(t),rJo=i(le),V1=s(le,"LI",{});var XCe=n(V1);xde=s(XCe,"STRONG",{});var Czr=n(xde);tJo=r(Czr,"bert"),Czr.forEach(t),aJo=r(XCe," \u2014 "),BI=s(XCe,"A",{href:!0});var Mzr=n(BI);sJo=r(Mzr,"TFBertForTokenClassification"),Mzr.forEach(t),nJo=r(XCe," (BERT model)"),XCe.forEach(t),lJo=i(le),W1=s(le,"LI",{});var QCe=n(W1);kde=s(QCe,"STRONG",{});var yzr=n(kde);iJo=r(yzr,"camembert"),yzr.forEach(t),dJo=r(QCe," \u2014 "),xI=s(QCe,"A",{href:!0});var wzr=n(xI);cJo=r(wzr,"TFCamembertForTokenClassification"),wzr.forEach(t),mJo=r(QCe," (CamemBERT model)"),QCe.forEach(t),fJo=i(le),H1=s(le,"LI",{});var VCe=n(H1);Rde=s(VCe,"STRONG",{});var Azr=n(Rde);hJo=r(Azr,"convbert"),Azr.forEach(t),gJo=r(VCe," \u2014 "),kI=s(VCe,"A",{href:!0});var Lzr=n(kI);uJo=r(Lzr,"TFConvBertForTokenClassification"),Lzr.forEach(t),pJo=r(VCe," (ConvBERT model)"),VCe.forEach(t),_Jo=i(le),U1=s(le,"LI",{});var WCe=n(U1);Pde=s(WCe,"STRONG",{});var Bzr=n(Pde);bJo=r(Bzr,"deberta"),Bzr.forEach(t),vJo=r(WCe," \u2014 "),RI=s(WCe,"A",{href:!0});var xzr=n(RI);TJo=r(xzr,"TFDebertaForTokenClassification"),xzr.forEach(t),FJo=r(WCe," (DeBERTa model)"),WCe.forEach(t),EJo=i(le),J1=s(le,"LI",{});var HCe=n(J1);Sde=s(HCe,"STRONG",{});var kzr=n(Sde);CJo=r(kzr,"deberta-v2"),kzr.forEach(t),MJo=r(HCe," \u2014 "),PI=s(HCe,"A",{href:!0});var Rzr=n(PI);yJo=r(Rzr,"TFDebertaV2ForTokenClassification"),Rzr.forEach(t),wJo=r(HCe," (DeBERTa-v2 model)"),HCe.forEach(t),AJo=i(le),K1=s(le,"LI",{});var UCe=n(K1);$de=s(UCe,"STRONG",{});var Pzr=n($de);LJo=r(Pzr,"distilbert"),Pzr.forEach(t),BJo=r(UCe," \u2014 "),SI=s(UCe,"A",{href:!0});var Szr=n(SI);xJo=r(Szr,"TFDistilBertForTokenClassification"),Szr.forEach(t),kJo=r(UCe," (DistilBERT model)"),UCe.forEach(t),RJo=i(le),Y1=s(le,"LI",{});var JCe=n(Y1);Ide=s(JCe,"STRONG",{});var $zr=n(Ide);PJo=r($zr,"electra"),$zr.forEach(t),SJo=r(JCe," \u2014 "),$I=s(JCe,"A",{href:!0});var Izr=n($I);$Jo=r(Izr,"TFElectraForTokenClassification"),Izr.forEach(t),IJo=r(JCe," (ELECTRA model)"),JCe.forEach(t),DJo=i(le),Z1=s(le,"LI",{});var KCe=n(Z1);Dde=s(KCe,"STRONG",{});var Dzr=n(Dde);NJo=r(Dzr,"flaubert"),Dzr.forEach(t),jJo=r(KCe," \u2014 "),II=s(KCe,"A",{href:!0});var Nzr=n(II);OJo=r(Nzr,"TFFlaubertForTokenClassification"),Nzr.forEach(t),GJo=r(KCe," (FlauBERT model)"),KCe.forEach(t),qJo=i(le),eF=s(le,"LI",{});var YCe=n(eF);Nde=s(YCe,"STRONG",{});var jzr=n(Nde);zJo=r(jzr,"funnel"),jzr.forEach(t),XJo=r(YCe," \u2014 "),DI=s(YCe,"A",{href:!0});var Ozr=n(DI);QJo=r(Ozr,"TFFunnelForTokenClassification"),Ozr.forEach(t),VJo=r(YCe," (Funnel Transformer model)"),YCe.forEach(t),WJo=i(le),oF=s(le,"LI",{});var ZCe=n(oF);jde=s(ZCe,"STRONG",{});var Gzr=n(jde);HJo=r(Gzr,"layoutlm"),Gzr.forEach(t),UJo=r(ZCe," \u2014 "),NI=s(ZCe,"A",{href:!0});var qzr=n(NI);JJo=r(qzr,"TFLayoutLMForTokenClassification"),qzr.forEach(t),KJo=r(ZCe," (LayoutLM model)"),ZCe.forEach(t),YJo=i(le),rF=s(le,"LI",{});var e3e=n(rF);Ode=s(e3e,"STRONG",{});var zzr=n(Ode);ZJo=r(zzr,"longformer"),zzr.forEach(t),eKo=r(e3e," \u2014 "),jI=s(e3e,"A",{href:!0});var Xzr=n(jI);oKo=r(Xzr,"TFLongformerForTokenClassification"),Xzr.forEach(t),rKo=r(e3e," (Longformer model)"),e3e.forEach(t),tKo=i(le),tF=s(le,"LI",{});var o3e=n(tF);Gde=s(o3e,"STRONG",{});var Qzr=n(Gde);aKo=r(Qzr,"mobilebert"),Qzr.forEach(t),sKo=r(o3e," \u2014 "),OI=s(o3e,"A",{href:!0});var Vzr=n(OI);nKo=r(Vzr,"TFMobileBertForTokenClassification"),Vzr.forEach(t),lKo=r(o3e," (MobileBERT model)"),o3e.forEach(t),iKo=i(le),aF=s(le,"LI",{});var r3e=n(aF);qde=s(r3e,"STRONG",{});var Wzr=n(qde);dKo=r(Wzr,"mpnet"),Wzr.forEach(t),cKo=r(r3e," \u2014 "),GI=s(r3e,"A",{href:!0});var Hzr=n(GI);mKo=r(Hzr,"TFMPNetForTokenClassification"),Hzr.forEach(t),fKo=r(r3e," (MPNet model)"),r3e.forEach(t),hKo=i(le),sF=s(le,"LI",{});var t3e=n(sF);zde=s(t3e,"STRONG",{});var Uzr=n(zde);gKo=r(Uzr,"rembert"),Uzr.forEach(t),uKo=r(t3e," \u2014 "),qI=s(t3e,"A",{href:!0});var Jzr=n(qI);pKo=r(Jzr,"TFRemBertForTokenClassification"),Jzr.forEach(t),_Ko=r(t3e," (RemBERT model)"),t3e.forEach(t),bKo=i(le),nF=s(le,"LI",{});var a3e=n(nF);Xde=s(a3e,"STRONG",{});var Kzr=n(Xde);vKo=r(Kzr,"roberta"),Kzr.forEach(t),TKo=r(a3e," \u2014 "),zI=s(a3e,"A",{href:!0});var Yzr=n(zI);FKo=r(Yzr,"TFRobertaForTokenClassification"),Yzr.forEach(t),EKo=r(a3e," (RoBERTa model)"),a3e.forEach(t),CKo=i(le),lF=s(le,"LI",{});var s3e=n(lF);Qde=s(s3e,"STRONG",{});var Zzr=n(Qde);MKo=r(Zzr,"roformer"),Zzr.forEach(t),yKo=r(s3e," \u2014 "),XI=s(s3e,"A",{href:!0});var eXr=n(XI);wKo=r(eXr,"TFRoFormerForTokenClassification"),eXr.forEach(t),AKo=r(s3e," (RoFormer model)"),s3e.forEach(t),LKo=i(le),iF=s(le,"LI",{});var n3e=n(iF);Vde=s(n3e,"STRONG",{});var oXr=n(Vde);BKo=r(oXr,"xlm"),oXr.forEach(t),xKo=r(n3e," \u2014 "),QI=s(n3e,"A",{href:!0});var rXr=n(QI);kKo=r(rXr,"TFXLMForTokenClassification"),rXr.forEach(t),RKo=r(n3e," (XLM model)"),n3e.forEach(t),PKo=i(le),dF=s(le,"LI",{});var l3e=n(dF);Wde=s(l3e,"STRONG",{});var tXr=n(Wde);SKo=r(tXr,"xlm-roberta"),tXr.forEach(t),$Ko=r(l3e," \u2014 "),VI=s(l3e,"A",{href:!0});var aXr=n(VI);IKo=r(aXr,"TFXLMRobertaForTokenClassification"),aXr.forEach(t),DKo=r(l3e," (XLM-RoBERTa model)"),l3e.forEach(t),NKo=i(le),cF=s(le,"LI",{});var i3e=n(cF);Hde=s(i3e,"STRONG",{});var sXr=n(Hde);jKo=r(sXr,"xlnet"),sXr.forEach(t),OKo=r(i3e," \u2014 "),WI=s(i3e,"A",{href:!0});var nXr=n(WI);GKo=r(nXr,"TFXLNetForTokenClassification"),nXr.forEach(t),qKo=r(i3e," (XLNet model)"),i3e.forEach(t),le.forEach(t),zKo=i(oa),Ude=s(oa,"P",{});var lXr=n(Ude);XKo=r(lXr,"Examples:"),lXr.forEach(t),QKo=i(oa),f(qy.$$.fragment,oa),oa.forEach(t),gl.forEach(t),vye=i(d),Gd=s(d,"H2",{class:!0});var A7e=n(Gd);mF=s(A7e,"A",{id:!0,class:!0,href:!0});var iXr=n(mF);Jde=s(iXr,"SPAN",{});var dXr=n(Jde);f(zy.$$.fragment,dXr),dXr.forEach(t),iXr.forEach(t),VKo=i(A7e),Kde=s(A7e,"SPAN",{});var cXr=n(Kde);WKo=r(cXr,"TFAutoModelForQuestionAnswering"),cXr.forEach(t),A7e.forEach(t),Tye=i(d),pr=s(d,"DIV",{class:!0});var pl=n(pr);f(Xy.$$.fragment,pl),HKo=i(pl),qd=s(pl,"P",{});var rO=n(qd);UKo=r(rO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Yde=s(rO,"CODE",{});var mXr=n(Yde);JKo=r(mXr,"from_pretrained()"),mXr.forEach(t),KKo=r(rO,` class method or the
`),Zde=s(rO,"CODE",{});var fXr=n(Zde);YKo=r(fXr,"from_config()"),fXr.forEach(t),ZKo=r(rO," class method."),rO.forEach(t),eYo=i(pl),Qy=s(pl,"P",{});var L7e=n(Qy);oYo=r(L7e,"This class cannot be instantiated directly using "),ece=s(L7e,"CODE",{});var hXr=n(ece);rYo=r(hXr,"__init__()"),hXr.forEach(t),tYo=r(L7e," (throws an error)."),L7e.forEach(t),aYo=i(pl),st=s(pl,"DIV",{class:!0});var _l=n(st);f(Vy.$$.fragment,_l),sYo=i(_l),oce=s(_l,"P",{});var gXr=n(oce);nYo=r(gXr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),gXr.forEach(t),lYo=i(_l),zd=s(_l,"P",{});var tO=n(zd);iYo=r(tO,`Note:
Loading a model from its configuration file does `),rce=s(tO,"STRONG",{});var uXr=n(rce);dYo=r(uXr,"not"),uXr.forEach(t),cYo=r(tO,` load the model weights. It only affects the
model\u2019s configuration. Use `),tce=s(tO,"CODE",{});var pXr=n(tce);mYo=r(pXr,"from_pretrained()"),pXr.forEach(t),fYo=r(tO,` to load the model
weights.`),tO.forEach(t),hYo=i(_l),ace=s(_l,"P",{});var _Xr=n(ace);gYo=r(_Xr,"Examples:"),_Xr.forEach(t),uYo=i(_l),f(Wy.$$.fragment,_l),_l.forEach(t),pYo=i(pl),bo=s(pl,"DIV",{class:!0});var ra=n(bo);f(Hy.$$.fragment,ra),_Yo=i(ra),sce=s(ra,"P",{});var bXr=n(sce);bYo=r(bXr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),bXr.forEach(t),vYo=i(ra),es=s(ra,"P",{});var tC=n(es);TYo=r(tC,"The model class to instantiate is selected based on the "),nce=s(tC,"CODE",{});var vXr=n(nce);FYo=r(vXr,"model_type"),vXr.forEach(t),EYo=r(tC,` property of the config object (either
passed as an argument or loaded from `),lce=s(tC,"CODE",{});var TXr=n(lce);CYo=r(TXr,"pretrained_model_name_or_path"),TXr.forEach(t),MYo=r(tC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),ice=s(tC,"CODE",{});var FXr=n(ice);yYo=r(FXr,"pretrained_model_name_or_path"),FXr.forEach(t),wYo=r(tC,":"),tC.forEach(t),AYo=i(ra),se=s(ra,"UL",{});var ie=n(se);fF=s(ie,"LI",{});var d3e=n(fF);dce=s(d3e,"STRONG",{});var EXr=n(dce);LYo=r(EXr,"albert"),EXr.forEach(t),BYo=r(d3e," \u2014 "),HI=s(d3e,"A",{href:!0});var CXr=n(HI);xYo=r(CXr,"TFAlbertForQuestionAnswering"),CXr.forEach(t),kYo=r(d3e," (ALBERT model)"),d3e.forEach(t),RYo=i(ie),hF=s(ie,"LI",{});var c3e=n(hF);cce=s(c3e,"STRONG",{});var MXr=n(cce);PYo=r(MXr,"bert"),MXr.forEach(t),SYo=r(c3e," \u2014 "),UI=s(c3e,"A",{href:!0});var yXr=n(UI);$Yo=r(yXr,"TFBertForQuestionAnswering"),yXr.forEach(t),IYo=r(c3e," (BERT model)"),c3e.forEach(t),DYo=i(ie),gF=s(ie,"LI",{});var m3e=n(gF);mce=s(m3e,"STRONG",{});var wXr=n(mce);NYo=r(wXr,"camembert"),wXr.forEach(t),jYo=r(m3e," \u2014 "),JI=s(m3e,"A",{href:!0});var AXr=n(JI);OYo=r(AXr,"TFCamembertForQuestionAnswering"),AXr.forEach(t),GYo=r(m3e," (CamemBERT model)"),m3e.forEach(t),qYo=i(ie),uF=s(ie,"LI",{});var f3e=n(uF);fce=s(f3e,"STRONG",{});var LXr=n(fce);zYo=r(LXr,"convbert"),LXr.forEach(t),XYo=r(f3e," \u2014 "),KI=s(f3e,"A",{href:!0});var BXr=n(KI);QYo=r(BXr,"TFConvBertForQuestionAnswering"),BXr.forEach(t),VYo=r(f3e," (ConvBERT model)"),f3e.forEach(t),WYo=i(ie),pF=s(ie,"LI",{});var h3e=n(pF);hce=s(h3e,"STRONG",{});var xXr=n(hce);HYo=r(xXr,"deberta"),xXr.forEach(t),UYo=r(h3e," \u2014 "),YI=s(h3e,"A",{href:!0});var kXr=n(YI);JYo=r(kXr,"TFDebertaForQuestionAnswering"),kXr.forEach(t),KYo=r(h3e," (DeBERTa model)"),h3e.forEach(t),YYo=i(ie),_F=s(ie,"LI",{});var g3e=n(_F);gce=s(g3e,"STRONG",{});var RXr=n(gce);ZYo=r(RXr,"deberta-v2"),RXr.forEach(t),eZo=r(g3e," \u2014 "),ZI=s(g3e,"A",{href:!0});var PXr=n(ZI);oZo=r(PXr,"TFDebertaV2ForQuestionAnswering"),PXr.forEach(t),rZo=r(g3e," (DeBERTa-v2 model)"),g3e.forEach(t),tZo=i(ie),bF=s(ie,"LI",{});var u3e=n(bF);uce=s(u3e,"STRONG",{});var SXr=n(uce);aZo=r(SXr,"distilbert"),SXr.forEach(t),sZo=r(u3e," \u2014 "),eD=s(u3e,"A",{href:!0});var $Xr=n(eD);nZo=r($Xr,"TFDistilBertForQuestionAnswering"),$Xr.forEach(t),lZo=r(u3e," (DistilBERT model)"),u3e.forEach(t),iZo=i(ie),vF=s(ie,"LI",{});var p3e=n(vF);pce=s(p3e,"STRONG",{});var IXr=n(pce);dZo=r(IXr,"electra"),IXr.forEach(t),cZo=r(p3e," \u2014 "),oD=s(p3e,"A",{href:!0});var DXr=n(oD);mZo=r(DXr,"TFElectraForQuestionAnswering"),DXr.forEach(t),fZo=r(p3e," (ELECTRA model)"),p3e.forEach(t),hZo=i(ie),TF=s(ie,"LI",{});var _3e=n(TF);_ce=s(_3e,"STRONG",{});var NXr=n(_ce);gZo=r(NXr,"flaubert"),NXr.forEach(t),uZo=r(_3e," \u2014 "),rD=s(_3e,"A",{href:!0});var jXr=n(rD);pZo=r(jXr,"TFFlaubertForQuestionAnsweringSimple"),jXr.forEach(t),_Zo=r(_3e," (FlauBERT model)"),_3e.forEach(t),bZo=i(ie),FF=s(ie,"LI",{});var b3e=n(FF);bce=s(b3e,"STRONG",{});var OXr=n(bce);vZo=r(OXr,"funnel"),OXr.forEach(t),TZo=r(b3e," \u2014 "),tD=s(b3e,"A",{href:!0});var GXr=n(tD);FZo=r(GXr,"TFFunnelForQuestionAnswering"),GXr.forEach(t),EZo=r(b3e," (Funnel Transformer model)"),b3e.forEach(t),CZo=i(ie),EF=s(ie,"LI",{});var v3e=n(EF);vce=s(v3e,"STRONG",{});var qXr=n(vce);MZo=r(qXr,"longformer"),qXr.forEach(t),yZo=r(v3e," \u2014 "),aD=s(v3e,"A",{href:!0});var zXr=n(aD);wZo=r(zXr,"TFLongformerForQuestionAnswering"),zXr.forEach(t),AZo=r(v3e," (Longformer model)"),v3e.forEach(t),LZo=i(ie),CF=s(ie,"LI",{});var T3e=n(CF);Tce=s(T3e,"STRONG",{});var XXr=n(Tce);BZo=r(XXr,"mobilebert"),XXr.forEach(t),xZo=r(T3e," \u2014 "),sD=s(T3e,"A",{href:!0});var QXr=n(sD);kZo=r(QXr,"TFMobileBertForQuestionAnswering"),QXr.forEach(t),RZo=r(T3e," (MobileBERT model)"),T3e.forEach(t),PZo=i(ie),MF=s(ie,"LI",{});var F3e=n(MF);Fce=s(F3e,"STRONG",{});var VXr=n(Fce);SZo=r(VXr,"mpnet"),VXr.forEach(t),$Zo=r(F3e," \u2014 "),nD=s(F3e,"A",{href:!0});var WXr=n(nD);IZo=r(WXr,"TFMPNetForQuestionAnswering"),WXr.forEach(t),DZo=r(F3e," (MPNet model)"),F3e.forEach(t),NZo=i(ie),yF=s(ie,"LI",{});var E3e=n(yF);Ece=s(E3e,"STRONG",{});var HXr=n(Ece);jZo=r(HXr,"rembert"),HXr.forEach(t),OZo=r(E3e," \u2014 "),lD=s(E3e,"A",{href:!0});var UXr=n(lD);GZo=r(UXr,"TFRemBertForQuestionAnswering"),UXr.forEach(t),qZo=r(E3e," (RemBERT model)"),E3e.forEach(t),zZo=i(ie),wF=s(ie,"LI",{});var C3e=n(wF);Cce=s(C3e,"STRONG",{});var JXr=n(Cce);XZo=r(JXr,"roberta"),JXr.forEach(t),QZo=r(C3e," \u2014 "),iD=s(C3e,"A",{href:!0});var KXr=n(iD);VZo=r(KXr,"TFRobertaForQuestionAnswering"),KXr.forEach(t),WZo=r(C3e," (RoBERTa model)"),C3e.forEach(t),HZo=i(ie),AF=s(ie,"LI",{});var M3e=n(AF);Mce=s(M3e,"STRONG",{});var YXr=n(Mce);UZo=r(YXr,"roformer"),YXr.forEach(t),JZo=r(M3e," \u2014 "),dD=s(M3e,"A",{href:!0});var ZXr=n(dD);KZo=r(ZXr,"TFRoFormerForQuestionAnswering"),ZXr.forEach(t),YZo=r(M3e," (RoFormer model)"),M3e.forEach(t),ZZo=i(ie),LF=s(ie,"LI",{});var y3e=n(LF);yce=s(y3e,"STRONG",{});var eQr=n(yce);eer=r(eQr,"xlm"),eQr.forEach(t),oer=r(y3e," \u2014 "),cD=s(y3e,"A",{href:!0});var oQr=n(cD);rer=r(oQr,"TFXLMForQuestionAnsweringSimple"),oQr.forEach(t),ter=r(y3e," (XLM model)"),y3e.forEach(t),aer=i(ie),BF=s(ie,"LI",{});var w3e=n(BF);wce=s(w3e,"STRONG",{});var rQr=n(wce);ser=r(rQr,"xlm-roberta"),rQr.forEach(t),ner=r(w3e," \u2014 "),mD=s(w3e,"A",{href:!0});var tQr=n(mD);ler=r(tQr,"TFXLMRobertaForQuestionAnswering"),tQr.forEach(t),ier=r(w3e," (XLM-RoBERTa model)"),w3e.forEach(t),der=i(ie),xF=s(ie,"LI",{});var A3e=n(xF);Ace=s(A3e,"STRONG",{});var aQr=n(Ace);cer=r(aQr,"xlnet"),aQr.forEach(t),mer=r(A3e," \u2014 "),fD=s(A3e,"A",{href:!0});var sQr=n(fD);fer=r(sQr,"TFXLNetForQuestionAnsweringSimple"),sQr.forEach(t),her=r(A3e," (XLNet model)"),A3e.forEach(t),ie.forEach(t),ger=i(ra),Lce=s(ra,"P",{});var nQr=n(Lce);uer=r(nQr,"Examples:"),nQr.forEach(t),per=i(ra),f(Uy.$$.fragment,ra),ra.forEach(t),pl.forEach(t),Fye=i(d),Xd=s(d,"H2",{class:!0});var B7e=n(Xd);kF=s(B7e,"A",{id:!0,class:!0,href:!0});var lQr=n(kF);Bce=s(lQr,"SPAN",{});var iQr=n(Bce);f(Jy.$$.fragment,iQr),iQr.forEach(t),lQr.forEach(t),_er=i(B7e),xce=s(B7e,"SPAN",{});var dQr=n(xce);ber=r(dQr,"FlaxAutoModel"),dQr.forEach(t),B7e.forEach(t),Eye=i(d),_r=s(d,"DIV",{class:!0});var bl=n(_r);f(Ky.$$.fragment,bl),ver=i(bl),Qd=s(bl,"P",{});var aO=n(Qd);Ter=r(aO,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),kce=s(aO,"CODE",{});var cQr=n(kce);Fer=r(cQr,"from_pretrained()"),cQr.forEach(t),Eer=r(aO,` class method or the
`),Rce=s(aO,"CODE",{});var mQr=n(Rce);Cer=r(mQr,"from_config()"),mQr.forEach(t),Mer=r(aO," class method."),aO.forEach(t),yer=i(bl),Yy=s(bl,"P",{});var x7e=n(Yy);wer=r(x7e,"This class cannot be instantiated directly using "),Pce=s(x7e,"CODE",{});var fQr=n(Pce);Aer=r(fQr,"__init__()"),fQr.forEach(t),Ler=r(x7e," (throws an error)."),x7e.forEach(t),Ber=i(bl),nt=s(bl,"DIV",{class:!0});var vl=n(nt);f(Zy.$$.fragment,vl),xer=i(vl),Sce=s(vl,"P",{});var hQr=n(Sce);ker=r(hQr,"Instantiates one of the base model classes of the library from a configuration."),hQr.forEach(t),Rer=i(vl),Vd=s(vl,"P",{});var sO=n(Vd);Per=r(sO,`Note:
Loading a model from its configuration file does `),$ce=s(sO,"STRONG",{});var gQr=n($ce);Ser=r(gQr,"not"),gQr.forEach(t),$er=r(sO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ice=s(sO,"CODE",{});var uQr=n(Ice);Ier=r(uQr,"from_pretrained()"),uQr.forEach(t),Der=r(sO,` to load the model
weights.`),sO.forEach(t),Ner=i(vl),Dce=s(vl,"P",{});var pQr=n(Dce);jer=r(pQr,"Examples:"),pQr.forEach(t),Oer=i(vl),f(ew.$$.fragment,vl),vl.forEach(t),Ger=i(bl),vo=s(bl,"DIV",{class:!0});var ta=n(vo);f(ow.$$.fragment,ta),qer=i(ta),Nce=s(ta,"P",{});var _Qr=n(Nce);zer=r(_Qr,"Instantiate one of the base model classes of the library from a pretrained model."),_Qr.forEach(t),Xer=i(ta),os=s(ta,"P",{});var aC=n(os);Qer=r(aC,"The model class to instantiate is selected based on the "),jce=s(aC,"CODE",{});var bQr=n(jce);Ver=r(bQr,"model_type"),bQr.forEach(t),Wer=r(aC,` property of the config object (either
passed as an argument or loaded from `),Oce=s(aC,"CODE",{});var vQr=n(Oce);Her=r(vQr,"pretrained_model_name_or_path"),vQr.forEach(t),Uer=r(aC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Gce=s(aC,"CODE",{});var TQr=n(Gce);Jer=r(TQr,"pretrained_model_name_or_path"),TQr.forEach(t),Ker=r(aC,":"),aC.forEach(t),Yer=i(ta),Y=s(ta,"UL",{});var oe=n(Y);RF=s(oe,"LI",{});var L3e=n(RF);qce=s(L3e,"STRONG",{});var FQr=n(qce);Zer=r(FQr,"albert"),FQr.forEach(t),eor=r(L3e," \u2014 "),hD=s(L3e,"A",{href:!0});var EQr=n(hD);oor=r(EQr,"FlaxAlbertModel"),EQr.forEach(t),ror=r(L3e," (ALBERT model)"),L3e.forEach(t),tor=i(oe),PF=s(oe,"LI",{});var B3e=n(PF);zce=s(B3e,"STRONG",{});var CQr=n(zce);aor=r(CQr,"bart"),CQr.forEach(t),sor=r(B3e," \u2014 "),gD=s(B3e,"A",{href:!0});var MQr=n(gD);nor=r(MQr,"FlaxBartModel"),MQr.forEach(t),lor=r(B3e," (BART model)"),B3e.forEach(t),ior=i(oe),SF=s(oe,"LI",{});var x3e=n(SF);Xce=s(x3e,"STRONG",{});var yQr=n(Xce);dor=r(yQr,"beit"),yQr.forEach(t),cor=r(x3e," \u2014 "),uD=s(x3e,"A",{href:!0});var wQr=n(uD);mor=r(wQr,"FlaxBeitModel"),wQr.forEach(t),hor=r(x3e," (BEiT model)"),x3e.forEach(t),gor=i(oe),$F=s(oe,"LI",{});var k3e=n($F);Qce=s(k3e,"STRONG",{});var AQr=n(Qce);uor=r(AQr,"bert"),AQr.forEach(t),por=r(k3e," \u2014 "),pD=s(k3e,"A",{href:!0});var LQr=n(pD);_or=r(LQr,"FlaxBertModel"),LQr.forEach(t),bor=r(k3e," (BERT model)"),k3e.forEach(t),vor=i(oe),IF=s(oe,"LI",{});var R3e=n(IF);Vce=s(R3e,"STRONG",{});var BQr=n(Vce);Tor=r(BQr,"big_bird"),BQr.forEach(t),For=r(R3e," \u2014 "),_D=s(R3e,"A",{href:!0});var xQr=n(_D);Eor=r(xQr,"FlaxBigBirdModel"),xQr.forEach(t),Cor=r(R3e," (BigBird model)"),R3e.forEach(t),Mor=i(oe),DF=s(oe,"LI",{});var P3e=n(DF);Wce=s(P3e,"STRONG",{});var kQr=n(Wce);yor=r(kQr,"blenderbot"),kQr.forEach(t),wor=r(P3e," \u2014 "),bD=s(P3e,"A",{href:!0});var RQr=n(bD);Aor=r(RQr,"FlaxBlenderbotModel"),RQr.forEach(t),Lor=r(P3e," (Blenderbot model)"),P3e.forEach(t),Bor=i(oe),NF=s(oe,"LI",{});var S3e=n(NF);Hce=s(S3e,"STRONG",{});var PQr=n(Hce);xor=r(PQr,"blenderbot-small"),PQr.forEach(t),kor=r(S3e," \u2014 "),vD=s(S3e,"A",{href:!0});var SQr=n(vD);Ror=r(SQr,"FlaxBlenderbotSmallModel"),SQr.forEach(t),Por=r(S3e," (BlenderbotSmall model)"),S3e.forEach(t),Sor=i(oe),jF=s(oe,"LI",{});var $3e=n(jF);Uce=s($3e,"STRONG",{});var $Qr=n(Uce);$or=r($Qr,"clip"),$Qr.forEach(t),Ior=r($3e," \u2014 "),TD=s($3e,"A",{href:!0});var IQr=n(TD);Dor=r(IQr,"FlaxCLIPModel"),IQr.forEach(t),Nor=r($3e," (CLIP model)"),$3e.forEach(t),jor=i(oe),OF=s(oe,"LI",{});var I3e=n(OF);Jce=s(I3e,"STRONG",{});var DQr=n(Jce);Oor=r(DQr,"distilbert"),DQr.forEach(t),Gor=r(I3e," \u2014 "),FD=s(I3e,"A",{href:!0});var NQr=n(FD);qor=r(NQr,"FlaxDistilBertModel"),NQr.forEach(t),zor=r(I3e," (DistilBERT model)"),I3e.forEach(t),Xor=i(oe),GF=s(oe,"LI",{});var D3e=n(GF);Kce=s(D3e,"STRONG",{});var jQr=n(Kce);Qor=r(jQr,"electra"),jQr.forEach(t),Vor=r(D3e," \u2014 "),ED=s(D3e,"A",{href:!0});var OQr=n(ED);Wor=r(OQr,"FlaxElectraModel"),OQr.forEach(t),Hor=r(D3e," (ELECTRA model)"),D3e.forEach(t),Uor=i(oe),qF=s(oe,"LI",{});var N3e=n(qF);Yce=s(N3e,"STRONG",{});var GQr=n(Yce);Jor=r(GQr,"gpt2"),GQr.forEach(t),Kor=r(N3e," \u2014 "),CD=s(N3e,"A",{href:!0});var qQr=n(CD);Yor=r(qQr,"FlaxGPT2Model"),qQr.forEach(t),Zor=r(N3e," (OpenAI GPT-2 model)"),N3e.forEach(t),err=i(oe),zF=s(oe,"LI",{});var j3e=n(zF);Zce=s(j3e,"STRONG",{});var zQr=n(Zce);orr=r(zQr,"gpt_neo"),zQr.forEach(t),rrr=r(j3e," \u2014 "),MD=s(j3e,"A",{href:!0});var XQr=n(MD);trr=r(XQr,"FlaxGPTNeoModel"),XQr.forEach(t),arr=r(j3e," (GPT Neo model)"),j3e.forEach(t),srr=i(oe),XF=s(oe,"LI",{});var O3e=n(XF);eme=s(O3e,"STRONG",{});var QQr=n(eme);nrr=r(QQr,"gptj"),QQr.forEach(t),lrr=r(O3e," \u2014 "),yD=s(O3e,"A",{href:!0});var VQr=n(yD);irr=r(VQr,"FlaxGPTJModel"),VQr.forEach(t),drr=r(O3e," (GPT-J model)"),O3e.forEach(t),crr=i(oe),QF=s(oe,"LI",{});var G3e=n(QF);ome=s(G3e,"STRONG",{});var WQr=n(ome);mrr=r(WQr,"marian"),WQr.forEach(t),frr=r(G3e," \u2014 "),wD=s(G3e,"A",{href:!0});var HQr=n(wD);hrr=r(HQr,"FlaxMarianModel"),HQr.forEach(t),grr=r(G3e," (Marian model)"),G3e.forEach(t),urr=i(oe),VF=s(oe,"LI",{});var q3e=n(VF);rme=s(q3e,"STRONG",{});var UQr=n(rme);prr=r(UQr,"mbart"),UQr.forEach(t),_rr=r(q3e," \u2014 "),AD=s(q3e,"A",{href:!0});var JQr=n(AD);brr=r(JQr,"FlaxMBartModel"),JQr.forEach(t),vrr=r(q3e," (mBART model)"),q3e.forEach(t),Trr=i(oe),WF=s(oe,"LI",{});var z3e=n(WF);tme=s(z3e,"STRONG",{});var KQr=n(tme);Frr=r(KQr,"mt5"),KQr.forEach(t),Err=r(z3e," \u2014 "),LD=s(z3e,"A",{href:!0});var YQr=n(LD);Crr=r(YQr,"FlaxMT5Model"),YQr.forEach(t),Mrr=r(z3e," (mT5 model)"),z3e.forEach(t),yrr=i(oe),HF=s(oe,"LI",{});var X3e=n(HF);ame=s(X3e,"STRONG",{});var ZQr=n(ame);wrr=r(ZQr,"pegasus"),ZQr.forEach(t),Arr=r(X3e," \u2014 "),BD=s(X3e,"A",{href:!0});var eVr=n(BD);Lrr=r(eVr,"FlaxPegasusModel"),eVr.forEach(t),Brr=r(X3e," (Pegasus model)"),X3e.forEach(t),xrr=i(oe),UF=s(oe,"LI",{});var Q3e=n(UF);sme=s(Q3e,"STRONG",{});var oVr=n(sme);krr=r(oVr,"roberta"),oVr.forEach(t),Rrr=r(Q3e," \u2014 "),xD=s(Q3e,"A",{href:!0});var rVr=n(xD);Prr=r(rVr,"FlaxRobertaModel"),rVr.forEach(t),Srr=r(Q3e," (RoBERTa model)"),Q3e.forEach(t),$rr=i(oe),JF=s(oe,"LI",{});var V3e=n(JF);nme=s(V3e,"STRONG",{});var tVr=n(nme);Irr=r(tVr,"t5"),tVr.forEach(t),Drr=r(V3e," \u2014 "),kD=s(V3e,"A",{href:!0});var aVr=n(kD);Nrr=r(aVr,"FlaxT5Model"),aVr.forEach(t),jrr=r(V3e," (T5 model)"),V3e.forEach(t),Orr=i(oe),KF=s(oe,"LI",{});var W3e=n(KF);lme=s(W3e,"STRONG",{});var sVr=n(lme);Grr=r(sVr,"vision-text-dual-encoder"),sVr.forEach(t),qrr=r(W3e," \u2014 "),RD=s(W3e,"A",{href:!0});var nVr=n(RD);zrr=r(nVr,"FlaxVisionTextDualEncoderModel"),nVr.forEach(t),Xrr=r(W3e," (VisionTextDualEncoder model)"),W3e.forEach(t),Qrr=i(oe),YF=s(oe,"LI",{});var H3e=n(YF);ime=s(H3e,"STRONG",{});var lVr=n(ime);Vrr=r(lVr,"vit"),lVr.forEach(t),Wrr=r(H3e," \u2014 "),PD=s(H3e,"A",{href:!0});var iVr=n(PD);Hrr=r(iVr,"FlaxViTModel"),iVr.forEach(t),Urr=r(H3e," (ViT model)"),H3e.forEach(t),Jrr=i(oe),ZF=s(oe,"LI",{});var U3e=n(ZF);dme=s(U3e,"STRONG",{});var dVr=n(dme);Krr=r(dVr,"wav2vec2"),dVr.forEach(t),Yrr=r(U3e," \u2014 "),SD=s(U3e,"A",{href:!0});var cVr=n(SD);Zrr=r(cVr,"FlaxWav2Vec2Model"),cVr.forEach(t),etr=r(U3e," (Wav2Vec2 model)"),U3e.forEach(t),oe.forEach(t),otr=i(ta),cme=s(ta,"P",{});var mVr=n(cme);rtr=r(mVr,"Examples:"),mVr.forEach(t),ttr=i(ta),f(rw.$$.fragment,ta),ta.forEach(t),bl.forEach(t),Cye=i(d),Wd=s(d,"H2",{class:!0});var k7e=n(Wd);eE=s(k7e,"A",{id:!0,class:!0,href:!0});var fVr=n(eE);mme=s(fVr,"SPAN",{});var hVr=n(mme);f(tw.$$.fragment,hVr),hVr.forEach(t),fVr.forEach(t),atr=i(k7e),fme=s(k7e,"SPAN",{});var gVr=n(fme);str=r(gVr,"FlaxAutoModelForCausalLM"),gVr.forEach(t),k7e.forEach(t),Mye=i(d),br=s(d,"DIV",{class:!0});var Tl=n(br);f(aw.$$.fragment,Tl),ntr=i(Tl),Hd=s(Tl,"P",{});var nO=n(Hd);ltr=r(nO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),hme=s(nO,"CODE",{});var uVr=n(hme);itr=r(uVr,"from_pretrained()"),uVr.forEach(t),dtr=r(nO,` class method or the
`),gme=s(nO,"CODE",{});var pVr=n(gme);ctr=r(pVr,"from_config()"),pVr.forEach(t),mtr=r(nO," class method."),nO.forEach(t),ftr=i(Tl),sw=s(Tl,"P",{});var R7e=n(sw);htr=r(R7e,"This class cannot be instantiated directly using "),ume=s(R7e,"CODE",{});var _Vr=n(ume);gtr=r(_Vr,"__init__()"),_Vr.forEach(t),utr=r(R7e," (throws an error)."),R7e.forEach(t),ptr=i(Tl),lt=s(Tl,"DIV",{class:!0});var Fl=n(lt);f(nw.$$.fragment,Fl),_tr=i(Fl),pme=s(Fl,"P",{});var bVr=n(pme);btr=r(bVr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),bVr.forEach(t),vtr=i(Fl),Ud=s(Fl,"P",{});var lO=n(Ud);Ttr=r(lO,`Note:
Loading a model from its configuration file does `),_me=s(lO,"STRONG",{});var vVr=n(_me);Ftr=r(vVr,"not"),vVr.forEach(t),Etr=r(lO,` load the model weights. It only affects the
model\u2019s configuration. Use `),bme=s(lO,"CODE",{});var TVr=n(bme);Ctr=r(TVr,"from_pretrained()"),TVr.forEach(t),Mtr=r(lO,` to load the model
weights.`),lO.forEach(t),ytr=i(Fl),vme=s(Fl,"P",{});var FVr=n(vme);wtr=r(FVr,"Examples:"),FVr.forEach(t),Atr=i(Fl),f(lw.$$.fragment,Fl),Fl.forEach(t),Ltr=i(Tl),To=s(Tl,"DIV",{class:!0});var aa=n(To);f(iw.$$.fragment,aa),Btr=i(aa),Tme=s(aa,"P",{});var EVr=n(Tme);xtr=r(EVr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),EVr.forEach(t),ktr=i(aa),rs=s(aa,"P",{});var sC=n(rs);Rtr=r(sC,"The model class to instantiate is selected based on the "),Fme=s(sC,"CODE",{});var CVr=n(Fme);Ptr=r(CVr,"model_type"),CVr.forEach(t),Str=r(sC,` property of the config object (either
passed as an argument or loaded from `),Eme=s(sC,"CODE",{});var MVr=n(Eme);$tr=r(MVr,"pretrained_model_name_or_path"),MVr.forEach(t),Itr=r(sC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Cme=s(sC,"CODE",{});var yVr=n(Cme);Dtr=r(yVr,"pretrained_model_name_or_path"),yVr.forEach(t),Ntr=r(sC,":"),sC.forEach(t),jtr=i(aa),Jd=s(aa,"UL",{});var iO=n(Jd);oE=s(iO,"LI",{});var J3e=n(oE);Mme=s(J3e,"STRONG",{});var wVr=n(Mme);Otr=r(wVr,"gpt2"),wVr.forEach(t),Gtr=r(J3e," \u2014 "),$D=s(J3e,"A",{href:!0});var AVr=n($D);qtr=r(AVr,"FlaxGPT2LMHeadModel"),AVr.forEach(t),ztr=r(J3e," (OpenAI GPT-2 model)"),J3e.forEach(t),Xtr=i(iO),rE=s(iO,"LI",{});var K3e=n(rE);yme=s(K3e,"STRONG",{});var LVr=n(yme);Qtr=r(LVr,"gpt_neo"),LVr.forEach(t),Vtr=r(K3e," \u2014 "),ID=s(K3e,"A",{href:!0});var BVr=n(ID);Wtr=r(BVr,"FlaxGPTNeoForCausalLM"),BVr.forEach(t),Htr=r(K3e," (GPT Neo model)"),K3e.forEach(t),Utr=i(iO),tE=s(iO,"LI",{});var Y3e=n(tE);wme=s(Y3e,"STRONG",{});var xVr=n(wme);Jtr=r(xVr,"gptj"),xVr.forEach(t),Ktr=r(Y3e," \u2014 "),DD=s(Y3e,"A",{href:!0});var kVr=n(DD);Ytr=r(kVr,"FlaxGPTJForCausalLM"),kVr.forEach(t),Ztr=r(Y3e," (GPT-J model)"),Y3e.forEach(t),iO.forEach(t),ear=i(aa),Ame=s(aa,"P",{});var RVr=n(Ame);oar=r(RVr,"Examples:"),RVr.forEach(t),rar=i(aa),f(dw.$$.fragment,aa),aa.forEach(t),Tl.forEach(t),yye=i(d),Kd=s(d,"H2",{class:!0});var P7e=n(Kd);aE=s(P7e,"A",{id:!0,class:!0,href:!0});var PVr=n(aE);Lme=s(PVr,"SPAN",{});var SVr=n(Lme);f(cw.$$.fragment,SVr),SVr.forEach(t),PVr.forEach(t),tar=i(P7e),Bme=s(P7e,"SPAN",{});var $Vr=n(Bme);aar=r($Vr,"FlaxAutoModelForPreTraining"),$Vr.forEach(t),P7e.forEach(t),wye=i(d),vr=s(d,"DIV",{class:!0});var El=n(vr);f(mw.$$.fragment,El),sar=i(El),Yd=s(El,"P",{});var dO=n(Yd);nar=r(dO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),xme=s(dO,"CODE",{});var IVr=n(xme);lar=r(IVr,"from_pretrained()"),IVr.forEach(t),iar=r(dO,` class method or the
`),kme=s(dO,"CODE",{});var DVr=n(kme);dar=r(DVr,"from_config()"),DVr.forEach(t),car=r(dO," class method."),dO.forEach(t),mar=i(El),fw=s(El,"P",{});var S7e=n(fw);far=r(S7e,"This class cannot be instantiated directly using "),Rme=s(S7e,"CODE",{});var NVr=n(Rme);har=r(NVr,"__init__()"),NVr.forEach(t),gar=r(S7e," (throws an error)."),S7e.forEach(t),uar=i(El),it=s(El,"DIV",{class:!0});var Cl=n(it);f(hw.$$.fragment,Cl),par=i(Cl),Pme=s(Cl,"P",{});var jVr=n(Pme);_ar=r(jVr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),jVr.forEach(t),bar=i(Cl),Zd=s(Cl,"P",{});var cO=n(Zd);Tar=r(cO,`Note:
Loading a model from its configuration file does `),Sme=s(cO,"STRONG",{});var OVr=n(Sme);Far=r(OVr,"not"),OVr.forEach(t),Ear=r(cO,` load the model weights. It only affects the
model\u2019s configuration. Use `),$me=s(cO,"CODE",{});var GVr=n($me);Car=r(GVr,"from_pretrained()"),GVr.forEach(t),Mar=r(cO,` to load the model
weights.`),cO.forEach(t),yar=i(Cl),Ime=s(Cl,"P",{});var qVr=n(Ime);war=r(qVr,"Examples:"),qVr.forEach(t),Aar=i(Cl),f(gw.$$.fragment,Cl),Cl.forEach(t),Lar=i(El),Fo=s(El,"DIV",{class:!0});var sa=n(Fo);f(uw.$$.fragment,sa),Bar=i(sa),Dme=s(sa,"P",{});var zVr=n(Dme);xar=r(zVr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),zVr.forEach(t),kar=i(sa),ts=s(sa,"P",{});var nC=n(ts);Rar=r(nC,"The model class to instantiate is selected based on the "),Nme=s(nC,"CODE",{});var XVr=n(Nme);Par=r(XVr,"model_type"),XVr.forEach(t),Sar=r(nC,` property of the config object (either
passed as an argument or loaded from `),jme=s(nC,"CODE",{});var QVr=n(jme);$ar=r(QVr,"pretrained_model_name_or_path"),QVr.forEach(t),Iar=r(nC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Ome=s(nC,"CODE",{});var VVr=n(Ome);Dar=r(VVr,"pretrained_model_name_or_path"),VVr.forEach(t),Nar=r(nC,":"),nC.forEach(t),jar=i(sa),ve=s(sa,"UL",{});var Le=n(ve);sE=s(Le,"LI",{});var Z3e=n(sE);Gme=s(Z3e,"STRONG",{});var WVr=n(Gme);Oar=r(WVr,"albert"),WVr.forEach(t),Gar=r(Z3e," \u2014 "),ND=s(Z3e,"A",{href:!0});var HVr=n(ND);qar=r(HVr,"FlaxAlbertForPreTraining"),HVr.forEach(t),zar=r(Z3e," (ALBERT model)"),Z3e.forEach(t),Xar=i(Le),nE=s(Le,"LI",{});var eMe=n(nE);qme=s(eMe,"STRONG",{});var UVr=n(qme);Qar=r(UVr,"bart"),UVr.forEach(t),Var=r(eMe," \u2014 "),jD=s(eMe,"A",{href:!0});var JVr=n(jD);War=r(JVr,"FlaxBartForConditionalGeneration"),JVr.forEach(t),Har=r(eMe," (BART model)"),eMe.forEach(t),Uar=i(Le),lE=s(Le,"LI",{});var oMe=n(lE);zme=s(oMe,"STRONG",{});var KVr=n(zme);Jar=r(KVr,"bert"),KVr.forEach(t),Kar=r(oMe," \u2014 "),OD=s(oMe,"A",{href:!0});var YVr=n(OD);Yar=r(YVr,"FlaxBertForPreTraining"),YVr.forEach(t),Zar=r(oMe," (BERT model)"),oMe.forEach(t),esr=i(Le),iE=s(Le,"LI",{});var rMe=n(iE);Xme=s(rMe,"STRONG",{});var ZVr=n(Xme);osr=r(ZVr,"big_bird"),ZVr.forEach(t),rsr=r(rMe," \u2014 "),GD=s(rMe,"A",{href:!0});var eWr=n(GD);tsr=r(eWr,"FlaxBigBirdForPreTraining"),eWr.forEach(t),asr=r(rMe," (BigBird model)"),rMe.forEach(t),ssr=i(Le),dE=s(Le,"LI",{});var tMe=n(dE);Qme=s(tMe,"STRONG",{});var oWr=n(Qme);nsr=r(oWr,"electra"),oWr.forEach(t),lsr=r(tMe," \u2014 "),qD=s(tMe,"A",{href:!0});var rWr=n(qD);isr=r(rWr,"FlaxElectraForPreTraining"),rWr.forEach(t),dsr=r(tMe," (ELECTRA model)"),tMe.forEach(t),csr=i(Le),cE=s(Le,"LI",{});var aMe=n(cE);Vme=s(aMe,"STRONG",{});var tWr=n(Vme);msr=r(tWr,"mbart"),tWr.forEach(t),fsr=r(aMe," \u2014 "),zD=s(aMe,"A",{href:!0});var aWr=n(zD);hsr=r(aWr,"FlaxMBartForConditionalGeneration"),aWr.forEach(t),gsr=r(aMe," (mBART model)"),aMe.forEach(t),usr=i(Le),mE=s(Le,"LI",{});var sMe=n(mE);Wme=s(sMe,"STRONG",{});var sWr=n(Wme);psr=r(sWr,"mt5"),sWr.forEach(t),_sr=r(sMe," \u2014 "),XD=s(sMe,"A",{href:!0});var nWr=n(XD);bsr=r(nWr,"FlaxMT5ForConditionalGeneration"),nWr.forEach(t),vsr=r(sMe," (mT5 model)"),sMe.forEach(t),Tsr=i(Le),fE=s(Le,"LI",{});var nMe=n(fE);Hme=s(nMe,"STRONG",{});var lWr=n(Hme);Fsr=r(lWr,"roberta"),lWr.forEach(t),Esr=r(nMe," \u2014 "),QD=s(nMe,"A",{href:!0});var iWr=n(QD);Csr=r(iWr,"FlaxRobertaForMaskedLM"),iWr.forEach(t),Msr=r(nMe," (RoBERTa model)"),nMe.forEach(t),ysr=i(Le),hE=s(Le,"LI",{});var lMe=n(hE);Ume=s(lMe,"STRONG",{});var dWr=n(Ume);wsr=r(dWr,"t5"),dWr.forEach(t),Asr=r(lMe," \u2014 "),VD=s(lMe,"A",{href:!0});var cWr=n(VD);Lsr=r(cWr,"FlaxT5ForConditionalGeneration"),cWr.forEach(t),Bsr=r(lMe," (T5 model)"),lMe.forEach(t),xsr=i(Le),gE=s(Le,"LI",{});var iMe=n(gE);Jme=s(iMe,"STRONG",{});var mWr=n(Jme);ksr=r(mWr,"wav2vec2"),mWr.forEach(t),Rsr=r(iMe," \u2014 "),WD=s(iMe,"A",{href:!0});var fWr=n(WD);Psr=r(fWr,"FlaxWav2Vec2ForPreTraining"),fWr.forEach(t),Ssr=r(iMe," (Wav2Vec2 model)"),iMe.forEach(t),Le.forEach(t),$sr=i(sa),Kme=s(sa,"P",{});var hWr=n(Kme);Isr=r(hWr,"Examples:"),hWr.forEach(t),Dsr=i(sa),f(pw.$$.fragment,sa),sa.forEach(t),El.forEach(t),Aye=i(d),ec=s(d,"H2",{class:!0});var $7e=n(ec);uE=s($7e,"A",{id:!0,class:!0,href:!0});var gWr=n(uE);Yme=s(gWr,"SPAN",{});var uWr=n(Yme);f(_w.$$.fragment,uWr),uWr.forEach(t),gWr.forEach(t),Nsr=i($7e),Zme=s($7e,"SPAN",{});var pWr=n(Zme);jsr=r(pWr,"FlaxAutoModelForMaskedLM"),pWr.forEach(t),$7e.forEach(t),Lye=i(d),Tr=s(d,"DIV",{class:!0});var Ml=n(Tr);f(bw.$$.fragment,Ml),Osr=i(Ml),oc=s(Ml,"P",{});var mO=n(oc);Gsr=r(mO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),efe=s(mO,"CODE",{});var _Wr=n(efe);qsr=r(_Wr,"from_pretrained()"),_Wr.forEach(t),zsr=r(mO,` class method or the
`),ofe=s(mO,"CODE",{});var bWr=n(ofe);Xsr=r(bWr,"from_config()"),bWr.forEach(t),Qsr=r(mO," class method."),mO.forEach(t),Vsr=i(Ml),vw=s(Ml,"P",{});var I7e=n(vw);Wsr=r(I7e,"This class cannot be instantiated directly using "),rfe=s(I7e,"CODE",{});var vWr=n(rfe);Hsr=r(vWr,"__init__()"),vWr.forEach(t),Usr=r(I7e," (throws an error)."),I7e.forEach(t),Jsr=i(Ml),dt=s(Ml,"DIV",{class:!0});var yl=n(dt);f(Tw.$$.fragment,yl),Ksr=i(yl),tfe=s(yl,"P",{});var TWr=n(tfe);Ysr=r(TWr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),TWr.forEach(t),Zsr=i(yl),rc=s(yl,"P",{});var fO=n(rc);enr=r(fO,`Note:
Loading a model from its configuration file does `),afe=s(fO,"STRONG",{});var FWr=n(afe);onr=r(FWr,"not"),FWr.forEach(t),rnr=r(fO,` load the model weights. It only affects the
model\u2019s configuration. Use `),sfe=s(fO,"CODE",{});var EWr=n(sfe);tnr=r(EWr,"from_pretrained()"),EWr.forEach(t),anr=r(fO,` to load the model
weights.`),fO.forEach(t),snr=i(yl),nfe=s(yl,"P",{});var CWr=n(nfe);nnr=r(CWr,"Examples:"),CWr.forEach(t),lnr=i(yl),f(Fw.$$.fragment,yl),yl.forEach(t),inr=i(Ml),Eo=s(Ml,"DIV",{class:!0});var na=n(Eo);f(Ew.$$.fragment,na),dnr=i(na),lfe=s(na,"P",{});var MWr=n(lfe);cnr=r(MWr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),MWr.forEach(t),mnr=i(na),as=s(na,"P",{});var lC=n(as);fnr=r(lC,"The model class to instantiate is selected based on the "),ife=s(lC,"CODE",{});var yWr=n(ife);hnr=r(yWr,"model_type"),yWr.forEach(t),gnr=r(lC,` property of the config object (either
passed as an argument or loaded from `),dfe=s(lC,"CODE",{});var wWr=n(dfe);unr=r(wWr,"pretrained_model_name_or_path"),wWr.forEach(t),pnr=r(lC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),cfe=s(lC,"CODE",{});var AWr=n(cfe);_nr=r(AWr,"pretrained_model_name_or_path"),AWr.forEach(t),bnr=r(lC,":"),lC.forEach(t),vnr=i(na),xe=s(na,"UL",{});var Ro=n(xe);pE=s(Ro,"LI",{});var dMe=n(pE);mfe=s(dMe,"STRONG",{});var LWr=n(mfe);Tnr=r(LWr,"albert"),LWr.forEach(t),Fnr=r(dMe," \u2014 "),HD=s(dMe,"A",{href:!0});var BWr=n(HD);Enr=r(BWr,"FlaxAlbertForMaskedLM"),BWr.forEach(t),Cnr=r(dMe," (ALBERT model)"),dMe.forEach(t),Mnr=i(Ro),_E=s(Ro,"LI",{});var cMe=n(_E);ffe=s(cMe,"STRONG",{});var xWr=n(ffe);ynr=r(xWr,"bart"),xWr.forEach(t),wnr=r(cMe," \u2014 "),UD=s(cMe,"A",{href:!0});var kWr=n(UD);Anr=r(kWr,"FlaxBartForConditionalGeneration"),kWr.forEach(t),Lnr=r(cMe," (BART model)"),cMe.forEach(t),Bnr=i(Ro),bE=s(Ro,"LI",{});var mMe=n(bE);hfe=s(mMe,"STRONG",{});var RWr=n(hfe);xnr=r(RWr,"bert"),RWr.forEach(t),knr=r(mMe," \u2014 "),JD=s(mMe,"A",{href:!0});var PWr=n(JD);Rnr=r(PWr,"FlaxBertForMaskedLM"),PWr.forEach(t),Pnr=r(mMe," (BERT model)"),mMe.forEach(t),Snr=i(Ro),vE=s(Ro,"LI",{});var fMe=n(vE);gfe=s(fMe,"STRONG",{});var SWr=n(gfe);$nr=r(SWr,"big_bird"),SWr.forEach(t),Inr=r(fMe," \u2014 "),KD=s(fMe,"A",{href:!0});var $Wr=n(KD);Dnr=r($Wr,"FlaxBigBirdForMaskedLM"),$Wr.forEach(t),Nnr=r(fMe," (BigBird model)"),fMe.forEach(t),jnr=i(Ro),TE=s(Ro,"LI",{});var hMe=n(TE);ufe=s(hMe,"STRONG",{});var IWr=n(ufe);Onr=r(IWr,"distilbert"),IWr.forEach(t),Gnr=r(hMe," \u2014 "),YD=s(hMe,"A",{href:!0});var DWr=n(YD);qnr=r(DWr,"FlaxDistilBertForMaskedLM"),DWr.forEach(t),znr=r(hMe," (DistilBERT model)"),hMe.forEach(t),Xnr=i(Ro),FE=s(Ro,"LI",{});var gMe=n(FE);pfe=s(gMe,"STRONG",{});var NWr=n(pfe);Qnr=r(NWr,"electra"),NWr.forEach(t),Vnr=r(gMe," \u2014 "),ZD=s(gMe,"A",{href:!0});var jWr=n(ZD);Wnr=r(jWr,"FlaxElectraForMaskedLM"),jWr.forEach(t),Hnr=r(gMe," (ELECTRA model)"),gMe.forEach(t),Unr=i(Ro),EE=s(Ro,"LI",{});var uMe=n(EE);_fe=s(uMe,"STRONG",{});var OWr=n(_fe);Jnr=r(OWr,"mbart"),OWr.forEach(t),Knr=r(uMe," \u2014 "),eN=s(uMe,"A",{href:!0});var GWr=n(eN);Ynr=r(GWr,"FlaxMBartForConditionalGeneration"),GWr.forEach(t),Znr=r(uMe," (mBART model)"),uMe.forEach(t),elr=i(Ro),CE=s(Ro,"LI",{});var pMe=n(CE);bfe=s(pMe,"STRONG",{});var qWr=n(bfe);olr=r(qWr,"roberta"),qWr.forEach(t),rlr=r(pMe," \u2014 "),oN=s(pMe,"A",{href:!0});var zWr=n(oN);tlr=r(zWr,"FlaxRobertaForMaskedLM"),zWr.forEach(t),alr=r(pMe," (RoBERTa model)"),pMe.forEach(t),Ro.forEach(t),slr=i(na),vfe=s(na,"P",{});var XWr=n(vfe);nlr=r(XWr,"Examples:"),XWr.forEach(t),llr=i(na),f(Cw.$$.fragment,na),na.forEach(t),Ml.forEach(t),Bye=i(d),tc=s(d,"H2",{class:!0});var D7e=n(tc);ME=s(D7e,"A",{id:!0,class:!0,href:!0});var QWr=n(ME);Tfe=s(QWr,"SPAN",{});var VWr=n(Tfe);f(Mw.$$.fragment,VWr),VWr.forEach(t),QWr.forEach(t),ilr=i(D7e),Ffe=s(D7e,"SPAN",{});var WWr=n(Ffe);dlr=r(WWr,"FlaxAutoModelForSeq2SeqLM"),WWr.forEach(t),D7e.forEach(t),xye=i(d),Fr=s(d,"DIV",{class:!0});var wl=n(Fr);f(yw.$$.fragment,wl),clr=i(wl),ac=s(wl,"P",{});var hO=n(ac);mlr=r(hO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Efe=s(hO,"CODE",{});var HWr=n(Efe);flr=r(HWr,"from_pretrained()"),HWr.forEach(t),hlr=r(hO,` class method or the
`),Cfe=s(hO,"CODE",{});var UWr=n(Cfe);glr=r(UWr,"from_config()"),UWr.forEach(t),ulr=r(hO," class method."),hO.forEach(t),plr=i(wl),ww=s(wl,"P",{});var N7e=n(ww);_lr=r(N7e,"This class cannot be instantiated directly using "),Mfe=s(N7e,"CODE",{});var JWr=n(Mfe);blr=r(JWr,"__init__()"),JWr.forEach(t),vlr=r(N7e," (throws an error)."),N7e.forEach(t),Tlr=i(wl),ct=s(wl,"DIV",{class:!0});var Al=n(ct);f(Aw.$$.fragment,Al),Flr=i(Al),yfe=s(Al,"P",{});var KWr=n(yfe);Elr=r(KWr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),KWr.forEach(t),Clr=i(Al),sc=s(Al,"P",{});var gO=n(sc);Mlr=r(gO,`Note:
Loading a model from its configuration file does `),wfe=s(gO,"STRONG",{});var YWr=n(wfe);ylr=r(YWr,"not"),YWr.forEach(t),wlr=r(gO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Afe=s(gO,"CODE",{});var ZWr=n(Afe);Alr=r(ZWr,"from_pretrained()"),ZWr.forEach(t),Llr=r(gO,` to load the model
weights.`),gO.forEach(t),Blr=i(Al),Lfe=s(Al,"P",{});var eHr=n(Lfe);xlr=r(eHr,"Examples:"),eHr.forEach(t),klr=i(Al),f(Lw.$$.fragment,Al),Al.forEach(t),Rlr=i(wl),Co=s(wl,"DIV",{class:!0});var la=n(Co);f(Bw.$$.fragment,la),Plr=i(la),Bfe=s(la,"P",{});var oHr=n(Bfe);Slr=r(oHr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),oHr.forEach(t),$lr=i(la),ss=s(la,"P",{});var iC=n(ss);Ilr=r(iC,"The model class to instantiate is selected based on the "),xfe=s(iC,"CODE",{});var rHr=n(xfe);Dlr=r(rHr,"model_type"),rHr.forEach(t),Nlr=r(iC,` property of the config object (either
passed as an argument or loaded from `),kfe=s(iC,"CODE",{});var tHr=n(kfe);jlr=r(tHr,"pretrained_model_name_or_path"),tHr.forEach(t),Olr=r(iC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Rfe=s(iC,"CODE",{});var aHr=n(Rfe);Glr=r(aHr,"pretrained_model_name_or_path"),aHr.forEach(t),qlr=r(iC,":"),iC.forEach(t),zlr=i(la),Me=s(la,"UL",{});var oo=n(Me);yE=s(oo,"LI",{});var _Me=n(yE);Pfe=s(_Me,"STRONG",{});var sHr=n(Pfe);Xlr=r(sHr,"bart"),sHr.forEach(t),Qlr=r(_Me," \u2014 "),rN=s(_Me,"A",{href:!0});var nHr=n(rN);Vlr=r(nHr,"FlaxBartForConditionalGeneration"),nHr.forEach(t),Wlr=r(_Me," (BART model)"),_Me.forEach(t),Hlr=i(oo),wE=s(oo,"LI",{});var bMe=n(wE);Sfe=s(bMe,"STRONG",{});var lHr=n(Sfe);Ulr=r(lHr,"blenderbot"),lHr.forEach(t),Jlr=r(bMe," \u2014 "),tN=s(bMe,"A",{href:!0});var iHr=n(tN);Klr=r(iHr,"FlaxBlenderbotForConditionalGeneration"),iHr.forEach(t),Ylr=r(bMe," (Blenderbot model)"),bMe.forEach(t),Zlr=i(oo),AE=s(oo,"LI",{});var vMe=n(AE);$fe=s(vMe,"STRONG",{});var dHr=n($fe);eir=r(dHr,"blenderbot-small"),dHr.forEach(t),oir=r(vMe," \u2014 "),aN=s(vMe,"A",{href:!0});var cHr=n(aN);rir=r(cHr,"FlaxBlenderbotSmallForConditionalGeneration"),cHr.forEach(t),tir=r(vMe," (BlenderbotSmall model)"),vMe.forEach(t),air=i(oo),LE=s(oo,"LI",{});var TMe=n(LE);Ife=s(TMe,"STRONG",{});var mHr=n(Ife);sir=r(mHr,"encoder-decoder"),mHr.forEach(t),nir=r(TMe," \u2014 "),sN=s(TMe,"A",{href:!0});var fHr=n(sN);lir=r(fHr,"FlaxEncoderDecoderModel"),fHr.forEach(t),iir=r(TMe," (Encoder decoder model)"),TMe.forEach(t),dir=i(oo),BE=s(oo,"LI",{});var FMe=n(BE);Dfe=s(FMe,"STRONG",{});var hHr=n(Dfe);cir=r(hHr,"marian"),hHr.forEach(t),mir=r(FMe," \u2014 "),nN=s(FMe,"A",{href:!0});var gHr=n(nN);fir=r(gHr,"FlaxMarianMTModel"),gHr.forEach(t),hir=r(FMe," (Marian model)"),FMe.forEach(t),gir=i(oo),xE=s(oo,"LI",{});var EMe=n(xE);Nfe=s(EMe,"STRONG",{});var uHr=n(Nfe);uir=r(uHr,"mbart"),uHr.forEach(t),pir=r(EMe," \u2014 "),lN=s(EMe,"A",{href:!0});var pHr=n(lN);_ir=r(pHr,"FlaxMBartForConditionalGeneration"),pHr.forEach(t),bir=r(EMe," (mBART model)"),EMe.forEach(t),vir=i(oo),kE=s(oo,"LI",{});var CMe=n(kE);jfe=s(CMe,"STRONG",{});var _Hr=n(jfe);Tir=r(_Hr,"mt5"),_Hr.forEach(t),Fir=r(CMe," \u2014 "),iN=s(CMe,"A",{href:!0});var bHr=n(iN);Eir=r(bHr,"FlaxMT5ForConditionalGeneration"),bHr.forEach(t),Cir=r(CMe," (mT5 model)"),CMe.forEach(t),Mir=i(oo),RE=s(oo,"LI",{});var MMe=n(RE);Ofe=s(MMe,"STRONG",{});var vHr=n(Ofe);yir=r(vHr,"pegasus"),vHr.forEach(t),wir=r(MMe," \u2014 "),dN=s(MMe,"A",{href:!0});var THr=n(dN);Air=r(THr,"FlaxPegasusForConditionalGeneration"),THr.forEach(t),Lir=r(MMe," (Pegasus model)"),MMe.forEach(t),Bir=i(oo),PE=s(oo,"LI",{});var yMe=n(PE);Gfe=s(yMe,"STRONG",{});var FHr=n(Gfe);xir=r(FHr,"t5"),FHr.forEach(t),kir=r(yMe," \u2014 "),cN=s(yMe,"A",{href:!0});var EHr=n(cN);Rir=r(EHr,"FlaxT5ForConditionalGeneration"),EHr.forEach(t),Pir=r(yMe," (T5 model)"),yMe.forEach(t),oo.forEach(t),Sir=i(la),qfe=s(la,"P",{});var CHr=n(qfe);$ir=r(CHr,"Examples:"),CHr.forEach(t),Iir=i(la),f(xw.$$.fragment,la),la.forEach(t),wl.forEach(t),kye=i(d),nc=s(d,"H2",{class:!0});var j7e=n(nc);SE=s(j7e,"A",{id:!0,class:!0,href:!0});var MHr=n(SE);zfe=s(MHr,"SPAN",{});var yHr=n(zfe);f(kw.$$.fragment,yHr),yHr.forEach(t),MHr.forEach(t),Dir=i(j7e),Xfe=s(j7e,"SPAN",{});var wHr=n(Xfe);Nir=r(wHr,"FlaxAutoModelForSequenceClassification"),wHr.forEach(t),j7e.forEach(t),Rye=i(d),Er=s(d,"DIV",{class:!0});var Ll=n(Er);f(Rw.$$.fragment,Ll),jir=i(Ll),lc=s(Ll,"P",{});var uO=n(lc);Oir=r(uO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),Qfe=s(uO,"CODE",{});var AHr=n(Qfe);Gir=r(AHr,"from_pretrained()"),AHr.forEach(t),qir=r(uO,` class method or the
`),Vfe=s(uO,"CODE",{});var LHr=n(Vfe);zir=r(LHr,"from_config()"),LHr.forEach(t),Xir=r(uO," class method."),uO.forEach(t),Qir=i(Ll),Pw=s(Ll,"P",{});var O7e=n(Pw);Vir=r(O7e,"This class cannot be instantiated directly using "),Wfe=s(O7e,"CODE",{});var BHr=n(Wfe);Wir=r(BHr,"__init__()"),BHr.forEach(t),Hir=r(O7e," (throws an error)."),O7e.forEach(t),Uir=i(Ll),mt=s(Ll,"DIV",{class:!0});var Bl=n(mt);f(Sw.$$.fragment,Bl),Jir=i(Bl),Hfe=s(Bl,"P",{});var xHr=n(Hfe);Kir=r(xHr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),xHr.forEach(t),Yir=i(Bl),ic=s(Bl,"P",{});var pO=n(ic);Zir=r(pO,`Note:
Loading a model from its configuration file does `),Ufe=s(pO,"STRONG",{});var kHr=n(Ufe);edr=r(kHr,"not"),kHr.forEach(t),odr=r(pO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Jfe=s(pO,"CODE",{});var RHr=n(Jfe);rdr=r(RHr,"from_pretrained()"),RHr.forEach(t),tdr=r(pO,` to load the model
weights.`),pO.forEach(t),adr=i(Bl),Kfe=s(Bl,"P",{});var PHr=n(Kfe);sdr=r(PHr,"Examples:"),PHr.forEach(t),ndr=i(Bl),f($w.$$.fragment,Bl),Bl.forEach(t),ldr=i(Ll),Mo=s(Ll,"DIV",{class:!0});var ia=n(Mo);f(Iw.$$.fragment,ia),idr=i(ia),Yfe=s(ia,"P",{});var SHr=n(Yfe);ddr=r(SHr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),SHr.forEach(t),cdr=i(ia),ns=s(ia,"P",{});var dC=n(ns);mdr=r(dC,"The model class to instantiate is selected based on the "),Zfe=s(dC,"CODE",{});var $Hr=n(Zfe);fdr=r($Hr,"model_type"),$Hr.forEach(t),hdr=r(dC,` property of the config object (either
passed as an argument or loaded from `),ehe=s(dC,"CODE",{});var IHr=n(ehe);gdr=r(IHr,"pretrained_model_name_or_path"),IHr.forEach(t),udr=r(dC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),ohe=s(dC,"CODE",{});var DHr=n(ohe);pdr=r(DHr,"pretrained_model_name_or_path"),DHr.forEach(t),_dr=r(dC,":"),dC.forEach(t),bdr=i(ia),ke=s(ia,"UL",{});var Po=n(ke);$E=s(Po,"LI",{});var wMe=n($E);rhe=s(wMe,"STRONG",{});var NHr=n(rhe);vdr=r(NHr,"albert"),NHr.forEach(t),Tdr=r(wMe," \u2014 "),mN=s(wMe,"A",{href:!0});var jHr=n(mN);Fdr=r(jHr,"FlaxAlbertForSequenceClassification"),jHr.forEach(t),Edr=r(wMe," (ALBERT model)"),wMe.forEach(t),Cdr=i(Po),IE=s(Po,"LI",{});var AMe=n(IE);the=s(AMe,"STRONG",{});var OHr=n(the);Mdr=r(OHr,"bart"),OHr.forEach(t),ydr=r(AMe," \u2014 "),fN=s(AMe,"A",{href:!0});var GHr=n(fN);wdr=r(GHr,"FlaxBartForSequenceClassification"),GHr.forEach(t),Adr=r(AMe," (BART model)"),AMe.forEach(t),Ldr=i(Po),DE=s(Po,"LI",{});var LMe=n(DE);ahe=s(LMe,"STRONG",{});var qHr=n(ahe);Bdr=r(qHr,"bert"),qHr.forEach(t),xdr=r(LMe," \u2014 "),hN=s(LMe,"A",{href:!0});var zHr=n(hN);kdr=r(zHr,"FlaxBertForSequenceClassification"),zHr.forEach(t),Rdr=r(LMe," (BERT model)"),LMe.forEach(t),Pdr=i(Po),NE=s(Po,"LI",{});var BMe=n(NE);she=s(BMe,"STRONG",{});var XHr=n(she);Sdr=r(XHr,"big_bird"),XHr.forEach(t),$dr=r(BMe," \u2014 "),gN=s(BMe,"A",{href:!0});var QHr=n(gN);Idr=r(QHr,"FlaxBigBirdForSequenceClassification"),QHr.forEach(t),Ddr=r(BMe," (BigBird model)"),BMe.forEach(t),Ndr=i(Po),jE=s(Po,"LI",{});var xMe=n(jE);nhe=s(xMe,"STRONG",{});var VHr=n(nhe);jdr=r(VHr,"distilbert"),VHr.forEach(t),Odr=r(xMe," \u2014 "),uN=s(xMe,"A",{href:!0});var WHr=n(uN);Gdr=r(WHr,"FlaxDistilBertForSequenceClassification"),WHr.forEach(t),qdr=r(xMe," (DistilBERT model)"),xMe.forEach(t),zdr=i(Po),OE=s(Po,"LI",{});var kMe=n(OE);lhe=s(kMe,"STRONG",{});var HHr=n(lhe);Xdr=r(HHr,"electra"),HHr.forEach(t),Qdr=r(kMe," \u2014 "),pN=s(kMe,"A",{href:!0});var UHr=n(pN);Vdr=r(UHr,"FlaxElectraForSequenceClassification"),UHr.forEach(t),Wdr=r(kMe," (ELECTRA model)"),kMe.forEach(t),Hdr=i(Po),GE=s(Po,"LI",{});var RMe=n(GE);ihe=s(RMe,"STRONG",{});var JHr=n(ihe);Udr=r(JHr,"mbart"),JHr.forEach(t),Jdr=r(RMe," \u2014 "),_N=s(RMe,"A",{href:!0});var KHr=n(_N);Kdr=r(KHr,"FlaxMBartForSequenceClassification"),KHr.forEach(t),Ydr=r(RMe," (mBART model)"),RMe.forEach(t),Zdr=i(Po),qE=s(Po,"LI",{});var PMe=n(qE);dhe=s(PMe,"STRONG",{});var YHr=n(dhe);ecr=r(YHr,"roberta"),YHr.forEach(t),ocr=r(PMe," \u2014 "),bN=s(PMe,"A",{href:!0});var ZHr=n(bN);rcr=r(ZHr,"FlaxRobertaForSequenceClassification"),ZHr.forEach(t),tcr=r(PMe," (RoBERTa model)"),PMe.forEach(t),Po.forEach(t),acr=i(ia),che=s(ia,"P",{});var eUr=n(che);scr=r(eUr,"Examples:"),eUr.forEach(t),ncr=i(ia),f(Dw.$$.fragment,ia),ia.forEach(t),Ll.forEach(t),Pye=i(d),dc=s(d,"H2",{class:!0});var G7e=n(dc);zE=s(G7e,"A",{id:!0,class:!0,href:!0});var oUr=n(zE);mhe=s(oUr,"SPAN",{});var rUr=n(mhe);f(Nw.$$.fragment,rUr),rUr.forEach(t),oUr.forEach(t),lcr=i(G7e),fhe=s(G7e,"SPAN",{});var tUr=n(fhe);icr=r(tUr,"FlaxAutoModelForQuestionAnswering"),tUr.forEach(t),G7e.forEach(t),Sye=i(d),Cr=s(d,"DIV",{class:!0});var xl=n(Cr);f(jw.$$.fragment,xl),dcr=i(xl),cc=s(xl,"P",{});var _O=n(cc);ccr=r(_O,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),hhe=s(_O,"CODE",{});var aUr=n(hhe);mcr=r(aUr,"from_pretrained()"),aUr.forEach(t),fcr=r(_O,` class method or the
`),ghe=s(_O,"CODE",{});var sUr=n(ghe);hcr=r(sUr,"from_config()"),sUr.forEach(t),gcr=r(_O," class method."),_O.forEach(t),ucr=i(xl),Ow=s(xl,"P",{});var q7e=n(Ow);pcr=r(q7e,"This class cannot be instantiated directly using "),uhe=s(q7e,"CODE",{});var nUr=n(uhe);_cr=r(nUr,"__init__()"),nUr.forEach(t),bcr=r(q7e," (throws an error)."),q7e.forEach(t),vcr=i(xl),ft=s(xl,"DIV",{class:!0});var kl=n(ft);f(Gw.$$.fragment,kl),Tcr=i(kl),phe=s(kl,"P",{});var lUr=n(phe);Fcr=r(lUr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),lUr.forEach(t),Ecr=i(kl),mc=s(kl,"P",{});var bO=n(mc);Ccr=r(bO,`Note:
Loading a model from its configuration file does `),_he=s(bO,"STRONG",{});var iUr=n(_he);Mcr=r(iUr,"not"),iUr.forEach(t),ycr=r(bO,` load the model weights. It only affects the
model\u2019s configuration. Use `),bhe=s(bO,"CODE",{});var dUr=n(bhe);wcr=r(dUr,"from_pretrained()"),dUr.forEach(t),Acr=r(bO,` to load the model
weights.`),bO.forEach(t),Lcr=i(kl),vhe=s(kl,"P",{});var cUr=n(vhe);Bcr=r(cUr,"Examples:"),cUr.forEach(t),xcr=i(kl),f(qw.$$.fragment,kl),kl.forEach(t),kcr=i(xl),yo=s(xl,"DIV",{class:!0});var da=n(yo);f(zw.$$.fragment,da),Rcr=i(da),The=s(da,"P",{});var mUr=n(The);Pcr=r(mUr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),mUr.forEach(t),Scr=i(da),ls=s(da,"P",{});var cC=n(ls);$cr=r(cC,"The model class to instantiate is selected based on the "),Fhe=s(cC,"CODE",{});var fUr=n(Fhe);Icr=r(fUr,"model_type"),fUr.forEach(t),Dcr=r(cC,` property of the config object (either
passed as an argument or loaded from `),Ehe=s(cC,"CODE",{});var hUr=n(Ehe);Ncr=r(hUr,"pretrained_model_name_or_path"),hUr.forEach(t),jcr=r(cC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Che=s(cC,"CODE",{});var gUr=n(Che);Ocr=r(gUr,"pretrained_model_name_or_path"),gUr.forEach(t),Gcr=r(cC,":"),cC.forEach(t),qcr=i(da),Re=s(da,"UL",{});var So=n(Re);XE=s(So,"LI",{});var SMe=n(XE);Mhe=s(SMe,"STRONG",{});var uUr=n(Mhe);zcr=r(uUr,"albert"),uUr.forEach(t),Xcr=r(SMe," \u2014 "),vN=s(SMe,"A",{href:!0});var pUr=n(vN);Qcr=r(pUr,"FlaxAlbertForQuestionAnswering"),pUr.forEach(t),Vcr=r(SMe," (ALBERT model)"),SMe.forEach(t),Wcr=i(So),QE=s(So,"LI",{});var $Me=n(QE);yhe=s($Me,"STRONG",{});var _Ur=n(yhe);Hcr=r(_Ur,"bart"),_Ur.forEach(t),Ucr=r($Me," \u2014 "),TN=s($Me,"A",{href:!0});var bUr=n(TN);Jcr=r(bUr,"FlaxBartForQuestionAnswering"),bUr.forEach(t),Kcr=r($Me," (BART model)"),$Me.forEach(t),Ycr=i(So),VE=s(So,"LI",{});var IMe=n(VE);whe=s(IMe,"STRONG",{});var vUr=n(whe);Zcr=r(vUr,"bert"),vUr.forEach(t),emr=r(IMe," \u2014 "),FN=s(IMe,"A",{href:!0});var TUr=n(FN);omr=r(TUr,"FlaxBertForQuestionAnswering"),TUr.forEach(t),rmr=r(IMe," (BERT model)"),IMe.forEach(t),tmr=i(So),WE=s(So,"LI",{});var DMe=n(WE);Ahe=s(DMe,"STRONG",{});var FUr=n(Ahe);amr=r(FUr,"big_bird"),FUr.forEach(t),smr=r(DMe," \u2014 "),EN=s(DMe,"A",{href:!0});var EUr=n(EN);nmr=r(EUr,"FlaxBigBirdForQuestionAnswering"),EUr.forEach(t),lmr=r(DMe," (BigBird model)"),DMe.forEach(t),imr=i(So),HE=s(So,"LI",{});var NMe=n(HE);Lhe=s(NMe,"STRONG",{});var CUr=n(Lhe);dmr=r(CUr,"distilbert"),CUr.forEach(t),cmr=r(NMe," \u2014 "),CN=s(NMe,"A",{href:!0});var MUr=n(CN);mmr=r(MUr,"FlaxDistilBertForQuestionAnswering"),MUr.forEach(t),fmr=r(NMe," (DistilBERT model)"),NMe.forEach(t),hmr=i(So),UE=s(So,"LI",{});var jMe=n(UE);Bhe=s(jMe,"STRONG",{});var yUr=n(Bhe);gmr=r(yUr,"electra"),yUr.forEach(t),umr=r(jMe," \u2014 "),MN=s(jMe,"A",{href:!0});var wUr=n(MN);pmr=r(wUr,"FlaxElectraForQuestionAnswering"),wUr.forEach(t),_mr=r(jMe," (ELECTRA model)"),jMe.forEach(t),bmr=i(So),JE=s(So,"LI",{});var OMe=n(JE);xhe=s(OMe,"STRONG",{});var AUr=n(xhe);vmr=r(AUr,"mbart"),AUr.forEach(t),Tmr=r(OMe," \u2014 "),yN=s(OMe,"A",{href:!0});var LUr=n(yN);Fmr=r(LUr,"FlaxMBartForQuestionAnswering"),LUr.forEach(t),Emr=r(OMe," (mBART model)"),OMe.forEach(t),Cmr=i(So),KE=s(So,"LI",{});var GMe=n(KE);khe=s(GMe,"STRONG",{});var BUr=n(khe);Mmr=r(BUr,"roberta"),BUr.forEach(t),ymr=r(GMe," \u2014 "),wN=s(GMe,"A",{href:!0});var xUr=n(wN);wmr=r(xUr,"FlaxRobertaForQuestionAnswering"),xUr.forEach(t),Amr=r(GMe," (RoBERTa model)"),GMe.forEach(t),So.forEach(t),Lmr=i(da),Rhe=s(da,"P",{});var kUr=n(Rhe);Bmr=r(kUr,"Examples:"),kUr.forEach(t),xmr=i(da),f(Xw.$$.fragment,da),da.forEach(t),xl.forEach(t),$ye=i(d),fc=s(d,"H2",{class:!0});var z7e=n(fc);YE=s(z7e,"A",{id:!0,class:!0,href:!0});var RUr=n(YE);Phe=s(RUr,"SPAN",{});var PUr=n(Phe);f(Qw.$$.fragment,PUr),PUr.forEach(t),RUr.forEach(t),kmr=i(z7e),She=s(z7e,"SPAN",{});var SUr=n(She);Rmr=r(SUr,"FlaxAutoModelForTokenClassification"),SUr.forEach(t),z7e.forEach(t),Iye=i(d),Mr=s(d,"DIV",{class:!0});var Rl=n(Mr);f(Vw.$$.fragment,Rl),Pmr=i(Rl),hc=s(Rl,"P",{});var vO=n(hc);Smr=r(vO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),$he=s(vO,"CODE",{});var $Ur=n($he);$mr=r($Ur,"from_pretrained()"),$Ur.forEach(t),Imr=r(vO,` class method or the
`),Ihe=s(vO,"CODE",{});var IUr=n(Ihe);Dmr=r(IUr,"from_config()"),IUr.forEach(t),Nmr=r(vO," class method."),vO.forEach(t),jmr=i(Rl),Ww=s(Rl,"P",{});var X7e=n(Ww);Omr=r(X7e,"This class cannot be instantiated directly using "),Dhe=s(X7e,"CODE",{});var DUr=n(Dhe);Gmr=r(DUr,"__init__()"),DUr.forEach(t),qmr=r(X7e," (throws an error)."),X7e.forEach(t),zmr=i(Rl),ht=s(Rl,"DIV",{class:!0});var Pl=n(ht);f(Hw.$$.fragment,Pl),Xmr=i(Pl),Nhe=s(Pl,"P",{});var NUr=n(Nhe);Qmr=r(NUr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),NUr.forEach(t),Vmr=i(Pl),gc=s(Pl,"P",{});var TO=n(gc);Wmr=r(TO,`Note:
Loading a model from its configuration file does `),jhe=s(TO,"STRONG",{});var jUr=n(jhe);Hmr=r(jUr,"not"),jUr.forEach(t),Umr=r(TO,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ohe=s(TO,"CODE",{});var OUr=n(Ohe);Jmr=r(OUr,"from_pretrained()"),OUr.forEach(t),Kmr=r(TO,` to load the model
weights.`),TO.forEach(t),Ymr=i(Pl),Ghe=s(Pl,"P",{});var GUr=n(Ghe);Zmr=r(GUr,"Examples:"),GUr.forEach(t),efr=i(Pl),f(Uw.$$.fragment,Pl),Pl.forEach(t),ofr=i(Rl),wo=s(Rl,"DIV",{class:!0});var ca=n(wo);f(Jw.$$.fragment,ca),rfr=i(ca),qhe=s(ca,"P",{});var qUr=n(qhe);tfr=r(qUr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),qUr.forEach(t),afr=i(ca),is=s(ca,"P",{});var mC=n(is);sfr=r(mC,"The model class to instantiate is selected based on the "),zhe=s(mC,"CODE",{});var zUr=n(zhe);nfr=r(zUr,"model_type"),zUr.forEach(t),lfr=r(mC,` property of the config object (either
passed as an argument or loaded from `),Xhe=s(mC,"CODE",{});var XUr=n(Xhe);ifr=r(XUr,"pretrained_model_name_or_path"),XUr.forEach(t),dfr=r(mC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Qhe=s(mC,"CODE",{});var QUr=n(Qhe);cfr=r(QUr,"pretrained_model_name_or_path"),QUr.forEach(t),mfr=r(mC,":"),mC.forEach(t),ffr=i(ca),yr=s(ca,"UL",{});var ma=n(yr);ZE=s(ma,"LI",{});var qMe=n(ZE);Vhe=s(qMe,"STRONG",{});var VUr=n(Vhe);hfr=r(VUr,"albert"),VUr.forEach(t),gfr=r(qMe," \u2014 "),AN=s(qMe,"A",{href:!0});var WUr=n(AN);ufr=r(WUr,"FlaxAlbertForTokenClassification"),WUr.forEach(t),pfr=r(qMe," (ALBERT model)"),qMe.forEach(t),_fr=i(ma),e4=s(ma,"LI",{});var zMe=n(e4);Whe=s(zMe,"STRONG",{});var HUr=n(Whe);bfr=r(HUr,"bert"),HUr.forEach(t),vfr=r(zMe," \u2014 "),LN=s(zMe,"A",{href:!0});var UUr=n(LN);Tfr=r(UUr,"FlaxBertForTokenClassification"),UUr.forEach(t),Ffr=r(zMe," (BERT model)"),zMe.forEach(t),Efr=i(ma),o4=s(ma,"LI",{});var XMe=n(o4);Hhe=s(XMe,"STRONG",{});var JUr=n(Hhe);Cfr=r(JUr,"big_bird"),JUr.forEach(t),Mfr=r(XMe," \u2014 "),BN=s(XMe,"A",{href:!0});var KUr=n(BN);yfr=r(KUr,"FlaxBigBirdForTokenClassification"),KUr.forEach(t),wfr=r(XMe," (BigBird model)"),XMe.forEach(t),Afr=i(ma),r4=s(ma,"LI",{});var QMe=n(r4);Uhe=s(QMe,"STRONG",{});var YUr=n(Uhe);Lfr=r(YUr,"distilbert"),YUr.forEach(t),Bfr=r(QMe," \u2014 "),xN=s(QMe,"A",{href:!0});var ZUr=n(xN);xfr=r(ZUr,"FlaxDistilBertForTokenClassification"),ZUr.forEach(t),kfr=r(QMe," (DistilBERT model)"),QMe.forEach(t),Rfr=i(ma),t4=s(ma,"LI",{});var VMe=n(t4);Jhe=s(VMe,"STRONG",{});var eJr=n(Jhe);Pfr=r(eJr,"electra"),eJr.forEach(t),Sfr=r(VMe," \u2014 "),kN=s(VMe,"A",{href:!0});var oJr=n(kN);$fr=r(oJr,"FlaxElectraForTokenClassification"),oJr.forEach(t),Ifr=r(VMe," (ELECTRA model)"),VMe.forEach(t),Dfr=i(ma),a4=s(ma,"LI",{});var WMe=n(a4);Khe=s(WMe,"STRONG",{});var rJr=n(Khe);Nfr=r(rJr,"roberta"),rJr.forEach(t),jfr=r(WMe," \u2014 "),RN=s(WMe,"A",{href:!0});var tJr=n(RN);Ofr=r(tJr,"FlaxRobertaForTokenClassification"),tJr.forEach(t),Gfr=r(WMe," (RoBERTa model)"),WMe.forEach(t),ma.forEach(t),qfr=i(ca),Yhe=s(ca,"P",{});var aJr=n(Yhe);zfr=r(aJr,"Examples:"),aJr.forEach(t),Xfr=i(ca),f(Kw.$$.fragment,ca),ca.forEach(t),Rl.forEach(t),Dye=i(d),uc=s(d,"H2",{class:!0});var Q7e=n(uc);s4=s(Q7e,"A",{id:!0,class:!0,href:!0});var sJr=n(s4);Zhe=s(sJr,"SPAN",{});var nJr=n(Zhe);f(Yw.$$.fragment,nJr),nJr.forEach(t),sJr.forEach(t),Qfr=i(Q7e),ege=s(Q7e,"SPAN",{});var lJr=n(ege);Vfr=r(lJr,"FlaxAutoModelForMultipleChoice"),lJr.forEach(t),Q7e.forEach(t),Nye=i(d),wr=s(d,"DIV",{class:!0});var Sl=n(wr);f(Zw.$$.fragment,Sl),Wfr=i(Sl),pc=s(Sl,"P",{});var FO=n(pc);Hfr=r(FO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),oge=s(FO,"CODE",{});var iJr=n(oge);Ufr=r(iJr,"from_pretrained()"),iJr.forEach(t),Jfr=r(FO,` class method or the
`),rge=s(FO,"CODE",{});var dJr=n(rge);Kfr=r(dJr,"from_config()"),dJr.forEach(t),Yfr=r(FO," class method."),FO.forEach(t),Zfr=i(Sl),e7=s(Sl,"P",{});var V7e=n(e7);ehr=r(V7e,"This class cannot be instantiated directly using "),tge=s(V7e,"CODE",{});var cJr=n(tge);ohr=r(cJr,"__init__()"),cJr.forEach(t),rhr=r(V7e," (throws an error)."),V7e.forEach(t),thr=i(Sl),gt=s(Sl,"DIV",{class:!0});var $l=n(gt);f(o7.$$.fragment,$l),ahr=i($l),age=s($l,"P",{});var mJr=n(age);shr=r(mJr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),mJr.forEach(t),nhr=i($l),_c=s($l,"P",{});var EO=n(_c);lhr=r(EO,`Note:
Loading a model from its configuration file does `),sge=s(EO,"STRONG",{});var fJr=n(sge);ihr=r(fJr,"not"),fJr.forEach(t),dhr=r(EO,` load the model weights. It only affects the
model\u2019s configuration. Use `),nge=s(EO,"CODE",{});var hJr=n(nge);chr=r(hJr,"from_pretrained()"),hJr.forEach(t),mhr=r(EO,` to load the model
weights.`),EO.forEach(t),fhr=i($l),lge=s($l,"P",{});var gJr=n(lge);hhr=r(gJr,"Examples:"),gJr.forEach(t),ghr=i($l),f(r7.$$.fragment,$l),$l.forEach(t),uhr=i(Sl),Ao=s(Sl,"DIV",{class:!0});var fa=n(Ao);f(t7.$$.fragment,fa),phr=i(fa),ige=s(fa,"P",{});var uJr=n(ige);_hr=r(uJr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),uJr.forEach(t),bhr=i(fa),ds=s(fa,"P",{});var fC=n(ds);vhr=r(fC,"The model class to instantiate is selected based on the "),dge=s(fC,"CODE",{});var pJr=n(dge);Thr=r(pJr,"model_type"),pJr.forEach(t),Fhr=r(fC,` property of the config object (either
passed as an argument or loaded from `),cge=s(fC,"CODE",{});var _Jr=n(cge);Ehr=r(_Jr,"pretrained_model_name_or_path"),_Jr.forEach(t),Chr=r(fC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),mge=s(fC,"CODE",{});var bJr=n(mge);Mhr=r(bJr,"pretrained_model_name_or_path"),bJr.forEach(t),yhr=r(fC,":"),fC.forEach(t),whr=i(fa),Ar=s(fa,"UL",{});var ha=n(Ar);n4=s(ha,"LI",{});var HMe=n(n4);fge=s(HMe,"STRONG",{});var vJr=n(fge);Ahr=r(vJr,"albert"),vJr.forEach(t),Lhr=r(HMe," \u2014 "),PN=s(HMe,"A",{href:!0});var TJr=n(PN);Bhr=r(TJr,"FlaxAlbertForMultipleChoice"),TJr.forEach(t),xhr=r(HMe," (ALBERT model)"),HMe.forEach(t),khr=i(ha),l4=s(ha,"LI",{});var UMe=n(l4);hge=s(UMe,"STRONG",{});var FJr=n(hge);Rhr=r(FJr,"bert"),FJr.forEach(t),Phr=r(UMe," \u2014 "),SN=s(UMe,"A",{href:!0});var EJr=n(SN);Shr=r(EJr,"FlaxBertForMultipleChoice"),EJr.forEach(t),$hr=r(UMe," (BERT model)"),UMe.forEach(t),Ihr=i(ha),i4=s(ha,"LI",{});var JMe=n(i4);gge=s(JMe,"STRONG",{});var CJr=n(gge);Dhr=r(CJr,"big_bird"),CJr.forEach(t),Nhr=r(JMe," \u2014 "),$N=s(JMe,"A",{href:!0});var MJr=n($N);jhr=r(MJr,"FlaxBigBirdForMultipleChoice"),MJr.forEach(t),Ohr=r(JMe," (BigBird model)"),JMe.forEach(t),Ghr=i(ha),d4=s(ha,"LI",{});var KMe=n(d4);uge=s(KMe,"STRONG",{});var yJr=n(uge);qhr=r(yJr,"distilbert"),yJr.forEach(t),zhr=r(KMe," \u2014 "),IN=s(KMe,"A",{href:!0});var wJr=n(IN);Xhr=r(wJr,"FlaxDistilBertForMultipleChoice"),wJr.forEach(t),Qhr=r(KMe," (DistilBERT model)"),KMe.forEach(t),Vhr=i(ha),c4=s(ha,"LI",{});var YMe=n(c4);pge=s(YMe,"STRONG",{});var AJr=n(pge);Whr=r(AJr,"electra"),AJr.forEach(t),Hhr=r(YMe," \u2014 "),DN=s(YMe,"A",{href:!0});var LJr=n(DN);Uhr=r(LJr,"FlaxElectraForMultipleChoice"),LJr.forEach(t),Jhr=r(YMe," (ELECTRA model)"),YMe.forEach(t),Khr=i(ha),m4=s(ha,"LI",{});var ZMe=n(m4);_ge=s(ZMe,"STRONG",{});var BJr=n(_ge);Yhr=r(BJr,"roberta"),BJr.forEach(t),Zhr=r(ZMe," \u2014 "),NN=s(ZMe,"A",{href:!0});var xJr=n(NN);egr=r(xJr,"FlaxRobertaForMultipleChoice"),xJr.forEach(t),ogr=r(ZMe," (RoBERTa model)"),ZMe.forEach(t),ha.forEach(t),rgr=i(fa),bge=s(fa,"P",{});var kJr=n(bge);tgr=r(kJr,"Examples:"),kJr.forEach(t),agr=i(fa),f(a7.$$.fragment,fa),fa.forEach(t),Sl.forEach(t),jye=i(d),bc=s(d,"H2",{class:!0});var W7e=n(bc);f4=s(W7e,"A",{id:!0,class:!0,href:!0});var RJr=n(f4);vge=s(RJr,"SPAN",{});var PJr=n(vge);f(s7.$$.fragment,PJr),PJr.forEach(t),RJr.forEach(t),sgr=i(W7e),Tge=s(W7e,"SPAN",{});var SJr=n(Tge);ngr=r(SJr,"FlaxAutoModelForNextSentencePrediction"),SJr.forEach(t),W7e.forEach(t),Oye=i(d),Lr=s(d,"DIV",{class:!0});var Il=n(Lr);f(n7.$$.fragment,Il),lgr=i(Il),vc=s(Il,"P",{});var CO=n(vc);igr=r(CO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Fge=s(CO,"CODE",{});var $Jr=n(Fge);dgr=r($Jr,"from_pretrained()"),$Jr.forEach(t),cgr=r(CO,` class method or the
`),Ege=s(CO,"CODE",{});var IJr=n(Ege);mgr=r(IJr,"from_config()"),IJr.forEach(t),fgr=r(CO," class method."),CO.forEach(t),hgr=i(Il),l7=s(Il,"P",{});var H7e=n(l7);ggr=r(H7e,"This class cannot be instantiated directly using "),Cge=s(H7e,"CODE",{});var DJr=n(Cge);ugr=r(DJr,"__init__()"),DJr.forEach(t),pgr=r(H7e," (throws an error)."),H7e.forEach(t),_gr=i(Il),ut=s(Il,"DIV",{class:!0});var Dl=n(ut);f(i7.$$.fragment,Dl),bgr=i(Dl),Mge=s(Dl,"P",{});var NJr=n(Mge);vgr=r(NJr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),NJr.forEach(t),Tgr=i(Dl),Tc=s(Dl,"P",{});var MO=n(Tc);Fgr=r(MO,`Note:
Loading a model from its configuration file does `),yge=s(MO,"STRONG",{});var jJr=n(yge);Egr=r(jJr,"not"),jJr.forEach(t),Cgr=r(MO,` load the model weights. It only affects the
model\u2019s configuration. Use `),wge=s(MO,"CODE",{});var OJr=n(wge);Mgr=r(OJr,"from_pretrained()"),OJr.forEach(t),ygr=r(MO,` to load the model
weights.`),MO.forEach(t),wgr=i(Dl),Age=s(Dl,"P",{});var GJr=n(Age);Agr=r(GJr,"Examples:"),GJr.forEach(t),Lgr=i(Dl),f(d7.$$.fragment,Dl),Dl.forEach(t),Bgr=i(Il),Lo=s(Il,"DIV",{class:!0});var ga=n(Lo);f(c7.$$.fragment,ga),xgr=i(ga),Lge=s(ga,"P",{});var qJr=n(Lge);kgr=r(qJr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),qJr.forEach(t),Rgr=i(ga),cs=s(ga,"P",{});var hC=n(cs);Pgr=r(hC,"The model class to instantiate is selected based on the "),Bge=s(hC,"CODE",{});var zJr=n(Bge);Sgr=r(zJr,"model_type"),zJr.forEach(t),$gr=r(hC,` property of the config object (either
passed as an argument or loaded from `),xge=s(hC,"CODE",{});var XJr=n(xge);Igr=r(XJr,"pretrained_model_name_or_path"),XJr.forEach(t),Dgr=r(hC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),kge=s(hC,"CODE",{});var QJr=n(kge);Ngr=r(QJr,"pretrained_model_name_or_path"),QJr.forEach(t),jgr=r(hC,":"),hC.forEach(t),Ogr=i(ga),Rge=s(ga,"UL",{});var VJr=n(Rge);h4=s(VJr,"LI",{});var e5e=n(h4);Pge=s(e5e,"STRONG",{});var WJr=n(Pge);Ggr=r(WJr,"bert"),WJr.forEach(t),qgr=r(e5e," \u2014 "),jN=s(e5e,"A",{href:!0});var HJr=n(jN);zgr=r(HJr,"FlaxBertForNextSentencePrediction"),HJr.forEach(t),Xgr=r(e5e," (BERT model)"),e5e.forEach(t),VJr.forEach(t),Qgr=i(ga),Sge=s(ga,"P",{});var UJr=n(Sge);Vgr=r(UJr,"Examples:"),UJr.forEach(t),Wgr=i(ga),f(m7.$$.fragment,ga),ga.forEach(t),Il.forEach(t),Gye=i(d),Fc=s(d,"H2",{class:!0});var U7e=n(Fc);g4=s(U7e,"A",{id:!0,class:!0,href:!0});var JJr=n(g4);$ge=s(JJr,"SPAN",{});var KJr=n($ge);f(f7.$$.fragment,KJr),KJr.forEach(t),JJr.forEach(t),Hgr=i(U7e),Ige=s(U7e,"SPAN",{});var YJr=n(Ige);Ugr=r(YJr,"FlaxAutoModelForImageClassification"),YJr.forEach(t),U7e.forEach(t),qye=i(d),Br=s(d,"DIV",{class:!0});var Nl=n(Br);f(h7.$$.fragment,Nl),Jgr=i(Nl),Ec=s(Nl,"P",{});var yO=n(Ec);Kgr=r(yO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Dge=s(yO,"CODE",{});var ZJr=n(Dge);Ygr=r(ZJr,"from_pretrained()"),ZJr.forEach(t),Zgr=r(yO,` class method or the
`),Nge=s(yO,"CODE",{});var eKr=n(Nge);eur=r(eKr,"from_config()"),eKr.forEach(t),our=r(yO," class method."),yO.forEach(t),rur=i(Nl),g7=s(Nl,"P",{});var J7e=n(g7);tur=r(J7e,"This class cannot be instantiated directly using "),jge=s(J7e,"CODE",{});var oKr=n(jge);aur=r(oKr,"__init__()"),oKr.forEach(t),sur=r(J7e," (throws an error)."),J7e.forEach(t),nur=i(Nl),pt=s(Nl,"DIV",{class:!0});var jl=n(pt);f(u7.$$.fragment,jl),lur=i(jl),Oge=s(jl,"P",{});var rKr=n(Oge);iur=r(rKr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),rKr.forEach(t),dur=i(jl),Cc=s(jl,"P",{});var wO=n(Cc);cur=r(wO,`Note:
Loading a model from its configuration file does `),Gge=s(wO,"STRONG",{});var tKr=n(Gge);mur=r(tKr,"not"),tKr.forEach(t),fur=r(wO,` load the model weights. It only affects the
model\u2019s configuration. Use `),qge=s(wO,"CODE",{});var aKr=n(qge);hur=r(aKr,"from_pretrained()"),aKr.forEach(t),gur=r(wO,` to load the model
weights.`),wO.forEach(t),uur=i(jl),zge=s(jl,"P",{});var sKr=n(zge);pur=r(sKr,"Examples:"),sKr.forEach(t),_ur=i(jl),f(p7.$$.fragment,jl),jl.forEach(t),bur=i(Nl),Bo=s(Nl,"DIV",{class:!0});var ua=n(Bo);f(_7.$$.fragment,ua),vur=i(ua),Xge=s(ua,"P",{});var nKr=n(Xge);Tur=r(nKr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),nKr.forEach(t),Fur=i(ua),ms=s(ua,"P",{});var gC=n(ms);Eur=r(gC,"The model class to instantiate is selected based on the "),Qge=s(gC,"CODE",{});var lKr=n(Qge);Cur=r(lKr,"model_type"),lKr.forEach(t),Mur=r(gC,` property of the config object (either
passed as an argument or loaded from `),Vge=s(gC,"CODE",{});var iKr=n(Vge);yur=r(iKr,"pretrained_model_name_or_path"),iKr.forEach(t),wur=r(gC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),Wge=s(gC,"CODE",{});var dKr=n(Wge);Aur=r(dKr,"pretrained_model_name_or_path"),dKr.forEach(t),Lur=r(gC,":"),gC.forEach(t),Bur=i(ua),b7=s(ua,"UL",{});var K7e=n(b7);u4=s(K7e,"LI",{});var o5e=n(u4);Hge=s(o5e,"STRONG",{});var cKr=n(Hge);xur=r(cKr,"beit"),cKr.forEach(t),kur=r(o5e," \u2014 "),ON=s(o5e,"A",{href:!0});var mKr=n(ON);Rur=r(mKr,"FlaxBeitForImageClassification"),mKr.forEach(t),Pur=r(o5e," (BEiT model)"),o5e.forEach(t),Sur=i(K7e),p4=s(K7e,"LI",{});var r5e=n(p4);Uge=s(r5e,"STRONG",{});var fKr=n(Uge);$ur=r(fKr,"vit"),fKr.forEach(t),Iur=r(r5e," \u2014 "),GN=s(r5e,"A",{href:!0});var hKr=n(GN);Dur=r(hKr,"FlaxViTForImageClassification"),hKr.forEach(t),Nur=r(r5e," (ViT model)"),r5e.forEach(t),K7e.forEach(t),jur=i(ua),Jge=s(ua,"P",{});var gKr=n(Jge);Our=r(gKr,"Examples:"),gKr.forEach(t),Gur=i(ua),f(v7.$$.fragment,ua),ua.forEach(t),Nl.forEach(t),zye=i(d),Mc=s(d,"H2",{class:!0});var Y7e=n(Mc);_4=s(Y7e,"A",{id:!0,class:!0,href:!0});var uKr=n(_4);Kge=s(uKr,"SPAN",{});var pKr=n(Kge);f(T7.$$.fragment,pKr),pKr.forEach(t),uKr.forEach(t),qur=i(Y7e),Yge=s(Y7e,"SPAN",{});var _Kr=n(Yge);zur=r(_Kr,"FlaxAutoModelForVision2Seq"),_Kr.forEach(t),Y7e.forEach(t),Xye=i(d),xr=s(d,"DIV",{class:!0});var Ol=n(xr);f(F7.$$.fragment,Ol),Xur=i(Ol),yc=s(Ol,"P",{});var AO=n(yc);Qur=r(AO,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),Zge=s(AO,"CODE",{});var bKr=n(Zge);Vur=r(bKr,"from_pretrained()"),bKr.forEach(t),Wur=r(AO,` class method or the
`),eue=s(AO,"CODE",{});var vKr=n(eue);Hur=r(vKr,"from_config()"),vKr.forEach(t),Uur=r(AO," class method."),AO.forEach(t),Jur=i(Ol),E7=s(Ol,"P",{});var Z7e=n(E7);Kur=r(Z7e,"This class cannot be instantiated directly using "),oue=s(Z7e,"CODE",{});var TKr=n(oue);Yur=r(TKr,"__init__()"),TKr.forEach(t),Zur=r(Z7e," (throws an error)."),Z7e.forEach(t),epr=i(Ol),_t=s(Ol,"DIV",{class:!0});var Gl=n(_t);f(C7.$$.fragment,Gl),opr=i(Gl),rue=s(Gl,"P",{});var FKr=n(rue);rpr=r(FKr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),FKr.forEach(t),tpr=i(Gl),wc=s(Gl,"P",{});var LO=n(wc);apr=r(LO,`Note:
Loading a model from its configuration file does `),tue=s(LO,"STRONG",{});var EKr=n(tue);spr=r(EKr,"not"),EKr.forEach(t),npr=r(LO,` load the model weights. It only affects the
model\u2019s configuration. Use `),aue=s(LO,"CODE",{});var CKr=n(aue);lpr=r(CKr,"from_pretrained()"),CKr.forEach(t),ipr=r(LO,` to load the model
weights.`),LO.forEach(t),dpr=i(Gl),sue=s(Gl,"P",{});var MKr=n(sue);cpr=r(MKr,"Examples:"),MKr.forEach(t),mpr=i(Gl),f(M7.$$.fragment,Gl),Gl.forEach(t),fpr=i(Ol),xo=s(Ol,"DIV",{class:!0});var pa=n(xo);f(y7.$$.fragment,pa),hpr=i(pa),nue=s(pa,"P",{});var yKr=n(nue);gpr=r(yKr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),yKr.forEach(t),upr=i(pa),fs=s(pa,"P",{});var uC=n(fs);ppr=r(uC,"The model class to instantiate is selected based on the "),lue=s(uC,"CODE",{});var wKr=n(lue);_pr=r(wKr,"model_type"),wKr.forEach(t),bpr=r(uC,` property of the config object (either
passed as an argument or loaded from `),iue=s(uC,"CODE",{});var AKr=n(iue);vpr=r(AKr,"pretrained_model_name_or_path"),AKr.forEach(t),Tpr=r(uC,` if possible), or when it\u2019s missing,
by falling back to using pattern matching on `),due=s(uC,"CODE",{});var LKr=n(due);Fpr=r(LKr,"pretrained_model_name_or_path"),LKr.forEach(t),Epr=r(uC,":"),uC.forEach(t),Cpr=i(pa),cue=s(pa,"UL",{});var BKr=n(cue);b4=s(BKr,"LI",{});var t5e=n(b4);mue=s(t5e,"STRONG",{});var xKr=n(mue);Mpr=r(xKr,"vision-encoder-decoder"),xKr.forEach(t),ypr=r(t5e," \u2014 "),qN=s(t5e,"A",{href:!0});var kKr=n(qN);wpr=r(kKr,"FlaxVisionEncoderDecoderModel"),kKr.forEach(t),Apr=r(t5e," (Vision Encoder decoder model)"),t5e.forEach(t),BKr.forEach(t),Lpr=i(pa),fue=s(pa,"P",{});var RKr=n(fue);Bpr=r(RKr,"Examples:"),RKr.forEach(t),xpr=i(pa),f(w7.$$.fragment,pa),pa.forEach(t),Ol.forEach(t),this.h()},h(){c(re,"name","hf:doc:metadata"),c(re,"content",JSON.stringify(GKr)),c(ue,"id","auto-classes"),c(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ue,"href","#auto-classes"),c(fe,"class","relative group"),c(hs,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig"),c(us,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),c(ps,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),c(Ul,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),c(Rc,"id","extending-the-auto-classes"),c(Rc,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rc,"href","#extending-the-auto-classes"),c(Jl,"class","relative group"),c(Sc,"id","transformers.AutoConfig"),c(Sc,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Sc,"href","#transformers.AutoConfig"),c(Kl,"class","relative group"),c(w0,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),c(A0,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig"),c(L0,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartConfig"),c(B0,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig"),c(x0,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertConfig"),c(k0,"href","/docs/transformers/master/en/model_doc/bertgeneration#transformers.BertGenerationConfig"),c(R0,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdConfig"),c(P0,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),c(S0,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig"),c($0,"href","/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallConfig"),c(I0,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig"),c(D0,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig"),c(N0,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig"),c(j0,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig"),c(O0,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig"),c(G0,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig"),c(q0,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Config"),c(z0,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig"),c(X0,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig"),c(Q0,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig"),c(V0,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig"),c(W0,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig"),c(H0,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderConfig"),c(U0,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig"),c(J0,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig"),c(K0,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig"),c(Y0,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"),c(Z0,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config"),c(eA,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),c(oA,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig"),c(rA,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig"),c(tA,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig"),c(aA,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig"),c(sA,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig"),c(nA,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),c(lA,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDConfig"),c(iA,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig"),c(dA,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig"),c(cA,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig"),c(mA,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config"),c(fA,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig"),c(hA,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig"),c(gA,"href","/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertConfig"),c(uA,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig"),c(pA,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig"),c(_A,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config"),c(bA,"href","/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTConfig"),c(vA,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig"),c(TA,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"),c(FA,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig"),c(EA,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig"),c(CA,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagConfig"),c(MA,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig"),c(yA,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig"),c(wA,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig"),c(AA,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig"),c(LA,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig"),c(BA,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig"),c(xA,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig"),c(kA,"href","/docs/transformers/master/en/model_doc/sew_d#transformers.SEWDConfig"),c(RA,"href","/docs/transformers/master/en/model_doc/speechencoderdecoder#transformers.SpeechEncoderDecoderConfig"),c(PA,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),c(SA,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),c($A,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig"),c(IA,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),c(DA,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Config"),c(NA,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig"),c(jA,"href","/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLConfig"),c(OA,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig"),c(GA,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"),c(qA,"href","/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatConfig"),c(zA,"href","/docs/transformers/master/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderConfig"),c(XA,"href","/docs/transformers/master/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderConfig"),c(QA,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig"),c(VA,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig"),c(WA,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),c(HA,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig"),c(UA,"href","/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetConfig"),c(JA,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaConfig"),c(KA,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig"),c(no,"class","docstring"),c(nf,"class","docstring"),c(Do,"class","docstring"),c(lf,"id","transformers.AutoTokenizer"),c(lf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lf,"href","#transformers.AutoTokenizer"),c(Zl,"class","relative group"),c(YA,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),c(ZA,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizer"),c(e6,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizerFast"),c(o6,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizer"),c(r6,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizerFast"),c(t6,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizer"),c(a6,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizerFast"),c(s6,"href","/docs/transformers/master/en/model_doc/bartpho#transformers.BartphoTokenizer"),c(n6,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),c(l6,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),c(i6,"href","/docs/transformers/master/en/model_doc/bertgeneration#transformers.BertGenerationTokenizer"),c(d6,"href","/docs/transformers/master/en/model_doc/bert_japanese#transformers.BertJapaneseTokenizer"),c(c6,"href","/docs/transformers/master/en/model_doc/bertweet#transformers.BertweetTokenizer"),c(m6,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdTokenizer"),c(f6,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdTokenizerFast"),c(h6,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(g6,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(u6,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),c(p6,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),c(_6,"href","/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallTokenizer"),c(b6,"href","/docs/transformers/master/en/model_doc/byt5#transformers.ByT5Tokenizer"),c(v6,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizer"),c(T6,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizerFast"),c(F6,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineTokenizer"),c(E6,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizer"),c(C6,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizerFast"),c(M6,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizer"),c(y6,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),c(w6,"href","/docs/transformers/master/en/model_doc/cpm#transformers.CpmTokenizer"),c(A6,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLTokenizer"),c(L6,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizer"),c(B6,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizerFast"),c(x6,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Tokenizer"),c(k6,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(R6,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(P6,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),c(S6,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),c($6,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizer"),c(I6,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizerFast"),c(D6,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertTokenizer"),c(N6,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizer"),c(j6,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizerFast"),c(O6,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTTokenizer"),c(G6,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer"),c(q6,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizerFast"),c(z6,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(X6,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(Q6,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),c(V6,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),c(W6,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(H6,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),c(U6,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(J6,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),c(K6,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),c(Y6,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),c(Z6,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),c(eL,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizer"),c(oL,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizerFast"),c(rL,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizer"),c(tL,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizerFast"),c(aL,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeTokenizer"),c(sL,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizer"),c(nL,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),c(lL,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),c(iL,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianTokenizer"),c(dL,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizer"),c(cL,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizerFast"),c(mL,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50Tokenizer"),c(fL,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50TokenizerFast"),c(hL,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),c(gL,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),c(uL,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizer"),c(pL,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),c(_L,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),c(bL,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),c(vL,"href","/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTTokenizer"),c(TL,"href","/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTTokenizerFast"),c(FL,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),c(EL,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),c(CL,"href","/docs/transformers/master/en/model_doc/phobert#transformers.PhobertTokenizer"),c(ML,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),c(yL,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),c(wL,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),c(AL,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagTokenizer"),c(LL,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizer"),c(BL,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizerFast"),c(xL,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizer"),c(kL,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizerFast"),c(RL,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizer"),c(PL,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),c(SL,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),c($L,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),c(IL,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizer"),c(DL,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),c(NL,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),c(jL,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),c(OL,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizer"),c(GL,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizerFast"),c(qL,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),c(zL,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),c(XL,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),c(QL,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),c(VL,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasTokenizer"),c(WL,"href","/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLTokenizer"),c(HL,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),c(UL,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMTokenizer"),c(JL,"href","/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetTokenizer"),c(KL,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaTokenizer"),c(YL,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaTokenizerFast"),c(ZL,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizer"),c(e8,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),c(o8,"href","/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained"),c(r8,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(ye,"class","docstring"),c(Sf,"class","docstring"),c(No,"class","docstring"),c($f,"id","transformers.AutoFeatureExtractor"),c($f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($f,"href","#transformers.AutoFeatureExtractor"),c(ti,"class","relative group"),c(t8,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),c(a8,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitFeatureExtractor"),c(s8,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPFeatureExtractor"),c(n8,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTFeatureExtractor"),c(l8,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrFeatureExtractor"),c(i8,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(d8,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),c(c8,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),c(m8,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),c(f8,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),c(h8,"href","/docs/transformers/master/en/main_classes/feature_extractor#transformers.feature_extraction_utils.FeatureExtractionMixin.save_pretrained"),c(Te,"class","docstring"),c(Dt,"class","docstring"),c(Vf,"id","transformers.AutoProcessor"),c(Vf,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Vf,"href","#transformers.AutoProcessor"),c(li,"class","relative group"),c(g8,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),c(u8,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPProcessor"),c(p8,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),c(_8,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),c(b8,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),c(v8,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRProcessor"),c(T8,"href","/docs/transformers/master/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderProcessor"),c(F8,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),c(Fe,"class","docstring"),c(Nt,"class","docstring"),c(oh,"id","transformers.AutoModel"),c(oh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(oh,"href","#transformers.AutoModel"),c(mi,"class","relative group"),c(kr,"class","docstring"),c(E8,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel"),c(C8,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartModel"),c(M8,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitModel"),c(y8,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),c(w8,"href","/docs/transformers/master/en/model_doc/bertgeneration#transformers.BertGenerationEncoder"),c(A8,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdModel"),c(L8,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),c(B8,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel"),c(x8,"href","/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallModel"),c(k8,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel"),c(R8,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineModel"),c(P8,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel"),c(S8,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel"),c($8,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel"),c(I8,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel"),c(D8,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2Model"),c(N8,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel"),c(j8,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrModel"),c(O8,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel"),c(G8,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder"),c(q8,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel"),c(z8,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel"),c(X8,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel"),c(Q8,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel"),c(V8,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel"),c(W8,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel"),c(H8,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model"),c(U8,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel"),c(J8,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel"),c(K8,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel"),c(Y8,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel"),c(Z8,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel"),c(eB,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel"),c(oB,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),c(rB,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDModel"),c(tB,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel"),c(aB,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeModel"),c(sB,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel"),c(nB,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model"),c(lB,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianModel"),c(iB,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel"),c(dB,"href","/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertModel"),c(cB,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel"),c(mB,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel"),c(fB,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model"),c(hB,"href","/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTModel"),c(gB,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel"),c(uB,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel"),c(pB,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),c(_B,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel"),c(bB,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel"),c(vB,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel"),c(TB,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),c(FB,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel"),c(EB,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel"),c(CB,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel"),c(MB,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWModel"),c(yB,"href","/docs/transformers/master/en/model_doc/sew_d#transformers.SEWDModel"),c(wB,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel"),c(AB,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel"),c(LB,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel"),c(BB,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Model"),c(xB,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel"),c(kB,"href","/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLModel"),c(RB,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel"),c(PB,"href","/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatModel"),c(SB,"href","/docs/transformers/master/en/model_doc/vision_text_dual_encoder#transformers.VisionTextDualEncoderModel"),c($B,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel"),c(IB,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTModel"),c(DB,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),c(NB,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel"),c(jB,"href","/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetModel"),c(OB,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaModel"),c(GB,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel"),c(Se,"class","docstring"),c(jo,"class","docstring"),c(Fg,"id","transformers.AutoModelForPreTraining"),c(Fg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fg,"href","#transformers.AutoModelForPreTraining"),c(gi,"class","relative group"),c(Rr,"class","docstring"),c(qB,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining"),c(zB,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(XB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining"),c(QB,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForPreTraining"),c(VB,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(WB,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(HB,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(UB,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2ForMaskedLM"),c(JB,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(KB,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining"),c(YB,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(ZB,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining"),c(e9,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(o9,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),c(r9,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(t9,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(a9,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(s9,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(n9,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining"),c(l9,"href","/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForPreTraining"),c(i9,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),c(d9,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(c9,"href","/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTLMHeadModel"),c(m9,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),c(f9,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(h9,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(g9,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(u9,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(p9,"href","/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLLMHeadModel"),c(_9,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),c(b9,"href","/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatForPreTraining"),c(v9,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),c(T9,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),c(F9,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(E9,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForMaskedLM"),c(C9,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c($e,"class","docstring"),c(Oo,"class","docstring"),c(su,"id","transformers.AutoModelForCausalLM"),c(su,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(su,"href","#transformers.AutoModelForCausalLM"),c(_i,"class","relative group"),c(Pr,"class","docstring"),c(M9,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM"),c(y9,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel"),c(w9,"href","/docs/transformers/master/en/model_doc/bertgeneration#transformers.BertGenerationDecoder"),c(A9,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForCausalLM"),c(L9,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),c(B9,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),c(x9,"href","/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallForCausalLM"),c(k9,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM"),c(R9,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),c(P9,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),c(S9,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),c($9,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM"),c(I9,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM"),c(D9,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM"),c(N9,"href","/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForCausalLM"),c(j9,"href","/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTLMHeadModel"),c(O9,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM"),c(G9,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),c(q9,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),c(z9,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),c(X9,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM"),c(Q9,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM"),c(V9,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM"),c(W9,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),c(H9,"href","/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLLMHeadModel"),c(U9,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM"),c(J9,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(K9,"href","/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetForCausalLM"),c(Y9,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForCausalLM"),c(Z9,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),c(Ie,"class","docstring"),c(Go,"class","docstring"),c(Du,"id","transformers.AutoModelForMaskedLM"),c(Du,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Du,"href","#transformers.AutoModelForMaskedLM"),c(Ti,"class","relative group"),c(Sr,"class","docstring"),c(ex,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM"),c(ox,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(rx,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM"),c(tx,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForMaskedLM"),c(ax,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),c(sx,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),c(nx,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),c(lx,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2ForMaskedLM"),c(ix,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(dx,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM"),c(cx,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),c(mx,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM"),c(fx,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM"),c(hx,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),c(gx,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),c(ux,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),c(px,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(_x,"href","/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForMaskedLM"),c(bx,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),c(vx,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),c(Tx,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),c(Fx,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),c(Ex,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM"),c(Cx,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM"),c(Mx,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),c(yx,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),c(wx,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),c(Ax,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),c(Lx,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),c(Bx,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForMaskedLM"),c(De,"class","docstring"),c(qo,"class","docstring"),c(pp,"id","transformers.AutoModelForSeq2SeqLM"),c(pp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pp,"href","#transformers.AutoModelForSeq2SeqLM"),c(Ci,"class","relative group"),c($r,"class","docstring"),c(xx,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),c(kx,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),c(Rx,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),c(Px,"href","/docs/transformers/master/en/model_doc/blenderbot_small#transformers.BlenderbotSmallForConditionalGeneration"),c(Sx,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.EncoderDecoderModel"),c($x,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),c(Ix,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration"),c(Dx,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),c(Nx,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel"),c(jx,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),c(Ox,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),c(Gx,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),c(qx,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),c(zx,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),c(Xx,"href","/docs/transformers/master/en/model_doc/xlmprophetnet#transformers.XLMProphetNetForConditionalGeneration"),c(Ne,"class","docstring"),c(zo,"class","docstring"),c(Pp,"id","transformers.AutoModelForSequenceClassification"),c(Pp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pp,"href","#transformers.AutoModelForSequenceClassification"),c(wi,"class","relative group"),c(Ir,"class","docstring"),c(Qx,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification"),c(Vx,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification"),c(Wx,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification"),c(Hx,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForSequenceClassification"),c(Ux,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),c(Jx,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),c(Kx,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification"),c(Yx,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),c(Zx,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),c(ek,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),c(ok,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2ForSequenceClassification"),c(rk,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(tk,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification"),c(ak,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),c(sk,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification"),c(nk,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),c(lk,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),c(ik,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),c(dk,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),c(ck,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification"),c(mk,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),c(fk,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),c(hk,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification"),c(gk,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),c(uk,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification"),c(pk,"href","/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForSequenceClassification"),c(_k,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),c(bk,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),c(vk,"href","/docs/transformers/master/en/model_doc/gpt#transformers.OpenAIGPTForSequenceClassification"),c(Tk,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),c(Fk,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),c(Ek,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),c(Ck,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),c(Mk,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),c(yk,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),c(wk,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),c(Ak,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification"),c(Lk,"href","/docs/transformers/master/en/model_doc/transformerxl#transformers.TransfoXLForSequenceClassification"),c(Bk,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification"),c(xk,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForSequenceClassification"),c(kk,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),c(je,"class","docstring"),c(Xo,"class","docstring"),c(E_,"id","transformers.AutoModelForMultipleChoice"),c(E_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(E_,"href","#transformers.AutoModelForMultipleChoice"),c(Bi,"class","relative group"),c(Dr,"class","docstring"),c(Rk,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice"),c(Pk,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice"),c(Sk,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForMultipleChoice"),c($k,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),c(Ik,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice"),c(Dk,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),c(Nk,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(jk,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice"),c(Ok,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),c(Gk,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice"),c(qk,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),c(zk,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice"),c(Xk,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),c(Qk,"href","/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForMultipleChoice"),c(Vk,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),c(Wk,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),c(Hk,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),c(Uk,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),c(Jk,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),c(Kk,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),c(Yk,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),c(Zk,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice"),c(eR,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForMultipleChoice"),c(oR,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),c(Oe,"class","docstring"),c(Qo,"class","docstring"),c(H_,"id","transformers.AutoModelForNextSentencePrediction"),c(H_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(H_,"href","#transformers.AutoModelForNextSentencePrediction"),c(Ri,"class","relative group"),c(Nr,"class","docstring"),c(rR,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction"),c(tR,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),c(aR,"href","/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForNextSentencePrediction"),c(sR,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),c(nR,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),c(Ge,"class","docstring"),c(Vo,"class","docstring"),c(ob,"id","transformers.AutoModelForTokenClassification"),c(ob,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ob,"href","#transformers.AutoModelForTokenClassification"),c($i,"class","relative group"),c(jr,"class","docstring"),c(lR,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification"),c(iR,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification"),c(dR,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForTokenClassification"),c(cR,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification"),c(mR,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification"),c(fR,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),c(hR,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification"),c(gR,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2ForTokenClassification"),c(uR,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(pR,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification"),c(_R,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),c(bR,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification"),c(vR,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification"),c(TR,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),c(FR,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification"),c(ER,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),c(CR,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),c(MR,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification"),c(yR,"href","/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForTokenClassification"),c(wR,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),c(AR,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),c(LR,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),c(BR,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification"),c(xR,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification"),c(kR,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),c(RR,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),c(PR,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification"),c(SR,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForTokenClassification"),c($R,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),c(qe,"class","docstring"),c(Wo,"class","docstring"),c(Rb,"id","transformers.AutoModelForQuestionAnswering"),c(Rb,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Rb,"href","#transformers.AutoModelForQuestionAnswering"),c(Ni,"class","relative group"),c(Or,"class","docstring"),c(IR,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),c(DR,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering"),c(NR,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering"),c(jR,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.BigBirdForQuestionAnswering"),c(OR,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),c(GR,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),c(qR,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering"),c(zR,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),c(XR,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),c(QR,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.DebertaV2ForQuestionAnswering"),c(VR,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(WR,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),c(HR,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),c(UR,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),c(JR,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),c(KR,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),c(YR,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),c(ZR,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),c(eP,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering"),c(oP,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),c(rP,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),c(tP,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),c(aP,"href","/docs/transformers/master/en/model_doc/megatron_bert#transformers.MegatronBertForQuestionAnswering"),c(sP,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),c(nP,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),c(lP,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),c(iP,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),c(dP,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),c(cP,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),c(mP,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),c(fP,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),c(hP,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),c(gP,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),c(uP,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.XLMRobertaForQuestionAnswering"),c(pP,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),c(ze,"class","docstring"),c(Ho,"class","docstring"),c(u2,"id","transformers.AutoModelForTableQuestionAnswering"),c(u2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(u2,"href","#transformers.AutoModelForTableQuestionAnswering"),c(Gi,"class","relative group"),c(Gr,"class","docstring"),c(_P,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),c(Xe,"class","docstring"),c(Uo,"class","docstring"),c(b2,"id","transformers.AutoModelForImageClassification"),c(b2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b2,"href","#transformers.AutoModelForImageClassification"),c(Xi,"class","relative group"),c(qr,"class","docstring"),c(bP,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification"),c(vP,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification"),c(TP,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),c(FP,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),c(EP,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),c(CP,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),c(MP,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),c(yP,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification"),c(wP,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification"),c(Qe,"class","docstring"),c(Jo,"class","docstring"),c(M2,"id","transformers.AutoModelForVision2Seq"),c(M2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M2,"href","#transformers.AutoModelForVision2Seq"),c(Wi,"class","relative group"),c(zr,"class","docstring"),c(AP,"href","/docs/transformers/master/en/model_doc/visionencoderdecoder#transformers.VisionEncoderDecoderModel"),c(Ve,"class","docstring"),c(Yo,"class","docstring"),c(A2,"id","transformers.AutoModelForAudioClassification"),c(A2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A2,"href","#transformers.AutoModelForAudioClassification"),c(Ji,"class","relative group"),c(Xr,"class","docstring"),c(LP,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification"),c(BP,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification"),c(xP,"href","/docs/transformers/master/en/model_doc/sew_d#transformers.SEWDForSequenceClassification"),c(kP,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),c(RP,"href","/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatForSequenceClassification"),c(PP,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),c(We,"class","docstring"),c(Zo,"class","docstring"),c($2,"id","transformers.AutoModelForCTC"),c($2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c($2,"href","#transformers.AutoModelForCTC"),c(Zi,"class","relative group"),c(Qr,"class","docstring"),c(SP,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC"),c($P,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC"),c(IP,"href","/docs/transformers/master/en/model_doc/sew_d#transformers.SEWDForCTC"),c(DP,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC"),c(NP,"href","/docs/transformers/master/en/model_doc/unispeech_sat#transformers.UniSpeechSatForCTC"),c(jP,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),c(He,"class","docstring"),c(or,"class","docstring"),c(z2,"id","transformers.AutoModelForSpeechSeq2Seq"),c(z2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(z2,"href","#transformers.AutoModelForSpeechSeq2Seq"),c(rd,"class","relative group"),c(Vr,"class","docstring"),c(OP,"href","/docs/transformers/master/en/model_doc/speechencoderdecoder#transformers.SpeechEncoderDecoderModel"),c(GP,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),c(Ue,"class","docstring"),c(tr,"class","docstring"),c(W2,"id","transformers.AutoModelForObjectDetection"),c(W2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(W2,"href","#transformers.AutoModelForObjectDetection"),c(sd,"class","relative group"),c(Wr,"class","docstring"),c(qP,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection"),c(Je,"class","docstring"),c(ar,"class","docstring"),c(J2,"id","transformers.AutoModelForImageSegmentation"),c(J2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(J2,"href","#transformers.AutoModelForImageSegmentation"),c(id,"class","relative group"),c(Hr,"class","docstring"),c(zP,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation"),c(Ke,"class","docstring"),c(sr,"class","docstring"),c(Z2,"id","transformers.TFAutoModel"),c(Z2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Z2,"href","#transformers.TFAutoModel"),c(md,"class","relative group"),c(Ur,"class","docstring"),c(XP,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel"),c(QP,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel"),c(VP,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel"),c(WP,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),c(HP,"href","/docs/transformers/master/en/model_doc/blenderbot_small#transformers.TFBlenderbotSmallModel"),c(UP,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel"),c(JP,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel"),c(KP,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel"),c(YP,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel"),c(ZP,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.TFDebertaV2Model"),c(eS,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(oS,"href","/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),c(rS,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel"),c(tS,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel"),c(aS,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel"),c(sS,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel"),c(nS,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model"),c(lS,"href","/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel"),c(iS,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),c(dS,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel"),c(cS,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel"),c(mS,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel"),c(fS,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel"),c(hS,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel"),c(gS,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel"),c(uS,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel"),c(pS,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model"),c(_S,"href","/docs/transformers/master/en/model_doc/gpt#transformers.TFOpenAIGPTModel"),c(bS,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel"),c(vS,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel"),c(TS,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel"),c(FS,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel"),c(ES,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model"),c(CS,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel"),c(MS,"href","/docs/transformers/master/en/model_doc/transformerxl#transformers.TFTransfoXLModel"),c(yS,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel"),c(wS,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),c(AS,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel"),c(LS,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaModel"),c(BS,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel"),c(lo,"class","docstring"),c(nr,"class","docstring"),c(Nv,"id","transformers.TFAutoModelForPreTraining"),c(Nv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Nv,"href","#transformers.TFAutoModelForPreTraining"),c(gd,"class","relative group"),c(Jr,"class","docstring"),c(xS,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining"),c(kS,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(RS,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining"),c(PS,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(SS,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c($S,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(IS,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining"),c(DS,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(NS,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),c(jS,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(OS,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(GS,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),c(qS,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),c(zS,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(XS,"href","/docs/transformers/master/en/model_doc/gpt#transformers.TFOpenAIGPTLMHeadModel"),c(QS,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(VS,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(WS,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(HS,"href","/docs/transformers/master/en/model_doc/transformerxl#transformers.TFTransfoXLLMHeadModel"),c(US,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(JS,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMaskedLM"),c(KS,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(io,"class","docstring"),c(lr,"class","docstring"),c(lT,"id","transformers.TFAutoModelForCausalLM"),c(lT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(lT,"href","#transformers.TFAutoModelForCausalLM"),c(_d,"class","relative group"),c(Kr,"class","docstring"),c(YS,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel"),c(ZS,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),c(e$,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),c(o$,"href","/docs/transformers/master/en/model_doc/gpt#transformers.TFOpenAIGPTLMHeadModel"),c(r$,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),c(t$,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),c(a$,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),c(s$,"href","/docs/transformers/master/en/model_doc/transformerxl#transformers.TFTransfoXLLMHeadModel"),c(n$,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(l$,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),c(co,"class","docstring"),c(ir,"class","docstring"),c(bT,"id","transformers.TFAutoModelForImageClassification"),c(bT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(bT,"href","#transformers.TFAutoModelForImageClassification"),c(Td,"class","relative group"),c(Yr,"class","docstring"),c(i$,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification"),c(mo,"class","docstring"),c(dr,"class","docstring"),c(TT,"id","transformers.TFAutoModelForMaskedLM"),c(TT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(TT,"href","#transformers.TFAutoModelForMaskedLM"),c(Cd,"class","relative group"),c(Zr,"class","docstring"),c(d$,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),c(c$,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM"),c(m$,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),c(f$,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),c(h$,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),c(g$,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.TFDebertaV2ForMaskedLM"),c(u$,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(p$,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM"),c(_$,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),c(b$,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),c(v$,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),c(T$,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),c(F$,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),c(E$,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),c(C$,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),c(M$,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),c(y$,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),c(w$,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),c(A$,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),c(L$,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMaskedLM"),c(fo,"class","docstring"),c(cr,"class","docstring"),c(GT,"id","transformers.TFAutoModelForSeq2SeqLM"),c(GT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(GT,"href","#transformers.TFAutoModelForSeq2SeqLM"),c(wd,"class","relative group"),c(et,"class","docstring"),c(B$,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),c(x$,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),c(k$,"href","/docs/transformers/master/en/model_doc/blenderbot_small#transformers.TFBlenderbotSmallForConditionalGeneration"),c(R$,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.TFEncoderDecoderModel"),c(P$,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),c(S$,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel"),c($$,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),c(I$,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),c(D$,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),c(N$,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),c(ho,"class","docstring"),c(mr,"class","docstring"),c(YT,"id","transformers.TFAutoModelForSequenceClassification"),c(YT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(YT,"href","#transformers.TFAutoModelForSequenceClassification"),c(Bd,"class","relative group"),c(ot,"class","docstring"),c(j$,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),c(O$,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification"),c(G$,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),c(q$,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),c(z$,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),c(X$,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),c(Q$,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.TFDebertaV2ForSequenceClassification"),c(V$,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(W$,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),c(H$,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),c(U$,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),c(J$,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),c(K$,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),c(Y$,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),c(Z$,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),c(eI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),c(oI,"href","/docs/transformers/master/en/model_doc/gpt#transformers.TFOpenAIGPTForSequenceClassification"),c(rI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),c(tI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),c(aI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),c(sI,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),c(nI,"href","/docs/transformers/master/en/model_doc/transformerxl#transformers.TFTransfoXLForSequenceClassification"),c(lI,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),c(iI,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForSequenceClassification"),c(dI,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),c(go,"class","docstring"),c(fr,"class","docstring"),c(M1,"id","transformers.TFAutoModelForMultipleChoice"),c(M1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(M1,"href","#transformers.TFAutoModelForMultipleChoice"),c(Rd,"class","relative group"),c(rt,"class","docstring"),c(cI,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),c(mI,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice"),c(fI,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),c(hI,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),c(gI,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(uI,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),c(pI,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),c(_I,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),c(bI,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),c(vI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),c(TI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),c(FI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),c(EI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),c(CI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),c(MI,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),c(yI,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForMultipleChoice"),c(wI,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),c(uo,"class","docstring"),c(hr,"class","docstring"),c(q1,"id","transformers.TFAutoModelForTableQuestionAnswering"),c(q1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q1,"href","#transformers.TFAutoModelForTableQuestionAnswering"),c($d,"class","relative group"),c(tt,"class","docstring"),c(AI,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),c(po,"class","docstring"),c(gr,"class","docstring"),c(X1,"id","transformers.TFAutoModelForTokenClassification"),c(X1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(X1,"href","#transformers.TFAutoModelForTokenClassification"),c(Nd,"class","relative group"),c(at,"class","docstring"),c(LI,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),c(BI,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification"),c(xI,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),c(kI,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),c(RI,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),c(PI,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.TFDebertaV2ForTokenClassification"),c(SI,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c($I,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification"),c(II,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),c(DI,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),c(NI,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),c(jI,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),c(OI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),c(GI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),c(qI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),c(zI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),c(XI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),c(QI,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),c(VI,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForTokenClassification"),c(WI,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),c(_o,"class","docstring"),c(ur,"class","docstring"),c(mF,"id","transformers.TFAutoModelForQuestionAnswering"),c(mF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(mF,"href","#transformers.TFAutoModelForQuestionAnswering"),c(Gd,"class","relative group"),c(st,"class","docstring"),c(HI,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),c(UI,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),c(JI,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),c(KI,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),c(YI,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),c(ZI,"href","/docs/transformers/master/en/model_doc/deberta_v2#transformers.TFDebertaV2ForQuestionAnswering"),c(eD,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(oD,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),c(rD,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),c(tD,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),c(aD,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),c(sD,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),c(nD,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),c(lD,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),c(iD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),c(dD,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),c(cD,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),c(mD,"href","/docs/transformers/master/en/model_doc/xlmroberta#transformers.TFXLMRobertaForQuestionAnswering"),c(fD,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),c(bo,"class","docstring"),c(pr,"class","docstring"),c(kF,"id","transformers.FlaxAutoModel"),c(kF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(kF,"href","#transformers.FlaxAutoModel"),c(Xd,"class","relative group"),c(nt,"class","docstring"),c(hD,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel"),c(gD,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel"),c(uD,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel"),c(pD,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel"),c(_D,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdModel"),c(bD,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),c(vD,"href","/docs/transformers/master/en/model_doc/blenderbot_small#transformers.FlaxBlenderbotSmallModel"),c(TD,"href","/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel"),c(FD,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),c(ED,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel"),c(CD,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model"),c(MD,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),c(yD,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel"),c(wD,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel"),c(AD,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel"),c(LD,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model"),c(BD,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel"),c(xD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel"),c(kD,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model"),c(RD,"href","/docs/transformers/master/en/model_doc/vision_text_dual_encoder#transformers.FlaxVisionTextDualEncoderModel"),c(PD,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel"),c(SD,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),c(vo,"class","docstring"),c(_r,"class","docstring"),c(eE,"id","transformers.FlaxAutoModelForCausalLM"),c(eE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(eE,"href","#transformers.FlaxAutoModelForCausalLM"),c(Wd,"class","relative group"),c(lt,"class","docstring"),c($D,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),c(ID,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),c(DD,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),c(To,"class","docstring"),c(br,"class","docstring"),c(aE,"id","transformers.FlaxAutoModelForPreTraining"),c(aE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(aE,"href","#transformers.FlaxAutoModelForPreTraining"),c(Kd,"class","relative group"),c(it,"class","docstring"),c(ND,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),c(jD,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(OD,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining"),c(GD,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForPreTraining"),c(qD,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),c(zD,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(XD,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(QD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(VD,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(WD,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),c(Fo,"class","docstring"),c(vr,"class","docstring"),c(uE,"id","transformers.FlaxAutoModelForMaskedLM"),c(uE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(uE,"href","#transformers.FlaxAutoModelForMaskedLM"),c(ec,"class","relative group"),c(dt,"class","docstring"),c(HD,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),c(UD,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(JD,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),c(KD,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForMaskedLM"),c(YD,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),c(ZD,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),c(eN,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(oN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),c(Eo,"class","docstring"),c(Tr,"class","docstring"),c(ME,"id","transformers.FlaxAutoModelForSeq2SeqLM"),c(ME,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ME,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),c(tc,"class","relative group"),c(ct,"class","docstring"),c(rN,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),c(tN,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),c(aN,"href","/docs/transformers/master/en/model_doc/blenderbot_small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),c(sN,"href","/docs/transformers/master/en/model_doc/encoderdecoder#transformers.FlaxEncoderDecoderModel"),c(nN,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel"),c(lN,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),c(iN,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),c(dN,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),c(cN,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),c(Co,"class","docstring"),c(Fr,"class","docstring"),c(SE,"id","transformers.FlaxAutoModelForSequenceClassification"),c(SE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(SE,"href","#transformers.FlaxAutoModelForSequenceClassification"),c(nc,"class","relative group"),c(mt,"class","docstring"),c(mN,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),c(fN,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),c(hN,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),c(gN,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForSequenceClassification"),c(uN,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),c(pN,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),c(_N,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),c(bN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),c(Mo,"class","docstring"),c(Er,"class","docstring"),c(zE,"id","transformers.FlaxAutoModelForQuestionAnswering"),c(zE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zE,"href","#transformers.FlaxAutoModelForQuestionAnswering"),c(dc,"class","relative group"),c(ft,"class","docstring"),c(vN,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),c(TN,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),c(FN,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),c(EN,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForQuestionAnswering"),c(CN,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),c(MN,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),c(yN,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),c(wN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),c(yo,"class","docstring"),c(Cr,"class","docstring"),c(YE,"id","transformers.FlaxAutoModelForTokenClassification"),c(YE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(YE,"href","#transformers.FlaxAutoModelForTokenClassification"),c(fc,"class","relative group"),c(ht,"class","docstring"),c(AN,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),c(LN,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),c(BN,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForTokenClassification"),c(xN,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),c(kN,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),c(RN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),c(wo,"class","docstring"),c(Mr,"class","docstring"),c(s4,"id","transformers.FlaxAutoModelForMultipleChoice"),c(s4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(s4,"href","#transformers.FlaxAutoModelForMultipleChoice"),c(uc,"class","relative group"),c(gt,"class","docstring"),c(PN,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),c(SN,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),c($N,"href","/docs/transformers/master/en/model_doc/bigbird#transformers.FlaxBigBirdForMultipleChoice"),c(IN,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),c(DN,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),c(NN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),c(Ao,"class","docstring"),c(wr,"class","docstring"),c(f4,"id","transformers.FlaxAutoModelForNextSentencePrediction"),c(f4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(f4,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),c(bc,"class","relative group"),c(ut,"class","docstring"),c(jN,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),c(Lo,"class","docstring"),c(Lr,"class","docstring"),c(g4,"id","transformers.FlaxAutoModelForImageClassification"),c(g4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(g4,"href","#transformers.FlaxAutoModelForImageClassification"),c(Fc,"class","relative group"),c(pt,"class","docstring"),c(ON,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),c(GN,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification"),c(Bo,"class","docstring"),c(Br,"class","docstring"),c(_4,"id","transformers.FlaxAutoModelForVision2Seq"),c(_4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(_4,"href","#transformers.FlaxAutoModelForVision2Seq"),c(Mc,"class","relative group"),c(_t,"class","docstring"),c(qN,"href","/docs/transformers/master/en/model_doc/visionencoderdecoder#transformers.FlaxVisionEncoderDecoderModel"),c(xo,"class","docstring"),c(xr,"class","docstring")},m(d,_){e(document.head,re),b(d,Pe,_),b(d,fe,_),e(fe,ue),e(ue,ro),h(ge,ro,null),e(fe,Ee),e(fe,$o),e($o,zl),b(d,Lc,_),b(d,Ot,_),e(Ot,Xl),e(Ot,Ql),e(Ql,pC),e(Ot,Bc),b(d,Be,_),b(d,ao,_),e(ao,Vl),e(ao,hs),e(hs,_C),e(ao,gs),e(ao,us),e(us,bC),e(ao,Wl),e(ao,ps),e(ps,vC),e(ao,Hl),b(d,xc,_),h(_a,d,_),b(d,so,_),b(d,pe,_),e(pe,T0),e(pe,Ul),e(Ul,F0),e(pe,E0),b(d,Io,_),b(d,ba,_),e(ba,C0),e(ba,kc),e(kc,M0),e(ba,e0e),b(d,a5e,_),b(d,Jl,_),e(Jl,Rc),e(Rc,BO),h(TC,BO,null),e(Jl,o0e),e(Jl,xO),e(xO,r0e),b(d,s5e,_),b(d,_s,_),e(_s,t0e),e(_s,kO),e(kO,a0e),e(_s,s0e),e(_s,RO),e(RO,n0e),e(_s,l0e),b(d,n5e,_),h(FC,d,_),b(d,l5e,_),b(d,y0,_),e(y0,i0e),b(d,i5e,_),h(Pc,d,_),b(d,d5e,_),b(d,Kl,_),e(Kl,Sc),e(Sc,PO),h(EC,PO,null),e(Kl,d0e),e(Kl,SO),e(SO,c0e),b(d,c5e,_),b(d,Do,_),h(CC,Do,null),e(Do,m0e),e(Do,MC),e(MC,f0e),e(MC,w0),e(w0,h0e),e(MC,g0e),e(Do,u0e),e(Do,yC),e(yC,p0e),e(yC,$O),e($O,_0e),e(yC,b0e),e(Do,v0e),e(Do,no),h(wC,no,null),e(no,T0e),e(no,IO),e(IO,F0e),e(no,E0e),e(no,Yl),e(Yl,C0e),e(Yl,DO),e(DO,M0e),e(Yl,y0e),e(Yl,NO),e(NO,w0e),e(Yl,A0e),e(no,L0e),e(no,v),e(v,$c),e($c,jO),e(jO,B0e),e($c,x0e),e($c,A0),e(A0,k0e),e($c,R0e),e(v,P0e),e(v,Ic),e(Ic,OO),e(OO,S0e),e(Ic,$0e),e(Ic,L0),e(L0,I0e),e(Ic,D0e),e(v,N0e),e(v,Dc),e(Dc,GO),e(GO,j0e),e(Dc,O0e),e(Dc,B0),e(B0,G0e),e(Dc,q0e),e(v,z0e),e(v,Nc),e(Nc,qO),e(qO,X0e),e(Nc,Q0e),e(Nc,x0),e(x0,V0e),e(Nc,W0e),e(v,H0e),e(v,jc),e(jc,zO),e(zO,U0e),e(jc,J0e),e(jc,k0),e(k0,K0e),e(jc,Y0e),e(v,Z0e),e(v,Oc),e(Oc,XO),e(XO,eAe),e(Oc,oAe),e(Oc,R0),e(R0,rAe),e(Oc,tAe),e(v,aAe),e(v,Gc),e(Gc,QO),e(QO,sAe),e(Gc,nAe),e(Gc,P0),e(P0,lAe),e(Gc,iAe),e(v,dAe),e(v,qc),e(qc,VO),e(VO,cAe),e(qc,mAe),e(qc,S0),e(S0,fAe),e(qc,hAe),e(v,gAe),e(v,zc),e(zc,WO),e(WO,uAe),e(zc,pAe),e(zc,$0),e($0,_Ae),e(zc,bAe),e(v,vAe),e(v,Xc),e(Xc,HO),e(HO,TAe),e(Xc,FAe),e(Xc,I0),e(I0,EAe),e(Xc,CAe),e(v,MAe),e(v,Qc),e(Qc,UO),e(UO,yAe),e(Qc,wAe),e(Qc,D0),e(D0,AAe),e(Qc,LAe),e(v,BAe),e(v,Vc),e(Vc,JO),e(JO,xAe),e(Vc,kAe),e(Vc,N0),e(N0,RAe),e(Vc,PAe),e(v,SAe),e(v,Wc),e(Wc,KO),e(KO,$Ae),e(Wc,IAe),e(Wc,j0),e(j0,DAe),e(Wc,NAe),e(v,jAe),e(v,Hc),e(Hc,YO),e(YO,OAe),e(Hc,GAe),e(Hc,O0),e(O0,qAe),e(Hc,zAe),e(v,XAe),e(v,Uc),e(Uc,ZO),e(ZO,QAe),e(Uc,VAe),e(Uc,G0),e(G0,WAe),e(Uc,HAe),e(v,UAe),e(v,Jc),e(Jc,eG),e(eG,JAe),e(Jc,KAe),e(Jc,q0),e(q0,YAe),e(Jc,ZAe),e(v,e6e),e(v,Kc),e(Kc,oG),e(oG,o6e),e(Kc,r6e),e(Kc,z0),e(z0,t6e),e(Kc,a6e),e(v,s6e),e(v,Yc),e(Yc,rG),e(rG,n6e),e(Yc,l6e),e(Yc,X0),e(X0,i6e),e(Yc,d6e),e(v,c6e),e(v,Zc),e(Zc,tG),e(tG,m6e),e(Zc,f6e),e(Zc,Q0),e(Q0,h6e),e(Zc,g6e),e(v,u6e),e(v,em),e(em,aG),e(aG,p6e),e(em,_6e),e(em,V0),e(V0,b6e),e(em,v6e),e(v,T6e),e(v,om),e(om,sG),e(sG,F6e),e(om,E6e),e(om,W0),e(W0,C6e),e(om,M6e),e(v,y6e),e(v,rm),e(rm,nG),e(nG,w6e),e(rm,A6e),e(rm,H0),e(H0,L6e),e(rm,B6e),e(v,x6e),e(v,tm),e(tm,lG),e(lG,k6e),e(tm,R6e),e(tm,U0),e(U0,P6e),e(tm,S6e),e(v,$6e),e(v,am),e(am,iG),e(iG,I6e),e(am,D6e),e(am,J0),e(J0,N6e),e(am,j6e),e(v,O6e),e(v,sm),e(sm,dG),e(dG,G6e),e(sm,q6e),e(sm,K0),e(K0,z6e),e(sm,X6e),e(v,Q6e),e(v,nm),e(nm,cG),e(cG,V6e),e(nm,W6e),e(nm,Y0),e(Y0,H6e),e(nm,U6e),e(v,J6e),e(v,lm),e(lm,mG),e(mG,K6e),e(lm,Y6e),e(lm,Z0),e(Z0,Z6e),e(lm,eLe),e(v,oLe),e(v,im),e(im,fG),e(fG,rLe),e(im,tLe),e(im,eA),e(eA,aLe),e(im,sLe),e(v,nLe),e(v,dm),e(dm,hG),e(hG,lLe),e(dm,iLe),e(dm,oA),e(oA,dLe),e(dm,cLe),e(v,mLe),e(v,cm),e(cm,gG),e(gG,fLe),e(cm,hLe),e(cm,rA),e(rA,gLe),e(cm,uLe),e(v,pLe),e(v,mm),e(mm,uG),e(uG,_Le),e(mm,bLe),e(mm,tA),e(tA,vLe),e(mm,TLe),e(v,FLe),e(v,fm),e(fm,pG),e(pG,ELe),e(fm,CLe),e(fm,aA),e(aA,MLe),e(fm,yLe),e(v,wLe),e(v,hm),e(hm,_G),e(_G,ALe),e(hm,LLe),e(hm,sA),e(sA,BLe),e(hm,xLe),e(v,kLe),e(v,gm),e(gm,bG),e(bG,RLe),e(gm,PLe),e(gm,nA),e(nA,SLe),e(gm,$Le),e(v,ILe),e(v,um),e(um,vG),e(vG,DLe),e(um,NLe),e(um,lA),e(lA,jLe),e(um,OLe),e(v,GLe),e(v,pm),e(pm,TG),e(TG,qLe),e(pm,zLe),e(pm,iA),e(iA,XLe),e(pm,QLe),e(v,VLe),e(v,_m),e(_m,FG),e(FG,WLe),e(_m,HLe),e(_m,dA),e(dA,ULe),e(_m,JLe),e(v,KLe),e(v,bm),e(bm,EG),e(EG,YLe),e(bm,ZLe),e(bm,cA),e(cA,e8e),e(bm,o8e),e(v,r8e),e(v,vm),e(vm,CG),e(CG,t8e),e(vm,a8e),e(vm,mA),e(mA,s8e),e(vm,n8e),e(v,l8e),e(v,Tm),e(Tm,MG),e(MG,i8e),e(Tm,d8e),e(Tm,fA),e(fA,c8e),e(Tm,m8e),e(v,f8e),e(v,Fm),e(Fm,yG),e(yG,h8e),e(Fm,g8e),e(Fm,hA),e(hA,u8e),e(Fm,p8e),e(v,_8e),e(v,Em),e(Em,wG),e(wG,b8e),e(Em,v8e),e(Em,gA),e(gA,T8e),e(Em,F8e),e(v,E8e),e(v,Cm),e(Cm,AG),e(AG,C8e),e(Cm,M8e),e(Cm,uA),e(uA,y8e),e(Cm,w8e),e(v,A8e),e(v,Mm),e(Mm,LG),e(LG,L8e),e(Mm,B8e),e(Mm,pA),e(pA,x8e),e(Mm,k8e),e(v,R8e),e(v,ym),e(ym,BG),e(BG,P8e),e(ym,S8e),e(ym,_A),e(_A,$8e),e(ym,I8e),e(v,D8e),e(v,wm),e(wm,xG),e(xG,N8e),e(wm,j8e),e(wm,bA),e(bA,O8e),e(wm,G8e),e(v,q8e),e(v,Am),e(Am,kG),e(kG,z8e),e(Am,X8e),e(Am,vA),e(vA,Q8e),e(Am,V8e),e(v,W8e),e(v,Lm),e(Lm,RG),e(RG,H8e),e(Lm,U8e),e(Lm,TA),e(TA,J8e),e(Lm,K8e),e(v,Y8e),e(v,Bm),e(Bm,PG),e(PG,Z8e),e(Bm,eBe),e(Bm,FA),e(FA,oBe),e(Bm,rBe),e(v,tBe),e(v,xm),e(xm,SG),e(SG,aBe),e(xm,sBe),e(xm,EA),e(EA,nBe),e(xm,lBe),e(v,iBe),e(v,km),e(km,$G),e($G,dBe),e(km,cBe),e(km,CA),e(CA,mBe),e(km,fBe),e(v,hBe),e(v,Rm),e(Rm,IG),e(IG,gBe),e(Rm,uBe),e(Rm,MA),e(MA,pBe),e(Rm,_Be),e(v,bBe),e(v,Pm),e(Pm,DG),e(DG,vBe),e(Pm,TBe),e(Pm,yA),e(yA,FBe),e(Pm,EBe),e(v,CBe),e(v,Sm),e(Sm,NG),e(NG,MBe),e(Sm,yBe),e(Sm,wA),e(wA,wBe),e(Sm,ABe),e(v,LBe),e(v,$m),e($m,jG),e(jG,BBe),e($m,xBe),e($m,AA),e(AA,kBe),e($m,RBe),e(v,PBe),e(v,Im),e(Im,OG),e(OG,SBe),e(Im,$Be),e(Im,LA),e(LA,IBe),e(Im,DBe),e(v,NBe),e(v,Dm),e(Dm,GG),e(GG,jBe),e(Dm,OBe),e(Dm,BA),e(BA,GBe),e(Dm,qBe),e(v,zBe),e(v,Nm),e(Nm,qG),e(qG,XBe),e(Nm,QBe),e(Nm,xA),e(xA,VBe),e(Nm,WBe),e(v,HBe),e(v,jm),e(jm,zG),e(zG,UBe),e(jm,JBe),e(jm,kA),e(kA,KBe),e(jm,YBe),e(v,ZBe),e(v,Om),e(Om,XG),e(XG,e9e),e(Om,o9e),e(Om,RA),e(RA,r9e),e(Om,t9e),e(v,a9e),e(v,Gm),e(Gm,QG),e(QG,s9e),e(Gm,n9e),e(Gm,PA),e(PA,l9e),e(Gm,i9e),e(v,d9e),e(v,qm),e(qm,VG),e(VG,c9e),e(qm,m9e),e(qm,SA),e(SA,f9e),e(qm,h9e),e(v,g9e),e(v,zm),e(zm,WG),e(WG,u9e),e(zm,p9e),e(zm,$A),e($A,_9e),e(zm,b9e),e(v,v9e),e(v,Xm),e(Xm,HG),e(HG,T9e),e(Xm,F9e),e(Xm,IA),e(IA,E9e),e(Xm,C9e),e(v,M9e),e(v,Qm),e(Qm,UG),e(UG,y9e),e(Qm,w9e),e(Qm,DA),e(DA,A9e),e(Qm,L9e),e(v,B9e),e(v,Vm),e(Vm,JG),e(JG,x9e),e(Vm,k9e),e(Vm,NA),e(NA,R9e),e(Vm,P9e),e(v,S9e),e(v,Wm),e(Wm,KG),e(KG,$9e),e(Wm,I9e),e(Wm,jA),e(jA,D9e),e(Wm,N9e),e(v,j9e),e(v,Hm),e(Hm,YG),e(YG,O9e),e(Hm,G9e),e(Hm,OA),e(OA,q9e),e(Hm,z9e),e(v,X9e),e(v,Um),e(Um,ZG),e(ZG,Q9e),e(Um,V9e),e(Um,GA),e(GA,W9e),e(Um,H9e),e(v,U9e),e(v,Jm),e(Jm,eq),e(eq,J9e),e(Jm,K9e),e(Jm,qA),e(qA,Y9e),e(Jm,Z9e),e(v,exe),e(v,Km),e(Km,oq),e(oq,oxe),e(Km,rxe),e(Km,zA),e(zA,txe),e(Km,axe),e(v,sxe),e(v,Ym),e(Ym,rq),e(rq,nxe),e(Ym,lxe),e(Ym,XA),e(XA,ixe),e(Ym,dxe),e(v,cxe),e(v,Zm),e(Zm,tq),e(tq,mxe),e(Zm,fxe),e(Zm,QA),e(QA,hxe),e(Zm,gxe),e(v,uxe),e(v,ef),e(ef,aq),e(aq,pxe),e(ef,_xe),e(ef,VA),e(VA,bxe),e(ef,vxe),e(v,Txe),e(v,of),e(of,sq),e(sq,Fxe),e(of,Exe),e(of,WA),e(WA,Cxe),e(of,Mxe),e(v,yxe),e(v,rf),e(rf,nq),e(nq,wxe),e(rf,Axe),e(rf,HA),e(HA,Lxe),e(rf,Bxe),e(v,xxe),e(v,tf),e(tf,lq),e(lq,kxe),e(tf,Rxe),e(tf,UA),e(UA,Pxe),e(tf,Sxe),e(v,$xe),e(v,af),e(af,iq),e(iq,Ixe),e(af,Dxe),e(af,JA),e(JA,Nxe),e(af,jxe),e(v,Oxe),e(v,sf),e(sf,dq),e(dq,Gxe),e(sf,qxe),e(sf,KA),e(KA,zxe),e(sf,Xxe),e(no,Qxe),e(no,cq),e(cq,Vxe),e(no,Wxe),h(AC,no,null),e(Do,Hxe),e(Do,nf),h(LC,nf,null),e(nf,Uxe),e(nf,mq),e(mq,Jxe),b(d,m5e,_),b(d,Zl,_),e(Zl,lf),e(lf,fq),h(BC,fq,null),e(Zl,Kxe),e(Zl,hq),e(hq,Yxe),b(d,f5e,_),b(d,No,_),h(xC,No,null),e(No,Zxe),e(No,kC),e(kC,eke),e(kC,YA),e(YA,oke),e(kC,rke),e(No,tke),e(No,RC),e(RC,ake),e(RC,gq),e(gq,ske),e(RC,nke),e(No,lke),e(No,ye),h(PC,ye,null),e(ye,ike),e(ye,uq),e(uq,dke),e(ye,cke),e(ye,va),e(va,mke),e(va,pq),e(pq,fke),e(va,hke),e(va,_q),e(_q,gke),e(va,uke),e(va,bq),e(bq,pke),e(va,_ke),e(ye,bke),e(ye,C),e(C,bs),e(bs,vq),e(vq,vke),e(bs,Tke),e(bs,ZA),e(ZA,Fke),e(bs,Eke),e(bs,e6),e(e6,Cke),e(bs,Mke),e(C,yke),e(C,vs),e(vs,Tq),e(Tq,wke),e(vs,Ake),e(vs,o6),e(o6,Lke),e(vs,Bke),e(vs,r6),e(r6,xke),e(vs,kke),e(C,Rke),e(C,Ts),e(Ts,Fq),e(Fq,Pke),e(Ts,Ske),e(Ts,t6),e(t6,$ke),e(Ts,Ike),e(Ts,a6),e(a6,Dke),e(Ts,Nke),e(C,jke),e(C,df),e(df,Eq),e(Eq,Oke),e(df,Gke),e(df,s6),e(s6,qke),e(df,zke),e(C,Xke),e(C,Fs),e(Fs,Cq),e(Cq,Qke),e(Fs,Vke),e(Fs,n6),e(n6,Wke),e(Fs,Hke),e(Fs,l6),e(l6,Uke),e(Fs,Jke),e(C,Kke),e(C,cf),e(cf,Mq),e(Mq,Yke),e(cf,Zke),e(cf,i6),e(i6,eRe),e(cf,oRe),e(C,rRe),e(C,mf),e(mf,yq),e(yq,tRe),e(mf,aRe),e(mf,d6),e(d6,sRe),e(mf,nRe),e(C,lRe),e(C,ff),e(ff,wq),e(wq,iRe),e(ff,dRe),e(ff,c6),e(c6,cRe),e(ff,mRe),e(C,fRe),e(C,Es),e(Es,Aq),e(Aq,hRe),e(Es,gRe),e(Es,m6),e(m6,uRe),e(Es,pRe),e(Es,f6),e(f6,_Re),e(Es,bRe),e(C,vRe),e(C,Cs),e(Cs,Lq),e(Lq,TRe),e(Cs,FRe),e(Cs,h6),e(h6,ERe),e(Cs,CRe),e(Cs,g6),e(g6,MRe),e(Cs,yRe),e(C,wRe),e(C,Ms),e(Ms,Bq),e(Bq,ARe),e(Ms,LRe),e(Ms,u6),e(u6,BRe),e(Ms,xRe),e(Ms,p6),e(p6,kRe),e(Ms,RRe),e(C,PRe),e(C,hf),e(hf,xq),e(xq,SRe),e(hf,$Re),e(hf,_6),e(_6,IRe),e(hf,DRe),e(C,NRe),e(C,gf),e(gf,kq),e(kq,jRe),e(gf,ORe),e(gf,b6),e(b6,GRe),e(gf,qRe),e(C,zRe),e(C,ys),e(ys,Rq),e(Rq,XRe),e(ys,QRe),e(ys,v6),e(v6,VRe),e(ys,WRe),e(ys,T6),e(T6,HRe),e(ys,URe),e(C,JRe),e(C,uf),e(uf,Pq),e(Pq,KRe),e(uf,YRe),e(uf,F6),e(F6,ZRe),e(uf,ePe),e(C,oPe),e(C,ws),e(ws,Sq),e(Sq,rPe),e(ws,tPe),e(ws,E6),e(E6,aPe),e(ws,sPe),e(ws,C6),e(C6,nPe),e(ws,lPe),e(C,iPe),e(C,As),e(As,$q),e($q,dPe),e(As,cPe),e(As,M6),e(M6,mPe),e(As,fPe),e(As,y6),e(y6,hPe),e(As,gPe),e(C,uPe),e(C,Ls),e(Ls,Iq),e(Iq,pPe),e(Ls,_Pe),e(Ls,w6),e(w6,bPe),e(Ls,vPe),e(Ls,Dq),e(Dq,TPe),e(Ls,FPe),e(C,EPe),e(C,pf),e(pf,Nq),e(Nq,CPe),e(pf,MPe),e(pf,A6),e(A6,yPe),e(pf,wPe),e(C,APe),e(C,Bs),e(Bs,jq),e(jq,LPe),e(Bs,BPe),e(Bs,L6),e(L6,xPe),e(Bs,kPe),e(Bs,B6),e(B6,RPe),e(Bs,PPe),e(C,SPe),e(C,_f),e(_f,Oq),e(Oq,$Pe),e(_f,IPe),e(_f,x6),e(x6,DPe),e(_f,NPe),e(C,jPe),e(C,xs),e(xs,Gq),e(Gq,OPe),e(xs,GPe),e(xs,k6),e(k6,qPe),e(xs,zPe),e(xs,R6),e(R6,XPe),e(xs,QPe),e(C,VPe),e(C,ks),e(ks,qq),e(qq,WPe),e(ks,HPe),e(ks,P6),e(P6,UPe),e(ks,JPe),e(ks,S6),e(S6,KPe),e(ks,YPe),e(C,ZPe),e(C,Rs),e(Rs,zq),e(zq,eSe),e(Rs,oSe),e(Rs,$6),e($6,rSe),e(Rs,tSe),e(Rs,I6),e(I6,aSe),e(Rs,sSe),e(C,nSe),e(C,bf),e(bf,Xq),e(Xq,lSe),e(bf,iSe),e(bf,D6),e(D6,dSe),e(bf,cSe),e(C,mSe),e(C,Ps),e(Ps,Qq),e(Qq,fSe),e(Ps,hSe),e(Ps,N6),e(N6,gSe),e(Ps,uSe),e(Ps,j6),e(j6,pSe),e(Ps,_Se),e(C,bSe),e(C,vf),e(vf,Vq),e(Vq,vSe),e(vf,TSe),e(vf,O6),e(O6,FSe),e(vf,ESe),e(C,CSe),e(C,Ss),e(Ss,Wq),e(Wq,MSe),e(Ss,ySe),e(Ss,G6),e(G6,wSe),e(Ss,ASe),e(Ss,q6),e(q6,LSe),e(Ss,BSe),e(C,xSe),e(C,$s),e($s,Hq),e(Hq,kSe),e($s,RSe),e($s,z6),e(z6,PSe),e($s,SSe),e($s,X6),e(X6,$Se),e($s,ISe),e(C,DSe),e(C,Is),e(Is,Uq),e(Uq,NSe),e(Is,jSe),e(Is,Q6),e(Q6,OSe),e(Is,GSe),e(Is,V6),e(V6,qSe),e(Is,zSe),e(C,XSe),e(C,Tf),e(Tf,Jq),e(Jq,QSe),e(Tf,VSe),e(Tf,W6),e(W6,WSe),e(Tf,HSe),e(C,USe),e(C,Ds),e(Ds,Kq),e(Kq,JSe),e(Ds,KSe),e(Ds,H6),e(H6,YSe),e(Ds,ZSe),e(Ds,U6),e(U6,e$e),e(Ds,o$e),e(C,r$e),e(C,Ns),e(Ns,Yq),e(Yq,t$e),e(Ns,a$e),e(Ns,J6),e(J6,s$e),e(Ns,n$e),e(Ns,K6),e(K6,l$e),e(Ns,i$e),e(C,d$e),e(C,js),e(js,Zq),e(Zq,c$e),e(js,m$e),e(js,Y6),e(Y6,f$e),e(js,h$e),e(js,Z6),e(Z6,g$e),e(js,u$e),e(C,p$e),e(C,Os),e(Os,ez),e(ez,_$e),e(Os,b$e),e(Os,eL),e(eL,v$e),e(Os,T$e),e(Os,oL),e(oL,F$e),e(Os,E$e),e(C,C$e),e(C,Gs),e(Gs,oz),e(oz,M$e),e(Gs,y$e),e(Gs,rL),e(rL,w$e),e(Gs,A$e),e(Gs,tL),e(tL,L$e),e(Gs,B$e),e(C,x$e),e(C,Ff),e(Ff,rz),e(rz,k$e),e(Ff,R$e),e(Ff,aL),e(aL,P$e),e(Ff,S$e),e(C,$$e),e(C,qs),e(qs,tz),e(tz,I$e),e(qs,D$e),e(qs,sL),e(sL,N$e),e(qs,j$e),e(qs,nL),e(nL,O$e),e(qs,G$e),e(C,q$e),e(C,Ef),e(Ef,az),e(az,z$e),e(Ef,X$e),e(Ef,lL),e(lL,Q$e),e(Ef,V$e),e(C,W$e),e(C,Cf),e(Cf,sz),e(sz,H$e),e(Cf,U$e),e(Cf,iL),e(iL,J$e),e(Cf,K$e),e(C,Y$e),e(C,zs),e(zs,nz),e(nz,Z$e),e(zs,eIe),e(zs,dL),e(dL,oIe),e(zs,rIe),e(zs,cL),e(cL,tIe),e(zs,aIe),e(C,sIe),e(C,Xs),e(Xs,lz),e(lz,nIe),e(Xs,lIe),e(Xs,mL),e(mL,iIe),e(Xs,dIe),e(Xs,fL),e(fL,cIe),e(Xs,mIe),e(C,fIe),e(C,Qs),e(Qs,iz),e(iz,hIe),e(Qs,gIe),e(Qs,hL),e(hL,uIe),e(Qs,pIe),e(Qs,gL),e(gL,_Ie),e(Qs,bIe),e(C,vIe),e(C,Vs),e(Vs,dz),e(dz,TIe),e(Vs,FIe),e(Vs,uL),e(uL,EIe),e(Vs,CIe),e(Vs,pL),e(pL,MIe),e(Vs,yIe),e(C,wIe),e(C,Ws),e(Ws,cz),e(cz,AIe),e(Ws,LIe),e(Ws,_L),e(_L,BIe),e(Ws,xIe),e(Ws,bL),e(bL,kIe),e(Ws,RIe),e(C,PIe),e(C,Hs),e(Hs,mz),e(mz,SIe),e(Hs,$Ie),e(Hs,vL),e(vL,IIe),e(Hs,DIe),e(Hs,TL),e(TL,NIe),e(Hs,jIe),e(C,OIe),e(C,Us),e(Us,fz),e(fz,GIe),e(Us,qIe),e(Us,FL),e(FL,zIe),e(Us,XIe),e(Us,EL),e(EL,QIe),e(Us,VIe),e(C,WIe),e(C,Mf),e(Mf,hz),e(hz,HIe),e(Mf,UIe),e(Mf,CL),e(CL,JIe),e(Mf,KIe),e(C,YIe),e(C,yf),e(yf,gz),e(gz,ZIe),e(yf,eDe),e(yf,ML),e(ML,oDe),e(yf,rDe),e(C,tDe),e(C,Js),e(Js,uz),e(uz,aDe),e(Js,sDe),e(Js,yL),e(yL,nDe),e(Js,lDe),e(Js,wL),e(wL,iDe),e(Js,dDe),e(C,cDe),e(C,wf),e(wf,pz),e(pz,mDe),e(wf,fDe),e(wf,AL),e(AL,hDe),e(wf,gDe),e(C,uDe),e(C,Ks),e(Ks,_z),e(_z,pDe),e(Ks,_De),e(Ks,LL),e(LL,bDe),e(Ks,vDe),e(Ks,BL),e(BL,TDe),e(Ks,FDe),e(C,EDe),e(C,Ys),e(Ys,bz),e(bz,CDe),e(Ys,MDe),e(Ys,xL),e(xL,yDe),e(Ys,wDe),e(Ys,kL),e(kL,ADe),e(Ys,LDe),e(C,BDe),e(C,Zs),e(Zs,vz),e(vz,xDe),e(Zs,kDe),e(Zs,RL),e(RL,RDe),e(Zs,PDe),e(Zs,PL),e(PL,SDe),e(Zs,$De),e(C,IDe),e(C,en),e(en,Tz),e(Tz,DDe),e(en,NDe),e(en,SL),e(SL,jDe),e(en,ODe),e(en,$L),e($L,GDe),e(en,qDe),e(C,zDe),e(C,on),e(on,Fz),e(Fz,XDe),e(on,QDe),e(on,IL),e(IL,VDe),e(on,WDe),e(on,DL),e(DL,HDe),e(on,UDe),e(C,JDe),e(C,Af),e(Af,Ez),e(Ez,KDe),e(Af,YDe),e(Af,NL),e(NL,ZDe),e(Af,eNe),e(C,oNe),e(C,Lf),e(Lf,Cz),e(Cz,rNe),e(Lf,tNe),e(Lf,jL),e(jL,aNe),e(Lf,sNe),e(C,nNe),e(C,rn),e(rn,Mz),e(Mz,lNe),e(rn,iNe),e(rn,OL),e(OL,dNe),e(rn,cNe),e(rn,GL),e(GL,mNe),e(rn,fNe),e(C,hNe),e(C,tn),e(tn,yz),e(yz,gNe),e(tn,uNe),e(tn,qL),e(qL,pNe),e(tn,_Ne),e(tn,zL),e(zL,bNe),e(tn,vNe),e(C,TNe),e(C,an),e(an,wz),e(wz,FNe),e(an,ENe),e(an,XL),e(XL,CNe),e(an,MNe),e(an,QL),e(QL,yNe),e(an,wNe),e(C,ANe),e(C,Bf),e(Bf,Az),e(Az,LNe),e(Bf,BNe),e(Bf,VL),e(VL,xNe),e(Bf,kNe),e(C,RNe),e(C,xf),e(xf,Lz),e(Lz,PNe),e(xf,SNe),e(xf,WL),e(WL,$Ne),e(xf,INe),e(C,DNe),e(C,kf),e(kf,Bz),e(Bz,NNe),e(kf,jNe),e(kf,HL),e(HL,ONe),e(kf,GNe),e(C,qNe),e(C,Rf),e(Rf,xz),e(xz,zNe),e(Rf,XNe),e(Rf,UL),e(UL,QNe),e(Rf,VNe),e(C,WNe),e(C,Pf),e(Pf,kz),e(kz,HNe),e(Pf,UNe),e(Pf,JL),e(JL,JNe),e(Pf,KNe),e(C,YNe),e(C,sn),e(sn,Rz),e(Rz,ZNe),e(sn,eje),e(sn,KL),e(KL,oje),e(sn,rje),e(sn,YL),e(YL,tje),e(sn,aje),e(C,sje),e(C,nn),e(nn,Pz),e(Pz,nje),e(nn,lje),e(nn,ZL),e(ZL,ije),e(nn,dje),e(nn,e8),e(e8,cje),e(nn,mje),e(ye,fje),e(ye,ei),e(ei,hje),e(ei,Sz),e(Sz,gje),e(ei,uje),e(ei,$z),e($z,pje),e(ei,_je),e(ye,bje),e(ye,oi),e(oi,Ta),e(Ta,vje),e(Ta,Iz),e(Iz,Tje),e(Ta,Fje),e(Ta,Dz),e(Dz,Eje),e(Ta,Cje),e(Ta,Nz),e(Nz,Mje),e(Ta,yje),e(oi,wje),e(oi,Fa),e(Fa,Aje),e(Fa,jz),e(jz,Lje),e(Fa,Bje),e(Fa,o8),e(o8,xje),e(Fa,kje),e(Fa,Oz),e(Oz,Rje),e(Fa,Pje),e(oi,Sje),e(oi,k),e(k,$je),e(k,Gz),e(Gz,Ije),e(k,Dje),e(k,qz),e(qz,Nje),e(k,jje),e(k,zz),e(zz,Oje),e(k,Gje),e(k,r8),e(r8,qje),e(k,zje),e(k,Xz),e(Xz,Xje),e(k,Qje),e(k,ri),e(ri,Vje),e(ri,Qz),e(Qz,Wje),e(ri,Hje),e(ri,Vz),e(Vz,Uje),e(ri,Jje),e(k,Kje),e(k,SC),e(SC,Yje),e(SC,Wz),e(Wz,Zje),e(SC,eOe),e(k,oOe),e(k,Hz),e(Hz,rOe),e(k,tOe),e(k,$C),e($C,aOe),e($C,Uz),e(Uz,sOe),e($C,nOe),e(k,lOe),e(k,Jz),e(Jz,iOe),e(k,dOe),e(k,Kz),e(Kz,cOe),e(k,mOe),e(k,Yz),e(Yz,fOe),e(k,hOe),e(k,Zz),e(Zz,gOe),e(k,uOe),e(k,eX),e(eX,pOe),e(k,_Oe),e(k,oX),e(oX,bOe),e(k,vOe),e(k,rX),e(rX,TOe),e(k,FOe),e(k,tX),e(tX,EOe),e(k,COe),e(k,aX),e(aX,MOe),e(k,yOe),e(k,sX),e(sX,wOe),e(k,AOe),e(k,IC),e(IC,LOe),e(IC,nX),e(nX,BOe),e(IC,xOe),e(k,kOe),e(k,lX),e(lX,ROe),e(k,POe),e(k,DC),e(DC,SOe),e(DC,iX),e(iX,$Oe),e(DC,IOe),e(k,DOe),e(k,NC),e(NC,NOe),e(NC,dX),e(dX,jOe),e(NC,OOe),e(k,GOe),e(k,cX),e(cX,qOe),e(k,zOe),e(k,mX),e(mX,XOe),e(k,QOe),e(k,fX),e(fX,VOe),e(k,WOe),e(k,hX),e(hX,HOe),e(k,UOe),e(k,gX),e(gX,JOe),e(k,KOe),e(k,uX),e(uX,YOe),e(k,ZOe),e(k,pX),e(pX,eGe),e(k,oGe),e(k,_X),e(_X,rGe),e(k,tGe),e(k,bX),e(bX,aGe),e(k,sGe),e(k,vX),e(vX,nGe),e(k,lGe),e(k,TX),e(TX,iGe),e(k,dGe),e(k,FX),e(FX,cGe),e(k,mGe),e(k,EX),e(EX,fGe),e(k,hGe),e(ye,gGe),e(ye,CX),e(CX,uGe),e(ye,pGe),h(jC,ye,null),e(No,_Ge),e(No,Sf),h(OC,Sf,null),e(Sf,bGe),e(Sf,MX),e(MX,vGe),b(d,h5e,_),b(d,ti,_),e(ti,$f),e($f,yX),h(GC,yX,null),e(ti,TGe),e(ti,wX),e(wX,FGe),b(d,g5e,_),b(d,Dt,_),h(qC,Dt,null),e(Dt,EGe),e(Dt,zC),e(zC,CGe),e(zC,t8),e(t8,MGe),e(zC,yGe),e(Dt,wGe),e(Dt,XC),e(XC,AGe),e(XC,AX),e(AX,LGe),e(XC,BGe),e(Dt,xGe),e(Dt,Te),h(QC,Te,null),e(Te,kGe),e(Te,LX),e(LX,RGe),e(Te,PGe),e(Te,Ea),e(Ea,SGe),e(Ea,BX),e(BX,$Ge),e(Ea,IGe),e(Ea,xX),e(xX,DGe),e(Ea,NGe),e(Ea,kX),e(kX,jGe),e(Ea,OGe),e(Te,GGe),e(Te,Ce),e(Ce,If),e(If,RX),e(RX,qGe),e(If,zGe),e(If,a8),e(a8,XGe),e(If,QGe),e(Ce,VGe),e(Ce,Df),e(Df,PX),e(PX,WGe),e(Df,HGe),e(Df,s8),e(s8,UGe),e(Df,JGe),e(Ce,KGe),e(Ce,Nf),e(Nf,SX),e(SX,YGe),e(Nf,ZGe),e(Nf,n8),e(n8,eqe),e(Nf,oqe),e(Ce,rqe),e(Ce,jf),e(jf,$X),e($X,tqe),e(jf,aqe),e(jf,l8),e(l8,sqe),e(jf,nqe),e(Ce,lqe),e(Ce,Of),e(Of,IX),e(IX,iqe),e(Of,dqe),e(Of,i8),e(i8,cqe),e(Of,mqe),e(Ce,fqe),e(Ce,Gf),e(Gf,DX),e(DX,hqe),e(Gf,gqe),e(Gf,d8),e(d8,uqe),e(Gf,pqe),e(Ce,_qe),e(Ce,qf),e(qf,NX),e(NX,bqe),e(qf,vqe),e(qf,c8),e(c8,Tqe),e(qf,Fqe),e(Ce,Eqe),e(Ce,zf),e(zf,jX),e(jX,Cqe),e(zf,Mqe),e(zf,m8),e(m8,yqe),e(zf,wqe),e(Ce,Aqe),e(Ce,Xf),e(Xf,OX),e(OX,Lqe),e(Xf,Bqe),e(Xf,f8),e(f8,xqe),e(Xf,kqe),e(Te,Rqe),e(Te,ai),e(ai,Pqe),e(ai,GX),e(GX,Sqe),e(ai,$qe),e(ai,qX),e(qX,Iqe),e(ai,Dqe),e(Te,Nqe),e(Te,si),e(si,Ca),e(Ca,jqe),e(Ca,zX),e(zX,Oqe),e(Ca,Gqe),e(Ca,XX),e(XX,qqe),e(Ca,zqe),e(Ca,QX),e(QX,Xqe),e(Ca,Qqe),e(si,Vqe),e(si,Ma),e(Ma,Wqe),e(Ma,VX),e(VX,Hqe),e(Ma,Uqe),e(Ma,h8),e(h8,Jqe),e(Ma,Kqe),e(Ma,WX),e(WX,Yqe),e(Ma,Zqe),e(si,eze),e(si,N),e(N,oze),e(N,HX),e(HX,rze),e(N,tze),e(N,UX),e(UX,aze),e(N,sze),e(N,ni),e(ni,nze),e(ni,JX),e(JX,lze),e(ni,ize),e(ni,KX),e(KX,dze),e(ni,cze),e(N,mze),e(N,VC),e(VC,fze),e(VC,YX),e(YX,hze),e(VC,gze),e(N,uze),e(N,ZX),e(ZX,pze),e(N,_ze),e(N,WC),e(WC,bze),e(WC,eQ),e(eQ,vze),e(WC,Tze),e(N,Fze),e(N,oQ),e(oQ,Eze),e(N,Cze),e(N,rQ),e(rQ,Mze),e(N,yze),e(N,tQ),e(tQ,wze),e(N,Aze),e(N,aQ),e(aQ,Lze),e(N,Bze),e(N,HC),e(HC,xze),e(HC,sQ),e(sQ,kze),e(HC,Rze),e(N,Pze),e(N,nQ),e(nQ,Sze),e(N,$ze),e(N,lQ),e(lQ,Ize),e(N,Dze),e(N,iQ),e(iQ,Nze),e(N,jze),e(N,dQ),e(dQ,Oze),e(N,Gze),e(N,cQ),e(cQ,qze),e(N,zze),e(N,mQ),e(mQ,Xze),e(N,Qze),e(N,fQ),e(fQ,Vze),e(N,Wze),e(N,hQ),e(hQ,Hze),e(N,Uze),e(N,UC),e(UC,Jze),e(UC,gQ),e(gQ,Kze),e(UC,Yze),e(N,Zze),e(N,uQ),e(uQ,eXe),e(N,oXe),e(N,pQ),e(pQ,rXe),e(N,tXe),e(N,_Q),e(_Q,aXe),e(N,sXe),e(N,bQ),e(bQ,nXe),e(N,lXe),e(N,vQ),e(vQ,iXe),e(N,dXe),e(N,TQ),e(TQ,cXe),e(N,mXe),e(N,FQ),e(FQ,fXe),e(N,hXe),e(N,EQ),e(EQ,gXe),e(N,uXe),e(N,CQ),e(CQ,pXe),e(N,_Xe),e(N,MQ),e(MQ,bXe),e(N,vXe),e(N,yQ),e(yQ,TXe),e(N,FXe),e(Te,EXe),h(Qf,Te,null),e(Te,CXe),e(Te,wQ),e(wQ,MXe),e(Te,yXe),h(JC,Te,null),b(d,u5e,_),b(d,li,_),e(li,Vf),e(Vf,AQ),h(KC,AQ,null),e(li,wXe),e(li,LQ),e(LQ,AXe),b(d,p5e,_),b(d,Nt,_),h(YC,Nt,null),e(Nt,LXe),e(Nt,ZC),e(ZC,BXe),e(ZC,g8),e(g8,xXe),e(ZC,kXe),e(Nt,RXe),e(Nt,e3),e(e3,PXe),e(e3,BQ),e(BQ,SXe),e(e3,$Xe),e(Nt,IXe),e(Nt,Fe),h(o3,Fe,null),e(Fe,DXe),e(Fe,xQ),e(xQ,NXe),e(Fe,jXe),e(Fe,ii),e(ii,OXe),e(ii,kQ),e(kQ,GXe),e(ii,qXe),e(ii,RQ),e(RQ,zXe),e(ii,XXe),e(Fe,QXe),e(Fe,to),e(to,Wf),e(Wf,PQ),e(PQ,VXe),e(Wf,WXe),e(Wf,u8),e(u8,HXe),e(Wf,UXe),e(to,JXe),e(to,Hf),e(Hf,SQ),e(SQ,KXe),e(Hf,YXe),e(Hf,p8),e(p8,ZXe),e(Hf,eQe),e(to,oQe),e(to,Uf),e(Uf,$Q),e($Q,rQe),e(Uf,tQe),e(Uf,_8),e(_8,aQe),e(Uf,sQe),e(to,nQe),e(to,Jf),e(Jf,IQ),e(IQ,lQe),e(Jf,iQe),e(Jf,b8),e(b8,dQe),e(Jf,cQe),e(to,mQe),e(to,Kf),e(Kf,DQ),e(DQ,fQe),e(Kf,hQe),e(Kf,v8),e(v8,gQe),e(Kf,uQe),e(to,pQe),e(to,Yf),e(Yf,NQ),e(NQ,_Qe),e(Yf,bQe),e(Yf,T8),e(T8,vQe),e(Yf,TQe),e(to,FQe),e(to,Zf),e(Zf,jQ),e(jQ,EQe),e(Zf,CQe),e(Zf,F8),e(F8,MQe),e(Zf,yQe),e(Fe,wQe),e(Fe,di),e(di,AQe),e(di,OQ),e(OQ,LQe),e(di,BQe),e(di,GQ),e(GQ,xQe),e(di,kQe),e(Fe,RQe),e(Fe,r3),e(r3,ya),e(ya,PQe),e(ya,qQ),e(qQ,SQe),e(ya,$Qe),e(ya,zQ),e(zQ,IQe),e(ya,DQe),e(ya,XQ),e(XQ,NQe),e(ya,jQe),e(r3,OQe),e(r3,D),e(D,GQe),e(D,QQ),e(QQ,qQe),e(D,zQe),e(D,VQ),e(VQ,XQe),e(D,QQe),e(D,WQ),e(WQ,VQe),e(D,WQe),e(D,ci),e(ci,HQe),e(ci,HQ),e(HQ,UQe),e(ci,JQe),e(ci,UQ),e(UQ,KQe),e(ci,YQe),e(D,ZQe),e(D,t3),e(t3,eVe),e(t3,JQ),e(JQ,oVe),e(t3,rVe),e(D,tVe),e(D,KQ),e(KQ,aVe),e(D,sVe),e(D,a3),e(a3,nVe),e(a3,YQ),e(YQ,lVe),e(a3,iVe),e(D,dVe),e(D,ZQ),e(ZQ,cVe),e(D,mVe),e(D,eV),e(eV,fVe),e(D,hVe),e(D,oV),e(oV,gVe),e(D,uVe),e(D,rV),e(rV,pVe),e(D,_Ve),e(D,s3),e(s3,bVe),e(s3,tV),e(tV,vVe),e(s3,TVe),e(D,FVe),e(D,aV),e(aV,EVe),e(D,CVe),e(D,sV),e(sV,MVe),e(D,yVe),e(D,nV),e(nV,wVe),e(D,AVe),e(D,lV),e(lV,LVe),e(D,BVe),e(D,iV),e(iV,xVe),e(D,kVe),e(D,dV),e(dV,RVe),e(D,PVe),e(D,cV),e(cV,SVe),e(D,$Ve),e(D,mV),e(mV,IVe),e(D,DVe),e(D,n3),e(n3,NVe),e(n3,fV),e(fV,jVe),e(n3,OVe),e(D,GVe),e(D,hV),e(hV,qVe),e(D,zVe),e(D,gV),e(gV,XVe),e(D,QVe),e(D,uV),e(uV,VVe),e(D,WVe),e(D,pV),e(pV,HVe),e(D,UVe),e(D,_V),e(_V,JVe),e(D,KVe),e(D,bV),e(bV,YVe),e(D,ZVe),e(D,vV),e(vV,eWe),e(D,oWe),e(D,TV),e(TV,rWe),e(D,tWe),e(D,FV),e(FV,aWe),e(D,sWe),e(D,EV),e(EV,nWe),e(D,lWe),e(D,CV),e(CV,iWe),e(D,dWe),e(Fe,cWe),h(eh,Fe,null),e(Fe,mWe),e(Fe,MV),e(MV,fWe),e(Fe,hWe),h(l3,Fe,null),b(d,_5e,_),b(d,mi,_),e(mi,oh),e(oh,yV),h(i3,yV,null),e(mi,gWe),e(mi,wV),e(wV,uWe),b(d,b5e,_),b(d,jo,_),h(d3,jo,null),e(jo,pWe),e(jo,fi),e(fi,_We),e(fi,AV),e(AV,bWe),e(fi,vWe),e(fi,LV),e(LV,TWe),e(fi,FWe),e(jo,EWe),e(jo,c3),e(c3,CWe),e(c3,BV),e(BV,MWe),e(c3,yWe),e(jo,wWe),e(jo,kr),h(m3,kr,null),e(kr,AWe),e(kr,xV),e(xV,LWe),e(kr,BWe),e(kr,hi),e(hi,xWe),e(hi,kV),e(kV,kWe),e(hi,RWe),e(hi,RV),e(RV,PWe),e(hi,SWe),e(kr,$We),e(kr,PV),e(PV,IWe),e(kr,DWe),h(f3,kr,null),e(jo,NWe),e(jo,Se),h(h3,Se,null),e(Se,jWe),e(Se,SV),e(SV,OWe),e(Se,GWe),e(Se,wa),e(wa,qWe),e(wa,$V),e($V,zWe),e(wa,XWe),e(wa,IV),e(IV,QWe),e(wa,VWe),e(wa,DV),e(DV,WWe),e(wa,HWe),e(Se,UWe),e(Se,F),e(F,rh),e(rh,NV),e(NV,JWe),e(rh,KWe),e(rh,E8),e(E8,YWe),e(rh,ZWe),e(F,eHe),e(F,th),e(th,jV),e(jV,oHe),e(th,rHe),e(th,C8),e(C8,tHe),e(th,aHe),e(F,sHe),e(F,ah),e(ah,OV),e(OV,nHe),e(ah,lHe),e(ah,M8),e(M8,iHe),e(ah,dHe),e(F,cHe),e(F,sh),e(sh,GV),e(GV,mHe),e(sh,fHe),e(sh,y8),e(y8,hHe),e(sh,gHe),e(F,uHe),e(F,nh),e(nh,qV),e(qV,pHe),e(nh,_He),e(nh,w8),e(w8,bHe),e(nh,vHe),e(F,THe),e(F,lh),e(lh,zV),e(zV,FHe),e(lh,EHe),e(lh,A8),e(A8,CHe),e(lh,MHe),e(F,yHe),e(F,ih),e(ih,XV),e(XV,wHe),e(ih,AHe),e(ih,L8),e(L8,LHe),e(ih,BHe),e(F,xHe),e(F,dh),e(dh,QV),e(QV,kHe),e(dh,RHe),e(dh,B8),e(B8,PHe),e(dh,SHe),e(F,$He),e(F,ch),e(ch,VV),e(VV,IHe),e(ch,DHe),e(ch,x8),e(x8,NHe),e(ch,jHe),e(F,OHe),e(F,mh),e(mh,WV),e(WV,GHe),e(mh,qHe),e(mh,k8),e(k8,zHe),e(mh,XHe),e(F,QHe),e(F,fh),e(fh,HV),e(HV,VHe),e(fh,WHe),e(fh,R8),e(R8,HHe),e(fh,UHe),e(F,JHe),e(F,hh),e(hh,UV),e(UV,KHe),e(hh,YHe),e(hh,P8),e(P8,ZHe),e(hh,eUe),e(F,oUe),e(F,gh),e(gh,JV),e(JV,rUe),e(gh,tUe),e(gh,S8),e(S8,aUe),e(gh,sUe),e(F,nUe),e(F,uh),e(uh,KV),e(KV,lUe),e(uh,iUe),e(uh,$8),e($8,dUe),e(uh,cUe),e(F,mUe),e(F,ph),e(ph,YV),e(YV,fUe),e(ph,hUe),e(ph,I8),e(I8,gUe),e(ph,uUe),e(F,pUe),e(F,_h),e(_h,ZV),e(ZV,_Ue),e(_h,bUe),e(_h,D8),e(D8,vUe),e(_h,TUe),e(F,FUe),e(F,bh),e(bh,eW),e(eW,EUe),e(bh,CUe),e(bh,N8),e(N8,MUe),e(bh,yUe),e(F,wUe),e(F,vh),e(vh,oW),e(oW,AUe),e(vh,LUe),e(vh,j8),e(j8,BUe),e(vh,xUe),e(F,kUe),e(F,Th),e(Th,rW),e(rW,RUe),e(Th,PUe),e(Th,O8),e(O8,SUe),e(Th,$Ue),e(F,IUe),e(F,Fh),e(Fh,tW),e(tW,DUe),e(Fh,NUe),e(Fh,G8),e(G8,jUe),e(Fh,OUe),e(F,GUe),e(F,Eh),e(Eh,aW),e(aW,qUe),e(Eh,zUe),e(Eh,q8),e(q8,XUe),e(Eh,QUe),e(F,VUe),e(F,Ch),e(Ch,sW),e(sW,WUe),e(Ch,HUe),e(Ch,z8),e(z8,UUe),e(Ch,JUe),e(F,KUe),e(F,Mh),e(Mh,nW),e(nW,YUe),e(Mh,ZUe),e(Mh,X8),e(X8,eJe),e(Mh,oJe),e(F,rJe),e(F,yh),e(yh,lW),e(lW,tJe),e(yh,aJe),e(yh,Q8),e(Q8,sJe),e(yh,nJe),e(F,lJe),e(F,ln),e(ln,iW),e(iW,iJe),e(ln,dJe),e(ln,V8),e(V8,cJe),e(ln,mJe),e(ln,W8),e(W8,fJe),e(ln,hJe),e(F,gJe),e(F,wh),e(wh,dW),e(dW,uJe),e(wh,pJe),e(wh,H8),e(H8,_Je),e(wh,bJe),e(F,vJe),e(F,Ah),e(Ah,cW),e(cW,TJe),e(Ah,FJe),e(Ah,U8),e(U8,EJe),e(Ah,CJe),e(F,MJe),e(F,Lh),e(Lh,mW),e(mW,yJe),e(Lh,wJe),e(Lh,J8),e(J8,AJe),e(Lh,LJe),e(F,BJe),e(F,Bh),e(Bh,fW),e(fW,xJe),e(Bh,kJe),e(Bh,K8),e(K8,RJe),e(Bh,PJe),e(F,SJe),e(F,xh),e(xh,hW),e(hW,$Je),e(xh,IJe),e(xh,Y8),e(Y8,DJe),e(xh,NJe),e(F,jJe),e(F,kh),e(kh,gW),e(gW,OJe),e(kh,GJe),e(kh,Z8),e(Z8,qJe),e(kh,zJe),e(F,XJe),e(F,Rh),e(Rh,uW),e(uW,QJe),e(Rh,VJe),e(Rh,eB),e(eB,WJe),e(Rh,HJe),e(F,UJe),e(F,Ph),e(Ph,pW),e(pW,JJe),e(Ph,KJe),e(Ph,oB),e(oB,YJe),e(Ph,ZJe),e(F,eKe),e(F,Sh),e(Sh,_W),e(_W,oKe),e(Sh,rKe),e(Sh,rB),e(rB,tKe),e(Sh,aKe),e(F,sKe),e(F,$h),e($h,bW),e(bW,nKe),e($h,lKe),e($h,tB),e(tB,iKe),e($h,dKe),e(F,cKe),e(F,Ih),e(Ih,vW),e(vW,mKe),e(Ih,fKe),e(Ih,aB),e(aB,hKe),e(Ih,gKe),e(F,uKe),e(F,Dh),e(Dh,TW),e(TW,pKe),e(Dh,_Ke),e(Dh,sB),e(sB,bKe),e(Dh,vKe),e(F,TKe),e(F,Nh),e(Nh,FW),e(FW,FKe),e(Nh,EKe),e(Nh,nB),e(nB,CKe),e(Nh,MKe),e(F,yKe),e(F,jh),e(jh,EW),e(EW,wKe),e(jh,AKe),e(jh,lB),e(lB,LKe),e(jh,BKe),e(F,xKe),e(F,Oh),e(Oh,CW),e(CW,kKe),e(Oh,RKe),e(Oh,iB),e(iB,PKe),e(Oh,SKe),e(F,$Ke),e(F,Gh),e(Gh,MW),e(MW,IKe),e(Gh,DKe),e(Gh,dB),e(dB,NKe),e(Gh,jKe),e(F,OKe),e(F,qh),e(qh,yW),e(yW,GKe),e(qh,qKe),e(qh,cB),e(cB,zKe),e(qh,XKe),e(F,QKe),e(F,zh),e(zh,wW),e(wW,VKe),e(zh,WKe),e(zh,mB),e(mB,HKe),e(zh,UKe),e(F,JKe),e(F,Xh),e(Xh,AW),e(AW,KKe),e(Xh,YKe),e(Xh,fB),e(fB,ZKe),e(Xh,eYe),e(F,oYe),e(F,Qh),e(Qh,LW),e(LW,rYe),e(Qh,tYe),e(Qh,hB),e(hB,aYe),e(Qh,sYe),e(F,nYe),e(F,Vh),e(Vh,BW),e(BW,lYe),e(Vh,iYe),e(Vh,gB),e(gB,dYe),e(Vh,cYe),e(F,mYe),e(F,Wh),e(Wh,xW),e(xW,fYe),e(Wh,hYe),e(Wh,uB),e(uB,gYe),e(Wh,uYe),e(F,pYe),e(F,Hh),e(Hh,kW),e(kW,_Ye),e(Hh,bYe),e(Hh,pB),e(pB,vYe),e(Hh,TYe),e(F,FYe),e(F,Uh),e(Uh,RW),e(RW,EYe),e(Uh,CYe),e(Uh,_B),e(_B,MYe),e(Uh,yYe),e(F,wYe),e(F,Jh),e(Jh,PW),e(PW,AYe),e(Jh,LYe),e(Jh,bB),e(bB,BYe),e(Jh,xYe),e(F,kYe),e(F,Kh),e(Kh,SW),e(SW,RYe),e(Kh,PYe),e(Kh,vB),e(vB,SYe),e(Kh,$Ye),e(F,IYe),e(F,Yh),e(Yh,$W),e($W,DYe),e(Yh,NYe),e(Yh,TB),e(TB,jYe),e(Yh,OYe),e(F,GYe),e(F,Zh),e(Zh,IW),e(IW,qYe),e(Zh,zYe),e(Zh,FB),e(FB,XYe),e(Zh,QYe),e(F,VYe),e(F,eg),e(eg,DW),e(DW,WYe),e(eg,HYe),e(eg,EB),e(EB,UYe),e(eg,JYe),e(F,KYe),e(F,og),e(og,NW),e(NW,YYe),e(og,ZYe),e(og,CB),e(CB,eZe),e(og,oZe),e(F,rZe),e(F,rg),e(rg,jW),e(jW,tZe),e(rg,aZe),e(rg,MB),e(MB,sZe),e(rg,nZe),e(F,lZe),e(F,tg),e(tg,OW),e(OW,iZe),e(tg,dZe),e(tg,yB),e(yB,cZe),e(tg,mZe),e(F,fZe),e(F,ag),e(ag,GW),e(GW,hZe),e(ag,gZe),e(ag,wB),e(wB,uZe),e(ag,pZe),e(F,_Ze),e(F,sg),e(sg,qW),e(qW,bZe),e(sg,vZe),e(sg,AB),e(AB,TZe),e(sg,FZe),e(F,EZe),e(F,ng),e(ng,zW),e(zW,CZe),e(ng,MZe),e(ng,LB),e(LB,yZe),e(ng,wZe),e(F,AZe),e(F,lg),e(lg,XW),e(XW,LZe),e(lg,BZe),e(lg,BB),e(BB,xZe),e(lg,kZe),e(F,RZe),e(F,ig),e(ig,QW),e(QW,PZe),e(ig,SZe),e(ig,xB),e(xB,$Ze),e(ig,IZe),e(F,DZe),e(F,dg),e(dg,VW),e(VW,NZe),e(dg,jZe),e(dg,kB),e(kB,OZe),e(dg,GZe),e(F,qZe),e(F,cg),e(cg,WW),e(WW,zZe),e(cg,XZe),e(cg,RB),e(RB,QZe),e(cg,VZe),e(F,WZe),e(F,mg),e(mg,HW),e(HW,HZe),e(mg,UZe),e(mg,PB),e(PB,JZe),e(mg,KZe),e(F,YZe),e(F,fg),e(fg,UW),e(UW,ZZe),e(fg,eeo),e(fg,SB),e(SB,oeo),e(fg,reo),e(F,teo),e(F,hg),e(hg,JW),e(JW,aeo),e(hg,seo),e(hg,$B),e($B,neo),e(hg,leo),e(F,ieo),e(F,gg),e(gg,KW),e(KW,deo),e(gg,ceo),e(gg,IB),e(IB,meo),e(gg,feo),e(F,heo),e(F,ug),e(ug,YW),e(YW,geo),e(ug,ueo),e(ug,DB),e(DB,peo),e(ug,_eo),e(F,beo),e(F,pg),e(pg,ZW),e(ZW,veo),e(pg,Teo),e(pg,NB),e(NB,Feo),e(pg,Eeo),e(F,Ceo),e(F,_g),e(_g,eH),e(eH,Meo),e(_g,yeo),e(_g,jB),e(jB,weo),e(_g,Aeo),e(F,Leo),e(F,bg),e(bg,oH),e(oH,Beo),e(bg,xeo),e(bg,OB),e(OB,keo),e(bg,Reo),e(F,Peo),e(F,vg),e(vg,rH),e(rH,Seo),e(vg,$eo),e(vg,GB),e(GB,Ieo),e(vg,Deo),e(Se,Neo),e(Se,Tg),e(Tg,jeo),e(Tg,tH),e(tH,Oeo),e(Tg,Geo),e(Tg,aH),e(aH,qeo),e(Se,zeo),e(Se,sH),e(sH,Xeo),e(Se,Qeo),h(g3,Se,null),b(d,v5e,_),b(d,gi,_),e(gi,Fg),e(Fg,nH),h(u3,nH,null),e(gi,Veo),e(gi,lH),e(lH,Weo),b(d,T5e,_),b(d,Oo,_),h(p3,Oo,null),e(Oo,Heo),e(Oo,ui),e(ui,Ueo),e(ui,iH),e(iH,Jeo),e(ui,Keo),e(ui,dH),e(dH,Yeo),e(ui,Zeo),e(Oo,eoo),e(Oo,_3),e(_3,ooo),e(_3,cH),e(cH,roo),e(_3,too),e(Oo,aoo),e(Oo,Rr),h(b3,Rr,null),e(Rr,soo),e(Rr,mH),e(mH,noo),e(Rr,loo),e(Rr,pi),e(pi,ioo),e(pi,fH),e(fH,doo),e(pi,coo),e(pi,hH),e(hH,moo),e(pi,foo),e(Rr,hoo),e(Rr,gH),e(gH,goo),e(Rr,uoo),h(v3,Rr,null),e(Oo,poo),e(Oo,$e),h(T3,$e,null),e($e,_oo),e($e,uH),e(uH,boo),e($e,voo),e($e,Aa),e(Aa,Too),e(Aa,pH),e(pH,Foo),e(Aa,Eoo),e(Aa,_H),e(_H,Coo),e(Aa,Moo),e(Aa,bH),e(bH,yoo),e(Aa,woo),e($e,Aoo),e($e,R),e(R,Eg),e(Eg,vH),e(vH,Loo),e(Eg,Boo),e(Eg,qB),e(qB,xoo),e(Eg,koo),e(R,Roo),e(R,Cg),e(Cg,TH),e(TH,Poo),e(Cg,Soo),e(Cg,zB),e(zB,$oo),e(Cg,Ioo),e(R,Doo),e(R,Mg),e(Mg,FH),e(FH,Noo),e(Mg,joo),e(Mg,XB),e(XB,Ooo),e(Mg,Goo),e(R,qoo),e(R,yg),e(yg,EH),e(EH,zoo),e(yg,Xoo),e(yg,QB),e(QB,Qoo),e(yg,Voo),e(R,Woo),e(R,wg),e(wg,CH),e(CH,Hoo),e(wg,Uoo),e(wg,VB),e(VB,Joo),e(wg,Koo),e(R,Yoo),e(R,Ag),e(Ag,MH),e(MH,Zoo),e(Ag,ero),e(Ag,WB),e(WB,oro),e(Ag,rro),e(R,tro),e(R,Lg),e(Lg,yH),e(yH,aro),e(Lg,sro),e(Lg,HB),e(HB,nro),e(Lg,lro),e(R,iro),e(R,Bg),e(Bg,wH),e(wH,dro),e(Bg,cro),e(Bg,UB),e(UB,mro),e(Bg,fro),e(R,hro),e(R,xg),e(xg,AH),e(AH,gro),e(xg,uro),e(xg,JB),e(JB,pro),e(xg,_ro),e(R,bro),e(R,kg),e(kg,LH),e(LH,vro),e(kg,Tro),e(kg,KB),e(KB,Fro),e(kg,Ero),e(R,Cro),e(R,Rg),e(Rg,BH),e(BH,Mro),e(Rg,yro),e(Rg,YB),e(YB,wro),e(Rg,Aro),e(R,Lro),e(R,Pg),e(Pg,xH),e(xH,Bro),e(Pg,xro),e(Pg,ZB),e(ZB,kro),e(Pg,Rro),e(R,Pro),e(R,Sg),e(Sg,kH),e(kH,Sro),e(Sg,$ro),e(Sg,e9),e(e9,Iro),e(Sg,Dro),e(R,Nro),e(R,$g),e($g,RH),e(RH,jro),e($g,Oro),e($g,o9),e(o9,Gro),e($g,qro),e(R,zro),e(R,Ig),e(Ig,PH),e(PH,Xro),e(Ig,Qro),e(Ig,r9),e(r9,Vro),e(Ig,Wro),e(R,Hro),e(R,Dg),e(Dg,SH),e(SH,Uro),e(Dg,Jro),e(Dg,t9),e(t9,Kro),e(Dg,Yro),e(R,Zro),e(R,Ng),e(Ng,$H),e($H,eto),e(Ng,oto),e(Ng,a9),e(a9,rto),e(Ng,tto),e(R,ato),e(R,jg),e(jg,IH),e(IH,sto),e(jg,nto),e(jg,s9),e(s9,lto),e(jg,ito),e(R,dto),e(R,Og),e(Og,DH),e(DH,cto),e(Og,mto),e(Og,n9),e(n9,fto),e(Og,hto),e(R,gto),e(R,Gg),e(Gg,NH),e(NH,uto),e(Gg,pto),e(Gg,l9),e(l9,_to),e(Gg,bto),e(R,vto),e(R,qg),e(qg,jH),e(jH,Tto),e(qg,Fto),e(qg,i9),e(i9,Eto),e(qg,Cto),e(R,Mto),e(R,zg),e(zg,OH),e(OH,yto),e(zg,wto),e(zg,d9),e(d9,Ato),e(zg,Lto),e(R,Bto),e(R,Xg),e(Xg,GH),e(GH,xto),e(Xg,kto),e(Xg,c9),e(c9,Rto),e(Xg,Pto),e(R,Sto),e(R,Qg),e(Qg,qH),e(qH,$to),e(Qg,Ito),e(Qg,m9),e(m9,Dto),e(Qg,Nto),e(R,jto),e(R,Vg),e(Vg,zH),e(zH,Oto),e(Vg,Gto),e(Vg,f9),e(f9,qto),e(Vg,zto),e(R,Xto),e(R,Wg),e(Wg,XH),e(XH,Qto),e(Wg,Vto),e(Wg,h9),e(h9,Wto),e(Wg,Hto),e(R,Uto),e(R,Hg),e(Hg,QH),e(QH,Jto),e(Hg,Kto),e(Hg,g9),e(g9,Yto),e(Hg,Zto),e(R,eao),e(R,Ug),e(Ug,VH),e(VH,oao),e(Ug,rao),e(Ug,u9),e(u9,tao),e(Ug,aao),e(R,sao),e(R,Jg),e(Jg,WH),e(WH,nao),e(Jg,lao),e(Jg,p9),e(p9,iao),e(Jg,dao),e(R,cao),e(R,Kg),e(Kg,HH),e(HH,mao),e(Kg,fao),e(Kg,_9),e(_9,hao),e(Kg,gao),e(R,uao),e(R,Yg),e(Yg,UH),e(UH,pao),e(Yg,_ao),e(Yg,b9),e(b9,bao),e(Yg,vao),e(R,Tao),e(R,Zg),e(Zg,JH),e(JH,Fao),e(Zg,Eao),e(Zg,v9),e(v9,Cao),e(Zg,Mao),e(R,yao),e(R,eu),e(eu,KH),e(KH,wao),e(eu,Aao),e(eu,T9),e(T9,Lao),e(eu,Bao),e(R,xao),e(R,ou),e(ou,YH),e(YH,kao),e(ou,Rao),e(ou,F9),e(F9,Pao),e(ou,Sao),e(R,$ao),e(R,ru),e(ru,ZH),e(ZH,Iao),e(ru,Dao),e(ru,E9),e(E9,Nao),e(ru,jao),e(R,Oao),e(R,tu),e(tu,eU),e(eU,Gao),e(tu,qao),e(tu,C9),e(C9,zao),e(tu,Xao),e($e,Qao),e($e,au),e(au,Vao),e(au,oU),e(oU,Wao),e(au,Hao),e(au,rU),e(rU,Uao),e($e,Jao),e($e,tU),e(tU,Kao),e($e,Yao),h(F3,$e,null),b(d,F5e,_),b(d,_i,_),e(_i,su),e(su,aU),h(E3,aU,null),e(_i,Zao),e(_i,sU),e(sU,eso),b(d,E5e,_),b(d,Go,_),h(C3,Go,null),e(Go,oso),e(Go,bi),e(bi,rso),e(bi,nU),e(nU,tso),e(bi,aso),e(bi,lU),e(lU,sso),e(bi,nso),e(Go,lso),e(Go,M3),e(M3,iso),e(M3,iU),e(iU,dso),e(M3,cso),e(Go,mso),e(Go,Pr),h(y3,Pr,null),e(Pr,fso),e(Pr,dU),e(dU,hso),e(Pr,gso),e(Pr,vi),e(vi,uso),e(vi,cU),e(cU,pso),e(vi,_so),e(vi,mU),e(mU,bso),e(vi,vso),e(Pr,Tso),e(Pr,fU),e(fU,Fso),e(Pr,Eso),h(w3,Pr,null),e(Go,Cso),e(Go,Ie),h(A3,Ie,null),e(Ie,Mso),e(Ie,hU),e(hU,yso),e(Ie,wso),e(Ie,La),e(La,Aso),e(La,gU),e(gU,Lso),e(La,Bso),e(La,uU),e(uU,xso),e(La,kso),e(La,pU),e(pU,Rso),e(La,Pso),e(Ie,Sso),e(Ie,q),e(q,nu),e(nu,_U),e(_U,$so),e(nu,Iso),e(nu,M9),e(M9,Dso),e(nu,Nso),e(q,jso),e(q,lu),e(lu,bU),e(bU,Oso),e(lu,Gso),e(lu,y9),e(y9,qso),e(lu,zso),e(q,Xso),e(q,iu),e(iu,vU),e(vU,Qso),e(iu,Vso),e(iu,w9),e(w9,Wso),e(iu,Hso),e(q,Uso),e(q,du),e(du,TU),e(TU,Jso),e(du,Kso),e(du,A9),e(A9,Yso),e(du,Zso),e(q,eno),e(q,cu),e(cu,FU),e(FU,ono),e(cu,rno),e(cu,L9),e(L9,tno),e(cu,ano),e(q,sno),e(q,mu),e(mu,EU),e(EU,nno),e(mu,lno),e(mu,B9),e(B9,ino),e(mu,dno),e(q,cno),e(q,fu),e(fu,CU),e(CU,mno),e(fu,fno),e(fu,x9),e(x9,hno),e(fu,gno),e(q,uno),e(q,hu),e(hu,MU),e(MU,pno),e(hu,_no),e(hu,k9),e(k9,bno),e(hu,vno),e(q,Tno),e(q,gu),e(gu,yU),e(yU,Fno),e(gu,Eno),e(gu,R9),e(R9,Cno),e(gu,Mno),e(q,yno),e(q,uu),e(uu,wU),e(wU,wno),e(uu,Ano),e(uu,P9),e(P9,Lno),e(uu,Bno),e(q,xno),e(q,pu),e(pu,AU),e(AU,kno),e(pu,Rno),e(pu,S9),e(S9,Pno),e(pu,Sno),e(q,$no),e(q,_u),e(_u,LU),e(LU,Ino),e(_u,Dno),e(_u,$9),e($9,Nno),e(_u,jno),e(q,Ono),e(q,bu),e(bu,BU),e(BU,Gno),e(bu,qno),e(bu,I9),e(I9,zno),e(bu,Xno),e(q,Qno),e(q,vu),e(vu,xU),e(xU,Vno),e(vu,Wno),e(vu,D9),e(D9,Hno),e(vu,Uno),e(q,Jno),e(q,Tu),e(Tu,kU),e(kU,Kno),e(Tu,Yno),e(Tu,N9),e(N9,Zno),e(Tu,elo),e(q,olo),e(q,Fu),e(Fu,RU),e(RU,rlo),e(Fu,tlo),e(Fu,j9),e(j9,alo),e(Fu,slo),e(q,nlo),e(q,Eu),e(Eu,PU),e(PU,llo),e(Eu,ilo),e(Eu,O9),e(O9,dlo),e(Eu,clo),e(q,mlo),e(q,Cu),e(Cu,SU),e(SU,flo),e(Cu,hlo),e(Cu,G9),e(G9,glo),e(Cu,ulo),e(q,plo),e(q,Mu),e(Mu,$U),e($U,_lo),e(Mu,blo),e(Mu,q9),e(q9,vlo),e(Mu,Tlo),e(q,Flo),e(q,yu),e(yu,IU),e(IU,Elo),e(yu,Clo),e(yu,z9),e(z9,Mlo),e(yu,ylo),e(q,wlo),e(q,wu),e(wu,DU),e(DU,Alo),e(wu,Llo),e(wu,X9),e(X9,Blo),e(wu,xlo),e(q,klo),e(q,Au),e(Au,NU),e(NU,Rlo),e(Au,Plo),e(Au,Q9),e(Q9,Slo),e(Au,$lo),e(q,Ilo),e(q,Lu),e(Lu,jU),e(jU,Dlo),e(Lu,Nlo),e(Lu,V9),e(V9,jlo),e(Lu,Olo),e(q,Glo),e(q,Bu),e(Bu,OU),e(OU,qlo),e(Bu,zlo),e(Bu,W9),e(W9,Xlo),e(Bu,Qlo),e(q,Vlo),e(q,xu),e(xu,GU),e(GU,Wlo),e(xu,Hlo),e(xu,H9),e(H9,Ulo),e(xu,Jlo),e(q,Klo),e(q,ku),e(ku,qU),e(qU,Ylo),e(ku,Zlo),e(ku,U9),e(U9,eio),e(ku,oio),e(q,rio),e(q,Ru),e(Ru,zU),e(zU,tio),e(Ru,aio),e(Ru,J9),e(J9,sio),e(Ru,nio),e(q,lio),e(q,Pu),e(Pu,XU),e(XU,iio),e(Pu,dio),e(Pu,K9),e(K9,cio),e(Pu,mio),e(q,fio),e(q,Su),e(Su,QU),e(QU,hio),e(Su,gio),e(Su,Y9),e(Y9,uio),e(Su,pio),e(q,_io),e(q,$u),e($u,VU),e(VU,bio),e($u,vio),e($u,Z9),e(Z9,Tio),e($u,Fio),e(Ie,Eio),e(Ie,Iu),e(Iu,Cio),e(Iu,WU),e(WU,Mio),e(Iu,yio),e(Iu,HU),e(HU,wio),e(Ie,Aio),e(Ie,UU),e(UU,Lio),e(Ie,Bio),h(L3,Ie,null),b(d,C5e,_),b(d,Ti,_),e(Ti,Du),e(Du,JU),h(B3,JU,null),e(Ti,xio),e(Ti,KU),e(KU,kio),b(d,M5e,_),b(d,qo,_),h(x3,qo,null),e(qo,Rio),e(qo,Fi),e(Fi,Pio),e(Fi,YU),e(YU,Sio),e(Fi,$io),e(Fi,ZU),e(ZU,Iio),e(Fi,Dio),e(qo,Nio),e(qo,k3),e(k3,jio),e(k3,eJ),e(eJ,Oio),e(k3,Gio),e(qo,qio),e(qo,Sr),h(R3,Sr,null),e(Sr,zio),e(Sr,oJ),e(oJ,Xio),e(Sr,Qio),e(Sr,Ei),e(Ei,Vio),e(Ei,rJ),e(rJ,Wio),e(Ei,Hio),e(Ei,tJ),e(tJ,Uio),e(Ei,Jio),e(Sr,Kio),e(Sr,aJ),e(aJ,Yio),e(Sr,Zio),h(P3,Sr,null),e(qo,edo),e(qo,De),h(S3,De,null),e(De,odo),e(De,sJ),e(sJ,rdo),e(De,tdo),e(De,Ba),e(Ba,ado),e(Ba,nJ),e(nJ,sdo),e(Ba,ndo),e(Ba,lJ),e(lJ,ldo),e(Ba,ido),e(Ba,iJ),e(iJ,ddo),e(Ba,cdo),e(De,mdo),e(De,O),e(O,Nu),e(Nu,dJ),e(dJ,fdo),e(Nu,hdo),e(Nu,ex),e(ex,gdo),e(Nu,udo),e(O,pdo),e(O,ju),e(ju,cJ),e(cJ,_do),e(ju,bdo),e(ju,ox),e(ox,vdo),e(ju,Tdo),e(O,Fdo),e(O,Ou),e(Ou,mJ),e(mJ,Edo),e(Ou,Cdo),e(Ou,rx),e(rx,Mdo),e(Ou,ydo),e(O,wdo),e(O,Gu),e(Gu,fJ),e(fJ,Ado),e(Gu,Ldo),e(Gu,tx),e(tx,Bdo),e(Gu,xdo),e(O,kdo),e(O,qu),e(qu,hJ),e(hJ,Rdo),e(qu,Pdo),e(qu,ax),e(ax,Sdo),e(qu,$do),e(O,Ido),e(O,zu),e(zu,gJ),e(gJ,Ddo),e(zu,Ndo),e(zu,sx),e(sx,jdo),e(zu,Odo),e(O,Gdo),e(O,Xu),e(Xu,uJ),e(uJ,qdo),e(Xu,zdo),e(Xu,nx),e(nx,Xdo),e(Xu,Qdo),e(O,Vdo),e(O,Qu),e(Qu,pJ),e(pJ,Wdo),e(Qu,Hdo),e(Qu,lx),e(lx,Udo),e(Qu,Jdo),e(O,Kdo),e(O,Vu),e(Vu,_J),e(_J,Ydo),e(Vu,Zdo),e(Vu,ix),e(ix,eco),e(Vu,oco),e(O,rco),e(O,Wu),e(Wu,bJ),e(bJ,tco),e(Wu,aco),e(Wu,dx),e(dx,sco),e(Wu,nco),e(O,lco),e(O,Hu),e(Hu,vJ),e(vJ,ico),e(Hu,dco),e(Hu,cx),e(cx,cco),e(Hu,mco),e(O,fco),e(O,Uu),e(Uu,TJ),e(TJ,hco),e(Uu,gco),e(Uu,mx),e(mx,uco),e(Uu,pco),e(O,_co),e(O,Ju),e(Ju,FJ),e(FJ,bco),e(Ju,vco),e(Ju,fx),e(fx,Tco),e(Ju,Fco),e(O,Eco),e(O,Ku),e(Ku,EJ),e(EJ,Cco),e(Ku,Mco),e(Ku,hx),e(hx,yco),e(Ku,wco),e(O,Aco),e(O,Yu),e(Yu,CJ),e(CJ,Lco),e(Yu,Bco),e(Yu,gx),e(gx,xco),e(Yu,kco),e(O,Rco),e(O,Zu),e(Zu,MJ),e(MJ,Pco),e(Zu,Sco),e(Zu,ux),e(ux,$co),e(Zu,Ico),e(O,Dco),e(O,ep),e(ep,yJ),e(yJ,Nco),e(ep,jco),e(ep,px),e(px,Oco),e(ep,Gco),e(O,qco),e(O,op),e(op,wJ),e(wJ,zco),e(op,Xco),e(op,_x),e(_x,Qco),e(op,Vco),e(O,Wco),e(O,rp),e(rp,AJ),e(AJ,Hco),e(rp,Uco),e(rp,bx),e(bx,Jco),e(rp,Kco),e(O,Yco),e(O,tp),e(tp,LJ),e(LJ,Zco),e(tp,emo),e(tp,vx),e(vx,omo),e(tp,rmo),e(O,tmo),e(O,ap),e(ap,BJ),e(BJ,amo),e(ap,smo),e(ap,Tx),e(Tx,nmo),e(ap,lmo),e(O,imo),e(O,sp),e(sp,xJ),e(xJ,dmo),e(sp,cmo),e(sp,Fx),e(Fx,mmo),e(sp,fmo),e(O,hmo),e(O,np),e(np,kJ),e(kJ,gmo),e(np,umo),e(np,Ex),e(Ex,pmo),e(np,_mo),e(O,bmo),e(O,lp),e(lp,RJ),e(RJ,vmo),e(lp,Tmo),e(lp,Cx),e(Cx,Fmo),e(lp,Emo),e(O,Cmo),e(O,ip),e(ip,PJ),e(PJ,Mmo),e(ip,ymo),e(ip,Mx),e(Mx,wmo),e(ip,Amo),e(O,Lmo),e(O,dp),e(dp,SJ),e(SJ,Bmo),e(dp,xmo),e(dp,yx),e(yx,kmo),e(dp,Rmo),e(O,Pmo),e(O,cp),e(cp,$J),e($J,Smo),e(cp,$mo),e(cp,wx),e(wx,Imo),e(cp,Dmo),e(O,Nmo),e(O,mp),e(mp,IJ),e(IJ,jmo),e(mp,Omo),e(mp,Ax),e(Ax,Gmo),e(mp,qmo),e(O,zmo),e(O,fp),e(fp,DJ),e(DJ,Xmo),e(fp,Qmo),e(fp,NJ),e(NJ,Vmo),e(fp,Wmo),e(O,Hmo),e(O,hp),e(hp,jJ),e(jJ,Umo),e(hp,Jmo),e(hp,Lx),e(Lx,Kmo),e(hp,Ymo),e(O,Zmo),e(O,gp),e(gp,OJ),e(OJ,efo),e(gp,ofo),e(gp,Bx),e(Bx,rfo),e(gp,tfo),e(De,afo),e(De,up),e(up,sfo),e(up,GJ),e(GJ,nfo),e(up,lfo),e(up,qJ),e(qJ,ifo),e(De,dfo),e(De,zJ),e(zJ,cfo),e(De,mfo),h($3,De,null),b(d,y5e,_),b(d,Ci,_),e(Ci,pp),e(pp,XJ),h(I3,XJ,null),e(Ci,ffo),e(Ci,QJ),e(QJ,hfo),b(d,w5e,_),b(d,zo,_),h(D3,zo,null),e(zo,gfo),e(zo,Mi),e(Mi,ufo),e(Mi,VJ),e(VJ,pfo),e(Mi,_fo),e(Mi,WJ),e(WJ,bfo),e(Mi,vfo),e(zo,Tfo),e(zo,N3),e(N3,Ffo),e(N3,HJ),e(HJ,Efo),e(N3,Cfo),e(zo,Mfo),e(zo,$r),h(j3,$r,null),e($r,yfo),e($r,UJ),e(UJ,wfo),e($r,Afo),e($r,yi),e(yi,Lfo),e(yi,JJ),e(JJ,Bfo),e(yi,xfo),e(yi,KJ),e(KJ,kfo),e(yi,Rfo),e($r,Pfo),e($r,YJ),e(YJ,Sfo),e($r,$fo),h(O3,$r,null),e(zo,Ifo),e(zo,Ne),h(G3,Ne,null),e(Ne,Dfo),e(Ne,ZJ),e(ZJ,Nfo),e(Ne,jfo),e(Ne,xa),e(xa,Ofo),e(xa,eK),e(eK,Gfo),e(xa,qfo),e(xa,oK),e(oK,zfo),e(xa,Xfo),e(xa,rK),e(rK,Qfo),e(xa,Vfo),e(Ne,Wfo),e(Ne,me),e(me,_p),e(_p,tK),e(tK,Hfo),e(_p,Ufo),e(_p,xx),e(xx,Jfo),e(_p,Kfo),e(me,Yfo),e(me,bp),e(bp,aK),e(aK,Zfo),e(bp,eho),e(bp,kx),e(kx,oho),e(bp,rho),e(me,tho),e(me,vp),e(vp,sK),e(sK,aho),e(vp,sho),e(vp,Rx),e(Rx,nho),e(vp,lho),e(me,iho),e(me,Tp),e(Tp,nK),e(nK,dho),e(Tp,cho),e(Tp,Px),e(Px,mho),e(Tp,fho),e(me,hho),e(me,Fp),e(Fp,lK),e(lK,gho),e(Fp,uho),e(Fp,Sx),e(Sx,pho),e(Fp,_ho),e(me,bho),e(me,Ep),e(Ep,iK),e(iK,vho),e(Ep,Tho),e(Ep,$x),e($x,Fho),e(Ep,Eho),e(me,Cho),e(me,Cp),e(Cp,dK),e(dK,Mho),e(Cp,yho),e(Cp,Ix),e(Ix,who),e(Cp,Aho),e(me,Lho),e(me,Mp),e(Mp,cK),e(cK,Bho),e(Mp,xho),e(Mp,Dx),e(Dx,kho),e(Mp,Rho),e(me,Pho),e(me,yp),e(yp,mK),e(mK,Sho),e(yp,$ho),e(yp,Nx),e(Nx,Iho),e(yp,Dho),e(me,Nho),e(me,wp),e(wp,fK),e(fK,jho),e(wp,Oho),e(wp,jx),e(jx,Gho),e(wp,qho),e(me,zho),e(me,Ap),e(Ap,hK),e(hK,Xho),e(Ap,Qho),e(Ap,Ox),e(Ox,Vho),e(Ap,Who),e(me,Hho),e(me,Lp),e(Lp,gK),e(gK,Uho),e(Lp,Jho),e(Lp,Gx),e(Gx,Kho),e(Lp,Yho),e(me,Zho),e(me,Bp),e(Bp,uK),e(uK,ego),e(Bp,ogo),e(Bp,qx),e(qx,rgo),e(Bp,tgo),e(me,ago),e(me,xp),e(xp,pK),e(pK,sgo),e(xp,ngo),e(xp,zx),e(zx,lgo),e(xp,igo),e(me,dgo),e(me,kp),e(kp,_K),e(_K,cgo),e(kp,mgo),e(kp,Xx),e(Xx,fgo),e(kp,hgo),e(Ne,ggo),e(Ne,Rp),e(Rp,ugo),e(Rp,bK),e(bK,pgo),e(Rp,_go),e(Rp,vK),e(vK,bgo),e(Ne,vgo),e(Ne,TK),e(TK,Tgo),e(Ne,Fgo),h(q3,Ne,null),b(d,A5e,_),b(d,wi,_),e(wi,Pp),e(Pp,FK),h(z3,FK,null),e(wi,Ego),e(wi,EK),e(EK,Cgo),b(d,L5e,_),b(d,Xo,_),h(X3,Xo,null),e(Xo,Mgo),e(Xo,Ai),e(Ai,ygo),e(Ai,CK),e(CK,wgo),e(Ai,Ago),e(Ai,MK),e(MK,Lgo),e(Ai,Bgo),e(Xo,xgo),e(Xo,Q3),e(Q3,kgo),e(Q3,yK),e(yK,Rgo),e(Q3,Pgo),e(Xo,Sgo),e(Xo,Ir),h(V3,Ir,null),e(Ir,$go),e(Ir,wK),e(wK,Igo),e(Ir,Dgo),e(Ir,Li),e(Li,Ngo),e(Li,AK),e(AK,jgo),e(Li,Ogo),e(Li,LK),e(LK,Ggo),e(Li,qgo),e(Ir,zgo),e(Ir,BK),e(BK,Xgo),e(Ir,Qgo),h(W3,Ir,null),e(Xo,Vgo),e(Xo,je),h(H3,je,null),e(je,Wgo),e(je,xK),e(xK,Hgo),e(je,Ugo),e(je,ka),e(ka,Jgo),e(ka,kK),e(kK,Kgo),e(ka,Ygo),e(ka,RK),e(RK,Zgo),e(ka,euo),e(ka,PK),e(PK,ouo),e(ka,ruo),e(je,tuo),e(je,A),e(A,Sp),e(Sp,SK),e(SK,auo),e(Sp,suo),e(Sp,Qx),e(Qx,nuo),e(Sp,luo),e(A,iuo),e(A,$p),e($p,$K),e($K,duo),e($p,cuo),e($p,Vx),e(Vx,muo),e($p,fuo),e(A,huo),e(A,Ip),e(Ip,IK),e(IK,guo),e(Ip,uuo),e(Ip,Wx),e(Wx,puo),e(Ip,_uo),e(A,buo),e(A,Dp),e(Dp,DK),e(DK,vuo),e(Dp,Tuo),e(Dp,Hx),e(Hx,Fuo),e(Dp,Euo),e(A,Cuo),e(A,Np),e(Np,NK),e(NK,Muo),e(Np,yuo),e(Np,Ux),e(Ux,wuo),e(Np,Auo),e(A,Luo),e(A,jp),e(jp,jK),e(jK,Buo),e(jp,xuo),e(jp,Jx),e(Jx,kuo),e(jp,Ruo),e(A,Puo),e(A,Op),e(Op,OK),e(OK,Suo),e(Op,$uo),e(Op,Kx),e(Kx,Iuo),e(Op,Duo),e(A,Nuo),e(A,Gp),e(Gp,GK),e(GK,juo),e(Gp,Ouo),e(Gp,Yx),e(Yx,Guo),e(Gp,quo),e(A,zuo),e(A,qp),e(qp,qK),e(qK,Xuo),e(qp,Quo),e(qp,Zx),e(Zx,Vuo),e(qp,Wuo),e(A,Huo),e(A,zp),e(zp,zK),e(zK,Uuo),e(zp,Juo),e(zp,ek),e(ek,Kuo),e(zp,Yuo),e(A,Zuo),e(A,Xp),e(Xp,XK),e(XK,epo),e(Xp,opo),e(Xp,ok),e(ok,rpo),e(Xp,tpo),e(A,apo),e(A,Qp),e(Qp,QK),e(QK,spo),e(Qp,npo),e(Qp,rk),e(rk,lpo),e(Qp,ipo),e(A,dpo),e(A,Vp),e(Vp,VK),e(VK,cpo),e(Vp,mpo),e(Vp,tk),e(tk,fpo),e(Vp,hpo),e(A,gpo),e(A,Wp),e(Wp,WK),e(WK,upo),e(Wp,ppo),e(Wp,ak),e(ak,_po),e(Wp,bpo),e(A,vpo),e(A,Hp),e(Hp,HK),e(HK,Tpo),e(Hp,Fpo),e(Hp,sk),e(sk,Epo),e(Hp,Cpo),e(A,Mpo),e(A,Up),e(Up,UK),e(UK,ypo),e(Up,wpo),e(Up,nk),e(nk,Apo),e(Up,Lpo),e(A,Bpo),e(A,Jp),e(Jp,JK),e(JK,xpo),e(Jp,kpo),e(Jp,lk),e(lk,Rpo),e(Jp,Ppo),e(A,Spo),e(A,Kp),e(Kp,KK),e(KK,$po),e(Kp,Ipo),e(Kp,ik),e(ik,Dpo),e(Kp,Npo),e(A,jpo),e(A,Yp),e(Yp,YK),e(YK,Opo),e(Yp,Gpo),e(Yp,dk),e(dk,qpo),e(Yp,zpo),e(A,Xpo),e(A,Zp),e(Zp,ZK),e(ZK,Qpo),e(Zp,Vpo),e(Zp,ck),e(ck,Wpo),e(Zp,Hpo),e(A,Upo),e(A,e_),e(e_,eY),e(eY,Jpo),e(e_,Kpo),e(e_,mk),e(mk,Ypo),e(e_,Zpo),e(A,e_o),e(A,o_),e(o_,oY),e(oY,o_o),e(o_,r_o),e(o_,fk),e(fk,t_o),e(o_,a_o),e(A,s_o),e(A,r_),e(r_,rY),e(rY,n_o),e(r_,l_o),e(r_,hk),e(hk,i_o),e(r_,d_o),e(A,c_o),e(A,t_),e(t_,tY),e(tY,m_o),e(t_,f_o),e(t_,gk),e(gk,h_o),e(t_,g_o),e(A,u_o),e(A,a_),e(a_,aY),e(aY,p_o),e(a_,__o),e(a_,uk),e(uk,b_o),e(a_,v_o),e(A,T_o),e(A,s_),e(s_,sY),e(sY,F_o),e(s_,E_o),e(s_,pk),e(pk,C_o),e(s_,M_o),e(A,y_o),e(A,n_),e(n_,nY),e(nY,w_o),e(n_,A_o),e(n_,_k),e(_k,L_o),e(n_,B_o),e(A,x_o),e(A,l_),e(l_,lY),e(lY,k_o),e(l_,R_o),e(l_,bk),e(bk,P_o),e(l_,S_o),e(A,$_o),e(A,i_),e(i_,iY),e(iY,I_o),e(i_,D_o),e(i_,vk),e(vk,N_o),e(i_,j_o),e(A,O_o),e(A,d_),e(d_,dY),e(dY,G_o),e(d_,q_o),e(d_,Tk),e(Tk,z_o),e(d_,X_o),e(A,Q_o),e(A,c_),e(c_,cY),e(cY,V_o),e(c_,W_o),e(c_,Fk),e(Fk,H_o),e(c_,U_o),e(A,J_o),e(A,m_),e(m_,mY),e(mY,K_o),e(m_,Y_o),e(m_,Ek),e(Ek,Z_o),e(m_,ebo),e(A,obo),e(A,f_),e(f_,fY),e(fY,rbo),e(f_,tbo),e(f_,Ck),e(Ck,abo),e(f_,sbo),e(A,nbo),e(A,h_),e(h_,hY),e(hY,lbo),e(h_,ibo),e(h_,Mk),e(Mk,dbo),e(h_,cbo),e(A,mbo),e(A,g_),e(g_,gY),e(gY,fbo),e(g_,hbo),e(g_,yk),e(yk,gbo),e(g_,ubo),e(A,pbo),e(A,u_),e(u_,uY),e(uY,_bo),e(u_,bbo),e(u_,wk),e(wk,vbo),e(u_,Tbo),e(A,Fbo),e(A,p_),e(p_,pY),e(pY,Ebo),e(p_,Cbo),e(p_,Ak),e(Ak,Mbo),e(p_,ybo),e(A,wbo),e(A,__),e(__,_Y),e(_Y,Abo),e(__,Lbo),e(__,Lk),e(Lk,Bbo),e(__,xbo),e(A,kbo),e(A,b_),e(b_,bY),e(bY,Rbo),e(b_,Pbo),e(b_,Bk),e(Bk,Sbo),e(b_,$bo),e(A,Ibo),e(A,v_),e(v_,vY),e(vY,Dbo),e(v_,Nbo),e(v_,xk),e(xk,jbo),e(v_,Obo),e(A,Gbo),e(A,T_),e(T_,TY),e(TY,qbo),e(T_,zbo),e(T_,kk),e(kk,Xbo),e(T_,Qbo),e(je,Vbo),e(je,F_),e(F_,Wbo),e(F_,FY),e(FY,Hbo),e(F_,Ubo),e(F_,EY),e(EY,Jbo),e(je,Kbo),e(je,CY),e(CY,Ybo),e(je,Zbo),h(U3,je,null),b(d,B5e,_),b(d,Bi,_),e(Bi,E_),e(E_,MY),h(J3,MY,null),e(Bi,e2o),e(Bi,yY),e(yY,o2o),b(d,x5e,_),b(d,Qo,_),h(K3,Qo,null),e(Qo,r2o),e(Qo,xi),e(xi,t2o),e(xi,wY),e(wY,a2o),e(xi,s2o),e(xi,AY),e(AY,n2o),e(xi,l2o),e(Qo,i2o),e(Qo,Y3),e(Y3,d2o),e(Y3,LY),e(LY,c2o),e(Y3,m2o),e(Qo,f2o),e(Qo,Dr),h(Z3,Dr,null),e(Dr,h2o),e(Dr,BY),e(BY,g2o),e(Dr,u2o),e(Dr,ki),e(ki,p2o),e(ki,xY),e(xY,_2o),e(ki,b2o),e(ki,kY),e(kY,v2o),e(ki,T2o),e(Dr,F2o),e(Dr,RY),e(RY,E2o),e(Dr,C2o),h(eM,Dr,null),e(Qo,M2o),e(Qo,Oe),h(oM,Oe,null),e(Oe,y2o),e(Oe,PY),e(PY,w2o),e(Oe,A2o),e(Oe,Ra),e(Ra,L2o),e(Ra,SY),e(SY,B2o),e(Ra,x2o),e(Ra,$Y),e($Y,k2o),e(Ra,R2o),e(Ra,IY),e(IY,P2o),e(Ra,S2o),e(Oe,$2o),e(Oe,H),e(H,C_),e(C_,DY),e(DY,I2o),e(C_,D2o),e(C_,Rk),e(Rk,N2o),e(C_,j2o),e(H,O2o),e(H,M_),e(M_,NY),e(NY,G2o),e(M_,q2o),e(M_,Pk),e(Pk,z2o),e(M_,X2o),e(H,Q2o),e(H,y_),e(y_,jY),e(jY,V2o),e(y_,W2o),e(y_,Sk),e(Sk,H2o),e(y_,U2o),e(H,J2o),e(H,w_),e(w_,OY),e(OY,K2o),e(w_,Y2o),e(w_,$k),e($k,Z2o),e(w_,evo),e(H,ovo),e(H,A_),e(A_,GY),e(GY,rvo),e(A_,tvo),e(A_,Ik),e(Ik,avo),e(A_,svo),e(H,nvo),e(H,L_),e(L_,qY),e(qY,lvo),e(L_,ivo),e(L_,Dk),e(Dk,dvo),e(L_,cvo),e(H,mvo),e(H,B_),e(B_,zY),e(zY,fvo),e(B_,hvo),e(B_,Nk),e(Nk,gvo),e(B_,uvo),e(H,pvo),e(H,x_),e(x_,XY),e(XY,_vo),e(x_,bvo),e(x_,jk),e(jk,vvo),e(x_,Tvo),e(H,Fvo),e(H,k_),e(k_,QY),e(QY,Evo),e(k_,Cvo),e(k_,Ok),e(Ok,Mvo),e(k_,yvo),e(H,wvo),e(H,R_),e(R_,VY),e(VY,Avo),e(R_,Lvo),e(R_,Gk),e(Gk,Bvo),e(R_,xvo),e(H,kvo),e(H,P_),e(P_,WY),e(WY,Rvo),e(P_,Pvo),e(P_,qk),e(qk,Svo),e(P_,$vo),e(H,Ivo),e(H,S_),e(S_,HY),e(HY,Dvo),e(S_,Nvo),e(S_,zk),e(zk,jvo),e(S_,Ovo),e(H,Gvo),e(H,$_),e($_,UY),e(UY,qvo),e($_,zvo),e($_,Xk),e(Xk,Xvo),e($_,Qvo),e(H,Vvo),e(H,I_),e(I_,JY),e(JY,Wvo),e(I_,Hvo),e(I_,Qk),e(Qk,Uvo),e(I_,Jvo),e(H,Kvo),e(H,D_),e(D_,KY),e(KY,Yvo),e(D_,Zvo),e(D_,Vk),e(Vk,eTo),e(D_,oTo),e(H,rTo),e(H,N_),e(N_,YY),e(YY,tTo),e(N_,aTo),e(N_,Wk),e(Wk,sTo),e(N_,nTo),e(H,lTo),e(H,j_),e(j_,ZY),e(ZY,iTo),e(j_,dTo),e(j_,Hk),e(Hk,cTo),e(j_,mTo),e(H,fTo),e(H,O_),e(O_,eZ),e(eZ,hTo),e(O_,gTo),e(O_,Uk),e(Uk,uTo),e(O_,pTo),e(H,_To),e(H,G_),e(G_,oZ),e(oZ,bTo),e(G_,vTo),e(G_,Jk),e(Jk,TTo),e(G_,FTo),e(H,ETo),e(H,q_),e(q_,rZ),e(rZ,CTo),e(q_,MTo),e(q_,Kk),e(Kk,yTo),e(q_,wTo),e(H,ATo),e(H,z_),e(z_,tZ),e(tZ,LTo),e(z_,BTo),e(z_,Yk),e(Yk,xTo),e(z_,kTo),e(H,RTo),e(H,X_),e(X_,aZ),e(aZ,PTo),e(X_,STo),e(X_,Zk),e(Zk,$To),e(X_,ITo),e(H,DTo),e(H,Q_),e(Q_,sZ),e(sZ,NTo),e(Q_,jTo),e(Q_,eR),e(eR,OTo),e(Q_,GTo),e(H,qTo),e(H,V_),e(V_,nZ),e(nZ,zTo),e(V_,XTo),e(V_,oR),e(oR,QTo),e(V_,VTo),e(Oe,WTo),e(Oe,W_),e(W_,HTo),e(W_,lZ),e(lZ,UTo),e(W_,JTo),e(W_,iZ),e(iZ,KTo),e(Oe,YTo),e(Oe,dZ),e(dZ,ZTo),e(Oe,e1o),h(rM,Oe,null),b(d,k5e,_),b(d,Ri,_),e(Ri,H_),e(H_,cZ),h(tM,cZ,null),e(Ri,o1o),e(Ri,mZ),e(mZ,r1o),b(d,R5e,_),b(d,Vo,_),h(aM,Vo,null),e(Vo,t1o),e(Vo,Pi),e(Pi,a1o),e(Pi,fZ),e(fZ,s1o),e(Pi,n1o),e(Pi,hZ),e(hZ,l1o),e(Pi,i1o),e(Vo,d1o),e(Vo,sM),e(sM,c1o),e(sM,gZ),e(gZ,m1o),e(sM,f1o),e(Vo,h1o),e(Vo,Nr),h(nM,Nr,null),e(Nr,g1o),e(Nr,uZ),e(uZ,u1o),e(Nr,p1o),e(Nr,Si),e(Si,_1o),e(Si,pZ),e(pZ,b1o),e(Si,v1o),e(Si,_Z),e(_Z,T1o),e(Si,F1o),e(Nr,E1o),e(Nr,bZ),e(bZ,C1o),e(Nr,M1o),h(lM,Nr,null),e(Vo,y1o),e(Vo,Ge),h(iM,Ge,null),e(Ge,w1o),e(Ge,vZ),e(vZ,A1o),e(Ge,L1o),e(Ge,Pa),e(Pa,B1o),e(Pa,TZ),e(TZ,x1o),e(Pa,k1o),e(Pa,FZ),e(FZ,R1o),e(Pa,P1o),e(Pa,EZ),e(EZ,S1o),e(Pa,$1o),e(Ge,I1o),e(Ge,jt),e(jt,U_),e(U_,CZ),e(CZ,D1o),e(U_,N1o),e(U_,rR),e(rR,j1o),e(U_,O1o),e(jt,G1o),e(jt,J_),e(J_,MZ),e(MZ,q1o),e(J_,z1o),e(J_,tR),e(tR,X1o),e(J_,Q1o),e(jt,V1o),e(jt,K_),e(K_,yZ),e(yZ,W1o),e(K_,H1o),e(K_,aR),e(aR,U1o),e(K_,J1o),e(jt,K1o),e(jt,Y_),e(Y_,wZ),e(wZ,Y1o),e(Y_,Z1o),e(Y_,sR),e(sR,eFo),e(Y_,oFo),e(jt,rFo),e(jt,Z_),e(Z_,AZ),e(AZ,tFo),e(Z_,aFo),e(Z_,nR),e(nR,sFo),e(Z_,nFo),e(Ge,lFo),e(Ge,eb),e(eb,iFo),e(eb,LZ),e(LZ,dFo),e(eb,cFo),e(eb,BZ),e(BZ,mFo),e(Ge,fFo),e(Ge,xZ),e(xZ,hFo),e(Ge,gFo),h(dM,Ge,null),b(d,P5e,_),b(d,$i,_),e($i,ob),e(ob,kZ),h(cM,kZ,null),e($i,uFo),e($i,RZ),e(RZ,pFo),b(d,S5e,_),b(d,Wo,_),h(mM,Wo,null),e(Wo,_Fo),e(Wo,Ii),e(Ii,bFo),e(Ii,PZ),e(PZ,vFo),e(Ii,TFo),e(Ii,SZ),e(SZ,FFo),e(Ii,EFo),e(Wo,CFo),e(Wo,fM),e(fM,MFo),e(fM,$Z),e($Z,yFo),e(fM,wFo),e(Wo,AFo),e(Wo,jr),h(hM,jr,null),e(jr,LFo),e(jr,IZ),e(IZ,BFo),e(jr,xFo),e(jr,Di),e(Di,kFo),e(Di,DZ),e(DZ,RFo),e(Di,PFo),e(Di,NZ),e(NZ,SFo),e(Di,$Fo),e(jr,IFo),e(jr,jZ),e(jZ,DFo),e(jr,NFo),h(gM,jr,null),e(Wo,jFo),e(Wo,qe),h(uM,qe,null),e(qe,OFo),e(qe,OZ),e(OZ,GFo),e(qe,qFo),e(qe,Sa),e(Sa,zFo),e(Sa,GZ),e(GZ,XFo),e(Sa,QFo),e(Sa,qZ),e(qZ,VFo),e(Sa,WFo),e(Sa,zZ),e(zZ,HFo),e(Sa,UFo),e(qe,JFo),e(qe,X),e(X,rb),e(rb,XZ),e(XZ,KFo),e(rb,YFo),e(rb,lR),e(lR,ZFo),e(rb,eEo),e(X,oEo),e(X,tb),e(tb,QZ),e(QZ,rEo),e(tb,tEo),e(tb,iR),e(iR,aEo),e(tb,sEo),e(X,nEo),e(X,ab),e(ab,VZ),e(VZ,lEo),e(ab,iEo),e(ab,dR),e(dR,dEo),e(ab,cEo),e(X,mEo),e(X,sb),e(sb,WZ),e(WZ,fEo),e(sb,hEo),e(sb,cR),e(cR,gEo),e(sb,uEo),e(X,pEo),e(X,nb),e(nb,HZ),e(HZ,_Eo),e(nb,bEo),e(nb,mR),e(mR,vEo),e(nb,TEo),e(X,FEo),e(X,lb),e(lb,UZ),e(UZ,EEo),e(lb,CEo),e(lb,fR),e(fR,MEo),e(lb,yEo),e(X,wEo),e(X,ib),e(ib,JZ),e(JZ,AEo),e(ib,LEo),e(ib,hR),e(hR,BEo),e(ib,xEo),e(X,kEo),e(X,db),e(db,KZ),e(KZ,REo),e(db,PEo),e(db,gR),e(gR,SEo),e(db,$Eo),e(X,IEo),e(X,cb),e(cb,YZ),e(YZ,DEo),e(cb,NEo),e(cb,uR),e(uR,jEo),e(cb,OEo),e(X,GEo),e(X,mb),e(mb,ZZ),e(ZZ,qEo),e(mb,zEo),e(mb,pR),e(pR,XEo),e(mb,QEo),e(X,VEo),e(X,fb),e(fb,eee),e(eee,WEo),e(fb,HEo),e(fb,_R),e(_R,UEo),e(fb,JEo),e(X,KEo),e(X,hb),e(hb,oee),e(oee,YEo),e(hb,ZEo),e(hb,bR),e(bR,e4o),e(hb,o4o),e(X,r4o),e(X,gb),e(gb,ree),e(ree,t4o),e(gb,a4o),e(gb,vR),e(vR,s4o),e(gb,n4o),e(X,l4o),e(X,ub),e(ub,tee),e(tee,i4o),e(ub,d4o),e(ub,TR),e(TR,c4o),e(ub,m4o),e(X,f4o),e(X,pb),e(pb,aee),e(aee,h4o),e(pb,g4o),e(pb,FR),e(FR,u4o),e(pb,p4o),e(X,_4o),e(X,_b),e(_b,see),e(see,b4o),e(_b,v4o),e(_b,ER),e(ER,T4o),e(_b,F4o),e(X,E4o),e(X,bb),e(bb,nee),e(nee,C4o),e(bb,M4o),e(bb,CR),e(CR,y4o),e(bb,w4o),e(X,A4o),e(X,vb),e(vb,lee),e(lee,L4o),e(vb,B4o),e(vb,MR),e(MR,x4o),e(vb,k4o),e(X,R4o),e(X,Tb),e(Tb,iee),e(iee,P4o),e(Tb,S4o),e(Tb,yR),e(yR,$4o),e(Tb,I4o),e(X,D4o),e(X,Fb),e(Fb,dee),e(dee,N4o),e(Fb,j4o),e(Fb,wR),e(wR,O4o),e(Fb,G4o),e(X,q4o),e(X,Eb),e(Eb,cee),e(cee,z4o),e(Eb,X4o),e(Eb,AR),e(AR,Q4o),e(Eb,V4o),e(X,W4o),e(X,Cb),e(Cb,mee),e(mee,H4o),e(Cb,U4o),e(Cb,LR),e(LR,J4o),e(Cb,K4o),e(X,Y4o),e(X,Mb),e(Mb,fee),e(fee,Z4o),e(Mb,eCo),e(Mb,BR),e(BR,oCo),e(Mb,rCo),e(X,tCo),e(X,yb),e(yb,hee),e(hee,aCo),e(yb,sCo),e(yb,xR),e(xR,nCo),e(yb,lCo),e(X,iCo),e(X,wb),e(wb,gee),e(gee,dCo),e(wb,cCo),e(wb,kR),e(kR,mCo),e(wb,fCo),e(X,hCo),e(X,Ab),e(Ab,uee),e(uee,gCo),e(Ab,uCo),e(Ab,RR),e(RR,pCo),e(Ab,_Co),e(X,bCo),e(X,Lb),e(Lb,pee),e(pee,vCo),e(Lb,TCo),e(Lb,PR),e(PR,FCo),e(Lb,ECo),e(X,CCo),e(X,Bb),e(Bb,_ee),e(_ee,MCo),e(Bb,yCo),e(Bb,SR),e(SR,wCo),e(Bb,ACo),e(X,LCo),e(X,xb),e(xb,bee),e(bee,BCo),e(xb,xCo),e(xb,$R),e($R,kCo),e(xb,RCo),e(qe,PCo),e(qe,kb),e(kb,SCo),e(kb,vee),e(vee,$Co),e(kb,ICo),e(kb,Tee),e(Tee,DCo),e(qe,NCo),e(qe,Fee),e(Fee,jCo),e(qe,OCo),h(pM,qe,null),b(d,$5e,_),b(d,Ni,_),e(Ni,Rb),e(Rb,Eee),h(_M,Eee,null),e(Ni,GCo),e(Ni,Cee),e(Cee,qCo),b(d,I5e,_),b(d,Ho,_),h(bM,Ho,null),e(Ho,zCo),e(Ho,ji),e(ji,XCo),e(ji,Mee),e(Mee,QCo),e(ji,VCo),e(ji,yee),e(yee,WCo),e(ji,HCo),e(Ho,UCo),e(Ho,vM),e(vM,JCo),e(vM,wee),e(wee,KCo),e(vM,YCo),e(Ho,ZCo),e(Ho,Or),h(TM,Or,null),e(Or,e3o),e(Or,Aee),e(Aee,o3o),e(Or,r3o),e(Or,Oi),e(Oi,t3o),e(Oi,Lee),e(Lee,a3o),e(Oi,s3o),e(Oi,Bee),e(Bee,n3o),e(Oi,l3o),e(Or,i3o),e(Or,xee),e(xee,d3o),e(Or,c3o),h(FM,Or,null),e(Ho,m3o),e(Ho,ze),h(EM,ze,null),e(ze,f3o),e(ze,kee),e(kee,h3o),e(ze,g3o),e(ze,$a),e($a,u3o),e($a,Ree),e(Ree,p3o),e($a,_3o),e($a,Pee),e(Pee,b3o),e($a,v3o),e($a,See),e(See,T3o),e($a,F3o),e(ze,E3o),e(ze,S),e(S,Pb),e(Pb,$ee),e($ee,C3o),e(Pb,M3o),e(Pb,IR),e(IR,y3o),e(Pb,w3o),e(S,A3o),e(S,Sb),e(Sb,Iee),e(Iee,L3o),e(Sb,B3o),e(Sb,DR),e(DR,x3o),e(Sb,k3o),e(S,R3o),e(S,$b),e($b,Dee),e(Dee,P3o),e($b,S3o),e($b,NR),e(NR,$3o),e($b,I3o),e(S,D3o),e(S,Ib),e(Ib,Nee),e(Nee,N3o),e(Ib,j3o),e(Ib,jR),e(jR,O3o),e(Ib,G3o),e(S,q3o),e(S,Db),e(Db,jee),e(jee,z3o),e(Db,X3o),e(Db,OR),e(OR,Q3o),e(Db,V3o),e(S,W3o),e(S,Nb),e(Nb,Oee),e(Oee,H3o),e(Nb,U3o),e(Nb,GR),e(GR,J3o),e(Nb,K3o),e(S,Y3o),e(S,jb),e(jb,Gee),e(Gee,Z3o),e(jb,eMo),e(jb,qR),e(qR,oMo),e(jb,rMo),e(S,tMo),e(S,Ob),e(Ob,qee),e(qee,aMo),e(Ob,sMo),e(Ob,zR),e(zR,nMo),e(Ob,lMo),e(S,iMo),e(S,Gb),e(Gb,zee),e(zee,dMo),e(Gb,cMo),e(Gb,XR),e(XR,mMo),e(Gb,fMo),e(S,hMo),e(S,qb),e(qb,Xee),e(Xee,gMo),e(qb,uMo),e(qb,QR),e(QR,pMo),e(qb,_Mo),e(S,bMo),e(S,zb),e(zb,Qee),e(Qee,vMo),e(zb,TMo),e(zb,VR),e(VR,FMo),e(zb,EMo),e(S,CMo),e(S,Xb),e(Xb,Vee),e(Vee,MMo),e(Xb,yMo),e(Xb,WR),e(WR,wMo),e(Xb,AMo),e(S,LMo),e(S,Qb),e(Qb,Wee),e(Wee,BMo),e(Qb,xMo),e(Qb,HR),e(HR,kMo),e(Qb,RMo),e(S,PMo),e(S,Vb),e(Vb,Hee),e(Hee,SMo),e(Vb,$Mo),e(Vb,UR),e(UR,IMo),e(Vb,DMo),e(S,NMo),e(S,Wb),e(Wb,Uee),e(Uee,jMo),e(Wb,OMo),e(Wb,JR),e(JR,GMo),e(Wb,qMo),e(S,zMo),e(S,Hb),e(Hb,Jee),e(Jee,XMo),e(Hb,QMo),e(Hb,KR),e(KR,VMo),e(Hb,WMo),e(S,HMo),e(S,Ub),e(Ub,Kee),e(Kee,UMo),e(Ub,JMo),e(Ub,YR),e(YR,KMo),e(Ub,YMo),e(S,ZMo),e(S,Jb),e(Jb,Yee),e(Yee,e5o),e(Jb,o5o),e(Jb,ZR),e(ZR,r5o),e(Jb,t5o),e(S,a5o),e(S,Kb),e(Kb,Zee),e(Zee,s5o),e(Kb,n5o),e(Kb,eP),e(eP,l5o),e(Kb,i5o),e(S,d5o),e(S,Yb),e(Yb,eoe),e(eoe,c5o),e(Yb,m5o),e(Yb,oP),e(oP,f5o),e(Yb,h5o),e(S,g5o),e(S,Zb),e(Zb,ooe),e(ooe,u5o),e(Zb,p5o),e(Zb,rP),e(rP,_5o),e(Zb,b5o),e(S,v5o),e(S,e2),e(e2,roe),e(roe,T5o),e(e2,F5o),e(e2,tP),e(tP,E5o),e(e2,C5o),e(S,M5o),e(S,o2),e(o2,toe),e(toe,y5o),e(o2,w5o),e(o2,aP),e(aP,A5o),e(o2,L5o),e(S,B5o),e(S,r2),e(r2,aoe),e(aoe,x5o),e(r2,k5o),e(r2,sP),e(sP,R5o),e(r2,P5o),e(S,S5o),e(S,t2),e(t2,soe),e(soe,$5o),e(t2,I5o),e(t2,nP),e(nP,D5o),e(t2,N5o),e(S,j5o),e(S,a2),e(a2,noe),e(noe,O5o),e(a2,G5o),e(a2,lP),e(lP,q5o),e(a2,z5o),e(S,X5o),e(S,s2),e(s2,loe),e(loe,Q5o),e(s2,V5o),e(s2,iP),e(iP,W5o),e(s2,H5o),e(S,U5o),e(S,n2),e(n2,ioe),e(ioe,J5o),e(n2,K5o),e(n2,dP),e(dP,Y5o),e(n2,Z5o),e(S,eyo),e(S,l2),e(l2,doe),e(doe,oyo),e(l2,ryo),e(l2,cP),e(cP,tyo),e(l2,ayo),e(S,syo),e(S,i2),e(i2,coe),e(coe,nyo),e(i2,lyo),e(i2,mP),e(mP,iyo),e(i2,dyo),e(S,cyo),e(S,d2),e(d2,moe),e(moe,myo),e(d2,fyo),e(d2,fP),e(fP,hyo),e(d2,gyo),e(S,uyo),e(S,c2),e(c2,foe),e(foe,pyo),e(c2,_yo),e(c2,hP),e(hP,byo),e(c2,vyo),e(S,Tyo),e(S,m2),e(m2,hoe),e(hoe,Fyo),e(m2,Eyo),e(m2,gP),e(gP,Cyo),e(m2,Myo),e(S,yyo),e(S,f2),e(f2,goe),e(goe,wyo),e(f2,Ayo),e(f2,uP),e(uP,Lyo),e(f2,Byo),e(S,xyo),e(S,h2),e(h2,uoe),e(uoe,kyo),e(h2,Ryo),e(h2,pP),e(pP,Pyo),e(h2,Syo),e(ze,$yo),e(ze,g2),e(g2,Iyo),e(g2,poe),e(poe,Dyo),e(g2,Nyo),e(g2,_oe),e(_oe,jyo),e(ze,Oyo),e(ze,boe),e(boe,Gyo),e(ze,qyo),h(CM,ze,null),b(d,D5e,_),b(d,Gi,_),e(Gi,u2),e(u2,voe),h(MM,voe,null),e(Gi,zyo),e(Gi,Toe),e(Toe,Xyo),b(d,N5e,_),b(d,Uo,_),h(yM,Uo,null),e(Uo,Qyo),e(Uo,qi),e(qi,Vyo),e(qi,Foe),e(Foe,Wyo),e(qi,Hyo),e(qi,Eoe),e(Eoe,Uyo),e(qi,Jyo),e(Uo,Kyo),e(Uo,wM),e(wM,Yyo),e(wM,Coe),e(Coe,Zyo),e(wM,ewo),e(Uo,owo),e(Uo,Gr),h(AM,Gr,null),e(Gr,rwo),e(Gr,Moe),e(Moe,two),e(Gr,awo),e(Gr,zi),e(zi,swo),e(zi,yoe),e(yoe,nwo),e(zi,lwo),e(zi,woe),e(woe,iwo),e(zi,dwo),e(Gr,cwo),e(Gr,Aoe),e(Aoe,mwo),e(Gr,fwo),h(LM,Gr,null),e(Uo,hwo),e(Uo,Xe),h(BM,Xe,null),e(Xe,gwo),e(Xe,Loe),e(Loe,uwo),e(Xe,pwo),e(Xe,Ia),e(Ia,_wo),e(Ia,Boe),e(Boe,bwo),e(Ia,vwo),e(Ia,xoe),e(xoe,Two),e(Ia,Fwo),e(Ia,koe),e(koe,Ewo),e(Ia,Cwo),e(Xe,Mwo),e(Xe,Roe),e(Roe,p2),e(p2,Poe),e(Poe,ywo),e(p2,wwo),e(p2,_P),e(_P,Awo),e(p2,Lwo),e(Xe,Bwo),e(Xe,_2),e(_2,xwo),e(_2,Soe),e(Soe,kwo),e(_2,Rwo),e(_2,$oe),e($oe,Pwo),e(Xe,Swo),e(Xe,Ioe),e(Ioe,$wo),e(Xe,Iwo),h(xM,Xe,null),b(d,j5e,_),b(d,Xi,_),e(Xi,b2),e(b2,Doe),h(kM,Doe,null),e(Xi,Dwo),e(Xi,Noe),e(Noe,Nwo),b(d,O5e,_),b(d,Jo,_),h(RM,Jo,null),e(Jo,jwo),e(Jo,Qi),e(Qi,Owo),e(Qi,joe),e(joe,Gwo),e(Qi,qwo),e(Qi,Ooe),e(Ooe,zwo),e(Qi,Xwo),e(Jo,Qwo),e(Jo,PM),e(PM,Vwo),e(PM,Goe),e(Goe,Wwo),e(PM,Hwo),e(Jo,Uwo),e(Jo,qr),h(SM,qr,null),e(qr,Jwo),e(qr,qoe),e(qoe,Kwo),e(qr,Ywo),e(qr,Vi),e(Vi,Zwo),e(Vi,zoe),e(zoe,e7o),e(Vi,o7o),e(Vi,Xoe),e(Xoe,r7o),e(Vi,t7o),e(qr,a7o),e(qr,Qoe),e(Qoe,s7o),e(qr,n7o),h($M,qr,null),e(Jo,l7o),e(Jo,Qe),h(IM,Qe,null),e(Qe,i7o),e(Qe,Voe),e(Voe,d7o),e(Qe,c7o),e(Qe,Da),e(Da,m7o),e(Da,Woe),e(Woe,f7o),e(Da,h7o),e(Da,Hoe),e(Hoe,g7o),e(Da,u7o),e(Da,Uoe),e(Uoe,p7o),e(Da,_7o),e(Qe,b7o),e(Qe,Ko),e(Ko,v2),e(v2,Joe),e(Joe,v7o),e(v2,T7o),e(v2,bP),e(bP,F7o),e(v2,E7o),e(Ko,C7o),e(Ko,dn),e(dn,Koe),e(Koe,M7o),e(dn,y7o),e(dn,vP),e(vP,w7o),e(dn,A7o),e(dn,TP),e(TP,L7o),e(dn,B7o),e(Ko,x7o),e(Ko,T2),e(T2,Yoe),e(Yoe,k7o),e(T2,R7o),e(T2,FP),e(FP,P7o),e(T2,S7o),e(Ko,$7o),e(Ko,Gt),e(Gt,Zoe),e(Zoe,I7o),e(Gt,D7o),e(Gt,EP),e(EP,N7o),e(Gt,j7o),e(Gt,CP),e(CP,O7o),e(Gt,G7o),e(Gt,MP),e(MP,q7o),e(Gt,z7o),e(Ko,X7o),e(Ko,F2),e(F2,ere),e(ere,Q7o),e(F2,V7o),e(F2,yP),e(yP,W7o),e(F2,H7o),e(Ko,U7o),e(Ko,E2),e(E2,ore),e(ore,J7o),e(E2,K7o),e(E2,wP),e(wP,Y7o),e(E2,Z7o),e(Qe,e0o),e(Qe,C2),e(C2,o0o),e(C2,rre),e(rre,r0o),e(C2,t0o),e(C2,tre),e(tre,a0o),e(Qe,s0o),e(Qe,are),e(are,n0o),e(Qe,l0o),h(DM,Qe,null),b(d,G5e,_),b(d,Wi,_),e(Wi,M2),e(M2,sre),h(NM,sre,null),e(Wi,i0o),e(Wi,nre),e(nre,d0o),b(d,q5e,_),b(d,Yo,_),h(jM,Yo,null),e(Yo,c0o),e(Yo,Hi),e(Hi,m0o),e(Hi,lre),e(lre,f0o),e(Hi,h0o),e(Hi,ire),e(ire,g0o),e(Hi,u0o),e(Yo,p0o),e(Yo,OM),e(OM,_0o),e(OM,dre),e(dre,b0o),e(OM,v0o),e(Yo,T0o),e(Yo,zr),h(GM,zr,null),e(zr,F0o),e(zr,cre),e(cre,E0o),e(zr,C0o),e(zr,Ui),e(Ui,M0o),e(Ui,mre),e(mre,y0o),e(Ui,w0o),e(Ui,fre),e(fre,A0o),e(Ui,L0o),e(zr,B0o),e(zr,hre),e(hre,x0o),e(zr,k0o),h(qM,zr,null),e(Yo,R0o),e(Yo,Ve),h(zM,Ve,null),e(Ve,P0o),e(Ve,gre),e(gre,S0o),e(Ve,$0o),e(Ve,Na),e(Na,I0o),e(Na,ure),e(ure,D0o),e(Na,N0o),e(Na,pre),e(pre,j0o),e(Na,O0o),e(Na,_re),e(_re,G0o),e(Na,q0o),e(Ve,z0o),e(Ve,bre),e(bre,y2),e(y2,vre),e(vre,X0o),e(y2,Q0o),e(y2,AP),e(AP,V0o),e(y2,W0o),e(Ve,H0o),e(Ve,w2),e(w2,U0o),e(w2,Tre),e(Tre,J0o),e(w2,K0o),e(w2,Fre),e(Fre,Y0o),e(Ve,Z0o),e(Ve,Ere),e(Ere,eAo),e(Ve,oAo),h(XM,Ve,null),b(d,z5e,_),b(d,Ji,_),e(Ji,A2),e(A2,Cre),h(QM,Cre,null),e(Ji,rAo),e(Ji,Mre),e(Mre,tAo),b(d,X5e,_),b(d,Zo,_),h(VM,Zo,null),e(Zo,aAo),e(Zo,Ki),e(Ki,sAo),e(Ki,yre),e(yre,nAo),e(Ki,lAo),e(Ki,wre),e(wre,iAo),e(Ki,dAo),e(Zo,cAo),e(Zo,WM),e(WM,mAo),e(WM,Are),e(Are,fAo),e(WM,hAo),e(Zo,gAo),e(Zo,Xr),h(HM,Xr,null),e(Xr,uAo),e(Xr,Lre),e(Lre,pAo),e(Xr,_Ao),e(Xr,Yi),e(Yi,bAo),e(Yi,Bre),e(Bre,vAo),e(Yi,TAo),e(Yi,xre),e(xre,FAo),e(Yi,EAo),e(Xr,CAo),e(Xr,kre),e(kre,MAo),e(Xr,yAo),h(UM,Xr,null),e(Zo,wAo),e(Zo,We),h(JM,We,null),e(We,AAo),e(We,Rre),e(Rre,LAo),e(We,BAo),e(We,ja),e(ja,xAo),e(ja,Pre),e(Pre,kAo),e(ja,RAo),e(ja,Sre),e(Sre,PAo),e(ja,SAo),e(ja,$re),e($re,$Ao),e(ja,IAo),e(We,DAo),e(We,er),e(er,L2),e(L2,Ire),e(Ire,NAo),e(L2,jAo),e(L2,LP),e(LP,OAo),e(L2,GAo),e(er,qAo),e(er,B2),e(B2,Dre),e(Dre,zAo),e(B2,XAo),e(B2,BP),e(BP,QAo),e(B2,VAo),e(er,WAo),e(er,x2),e(x2,Nre),e(Nre,HAo),e(x2,UAo),e(x2,xP),e(xP,JAo),e(x2,KAo),e(er,YAo),e(er,k2),e(k2,jre),e(jre,ZAo),e(k2,e6o),e(k2,kP),e(kP,o6o),e(k2,r6o),e(er,t6o),e(er,R2),e(R2,Ore),e(Ore,a6o),e(R2,s6o),e(R2,RP),e(RP,n6o),e(R2,l6o),e(er,i6o),e(er,P2),e(P2,Gre),e(Gre,d6o),e(P2,c6o),e(P2,PP),e(PP,m6o),e(P2,f6o),e(We,h6o),e(We,S2),e(S2,g6o),e(S2,qre),e(qre,u6o),e(S2,p6o),e(S2,zre),e(zre,_6o),e(We,b6o),e(We,Xre),e(Xre,v6o),e(We,T6o),h(KM,We,null),b(d,Q5e,_),b(d,Zi,_),e(Zi,$2),e($2,Qre),h(YM,Qre,null),e(Zi,F6o),e(Zi,Vre),e(Vre,E6o),b(d,V5e,_),b(d,or,_),h(ZM,or,null),e(or,C6o),e(or,ed),e(ed,M6o),e(ed,Wre),e(Wre,y6o),e(ed,w6o),e(ed,Hre),e(Hre,A6o),e(ed,L6o),e(or,B6o),e(or,e5),e(e5,x6o),e(e5,Ure),e(Ure,k6o),e(e5,R6o),e(or,P6o),e(or,Qr),h(o5,Qr,null),e(Qr,S6o),e(Qr,Jre),e(Jre,$6o),e(Qr,I6o),e(Qr,od),e(od,D6o),e(od,Kre),e(Kre,N6o),e(od,j6o),e(od,Yre),e(Yre,O6o),e(od,G6o),e(Qr,q6o),e(Qr,Zre),e(Zre,z6o),e(Qr,X6o),h(r5,Qr,null),e(or,Q6o),e(or,He),h(t5,He,null),e(He,V6o),e(He,ete),e(ete,W6o),e(He,H6o),e(He,Oa),e(Oa,U6o),e(Oa,ote),e(ote,J6o),e(Oa,K6o),e(Oa,rte),e(rte,Y6o),e(Oa,Z6o),e(Oa,tte),e(tte,eLo),e(Oa,oLo),e(He,rLo),e(He,rr),e(rr,I2),e(I2,ate),e(ate,tLo),e(I2,aLo),e(I2,SP),e(SP,sLo),e(I2,nLo),e(rr,lLo),e(rr,D2),e(D2,ste),e(ste,iLo),e(D2,dLo),e(D2,$P),e($P,cLo),e(D2,mLo),e(rr,fLo),e(rr,N2),e(N2,nte),e(nte,hLo),e(N2,gLo),e(N2,IP),e(IP,uLo),e(N2,pLo),e(rr,_Lo),e(rr,j2),e(j2,lte),e(lte,bLo),e(j2,vLo),e(j2,DP),e(DP,TLo),e(j2,FLo),e(rr,ELo),e(rr,O2),e(O2,ite),e(ite,CLo),e(O2,MLo),e(O2,NP),e(NP,yLo),e(O2,wLo),e(rr,ALo),e(rr,G2),e(G2,dte),e(dte,LLo),e(G2,BLo),e(G2,jP),e(jP,xLo),e(G2,kLo),e(He,RLo),e(He,q2),e(q2,PLo),e(q2,cte),e(cte,SLo),e(q2,$Lo),e(q2,mte),e(mte,ILo),e(He,DLo),e(He,fte),e(fte,NLo),e(He,jLo),h(a5,He,null),b(d,W5e,_),b(d,rd,_),e(rd,z2),e(z2,hte),h(s5,hte,null),e(rd,OLo),e(rd,gte),e(gte,GLo),b(d,H5e,_),b(d,tr,_),h(n5,tr,null),e(tr,qLo),e(tr,td),e(td,zLo),e(td,ute),e(ute,XLo),e(td,QLo),e(td,pte),e(pte,VLo),e(td,WLo),e(tr,HLo),e(tr,l5),e(l5,ULo),e(l5,_te),e(_te,JLo),e(l5,KLo),e(tr,YLo),e(tr,Vr),h(i5,Vr,null),e(Vr,ZLo),e(Vr,bte),e(bte,e8o),e(Vr,o8o),e(Vr,ad),e(ad,r8o),e(ad,vte),e(vte,t8o),e(ad,a8o),e(ad,Tte),e(Tte,s8o),e(ad,n8o),e(Vr,l8o),e(Vr,Fte),e(Fte,i8o),e(Vr,d8o),h(d5,Vr,null),e(tr,c8o),e(tr,Ue),h(c5,Ue,null),e(Ue,m8o),e(Ue,Ete),e(Ete,f8o),e(Ue,h8o),e(Ue,Ga),e(Ga,g8o),e(Ga,Cte),e(Cte,u8o),e(Ga,p8o),e(Ga,Mte),e(Mte,_8o),e(Ga,b8o),e(Ga,yte),e(yte,v8o),e(Ga,T8o),e(Ue,F8o),e(Ue,m5),e(m5,X2),e(X2,wte),e(wte,E8o),e(X2,C8o),e(X2,OP),e(OP,M8o),e(X2,y8o),e(m5,w8o),e(m5,Q2),e(Q2,Ate),e(Ate,A8o),e(Q2,L8o),e(Q2,GP),e(GP,B8o),e(Q2,x8o),e(Ue,k8o),e(Ue,V2),e(V2,R8o),e(V2,Lte),e(Lte,P8o),e(V2,S8o),e(V2,Bte),e(Bte,$8o),e(Ue,I8o),e(Ue,xte),e(xte,D8o),e(Ue,N8o),h(f5,Ue,null),b(d,U5e,_),b(d,sd,_),e(sd,W2),e(W2,kte),h(h5,kte,null),e(sd,j8o),e(sd,Rte),e(Rte,O8o),b(d,J5e,_),b(d,ar,_),h(g5,ar,null),e(ar,G8o),e(ar,nd),e(nd,q8o),e(nd,Pte),e(Pte,z8o),e(nd,X8o),e(nd,Ste),e(Ste,Q8o),e(nd,V8o),e(ar,W8o),e(ar,u5),e(u5,H8o),e(u5,$te),e($te,U8o),e(u5,J8o),e(ar,K8o),e(ar,Wr),h(p5,Wr,null),e(Wr,Y8o),e(Wr,Ite),e(Ite,Z8o),e(Wr,eBo),e(Wr,ld),e(ld,oBo),e(ld,Dte),e(Dte,rBo),e(ld,tBo),e(ld,Nte),e(Nte,aBo),e(ld,sBo),e(Wr,nBo),e(Wr,jte),e(jte,lBo),e(Wr,iBo),h(_5,Wr,null),e(ar,dBo),e(ar,Je),h(b5,Je,null),e(Je,cBo),e(Je,Ote),e(Ote,mBo),e(Je,fBo),e(Je,qa),e(qa,hBo),e(qa,Gte),e(Gte,gBo),e(qa,uBo),e(qa,qte),e(qte,pBo),e(qa,_Bo),e(qa,zte),e(zte,bBo),e(qa,vBo),e(Je,TBo),e(Je,Xte),e(Xte,H2),e(H2,Qte),e(Qte,FBo),e(H2,EBo),e(H2,qP),e(qP,CBo),e(H2,MBo),e(Je,yBo),e(Je,U2),e(U2,wBo),e(U2,Vte),e(Vte,ABo),e(U2,LBo),e(U2,Wte),e(Wte,BBo),e(Je,xBo),e(Je,Hte),e(Hte,kBo),e(Je,RBo),h(v5,Je,null),b(d,K5e,_),b(d,id,_),e(id,J2),e(J2,Ute),h(T5,Ute,null),e(id,PBo),e(id,Jte),e(Jte,SBo),b(d,Y5e,_),b(d,sr,_),h(F5,sr,null),e(sr,$Bo),e(sr,dd),e(dd,IBo),e(dd,Kte),e(Kte,DBo),e(dd,NBo),e(dd,Yte),e(Yte,jBo),e(dd,OBo),e(sr,GBo),e(sr,E5),e(E5,qBo),e(E5,Zte),e(Zte,zBo),e(E5,XBo),e(sr,QBo),e(sr,Hr),h(C5,Hr,null),e(Hr,VBo),e(Hr,eae),e(eae,WBo),e(Hr,HBo),e(Hr,cd),e(cd,UBo),e(cd,oae),e(oae,JBo),e(cd,KBo),e(cd,rae),e(rae,YBo),e(cd,ZBo),e(Hr,e9o),e(Hr,tae),e(tae,o9o),e(Hr,r9o),h(M5,Hr,null),e(sr,t9o),e(sr,Ke),h(y5,Ke,null),e(Ke,a9o),e(Ke,aae),e(aae,s9o),e(Ke,n9o),e(Ke,za),e(za,l9o),e(za,sae),e(sae,i9o),e(za,d9o),e(za,nae),e(nae,c9o),e(za,m9o),e(za,lae),e(lae,f9o),e(za,h9o),e(Ke,g9o),e(Ke,iae),e(iae,K2),e(K2,dae),e(dae,u9o),e(K2,p9o),e(K2,zP),e(zP,_9o),e(K2,b9o),e(Ke,v9o),e(Ke,Y2),e(Y2,T9o),e(Y2,cae),e(cae,F9o),e(Y2,E9o),e(Y2,mae),e(mae,C9o),e(Ke,M9o),e(Ke,fae),e(fae,y9o),e(Ke,w9o),h(w5,Ke,null),b(d,Z5e,_),b(d,md,_),e(md,Z2),e(Z2,hae),h(A5,hae,null),e(md,A9o),e(md,gae),e(gae,L9o),b(d,eye,_),b(d,nr,_),h(L5,nr,null),e(nr,B9o),e(nr,fd),e(fd,x9o),e(fd,uae),e(uae,k9o),e(fd,R9o),e(fd,pae),e(pae,P9o),e(fd,S9o),e(nr,$9o),e(nr,B5),e(B5,I9o),e(B5,_ae),e(_ae,D9o),e(B5,N9o),e(nr,j9o),e(nr,Ur),h(x5,Ur,null),e(Ur,O9o),e(Ur,bae),e(bae,G9o),e(Ur,q9o),e(Ur,hd),e(hd,z9o),e(hd,vae),e(vae,X9o),e(hd,Q9o),e(hd,Tae),e(Tae,V9o),e(hd,W9o),e(Ur,H9o),e(Ur,Fae),e(Fae,U9o),e(Ur,J9o),h(k5,Ur,null),e(nr,K9o),e(nr,lo),h(R5,lo,null),e(lo,Y9o),e(lo,Eae),e(Eae,Z9o),e(lo,exo),e(lo,Xa),e(Xa,oxo),e(Xa,Cae),e(Cae,rxo),e(Xa,txo),e(Xa,Mae),e(Mae,axo),e(Xa,sxo),e(Xa,yae),e(yae,nxo),e(Xa,lxo),e(lo,ixo),e(lo,B),e(B,ev),e(ev,wae),e(wae,dxo),e(ev,cxo),e(ev,XP),e(XP,mxo),e(ev,fxo),e(B,hxo),e(B,ov),e(ov,Aae),e(Aae,gxo),e(ov,uxo),e(ov,QP),e(QP,pxo),e(ov,_xo),e(B,bxo),e(B,rv),e(rv,Lae),e(Lae,vxo),e(rv,Txo),e(rv,VP),e(VP,Fxo),e(rv,Exo),e(B,Cxo),e(B,tv),e(tv,Bae),e(Bae,Mxo),e(tv,yxo),e(tv,WP),e(WP,wxo),e(tv,Axo),e(B,Lxo),e(B,av),e(av,xae),e(xae,Bxo),e(av,xxo),e(av,HP),e(HP,kxo),e(av,Rxo),e(B,Pxo),e(B,sv),e(sv,kae),e(kae,Sxo),e(sv,$xo),e(sv,UP),e(UP,Ixo),e(sv,Dxo),e(B,Nxo),e(B,nv),e(nv,Rae),e(Rae,jxo),e(nv,Oxo),e(nv,JP),e(JP,Gxo),e(nv,qxo),e(B,zxo),e(B,lv),e(lv,Pae),e(Pae,Xxo),e(lv,Qxo),e(lv,KP),e(KP,Vxo),e(lv,Wxo),e(B,Hxo),e(B,iv),e(iv,Sae),e(Sae,Uxo),e(iv,Jxo),e(iv,YP),e(YP,Kxo),e(iv,Yxo),e(B,Zxo),e(B,dv),e(dv,$ae),e($ae,eko),e(dv,oko),e(dv,ZP),e(ZP,rko),e(dv,tko),e(B,ako),e(B,cv),e(cv,Iae),e(Iae,sko),e(cv,nko),e(cv,eS),e(eS,lko),e(cv,iko),e(B,dko),e(B,mv),e(mv,Dae),e(Dae,cko),e(mv,mko),e(mv,oS),e(oS,fko),e(mv,hko),e(B,gko),e(B,fv),e(fv,Nae),e(Nae,uko),e(fv,pko),e(fv,rS),e(rS,_ko),e(fv,bko),e(B,vko),e(B,hv),e(hv,jae),e(jae,Tko),e(hv,Fko),e(hv,tS),e(tS,Eko),e(hv,Cko),e(B,Mko),e(B,cn),e(cn,Oae),e(Oae,yko),e(cn,wko),e(cn,aS),e(aS,Ako),e(cn,Lko),e(cn,sS),e(sS,Bko),e(cn,xko),e(B,kko),e(B,gv),e(gv,Gae),e(Gae,Rko),e(gv,Pko),e(gv,nS),e(nS,Sko),e(gv,$ko),e(B,Iko),e(B,uv),e(uv,qae),e(qae,Dko),e(uv,Nko),e(uv,lS),e(lS,jko),e(uv,Oko),e(B,Gko),e(B,pv),e(pv,zae),e(zae,qko),e(pv,zko),e(pv,iS),e(iS,Xko),e(pv,Qko),e(B,Vko),e(B,_v),e(_v,Xae),e(Xae,Wko),e(_v,Hko),e(_v,dS),e(dS,Uko),e(_v,Jko),e(B,Kko),e(B,bv),e(bv,Qae),e(Qae,Yko),e(bv,Zko),e(bv,cS),e(cS,eRo),e(bv,oRo),e(B,rRo),e(B,vv),e(vv,Vae),e(Vae,tRo),e(vv,aRo),e(vv,mS),e(mS,sRo),e(vv,nRo),e(B,lRo),e(B,Tv),e(Tv,Wae),e(Wae,iRo),e(Tv,dRo),e(Tv,fS),e(fS,cRo),e(Tv,mRo),e(B,fRo),e(B,Fv),e(Fv,Hae),e(Hae,hRo),e(Fv,gRo),e(Fv,hS),e(hS,uRo),e(Fv,pRo),e(B,_Ro),e(B,Ev),e(Ev,Uae),e(Uae,bRo),e(Ev,vRo),e(Ev,gS),e(gS,TRo),e(Ev,FRo),e(B,ERo),e(B,Cv),e(Cv,Jae),e(Jae,CRo),e(Cv,MRo),e(Cv,uS),e(uS,yRo),e(Cv,wRo),e(B,ARo),e(B,Mv),e(Mv,Kae),e(Kae,LRo),e(Mv,BRo),e(Mv,pS),e(pS,xRo),e(Mv,kRo),e(B,RRo),e(B,yv),e(yv,Yae),e(Yae,PRo),e(yv,SRo),e(yv,_S),e(_S,$Ro),e(yv,IRo),e(B,DRo),e(B,wv),e(wv,Zae),e(Zae,NRo),e(wv,jRo),e(wv,bS),e(bS,ORo),e(wv,GRo),e(B,qRo),e(B,Av),e(Av,ese),e(ese,zRo),e(Av,XRo),e(Av,vS),e(vS,QRo),e(Av,VRo),e(B,WRo),e(B,Lv),e(Lv,ose),e(ose,HRo),e(Lv,URo),e(Lv,TS),e(TS,JRo),e(Lv,KRo),e(B,YRo),e(B,Bv),e(Bv,rse),e(rse,ZRo),e(Bv,ePo),e(Bv,FS),e(FS,oPo),e(Bv,rPo),e(B,tPo),e(B,xv),e(xv,tse),e(tse,aPo),e(xv,sPo),e(xv,ES),e(ES,nPo),e(xv,lPo),e(B,iPo),e(B,kv),e(kv,ase),e(ase,dPo),e(kv,cPo),e(kv,CS),e(CS,mPo),e(kv,fPo),e(B,hPo),e(B,Rv),e(Rv,sse),e(sse,gPo),e(Rv,uPo),e(Rv,MS),e(MS,pPo),e(Rv,_Po),e(B,bPo),e(B,Pv),e(Pv,nse),e(nse,vPo),e(Pv,TPo),e(Pv,yS),e(yS,FPo),e(Pv,EPo),e(B,CPo),e(B,Sv),e(Sv,lse),e(lse,MPo),e(Sv,yPo),e(Sv,wS),e(wS,wPo),e(Sv,APo),e(B,LPo),e(B,$v),e($v,ise),e(ise,BPo),e($v,xPo),e($v,AS),e(AS,kPo),e($v,RPo),e(B,PPo),e(B,Iv),e(Iv,dse),e(dse,SPo),e(Iv,$Po),e(Iv,LS),e(LS,IPo),e(Iv,DPo),e(B,NPo),e(B,Dv),e(Dv,cse),e(cse,jPo),e(Dv,OPo),e(Dv,BS),e(BS,GPo),e(Dv,qPo),e(lo,zPo),e(lo,mse),e(mse,XPo),e(lo,QPo),h(P5,lo,null),b(d,oye,_),b(d,gd,_),e(gd,Nv),e(Nv,fse),h(S5,fse,null),e(gd,VPo),e(gd,hse),e(hse,WPo),b(d,rye,_),b(d,lr,_),h($5,lr,null),e(lr,HPo),e(lr,ud),e(ud,UPo),e(ud,gse),e(gse,JPo),e(ud,KPo),e(ud,use),e(use,YPo),e(ud,ZPo),e(lr,eSo),e(lr,I5),e(I5,oSo),e(I5,pse),e(pse,rSo),e(I5,tSo),e(lr,aSo),e(lr,Jr),h(D5,Jr,null),e(Jr,sSo),e(Jr,_se),e(_se,nSo),e(Jr,lSo),e(Jr,pd),e(pd,iSo),e(pd,bse),e(bse,dSo),e(pd,cSo),e(pd,vse),e(vse,mSo),e(pd,fSo),e(Jr,hSo),e(Jr,Tse),e(Tse,gSo),e(Jr,uSo),h(N5,Jr,null),e(lr,pSo),e(lr,io),h(j5,io,null),e(io,_So),e(io,Fse),e(Fse,bSo),e(io,vSo),e(io,Qa),e(Qa,TSo),e(Qa,Ese),e(Ese,FSo),e(Qa,ESo),e(Qa,Cse),e(Cse,CSo),e(Qa,MSo),e(Qa,Mse),e(Mse,ySo),e(Qa,wSo),e(io,ASo),e(io,K),e(K,jv),e(jv,yse),e(yse,LSo),e(jv,BSo),e(jv,xS),e(xS,xSo),e(jv,kSo),e(K,RSo),e(K,Ov),e(Ov,wse),e(wse,PSo),e(Ov,SSo),e(Ov,kS),e(kS,$So),e(Ov,ISo),e(K,DSo),e(K,Gv),e(Gv,Ase),e(Ase,NSo),e(Gv,jSo),e(Gv,RS),e(RS,OSo),e(Gv,GSo),e(K,qSo),e(K,qv),e(qv,Lse),e(Lse,zSo),e(qv,XSo),e(qv,PS),e(PS,QSo),e(qv,VSo),e(K,WSo),e(K,zv),e(zv,Bse),e(Bse,HSo),e(zv,USo),e(zv,SS),e(SS,JSo),e(zv,KSo),e(K,YSo),e(K,Xv),e(Xv,xse),e(xse,ZSo),e(Xv,e$o),e(Xv,$S),e($S,o$o),e(Xv,r$o),e(K,t$o),e(K,Qv),e(Qv,kse),e(kse,a$o),e(Qv,s$o),e(Qv,IS),e(IS,n$o),e(Qv,l$o),e(K,i$o),e(K,Vv),e(Vv,Rse),e(Rse,d$o),e(Vv,c$o),e(Vv,DS),e(DS,m$o),e(Vv,f$o),e(K,h$o),e(K,Wv),e(Wv,Pse),e(Pse,g$o),e(Wv,u$o),e(Wv,NS),e(NS,p$o),e(Wv,_$o),e(K,b$o),e(K,Hv),e(Hv,Sse),e(Sse,v$o),e(Hv,T$o),e(Hv,jS),e(jS,F$o),e(Hv,E$o),e(K,C$o),e(K,Uv),e(Uv,$se),e($se,M$o),e(Uv,y$o),e(Uv,OS),e(OS,w$o),e(Uv,A$o),e(K,L$o),e(K,Jv),e(Jv,Ise),e(Ise,B$o),e(Jv,x$o),e(Jv,GS),e(GS,k$o),e(Jv,R$o),e(K,P$o),e(K,Kv),e(Kv,Dse),e(Dse,S$o),e(Kv,$$o),e(Kv,qS),e(qS,I$o),e(Kv,D$o),e(K,N$o),e(K,Yv),e(Yv,Nse),e(Nse,j$o),e(Yv,O$o),e(Yv,zS),e(zS,G$o),e(Yv,q$o),e(K,z$o),e(K,Zv),e(Zv,jse),e(jse,X$o),e(Zv,Q$o),e(Zv,XS),e(XS,V$o),e(Zv,W$o),e(K,H$o),e(K,eT),e(eT,Ose),e(Ose,U$o),e(eT,J$o),e(eT,QS),e(QS,K$o),e(eT,Y$o),e(K,Z$o),e(K,oT),e(oT,Gse),e(Gse,eIo),e(oT,oIo),e(oT,VS),e(VS,rIo),e(oT,tIo),e(K,aIo),e(K,rT),e(rT,qse),e(qse,sIo),e(rT,nIo),e(rT,WS),e(WS,lIo),e(rT,iIo),e(K,dIo),e(K,tT),e(tT,zse),e(zse,cIo),e(tT,mIo),e(tT,HS),e(HS,fIo),e(tT,hIo),e(K,gIo),e(K,aT),e(aT,Xse),e(Xse,uIo),e(aT,pIo),e(aT,US),e(US,_Io),e(aT,bIo),e(K,vIo),e(K,sT),e(sT,Qse),e(Qse,TIo),e(sT,FIo),e(sT,JS),e(JS,EIo),e(sT,CIo),e(K,MIo),e(K,nT),e(nT,Vse),e(Vse,yIo),e(nT,wIo),e(nT,KS),e(KS,AIo),e(nT,LIo),e(io,BIo),e(io,Wse),e(Wse,xIo),e(io,kIo),h(O5,io,null),b(d,tye,_),b(d,_d,_),e(_d,lT),e(lT,Hse),h(G5,Hse,null),e(_d,RIo),e(_d,Use),e(Use,PIo),b(d,aye,_),b(d,ir,_),h(q5,ir,null),e(ir,SIo),e(ir,bd),e(bd,$Io),e(bd,Jse),e(Jse,IIo),e(bd,DIo),e(bd,Kse),e(Kse,NIo),e(bd,jIo),e(ir,OIo),e(ir,z5),e(z5,GIo),e(z5,Yse),e(Yse,qIo),e(z5,zIo),e(ir,XIo),e(ir,Kr),h(X5,Kr,null),e(Kr,QIo),e(Kr,Zse),e(Zse,VIo),e(Kr,WIo),e(Kr,vd),e(vd,HIo),e(vd,ene),e(ene,UIo),e(vd,JIo),e(vd,one),e(one,KIo),e(vd,YIo),e(Kr,ZIo),e(Kr,rne),e(rne,eDo),e(Kr,oDo),h(Q5,Kr,null),e(ir,rDo),e(ir,co),h(V5,co,null),e(co,tDo),e(co,tne),e(tne,aDo),e(co,sDo),e(co,Va),e(Va,nDo),e(Va,ane),e(ane,lDo),e(Va,iDo),e(Va,sne),e(sne,dDo),e(Va,cDo),e(Va,nne),e(nne,mDo),e(Va,fDo),e(co,hDo),e(co,_e),e(_e,iT),e(iT,lne),e(lne,gDo),e(iT,uDo),e(iT,YS),e(YS,pDo),e(iT,_Do),e(_e,bDo),e(_e,dT),e(dT,ine),e(ine,vDo),e(dT,TDo),e(dT,ZS),e(ZS,FDo),e(dT,EDo),e(_e,CDo),e(_e,cT),e(cT,dne),e(dne,MDo),e(cT,yDo),e(cT,e$),e(e$,wDo),e(cT,ADo),e(_e,LDo),e(_e,mT),e(mT,cne),e(cne,BDo),e(mT,xDo),e(mT,o$),e(o$,kDo),e(mT,RDo),e(_e,PDo),e(_e,fT),e(fT,mne),e(mne,SDo),e(fT,$Do),e(fT,r$),e(r$,IDo),e(fT,DDo),e(_e,NDo),e(_e,hT),e(hT,fne),e(fne,jDo),e(hT,ODo),e(hT,t$),e(t$,GDo),e(hT,qDo),e(_e,zDo),e(_e,gT),e(gT,hne),e(hne,XDo),e(gT,QDo),e(gT,a$),e(a$,VDo),e(gT,WDo),e(_e,HDo),e(_e,uT),e(uT,gne),e(gne,UDo),e(uT,JDo),e(uT,s$),e(s$,KDo),e(uT,YDo),e(_e,ZDo),e(_e,pT),e(pT,une),e(une,eNo),e(pT,oNo),e(pT,n$),e(n$,rNo),e(pT,tNo),e(_e,aNo),e(_e,_T),e(_T,pne),e(pne,sNo),e(_T,nNo),e(_T,l$),e(l$,lNo),e(_T,iNo),e(co,dNo),e(co,_ne),e(_ne,cNo),e(co,mNo),h(W5,co,null),b(d,sye,_),b(d,Td,_),e(Td,bT),e(bT,bne),h(H5,bne,null),e(Td,fNo),e(Td,vne),e(vne,hNo),b(d,nye,_),b(d,dr,_),h(U5,dr,null),e(dr,gNo),e(dr,Fd),e(Fd,uNo),e(Fd,Tne),e(Tne,pNo),e(Fd,_No),e(Fd,Fne),e(Fne,bNo),e(Fd,vNo),e(dr,TNo),e(dr,J5),e(J5,FNo),e(J5,Ene),e(Ene,ENo),e(J5,CNo),e(dr,MNo),e(dr,Yr),h(K5,Yr,null),e(Yr,yNo),e(Yr,Cne),e(Cne,wNo),e(Yr,ANo),e(Yr,Ed),e(Ed,LNo),e(Ed,Mne),e(Mne,BNo),e(Ed,xNo),e(Ed,yne),e(yne,kNo),e(Ed,RNo),e(Yr,PNo),e(Yr,wne),e(wne,SNo),e(Yr,$No),h(Y5,Yr,null),e(dr,INo),e(dr,mo),h(Z5,mo,null),e(mo,DNo),e(mo,Ane),e(Ane,NNo),e(mo,jNo),e(mo,Wa),e(Wa,ONo),e(Wa,Lne),e(Lne,GNo),e(Wa,qNo),e(Wa,Bne),e(Bne,zNo),e(Wa,XNo),e(Wa,xne),e(xne,QNo),e(Wa,VNo),e(mo,WNo),e(mo,kne),e(kne,vT),e(vT,Rne),e(Rne,HNo),e(vT,UNo),e(vT,i$),e(i$,JNo),e(vT,KNo),e(mo,YNo),e(mo,Pne),e(Pne,ZNo),e(mo,ejo),h(ey,mo,null),b(d,lye,_),b(d,Cd,_),e(Cd,TT),e(TT,Sne),h(oy,Sne,null),e(Cd,ojo),e(Cd,$ne),e($ne,rjo),b(d,iye,_),b(d,cr,_),h(ry,cr,null),e(cr,tjo),e(cr,Md),e(Md,ajo),e(Md,Ine),e(Ine,sjo),e(Md,njo),e(Md,Dne),e(Dne,ljo),e(Md,ijo),e(cr,djo),e(cr,ty),e(ty,cjo),e(ty,Nne),e(Nne,mjo),e(ty,fjo),e(cr,hjo),e(cr,Zr),h(ay,Zr,null),e(Zr,gjo),e(Zr,jne),e(jne,ujo),e(Zr,pjo),e(Zr,yd),e(yd,_jo),e(yd,One),e(One,bjo),e(yd,vjo),e(yd,Gne),e(Gne,Tjo),e(yd,Fjo),e(Zr,Ejo),e(Zr,qne),e(qne,Cjo),e(Zr,Mjo),h(sy,Zr,null),e(cr,yjo),e(cr,fo),h(ny,fo,null),e(fo,wjo),e(fo,zne),e(zne,Ajo),e(fo,Ljo),e(fo,Ha),e(Ha,Bjo),e(Ha,Xne),e(Xne,xjo),e(Ha,kjo),e(Ha,Qne),e(Qne,Rjo),e(Ha,Pjo),e(Ha,Vne),e(Vne,Sjo),e(Ha,$jo),e(fo,Ijo),e(fo,te),e(te,FT),e(FT,Wne),e(Wne,Djo),e(FT,Njo),e(FT,d$),e(d$,jjo),e(FT,Ojo),e(te,Gjo),e(te,ET),e(ET,Hne),e(Hne,qjo),e(ET,zjo),e(ET,c$),e(c$,Xjo),e(ET,Qjo),e(te,Vjo),e(te,CT),e(CT,Une),e(Une,Wjo),e(CT,Hjo),e(CT,m$),e(m$,Ujo),e(CT,Jjo),e(te,Kjo),e(te,MT),e(MT,Jne),e(Jne,Yjo),e(MT,Zjo),e(MT,f$),e(f$,eOo),e(MT,oOo),e(te,rOo),e(te,yT),e(yT,Kne),e(Kne,tOo),e(yT,aOo),e(yT,h$),e(h$,sOo),e(yT,nOo),e(te,lOo),e(te,wT),e(wT,Yne),e(Yne,iOo),e(wT,dOo),e(wT,g$),e(g$,cOo),e(wT,mOo),e(te,fOo),e(te,AT),e(AT,Zne),e(Zne,hOo),e(AT,gOo),e(AT,u$),e(u$,uOo),e(AT,pOo),e(te,_Oo),e(te,LT),e(LT,ele),e(ele,bOo),e(LT,vOo),e(LT,p$),e(p$,TOo),e(LT,FOo),e(te,EOo),e(te,BT),e(BT,ole),e(ole,COo),e(BT,MOo),e(BT,_$),e(_$,yOo),e(BT,wOo),e(te,AOo),e(te,xT),e(xT,rle),e(rle,LOo),e(xT,BOo),e(xT,b$),e(b$,xOo),e(xT,kOo),e(te,ROo),e(te,kT),e(kT,tle),e(tle,POo),e(kT,SOo),e(kT,v$),e(v$,$Oo),e(kT,IOo),e(te,DOo),e(te,RT),e(RT,ale),e(ale,NOo),e(RT,jOo),e(RT,T$),e(T$,OOo),e(RT,GOo),e(te,qOo),e(te,PT),e(PT,sle),e(sle,zOo),e(PT,XOo),e(PT,F$),e(F$,QOo),e(PT,VOo),e(te,WOo),e(te,ST),e(ST,nle),e(nle,HOo),e(ST,UOo),e(ST,E$),e(E$,JOo),e(ST,KOo),e(te,YOo),e(te,$T),e($T,lle),e(lle,ZOo),e($T,eGo),e($T,C$),e(C$,oGo),e($T,rGo),e(te,tGo),e(te,IT),e(IT,ile),e(ile,aGo),e(IT,sGo),e(IT,M$),e(M$,nGo),e(IT,lGo),e(te,iGo),e(te,DT),e(DT,dle),e(dle,dGo),e(DT,cGo),e(DT,y$),e(y$,mGo),e(DT,fGo),e(te,hGo),e(te,NT),e(NT,cle),e(cle,gGo),e(NT,uGo),e(NT,w$),e(w$,pGo),e(NT,_Go),e(te,bGo),e(te,jT),e(jT,mle),e(mle,vGo),e(jT,TGo),e(jT,A$),e(A$,FGo),e(jT,EGo),e(te,CGo),e(te,OT),e(OT,fle),e(fle,MGo),e(OT,yGo),e(OT,L$),e(L$,wGo),e(OT,AGo),e(fo,LGo),e(fo,hle),e(hle,BGo),e(fo,xGo),h(ly,fo,null),b(d,dye,_),b(d,wd,_),e(wd,GT),e(GT,gle),h(iy,gle,null),e(wd,kGo),e(wd,ule),e(ule,RGo),b(d,cye,_),b(d,mr,_),h(dy,mr,null),e(mr,PGo),e(mr,Ad),e(Ad,SGo),e(Ad,ple),e(ple,$Go),e(Ad,IGo),e(Ad,_le),e(_le,DGo),e(Ad,NGo),e(mr,jGo),e(mr,cy),e(cy,OGo),e(cy,ble),e(ble,GGo),e(cy,qGo),e(mr,zGo),e(mr,et),h(my,et,null),e(et,XGo),e(et,vle),e(vle,QGo),e(et,VGo),e(et,Ld),e(Ld,WGo),e(Ld,Tle),e(Tle,HGo),e(Ld,UGo),e(Ld,Fle),e(Fle,JGo),e(Ld,KGo),e(et,YGo),e(et,Ele),e(Ele,ZGo),e(et,eqo),h(fy,et,null),e(mr,oqo),e(mr,ho),h(hy,ho,null),e(ho,rqo),e(ho,Cle),e(Cle,tqo),e(ho,aqo),e(ho,Ua),e(Ua,sqo),e(Ua,Mle),e(Mle,nqo),e(Ua,lqo),e(Ua,yle),e(yle,iqo),e(Ua,dqo),e(Ua,wle),e(wle,cqo),e(Ua,mqo),e(ho,fqo),e(ho,be),e(be,qT),e(qT,Ale),e(Ale,hqo),e(qT,gqo),e(qT,B$),e(B$,uqo),e(qT,pqo),e(be,_qo),e(be,zT),e(zT,Lle),e(Lle,bqo),e(zT,vqo),e(zT,x$),e(x$,Tqo),e(zT,Fqo),e(be,Eqo),e(be,XT),e(XT,Ble),e(Ble,Cqo),e(XT,Mqo),e(XT,k$),e(k$,yqo),e(XT,wqo),e(be,Aqo),e(be,QT),e(QT,xle),e(xle,Lqo),e(QT,Bqo),e(QT,R$),e(R$,xqo),e(QT,kqo),e(be,Rqo),e(be,VT),e(VT,kle),e(kle,Pqo),e(VT,Sqo),e(VT,P$),e(P$,$qo),e(VT,Iqo),e(be,Dqo),e(be,WT),e(WT,Rle),e(Rle,Nqo),e(WT,jqo),e(WT,S$),e(S$,Oqo),e(WT,Gqo),e(be,qqo),e(be,HT),e(HT,Ple),e(Ple,zqo),e(HT,Xqo),e(HT,$$),e($$,Qqo),e(HT,Vqo),e(be,Wqo),e(be,UT),e(UT,Sle),e(Sle,Hqo),e(UT,Uqo),e(UT,I$),e(I$,Jqo),e(UT,Kqo),e(be,Yqo),e(be,JT),e(JT,$le),e($le,Zqo),e(JT,ezo),e(JT,D$),e(D$,ozo),e(JT,rzo),e(be,tzo),e(be,KT),e(KT,Ile),e(Ile,azo),e(KT,szo),e(KT,N$),e(N$,nzo),e(KT,lzo),e(ho,izo),e(ho,Dle),e(Dle,dzo),e(ho,czo),h(gy,ho,null),b(d,mye,_),b(d,Bd,_),e(Bd,YT),e(YT,Nle),h(uy,Nle,null),e(Bd,mzo),e(Bd,jle),e(jle,fzo),b(d,fye,_),b(d,fr,_),h(py,fr,null),e(fr,hzo),e(fr,xd),e(xd,gzo),e(xd,Ole),e(Ole,uzo),e(xd,pzo),e(xd,Gle),e(Gle,_zo),e(xd,bzo),e(fr,vzo),e(fr,_y),e(_y,Tzo),e(_y,qle),e(qle,Fzo),e(_y,Ezo),e(fr,Czo),e(fr,ot),h(by,ot,null),e(ot,Mzo),e(ot,zle),e(zle,yzo),e(ot,wzo),e(ot,kd),e(kd,Azo),e(kd,Xle),e(Xle,Lzo),e(kd,Bzo),e(kd,Qle),e(Qle,xzo),e(kd,kzo),e(ot,Rzo),e(ot,Vle),e(Vle,Pzo),e(ot,Szo),h(vy,ot,null),e(fr,$zo),e(fr,go),h(Ty,go,null),e(go,Izo),e(go,Wle),e(Wle,Dzo),e(go,Nzo),e(go,Ja),e(Ja,jzo),e(Ja,Hle),e(Hle,Ozo),e(Ja,Gzo),e(Ja,Ule),e(Ule,qzo),e(Ja,zzo),e(Ja,Jle),e(Jle,Xzo),e(Ja,Qzo),e(go,Vzo),e(go,W),e(W,ZT),e(ZT,Kle),e(Kle,Wzo),e(ZT,Hzo),e(ZT,j$),e(j$,Uzo),e(ZT,Jzo),e(W,Kzo),e(W,e1),e(e1,Yle),e(Yle,Yzo),e(e1,Zzo),e(e1,O$),e(O$,eXo),e(e1,oXo),e(W,rXo),e(W,o1),e(o1,Zle),e(Zle,tXo),e(o1,aXo),e(o1,G$),e(G$,sXo),e(o1,nXo),e(W,lXo),e(W,r1),e(r1,eie),e(eie,iXo),e(r1,dXo),e(r1,q$),e(q$,cXo),e(r1,mXo),e(W,fXo),e(W,t1),e(t1,oie),e(oie,hXo),e(t1,gXo),e(t1,z$),e(z$,uXo),e(t1,pXo),e(W,_Xo),e(W,a1),e(a1,rie),e(rie,bXo),e(a1,vXo),e(a1,X$),e(X$,TXo),e(a1,FXo),e(W,EXo),e(W,s1),e(s1,tie),e(tie,CXo),e(s1,MXo),e(s1,Q$),e(Q$,yXo),e(s1,wXo),e(W,AXo),e(W,n1),e(n1,aie),e(aie,LXo),e(n1,BXo),e(n1,V$),e(V$,xXo),e(n1,kXo),e(W,RXo),e(W,l1),e(l1,sie),e(sie,PXo),e(l1,SXo),e(l1,W$),e(W$,$Xo),e(l1,IXo),e(W,DXo),e(W,i1),e(i1,nie),e(nie,NXo),e(i1,jXo),e(i1,H$),e(H$,OXo),e(i1,GXo),e(W,qXo),e(W,d1),e(d1,lie),e(lie,zXo),e(d1,XXo),e(d1,U$),e(U$,QXo),e(d1,VXo),e(W,WXo),e(W,c1),e(c1,iie),e(iie,HXo),e(c1,UXo),e(c1,J$),e(J$,JXo),e(c1,KXo),e(W,YXo),e(W,m1),e(m1,die),e(die,ZXo),e(m1,eQo),e(m1,K$),e(K$,oQo),e(m1,rQo),e(W,tQo),e(W,f1),e(f1,cie),e(cie,aQo),e(f1,sQo),e(f1,Y$),e(Y$,nQo),e(f1,lQo),e(W,iQo),e(W,h1),e(h1,mie),e(mie,dQo),e(h1,cQo),e(h1,Z$),e(Z$,mQo),e(h1,fQo),e(W,hQo),e(W,g1),e(g1,fie),e(fie,gQo),e(g1,uQo),e(g1,eI),e(eI,pQo),e(g1,_Qo),e(W,bQo),e(W,u1),e(u1,hie),e(hie,vQo),e(u1,TQo),e(u1,oI),e(oI,FQo),e(u1,EQo),e(W,CQo),e(W,p1),e(p1,gie),e(gie,MQo),e(p1,yQo),e(p1,rI),e(rI,wQo),e(p1,AQo),e(W,LQo),e(W,_1),e(_1,uie),e(uie,BQo),e(_1,xQo),e(_1,tI),e(tI,kQo),e(_1,RQo),e(W,PQo),e(W,b1),e(b1,pie),e(pie,SQo),e(b1,$Qo),e(b1,aI),e(aI,IQo),e(b1,DQo),e(W,NQo),e(W,v1),e(v1,_ie),e(_ie,jQo),e(v1,OQo),e(v1,sI),e(sI,GQo),e(v1,qQo),e(W,zQo),e(W,T1),e(T1,bie),e(bie,XQo),e(T1,QQo),e(T1,nI),e(nI,VQo),e(T1,WQo),e(W,HQo),e(W,F1),e(F1,vie),e(vie,UQo),e(F1,JQo),e(F1,lI),e(lI,KQo),e(F1,YQo),e(W,ZQo),e(W,E1),e(E1,Tie),e(Tie,eVo),e(E1,oVo),e(E1,iI),e(iI,rVo),e(E1,tVo),e(W,aVo),e(W,C1),e(C1,Fie),e(Fie,sVo),e(C1,nVo),e(C1,dI),e(dI,lVo),e(C1,iVo),e(go,dVo),e(go,Eie),e(Eie,cVo),e(go,mVo),h(Fy,go,null),b(d,hye,_),b(d,Rd,_),e(Rd,M1),e(M1,Cie),h(Ey,Cie,null),e(Rd,fVo),e(Rd,Mie),e(Mie,hVo),b(d,gye,_),b(d,hr,_),h(Cy,hr,null),e(hr,gVo),e(hr,Pd),e(Pd,uVo),e(Pd,yie),e(yie,pVo),e(Pd,_Vo),e(Pd,wie),e(wie,bVo),e(Pd,vVo),e(hr,TVo),e(hr,My),e(My,FVo),e(My,Aie),e(Aie,EVo),e(My,CVo),e(hr,MVo),e(hr,rt),h(yy,rt,null),e(rt,yVo),e(rt,Lie),e(Lie,wVo),e(rt,AVo),e(rt,Sd),e(Sd,LVo),e(Sd,Bie),e(Bie,BVo),e(Sd,xVo),e(Sd,xie),e(xie,kVo),e(Sd,RVo),e(rt,PVo),e(rt,kie),e(kie,SVo),e(rt,$Vo),h(wy,rt,null),e(hr,IVo),e(hr,uo),h(Ay,uo,null),e(uo,DVo),e(uo,Rie),e(Rie,NVo),e(uo,jVo),e(uo,Ka),e(Ka,OVo),e(Ka,Pie),e(Pie,GVo),e(Ka,qVo),e(Ka,Sie),e(Sie,zVo),e(Ka,XVo),e(Ka,$ie),e($ie,QVo),e(Ka,VVo),e(uo,WVo),e(uo,de),e(de,y1),e(y1,Iie),e(Iie,HVo),e(y1,UVo),e(y1,cI),e(cI,JVo),e(y1,KVo),e(de,YVo),e(de,w1),e(w1,Die),e(Die,ZVo),e(w1,eWo),e(w1,mI),e(mI,oWo),e(w1,rWo),e(de,tWo),e(de,A1),e(A1,Nie),e(Nie,aWo),e(A1,sWo),e(A1,fI),e(fI,nWo),e(A1,lWo),e(de,iWo),e(de,L1),e(L1,jie),e(jie,dWo),e(L1,cWo),e(L1,hI),e(hI,mWo),e(L1,fWo),e(de,hWo),e(de,B1),e(B1,Oie),e(Oie,gWo),e(B1,uWo),e(B1,gI),e(gI,pWo),e(B1,_Wo),e(de,bWo),e(de,x1),e(x1,Gie),e(Gie,vWo),e(x1,TWo),e(x1,uI),e(uI,FWo),e(x1,EWo),e(de,CWo),e(de,k1),e(k1,qie),e(qie,MWo),e(k1,yWo),e(k1,pI),e(pI,wWo),e(k1,AWo),e(de,LWo),e(de,R1),e(R1,zie),e(zie,BWo),e(R1,xWo),e(R1,_I),e(_I,kWo),e(R1,RWo),e(de,PWo),e(de,P1),e(P1,Xie),e(Xie,SWo),e(P1,$Wo),e(P1,bI),e(bI,IWo),e(P1,DWo),e(de,NWo),e(de,S1),e(S1,Qie),e(Qie,jWo),e(S1,OWo),e(S1,vI),e(vI,GWo),e(S1,qWo),e(de,zWo),e(de,$1),e($1,Vie),e(Vie,XWo),e($1,QWo),e($1,TI),e(TI,VWo),e($1,WWo),e(de,HWo),e(de,I1),e(I1,Wie),e(Wie,UWo),e(I1,JWo),e(I1,FI),e(FI,KWo),e(I1,YWo),e(de,ZWo),e(de,D1),e(D1,Hie),e(Hie,eHo),e(D1,oHo),e(D1,EI),e(EI,rHo),e(D1,tHo),e(de,aHo),e(de,N1),e(N1,Uie),e(Uie,sHo),e(N1,nHo),e(N1,CI),e(CI,lHo),e(N1,iHo),e(de,dHo),e(de,j1),e(j1,Jie),e(Jie,cHo),e(j1,mHo),e(j1,MI),e(MI,fHo),e(j1,hHo),e(de,gHo),e(de,O1),e(O1,Kie),e(Kie,uHo),e(O1,pHo),e(O1,yI),e(yI,_Ho),e(O1,bHo),e(de,vHo),e(de,G1),e(G1,Yie),e(Yie,THo),e(G1,FHo),e(G1,wI),e(wI,EHo),e(G1,CHo),e(uo,MHo),e(uo,Zie),e(Zie,yHo),e(uo,wHo),h(Ly,uo,null),b(d,uye,_),b(d,$d,_),e($d,q1),e(q1,ede),h(By,ede,null),e($d,AHo),e($d,ode),e(ode,LHo),b(d,pye,_),b(d,gr,_),h(xy,gr,null),e(gr,BHo),e(gr,Id),e(Id,xHo),e(Id,rde),e(rde,kHo),e(Id,RHo),e(Id,tde),e(tde,PHo),e(Id,SHo),e(gr,$Ho),e(gr,ky),e(ky,IHo),e(ky,ade),e(ade,DHo),e(ky,NHo),e(gr,jHo),e(gr,tt),h(Ry,tt,null),e(tt,OHo),e(tt,sde),e(sde,GHo),e(tt,qHo),e(tt,Dd),e(Dd,zHo),e(Dd,nde),e(nde,XHo),e(Dd,QHo),e(Dd,lde),e(lde,VHo),e(Dd,WHo),e(tt,HHo),e(tt,ide),e(ide,UHo),e(tt,JHo),h(Py,tt,null),e(gr,KHo),e(gr,po),h(Sy,po,null),e(po,YHo),e(po,dde),e(dde,ZHo),e(po,eUo),e(po,Ya),e(Ya,oUo),e(Ya,cde),e(cde,rUo),e(Ya,tUo),e(Ya,mde),e(mde,aUo),e(Ya,sUo),e(Ya,fde),e(fde,nUo),e(Ya,lUo),e(po,iUo),e(po,hde),e(hde,z1),e(z1,gde),e(gde,dUo),e(z1,cUo),e(z1,AI),e(AI,mUo),e(z1,fUo),e(po,hUo),e(po,ude),e(ude,gUo),e(po,uUo),h($y,po,null),b(d,_ye,_),b(d,Nd,_),e(Nd,X1),e(X1,pde),h(Iy,pde,null),e(Nd,pUo),e(Nd,_de),e(_de,_Uo),b(d,bye,_),b(d,ur,_),h(Dy,ur,null),e(ur,bUo),e(ur,jd),e(jd,vUo),e(jd,bde),e(bde,TUo),e(jd,FUo),e(jd,vde),e(vde,EUo),e(jd,CUo),e(ur,MUo),e(ur,Ny),e(Ny,yUo),e(Ny,Tde),e(Tde,wUo),e(Ny,AUo),e(ur,LUo),e(ur,at),h(jy,at,null),e(at,BUo),e(at,Fde),e(Fde,xUo),e(at,kUo),e(at,Od),e(Od,RUo),e(Od,Ede),e(Ede,PUo),e(Od,SUo),e(Od,Cde),e(Cde,$Uo),e(Od,IUo),e(at,DUo),e(at,Mde),e(Mde,NUo),e(at,jUo),h(Oy,at,null),e(ur,OUo),e(ur,_o),h(Gy,_o,null),e(_o,GUo),e(_o,yde),e(yde,qUo),e(_o,zUo),e(_o,Za),e(Za,XUo),e(Za,wde),e(wde,QUo),e(Za,VUo),e(Za,Ade),e(Ade,WUo),e(Za,HUo),e(Za,Lde),e(Lde,UUo),e(Za,JUo),e(_o,KUo),e(_o,ae),e(ae,Q1),e(Q1,Bde),e(Bde,YUo),e(Q1,ZUo),e(Q1,LI),e(LI,eJo),e(Q1,oJo),e(ae,rJo),e(ae,V1),e(V1,xde),e(xde,tJo),e(V1,aJo),e(V1,BI),e(BI,sJo),e(V1,nJo),e(ae,lJo),e(ae,W1),e(W1,kde),e(kde,iJo),e(W1,dJo),e(W1,xI),e(xI,cJo),e(W1,mJo),e(ae,fJo),e(ae,H1),e(H1,Rde),e(Rde,hJo),e(H1,gJo),e(H1,kI),e(kI,uJo),e(H1,pJo),e(ae,_Jo),e(ae,U1),e(U1,Pde),e(Pde,bJo),e(U1,vJo),e(U1,RI),e(RI,TJo),e(U1,FJo),e(ae,EJo),e(ae,J1),e(J1,Sde),e(Sde,CJo),e(J1,MJo),e(J1,PI),e(PI,yJo),e(J1,wJo),e(ae,AJo),e(ae,K1),e(K1,$de),e($de,LJo),e(K1,BJo),e(K1,SI),e(SI,xJo),e(K1,kJo),e(ae,RJo),e(ae,Y1),e(Y1,Ide),e(Ide,PJo),e(Y1,SJo),e(Y1,$I),e($I,$Jo),e(Y1,IJo),e(ae,DJo),e(ae,Z1),e(Z1,Dde),e(Dde,NJo),e(Z1,jJo),e(Z1,II),e(II,OJo),e(Z1,GJo),e(ae,qJo),e(ae,eF),e(eF,Nde),e(Nde,zJo),e(eF,XJo),e(eF,DI),e(DI,QJo),e(eF,VJo),e(ae,WJo),e(ae,oF),e(oF,jde),e(jde,HJo),e(oF,UJo),e(oF,NI),e(NI,JJo),e(oF,KJo),e(ae,YJo),e(ae,rF),e(rF,Ode),e(Ode,ZJo),e(rF,eKo),e(rF,jI),e(jI,oKo),e(rF,rKo),e(ae,tKo),e(ae,tF),e(tF,Gde),e(Gde,aKo),e(tF,sKo),e(tF,OI),e(OI,nKo),e(tF,lKo),e(ae,iKo),e(ae,aF),e(aF,qde),e(qde,dKo),e(aF,cKo),e(aF,GI),e(GI,mKo),e(aF,fKo),e(ae,hKo),e(ae,sF),e(sF,zde),e(zde,gKo),e(sF,uKo),e(sF,qI),e(qI,pKo),e(sF,_Ko),e(ae,bKo),e(ae,nF),e(nF,Xde),e(Xde,vKo),e(nF,TKo),e(nF,zI),e(zI,FKo),e(nF,EKo),e(ae,CKo),e(ae,lF),e(lF,Qde),e(Qde,MKo),e(lF,yKo),e(lF,XI),e(XI,wKo),e(lF,AKo),e(ae,LKo),e(ae,iF),e(iF,Vde),e(Vde,BKo),e(iF,xKo),e(iF,QI),e(QI,kKo),e(iF,RKo),e(ae,PKo),e(ae,dF),e(dF,Wde),e(Wde,SKo),e(dF,$Ko),e(dF,VI),e(VI,IKo),e(dF,DKo),e(ae,NKo),e(ae,cF),e(cF,Hde),e(Hde,jKo),e(cF,OKo),e(cF,WI),e(WI,GKo),e(cF,qKo),e(_o,zKo),e(_o,Ude),e(Ude,XKo),e(_o,QKo),h(qy,_o,null),b(d,vye,_),b(d,Gd,_),e(Gd,mF),e(mF,Jde),h(zy,Jde,null),e(Gd,VKo),e(Gd,Kde),e(Kde,WKo),b(d,Tye,_),b(d,pr,_),h(Xy,pr,null),e(pr,HKo),e(pr,qd),e(qd,UKo),e(qd,Yde),e(Yde,JKo),e(qd,KKo),e(qd,Zde),e(Zde,YKo),e(qd,ZKo),e(pr,eYo),e(pr,Qy),e(Qy,oYo),e(Qy,ece),e(ece,rYo),e(Qy,tYo),e(pr,aYo),e(pr,st),h(Vy,st,null),e(st,sYo),e(st,oce),e(oce,nYo),e(st,lYo),e(st,zd),e(zd,iYo),e(zd,rce),e(rce,dYo),e(zd,cYo),e(zd,tce),e(tce,mYo),e(zd,fYo),e(st,hYo),e(st,ace),e(ace,gYo),e(st,uYo),h(Wy,st,null),e(pr,pYo),e(pr,bo),h(Hy,bo,null),e(bo,_Yo),e(bo,sce),e(sce,bYo),e(bo,vYo),e(bo,es),e(es,TYo),e(es,nce),e(nce,FYo),e(es,EYo),e(es,lce),e(lce,CYo),e(es,MYo),e(es,ice),e(ice,yYo),e(es,wYo),e(bo,AYo),e(bo,se),e(se,fF),e(fF,dce),e(dce,LYo),e(fF,BYo),e(fF,HI),e(HI,xYo),e(fF,kYo),e(se,RYo),e(se,hF),e(hF,cce),e(cce,PYo),e(hF,SYo),e(hF,UI),e(UI,$Yo),e(hF,IYo),e(se,DYo),e(se,gF),e(gF,mce),e(mce,NYo),e(gF,jYo),e(gF,JI),e(JI,OYo),e(gF,GYo),e(se,qYo),e(se,uF),e(uF,fce),e(fce,zYo),e(uF,XYo),e(uF,KI),e(KI,QYo),e(uF,VYo),e(se,WYo),e(se,pF),e(pF,hce),e(hce,HYo),e(pF,UYo),e(pF,YI),e(YI,JYo),e(pF,KYo),e(se,YYo),e(se,_F),e(_F,gce),e(gce,ZYo),e(_F,eZo),e(_F,ZI),e(ZI,oZo),e(_F,rZo),e(se,tZo),e(se,bF),e(bF,uce),e(uce,aZo),e(bF,sZo),e(bF,eD),e(eD,nZo),e(bF,lZo),e(se,iZo),e(se,vF),e(vF,pce),e(pce,dZo),e(vF,cZo),e(vF,oD),e(oD,mZo),e(vF,fZo),e(se,hZo),e(se,TF),e(TF,_ce),e(_ce,gZo),e(TF,uZo),e(TF,rD),e(rD,pZo),e(TF,_Zo),e(se,bZo),e(se,FF),e(FF,bce),e(bce,vZo),e(FF,TZo),e(FF,tD),e(tD,FZo),e(FF,EZo),e(se,CZo),e(se,EF),e(EF,vce),e(vce,MZo),e(EF,yZo),e(EF,aD),e(aD,wZo),e(EF,AZo),e(se,LZo),e(se,CF),e(CF,Tce),e(Tce,BZo),e(CF,xZo),e(CF,sD),e(sD,kZo),e(CF,RZo),e(se,PZo),e(se,MF),e(MF,Fce),e(Fce,SZo),e(MF,$Zo),e(MF,nD),e(nD,IZo),e(MF,DZo),e(se,NZo),e(se,yF),e(yF,Ece),e(Ece,jZo),e(yF,OZo),e(yF,lD),e(lD,GZo),e(yF,qZo),e(se,zZo),e(se,wF),e(wF,Cce),e(Cce,XZo),e(wF,QZo),e(wF,iD),e(iD,VZo),e(wF,WZo),e(se,HZo),e(se,AF),e(AF,Mce),e(Mce,UZo),e(AF,JZo),e(AF,dD),e(dD,KZo),e(AF,YZo),e(se,ZZo),e(se,LF),e(LF,yce),e(yce,eer),e(LF,oer),e(LF,cD),e(cD,rer),e(LF,ter),e(se,aer),e(se,BF),e(BF,wce),e(wce,ser),e(BF,ner),e(BF,mD),e(mD,ler),e(BF,ier),e(se,der),e(se,xF),e(xF,Ace),e(Ace,cer),e(xF,mer),e(xF,fD),e(fD,fer),e(xF,her),e(bo,ger),e(bo,Lce),e(Lce,uer),e(bo,per),h(Uy,bo,null),b(d,Fye,_),b(d,Xd,_),e(Xd,kF),e(kF,Bce),h(Jy,Bce,null),e(Xd,_er),e(Xd,xce),e(xce,ber),b(d,Eye,_),b(d,_r,_),h(Ky,_r,null),e(_r,ver),e(_r,Qd),e(Qd,Ter),e(Qd,kce),e(kce,Fer),e(Qd,Eer),e(Qd,Rce),e(Rce,Cer),e(Qd,Mer),e(_r,yer),e(_r,Yy),e(Yy,wer),e(Yy,Pce),e(Pce,Aer),e(Yy,Ler),e(_r,Ber),e(_r,nt),h(Zy,nt,null),e(nt,xer),e(nt,Sce),e(Sce,ker),e(nt,Rer),e(nt,Vd),e(Vd,Per),e(Vd,$ce),e($ce,Ser),e(Vd,$er),e(Vd,Ice),e(Ice,Ier),e(Vd,Der),e(nt,Ner),e(nt,Dce),e(Dce,jer),e(nt,Oer),h(ew,nt,null),e(_r,Ger),e(_r,vo),h(ow,vo,null),e(vo,qer),e(vo,Nce),e(Nce,zer),e(vo,Xer),e(vo,os),e(os,Qer),e(os,jce),e(jce,Ver),e(os,Wer),e(os,Oce),e(Oce,Her),e(os,Uer),e(os,Gce),e(Gce,Jer),e(os,Ker),e(vo,Yer),e(vo,Y),e(Y,RF),e(RF,qce),e(qce,Zer),e(RF,eor),e(RF,hD),e(hD,oor),e(RF,ror),e(Y,tor),e(Y,PF),e(PF,zce),e(zce,aor),e(PF,sor),e(PF,gD),e(gD,nor),e(PF,lor),e(Y,ior),e(Y,SF),e(SF,Xce),e(Xce,dor),e(SF,cor),e(SF,uD),e(uD,mor),e(SF,hor),e(Y,gor),e(Y,$F),e($F,Qce),e(Qce,uor),e($F,por),e($F,pD),e(pD,_or),e($F,bor),e(Y,vor),e(Y,IF),e(IF,Vce),e(Vce,Tor),e(IF,For),e(IF,_D),e(_D,Eor),e(IF,Cor),e(Y,Mor),e(Y,DF),e(DF,Wce),e(Wce,yor),e(DF,wor),e(DF,bD),e(bD,Aor),e(DF,Lor),e(Y,Bor),e(Y,NF),e(NF,Hce),e(Hce,xor),e(NF,kor),e(NF,vD),e(vD,Ror),e(NF,Por),e(Y,Sor),e(Y,jF),e(jF,Uce),e(Uce,$or),e(jF,Ior),e(jF,TD),e(TD,Dor),e(jF,Nor),e(Y,jor),e(Y,OF),e(OF,Jce),e(Jce,Oor),e(OF,Gor),e(OF,FD),e(FD,qor),e(OF,zor),e(Y,Xor),e(Y,GF),e(GF,Kce),e(Kce,Qor),e(GF,Vor),e(GF,ED),e(ED,Wor),e(GF,Hor),e(Y,Uor),e(Y,qF),e(qF,Yce),e(Yce,Jor),e(qF,Kor),e(qF,CD),e(CD,Yor),e(qF,Zor),e(Y,err),e(Y,zF),e(zF,Zce),e(Zce,orr),e(zF,rrr),e(zF,MD),e(MD,trr),e(zF,arr),e(Y,srr),e(Y,XF),e(XF,eme),e(eme,nrr),e(XF,lrr),e(XF,yD),e(yD,irr),e(XF,drr),e(Y,crr),e(Y,QF),e(QF,ome),e(ome,mrr),e(QF,frr),e(QF,wD),e(wD,hrr),e(QF,grr),e(Y,urr),e(Y,VF),e(VF,rme),e(rme,prr),e(VF,_rr),e(VF,AD),e(AD,brr),e(VF,vrr),e(Y,Trr),e(Y,WF),e(WF,tme),e(tme,Frr),e(WF,Err),e(WF,LD),e(LD,Crr),e(WF,Mrr),e(Y,yrr),e(Y,HF),e(HF,ame),e(ame,wrr),e(HF,Arr),e(HF,BD),e(BD,Lrr),e(HF,Brr),e(Y,xrr),e(Y,UF),e(UF,sme),e(sme,krr),e(UF,Rrr),e(UF,xD),e(xD,Prr),e(UF,Srr),e(Y,$rr),e(Y,JF),e(JF,nme),e(nme,Irr),e(JF,Drr),e(JF,kD),e(kD,Nrr),e(JF,jrr),e(Y,Orr),e(Y,KF),e(KF,lme),e(lme,Grr),e(KF,qrr),e(KF,RD),e(RD,zrr),e(KF,Xrr),e(Y,Qrr),e(Y,YF),e(YF,ime),e(ime,Vrr),e(YF,Wrr),e(YF,PD),e(PD,Hrr),e(YF,Urr),e(Y,Jrr),e(Y,ZF),e(ZF,dme),e(dme,Krr),e(ZF,Yrr),e(ZF,SD),e(SD,Zrr),e(ZF,etr),e(vo,otr),e(vo,cme),e(cme,rtr),e(vo,ttr),h(rw,vo,null),b(d,Cye,_),b(d,Wd,_),e(Wd,eE),e(eE,mme),h(tw,mme,null),e(Wd,atr),e(Wd,fme),e(fme,str),b(d,Mye,_),b(d,br,_),h(aw,br,null),e(br,ntr),e(br,Hd),e(Hd,ltr),e(Hd,hme),e(hme,itr),e(Hd,dtr),e(Hd,gme),e(gme,ctr),e(Hd,mtr),e(br,ftr),e(br,sw),e(sw,htr),e(sw,ume),e(ume,gtr),e(sw,utr),e(br,ptr),e(br,lt),h(nw,lt,null),e(lt,_tr),e(lt,pme),e(pme,btr),e(lt,vtr),e(lt,Ud),e(Ud,Ttr),e(Ud,_me),e(_me,Ftr),e(Ud,Etr),e(Ud,bme),e(bme,Ctr),e(Ud,Mtr),e(lt,ytr),e(lt,vme),e(vme,wtr),e(lt,Atr),h(lw,lt,null),e(br,Ltr),e(br,To),h(iw,To,null),e(To,Btr),e(To,Tme),e(Tme,xtr),e(To,ktr),e(To,rs),e(rs,Rtr),e(rs,Fme),e(Fme,Ptr),e(rs,Str),e(rs,Eme),e(Eme,$tr),e(rs,Itr),e(rs,Cme),e(Cme,Dtr),e(rs,Ntr),e(To,jtr),e(To,Jd),e(Jd,oE),e(oE,Mme),e(Mme,Otr),e(oE,Gtr),e(oE,$D),e($D,qtr),e(oE,ztr),e(Jd,Xtr),e(Jd,rE),e(rE,yme),e(yme,Qtr),e(rE,Vtr),e(rE,ID),e(ID,Wtr),e(rE,Htr),e(Jd,Utr),e(Jd,tE),e(tE,wme),e(wme,Jtr),e(tE,Ktr),e(tE,DD),e(DD,Ytr),e(tE,Ztr),e(To,ear),e(To,Ame),e(Ame,oar),e(To,rar),h(dw,To,null),b(d,yye,_),b(d,Kd,_),e(Kd,aE),e(aE,Lme),h(cw,Lme,null),e(Kd,tar),e(Kd,Bme),e(Bme,aar),b(d,wye,_),b(d,vr,_),h(mw,vr,null),e(vr,sar),e(vr,Yd),e(Yd,nar),e(Yd,xme),e(xme,lar),e(Yd,iar),e(Yd,kme),e(kme,dar),e(Yd,car),e(vr,mar),e(vr,fw),e(fw,far),e(fw,Rme),e(Rme,har),e(fw,gar),e(vr,uar),e(vr,it),h(hw,it,null),e(it,par),e(it,Pme),e(Pme,_ar),e(it,bar),e(it,Zd),e(Zd,Tar),e(Zd,Sme),e(Sme,Far),e(Zd,Ear),e(Zd,$me),e($me,Car),e(Zd,Mar),e(it,yar),e(it,Ime),e(Ime,war),e(it,Aar),h(gw,it,null),e(vr,Lar),e(vr,Fo),h(uw,Fo,null),e(Fo,Bar),e(Fo,Dme),e(Dme,xar),e(Fo,kar),e(Fo,ts),e(ts,Rar),e(ts,Nme),e(Nme,Par),e(ts,Sar),e(ts,jme),e(jme,$ar),e(ts,Iar),e(ts,Ome),e(Ome,Dar),e(ts,Nar),e(Fo,jar),e(Fo,ve),e(ve,sE),e(sE,Gme),e(Gme,Oar),e(sE,Gar),e(sE,ND),e(ND,qar),e(sE,zar),e(ve,Xar),e(ve,nE),e(nE,qme),e(qme,Qar),e(nE,Var),e(nE,jD),e(jD,War),e(nE,Har),e(ve,Uar),e(ve,lE),e(lE,zme),e(zme,Jar),e(lE,Kar),e(lE,OD),e(OD,Yar),e(lE,Zar),e(ve,esr),e(ve,iE),e(iE,Xme),e(Xme,osr),e(iE,rsr),e(iE,GD),e(GD,tsr),e(iE,asr),e(ve,ssr),e(ve,dE),e(dE,Qme),e(Qme,nsr),e(dE,lsr),e(dE,qD),e(qD,isr),e(dE,dsr),e(ve,csr),e(ve,cE),e(cE,Vme),e(Vme,msr),e(cE,fsr),e(cE,zD),e(zD,hsr),e(cE,gsr),e(ve,usr),e(ve,mE),e(mE,Wme),e(Wme,psr),e(mE,_sr),e(mE,XD),e(XD,bsr),e(mE,vsr),e(ve,Tsr),e(ve,fE),e(fE,Hme),e(Hme,Fsr),e(fE,Esr),e(fE,QD),e(QD,Csr),e(fE,Msr),e(ve,ysr),e(ve,hE),e(hE,Ume),e(Ume,wsr),e(hE,Asr),e(hE,VD),e(VD,Lsr),e(hE,Bsr),e(ve,xsr),e(ve,gE),e(gE,Jme),e(Jme,ksr),e(gE,Rsr),e(gE,WD),e(WD,Psr),e(gE,Ssr),e(Fo,$sr),e(Fo,Kme),e(Kme,Isr),e(Fo,Dsr),h(pw,Fo,null),b(d,Aye,_),b(d,ec,_),e(ec,uE),e(uE,Yme),h(_w,Yme,null),e(ec,Nsr),e(ec,Zme),e(Zme,jsr),b(d,Lye,_),b(d,Tr,_),h(bw,Tr,null),e(Tr,Osr),e(Tr,oc),e(oc,Gsr),e(oc,efe),e(efe,qsr),e(oc,zsr),e(oc,ofe),e(ofe,Xsr),e(oc,Qsr),e(Tr,Vsr),e(Tr,vw),e(vw,Wsr),e(vw,rfe),e(rfe,Hsr),e(vw,Usr),e(Tr,Jsr),e(Tr,dt),h(Tw,dt,null),e(dt,Ksr),e(dt,tfe),e(tfe,Ysr),e(dt,Zsr),e(dt,rc),e(rc,enr),e(rc,afe),e(afe,onr),e(rc,rnr),e(rc,sfe),e(sfe,tnr),e(rc,anr),e(dt,snr),e(dt,nfe),e(nfe,nnr),e(dt,lnr),h(Fw,dt,null),e(Tr,inr),e(Tr,Eo),h(Ew,Eo,null),e(Eo,dnr),e(Eo,lfe),e(lfe,cnr),e(Eo,mnr),e(Eo,as),e(as,fnr),e(as,ife),e(ife,hnr),e(as,gnr),e(as,dfe),e(dfe,unr),e(as,pnr),e(as,cfe),e(cfe,_nr),e(as,bnr),e(Eo,vnr),e(Eo,xe),e(xe,pE),e(pE,mfe),e(mfe,Tnr),e(pE,Fnr),e(pE,HD),e(HD,Enr),e(pE,Cnr),e(xe,Mnr),e(xe,_E),e(_E,ffe),e(ffe,ynr),e(_E,wnr),e(_E,UD),e(UD,Anr),e(_E,Lnr),e(xe,Bnr),e(xe,bE),e(bE,hfe),e(hfe,xnr),e(bE,knr),e(bE,JD),e(JD,Rnr),e(bE,Pnr),e(xe,Snr),e(xe,vE),e(vE,gfe),e(gfe,$nr),e(vE,Inr),e(vE,KD),e(KD,Dnr),e(vE,Nnr),e(xe,jnr),e(xe,TE),e(TE,ufe),e(ufe,Onr),e(TE,Gnr),e(TE,YD),e(YD,qnr),e(TE,znr),e(xe,Xnr),e(xe,FE),e(FE,pfe),e(pfe,Qnr),e(FE,Vnr),e(FE,ZD),e(ZD,Wnr),e(FE,Hnr),e(xe,Unr),e(xe,EE),e(EE,_fe),e(_fe,Jnr),e(EE,Knr),e(EE,eN),e(eN,Ynr),e(EE,Znr),e(xe,elr),e(xe,CE),e(CE,bfe),e(bfe,olr),e(CE,rlr),e(CE,oN),e(oN,tlr),e(CE,alr),e(Eo,slr),e(Eo,vfe),e(vfe,nlr),e(Eo,llr),h(Cw,Eo,null),b(d,Bye,_),b(d,tc,_),e(tc,ME),e(ME,Tfe),h(Mw,Tfe,null),e(tc,ilr),e(tc,Ffe),e(Ffe,dlr),b(d,xye,_),b(d,Fr,_),h(yw,Fr,null),e(Fr,clr),e(Fr,ac),e(ac,mlr),e(ac,Efe),e(Efe,flr),e(ac,hlr),e(ac,Cfe),e(Cfe,glr),e(ac,ulr),e(Fr,plr),e(Fr,ww),e(ww,_lr),e(ww,Mfe),e(Mfe,blr),e(ww,vlr),e(Fr,Tlr),e(Fr,ct),h(Aw,ct,null),e(ct,Flr),e(ct,yfe),e(yfe,Elr),e(ct,Clr),e(ct,sc),e(sc,Mlr),e(sc,wfe),e(wfe,ylr),e(sc,wlr),e(sc,Afe),e(Afe,Alr),e(sc,Llr),e(ct,Blr),e(ct,Lfe),e(Lfe,xlr),e(ct,klr),h(Lw,ct,null),e(Fr,Rlr),e(Fr,Co),h(Bw,Co,null),e(Co,Plr),e(Co,Bfe),e(Bfe,Slr),e(Co,$lr),e(Co,ss),e(ss,Ilr),e(ss,xfe),e(xfe,Dlr),e(ss,Nlr),e(ss,kfe),e(kfe,jlr),e(ss,Olr),e(ss,Rfe),e(Rfe,Glr),e(ss,qlr),e(Co,zlr),e(Co,Me),e(Me,yE),e(yE,Pfe),e(Pfe,Xlr),e(yE,Qlr),e(yE,rN),e(rN,Vlr),e(yE,Wlr),e(Me,Hlr),e(Me,wE),e(wE,Sfe),e(Sfe,Ulr),e(wE,Jlr),e(wE,tN),e(tN,Klr),e(wE,Ylr),e(Me,Zlr),e(Me,AE),e(AE,$fe),e($fe,eir),e(AE,oir),e(AE,aN),e(aN,rir),e(AE,tir),e(Me,air),e(Me,LE),e(LE,Ife),e(Ife,sir),e(LE,nir),e(LE,sN),e(sN,lir),e(LE,iir),e(Me,dir),e(Me,BE),e(BE,Dfe),e(Dfe,cir),e(BE,mir),e(BE,nN),e(nN,fir),e(BE,hir),e(Me,gir),e(Me,xE),e(xE,Nfe),e(Nfe,uir),e(xE,pir),e(xE,lN),e(lN,_ir),e(xE,bir),e(Me,vir),e(Me,kE),e(kE,jfe),e(jfe,Tir),e(kE,Fir),e(kE,iN),e(iN,Eir),e(kE,Cir),e(Me,Mir),e(Me,RE),e(RE,Ofe),e(Ofe,yir),e(RE,wir),e(RE,dN),e(dN,Air),e(RE,Lir),e(Me,Bir),e(Me,PE),e(PE,Gfe),e(Gfe,xir),e(PE,kir),e(PE,cN),e(cN,Rir),e(PE,Pir),e(Co,Sir),e(Co,qfe),e(qfe,$ir),e(Co,Iir),h(xw,Co,null),b(d,kye,_),b(d,nc,_),e(nc,SE),e(SE,zfe),h(kw,zfe,null),e(nc,Dir),e(nc,Xfe),e(Xfe,Nir),b(d,Rye,_),b(d,Er,_),h(Rw,Er,null),e(Er,jir),e(Er,lc),e(lc,Oir),e(lc,Qfe),e(Qfe,Gir),e(lc,qir),e(lc,Vfe),e(Vfe,zir),e(lc,Xir),e(Er,Qir),e(Er,Pw),e(Pw,Vir),e(Pw,Wfe),e(Wfe,Wir),e(Pw,Hir),e(Er,Uir),e(Er,mt),h(Sw,mt,null),e(mt,Jir),e(mt,Hfe),e(Hfe,Kir),e(mt,Yir),e(mt,ic),e(ic,Zir),e(ic,Ufe),e(Ufe,edr),e(ic,odr),e(ic,Jfe),e(Jfe,rdr),e(ic,tdr),e(mt,adr),e(mt,Kfe),e(Kfe,sdr),e(mt,ndr),h($w,mt,null),e(Er,ldr),e(Er,Mo),h(Iw,Mo,null),e(Mo,idr),e(Mo,Yfe),e(Yfe,ddr),e(Mo,cdr),e(Mo,ns),e(ns,mdr),e(ns,Zfe),e(Zfe,fdr),e(ns,hdr),e(ns,ehe),e(ehe,gdr),e(ns,udr),e(ns,ohe),e(ohe,pdr),e(ns,_dr),e(Mo,bdr),e(Mo,ke),e(ke,$E),e($E,rhe),e(rhe,vdr),e($E,Tdr),e($E,mN),e(mN,Fdr),e($E,Edr),e(ke,Cdr),e(ke,IE),e(IE,the),e(the,Mdr),e(IE,ydr),e(IE,fN),e(fN,wdr),e(IE,Adr),e(ke,Ldr),e(ke,DE),e(DE,ahe),e(ahe,Bdr),e(DE,xdr),e(DE,hN),e(hN,kdr),e(DE,Rdr),e(ke,Pdr),e(ke,NE),e(NE,she),e(she,Sdr),e(NE,$dr),e(NE,gN),e(gN,Idr),e(NE,Ddr),e(ke,Ndr),e(ke,jE),e(jE,nhe),e(nhe,jdr),e(jE,Odr),e(jE,uN),e(uN,Gdr),e(jE,qdr),e(ke,zdr),e(ke,OE),e(OE,lhe),e(lhe,Xdr),e(OE,Qdr),e(OE,pN),e(pN,Vdr),e(OE,Wdr),e(ke,Hdr),e(ke,GE),e(GE,ihe),e(ihe,Udr),e(GE,Jdr),e(GE,_N),e(_N,Kdr),e(GE,Ydr),e(ke,Zdr),e(ke,qE),e(qE,dhe),e(dhe,ecr),e(qE,ocr),e(qE,bN),e(bN,rcr),e(qE,tcr),e(Mo,acr),e(Mo,che),e(che,scr),e(Mo,ncr),h(Dw,Mo,null),b(d,Pye,_),b(d,dc,_),e(dc,zE),e(zE,mhe),h(Nw,mhe,null),e(dc,lcr),e(dc,fhe),e(fhe,icr),b(d,Sye,_),b(d,Cr,_),h(jw,Cr,null),e(Cr,dcr),e(Cr,cc),e(cc,ccr),e(cc,hhe),e(hhe,mcr),e(cc,fcr),e(cc,ghe),e(ghe,hcr),e(cc,gcr),e(Cr,ucr),e(Cr,Ow),e(Ow,pcr),e(Ow,uhe),e(uhe,_cr),e(Ow,bcr),e(Cr,vcr),e(Cr,ft),h(Gw,ft,null),e(ft,Tcr),e(ft,phe),e(phe,Fcr),e(ft,Ecr),e(ft,mc),e(mc,Ccr),e(mc,_he),e(_he,Mcr),e(mc,ycr),e(mc,bhe),e(bhe,wcr),e(mc,Acr),e(ft,Lcr),e(ft,vhe),e(vhe,Bcr),e(ft,xcr),h(qw,ft,null),e(Cr,kcr),e(Cr,yo),h(zw,yo,null),e(yo,Rcr),e(yo,The),e(The,Pcr),e(yo,Scr),e(yo,ls),e(ls,$cr),e(ls,Fhe),e(Fhe,Icr),e(ls,Dcr),e(ls,Ehe),e(Ehe,Ncr),e(ls,jcr),e(ls,Che),e(Che,Ocr),e(ls,Gcr),e(yo,qcr),e(yo,Re),e(Re,XE),e(XE,Mhe),e(Mhe,zcr),e(XE,Xcr),e(XE,vN),e(vN,Qcr),e(XE,Vcr),e(Re,Wcr),e(Re,QE),e(QE,yhe),e(yhe,Hcr),e(QE,Ucr),e(QE,TN),e(TN,Jcr),e(QE,Kcr),e(Re,Ycr),e(Re,VE),e(VE,whe),e(whe,Zcr),e(VE,emr),e(VE,FN),e(FN,omr),e(VE,rmr),e(Re,tmr),e(Re,WE),e(WE,Ahe),e(Ahe,amr),e(WE,smr),e(WE,EN),e(EN,nmr),e(WE,lmr),e(Re,imr),e(Re,HE),e(HE,Lhe),e(Lhe,dmr),e(HE,cmr),e(HE,CN),e(CN,mmr),e(HE,fmr),e(Re,hmr),e(Re,UE),e(UE,Bhe),e(Bhe,gmr),e(UE,umr),e(UE,MN),e(MN,pmr),e(UE,_mr),e(Re,bmr),e(Re,JE),e(JE,xhe),e(xhe,vmr),e(JE,Tmr),e(JE,yN),e(yN,Fmr),e(JE,Emr),e(Re,Cmr),e(Re,KE),e(KE,khe),e(khe,Mmr),e(KE,ymr),e(KE,wN),e(wN,wmr),e(KE,Amr),e(yo,Lmr),e(yo,Rhe),e(Rhe,Bmr),e(yo,xmr),h(Xw,yo,null),b(d,$ye,_),b(d,fc,_),e(fc,YE),e(YE,Phe),h(Qw,Phe,null),e(fc,kmr),e(fc,She),e(She,Rmr),b(d,Iye,_),b(d,Mr,_),h(Vw,Mr,null),e(Mr,Pmr),e(Mr,hc),e(hc,Smr),e(hc,$he),e($he,$mr),e(hc,Imr),e(hc,Ihe),e(Ihe,Dmr),e(hc,Nmr),e(Mr,jmr),e(Mr,Ww),e(Ww,Omr),e(Ww,Dhe),e(Dhe,Gmr),e(Ww,qmr),e(Mr,zmr),e(Mr,ht),h(Hw,ht,null),e(ht,Xmr),e(ht,Nhe),e(Nhe,Qmr),e(ht,Vmr),e(ht,gc),e(gc,Wmr),e(gc,jhe),e(jhe,Hmr),e(gc,Umr),e(gc,Ohe),e(Ohe,Jmr),e(gc,Kmr),e(ht,Ymr),e(ht,Ghe),e(Ghe,Zmr),e(ht,efr),h(Uw,ht,null),e(Mr,ofr),e(Mr,wo),h(Jw,wo,null),e(wo,rfr),e(wo,qhe),e(qhe,tfr),e(wo,afr),e(wo,is),e(is,sfr),e(is,zhe),e(zhe,nfr),e(is,lfr),e(is,Xhe),e(Xhe,ifr),e(is,dfr),e(is,Qhe),e(Qhe,cfr),e(is,mfr),e(wo,ffr),e(wo,yr),e(yr,ZE),e(ZE,Vhe),e(Vhe,hfr),e(ZE,gfr),e(ZE,AN),e(AN,ufr),e(ZE,pfr),e(yr,_fr),e(yr,e4),e(e4,Whe),e(Whe,bfr),e(e4,vfr),e(e4,LN),e(LN,Tfr),e(e4,Ffr),e(yr,Efr),e(yr,o4),e(o4,Hhe),e(Hhe,Cfr),e(o4,Mfr),e(o4,BN),e(BN,yfr),e(o4,wfr),e(yr,Afr),e(yr,r4),e(r4,Uhe),e(Uhe,Lfr),e(r4,Bfr),e(r4,xN),e(xN,xfr),e(r4,kfr),e(yr,Rfr),e(yr,t4),e(t4,Jhe),e(Jhe,Pfr),e(t4,Sfr),e(t4,kN),e(kN,$fr),e(t4,Ifr),e(yr,Dfr),e(yr,a4),e(a4,Khe),e(Khe,Nfr),e(a4,jfr),e(a4,RN),e(RN,Ofr),e(a4,Gfr),e(wo,qfr),e(wo,Yhe),e(Yhe,zfr),e(wo,Xfr),h(Kw,wo,null),b(d,Dye,_),b(d,uc,_),e(uc,s4),e(s4,Zhe),h(Yw,Zhe,null),e(uc,Qfr),e(uc,ege),e(ege,Vfr),b(d,Nye,_),b(d,wr,_),h(Zw,wr,null),e(wr,Wfr),e(wr,pc),e(pc,Hfr),e(pc,oge),e(oge,Ufr),e(pc,Jfr),e(pc,rge),e(rge,Kfr),e(pc,Yfr),e(wr,Zfr),e(wr,e7),e(e7,ehr),e(e7,tge),e(tge,ohr),e(e7,rhr),e(wr,thr),e(wr,gt),h(o7,gt,null),e(gt,ahr),e(gt,age),e(age,shr),e(gt,nhr),e(gt,_c),e(_c,lhr),e(_c,sge),e(sge,ihr),e(_c,dhr),e(_c,nge),e(nge,chr),e(_c,mhr),e(gt,fhr),e(gt,lge),e(lge,hhr),e(gt,ghr),h(r7,gt,null),e(wr,uhr),e(wr,Ao),h(t7,Ao,null),e(Ao,phr),e(Ao,ige),e(ige,_hr),e(Ao,bhr),e(Ao,ds),e(ds,vhr),e(ds,dge),e(dge,Thr),e(ds,Fhr),e(ds,cge),e(cge,Ehr),e(ds,Chr),e(ds,mge),e(mge,Mhr),e(ds,yhr),e(Ao,whr),e(Ao,Ar),e(Ar,n4),e(n4,fge),e(fge,Ahr),e(n4,Lhr),e(n4,PN),e(PN,Bhr),e(n4,xhr),e(Ar,khr),e(Ar,l4),e(l4,hge),e(hge,Rhr),e(l4,Phr),e(l4,SN),e(SN,Shr),e(l4,$hr),e(Ar,Ihr),e(Ar,i4),e(i4,gge),e(gge,Dhr),e(i4,Nhr),e(i4,$N),e($N,jhr),e(i4,Ohr),e(Ar,Ghr),e(Ar,d4),e(d4,uge),e(uge,qhr),e(d4,zhr),e(d4,IN),e(IN,Xhr),e(d4,Qhr),e(Ar,Vhr),e(Ar,c4),e(c4,pge),e(pge,Whr),e(c4,Hhr),e(c4,DN),e(DN,Uhr),e(c4,Jhr),e(Ar,Khr),e(Ar,m4),e(m4,_ge),e(_ge,Yhr),e(m4,Zhr),e(m4,NN),e(NN,egr),e(m4,ogr),e(Ao,rgr),e(Ao,bge),e(bge,tgr),e(Ao,agr),h(a7,Ao,null),b(d,jye,_),b(d,bc,_),e(bc,f4),e(f4,vge),h(s7,vge,null),e(bc,sgr),e(bc,Tge),e(Tge,ngr),b(d,Oye,_),b(d,Lr,_),h(n7,Lr,null),e(Lr,lgr),e(Lr,vc),e(vc,igr),e(vc,Fge),e(Fge,dgr),e(vc,cgr),e(vc,Ege),e(Ege,mgr),e(vc,fgr),e(Lr,hgr),e(Lr,l7),e(l7,ggr),e(l7,Cge),e(Cge,ugr),e(l7,pgr),e(Lr,_gr),e(Lr,ut),h(i7,ut,null),e(ut,bgr),e(ut,Mge),e(Mge,vgr),e(ut,Tgr),e(ut,Tc),e(Tc,Fgr),e(Tc,yge),e(yge,Egr),e(Tc,Cgr),e(Tc,wge),e(wge,Mgr),e(Tc,ygr),e(ut,wgr),e(ut,Age),e(Age,Agr),e(ut,Lgr),h(d7,ut,null),e(Lr,Bgr),e(Lr,Lo),h(c7,Lo,null),e(Lo,xgr),e(Lo,Lge),e(Lge,kgr),e(Lo,Rgr),e(Lo,cs),e(cs,Pgr),e(cs,Bge),e(Bge,Sgr),e(cs,$gr),e(cs,xge),e(xge,Igr),e(cs,Dgr),e(cs,kge),e(kge,Ngr),e(cs,jgr),e(Lo,Ogr),e(Lo,Rge),e(Rge,h4),e(h4,Pge),e(Pge,Ggr),e(h4,qgr),e(h4,jN),e(jN,zgr),e(h4,Xgr),e(Lo,Qgr),e(Lo,Sge),e(Sge,Vgr),e(Lo,Wgr),h(m7,Lo,null),b(d,Gye,_),b(d,Fc,_),e(Fc,g4),e(g4,$ge),h(f7,$ge,null),e(Fc,Hgr),e(Fc,Ige),e(Ige,Ugr),b(d,qye,_),b(d,Br,_),h(h7,Br,null),e(Br,Jgr),e(Br,Ec),e(Ec,Kgr),e(Ec,Dge),e(Dge,Ygr),e(Ec,Zgr),e(Ec,Nge),e(Nge,eur),e(Ec,our),e(Br,rur),e(Br,g7),e(g7,tur),e(g7,jge),e(jge,aur),e(g7,sur),e(Br,nur),e(Br,pt),h(u7,pt,null),e(pt,lur),e(pt,Oge),e(Oge,iur),e(pt,dur),e(pt,Cc),e(Cc,cur),e(Cc,Gge),e(Gge,mur),e(Cc,fur),e(Cc,qge),e(qge,hur),e(Cc,gur),e(pt,uur),e(pt,zge),e(zge,pur),e(pt,_ur),h(p7,pt,null),e(Br,bur),e(Br,Bo),h(_7,Bo,null),e(Bo,vur),e(Bo,Xge),e(Xge,Tur),e(Bo,Fur),e(Bo,ms),e(ms,Eur),e(ms,Qge),e(Qge,Cur),e(ms,Mur),e(ms,Vge),e(Vge,yur),e(ms,wur),e(ms,Wge),e(Wge,Aur),e(ms,Lur),e(Bo,Bur),e(Bo,b7),e(b7,u4),e(u4,Hge),e(Hge,xur),e(u4,kur),e(u4,ON),e(ON,Rur),e(u4,Pur),e(b7,Sur),e(b7,p4),e(p4,Uge),e(Uge,$ur),e(p4,Iur),e(p4,GN),e(GN,Dur),e(p4,Nur),e(Bo,jur),e(Bo,Jge),e(Jge,Our),e(Bo,Gur),h(v7,Bo,null),b(d,zye,_),b(d,Mc,_),e(Mc,_4),e(_4,Kge),h(T7,Kge,null),e(Mc,qur),e(Mc,Yge),e(Yge,zur),b(d,Xye,_),b(d,xr,_),h(F7,xr,null),e(xr,Xur),e(xr,yc),e(yc,Qur),e(yc,Zge),e(Zge,Vur),e(yc,Wur),e(yc,eue),e(eue,Hur),e(yc,Uur),e(xr,Jur),e(xr,E7),e(E7,Kur),e(E7,oue),e(oue,Yur),e(E7,Zur),e(xr,epr),e(xr,_t),h(C7,_t,null),e(_t,opr),e(_t,rue),e(rue,rpr),e(_t,tpr),e(_t,wc),e(wc,apr),e(wc,tue),e(tue,spr),e(wc,npr),e(wc,aue),e(aue,lpr),e(wc,ipr),e(_t,dpr),e(_t,sue),e(sue,cpr),e(_t,mpr),h(M7,_t,null),e(xr,fpr),e(xr,xo),h(y7,xo,null),e(xo,hpr),e(xo,nue),e(nue,gpr),e(xo,upr),e(xo,fs),e(fs,ppr),e(fs,lue),e(lue,_pr),e(fs,bpr),e(fs,iue),e(iue,vpr),e(fs,Tpr),e(fs,due),e(due,Fpr),e(fs,Epr),e(xo,Cpr),e(xo,cue),e(cue,b4),e(b4,mue),e(mue,Mpr),e(b4,ypr),e(b4,qN),e(qN,wpr),e(b4,Apr),e(xo,Lpr),e(xo,fue),e(fue,Bpr),e(xo,xpr),h(w7,xo,null),Qye=!0},p(d,[_]){const A7={};_&2&&(A7.$$scope={dirty:_,ctx:d}),Pc.$set(A7);const hue={};_&2&&(hue.$$scope={dirty:_,ctx:d}),Qf.$set(hue);const gue={};_&2&&(gue.$$scope={dirty:_,ctx:d}),eh.$set(gue)},i(d){Qye||(g(ge.$$.fragment,d),g(_a.$$.fragment,d),g(TC.$$.fragment,d),g(FC.$$.fragment,d),g(Pc.$$.fragment,d),g(EC.$$.fragment,d),g(CC.$$.fragment,d),g(wC.$$.fragment,d),g(AC.$$.fragment,d),g(LC.$$.fragment,d),g(BC.$$.fragment,d),g(xC.$$.fragment,d),g(PC.$$.fragment,d),g(jC.$$.fragment,d),g(OC.$$.fragment,d),g(GC.$$.fragment,d),g(qC.$$.fragment,d),g(QC.$$.fragment,d),g(Qf.$$.fragment,d),g(JC.$$.fragment,d),g(KC.$$.fragment,d),g(YC.$$.fragment,d),g(o3.$$.fragment,d),g(eh.$$.fragment,d),g(l3.$$.fragment,d),g(i3.$$.fragment,d),g(d3.$$.fragment,d),g(m3.$$.fragment,d),g(f3.$$.fragment,d),g(h3.$$.fragment,d),g(g3.$$.fragment,d),g(u3.$$.fragment,d),g(p3.$$.fragment,d),g(b3.$$.fragment,d),g(v3.$$.fragment,d),g(T3.$$.fragment,d),g(F3.$$.fragment,d),g(E3.$$.fragment,d),g(C3.$$.fragment,d),g(y3.$$.fragment,d),g(w3.$$.fragment,d),g(A3.$$.fragment,d),g(L3.$$.fragment,d),g(B3.$$.fragment,d),g(x3.$$.fragment,d),g(R3.$$.fragment,d),g(P3.$$.fragment,d),g(S3.$$.fragment,d),g($3.$$.fragment,d),g(I3.$$.fragment,d),g(D3.$$.fragment,d),g(j3.$$.fragment,d),g(O3.$$.fragment,d),g(G3.$$.fragment,d),g(q3.$$.fragment,d),g(z3.$$.fragment,d),g(X3.$$.fragment,d),g(V3.$$.fragment,d),g(W3.$$.fragment,d),g(H3.$$.fragment,d),g(U3.$$.fragment,d),g(J3.$$.fragment,d),g(K3.$$.fragment,d),g(Z3.$$.fragment,d),g(eM.$$.fragment,d),g(oM.$$.fragment,d),g(rM.$$.fragment,d),g(tM.$$.fragment,d),g(aM.$$.fragment,d),g(nM.$$.fragment,d),g(lM.$$.fragment,d),g(iM.$$.fragment,d),g(dM.$$.fragment,d),g(cM.$$.fragment,d),g(mM.$$.fragment,d),g(hM.$$.fragment,d),g(gM.$$.fragment,d),g(uM.$$.fragment,d),g(pM.$$.fragment,d),g(_M.$$.fragment,d),g(bM.$$.fragment,d),g(TM.$$.fragment,d),g(FM.$$.fragment,d),g(EM.$$.fragment,d),g(CM.$$.fragment,d),g(MM.$$.fragment,d),g(yM.$$.fragment,d),g(AM.$$.fragment,d),g(LM.$$.fragment,d),g(BM.$$.fragment,d),g(xM.$$.fragment,d),g(kM.$$.fragment,d),g(RM.$$.fragment,d),g(SM.$$.fragment,d),g($M.$$.fragment,d),g(IM.$$.fragment,d),g(DM.$$.fragment,d),g(NM.$$.fragment,d),g(jM.$$.fragment,d),g(GM.$$.fragment,d),g(qM.$$.fragment,d),g(zM.$$.fragment,d),g(XM.$$.fragment,d),g(QM.$$.fragment,d),g(VM.$$.fragment,d),g(HM.$$.fragment,d),g(UM.$$.fragment,d),g(JM.$$.fragment,d),g(KM.$$.fragment,d),g(YM.$$.fragment,d),g(ZM.$$.fragment,d),g(o5.$$.fragment,d),g(r5.$$.fragment,d),g(t5.$$.fragment,d),g(a5.$$.fragment,d),g(s5.$$.fragment,d),g(n5.$$.fragment,d),g(i5.$$.fragment,d),g(d5.$$.fragment,d),g(c5.$$.fragment,d),g(f5.$$.fragment,d),g(h5.$$.fragment,d),g(g5.$$.fragment,d),g(p5.$$.fragment,d),g(_5.$$.fragment,d),g(b5.$$.fragment,d),g(v5.$$.fragment,d),g(T5.$$.fragment,d),g(F5.$$.fragment,d),g(C5.$$.fragment,d),g(M5.$$.fragment,d),g(y5.$$.fragment,d),g(w5.$$.fragment,d),g(A5.$$.fragment,d),g(L5.$$.fragment,d),g(x5.$$.fragment,d),g(k5.$$.fragment,d),g(R5.$$.fragment,d),g(P5.$$.fragment,d),g(S5.$$.fragment,d),g($5.$$.fragment,d),g(D5.$$.fragment,d),g(N5.$$.fragment,d),g(j5.$$.fragment,d),g(O5.$$.fragment,d),g(G5.$$.fragment,d),g(q5.$$.fragment,d),g(X5.$$.fragment,d),g(Q5.$$.fragment,d),g(V5.$$.fragment,d),g(W5.$$.fragment,d),g(H5.$$.fragment,d),g(U5.$$.fragment,d),g(K5.$$.fragment,d),g(Y5.$$.fragment,d),g(Z5.$$.fragment,d),g(ey.$$.fragment,d),g(oy.$$.fragment,d),g(ry.$$.fragment,d),g(ay.$$.fragment,d),g(sy.$$.fragment,d),g(ny.$$.fragment,d),g(ly.$$.fragment,d),g(iy.$$.fragment,d),g(dy.$$.fragment,d),g(my.$$.fragment,d),g(fy.$$.fragment,d),g(hy.$$.fragment,d),g(gy.$$.fragment,d),g(uy.$$.fragment,d),g(py.$$.fragment,d),g(by.$$.fragment,d),g(vy.$$.fragment,d),g(Ty.$$.fragment,d),g(Fy.$$.fragment,d),g(Ey.$$.fragment,d),g(Cy.$$.fragment,d),g(yy.$$.fragment,d),g(wy.$$.fragment,d),g(Ay.$$.fragment,d),g(Ly.$$.fragment,d),g(By.$$.fragment,d),g(xy.$$.fragment,d),g(Ry.$$.fragment,d),g(Py.$$.fragment,d),g(Sy.$$.fragment,d),g($y.$$.fragment,d),g(Iy.$$.fragment,d),g(Dy.$$.fragment,d),g(jy.$$.fragment,d),g(Oy.$$.fragment,d),g(Gy.$$.fragment,d),g(qy.$$.fragment,d),g(zy.$$.fragment,d),g(Xy.$$.fragment,d),g(Vy.$$.fragment,d),g(Wy.$$.fragment,d),g(Hy.$$.fragment,d),g(Uy.$$.fragment,d),g(Jy.$$.fragment,d),g(Ky.$$.fragment,d),g(Zy.$$.fragment,d),g(ew.$$.fragment,d),g(ow.$$.fragment,d),g(rw.$$.fragment,d),g(tw.$$.fragment,d),g(aw.$$.fragment,d),g(nw.$$.fragment,d),g(lw.$$.fragment,d),g(iw.$$.fragment,d),g(dw.$$.fragment,d),g(cw.$$.fragment,d),g(mw.$$.fragment,d),g(hw.$$.fragment,d),g(gw.$$.fragment,d),g(uw.$$.fragment,d),g(pw.$$.fragment,d),g(_w.$$.fragment,d),g(bw.$$.fragment,d),g(Tw.$$.fragment,d),g(Fw.$$.fragment,d),g(Ew.$$.fragment,d),g(Cw.$$.fragment,d),g(Mw.$$.fragment,d),g(yw.$$.fragment,d),g(Aw.$$.fragment,d),g(Lw.$$.fragment,d),g(Bw.$$.fragment,d),g(xw.$$.fragment,d),g(kw.$$.fragment,d),g(Rw.$$.fragment,d),g(Sw.$$.fragment,d),g($w.$$.fragment,d),g(Iw.$$.fragment,d),g(Dw.$$.fragment,d),g(Nw.$$.fragment,d),g(jw.$$.fragment,d),g(Gw.$$.fragment,d),g(qw.$$.fragment,d),g(zw.$$.fragment,d),g(Xw.$$.fragment,d),g(Qw.$$.fragment,d),g(Vw.$$.fragment,d),g(Hw.$$.fragment,d),g(Uw.$$.fragment,d),g(Jw.$$.fragment,d),g(Kw.$$.fragment,d),g(Yw.$$.fragment,d),g(Zw.$$.fragment,d),g(o7.$$.fragment,d),g(r7.$$.fragment,d),g(t7.$$.fragment,d),g(a7.$$.fragment,d),g(s7.$$.fragment,d),g(n7.$$.fragment,d),g(i7.$$.fragment,d),g(d7.$$.fragment,d),g(c7.$$.fragment,d),g(m7.$$.fragment,d),g(f7.$$.fragment,d),g(h7.$$.fragment,d),g(u7.$$.fragment,d),g(p7.$$.fragment,d),g(_7.$$.fragment,d),g(v7.$$.fragment,d),g(T7.$$.fragment,d),g(F7.$$.fragment,d),g(C7.$$.fragment,d),g(M7.$$.fragment,d),g(y7.$$.fragment,d),g(w7.$$.fragment,d),Qye=!0)},o(d){u(ge.$$.fragment,d),u(_a.$$.fragment,d),u(TC.$$.fragment,d),u(FC.$$.fragment,d),u(Pc.$$.fragment,d),u(EC.$$.fragment,d),u(CC.$$.fragment,d),u(wC.$$.fragment,d),u(AC.$$.fragment,d),u(LC.$$.fragment,d),u(BC.$$.fragment,d),u(xC.$$.fragment,d),u(PC.$$.fragment,d),u(jC.$$.fragment,d),u(OC.$$.fragment,d),u(GC.$$.fragment,d),u(qC.$$.fragment,d),u(QC.$$.fragment,d),u(Qf.$$.fragment,d),u(JC.$$.fragment,d),u(KC.$$.fragment,d),u(YC.$$.fragment,d),u(o3.$$.fragment,d),u(eh.$$.fragment,d),u(l3.$$.fragment,d),u(i3.$$.fragment,d),u(d3.$$.fragment,d),u(m3.$$.fragment,d),u(f3.$$.fragment,d),u(h3.$$.fragment,d),u(g3.$$.fragment,d),u(u3.$$.fragment,d),u(p3.$$.fragment,d),u(b3.$$.fragment,d),u(v3.$$.fragment,d),u(T3.$$.fragment,d),u(F3.$$.fragment,d),u(E3.$$.fragment,d),u(C3.$$.fragment,d),u(y3.$$.fragment,d),u(w3.$$.fragment,d),u(A3.$$.fragment,d),u(L3.$$.fragment,d),u(B3.$$.fragment,d),u(x3.$$.fragment,d),u(R3.$$.fragment,d),u(P3.$$.fragment,d),u(S3.$$.fragment,d),u($3.$$.fragment,d),u(I3.$$.fragment,d),u(D3.$$.fragment,d),u(j3.$$.fragment,d),u(O3.$$.fragment,d),u(G3.$$.fragment,d),u(q3.$$.fragment,d),u(z3.$$.fragment,d),u(X3.$$.fragment,d),u(V3.$$.fragment,d),u(W3.$$.fragment,d),u(H3.$$.fragment,d),u(U3.$$.fragment,d),u(J3.$$.fragment,d),u(K3.$$.fragment,d),u(Z3.$$.fragment,d),u(eM.$$.fragment,d),u(oM.$$.fragment,d),u(rM.$$.fragment,d),u(tM.$$.fragment,d),u(aM.$$.fragment,d),u(nM.$$.fragment,d),u(lM.$$.fragment,d),u(iM.$$.fragment,d),u(dM.$$.fragment,d),u(cM.$$.fragment,d),u(mM.$$.fragment,d),u(hM.$$.fragment,d),u(gM.$$.fragment,d),u(uM.$$.fragment,d),u(pM.$$.fragment,d),u(_M.$$.fragment,d),u(bM.$$.fragment,d),u(TM.$$.fragment,d),u(FM.$$.fragment,d),u(EM.$$.fragment,d),u(CM.$$.fragment,d),u(MM.$$.fragment,d),u(yM.$$.fragment,d),u(AM.$$.fragment,d),u(LM.$$.fragment,d),u(BM.$$.fragment,d),u(xM.$$.fragment,d),u(kM.$$.fragment,d),u(RM.$$.fragment,d),u(SM.$$.fragment,d),u($M.$$.fragment,d),u(IM.$$.fragment,d),u(DM.$$.fragment,d),u(NM.$$.fragment,d),u(jM.$$.fragment,d),u(GM.$$.fragment,d),u(qM.$$.fragment,d),u(zM.$$.fragment,d),u(XM.$$.fragment,d),u(QM.$$.fragment,d),u(VM.$$.fragment,d),u(HM.$$.fragment,d),u(UM.$$.fragment,d),u(JM.$$.fragment,d),u(KM.$$.fragment,d),u(YM.$$.fragment,d),u(ZM.$$.fragment,d),u(o5.$$.fragment,d),u(r5.$$.fragment,d),u(t5.$$.fragment,d),u(a5.$$.fragment,d),u(s5.$$.fragment,d),u(n5.$$.fragment,d),u(i5.$$.fragment,d),u(d5.$$.fragment,d),u(c5.$$.fragment,d),u(f5.$$.fragment,d),u(h5.$$.fragment,d),u(g5.$$.fragment,d),u(p5.$$.fragment,d),u(_5.$$.fragment,d),u(b5.$$.fragment,d),u(v5.$$.fragment,d),u(T5.$$.fragment,d),u(F5.$$.fragment,d),u(C5.$$.fragment,d),u(M5.$$.fragment,d),u(y5.$$.fragment,d),u(w5.$$.fragment,d),u(A5.$$.fragment,d),u(L5.$$.fragment,d),u(x5.$$.fragment,d),u(k5.$$.fragment,d),u(R5.$$.fragment,d),u(P5.$$.fragment,d),u(S5.$$.fragment,d),u($5.$$.fragment,d),u(D5.$$.fragment,d),u(N5.$$.fragment,d),u(j5.$$.fragment,d),u(O5.$$.fragment,d),u(G5.$$.fragment,d),u(q5.$$.fragment,d),u(X5.$$.fragment,d),u(Q5.$$.fragment,d),u(V5.$$.fragment,d),u(W5.$$.fragment,d),u(H5.$$.fragment,d),u(U5.$$.fragment,d),u(K5.$$.fragment,d),u(Y5.$$.fragment,d),u(Z5.$$.fragment,d),u(ey.$$.fragment,d),u(oy.$$.fragment,d),u(ry.$$.fragment,d),u(ay.$$.fragment,d),u(sy.$$.fragment,d),u(ny.$$.fragment,d),u(ly.$$.fragment,d),u(iy.$$.fragment,d),u(dy.$$.fragment,d),u(my.$$.fragment,d),u(fy.$$.fragment,d),u(hy.$$.fragment,d),u(gy.$$.fragment,d),u(uy.$$.fragment,d),u(py.$$.fragment,d),u(by.$$.fragment,d),u(vy.$$.fragment,d),u(Ty.$$.fragment,d),u(Fy.$$.fragment,d),u(Ey.$$.fragment,d),u(Cy.$$.fragment,d),u(yy.$$.fragment,d),u(wy.$$.fragment,d),u(Ay.$$.fragment,d),u(Ly.$$.fragment,d),u(By.$$.fragment,d),u(xy.$$.fragment,d),u(Ry.$$.fragment,d),u(Py.$$.fragment,d),u(Sy.$$.fragment,d),u($y.$$.fragment,d),u(Iy.$$.fragment,d),u(Dy.$$.fragment,d),u(jy.$$.fragment,d),u(Oy.$$.fragment,d),u(Gy.$$.fragment,d),u(qy.$$.fragment,d),u(zy.$$.fragment,d),u(Xy.$$.fragment,d),u(Vy.$$.fragment,d),u(Wy.$$.fragment,d),u(Hy.$$.fragment,d),u(Uy.$$.fragment,d),u(Jy.$$.fragment,d),u(Ky.$$.fragment,d),u(Zy.$$.fragment,d),u(ew.$$.fragment,d),u(ow.$$.fragment,d),u(rw.$$.fragment,d),u(tw.$$.fragment,d),u(aw.$$.fragment,d),u(nw.$$.fragment,d),u(lw.$$.fragment,d),u(iw.$$.fragment,d),u(dw.$$.fragment,d),u(cw.$$.fragment,d),u(mw.$$.fragment,d),u(hw.$$.fragment,d),u(gw.$$.fragment,d),u(uw.$$.fragment,d),u(pw.$$.fragment,d),u(_w.$$.fragment,d),u(bw.$$.fragment,d),u(Tw.$$.fragment,d),u(Fw.$$.fragment,d),u(Ew.$$.fragment,d),u(Cw.$$.fragment,d),u(Mw.$$.fragment,d),u(yw.$$.fragment,d),u(Aw.$$.fragment,d),u(Lw.$$.fragment,d),u(Bw.$$.fragment,d),u(xw.$$.fragment,d),u(kw.$$.fragment,d),u(Rw.$$.fragment,d),u(Sw.$$.fragment,d),u($w.$$.fragment,d),u(Iw.$$.fragment,d),u(Dw.$$.fragment,d),u(Nw.$$.fragment,d),u(jw.$$.fragment,d),u(Gw.$$.fragment,d),u(qw.$$.fragment,d),u(zw.$$.fragment,d),u(Xw.$$.fragment,d),u(Qw.$$.fragment,d),u(Vw.$$.fragment,d),u(Hw.$$.fragment,d),u(Uw.$$.fragment,d),u(Jw.$$.fragment,d),u(Kw.$$.fragment,d),u(Yw.$$.fragment,d),u(Zw.$$.fragment,d),u(o7.$$.fragment,d),u(r7.$$.fragment,d),u(t7.$$.fragment,d),u(a7.$$.fragment,d),u(s7.$$.fragment,d),u(n7.$$.fragment,d),u(i7.$$.fragment,d),u(d7.$$.fragment,d),u(c7.$$.fragment,d),u(m7.$$.fragment,d),u(f7.$$.fragment,d),u(h7.$$.fragment,d),u(u7.$$.fragment,d),u(p7.$$.fragment,d),u(_7.$$.fragment,d),u(v7.$$.fragment,d),u(T7.$$.fragment,d),u(F7.$$.fragment,d),u(C7.$$.fragment,d),u(M7.$$.fragment,d),u(y7.$$.fragment,d),u(w7.$$.fragment,d),Qye=!1},d(d){t(re),d&&t(Pe),d&&t(fe),p(ge),d&&t(Lc),d&&t(Ot),d&&t(Be),d&&t(ao),d&&t(xc),p(_a,d),d&&t(so),d&&t(pe),d&&t(Io),d&&t(ba),d&&t(a5e),d&&t(Jl),p(TC),d&&t(s5e),d&&t(_s),d&&t(n5e),p(FC,d),d&&t(l5e),d&&t(y0),d&&t(i5e),p(Pc,d),d&&t(d5e),d&&t(Kl),p(EC),d&&t(c5e),d&&t(Do),p(CC),p(wC),p(AC),p(LC),d&&t(m5e),d&&t(Zl),p(BC),d&&t(f5e),d&&t(No),p(xC),p(PC),p(jC),p(OC),d&&t(h5e),d&&t(ti),p(GC),d&&t(g5e),d&&t(Dt),p(qC),p(QC),p(Qf),p(JC),d&&t(u5e),d&&t(li),p(KC),d&&t(p5e),d&&t(Nt),p(YC),p(o3),p(eh),p(l3),d&&t(_5e),d&&t(mi),p(i3),d&&t(b5e),d&&t(jo),p(d3),p(m3),p(f3),p(h3),p(g3),d&&t(v5e),d&&t(gi),p(u3),d&&t(T5e),d&&t(Oo),p(p3),p(b3),p(v3),p(T3),p(F3),d&&t(F5e),d&&t(_i),p(E3),d&&t(E5e),d&&t(Go),p(C3),p(y3),p(w3),p(A3),p(L3),d&&t(C5e),d&&t(Ti),p(B3),d&&t(M5e),d&&t(qo),p(x3),p(R3),p(P3),p(S3),p($3),d&&t(y5e),d&&t(Ci),p(I3),d&&t(w5e),d&&t(zo),p(D3),p(j3),p(O3),p(G3),p(q3),d&&t(A5e),d&&t(wi),p(z3),d&&t(L5e),d&&t(Xo),p(X3),p(V3),p(W3),p(H3),p(U3),d&&t(B5e),d&&t(Bi),p(J3),d&&t(x5e),d&&t(Qo),p(K3),p(Z3),p(eM),p(oM),p(rM),d&&t(k5e),d&&t(Ri),p(tM),d&&t(R5e),d&&t(Vo),p(aM),p(nM),p(lM),p(iM),p(dM),d&&t(P5e),d&&t($i),p(cM),d&&t(S5e),d&&t(Wo),p(mM),p(hM),p(gM),p(uM),p(pM),d&&t($5e),d&&t(Ni),p(_M),d&&t(I5e),d&&t(Ho),p(bM),p(TM),p(FM),p(EM),p(CM),d&&t(D5e),d&&t(Gi),p(MM),d&&t(N5e),d&&t(Uo),p(yM),p(AM),p(LM),p(BM),p(xM),d&&t(j5e),d&&t(Xi),p(kM),d&&t(O5e),d&&t(Jo),p(RM),p(SM),p($M),p(IM),p(DM),d&&t(G5e),d&&t(Wi),p(NM),d&&t(q5e),d&&t(Yo),p(jM),p(GM),p(qM),p(zM),p(XM),d&&t(z5e),d&&t(Ji),p(QM),d&&t(X5e),d&&t(Zo),p(VM),p(HM),p(UM),p(JM),p(KM),d&&t(Q5e),d&&t(Zi),p(YM),d&&t(V5e),d&&t(or),p(ZM),p(o5),p(r5),p(t5),p(a5),d&&t(W5e),d&&t(rd),p(s5),d&&t(H5e),d&&t(tr),p(n5),p(i5),p(d5),p(c5),p(f5),d&&t(U5e),d&&t(sd),p(h5),d&&t(J5e),d&&t(ar),p(g5),p(p5),p(_5),p(b5),p(v5),d&&t(K5e),d&&t(id),p(T5),d&&t(Y5e),d&&t(sr),p(F5),p(C5),p(M5),p(y5),p(w5),d&&t(Z5e),d&&t(md),p(A5),d&&t(eye),d&&t(nr),p(L5),p(x5),p(k5),p(R5),p(P5),d&&t(oye),d&&t(gd),p(S5),d&&t(rye),d&&t(lr),p($5),p(D5),p(N5),p(j5),p(O5),d&&t(tye),d&&t(_d),p(G5),d&&t(aye),d&&t(ir),p(q5),p(X5),p(Q5),p(V5),p(W5),d&&t(sye),d&&t(Td),p(H5),d&&t(nye),d&&t(dr),p(U5),p(K5),p(Y5),p(Z5),p(ey),d&&t(lye),d&&t(Cd),p(oy),d&&t(iye),d&&t(cr),p(ry),p(ay),p(sy),p(ny),p(ly),d&&t(dye),d&&t(wd),p(iy),d&&t(cye),d&&t(mr),p(dy),p(my),p(fy),p(hy),p(gy),d&&t(mye),d&&t(Bd),p(uy),d&&t(fye),d&&t(fr),p(py),p(by),p(vy),p(Ty),p(Fy),d&&t(hye),d&&t(Rd),p(Ey),d&&t(gye),d&&t(hr),p(Cy),p(yy),p(wy),p(Ay),p(Ly),d&&t(uye),d&&t($d),p(By),d&&t(pye),d&&t(gr),p(xy),p(Ry),p(Py),p(Sy),p($y),d&&t(_ye),d&&t(Nd),p(Iy),d&&t(bye),d&&t(ur),p(Dy),p(jy),p(Oy),p(Gy),p(qy),d&&t(vye),d&&t(Gd),p(zy),d&&t(Tye),d&&t(pr),p(Xy),p(Vy),p(Wy),p(Hy),p(Uy),d&&t(Fye),d&&t(Xd),p(Jy),d&&t(Eye),d&&t(_r),p(Ky),p(Zy),p(ew),p(ow),p(rw),d&&t(Cye),d&&t(Wd),p(tw),d&&t(Mye),d&&t(br),p(aw),p(nw),p(lw),p(iw),p(dw),d&&t(yye),d&&t(Kd),p(cw),d&&t(wye),d&&t(vr),p(mw),p(hw),p(gw),p(uw),p(pw),d&&t(Aye),d&&t(ec),p(_w),d&&t(Lye),d&&t(Tr),p(bw),p(Tw),p(Fw),p(Ew),p(Cw),d&&t(Bye),d&&t(tc),p(Mw),d&&t(xye),d&&t(Fr),p(yw),p(Aw),p(Lw),p(Bw),p(xw),d&&t(kye),d&&t(nc),p(kw),d&&t(Rye),d&&t(Er),p(Rw),p(Sw),p($w),p(Iw),p(Dw),d&&t(Pye),d&&t(dc),p(Nw),d&&t(Sye),d&&t(Cr),p(jw),p(Gw),p(qw),p(zw),p(Xw),d&&t($ye),d&&t(fc),p(Qw),d&&t(Iye),d&&t(Mr),p(Vw),p(Hw),p(Uw),p(Jw),p(Kw),d&&t(Dye),d&&t(uc),p(Yw),d&&t(Nye),d&&t(wr),p(Zw),p(o7),p(r7),p(t7),p(a7),d&&t(jye),d&&t(bc),p(s7),d&&t(Oye),d&&t(Lr),p(n7),p(i7),p(d7),p(c7),p(m7),d&&t(Gye),d&&t(Fc),p(f7),d&&t(qye),d&&t(Br),p(h7),p(u7),p(p7),p(_7),p(v7),d&&t(zye),d&&t(Mc),p(T7),d&&t(Xye),d&&t(xr),p(F7),p(C7),p(M7),p(y7),p(w7)}}}const GKr={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function qKr(ql,re,Pe){let{fw:fe}=re;return ql.$$set=ue=>{"fw"in ue&&Pe(0,fe=ue.fw)},[fe]}class UKr extends PKr{constructor(re){super();SKr(this,re,qKr,OKr,$Kr,{fw:0})}}export{UKr as default,GKr as metadata};
