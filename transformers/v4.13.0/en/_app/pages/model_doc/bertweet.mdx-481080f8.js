import{S as zs,i as Bs,s as qs,e as n,k as o,w as u,t as i,M as Ls,c as a,d as s,m as l,a as r,x as _,h as p,b as c,F as e,g as h,y as w,L as As,q as k,o as v,B as b}from"../../chunks/vendor-68651a14.js";import{D as N}from"../../chunks/Docstring-d699e906.js";import{C as Rs}from"../../chunks/CodeBlock-b98730f5.js";import{I as Gt}from"../../chunks/IconCopyLink-8ff17449.js";import"../../chunks/CopyButton-e88c769b.js";function Ds(Je){let E,se,g,T,me,C,Ge,fe,Ke,qe,z,q,de,F,et,he,tt,Le,L,st,M,nt,at,Ae,ne,rt,Re,ae,ge,ot,De,re,it,Pe,O,xe,$,lt,S,pt,ct,V,mt,ft,je,B,A,ue,U,dt,_e,ht,Ie,m,W,gt,we,ut,_t,H,wt,oe,kt,vt,bt,R,X,Tt,ke,Et,$t,y,Q,yt,ve,zt,Bt,Y,ie,qt,be,Lt,At,le,Rt,Te,Dt,Pt,D,Z,xt,Ee,jt,It,P,J,Nt,$e,Ct,Ft,x,G,Mt,K,Ot,ye,St,Vt,Ut,j,ee,Wt,ze,Ht,Xt,I,te,Qt,Be,Yt,Ne;return C=new Gt({}),F=new Gt({}),O=new Rs({props:{code:`import torch
from transformers import AutoModel, AutoTokenizer 

bertweet = AutoModel.from_pretrained("vinai/bertweet-base")

# For transformers v4.x+: 
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base", use_fast=False)

# For transformers v3.x: 
# tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")

# INPUT TWEET IS ALREADY NORMALIZED!
line = "SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:"

input_ids = torch.tensor([tokenizer.encode(line)])

with torch.no_grad():
    features = bertweet(input_ids)  # Models outputs are now tuples

# With TensorFlow 2.0+:
# from transformers import TFAutoModel
# bertweet = TFAutoModel.from_pretrained("vinai/bertweet-base"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">import</span> torch</span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer </span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">bertweet = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># For transformers v4.x+: </span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>, use_fast=<span class="hljs-literal">False</span>)</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># For transformers v3.x: </span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/bertweet-base&quot;)</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># INPUT TWEET IS ALREADY NORMALIZED!</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">line = <span class="hljs-string">&quot;SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:&quot;</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python">input_ids = torch.tensor([tokenizer.encode(line)])</span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-keyword">with</span> torch.no_grad():</span>
<span class="hljs-meta">...</span> <span class="language-python">    features = bertweet(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span></span>

<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># With TensorFlow 2.0+:</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># from transformers import TFAutoModel</span></span>
<span class="hljs-meta">&gt;&gt;&gt;</span> <span class="language-python"><span class="hljs-comment"># bertweet = TFAutoModel.from_pretrained(&quot;vinai/bertweet-base&quot;)</span></span>`}}),U=new Gt({}),W=new N({props:{name:"class transformers.BertweetTokenizer",anchor:"transformers.BertweetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"normalization",val:" = False"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bertweet/tokenization_bertweet.py#L68",parametersDescription:[{anchor:"transformers.BertweetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertweetTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.BertweetTokenizer.normalization",description:`<strong>normalization</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply a normalization preprocess.`,name:"normalization"},{anchor:"transformers.BertweetTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.`,name:"bos_token"}]}}),X=new N({props:{name:"add_from_file",anchor:"transformers.BertweetTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bertweet/tokenization_bertweet.py#L408"}}),Q=new N({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bertweet/tokenization_bertweet.py#L177",parametersDescription:[{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Z=new N({props:{name:"convert_tokens_to_string",anchor:"transformers.BertweetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bertweet/tokenization_bertweet.py#L378"}}),J=new N({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bertweet/tokenization_bertweet.py#L231",parametersDescription:[{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),G=new N({props:{name:"get_special_tokens_mask",anchor:"transformers.BertweetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bertweet/tokenization_bertweet.py#L203",parametersDescription:[{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ee=new N({props:{name:"normalizeToken",anchor:"transformers.BertweetTokenizer.normalizeToken",parameters:[{name:"token",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bertweet/tokenization_bertweet.py#L351"}}),te=new N({props:{name:"normalizeTweet",anchor:"transformers.BertweetTokenizer.normalizeTweet",parameters:[{name:"tweet",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/bertweet/tokenization_bertweet.py#L317"}}),{c(){E=n("meta"),se=o(),g=n("h1"),T=n("a"),me=n("span"),u(C.$$.fragment),Ge=o(),fe=n("span"),Ke=i("BERTweet"),qe=o(),z=n("h2"),q=n("a"),de=n("span"),u(F.$$.fragment),et=o(),he=n("span"),tt=i("Overview"),Le=o(),L=n("p"),st=i("The BERTweet model was proposed in "),M=n("a"),nt=i("BERTweet: A pre-trained language model for English Tweets"),at=i(" by Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen."),Ae=o(),ne=n("p"),rt=i("The abstract from the paper is the following:"),Re=o(),ae=n("p"),ge=n("em"),ot=i(`We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having
the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et
al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al.,
2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks:
Part-of-speech tagging, Named-entity recognition and text classification.`),De=o(),re=n("p"),it=i("Example of use:"),Pe=o(),u(O.$$.fragment),xe=o(),$=n("p"),lt=i("This model was contributed by "),S=n("a"),pt=i("dqnguyen"),ct=i(". The original code can be found "),V=n("a"),mt=i("here"),ft=i("."),je=o(),B=n("h2"),A=n("a"),ue=n("span"),u(U.$$.fragment),dt=o(),_e=n("span"),ht=i("BertweetTokenizer"),Ie=o(),m=n("div"),u(W.$$.fragment),gt=o(),we=n("p"),ut=i("Constructs a BERTweet tokenizer, using Byte-Pair-Encoding."),_t=o(),H=n("p"),wt=i("This tokenizer inherits from "),oe=n("a"),kt=i("PreTrainedTokenizer"),vt=i(` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),bt=o(),R=n("div"),u(X.$$.fragment),Tt=o(),ke=n("p"),Et=i("Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),$t=o(),y=n("div"),u(Q.$$.fragment),yt=o(),ve=n("p"),zt=i(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`),Bt=o(),Y=n("ul"),ie=n("li"),qt=i("single sequence: "),be=n("code"),Lt=i("<s> X </s>"),At=o(),le=n("li"),Rt=i("pair of sequences: "),Te=n("code"),Dt=i("<s> A </s></s> B </s>"),Pt=o(),D=n("div"),u(Z.$$.fragment),xt=o(),Ee=n("p"),jt=i("Converts a sequence of tokens (string) in a single string."),It=o(),P=n("div"),u(J.$$.fragment),Nt=o(),$e=n("p"),Ct=i(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`),Ft=o(),x=n("div"),u(G.$$.fragment),Mt=o(),K=n("p"),Ot=i(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),ye=n("code"),St=i("prepare_for_model"),Vt=i(" method."),Ut=o(),j=n("div"),u(ee.$$.fragment),Wt=o(),ze=n("p"),Ht=i("Normalize tokens in a Tweet"),Xt=o(),I=n("div"),u(te.$$.fragment),Qt=o(),Be=n("p"),Yt=i("Normalize a raw Tweet"),this.h()},l(t){const f=Ls('[data-svelte="svelte-1phssyn"]',document.head);E=a(f,"META",{name:!0,content:!0}),f.forEach(s),se=l(t),g=a(t,"H1",{class:!0});var Ce=r(g);T=a(Ce,"A",{id:!0,class:!0,href:!0});var Kt=r(T);me=a(Kt,"SPAN",{});var es=r(me);_(C.$$.fragment,es),es.forEach(s),Kt.forEach(s),Ge=l(Ce),fe=a(Ce,"SPAN",{});var ts=r(fe);Ke=p(ts,"BERTweet"),ts.forEach(s),Ce.forEach(s),qe=l(t),z=a(t,"H2",{class:!0});var Fe=r(z);q=a(Fe,"A",{id:!0,class:!0,href:!0});var ss=r(q);de=a(ss,"SPAN",{});var ns=r(de);_(F.$$.fragment,ns),ns.forEach(s),ss.forEach(s),et=l(Fe),he=a(Fe,"SPAN",{});var as=r(he);tt=p(as,"Overview"),as.forEach(s),Fe.forEach(s),Le=l(t),L=a(t,"P",{});var Me=r(L);st=p(Me,"The BERTweet model was proposed in "),M=a(Me,"A",{href:!0,rel:!0});var rs=r(M);nt=p(rs,"BERTweet: A pre-trained language model for English Tweets"),rs.forEach(s),at=p(Me," by Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen."),Me.forEach(s),Ae=l(t),ne=a(t,"P",{});var os=r(ne);rt=p(os,"The abstract from the paper is the following:"),os.forEach(s),Re=l(t),ae=a(t,"P",{});var is=r(ae);ge=a(is,"EM",{});var ls=r(ge);ot=p(ls,`We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having
the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et
al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al.,
2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks:
Part-of-speech tagging, Named-entity recognition and text classification.`),ls.forEach(s),is.forEach(s),De=l(t),re=a(t,"P",{});var ps=r(re);it=p(ps,"Example of use:"),ps.forEach(s),Pe=l(t),_(O.$$.fragment,t),xe=l(t),$=a(t,"P",{});var pe=r($);lt=p(pe,"This model was contributed by "),S=a(pe,"A",{href:!0,rel:!0});var cs=r(S);pt=p(cs,"dqnguyen"),cs.forEach(s),ct=p(pe,". The original code can be found "),V=a(pe,"A",{href:!0,rel:!0});var ms=r(V);mt=p(ms,"here"),ms.forEach(s),ft=p(pe,"."),pe.forEach(s),je=l(t),B=a(t,"H2",{class:!0});var Oe=r(B);A=a(Oe,"A",{id:!0,class:!0,href:!0});var fs=r(A);ue=a(fs,"SPAN",{});var ds=r(ue);_(U.$$.fragment,ds),ds.forEach(s),fs.forEach(s),dt=l(Oe),_e=a(Oe,"SPAN",{});var hs=r(_e);ht=p(hs,"BertweetTokenizer"),hs.forEach(s),Oe.forEach(s),Ie=l(t),m=a(t,"DIV",{class:!0});var d=r(m);_(W.$$.fragment,d),gt=l(d),we=a(d,"P",{});var gs=r(we);ut=p(gs,"Constructs a BERTweet tokenizer, using Byte-Pair-Encoding."),gs.forEach(s),_t=l(d),H=a(d,"P",{});var Se=r(H);wt=p(Se,"This tokenizer inherits from "),oe=a(Se,"A",{href:!0});var us=r(oe);kt=p(us,"PreTrainedTokenizer"),us.forEach(s),vt=p(Se,` which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.`),Se.forEach(s),bt=l(d),R=a(d,"DIV",{class:!0});var Ve=r(R);_(X.$$.fragment,Ve),Tt=l(Ve),ke=a(Ve,"P",{});var _s=r(ke);Et=p(_s,"Loads a pre-existing dictionary from a text file and adds its symbols to this instance."),_s.forEach(s),Ve.forEach(s),$t=l(d),y=a(d,"DIV",{class:!0});var ce=r(y);_(Q.$$.fragment,ce),yt=l(ce),ve=a(ce,"P",{});var ws=r(ve);zt=p(ws,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`),ws.forEach(s),Bt=l(ce),Y=a(ce,"UL",{});var Ue=r(Y);ie=a(Ue,"LI",{});var Zt=r(ie);qt=p(Zt,"single sequence: "),be=a(Zt,"CODE",{});var ks=r(be);Lt=p(ks,"<s> X </s>"),ks.forEach(s),Zt.forEach(s),At=l(Ue),le=a(Ue,"LI",{});var Jt=r(le);Rt=p(Jt,"pair of sequences: "),Te=a(Jt,"CODE",{});var vs=r(Te);Dt=p(vs,"<s> A </s></s> B </s>"),vs.forEach(s),Jt.forEach(s),Ue.forEach(s),ce.forEach(s),Pt=l(d),D=a(d,"DIV",{class:!0});var We=r(D);_(Z.$$.fragment,We),xt=l(We),Ee=a(We,"P",{});var bs=r(Ee);jt=p(bs,"Converts a sequence of tokens (string) in a single string."),bs.forEach(s),We.forEach(s),It=l(d),P=a(d,"DIV",{class:!0});var He=r(P);_(J.$$.fragment,He),Nt=l(He),$e=a(He,"P",{});var Ts=r($e);Ct=p(Ts,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`),Ts.forEach(s),He.forEach(s),Ft=l(d),x=a(d,"DIV",{class:!0});var Xe=r(x);_(G.$$.fragment,Xe),Mt=l(Xe),K=a(Xe,"P",{});var Qe=r(K);Ot=p(Qe,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),ye=a(Qe,"CODE",{});var Es=r(ye);St=p(Es,"prepare_for_model"),Es.forEach(s),Vt=p(Qe," method."),Qe.forEach(s),Xe.forEach(s),Ut=l(d),j=a(d,"DIV",{class:!0});var Ye=r(j);_(ee.$$.fragment,Ye),Wt=l(Ye),ze=a(Ye,"P",{});var $s=r(ze);Ht=p($s,"Normalize tokens in a Tweet"),$s.forEach(s),Ye.forEach(s),Xt=l(d),I=a(d,"DIV",{class:!0});var Ze=r(I);_(te.$$.fragment,Ze),Qt=l(Ze),Be=a(Ze,"P",{});var ys=r(Be);Yt=p(ys,"Normalize a raw Tweet"),ys.forEach(s),Ze.forEach(s),d.forEach(s),this.h()},h(){c(E,"name","hf:doc:metadata"),c(E,"content",JSON.stringify(Ps)),c(T,"id","bertweet"),c(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T,"href","#bertweet"),c(g,"class","relative group"),c(q,"id","overview"),c(q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(q,"href","#overview"),c(z,"class","relative group"),c(M,"href","https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf"),c(M,"rel","nofollow"),c(S,"href","https://huggingface.co/dqnguyen"),c(S,"rel","nofollow"),c(V,"href","https://github.com/VinAIResearch/BERTweet"),c(V,"rel","nofollow"),c(A,"id","transformers.BertweetTokenizer"),c(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(A,"href","#transformers.BertweetTokenizer"),c(B,"class","relative group"),c(oe,"href","/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(R,"class","docstring"),c(y,"class","docstring"),c(D,"class","docstring"),c(P,"class","docstring"),c(x,"class","docstring"),c(j,"class","docstring"),c(I,"class","docstring"),c(m,"class","docstring")},m(t,f){e(document.head,E),h(t,se,f),h(t,g,f),e(g,T),e(T,me),w(C,me,null),e(g,Ge),e(g,fe),e(fe,Ke),h(t,qe,f),h(t,z,f),e(z,q),e(q,de),w(F,de,null),e(z,et),e(z,he),e(he,tt),h(t,Le,f),h(t,L,f),e(L,st),e(L,M),e(M,nt),e(L,at),h(t,Ae,f),h(t,ne,f),e(ne,rt),h(t,Re,f),h(t,ae,f),e(ae,ge),e(ge,ot),h(t,De,f),h(t,re,f),e(re,it),h(t,Pe,f),w(O,t,f),h(t,xe,f),h(t,$,f),e($,lt),e($,S),e(S,pt),e($,ct),e($,V),e(V,mt),e($,ft),h(t,je,f),h(t,B,f),e(B,A),e(A,ue),w(U,ue,null),e(B,dt),e(B,_e),e(_e,ht),h(t,Ie,f),h(t,m,f),w(W,m,null),e(m,gt),e(m,we),e(we,ut),e(m,_t),e(m,H),e(H,wt),e(H,oe),e(oe,kt),e(H,vt),e(m,bt),e(m,R),w(X,R,null),e(R,Tt),e(R,ke),e(ke,Et),e(m,$t),e(m,y),w(Q,y,null),e(y,yt),e(y,ve),e(ve,zt),e(y,Bt),e(y,Y),e(Y,ie),e(ie,qt),e(ie,be),e(be,Lt),e(Y,At),e(Y,le),e(le,Rt),e(le,Te),e(Te,Dt),e(m,Pt),e(m,D),w(Z,D,null),e(D,xt),e(D,Ee),e(Ee,jt),e(m,It),e(m,P),w(J,P,null),e(P,Nt),e(P,$e),e($e,Ct),e(m,Ft),e(m,x),w(G,x,null),e(x,Mt),e(x,K),e(K,Ot),e(K,ye),e(ye,St),e(K,Vt),e(m,Ut),e(m,j),w(ee,j,null),e(j,Wt),e(j,ze),e(ze,Ht),e(m,Xt),e(m,I),w(te,I,null),e(I,Qt),e(I,Be),e(Be,Yt),Ne=!0},p:As,i(t){Ne||(k(C.$$.fragment,t),k(F.$$.fragment,t),k(O.$$.fragment,t),k(U.$$.fragment,t),k(W.$$.fragment,t),k(X.$$.fragment,t),k(Q.$$.fragment,t),k(Z.$$.fragment,t),k(J.$$.fragment,t),k(G.$$.fragment,t),k(ee.$$.fragment,t),k(te.$$.fragment,t),Ne=!0)},o(t){v(C.$$.fragment,t),v(F.$$.fragment,t),v(O.$$.fragment,t),v(U.$$.fragment,t),v(W.$$.fragment,t),v(X.$$.fragment,t),v(Q.$$.fragment,t),v(Z.$$.fragment,t),v(J.$$.fragment,t),v(G.$$.fragment,t),v(ee.$$.fragment,t),v(te.$$.fragment,t),Ne=!1},d(t){s(E),t&&s(se),t&&s(g),b(C),t&&s(qe),t&&s(z),b(F),t&&s(Le),t&&s(L),t&&s(Ae),t&&s(ne),t&&s(Re),t&&s(ae),t&&s(De),t&&s(re),t&&s(Pe),b(O,t),t&&s(xe),t&&s($),t&&s(je),t&&s(B),b(U),t&&s(Ie),t&&s(m),b(W),b(X),b(Q),b(Z),b(J),b(G),b(ee),b(te)}}}const Ps={local:"bertweet",sections:[{local:"overview",title:"Overview"},{local:"transformers.BertweetTokenizer",title:"BertweetTokenizer"}],title:"BERTweet"};function xs(Je,E,se){let{fw:g}=E;return Je.$$set=T=>{"fw"in T&&se(0,g=T.fw)},[g]}class Ms extends zs{constructor(E){super();Bs(this,E,xs,Ds,qs,{fw:0})}}export{Ms as default,Ps as metadata};
