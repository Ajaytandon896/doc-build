import{S as Ji,i as Ki,s as Qi,e as a,k as l,w as u,t as o,L as Xi,c as n,d as r,m,a as s,x as f,h as i,b as c,M as ka,J as t,g as d,y as g,K as Yi,q as _,o as w,B as v}from"../../chunks/vendor-e859c359.js";import{D as $}from"../../chunks/Docstring-ade913b3.js";import{C as Na}from"../../chunks/CodeBlock-ce4317c2.js";import{I as Z}from"../../chunks/IconCopyLink-5fae3b20.js";import"../../chunks/CopyButton-77addb3d.js";function Zi(Fa){let S,dt,E,x,bt,_e,Ca,$t,Oa,Lr,ee,Ra,At,ja,qa,Pr,I,zt,Ua,Ga,we,Ma,Et,Va,Ha,Ba,xt,Ja,Wr,k,te,Tt,ve,Ka,Dt,Qa,Sr,T,ye,Xa,be,Ya,$e,Za,en,tn,re,Ae,rn,Lt,an,Ir,N,ae,Pt,ze,nn,Wt,sn,kr,h,Ee,on,ht,ln,xe,mn,cn,b,pn,St,dn,hn,Te,un,fn,It,gn,_n,kt,wn,vn,Nt,yn,bn,Ft,$n,An,Ct,zn,En,xn,Ot,Tn,Dn,De,Ln,Le,Pn,Wn,Sn,D,Pe,Rt,In,kn,We,jt,Nn,Fn,Se,Cn,Ie,On,Rn,jn,qt,Ut,qn,Un,Gt,Mt,Gn,Mn,Vt,Ht,Vn,Hn,Bt,Bn,Jn,ke,Kn,Jt,Qn,Xn,Ne,Yn,L,Zn,Kt,es,ts,ut,rs,as,Qt,ns,ss,os,Fe,is,Xt,ls,ms,Ce,cs,ne,Oe,ps,Yt,ds,Nr,F,se,Zt,Re,hs,er,us,Fr,z,je,fs,C,gs,tr,_s,ws,qe,vs,ys,bs,rr,$s,As,oe,Ue,zs,ar,Es,Cr,O,Ge,xs,nr,Ts,Or,R,ie,sr,Me,Ds,or,Ls,Rr,j,le,ir,Ve,Ps,lr,Ws,jr,q,He,Ss,mr,Is,qr,U,Be,ks,cr,Ns,Ur,G,Je,Fs,pr,Cs,Gr,M,Ke,Os,dr,Rs,Mr,Qe,yo,Vr,V,Xe,js,hr,qs,Hr,Ye,bo,Br,H,Ze,Us,ur,Gs,Jr,et,$o,Kr,B,tt,Ms,fr,Vs,Qr,rt,Ao,Xr,P,at,Hs,nt,Bs,gr,Js,Ks,Qs,me,Xs,_r,Ys,Zs,st,eo,Yr,J,ce,wr,ot,to,vr,ro,Zr,K,it,ao,yr,no,ea,Q,pe,br,lt,so,$r,oo,ta,X,de,Ar,mt,io,zr,lo,ra,W,ct,mo,Y,co,Er,po,ho,xr,uo,fo,go,he,pt,_o,Tr,wo,aa;return _e=new Z({}),ve=new Z({}),ye=new $({props:{name:"class transformers.AdamW",anchor:"transformers.AdamW",parameters:[{name:"params",val:": typing.Iterable[torch.nn.parameter.Parameter]"},{name:"lr",val:": float = 0.001"},{name:"betas",val:": typing.Tuple[float, float] = (0.9, 0.999)"},{name:"eps",val:": float = 1e-06"},{name:"weight_decay",val:": float = 0.0"},{name:"correct_bias",val:": bool = True"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L272",parametersDescription:[{anchor:"transformers.AdamW.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.AdamW.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The learning rate to use.`,name:"lr"},{anchor:"transformers.AdamW.betas",description:`<strong>betas</strong> (<code>Tuple[float,float]</code>, <em>optional</em>, defaults to (0.9, 0.999)) &#x2014;
Adam&#x2019;s betas parameters (b1, b2).`,name:"betas"},{anchor:"transformers.AdamW.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
Adam&#x2019;s epsilon for numerical stability.`,name:"eps"},{anchor:"transformers.AdamW.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Decoupled weight decay to apply.`,name:"weight_decay"},{anchor:"transformers.AdamW.correct_bias",description:`<strong>correct_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <em>True</em>) &#x2014;
Whether or not to correct bias in Adam (for instance, in Bert TF repository they use <code>False</code>).`,name:"correct_bias"}]}}),Ae=new $({props:{name:"step",anchor:"transformers.AdamW.step",parameters:[{name:"closure",val:": typing.Callable = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L313",parametersDescription:[{anchor:"transformers.AdamW.step.closure",description:"<strong>closure</strong> (<code>Callable</code>, <em>optional</em>) &#x2014; A closure that reevaluates the model and returns the loss.",name:"closure"}]}}),ze=new Z({}),Ee=new $({props:{name:"class transformers.Adafactor",anchor:"transformers.Adafactor",parameters:[{name:"params",val:""},{name:"lr",val:" = None"},{name:"eps",val:" = (1e-30, 0.001)"},{name:"clip_threshold",val:" = 1.0"},{name:"decay_rate",val:" = -0.8"},{name:"beta1",val:" = None"},{name:"weight_decay",val:" = 0.0"},{name:"scale_parameter",val:" = True"},{name:"relative_step",val:" = True"},{name:"warmup_init",val:" = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L375",parametersDescription:[{anchor:"transformers.Adafactor.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.Adafactor.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The external learning rate.`,name:"lr"},{anchor:"transformers.Adafactor.eps",description:`<strong>eps</strong> (<code>Tuple[float, float]</code>, <em>optional</em>, defaults to (1e-30, 1e-3)) &#x2014;
Regularization constants for square gradient and parameter scale respectively`,name:"eps"},{anchor:"transformers.Adafactor.clip_threshold",description:`<strong>clip_threshold</strong> (<code>float</code>, <em>optional</em>, defaults 1.0) &#x2014;
Threshold of root mean square of final gradient update`,name:"clip_threshold"},{anchor:"transformers.Adafactor.decay_rate",description:`<strong>decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to -0.8) &#x2014;
Coefficient used to compute running averages of square`,name:"decay_rate"},{anchor:"transformers.Adafactor.beta1",description:`<strong>beta1</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Coefficient used for computing running averages of gradient`,name:"beta1"},{anchor:"transformers.Adafactor.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Weight decay (L2 penalty)`,name:"weight_decay"},{anchor:"transformers.Adafactor.scale_parameter",description:`<strong>scale_parameter</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, learning rate is scaled by root mean square`,name:"scale_parameter"},{anchor:"transformers.Adafactor.relative_step",description:`<strong>relative_step</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, time-dependent learning rate is computed instead of external learning rate`,name:"relative_step"},{anchor:"transformers.Adafactor.warmup_init",description:`<strong>warmup_init</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Time-dependent learning rate computation depends on whether warm-up initialization is being used`,name:"warmup_init"}]}}),ke=new Na({props:{code:"Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3),",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">False</span>, relative_step=<span class="hljs-literal">False</span>, warmup_init=<span class="hljs-literal">False</span>, lr=<span class="hljs-number">1e-3</span>)'}}),Ne=new Na({props:{code:"Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None),",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)'}}),Fe=new Na({props:{code:`from transformers.optimization import Adafactor, AdafactorSchedule
optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler)),`,highlighted:`<span class="hljs-keyword">from</span> transformers.optimization <span class="hljs-keyword">import</span> Adafactor, AdafactorSchedule
optimizer = Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`}}),Ce=new Na({props:{code:`# replace AdamW with Adafactor
optimizer = Adafactor(
    model.parameters(),
    lr=1e-3,
    eps=(1e-30, 1e-3),
    clip_threshold=1.0,
    decay_rate=-0.8,
    beta1=None,
    weight_decay=0.0,
    relative_step=False,
    scale_parameter=False,
    warmup_init=False
),`,highlighted:`<span class="hljs-comment"># replace AdamW with Adafactor</span>
optimizer = Adafactor(
    model.parameters(),
    lr=<span class="hljs-number">1e-3</span>,
    eps=(<span class="hljs-number">1e-30</span>, <span class="hljs-number">1e-3</span>),
    clip_threshold=<span class="hljs-number">1.0</span>,
    decay_rate=-<span class="hljs-number">0.8</span>,
    beta1=<span class="hljs-literal">None</span>,
    weight_decay=<span class="hljs-number">0.0</span>,
    relative_step=<span class="hljs-literal">False</span>,
    scale_parameter=<span class="hljs-literal">False</span>,
    warmup_init=<span class="hljs-literal">False</span>
)`}}),Oe=new $({props:{name:"step",anchor:"transformers.Adafactor.step",parameters:[{name:"closure",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L510",parametersDescription:[{anchor:"transformers.Adafactor.step.closure",description:`<strong>closure</strong> (callable, optional) &#x2014; A closure that reevaluates the model
and returns the loss.`,name:"closure"}]}}),Re=new Z({}),je=new $({props:{name:"class transformers.AdamWeightDecay",anchor:"transformers.AdamWeightDecay",parameters:[{name:"learning_rate",val:": typing.Union[float, keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule] = 0.001"},{name:"beta_1",val:": float = 0.9"},{name:"beta_2",val:": float = 0.999"},{name:"epsilon",val:": float = 1e-07"},{name:"amsgrad",val:": bool = False"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"include_in_weight_decay",val:": typing.Optional[typing.List[str]] = None"},{name:"exclude_from_weight_decay",val:": typing.Optional[typing.List[str]] = None"},{name:"name",val:": str = 'AdamWeightDecay'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L152",parametersDescription:[{anchor:"transformers.AdamWeightDecay.learning_rate",description:`<strong>learning_rate</strong> (<code>Union[float, tf.keras.optimizers.schedules.LearningRateSchedule]</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The learning rate to use or a schedule.`,name:"learning_rate"},{anchor:"transformers.AdamWeightDecay.beta_1",description:`<strong>beta_1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 parameter in Adam, which is the exponential decay rate for the 1st momentum estimates.`,name:"beta_1"},{anchor:"transformers.AdamWeightDecay.beta_2",description:`<strong>beta_2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 parameter in Adam, which is the exponential decay rate for the 2nd momentum estimates.`,name:"beta_2"},{anchor:"transformers.AdamWeightDecay.epsilon",description:`<strong>epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The epsilon parameter in Adam, which is a small constant for numerical stability.`,name:"epsilon"},{anchor:"transformers.AdamWeightDecay.amsgrad",description:`<strong>amsgrad</strong> (<code>bool</code>, <em>optional</em>, default to <em>False</em>) &#x2014;
Whether to apply AMSGrad variant of this algorithm or not, see <a href="https://arxiv.org/abs/1904.09237" rel="nofollow">On the Convergence of Adam and Beyond</a>.`,name:"amsgrad"},{anchor:"transformers.AdamWeightDecay.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to apply.`,name:"weight_decay_rate"},{anchor:"transformers.AdamWeightDecay.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters by default (unless they are in <code>exclude_from_weight_decay</code>).`,name:"include_in_weight_decay"},{anchor:"transformers.AdamWeightDecay.exclude_from_weight_decay",description:`<strong>exclude_from_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to exclude from applying weight decay to. If a
<code>include_in_weight_decay</code> is passed, the names in it will supersede this list.`,name:"exclude_from_weight_decay"},{anchor:"transformers.AdamWeightDecay.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>, defaults to &#x2018;AdamWeightDecay&#x2019;) &#x2014;
Optional name for the operations created when applying gradients.
kwargs &#x2014;
Keyword arguments. Allowed to be {<code>clipnorm</code>, <code>clipvalue</code>, <code>lr</code>, <code>decay</code>}. <code>clipnorm</code> is clip
gradients by norm; <code>clipvalue</code> is clip gradients by value, <code>decay</code> is included for backward
compatibility to allow time inverse decay of learning rate. <code>lr</code> is included for backward compatibility,
recommended to use <code>learning_rate</code> instead.`,name:"name"}]}}),Ue=new $({props:{name:"from_config",anchor:"transformers.AdamWeightDecay.from_config",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L209"}}),Ge=new $({props:{name:"transformers.create_optimizer",anchor:"transformers.create_optimizer",parameters:[{name:"init_lr",val:": float"},{name:"num_train_steps",val:": int"},{name:"num_warmup_steps",val:": int"},{name:"min_lr_ratio",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"power",val:": float = 1.0"},{name:"include_in_weight_decay",val:": typing.Optional[typing.List[str]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L82",parametersDescription:[{anchor:"transformers.create_optimizer.init_lr",description:`<strong>init_lr</strong> (<code>float</code>) &#x2014;
The desired learning rate at the end of the warmup phase.`,name:"init_lr"},{anchor:"transformers.create_optimizer.num_train_steps",description:`<strong>num_train_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_train_steps"},{anchor:"transformers.create_optimizer.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of warmup steps.`,name:"num_warmup_steps"},{anchor:"transformers.create_optimizer.min_lr_ratio",description:`<strong>min_lr_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The final learning rate at the end of the linear decay will be <code>init_lr * min_lr_ratio</code>.`,name:"min_lr_ratio"},{anchor:"transformers.create_optimizer.adam_beta1",description:`<strong>adam_beta1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 to use in Adam.`,name:"adam_beta1"},{anchor:"transformers.create_optimizer.adam_beta2",description:`<strong>adam_beta2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 to use in Adam.`,name:"adam_beta2"},{anchor:"transformers.create_optimizer.adam_epsilon",description:`<strong>adam_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-8) &#x2014;
The epsilon to use in Adam.`,name:"adam_epsilon"},{anchor:"transformers.create_optimizer.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to use.`,name:"weight_decay_rate"},{anchor:"transformers.create_optimizer.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The power to use for PolynomialDecay.`,name:"power"},{anchor:"transformers.create_optimizer.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters except bias and layer norm parameters.`,name:"include_in_weight_decay"}]}}),Me=new Z({}),Ve=new Z({}),He=new $({props:{name:"class transformers.SchedulerType",anchor:"transformers.SchedulerType",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_utils.py#L282"}}),Be=new $({props:{name:"transformers.get_scheduler",anchor:"transformers.get_scheduler",parameters:[{name:"name",val:": typing.Union[str, transformers.trainer_utils.SchedulerType]"},{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": typing.Optional[int] = None"},{name:"num_training_steps",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L232",parametersDescription:[{anchor:"transformers.get_scheduler.name",description:`<strong>name</strong> (<code>str</code> or &#x201C;<code>SchedulerType</code>) &#x2014;
The name of the scheduler to use.`,name:"name"},{anchor:"transformers.get_scheduler.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.Optimizer</code>) &#x2014;
The optimizer that will be used during training.`,name:"optimizer"},{anchor:"transformers.get_scheduler.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of warmup steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_warmup_steps"},{anchor:"transformers.get_scheduler.num_training_steps",description:`<strong>num_training_steps</strong> (\`int&#x201C;, <em>optional</em>) &#x2014;
The number of training steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_training_steps"}]}}),Je=new $({props:{name:"transformers.get_constant_schedule",anchor:"transformers.get_constant_schedule",parameters:[{name:"optimizer",val:": Optimizer"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L33",parametersDescription:[{anchor:"transformers.get_constant_schedule.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Ke=new $({props:{name:"transformers.get_constant_schedule_with_warmup",anchor:"transformers.get_constant_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L49",parametersDescription:[{anchor:"transformers.get_constant_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_constant_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Xe=new $({props:{name:"transformers.get_cosine_schedule_with_warmup",anchor:"transformers.get_cosine_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": float = 0.5"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L103",parametersDescription:[{anchor:"transformers.get_cosine_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0
following a half-cosine).`,name:"num_cycles"},{anchor:"transformers.get_cosine_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Ze=new $({props:{name:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": int = 1"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L137",parametersDescription:[{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of hard restarts to use.`,name:"num_cycles"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),tt=new $({props:{name:"transformers.get_linear_schedule_with_warmup",anchor:"transformers.get_linear_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"last_epoch",val:" = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L74",parametersDescription:[{anchor:"transformers.get_linear_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_linear_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),at=new $({props:{name:"transformers.get_polynomial_decay_schedule_with_warmup",anchor:"transformers.get_polynomial_decay_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"lr_end",val:" = 1e-07"},{name:"power",val:" = 1.0"},{name:"last_epoch",val:" = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L172",parametersDescription:[{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.lr_end",description:`<strong>lr_end</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The end LR.`,name:"lr_end"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Power factor.`,name:"power"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),ot=new Z({}),it=new $({props:{name:"class transformers.WarmUp",anchor:"transformers.WarmUp",parameters:[{name:"initial_learning_rate",val:": float"},{name:"decay_schedule_fn",val:": typing.Callable"},{name:"warmup_steps",val:": int"},{name:"power",val:": float = 1.0"},{name:"name",val:": str = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L24",parametersDescription:[{anchor:"transformers.WarmUp.initial_learning_rate",description:`<strong>initial_learning_rate</strong> (<code>float</code>) &#x2014;
The initial learning rate for the schedule after the warmup (so this will be the learning rate at the end
of the warmup).`,name:"initial_learning_rate"},{anchor:"transformers.WarmUp.decay_schedule_fn",description:`<strong>decay_schedule_fn</strong> (<code>Callable</code>) &#x2014;
The schedule function to apply after the warmup for the rest of training.`,name:"decay_schedule_fn"},{anchor:"transformers.WarmUp.warmup_steps",description:`<strong>warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup part of training.`,name:"warmup_steps"},{anchor:"transformers.WarmUp.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
The power to use for the polynomial warmup (defaults is a linear warmup).`,name:"power"},{anchor:"transformers.WarmUp.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Optional name prefix for the returned tensors during the schedule.`,name:"name"}]}}),lt=new Z({}),mt=new Z({}),ct=new $({props:{name:"class transformers.GradientAccumulator",anchor:"transformers.GradientAccumulator",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L282"}}),pt=new $({props:{name:"reset",anchor:"transformers.GradientAccumulator.reset",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L344"}}),{c(){S=a("meta"),dt=l(),E=a("h1"),x=a("a"),bt=a("span"),u(_e.$$.fragment),Ca=l(),$t=a("span"),Oa=o("Optimization"),Lr=l(),ee=a("p"),Ra=o("The "),At=a("code"),ja=o(".optimization"),qa=o(" module provides:"),Pr=l(),I=a("ul"),zt=a("li"),Ua=o("an optimizer with weight decay fixed that can be used to fine-tuned models, and"),Ga=l(),we=a("li"),Ma=o("several schedules in the form of schedule objects that inherit from "),Et=a("code"),Va=o("_LRSchedule"),Ha=o(":"),Ba=l(),xt=a("li"),Ja=o("a gradient accumulation class to accumulate the gradients of multiple batches"),Wr=l(),k=a("h2"),te=a("a"),Tt=a("span"),u(ve.$$.fragment),Ka=l(),Dt=a("span"),Qa=o("AdamW (PyTorch)"),Sr=l(),T=a("div"),u(ye.$$.fragment),Xa=l(),be=a("p"),Ya=o("Implements Adam algorithm with weight decay fix as introduced in "),$e=a("a"),Za=o("Decoupled Weight Decay Regularization"),en=o("."),tn=l(),re=a("div"),u(Ae.$$.fragment),rn=l(),Lt=a("p"),an=o("Performs a single optimization step."),Ir=l(),N=a("h2"),ae=a("a"),Pt=a("span"),u(ze.$$.fragment),nn=l(),Wt=a("span"),sn=o("AdaFactor (PyTorch)"),kr=l(),h=a("div"),u(Ee.$$.fragment),on=l(),ht=a("p"),ln=o(`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
`),xe=a("a"),mn=o("https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),cn=l(),b=a("p"),pn=o("Paper: "),St=a("em"),dn=o("Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"),hn=l(),Te=a("a"),un=o("https://arxiv.org/abs/1804.04235"),fn=o(` Note that
this optimizer internally adjusts the learning rate depending on the `),It=a("em"),gn=o("scale_parameter"),_n=o(", "),kt=a("em"),wn=o("relative_step"),vn=o(` and
`),Nt=a("em"),yn=o("warmup_init"),bn=o(" options. To use a manual (external) learning rate schedule you should set "),Ft=a("em"),$n=o("scale_parameter=False"),An=o(` and
`),Ct=a("em"),zn=o("relative_step=False"),En=o("."),xn=l(),Ot=a("p"),Tn=o("This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested."),Dn=l(),De=a("p"),Ln=o("Recommended T5 finetuning settings ("),Le=a("a"),Pn=o("https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),Wn=o("):"),Sn=l(),D=a("ul"),Pe=a("li"),Rt=a("p"),In=o("Training without LR warmup or clip_threshold is not recommended."),kn=l(),We=a("ul"),jt=a("li"),Nn=o("use scheduled LR warm-up to fixed LR"),Fn=l(),Se=a("li"),Cn=o("use clip_threshold=1.0 ("),Ie=a("a"),On=o("https://arxiv.org/abs/1804.04235"),Rn=o(")"),jn=l(),qt=a("li"),Ut=a("p"),qn=o("Disable relative updates"),Un=l(),Gt=a("li"),Mt=a("p"),Gn=o("Use scale_parameter=False"),Mn=l(),Vt=a("li"),Ht=a("p"),Vn=o("Additional optimizer operations like gradient clipping should not be used alongside Adafactor"),Hn=l(),Bt=a("p"),Bn=o("Example:"),Jn=l(),u(ke.$$.fragment),Kn=l(),Jt=a("p"),Qn=o("Others reported the following combination to work well:"),Xn=l(),u(Ne.$$.fragment),Yn=l(),L=a("p"),Zn=o("When using "),Kt=a("code"),es=o("lr=None"),ts=o(" with "),ut=a("a"),rs=o("Trainer"),as=o(" you will most likely need to use "),Qt=a("code"),ns=o("AdafactorSchedule"),ss=o(" scheduler as following:"),os=l(),u(Fe.$$.fragment),is=l(),Xt=a("p"),ls=o("Usage:"),ms=l(),u(Ce.$$.fragment),cs=l(),ne=a("div"),u(Oe.$$.fragment),ps=l(),Yt=a("p"),ds=o("Performs a single optimization step"),Nr=l(),F=a("h2"),se=a("a"),Zt=a("span"),u(Re.$$.fragment),hs=l(),er=a("span"),us=o("AdamWeightDecay (TensorFlow)"),Fr=l(),z=a("div"),u(je.$$.fragment),fs=l(),C=a("p"),gs=o(`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is `),tr=a("em"),_s=o("not"),ws=o(` the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in `),qe=a("a"),vs=o("Decoupled Weight Decay Regularization"),ys=o("."),bs=l(),rr=a("p"),$s=o(`Instead we want ot decay the weights in a manner that doesn\u2019t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`),As=l(),oe=a("div"),u(Ue.$$.fragment),zs=l(),ar=a("p"),Es=o("Creates an optimizer from its config with WarmUp custom object."),Cr=l(),O=a("div"),u(Ge.$$.fragment),xs=l(),nr=a("p"),Ts=o("Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay."),Or=l(),R=a("h2"),ie=a("a"),sr=a("span"),u(Me.$$.fragment),Ds=l(),or=a("span"),Ls=o("Schedules"),Rr=l(),j=a("h3"),le=a("a"),ir=a("span"),u(Ve.$$.fragment),Ps=l(),lr=a("span"),Ws=o("Learning Rate Schedules (Pytorch)"),jr=l(),q=a("div"),u(He.$$.fragment),Ss=l(),mr=a("p"),Is=o("An enumeration."),qr=l(),U=a("div"),u(Be.$$.fragment),ks=l(),cr=a("p"),Ns=o("Unified API to get any scheduler from its name."),Ur=l(),G=a("div"),u(Je.$$.fragment),Fs=l(),pr=a("p"),Cs=o("Create a schedule with a constant learning rate, using the learning rate set in optimizer."),Gr=l(),M=a("div"),u(Ke.$$.fragment),Os=l(),dr=a("p"),Rs=o(`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`),Mr=l(),Qe=a("img"),Vr=l(),V=a("div"),u(Xe.$$.fragment),js=l(),hr=a("p"),qs=o(`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`),Hr=l(),Ye=a("img"),Br=l(),H=a("div"),u(Ze.$$.fragment),Us=l(),ur=a("p"),Gs=o(`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`),Jr=l(),et=a("img"),Kr=l(),B=a("div"),u(tt.$$.fragment),Ms=l(),fr=a("p"),Vs=o(`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`),Qr=l(),rt=a("img"),Xr=l(),P=a("div"),u(at.$$.fragment),Hs=l(),nt=a("p"),Bs=o(`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by `),gr=a("em"),Js=o("lr_end"),Ks=o(`, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`),Qs=l(),me=a("p"),Xs=o("Note: "),_r=a("em"),Ys=o("power"),Zs=o(` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
`),st=a("a"),eo=o("https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),Yr=l(),J=a("h3"),ce=a("a"),wr=a("span"),u(ot.$$.fragment),to=l(),vr=a("span"),ro=o("Warmup (TensorFlow)"),Zr=l(),K=a("div"),u(it.$$.fragment),ao=l(),yr=a("p"),no=o("Applies a warmup schedule on a given learning rate decay schedule."),ea=l(),Q=a("h2"),pe=a("a"),br=a("span"),u(lt.$$.fragment),so=l(),$r=a("span"),oo=o("Gradient Strategies"),ta=l(),X=a("h3"),de=a("a"),Ar=a("span"),u(mt.$$.fragment),io=l(),zr=a("span"),lo=o("GradientAccumulator (TensorFlow)"),ra=l(),W=a("div"),u(ct.$$.fragment),mo=l(),Y=a("p"),co=o(`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call `),Er=a("code"),po=o(".gradients"),ho=o(", scale the gradients if required, and pass the result to "),xr=a("code"),uo=o("apply_gradients"),fo=o("."),go=l(),he=a("div"),u(pt.$$.fragment),_o=l(),Tr=a("p"),wo=o("Resets the accumulated gradients on the current replica."),this.h()},l(e){const p=Xi('[data-svelte="svelte-1phssyn"]',document.head);S=n(p,"META",{name:!0,content:!0}),p.forEach(r),dt=m(e),E=n(e,"H1",{class:!0});var na=s(E);x=n(na,"A",{id:!0,class:!0,href:!0});var zo=s(x);bt=n(zo,"SPAN",{});var Eo=s(bt);f(_e.$$.fragment,Eo),Eo.forEach(r),zo.forEach(r),Ca=m(na),$t=n(na,"SPAN",{});var xo=s($t);Oa=i(xo,"Optimization"),xo.forEach(r),na.forEach(r),Lr=m(e),ee=n(e,"P",{});var sa=s(ee);Ra=i(sa,"The "),At=n(sa,"CODE",{});var To=s(At);ja=i(To,".optimization"),To.forEach(r),qa=i(sa," module provides:"),sa.forEach(r),Pr=m(e),I=n(e,"UL",{});var ft=s(I);zt=n(ft,"LI",{});var Do=s(zt);Ua=i(Do,"an optimizer with weight decay fixed that can be used to fine-tuned models, and"),Do.forEach(r),Ga=m(ft),we=n(ft,"LI",{});var oa=s(we);Ma=i(oa,"several schedules in the form of schedule objects that inherit from "),Et=n(oa,"CODE",{});var Lo=s(Et);Va=i(Lo,"_LRSchedule"),Lo.forEach(r),Ha=i(oa,":"),oa.forEach(r),Ba=m(ft),xt=n(ft,"LI",{});var Po=s(xt);Ja=i(Po,"a gradient accumulation class to accumulate the gradients of multiple batches"),Po.forEach(r),ft.forEach(r),Wr=m(e),k=n(e,"H2",{class:!0});var ia=s(k);te=n(ia,"A",{id:!0,class:!0,href:!0});var Wo=s(te);Tt=n(Wo,"SPAN",{});var So=s(Tt);f(ve.$$.fragment,So),So.forEach(r),Wo.forEach(r),Ka=m(ia),Dt=n(ia,"SPAN",{});var Io=s(Dt);Qa=i(Io,"AdamW (PyTorch)"),Io.forEach(r),ia.forEach(r),Sr=m(e),T=n(e,"DIV",{class:!0});var gt=s(T);f(ye.$$.fragment,gt),Xa=m(gt),be=n(gt,"P",{});var la=s(be);Ya=i(la,"Implements Adam algorithm with weight decay fix as introduced in "),$e=n(la,"A",{href:!0,rel:!0});var ko=s($e);Za=i(ko,"Decoupled Weight Decay Regularization"),ko.forEach(r),en=i(la,"."),la.forEach(r),tn=m(gt),re=n(gt,"DIV",{class:!0});var ma=s(re);f(Ae.$$.fragment,ma),rn=m(ma),Lt=n(ma,"P",{});var No=s(Lt);an=i(No,"Performs a single optimization step."),No.forEach(r),ma.forEach(r),gt.forEach(r),Ir=m(e),N=n(e,"H2",{class:!0});var ca=s(N);ae=n(ca,"A",{id:!0,class:!0,href:!0});var Fo=s(ae);Pt=n(Fo,"SPAN",{});var Co=s(Pt);f(ze.$$.fragment,Co),Co.forEach(r),Fo.forEach(r),nn=m(ca),Wt=n(ca,"SPAN",{});var Oo=s(Wt);sn=i(Oo,"AdaFactor (PyTorch)"),Oo.forEach(r),ca.forEach(r),kr=m(e),h=n(e,"DIV",{class:!0});var y=s(h);f(Ee.$$.fragment,y),on=m(y),ht=n(y,"P",{});var vo=s(ht);ln=i(vo,`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
`),xe=n(vo,"A",{href:!0,rel:!0});var Ro=s(xe);mn=i(Ro,"https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),Ro.forEach(r),vo.forEach(r),cn=m(y),b=n(y,"P",{});var A=s(b);pn=i(A,"Paper: "),St=n(A,"EM",{});var jo=s(St);dn=i(jo,"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"),jo.forEach(r),hn=m(A),Te=n(A,"A",{href:!0,rel:!0});var qo=s(Te);un=i(qo,"https://arxiv.org/abs/1804.04235"),qo.forEach(r),fn=i(A,` Note that
this optimizer internally adjusts the learning rate depending on the `),It=n(A,"EM",{});var Uo=s(It);gn=i(Uo,"scale_parameter"),Uo.forEach(r),_n=i(A,", "),kt=n(A,"EM",{});var Go=s(kt);wn=i(Go,"relative_step"),Go.forEach(r),vn=i(A,` and
`),Nt=n(A,"EM",{});var Mo=s(Nt);yn=i(Mo,"warmup_init"),Mo.forEach(r),bn=i(A," options. To use a manual (external) learning rate schedule you should set "),Ft=n(A,"EM",{});var Vo=s(Ft);$n=i(Vo,"scale_parameter=False"),Vo.forEach(r),An=i(A,` and
`),Ct=n(A,"EM",{});var Ho=s(Ct);zn=i(Ho,"relative_step=False"),Ho.forEach(r),En=i(A,"."),A.forEach(r),xn=m(y),Ot=n(y,"P",{});var Bo=s(Ot);Tn=i(Bo,"This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested."),Bo.forEach(r),Dn=m(y),De=n(y,"P",{});var pa=s(De);Ln=i(pa,"Recommended T5 finetuning settings ("),Le=n(pa,"A",{href:!0,rel:!0});var Jo=s(Le);Pn=i(Jo,"https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),Jo.forEach(r),Wn=i(pa,"):"),pa.forEach(r),Sn=m(y),D=n(y,"UL",{});var ue=s(D);Pe=n(ue,"LI",{});var da=s(Pe);Rt=n(da,"P",{});var Ko=s(Rt);In=i(Ko,"Training without LR warmup or clip_threshold is not recommended."),Ko.forEach(r),kn=m(da),We=n(da,"UL",{});var ha=s(We);jt=n(ha,"LI",{});var Qo=s(jt);Nn=i(Qo,"use scheduled LR warm-up to fixed LR"),Qo.forEach(r),Fn=m(ha),Se=n(ha,"LI",{});var ua=s(Se);Cn=i(ua,"use clip_threshold=1.0 ("),Ie=n(ua,"A",{href:!0,rel:!0});var Xo=s(Ie);On=i(Xo,"https://arxiv.org/abs/1804.04235"),Xo.forEach(r),Rn=i(ua,")"),ua.forEach(r),ha.forEach(r),da.forEach(r),jn=m(ue),qt=n(ue,"LI",{});var Yo=s(qt);Ut=n(Yo,"P",{});var Zo=s(Ut);qn=i(Zo,"Disable relative updates"),Zo.forEach(r),Yo.forEach(r),Un=m(ue),Gt=n(ue,"LI",{});var ei=s(Gt);Mt=n(ei,"P",{});var ti=s(Mt);Gn=i(ti,"Use scale_parameter=False"),ti.forEach(r),ei.forEach(r),Mn=m(ue),Vt=n(ue,"LI",{});var ri=s(Vt);Ht=n(ri,"P",{});var ai=s(Ht);Vn=i(ai,"Additional optimizer operations like gradient clipping should not be used alongside Adafactor"),ai.forEach(r),ri.forEach(r),ue.forEach(r),Hn=m(y),Bt=n(y,"P",{});var ni=s(Bt);Bn=i(ni,"Example:"),ni.forEach(r),Jn=m(y),f(ke.$$.fragment,y),Kn=m(y),Jt=n(y,"P",{});var si=s(Jt);Qn=i(si,"Others reported the following combination to work well:"),si.forEach(r),Xn=m(y),f(Ne.$$.fragment,y),Yn=m(y),L=n(y,"P",{});var fe=s(L);Zn=i(fe,"When using "),Kt=n(fe,"CODE",{});var oi=s(Kt);es=i(oi,"lr=None"),oi.forEach(r),ts=i(fe," with "),ut=n(fe,"A",{href:!0});var ii=s(ut);rs=i(ii,"Trainer"),ii.forEach(r),as=i(fe," you will most likely need to use "),Qt=n(fe,"CODE",{});var li=s(Qt);ns=i(li,"AdafactorSchedule"),li.forEach(r),ss=i(fe," scheduler as following:"),fe.forEach(r),os=m(y),f(Fe.$$.fragment,y),is=m(y),Xt=n(y,"P",{});var mi=s(Xt);ls=i(mi,"Usage:"),mi.forEach(r),ms=m(y),f(Ce.$$.fragment,y),cs=m(y),ne=n(y,"DIV",{class:!0});var fa=s(ne);f(Oe.$$.fragment,fa),ps=m(fa),Yt=n(fa,"P",{});var ci=s(Yt);ds=i(ci,"Performs a single optimization step"),ci.forEach(r),fa.forEach(r),y.forEach(r),Nr=m(e),F=n(e,"H2",{class:!0});var ga=s(F);se=n(ga,"A",{id:!0,class:!0,href:!0});var pi=s(se);Zt=n(pi,"SPAN",{});var di=s(Zt);f(Re.$$.fragment,di),di.forEach(r),pi.forEach(r),hs=m(ga),er=n(ga,"SPAN",{});var hi=s(er);us=i(hi,"AdamWeightDecay (TensorFlow)"),hi.forEach(r),ga.forEach(r),Fr=m(e),z=n(e,"DIV",{class:!0});var ge=s(z);f(je.$$.fragment,ge),fs=m(ge),C=n(ge,"P",{});var _t=s(C);gs=i(_t,`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is `),tr=n(_t,"EM",{});var ui=s(tr);_s=i(ui,"not"),ui.forEach(r),ws=i(_t,` the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in `),qe=n(_t,"A",{href:!0,rel:!0});var fi=s(qe);vs=i(fi,"Decoupled Weight Decay Regularization"),fi.forEach(r),ys=i(_t,"."),_t.forEach(r),bs=m(ge),rr=n(ge,"P",{});var gi=s(rr);$s=i(gi,`Instead we want ot decay the weights in a manner that doesn\u2019t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`),gi.forEach(r),As=m(ge),oe=n(ge,"DIV",{class:!0});var _a=s(oe);f(Ue.$$.fragment,_a),zs=m(_a),ar=n(_a,"P",{});var _i=s(ar);Es=i(_i,"Creates an optimizer from its config with WarmUp custom object."),_i.forEach(r),_a.forEach(r),ge.forEach(r),Cr=m(e),O=n(e,"DIV",{class:!0});var wa=s(O);f(Ge.$$.fragment,wa),xs=m(wa),nr=n(wa,"P",{});var wi=s(nr);Ts=i(wi,"Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay."),wi.forEach(r),wa.forEach(r),Or=m(e),R=n(e,"H2",{class:!0});var va=s(R);ie=n(va,"A",{id:!0,class:!0,href:!0});var vi=s(ie);sr=n(vi,"SPAN",{});var yi=s(sr);f(Me.$$.fragment,yi),yi.forEach(r),vi.forEach(r),Ds=m(va),or=n(va,"SPAN",{});var bi=s(or);Ls=i(bi,"Schedules"),bi.forEach(r),va.forEach(r),Rr=m(e),j=n(e,"H3",{class:!0});var ya=s(j);le=n(ya,"A",{id:!0,class:!0,href:!0});var $i=s(le);ir=n($i,"SPAN",{});var Ai=s(ir);f(Ve.$$.fragment,Ai),Ai.forEach(r),$i.forEach(r),Ps=m(ya),lr=n(ya,"SPAN",{});var zi=s(lr);Ws=i(zi,"Learning Rate Schedules (Pytorch)"),zi.forEach(r),ya.forEach(r),jr=m(e),q=n(e,"DIV",{class:!0});var ba=s(q);f(He.$$.fragment,ba),Ss=m(ba),mr=n(ba,"P",{});var Ei=s(mr);Is=i(Ei,"An enumeration."),Ei.forEach(r),ba.forEach(r),qr=m(e),U=n(e,"DIV",{class:!0});var $a=s(U);f(Be.$$.fragment,$a),ks=m($a),cr=n($a,"P",{});var xi=s(cr);Ns=i(xi,"Unified API to get any scheduler from its name."),xi.forEach(r),$a.forEach(r),Ur=m(e),G=n(e,"DIV",{class:!0});var Aa=s(G);f(Je.$$.fragment,Aa),Fs=m(Aa),pr=n(Aa,"P",{});var Ti=s(pr);Cs=i(Ti,"Create a schedule with a constant learning rate, using the learning rate set in optimizer."),Ti.forEach(r),Aa.forEach(r),Gr=m(e),M=n(e,"DIV",{class:!0});var za=s(M);f(Ke.$$.fragment,za),Os=m(za),dr=n(za,"P",{});var Di=s(dr);Rs=i(Di,`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`),Di.forEach(r),za.forEach(r),Mr=m(e),Qe=n(e,"IMG",{alt:!0,src:!0}),Vr=m(e),V=n(e,"DIV",{class:!0});var Ea=s(V);f(Xe.$$.fragment,Ea),js=m(Ea),hr=n(Ea,"P",{});var Li=s(hr);qs=i(Li,`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`),Li.forEach(r),Ea.forEach(r),Hr=m(e),Ye=n(e,"IMG",{alt:!0,src:!0}),Br=m(e),H=n(e,"DIV",{class:!0});var xa=s(H);f(Ze.$$.fragment,xa),Us=m(xa),ur=n(xa,"P",{});var Pi=s(ur);Gs=i(Pi,`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`),Pi.forEach(r),xa.forEach(r),Jr=m(e),et=n(e,"IMG",{alt:!0,src:!0}),Kr=m(e),B=n(e,"DIV",{class:!0});var Ta=s(B);f(tt.$$.fragment,Ta),Ms=m(Ta),fr=n(Ta,"P",{});var Wi=s(fr);Vs=i(Wi,`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`),Wi.forEach(r),Ta.forEach(r),Qr=m(e),rt=n(e,"IMG",{alt:!0,src:!0}),Xr=m(e),P=n(e,"DIV",{class:!0});var wt=s(P);f(at.$$.fragment,wt),Hs=m(wt),nt=n(wt,"P",{});var Da=s(nt);Bs=i(Da,`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by `),gr=n(Da,"EM",{});var Si=s(gr);Js=i(Si,"lr_end"),Si.forEach(r),Ks=i(Da,`, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`),Da.forEach(r),Qs=m(wt),me=n(wt,"P",{});var Dr=s(me);Xs=i(Dr,"Note: "),_r=n(Dr,"EM",{});var Ii=s(_r);Ys=i(Ii,"power"),Ii.forEach(r),Zs=i(Dr,` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
`),st=n(Dr,"A",{href:!0,rel:!0});var ki=s(st);eo=i(ki,"https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),ki.forEach(r),Dr.forEach(r),wt.forEach(r),Yr=m(e),J=n(e,"H3",{class:!0});var La=s(J);ce=n(La,"A",{id:!0,class:!0,href:!0});var Ni=s(ce);wr=n(Ni,"SPAN",{});var Fi=s(wr);f(ot.$$.fragment,Fi),Fi.forEach(r),Ni.forEach(r),to=m(La),vr=n(La,"SPAN",{});var Ci=s(vr);ro=i(Ci,"Warmup (TensorFlow)"),Ci.forEach(r),La.forEach(r),Zr=m(e),K=n(e,"DIV",{class:!0});var Pa=s(K);f(it.$$.fragment,Pa),ao=m(Pa),yr=n(Pa,"P",{});var Oi=s(yr);no=i(Oi,"Applies a warmup schedule on a given learning rate decay schedule."),Oi.forEach(r),Pa.forEach(r),ea=m(e),Q=n(e,"H2",{class:!0});var Wa=s(Q);pe=n(Wa,"A",{id:!0,class:!0,href:!0});var Ri=s(pe);br=n(Ri,"SPAN",{});var ji=s(br);f(lt.$$.fragment,ji),ji.forEach(r),Ri.forEach(r),so=m(Wa),$r=n(Wa,"SPAN",{});var qi=s($r);oo=i(qi,"Gradient Strategies"),qi.forEach(r),Wa.forEach(r),ta=m(e),X=n(e,"H3",{class:!0});var Sa=s(X);de=n(Sa,"A",{id:!0,class:!0,href:!0});var Ui=s(de);Ar=n(Ui,"SPAN",{});var Gi=s(Ar);f(mt.$$.fragment,Gi),Gi.forEach(r),Ui.forEach(r),io=m(Sa),zr=n(Sa,"SPAN",{});var Mi=s(zr);lo=i(Mi,"GradientAccumulator (TensorFlow)"),Mi.forEach(r),Sa.forEach(r),ra=m(e),W=n(e,"DIV",{class:!0});var vt=s(W);f(ct.$$.fragment,vt),mo=m(vt),Y=n(vt,"P",{});var yt=s(Y);co=i(yt,`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call `),Er=n(yt,"CODE",{});var Vi=s(Er);po=i(Vi,".gradients"),Vi.forEach(r),ho=i(yt,", scale the gradients if required, and pass the result to "),xr=n(yt,"CODE",{});var Hi=s(xr);uo=i(Hi,"apply_gradients"),Hi.forEach(r),fo=i(yt,"."),yt.forEach(r),go=m(vt),he=n(vt,"DIV",{class:!0});var Ia=s(he);f(pt.$$.fragment,Ia),_o=m(Ia),Tr=n(Ia,"P",{});var Bi=s(Tr);wo=i(Bi,"Resets the accumulated gradients on the current replica."),Bi.forEach(r),Ia.forEach(r),vt.forEach(r),this.h()},h(){c(S,"name","hf:doc:metadata"),c(S,"content",JSON.stringify(el)),c(x,"id","optimization"),c(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(x,"href","#optimization"),c(E,"class","relative group"),c(te,"id","transformers.AdamW"),c(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(te,"href","#transformers.AdamW"),c(k,"class","relative group"),c($e,"href","https://arxiv.org/abs/1711.05101"),c($e,"rel","nofollow"),c(re,"class","docstring"),c(T,"class","docstring"),c(ae,"id","transformers.Adafactor"),c(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ae,"href","#transformers.Adafactor"),c(N,"class","relative group"),c(xe,"href","https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),c(xe,"rel","nofollow"),c(Te,"href","https://arxiv.org/abs/1804.04235"),c(Te,"rel","nofollow"),c(Le,"href","https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),c(Le,"rel","nofollow"),c(Ie,"href","https://arxiv.org/abs/1804.04235"),c(Ie,"rel","nofollow"),c(ut,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),c(ne,"class","docstring"),c(h,"class","docstring"),c(se,"id","transformers.AdamWeightDecay"),c(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(se,"href","#transformers.AdamWeightDecay"),c(F,"class","relative group"),c(qe,"href","https://arxiv.org/abs/1711.05101"),c(qe,"rel","nofollow"),c(oe,"class","docstring"),c(z,"class","docstring"),c(O,"class","docstring"),c(ie,"id","schedules"),c(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ie,"href","#schedules"),c(R,"class","relative group"),c(le,"id","transformers.SchedulerType"),c(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(le,"href","#transformers.SchedulerType"),c(j,"class","relative group"),c(q,"class","docstring"),c(U,"class","docstring"),c(G,"class","docstring"),c(M,"class","docstring"),c(Qe,"alt",""),ka(Qe.src,yo="/docs/transformers/master/en/imgs/warmup_constant_schedule.png")||c(Qe,"src",yo),c(V,"class","docstring"),c(Ye,"alt",""),ka(Ye.src,bo="/docs/transformers/master/en/imgs/warmup_cosine_schedule.png")||c(Ye,"src",bo),c(H,"class","docstring"),c(et,"alt",""),ka(et.src,$o="/docs/transformers/master/en/imgs/warmup_cosine_hard_restarts_schedule.png")||c(et,"src",$o),c(B,"class","docstring"),c(rt,"alt",""),ka(rt.src,Ao="/docs/transformers/master/en/imgs/warmup_linear_schedule.png")||c(rt,"src",Ao),c(st,"href","https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),c(st,"rel","nofollow"),c(P,"class","docstring"),c(ce,"id","transformers.WarmUp"),c(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ce,"href","#transformers.WarmUp"),c(J,"class","relative group"),c(K,"class","docstring"),c(pe,"id","gradient-strategies"),c(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pe,"href","#gradient-strategies"),c(Q,"class","relative group"),c(de,"id","transformers.GradientAccumulator"),c(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(de,"href","#transformers.GradientAccumulator"),c(X,"class","relative group"),c(he,"class","docstring"),c(W,"class","docstring")},m(e,p){t(document.head,S),d(e,dt,p),d(e,E,p),t(E,x),t(x,bt),g(_e,bt,null),t(E,Ca),t(E,$t),t($t,Oa),d(e,Lr,p),d(e,ee,p),t(ee,Ra),t(ee,At),t(At,ja),t(ee,qa),d(e,Pr,p),d(e,I,p),t(I,zt),t(zt,Ua),t(I,Ga),t(I,we),t(we,Ma),t(we,Et),t(Et,Va),t(we,Ha),t(I,Ba),t(I,xt),t(xt,Ja),d(e,Wr,p),d(e,k,p),t(k,te),t(te,Tt),g(ve,Tt,null),t(k,Ka),t(k,Dt),t(Dt,Qa),d(e,Sr,p),d(e,T,p),g(ye,T,null),t(T,Xa),t(T,be),t(be,Ya),t(be,$e),t($e,Za),t(be,en),t(T,tn),t(T,re),g(Ae,re,null),t(re,rn),t(re,Lt),t(Lt,an),d(e,Ir,p),d(e,N,p),t(N,ae),t(ae,Pt),g(ze,Pt,null),t(N,nn),t(N,Wt),t(Wt,sn),d(e,kr,p),d(e,h,p),g(Ee,h,null),t(h,on),t(h,ht),t(ht,ln),t(ht,xe),t(xe,mn),t(h,cn),t(h,b),t(b,pn),t(b,St),t(St,dn),t(b,hn),t(b,Te),t(Te,un),t(b,fn),t(b,It),t(It,gn),t(b,_n),t(b,kt),t(kt,wn),t(b,vn),t(b,Nt),t(Nt,yn),t(b,bn),t(b,Ft),t(Ft,$n),t(b,An),t(b,Ct),t(Ct,zn),t(b,En),t(h,xn),t(h,Ot),t(Ot,Tn),t(h,Dn),t(h,De),t(De,Ln),t(De,Le),t(Le,Pn),t(De,Wn),t(h,Sn),t(h,D),t(D,Pe),t(Pe,Rt),t(Rt,In),t(Pe,kn),t(Pe,We),t(We,jt),t(jt,Nn),t(We,Fn),t(We,Se),t(Se,Cn),t(Se,Ie),t(Ie,On),t(Se,Rn),t(D,jn),t(D,qt),t(qt,Ut),t(Ut,qn),t(D,Un),t(D,Gt),t(Gt,Mt),t(Mt,Gn),t(D,Mn),t(D,Vt),t(Vt,Ht),t(Ht,Vn),t(h,Hn),t(h,Bt),t(Bt,Bn),t(h,Jn),g(ke,h,null),t(h,Kn),t(h,Jt),t(Jt,Qn),t(h,Xn),g(Ne,h,null),t(h,Yn),t(h,L),t(L,Zn),t(L,Kt),t(Kt,es),t(L,ts),t(L,ut),t(ut,rs),t(L,as),t(L,Qt),t(Qt,ns),t(L,ss),t(h,os),g(Fe,h,null),t(h,is),t(h,Xt),t(Xt,ls),t(h,ms),g(Ce,h,null),t(h,cs),t(h,ne),g(Oe,ne,null),t(ne,ps),t(ne,Yt),t(Yt,ds),d(e,Nr,p),d(e,F,p),t(F,se),t(se,Zt),g(Re,Zt,null),t(F,hs),t(F,er),t(er,us),d(e,Fr,p),d(e,z,p),g(je,z,null),t(z,fs),t(z,C),t(C,gs),t(C,tr),t(tr,_s),t(C,ws),t(C,qe),t(qe,vs),t(C,ys),t(z,bs),t(z,rr),t(rr,$s),t(z,As),t(z,oe),g(Ue,oe,null),t(oe,zs),t(oe,ar),t(ar,Es),d(e,Cr,p),d(e,O,p),g(Ge,O,null),t(O,xs),t(O,nr),t(nr,Ts),d(e,Or,p),d(e,R,p),t(R,ie),t(ie,sr),g(Me,sr,null),t(R,Ds),t(R,or),t(or,Ls),d(e,Rr,p),d(e,j,p),t(j,le),t(le,ir),g(Ve,ir,null),t(j,Ps),t(j,lr),t(lr,Ws),d(e,jr,p),d(e,q,p),g(He,q,null),t(q,Ss),t(q,mr),t(mr,Is),d(e,qr,p),d(e,U,p),g(Be,U,null),t(U,ks),t(U,cr),t(cr,Ns),d(e,Ur,p),d(e,G,p),g(Je,G,null),t(G,Fs),t(G,pr),t(pr,Cs),d(e,Gr,p),d(e,M,p),g(Ke,M,null),t(M,Os),t(M,dr),t(dr,Rs),d(e,Mr,p),d(e,Qe,p),d(e,Vr,p),d(e,V,p),g(Xe,V,null),t(V,js),t(V,hr),t(hr,qs),d(e,Hr,p),d(e,Ye,p),d(e,Br,p),d(e,H,p),g(Ze,H,null),t(H,Us),t(H,ur),t(ur,Gs),d(e,Jr,p),d(e,et,p),d(e,Kr,p),d(e,B,p),g(tt,B,null),t(B,Ms),t(B,fr),t(fr,Vs),d(e,Qr,p),d(e,rt,p),d(e,Xr,p),d(e,P,p),g(at,P,null),t(P,Hs),t(P,nt),t(nt,Bs),t(nt,gr),t(gr,Js),t(nt,Ks),t(P,Qs),t(P,me),t(me,Xs),t(me,_r),t(_r,Ys),t(me,Zs),t(me,st),t(st,eo),d(e,Yr,p),d(e,J,p),t(J,ce),t(ce,wr),g(ot,wr,null),t(J,to),t(J,vr),t(vr,ro),d(e,Zr,p),d(e,K,p),g(it,K,null),t(K,ao),t(K,yr),t(yr,no),d(e,ea,p),d(e,Q,p),t(Q,pe),t(pe,br),g(lt,br,null),t(Q,so),t(Q,$r),t($r,oo),d(e,ta,p),d(e,X,p),t(X,de),t(de,Ar),g(mt,Ar,null),t(X,io),t(X,zr),t(zr,lo),d(e,ra,p),d(e,W,p),g(ct,W,null),t(W,mo),t(W,Y),t(Y,co),t(Y,Er),t(Er,po),t(Y,ho),t(Y,xr),t(xr,uo),t(Y,fo),t(W,go),t(W,he),g(pt,he,null),t(he,_o),t(he,Tr),t(Tr,wo),aa=!0},p:Yi,i(e){aa||(_(_e.$$.fragment,e),_(ve.$$.fragment,e),_(ye.$$.fragment,e),_(Ae.$$.fragment,e),_(ze.$$.fragment,e),_(Ee.$$.fragment,e),_(ke.$$.fragment,e),_(Ne.$$.fragment,e),_(Fe.$$.fragment,e),_(Ce.$$.fragment,e),_(Oe.$$.fragment,e),_(Re.$$.fragment,e),_(je.$$.fragment,e),_(Ue.$$.fragment,e),_(Ge.$$.fragment,e),_(Me.$$.fragment,e),_(Ve.$$.fragment,e),_(He.$$.fragment,e),_(Be.$$.fragment,e),_(Je.$$.fragment,e),_(Ke.$$.fragment,e),_(Xe.$$.fragment,e),_(Ze.$$.fragment,e),_(tt.$$.fragment,e),_(at.$$.fragment,e),_(ot.$$.fragment,e),_(it.$$.fragment,e),_(lt.$$.fragment,e),_(mt.$$.fragment,e),_(ct.$$.fragment,e),_(pt.$$.fragment,e),aa=!0)},o(e){w(_e.$$.fragment,e),w(ve.$$.fragment,e),w(ye.$$.fragment,e),w(Ae.$$.fragment,e),w(ze.$$.fragment,e),w(Ee.$$.fragment,e),w(ke.$$.fragment,e),w(Ne.$$.fragment,e),w(Fe.$$.fragment,e),w(Ce.$$.fragment,e),w(Oe.$$.fragment,e),w(Re.$$.fragment,e),w(je.$$.fragment,e),w(Ue.$$.fragment,e),w(Ge.$$.fragment,e),w(Me.$$.fragment,e),w(Ve.$$.fragment,e),w(He.$$.fragment,e),w(Be.$$.fragment,e),w(Je.$$.fragment,e),w(Ke.$$.fragment,e),w(Xe.$$.fragment,e),w(Ze.$$.fragment,e),w(tt.$$.fragment,e),w(at.$$.fragment,e),w(ot.$$.fragment,e),w(it.$$.fragment,e),w(lt.$$.fragment,e),w(mt.$$.fragment,e),w(ct.$$.fragment,e),w(pt.$$.fragment,e),aa=!1},d(e){r(S),e&&r(dt),e&&r(E),v(_e),e&&r(Lr),e&&r(ee),e&&r(Pr),e&&r(I),e&&r(Wr),e&&r(k),v(ve),e&&r(Sr),e&&r(T),v(ye),v(Ae),e&&r(Ir),e&&r(N),v(ze),e&&r(kr),e&&r(h),v(Ee),v(ke),v(Ne),v(Fe),v(Ce),v(Oe),e&&r(Nr),e&&r(F),v(Re),e&&r(Fr),e&&r(z),v(je),v(Ue),e&&r(Cr),e&&r(O),v(Ge),e&&r(Or),e&&r(R),v(Me),e&&r(Rr),e&&r(j),v(Ve),e&&r(jr),e&&r(q),v(He),e&&r(qr),e&&r(U),v(Be),e&&r(Ur),e&&r(G),v(Je),e&&r(Gr),e&&r(M),v(Ke),e&&r(Mr),e&&r(Qe),e&&r(Vr),e&&r(V),v(Xe),e&&r(Hr),e&&r(Ye),e&&r(Br),e&&r(H),v(Ze),e&&r(Jr),e&&r(et),e&&r(Kr),e&&r(B),v(tt),e&&r(Qr),e&&r(rt),e&&r(Xr),e&&r(P),v(at),e&&r(Yr),e&&r(J),v(ot),e&&r(Zr),e&&r(K),v(it),e&&r(ea),e&&r(Q),v(lt),e&&r(ta),e&&r(X),v(mt),e&&r(ra),e&&r(W),v(ct),v(pt)}}}const el={local:"optimization",sections:[{local:"transformers.AdamW",title:"AdamW (PyTorch)"},{local:"transformers.Adafactor",title:"AdaFactor (PyTorch)"},{local:"transformers.AdamWeightDecay",title:"AdamWeightDecay (TensorFlow)"},{local:"schedules",sections:[{local:"transformers.SchedulerType",title:"Learning Rate Schedules (Pytorch)"},{local:"transformers.WarmUp",title:"Warmup (TensorFlow)"}],title:"Schedules"},{local:"gradient-strategies",sections:[{local:"transformers.GradientAccumulator",title:"GradientAccumulator (TensorFlow)"}],title:"Gradient Strategies"}],title:"Optimization"};function tl(Fa,S,dt){let{fw:E}=S;return Fa.$$set=x=>{"fw"in x&&dt(0,E=x.fw)},[E]}class il extends Ji{constructor(S){super();Ki(this,S,tl,Zi,Qi,{fw:0})}}export{il as default,el as metadata};
