import{S as IV,i as zV,s as FV,e as n,k as l,w as h,t as a,M as LV,c as s,d as r,m as d,a as i,x as f,h as o,b as m,F as e,g as p,y as u,q as _,o as v,B as g}from"../../chunks/vendor-68651a14.js";import{T as Q0}from"../../chunks/Tip-3fbaa85f.js";import{D as E}from"../../chunks/Docstring-d699e906.js";import{C as j}from"../../chunks/CodeBlock-b98730f5.js";import{I as $}from"../../chunks/IconCopyLink-8ff17449.js";import"../../chunks/CopyButton-e88c769b.js";function qV(et){let w,I,k,A,ve,H,G,J,ge,re,R,ie,le,ae,de,Q,tt,be,x,M,ct,ye,q,pt,Ee,Wn,Bn,we,Vn,Yn,xe,Zn,Jn;return{c(){w=n("p"),I=a("The "),k=n("a"),A=a("Trainer"),ve=a(` class is optimized for \u{1F917} Transformers models and can have surprising behaviors
when you use it on other models. When using it on your own model, make sure:`),H=l(),G=n("ul"),J=n("li"),ge=a("your model always return tuples or subclasses of "),re=n("a"),R=a("ModelOutput"),ie=a("."),le=l(),ae=n("li"),de=a("your model can compute the loss if a "),Q=n("code"),tt=a("labels"),be=a(` argument is provided and that loss is returned as the first
element of the tuple (if your model returns tuples)`),x=l(),M=n("li"),ct=a("your model can accept multiple label arguments (use the "),ye=n("code"),q=a("label_names"),pt=a(` in your
`),Ee=n("a"),Wn=a("TrainingArguments"),Bn=a(" to indicate their name to the "),we=n("a"),Vn=a("Trainer"),Yn=a(`) but none
of them should be named `),xe=n("code"),Zn=a('"label"'),Jn=a("."),this.h()},l(W){w=s(W,"P",{});var Se=i(w);I=o(Se,"The "),k=s(Se,"A",{href:!0});var Nd=i(k);A=o(Nd,"Trainer"),Nd.forEach(r),ve=o(Se,` class is optimized for \u{1F917} Transformers models and can have surprising behaviors
when you use it on other models. When using it on your own model, make sure:`),Se.forEach(r),H=d(W),G=s(W,"UL",{});var rt=i(G);J=s(rt,"LI",{});var oe=i(J);ge=o(oe,"your model always return tuples or subclasses of "),re=s(oe,"A",{href:!0});var Id=i(re);R=o(Id,"ModelOutput"),Id.forEach(r),ie=o(oe,"."),oe.forEach(r),le=d(rt),ae=s(rt,"LI",{});var Ce=i(ae);de=o(Ce,"your model can compute the loss if a "),Q=s(Ce,"CODE",{});var zd=i(Q);tt=o(zd,"labels"),zd.forEach(r),be=o(Ce,` argument is provided and that loss is returned as the first
element of the tuple (if your model returns tuples)`),Ce.forEach(r),x=d(rt),M=s(rt,"LI",{});var ce=i(M);ct=o(ce,"your model can accept multiple label arguments (use the "),ye=s(ce,"CODE",{});var Ta=i(ye);q=o(Ta,"label_names"),Ta.forEach(r),pt=o(ce,` in your
`),Ee=s(ce,"A",{href:!0});var Fd=i(Ee);Wn=o(Fd,"TrainingArguments"),Fd.forEach(r),Bn=o(ce," to indicate their name to the "),we=s(ce,"A",{href:!0});var Ld=i(we);Vn=o(Ld,"Trainer"),Ld.forEach(r),Yn=o(ce,`) but none
of them should be named `),xe=s(ce,"CODE",{});var Qn=i(xe);Zn=o(Qn,'"label"'),Qn.forEach(r),Jn=o(ce,"."),ce.forEach(r),rt.forEach(r),this.h()},h(){m(k,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(re,"href","/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"),m(Ee,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments"),m(we,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer")},m(W,Se){p(W,w,Se),e(w,I),e(w,k),e(k,A),e(w,ve),p(W,H,Se),p(W,G,Se),e(G,J),e(J,ge),e(J,re),e(re,R),e(J,ie),e(G,le),e(G,ae),e(ae,de),e(ae,Q),e(Q,tt),e(ae,be),e(G,x),e(G,M),e(M,ct),e(M,ye),e(ye,q),e(M,pt),e(M,Ee),e(Ee,Wn),e(M,Bn),e(M,we),e(we,Vn),e(M,Yn),e(M,xe),e(xe,Zn),e(M,Jn)},d(W){W&&r(w),W&&r(H),W&&r(G)}}}function UV(et){let w,I,k,A,ve,H,G,J,ge,re,R,ie,le,ae,de,Q,tt;return{c(){w=n("p"),I=a("To use this method, you need to have provided a "),k=n("code"),A=a("model_init"),ve=a(` when initializing your
`),H=n("a"),G=a("Trainer"),J=a(`: we need to reinitialize the model at each new run. This is incompatible
with the `),ge=n("code"),re=a("optimizers"),R=a(" argument, so you need to subclass "),ie=n("a"),le=a("Trainer"),ae=a(` and override the
method `),de=n("a"),Q=a("create_optimizer_and_scheduler()"),tt=a(" for custom optimizer/scheduler."),this.h()},l(be){w=s(be,"P",{});var x=i(w);I=o(x,"To use this method, you need to have provided a "),k=s(x,"CODE",{});var M=i(k);A=o(M,"model_init"),M.forEach(r),ve=o(x,` when initializing your
`),H=s(x,"A",{href:!0});var ct=i(H);G=o(ct,"Trainer"),ct.forEach(r),J=o(x,`: we need to reinitialize the model at each new run. This is incompatible
with the `),ge=s(x,"CODE",{});var ye=i(ge);re=o(ye,"optimizers"),ye.forEach(r),R=o(x," argument, so you need to subclass "),ie=s(x,"A",{href:!0});var q=i(ie);le=o(q,"Trainer"),q.forEach(r),ae=o(x,` and override the
method `),de=s(x,"A",{href:!0});var pt=i(de);Q=o(pt,"create_optimizer_and_scheduler()"),pt.forEach(r),tt=o(x," for custom optimizer/scheduler."),x.forEach(r),this.h()},h(){m(H,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(ie,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(de,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer.create_optimizer_and_scheduler")},m(be,x){p(be,w,x),e(w,I),e(w,k),e(k,A),e(w,ve),e(w,H),e(H,G),e(w,J),e(w,ge),e(ge,re),e(w,R),e(w,ie),e(ie,le),e(w,ae),e(w,de),e(de,Q),e(w,tt)},d(be){be&&r(w)}}}function RV(et){let w,I;return{c(){w=n("p"),I=a(`If your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic
padding in a token classification task) the predictions will be padded (on the right) to allow for
concatenation into one array. The padding index is -100.`)},l(k){w=s(k,"P",{});var A=i(w);I=o(A,`If your predictions or labels have different sequence length (for instance because you\u2019re doing dynamic
padding in a token classification task) the predictions will be padded (on the right) to allow for
concatenation into one array. The padding index is -100.`),A.forEach(r)},m(k,A){p(k,w,A),e(w,I)},d(k){k&&r(w)}}}function MV(et){let w,I;return{c(){w=n("p"),I=a(`If your predictions or labels have different sequence lengths (for instance because you\u2019re doing dynamic
padding in a token classification task) the predictions will be padded (on the right) to allow for
concatenation into one array. The padding index is -100.`)},l(k){w=s(k,"P",{});var A=i(w);I=o(A,`If your predictions or labels have different sequence lengths (for instance because you\u2019re doing dynamic
padding in a token classification task) the predictions will be padded (on the right) to allow for
concatenation into one array. The padding index is -100.`),A.forEach(r)},m(k,A){p(k,w,A),e(w,I)},d(k){k&&r(w)}}}function jV(et){let w,I,k,A,ve,H,G,J,ge,re,R,ie,le,ae,de,Q,tt,be,x,M,ct,ye,q,pt,Ee,Wn,Bn,we,Vn,Yn,xe,Zn,Jn,W,Se,Nd,rt,oe,Id,Ce,zd,ce,Ta,Fd,Ld,Qn,mt,K0,qd,X0,e$,Ud,t$,r$,rb,P,ka,Fm,a$,o$,Lm,n$,s$,i$,Da,qm,l$,d$,Um,c$,p$,m$,Pa,Rm,h$,f$,Mm,u$,_$,v$,Rd,jm,g$,b$,y$,ht,Hm,E$,w$,Gm,$$,T$,Wm,k$,D$,P$,Md,Bm,A$,O$,x$,jd,Vm,S$,C$,N$,Hd,Ym,I$,z$,F$,Gd,Zm,L$,q$,U$,Wd,Jm,R$,M$,j$,Bd,Qm,H$,G$,W$,Vd,Km,B$,V$,Y$,Yd,Xm,Z$,J$,ab,Aa,ob,Oa,Q$,Zd,K$,X$,nb,Kn,sb,ft,e4,Jd,t4,r4,Qd,a4,o4,ib,pr,xa,eh,Xn,n4,th,s4,lb,b,es,i4,rh,l4,d4,ah,c4,p4,$e,Sa,oh,m4,h4,Kd,f4,u4,_4,K,nh,v4,g4,sh,b4,y4,ih,E4,w4,lh,$4,T4,dh,k4,D4,ch,P4,A4,O4,Xd,ph,x4,S4,C4,Ne,mh,N4,I4,hh,z4,F4,fh,L4,q4,uh,U4,R4,M4,Ie,_h,j4,H4,vh,G4,W4,gh,B4,V4,bh,Y4,Z4,J4,Ca,ts,Q4,rs,K4,yh,X4,e3,t3,Na,as,r3,os,a3,Eh,o3,n3,s3,ut,ns,i3,wh,l3,d3,$h,c3,p3,_t,ss,m3,Th,h3,f3,is,u3,kh,_3,v3,g3,vt,ls,b3,Dh,y3,E3,at,w3,Ph,$3,T3,Ah,k3,D3,Oh,P3,A3,O3,Ia,ds,x3,xh,S3,C3,ze,cs,N3,Sh,I3,z3,ps,F3,Ch,L3,q3,U3,Nh,R3,M3,gt,ms,j3,mr,H3,Ih,G3,W3,zh,B3,V3,Y3,Fh,Z3,J3,za,hs,Q3,fs,K3,ec,X3,e5,t5,bt,us,r5,_s,a5,Lh,o5,n5,s5,qh,i5,l5,yt,vs,d5,gs,c5,Uh,p5,m5,h5,Rh,f5,u5,Fe,bs,_5,ys,v5,Mh,g5,b5,y5,hr,E5,jh,w5,$5,Hh,T5,k5,D5,Gh,P5,A5,Et,Es,O5,Te,x5,Wh,S5,C5,Bh,N5,I5,Vh,z5,F5,Yh,L5,q5,U5,Fa,R5,La,ws,M5,$s,j5,Zh,H5,G5,W5,qa,Ts,B5,Jh,V5,Y5,Ua,ks,Z5,Ds,J5,Qh,Q5,K5,X5,wt,Ps,eT,As,tT,Kh,rT,aT,oT,Xh,nT,sT,z,Os,iT,ef,lT,dT,tf,cT,pT,rf,mT,hT,fr,fT,af,uT,_T,of,vT,gT,bT,nf,yT,ET,xs,wT,sf,lf,$T,TT,ot,df,ke,kT,cf,DT,PT,pf,AT,OT,mf,xT,ST,hf,CT,NT,IT,ff,ur,zT,uf,FT,LT,_f,qT,UT,RT,vf,tc,gf,MT,jT,HT,U,$t,bf,GT,WT,yf,BT,VT,Ef,YT,ZT,JT,wf,QT,KT,$f,XT,e6,Ss,t6,Tf,r6,a6,o6,nt,n6,kf,s6,i6,Df,l6,d6,Pf,c6,p6,m6,ne,h6,rc,f6,u6,Af,_6,v6,Of,g6,b6,xf,y6,E6,Sf,w6,$6,T6,S,k6,Cf,D6,P6,Nf,A6,O6,If,x6,S6,zf,C6,N6,Cs,I6,z6,Ff,F6,L6,Lf,q6,U6,qf,R6,M6,Uf,j6,H6,Rf,G6,W6,B6,De,V6,ac,Y6,Z6,Mf,J6,Q6,oc,K6,X6,jf,e7,t7,r7,Hf,a7,o7,Ra,Ns,n7,Gf,s7,i7,Tt,Is,l7,zs,d7,Wf,c7,p7,m7,nc,h7,Bf,f7,u7,kt,Fs,_7,Ls,v7,Vf,g7,b7,y7,qs,E7,Yf,w7,$7,T7,X,Us,k7,Zf,D7,P7,Rs,A7,Jf,O7,x7,S7,Ma,C7,Ms,N7,Qf,I7,z7,F7,_r,vr,L7,Kf,q7,U7,Xf,R7,M7,j7,js,H7,Hs,G7,eu,W7,B7,V7,Y7,gr,Z7,tu,J7,Q7,ru,K7,X7,e8,Dt,Gs,t8,br,r8,au,a8,o8,ou,n8,s8,i8,nu,l8,d8,Pt,Ws,c8,yr,p8,su,m8,h8,iu,f8,u8,_8,lu,v8,g8,ja,Bs,b8,st,y8,du,E8,w8,cu,$8,T8,pu,k8,D8,P8,Ha,Vs,A8,Ys,O8,mu,x8,S8,C8,Le,Zs,N8,Js,I8,hu,z8,F8,L8,fu,q8,U8,Qs,R8,uu,M8,j8,H8,At,Ks,G8,Xs,W8,_u,B8,V8,Y8,vu,Z8,J8,Ot,ei,Q8,gu,K8,X8,bu,e9,t9,Ga,ti,r9,yu,a9,o9,xt,ri,n9,Eu,s9,i9,wu,l9,db,Er,Wa,$u,ai,d9,Tu,c9,cb,wr,qe,oi,p9,ku,m9,h9,ni,f9,Du,u9,_9,v9,Pu,g9,b9,ee,si,y9,Au,E9,w9,ii,$9,Ou,T9,k9,D9,Ba,P9,li,A9,xu,O9,x9,S9,$r,Tr,C9,Su,N9,I9,Cu,z9,F9,L9,di,q9,ci,U9,Nu,R9,M9,j9,H9,kr,G9,Iu,W9,B9,zu,V9,Y9,pb,Dr,Va,Fu,pi,Z9,Lu,J9,mb,T,mi,Q9,qu,K9,X9,St,hi,ek,Uu,tk,rk,fi,ak,Ru,ok,nk,sk,Ct,ui,ik,Mu,lk,dk,_i,ck,ju,pk,mk,hk,Nt,vi,fk,gi,uk,Hu,_k,vk,gk,Gu,bk,yk,It,bi,Ek,yi,wk,Wu,$k,Tk,kk,Bu,Dk,Pk,zt,Ei,Ak,wi,Ok,Vu,xk,Sk,Ck,Yu,Nk,Ik,Ft,$i,zk,Ti,Fk,Zu,Lk,qk,Uk,Ju,Rk,Mk,pe,ki,jk,Qu,Hk,Gk,Di,Wk,Ku,Bk,Vk,Yk,Pi,Zk,Xu,Jk,Qk,Kk,Pr,Ar,Xk,e_,eD,tD,t_,rD,aD,oD,Ai,nD,Oi,sD,r_,iD,lD,dD,cD,Or,pD,a_,mD,hD,o_,fD,uD,_D,Lt,xi,vD,xr,gD,sc,bD,yD,ic,ED,wD,$D,n_,TD,kD,qt,Si,DD,s_,PD,AD,i_,OD,xD,Ut,Ci,SD,l_,CD,ND,d_,ID,zD,Ya,Ni,FD,Ii,LD,c_,qD,UD,RD,Ue,zi,MD,p_,jD,HD,Fi,GD,m_,WD,BD,VD,Li,YD,h_,ZD,JD,QD,Re,qi,KD,Ui,XD,f_,eP,tP,rP,Ri,aP,u_,oP,nP,sP,__,iP,lP,Za,Mi,dP,v_,cP,pP,Rt,ji,mP,g_,hP,fP,b_,uP,hb,Sr,Ja,y_,Hi,_P,E_,vP,fb,N,Gi,gP,Wi,bP,w_,yP,EP,wP,Cr,$P,lc,TP,kP,Bi,DP,PP,AP,me,Vi,OP,$_,xP,SP,Nr,CP,T_,NP,IP,k_,zP,FP,LP,Ir,qP,D_,UP,RP,P_,MP,jP,HP,Yi,GP,A_,WP,BP,VP,Qa,Zi,YP,O_,ZP,JP,Mt,Ji,QP,x_,KP,XP,zr,eA,S_,tA,rA,C_,aA,oA,nA,Ka,Qi,sA,Ki,iA,N_,lA,dA,cA,Xa,Xi,pA,I_,mA,hA,eo,el,fA,z_,uA,ub,Fr,to,F_,tl,_A,L_,vA,_b,se,rl,gA,al,bA,q_,yA,EA,wA,Lr,$A,dc,TA,kA,ol,DA,PA,AA,Pe,OA,nl,xA,U_,SA,CA,NA,R_,IA,zA,M_,FA,LA,j_,qA,UA,RA,C,MA,sl,jA,H_,HA,GA,WA,G_,BA,VA,il,YA,W_,ZA,JA,QA,B_,KA,XA,V_,eO,tO,Y_,rO,aO,ll,oO,Z_,nO,sO,iO,J_,lO,dO,Q_,cO,pO,K_,mO,hO,vb,qr,ro,X_,dl,fO,ev,uO,gb,it,cl,_O,pl,vO,tv,gO,bO,yO,Ur,EO,cc,wO,$O,ml,TO,kO,bb,Rr,ao,rv,hl,DO,av,PO,yb,he,AO,pc,OO,xO,ov,SO,CO,mc,NO,IO,nv,zO,FO,Eb,oo,LO,hc,qO,UO,wb,no,fc,sv,RO,MO,jO,uc,iv,HO,GO,$b,Me,WO,lv,BO,VO,dv,YO,ZO,_c,JO,QO,Tb,so,io,cv,KO,XO,pv,ex,tx,rx,vc,mv,ax,ox,kb,Mr,lo,hv,fl,nx,fv,sx,Db,je,ix,gc,lx,dx,uv,cx,px,_v,mx,hx,Pb,jt,fx,vv,ux,_x,bc,vx,gx,Ab,co,yc,gv,bx,yx,Ex,Ec,bv,wx,$x,Ob,He,Tx,wc,kx,Dx,yv,Px,Ax,Ev,Ox,xx,xb,te,Sx,$c,Cx,Nx,wv,Ix,zx,$v,Fx,Lx,Tv,qx,Ux,Tc,Rx,Mx,Sb,kc,jx,Cb,ul,Nb,Dc,Hx,Ib,_l,zb,Pc,Gx,Fb,vl,Lb,Ac,Wx,qb,Oc,Bx,Ub,gl,Rb,po,Vx,kv,Yx,Zx,Mb,jr,mo,Dv,bl,Jx,Pv,Qx,jb,fe,Kx,xc,Xx,eS,Av,tS,rS,Ov,aS,oS,xv,nS,sS,Hb,Ht,iS,yl,lS,dS,Sv,cS,pS,Gb,Hr,ho,Cv,El,mS,Nv,hS,Wb,fo,fS,Sc,uS,_S,Bb,Ge,vS,wl,gS,bS,$l,yS,ES,Tl,wS,$S,Vb,Cc,TS,Yb,Nc,Zb,Gr,uo,Iv,kl,kS,zv,DS,Jb,Ic,PS,Qb,Gt,AS,Dl,OS,xS,Pl,SS,CS,Kb,zc,NS,Xb,Al,ey,Fc,IS,ty,Wt,zS,Fv,FS,LS,Lv,qS,US,ry,Wr,_o,qv,Ol,RS,Uv,MS,ay,Lc,jS,oy,We,HS,Rv,GS,WS,Mv,BS,VS,jv,YS,ZS,ny,Bt,JS,Hv,QS,KS,Gv,XS,eC,sy,xl,iy,vo,tC,Sl,rC,aC,ly,Br,go,Wv,Cl,oC,Bv,nC,dy,qc,sC,cy,Nl,py,Vt,iC,Vv,lC,dC,Yv,cC,pC,my,Uc,mC,hy,Il,fy,Rc,hC,uy,bo,fC,Zv,uC,_C,_y,lt,Jv,vC,gC,Qv,bC,yC,Kv,EC,wC,vy,Mc,$C,gy,zl,by,jc,TC,yy,Be,kC,Xv,DC,PC,eg,AC,OC,tg,xC,SC,Ey,Vr,yo,rg,Fl,CC,ag,NC,wy,Yt,IC,og,zC,FC,ng,LC,qC,$y,Hc,UC,Ty,Gc,RC,ky,Eo,MC,sg,jC,HC,Dy,Ll,Py,B,GC,ig,WC,BC,lg,VC,YC,dg,ZC,JC,cg,QC,KC,pg,XC,eN,mg,tN,rN,Ay,Wc,aN,Oy,Yr,wo,hg,ql,oN,fg,nN,xy,Ve,sN,Ul,iN,lN,Bc,dN,cN,Rl,pN,mN,Sy,Ye,ug,hN,fN,_g,uN,_N,vg,vN,gN,gg,bN,Cy,Vc,yN,Ny,Ml,bg,EN,wN,Iy,Yc,$N,zy,jl,Fy,Zt,TN,yg,kN,DN,Eg,PN,AN,Ly,Hl,qy,$o,ON,wg,xN,SN,Uy,To,CN,Gl,NN,IN,Ry,ko,zN,Zc,FN,LN,My,Jc,qN,jy,Wl,$g,UN,RN,Hy,Bl,Gy,Qc,MN,Wy,Vl,By,Yl,Tg,jN,HN,Vy,Zl,Yy,Kc,GN,Zy,Jl,Jy,Xc,WN,Qy,Do,BN,Ql,VN,YN,Ky,Kl,kg,ZN,JN,Xy,Jt,QN,Dg,KN,XN,Pg,eI,tI,e2,Po,rI,Ag,aI,oI,t2,Xl,r2,ep,nI,a2,Ze,Og,sI,iI,xg,lI,dI,ed,cI,Sg,pI,mI,hI,td,fI,Cg,uI,_I,o2,rd,dt,vI,Ng,gI,bI,Ig,yI,EI,zg,wI,$I,n2,Ao,TI,Fg,kI,DI,s2,ad,i2,Zr,Lg,PI,AI,qg,OI,xI,l2,Qt,SI,Ug,CI,NI,Rg,II,zI,d2,tp,FI,c2,ue,Mg,LI,qI,jg,UI,RI,od,MI,Hg,jI,HI,GI,Jr,WI,Gg,BI,VI,Wg,YI,ZI,JI,Bg,QI,p2,rp,KI,m2,Oo,Qr,XI,Vg,ez,tz,Yg,rz,az,oz,Ae,nz,Zg,sz,iz,Jg,lz,dz,Qg,cz,pz,Kg,mz,hz,h2,Kr,xo,Xg,nd,fz,e1,uz,f2,So,_z,ap,vz,gz,u2,Xr,Co,t1,sd,bz,r1,yz,_2,No,Ez,op,wz,$z,v2,ea,Io,a1,id,Tz,o1,kz,g2,zo,Dz,np,Pz,Az,b2,ta,Fo,n1,ld,Oz,s1,xz,y2,Lo,Sz,sp,Cz,Nz,E2,ra,qo,i1,dd,Iz,l1,zz,w2,Uo,Fz,ip,Lz,qz,$2,aa,Ro,d1,cd,Uz,c1,Rz,T2,Mo,Mz,lp,jz,Hz,k2,oa,jo,p1,pd,Gz,m1,Wz,D2,Ho,Bz,dp,Vz,Yz,P2,na,Go,h1,md,Zz,f1,Jz,A2,Wo,Qz,cp,Kz,Xz,O2,sa,Bo,u1,hd,eF,_1,tF,x2,Vo,rF,pp,aF,oF,S2,ia,Yo,v1,fd,nF,g1,sF,C2,Zo,iF,mp,lF,dF,N2,la,Jo,b1,ud,cF,y1,pF,I2,Qo,mF,hp,hF,fF,z2,da,Ko,E1,_d,uF,w1,_F,F2,Xo,vF,fp,gF,bF,L2,ca,en,$1,vd,yF,T1,EF,q2,tn,wF,up,$F,TF,U2,pa,rn,k1,gd,kF,D1,DF,R2,an,PF,_p,AF,OF,M2,ma,on,P1,bd,xF,A1,SF,j2,nn,CF,vp,NF,IF,H2,ha,sn,O1,yd,zF,x1,FF,G2,fa,ln,S1,Ed,LF,C1,qF,W2,dn,UF,gp,RF,MF,B2,ua,cn,N1,wd,jF,I1,HF,V2,pn,GF,bp,WF,BF,Y2,_a,mn,z1,$d,VF,F1,YF,Z2,hn,ZF,yp,JF,QF,J2,va,fn,L1,Td,KF,q1,XF,Q2,un,eL,Ep,tL,rL,K2,ga,_n,U1,kd,aL,R1,oL,X2,vn,nL,wp,sL,iL,eE,ba,gn,M1,Dd,lL,j1,dL,tE,bn,cL,$p,pL,mL,rE,ya,yn,H1,Pd,hL,G1,fL,aE,En,uL,Tp,_L,vL,oE,Ea,wn,W1,Ad,gL,B1,bL,nE,$n,yL,kp,EL,wL,sE;return H=new $({}),Aa=new Q0({props:{warning:!0,$$slots:{default:[qV]},$$scope:{ctx:et}}}),Kn=new j({props:{code:`from torch import nn
from transformers import Trainer

class MultilabelTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get('logits')
        loss_fct = nn.BCEWithLogitsLoss()
        loss = loss_fct(logits.view(-1, self.model.config.num_labels),
                        labels.float().view(-1, self.model.config.num_labels))
        return (loss, outputs) if return_outputs else loss,`,highlighted:`<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

<span class="hljs-keyword">class</span> <span class="hljs-title class_">MultilabelTrainer</span>(<span class="hljs-title class_ inherited__">Trainer</span>):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">self, model, inputs, return_outputs=<span class="hljs-literal">False</span></span>):
        labels = inputs.get(<span class="hljs-string">&quot;labels&quot;</span>)
        outputs = model(**inputs)
        logits = outputs.get(<span class="hljs-string">&#x27;logits&#x27;</span>)
        loss_fct = nn.BCEWithLogitsLoss()
        loss = loss_fct(logits.view(-<span class="hljs-number">1</span>, self.model.config.num_labels),
                        labels.<span class="hljs-built_in">float</span>().view(-<span class="hljs-number">1</span>, self.model.config.num_labels))
        <span class="hljs-keyword">return</span> (loss, outputs) <span class="hljs-keyword">if</span> return_outputs <span class="hljs-keyword">else</span> loss`}}),Xn=new $({}),es=new E({props:{name:"class transformers.Trainer",anchor:"transformers.Trainer",parameters:[{name:"model",val:": typing.Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None"},{name:"args",val:": TrainingArguments = None"},{name:"data_collator",val:": typing.Optional[DataCollator] = None"},{name:"train_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"tokenizer",val:": typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None"},{name:"model_init",val:": typing.Callable[[], transformers.modeling_utils.PreTrainedModel] = None"},{name:"compute_metrics",val:": typing.Union[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict], NoneType] = None"},{name:"callbacks",val:": typing.Optional[typing.List[transformers.trainer_callback.TrainerCallback]] = None"},{name:"optimizers",val:": typing.Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None)"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L197",parametersDescription:[{anchor:"transformers.Trainer.model",description:`<strong>model</strong> (<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a> or <code>torch.nn.Module</code>, <em>optional</em>) &#x2014;
The model to train, evaluate or use for predictions. If not provided, a <code>model_init</code> must be passed.`,name:"model"}]}}),ts=new E({props:{name:"add_callback",anchor:"transformers.Trainer.add_callback",parameters:[{name:"callback",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L491",parametersDescription:[{anchor:"transformers.Trainer.add_callback.callback",description:`<strong>callback</strong> (<code>type</code> or <code>TrainerCallback</code>) &#x2014;
A <code>TrainerCallback</code> class or an instance of a <code>TrainerCallback</code>.
In the first case, will instantiate a member of that class.`,name:"callback"}]}}),as=new E({props:{name:"autocast_smart_context_manager",anchor:"transformers.Trainer.autocast_smart_context_manager",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1842"}}),ns=new E({props:{name:"compute_loss",anchor:"transformers.Trainer.compute_loss",parameters:[{name:"model",val:""},{name:"inputs",val:""},{name:"return_outputs",val:" = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1906"}}),ss=new E({props:{name:"create_optimizer",anchor:"transformers.Trainer.create_optimizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L801"}}),ls=new E({props:{name:"create_optimizer_and_scheduler",anchor:"transformers.Trainer.create_optimizer_and_scheduler",parameters:[{name:"num_training_steps",val:": int"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L790"}}),ds=new E({props:{name:"create_scheduler",anchor:"transformers.Trainer.create_scheduler",parameters:[{name:"num_training_steps",val:": int"},{name:"optimizer",val:": Optimizer = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L846",parametersDescription:[{anchor:"transformers.Trainer.create_scheduler.num_training_steps",description:"<strong>num_training_steps</strong> (int) &#x2014; The number of training steps to do.",name:"num_training_steps"}]}}),cs=new E({props:{name:"evaluate",anchor:"transformers.Trainer.evaluate",parameters:[{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L2114",parametersDescription:[{anchor:"transformers.Trainer.evaluate.eval_dataset",description:`<strong>eval_dataset</strong> (<code>Dataset</code>, <em>optional</em>) &#x2014;
Pass a dataset if you wish to override <code>self.eval_dataset</code>. If it is an <code>datasets.Dataset</code>,
columns not accepted by the <code>model.forward()</code> method are automatically removed. It must implement the
<code>__len__</code> method.`,name:"eval_dataset"},{anchor:"transformers.Trainer.evaluate.ignore_keys",description:`<strong>ignore_keys</strong> (<code>Lst[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"transformers.Trainer.evaluate.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;eval&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;eval_bleu&#x201D; if the prefix is &#x201C;eval&#x201D; (default)`,name:"metric_key_prefix"}],returnDescription:`
<p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The
dictionary also contains the epoch number which comes from the training state.</p>
`}}),ms=new E({props:{name:"evaluation_loop",anchor:"transformers.Trainer.evaluation_loop",parameters:[{name:"dataloader",val:": DataLoader"},{name:"description",val:": str"},{name:"prediction_loss_only",val:": typing.Optional[bool] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L2240"}}),hs=new E({props:{name:"floating_point_ops",anchor:"transformers.Trainer.floating_point_ops",parameters:[{name:"inputs",val:": typing.Dict[str, typing.Union[torch.Tensor, typing.Any]]"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L2556",parametersDescription:[{anchor:"transformers.Trainer.floating_point_ops.inputs",description:`<strong>inputs</strong> (<code>Dict[str, Union[torch.Tensor, Any]]</code>) &#x2014;
The inputs and targets of the model.`,name:"inputs"}],returnDescription:`
<p>The number of floating-point operations.</p>
`,returnType:`
<p><code>int</code></p>
`}}),us=new E({props:{name:"get_eval_dataloader",anchor:"transformers.Trainer.get_eval_dataloader",parameters:[{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L700",parametersDescription:[{anchor:"transformers.Trainer.get_eval_dataloader.eval_dataset",description:`<strong>eval_dataset</strong> (<code>torch.utils.data.Dataset</code>, <em>optional</em>) &#x2014;
If provided, will override <code>self.eval_dataset</code>. If it is an <code>datasets.Dataset</code>, columns not
accepted by the <code>model.forward()</code> method are automatically removed. It must implement <code>__len__</code>.`,name:"eval_dataset"}]}}),vs=new E({props:{name:"get_test_dataloader",anchor:"transformers.Trainer.get_test_dataloader",parameters:[{name:"test_dataset",val:": Dataset"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L747",parametersDescription:[{anchor:"transformers.Trainer.get_test_dataloader.test_dataset",description:`<strong>test_dataset</strong> (<code>torch.utils.data.Dataset</code>, <em>optional</em>) &#x2014;
The test dataset to use. If it is an <code>datasets.Dataset</code>, columns not accepted by the
<code>model.forward()</code> method are automatically removed. It must implement <code>__len__</code>.`,name:"test_dataset"}]}}),bs=new E({props:{name:"get_train_dataloader",anchor:"transformers.Trainer.get_train_dataloader",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L625"}}),Es=new E({props:{name:"hyperparameter_search",anchor:"transformers.Trainer.hyperparameter_search",parameters:[{name:"hp_space",val:": typing.Union[typing.Callable[[ForwardRef('optuna.Trial')], typing.Dict[str, float]], NoneType] = None"},{name:"compute_objective",val:": typing.Union[typing.Callable[[typing.Dict[str, float]], float], NoneType] = None"},{name:"n_trials",val:": int = 20"},{name:"direction",val:": str = 'minimize'"},{name:"backend",val:": typing.Union[ForwardRef('str'), transformers.trainer_utils.HPSearchBackend, NoneType] = None"},{name:"hp_name",val:": typing.Union[typing.Callable[[ForwardRef('optuna.Trial')], str], NoneType] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1705",parametersDescription:[{anchor:"transformers.Trainer.hyperparameter_search.hp_space",description:`<strong>hp_space</strong> (<code>Callable[[&quot;optuna.Trial&quot;], Dict[str, float]]</code>, <em>optional</em>) &#x2014;
A function that defines the hyperparameter search space. Will default to
<code>default_hp_space_optuna()</code> or
<code>default_hp_space_ray()</code> or
<code>default_hp_space_sigopt()</code> depending on your backend.`,name:"hp_space"},{anchor:"transformers.Trainer.hyperparameter_search.compute_objective",description:`<strong>compute_objective</strong> (<code>Callable[[Dict[str, float]], float]</code>, <em>optional</em>) &#x2014;
A function computing the objective to minimize or maximize from the metrics returned by the
<code>evaluate</code> method. Will default to <code>default_compute_objective()</code>.`,name:"compute_objective"},{anchor:"transformers.Trainer.hyperparameter_search.n_trials",description:`<strong>n_trials</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
The number of trial runs to test.`,name:"n_trials"},{anchor:"transformers.Trainer.hyperparameter_search.direction(str,",description:`<strong>direction(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;minimize&quot;</code>) &#x2014;
Whether to optimize greater or lower objects. Can be <code>&quot;minimize&quot;</code> or <code>&quot;maximize&quot;</code>, you should
pick <code>&quot;minimize&quot;</code> when optimizing the validation loss, <code>&quot;maximize&quot;</code> when optimizing one or
several metrics.`,name:"direction(str,"},{anchor:"transformers.Trainer.hyperparameter_search.backend(str",description:`<strong>backend(<code>str</code></strong> or <code>HPSearchBackend</code>, <em>optional</em>) &#x2014;
The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending
on which one is installed. If all are installed, will default to optuna.
kwargs &#x2014;
Additional keyword arguments passed along to <code>optuna.create_study</code> or <code>ray.tune.run</code>. For
more information see:</p>
<ul>
<li>the documentation of <a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html" rel="nofollow">optuna.create_study</a></li>
<li>the documentation of <a href="https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run" rel="nofollow">tune.run</a></li>
<li>the documentation of <a href="https://app.sigopt.com/docs/endpoints/experiments/create" rel="nofollow">sigopt</a></li>
</ul>`,name:"backend(str"}],returnDescription:`
<p>All the information about the best run.</p>
`,returnType:`
<p><code>transformers.trainer_utils.BestRun</code></p>
`}}),Fa=new Q0({props:{warning:!0,$$slots:{default:[UV]},$$scope:{ctx:et}}}),ws=new E({props:{name:"init_git_repo",anchor:"transformers.Trainer.init_git_repo",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L2574"}}),Ts=new E({props:{name:"is_local_process_zero",anchor:"transformers.Trainer.is_local_process_zero",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1930"}}),ks=new E({props:{name:"is_world_process_zero",anchor:"transformers.Trainer.is_world_process_zero",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1937"}}),Ps=new E({props:{name:"log",anchor:"transformers.Trainer.log",parameters:[{name:"logs",val:": typing.Dict[str, float]"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1796",parametersDescription:[{anchor:"transformers.Trainer.log.logs",description:`<strong>logs</strong> (<code>Dict[str, float]</code>) &#x2014;
The values to log.`,name:"logs"}]}}),Os=new E({props:{name:"log_metrics",anchor:"transformers.trainer_pt_utils.log_metrics",parameters:[{name:"split",val:""},{name:"metrics",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_pt_utils.py#L858",parametersDescription:[{anchor:"transformers.trainer_pt_utils.log_metrics.split",description:`<strong>split</strong> (<code>str</code>) &#x2014;
Mode/split name: one of <code>train</code>, <code>eval</code>, <code>test</code>`,name:"split"},{anchor:"transformers.trainer_pt_utils.log_metrics.metrics",description:`<strong>metrics</strong> (<code>Dict[str, float]</code>) &#x2014;
The metrics returned from train/evaluate/predictmetrics: metrics dict`,name:"metrics"}]}}),xs=new j({props:{code:`init_mem_cpu_alloc_delta   =     1301MB
init_mem_cpu_peaked_delta  =      154MB
init_mem_gpu_alloc_delta   =      230MB
init_mem_gpu_peaked_delta  =        0MB
train_mem_cpu_alloc_delta  =     1345MB
train_mem_cpu_peaked_delta =        0MB
train_mem_gpu_alloc_delta  =      693MB
train_mem_gpu_peaked_delta =        7MB,`,highlighted:`init_mem_cpu_alloc_delta   =     1301MB
init_mem_cpu_peaked_delta  =      154MB
init_mem_gpu_alloc_delta   =      230MB
init_mem_gpu_peaked_delta  =        0MB
train_mem_cpu_alloc_delta  =     1345MB
train_mem_cpu_peaked_delta =        0MB
train_mem_gpu_alloc_delta  =      693MB
train_mem_gpu_peaked_delta =        7MB`}}),Ns=new E({props:{name:"metrics_format",anchor:"transformers.trainer_pt_utils.metrics_format",parameters:[{name:"metrics",val:": typing.Dict[str, float]"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_pt_utils.py#L832",parametersDescription:[{anchor:"transformers.trainer_pt_utils.metrics_format.metrics",description:`<strong>metrics</strong> (<code>Dict[str, float]</code>) &#x2014;
The metrics returned from train/evaluate/predict`,name:"metrics"}],returnDescription:`
<p>The reformatted metrics</p>
`,returnType:`
<p>metrics (<code>Dict[str, float]</code>)</p>
`}}),Is=new E({props:{name:"num_examples",anchor:"transformers.Trainer.num_examples",parameters:[{name:"dataloader",val:": DataLoader"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L863"}}),Fs=new E({props:{name:"pop_callback",anchor:"transformers.Trainer.pop_callback",parameters:[{name:"callback",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L502",parametersDescription:[{anchor:"transformers.Trainer.pop_callback.callback",description:`<strong>callback</strong> (<code>type</code> or <code>TrainerCallback</code>) &#x2014;
A <code>TrainerCallback</code> class or an instance of a <code>TrainerCallback</code>.
In the first case, will pop the first member of that class found in the list of callbacks.`,name:"callback"}],returnDescription:`
<p>The callback removed, if found.</p>
`,returnType:`
<p><code>TrainerCallback</code></p>
`}}),Us=new E({props:{name:"predict",anchor:"transformers.Trainer.predict",parameters:[{name:"test_dataset",val:": Dataset"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'test'"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L2183",parametersDescription:[{anchor:"transformers.Trainer.predict.test_dataset",description:`<strong>test_dataset</strong> (<code>Dataset</code>) &#x2014;
Dataset to run the predictions on. If it is an <code>datasets.Dataset</code>, columns not accepted by the
<code>model.forward()</code> method are automatically removed. Has to implement the method <code>__len__</code>`,name:"test_dataset"},{anchor:"transformers.Trainer.predict.ignore_keys",description:`<strong>ignore_keys</strong> (<code>Lst[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"transformers.Trainer.predict.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;test&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;test_bleu&#x201D; if the prefix is &#x201C;test&#x201D; (default)`,name:"metric_key_prefix"}]}}),Ma=new Q0({props:{$$slots:{default:[RV]},$$scope:{ctx:et}}}),Gs=new E({props:{name:"prediction_loop",anchor:"transformers.Trainer.prediction_loop",parameters:[{name:"dataloader",val:": DataLoader"},{name:"description",val:": str"},{name:"prediction_loss_only",val:": typing.Optional[bool] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L2737"}}),Ws=new E({props:{name:"prediction_step",anchor:"transformers.Trainer.prediction_step",parameters:[{name:"model",val:": Module"},{name:"inputs",val:": typing.Dict[str, typing.Union[torch.Tensor, typing.Any]]"},{name:"prediction_loss_only",val:": bool"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L2459",parametersDescription:[{anchor:"transformers.Trainer.prediction_step.model",description:`<strong>model</strong> (<code>nn.Module</code>) &#x2014;
The model to evaluate.`,name:"model"},{anchor:"transformers.Trainer.prediction_step.inputs",description:`<strong>inputs</strong> (<code>Dict[str, Union[torch.Tensor, Any]]</code>) &#x2014;
The inputs and targets of the model.</p>
<p>The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
argument <code>labels</code>. Check your model&#x2019;s documentation for all accepted arguments.`,name:"inputs"},{anchor:"transformers.Trainer.prediction_step.prediction_loss_only",description:`<strong>prediction_loss_only</strong> (<code>bool</code>) &#x2014;
Whether or not to return the loss only.`,name:"prediction_loss_only"},{anchor:"transformers.Trainer.prediction_step.ignore_keys",description:`<strong>ignore_keys</strong> (<code>Lst[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"}],returnDescription:`
<p>A tuple with the loss,
logits and labels (each being optional).</p>
`,returnType:`
<p>Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]</p>
`}}),Bs=new E({props:{name:"push_to_hub",anchor:"transformers.Trainer.push_to_hub",parameters:[{name:"commit_message",val:": typing.Optional[str] = 'End of training'"},{name:"blocking",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L2688",parametersDescription:[{anchor:"transformers.Trainer.push_to_hub.commit_message",description:`<strong>commit_message</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;End of training&quot;</code>) &#x2014;
Message to commit while pushing.`,name:"commit_message"},{anchor:"transformers.Trainer.push_to_hub.blocking",description:`<strong>blocking</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether the function should return only when the <code>git push</code> has finished.
kwargs &#x2014;
Additional keyword arguments passed along to <code>create_model_card()</code>.`,name:"blocking"}],returnDescription:`
<p>The url of the commit of your model in the given repository if <code>blocking=False</code>, a tuple with the url
of the commit and an object to track the progress of the commit if <code>blocking=True</code></p>
`}}),Vs=new E({props:{name:"remove_callback",anchor:"transformers.Trainer.remove_callback",parameters:[{name:"callback",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L518",parametersDescription:[{anchor:"transformers.Trainer.remove_callback.callback",description:`<strong>callback</strong> (<code>type</code> or <code>TrainerCallback</code>) &#x2014;
A <code>TrainerCallback</code> class or an instance of a <code>TrainerCallback</code>.
In the first case, will remove the first member of that class found in the list of callbacks.`,name:"callback"}]}}),Zs=new E({props:{name:"save_metrics",anchor:"transformers.trainer_pt_utils.save_metrics",parameters:[{name:"split",val:""},{name:"metrics",val:""},{name:"combined",val:" = True"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_pt_utils.py#L947",parametersDescription:[{anchor:"transformers.trainer_pt_utils.save_metrics.split",description:`<strong>split</strong> (<code>str</code>) &#x2014;
Mode/split name: one of <code>train</code>, <code>eval</code>, <code>test</code>, <code>all</code>`,name:"split"},{anchor:"transformers.trainer_pt_utils.save_metrics.metrics",description:`<strong>metrics</strong> (<code>Dict[str, float]</code>) &#x2014;
The metrics returned from train/evaluate/predict`,name:"metrics"},{anchor:"transformers.trainer_pt_utils.save_metrics.combined",description:`<strong>combined</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Creates combined metrics by updating <code>all_results.json</code> with metrics of this call`,name:"combined"}]}}),Ks=new E({props:{name:"save_model",anchor:"transformers.Trainer.save_model",parameters:[{name:"output_dir",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1949"}}),ei=new E({props:{name:"save_state",anchor:"transformers.trainer_pt_utils.save_state",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_pt_utils.py#L985"}}),ti=new E({props:{name:"train",anchor:"transformers.Trainer.train",parameters:[{name:"resume_from_checkpoint",val:": typing.Union[str, bool, NoneType] = None"},{name:"trial",val:": typing.Union[ForwardRef('optuna.Trial'), typing.Dict[str, typing.Any]] = None"},{name:"ignore_keys_for_eval",val:": typing.Optional[typing.List[str]] = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1019",parametersDescription:[{anchor:"transformers.Trainer.train.resume_from_checkpoint",description:`<strong>resume_from_checkpoint</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
If a <code>str</code>, local path to a saved checkpoint as saved by a previous instance of
<a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>. If a <code>bool</code> and equals <em>True</em>, load the last checkpoint in
<em>args.output_dir</em> as saved by a previous instance of <a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>. If present,
training will resume from the model/optimizer/scheduler states loaded here.`,name:"resume_from_checkpoint"},{anchor:"transformers.Trainer.train.trial",description:`<strong>trial</strong> (<code>optuna.Trial</code> or <code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The trial run or the hyperparameter dictionary for hyperparameter search.`,name:"trial"},{anchor:"transformers.Trainer.train.ignore_keys_for_eval",description:`<strong>ignore_keys_for_eval</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions for evaluation during the training.
kwargs &#x2014;
Additional keyword arguments used to hide deprecated arguments`,name:"ignore_keys_for_eval"}]}}),ri=new E({props:{name:"training_step",anchor:"transformers.Trainer.training_step",parameters:[{name:"model",val:": Module"},{name:"inputs",val:": typing.Dict[str, typing.Union[torch.Tensor, typing.Any]]"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py#L1857",parametersDescription:[{anchor:"transformers.Trainer.training_step.model",description:`<strong>model</strong> (<code>nn.Module</code>) &#x2014;
The model to train.`,name:"model"},{anchor:"transformers.Trainer.training_step.inputs",description:`<strong>inputs</strong> (<code>Dict[str, Union[torch.Tensor, Any]]</code>) &#x2014;
The inputs and targets of the model.</p>
<p>The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
argument <code>labels</code>. Check your model&#x2019;s documentation for all accepted arguments.`,name:"inputs"}],returnDescription:`
<p>The tensor with training loss on this batch.</p>
`,returnType:`
<p><code>torch.Tensor</code></p>
`}}),ai=new $({}),oi=new E({props:{name:"evaluate",anchor:"transformers.Seq2SeqTrainer.evaluate",parameters:[{name:"eval_dataset",val:": typing.Optional[torch.utils.data.dataset.Dataset] = None"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"num_beams",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_seq2seq.py#L31",parametersDescription:[{anchor:"transformers.Seq2SeqTrainer.evaluate.eval_dataset",description:`<strong>eval_dataset</strong> (<code>Dataset</code>, <em>optional</em>) &#x2014;
Pass a dataset if you wish to override <code>self.eval_dataset</code>. If it is an <code>datasets.Dataset</code>,
columns not accepted by the <code>model.forward()</code> method are automatically removed. It must implement the
<code>__len__</code> method.`,name:"eval_dataset"},{anchor:"transformers.Seq2SeqTrainer.evaluate.ignore_keys",description:`<strong>ignore_keys</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"transformers.Seq2SeqTrainer.evaluate.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;eval&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;eval_bleu&#x201D; if the prefix is <code>&quot;eval&quot;</code> (default)`,name:"metric_key_prefix"},{anchor:"transformers.Seq2SeqTrainer.evaluate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum target length to use when predicting with the generate method.`,name:"max_length"},{anchor:"transformers.Seq2SeqTrainer.evaluate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of beams for beam search that will be used when predicting with the generate method. 1 means no
beam search.`,name:"num_beams"}],returnDescription:`
<p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The
dictionary also contains the epoch number which comes from the training state.</p>
`}}),si=new E({props:{name:"predict",anchor:"transformers.Seq2SeqTrainer.predict",parameters:[{name:"test_dataset",val:": Dataset"},{name:"ignore_keys",val:": typing.Optional[typing.List[str]] = None"},{name:"metric_key_prefix",val:": str = 'eval'"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"num_beams",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_seq2seq.py#L72",parametersDescription:[{anchor:"transformers.Seq2SeqTrainer.predict.test_dataset",description:`<strong>test_dataset</strong> (<code>Dataset</code>) &#x2014;
Dataset to run the predictions on. If it is an <code>datasets.Dataset</code>, columns not accepted by the
<code>model.forward()</code> method are automatically removed. Has to implement the method <code>__len__</code>`,name:"test_dataset"},{anchor:"transformers.Seq2SeqTrainer.predict.ignore_keys",description:`<strong>ignore_keys</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
A list of keys in the output of your model (if it is a dictionary) that should be ignored when
gathering predictions.`,name:"ignore_keys"},{anchor:"transformers.Seq2SeqTrainer.predict.metric_key_prefix",description:`<strong>metric_key_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;eval&quot;</code>) &#x2014;
An optional prefix to be used as the metrics key prefix. For example the metrics &#x201C;bleu&#x201D; will be named
&#x201C;eval_bleu&#x201D; if the prefix is <code>&quot;eval&quot;</code> (default)`,name:"metric_key_prefix"},{anchor:"transformers.Seq2SeqTrainer.predict.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum target length to use when predicting with the generate method.`,name:"max_length"},{anchor:"transformers.Seq2SeqTrainer.predict.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of beams for beam search that will be used when predicting with the generate method. 1 means no
beam search.`,name:"num_beams"}]}}),Ba=new Q0({props:{$$slots:{default:[MV]},$$scope:{ctx:et}}}),pi=new $({}),mi=new E({props:{name:"class transformers.TFTrainer",anchor:"transformers.TFTrainer",parameters:[{name:"model",val:": TFPreTrainedModel"},{name:"args",val:": TFTrainingArguments"},{name:"train_dataset",val:": typing.Optional[tensorflow.python.data.ops.dataset_ops.DatasetV2] = None"},{name:"eval_dataset",val:": typing.Optional[tensorflow.python.data.ops.dataset_ops.DatasetV2] = None"},{name:"compute_metrics",val:": typing.Union[typing.Callable[[transformers.trainer_utils.EvalPrediction], typing.Dict], NoneType] = None"},{name:"tb_writer",val:": typing.Optional[tensorflow.python.ops.summary_ops_v2.SummaryWriter] = None"},{name:"optimizers",val:": typing.Tuple[keras.optimizer_v2.optimizer_v2.OptimizerV2, keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule] = (None, None)"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L51",parametersDescription:[{anchor:"transformers.TFTrainer.model",description:`<strong>model</strong> (<a href="/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>) &#x2014;
The model to train, evaluate or use for predictions.`,name:"model"},{anchor:"transformers.TFTrainer.args",description:`<strong>args</strong> (<a href="/docs/transformers/master/en/main_classes/trainer#transformers.TFTrainingArguments">TFTrainingArguments</a>) &#x2014;
The arguments to tweak training.`,name:"args"},{anchor:"transformers.TFTrainer.train_dataset",description:`<strong>train_dataset</strong> (<code>Dataset</code>, <em>optional</em>) &#x2014;
The dataset to use for training. The dataset should yield tuples of <code>(features, labels)</code> where
<code>features</code> is a dict of input features and <code>labels</code> is the labels. If <code>labels</code> is a tensor, the loss
is calculated by the model by calling <code>model(features, labels=labels)</code>. If <code>labels</code> is a dict, such as
when using a QuestionAnswering head model with multiple targets, the loss is instead calculated by calling
<code>model(features, **labels)</code>.`,name:"train_dataset"},{anchor:"transformers.TFTrainer.eval_dataset",description:`<strong>eval_dataset</strong> (<code>Dataset</code>, <em>optional</em>) &#x2014;
The dataset to use for evaluation. The dataset should yield tuples of <code>(features, labels)</code> where
<code>features</code> is a dict of input features and <code>labels</code> is the labels. If <code>labels</code> is a tensor, the loss
is calculated by the model by calling <code>model(features, labels=labels)</code>. If <code>labels</code> is a dict, such as
when using a QuestionAnswering head model with multiple targets, the loss is instead calculated by calling
<code>model(features, **labels)</code>.`,name:"eval_dataset"},{anchor:"transformers.TFTrainer.compute_metrics",description:`<strong>compute_metrics</strong> (<code>Callable[[EvalPrediction], Dict]</code>, <em>optional</em>) &#x2014;
The function that will be used to compute metrics at evaluation. Must take a
<a href="/docs/transformers/master/en/internal/trainer_utils#transformers.EvalPrediction">EvalPrediction</a> and return a dictionary string to metric values.`,name:"compute_metrics"},{anchor:"transformers.TFTrainer.tb_writer",description:`<strong>tb_writer</strong> (<code>tf.summary.SummaryWriter</code>, <em>optional</em>) &#x2014;
Object to write to TensorBoard.`,name:"tb_writer"},{anchor:"transformers.TFTrainer.optimizers",description:`<strong>optimizers</strong> (<code>Tuple[tf.keras.optimizers.Optimizer, tf.keras.optimizers.schedules.LearningRateSchedule]</code>, <em>optional</em>) &#x2014;
A tuple containing the optimizer and the scheduler to use. The optimizer default to an instance of
<code>tf.keras.optimizers.Adam</code> if <code>args.weight_decay_rate</code> is 0 else an instance of
<a href="/docs/transformers/master/en/main_classes/optimizer_schedules#transformers.AdamWeightDecay">AdamWeightDecay</a>. The scheduler will default to an instance of
<code>tf.keras.optimizers.schedules.PolynomialDecay</code> if <code>args.num_warmup_steps</code> is 0 else an
instance of <a href="/docs/transformers/master/en/main_classes/optimizer_schedules#transformers.WarmUp">WarmUp</a>.`,name:"optimizers"}]}}),hi=new E({props:{name:"create_optimizer_and_scheduler",anchor:"transformers.TFTrainer.create_optimizer_and_scheduler",parameters:[{name:"num_training_steps",val:": int"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L222"}}),ui=new E({props:{name:"evaluate",anchor:"transformers.TFTrainer.evaluate",parameters:[{name:"eval_dataset",val:": typing.Optional[tensorflow.python.data.ops.dataset_ops.DatasetV2] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L415",parametersDescription:[{anchor:"transformers.TFTrainer.evaluate.eval_dataset",description:`<strong>eval_dataset</strong> (<code>Dataset</code>, <em>optional</em>) &#x2014;
Pass a dataset if you wish to override <code>self.eval_dataset</code>. The dataset should yield tuples of
<code>(features, labels)</code> where <code>features</code> is a dict of input features and <code>labels</code> is the labels. If
<code>labels</code> is a tensor, the loss is calculated by the model by calling <code>model(features, labels=labels)</code>. If <code>labels</code> is a dict, such as when using a QuestionAnswering head model with
multiple targets, the loss is instead calculated by calling <code>model(features, **labels)</code>.`,name:"eval_dataset"}],returnDescription:`
<p>A dictionary containing the evaluation loss and the potential metrics computed from the predictions.</p>
`}}),vi=new E({props:{name:"get_eval_tfdataset",anchor:"transformers.TFTrainer.get_eval_tfdataset",parameters:[{name:"eval_dataset",val:": typing.Optional[tensorflow.python.data.ops.dataset_ops.DatasetV2] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L164",parametersDescription:[{anchor:"transformers.TFTrainer.get_eval_tfdataset.eval_dataset",description:`<strong>eval_dataset</strong> (<code>Dataset</code>, <em>optional</em>) &#x2014;
If provided, will override <em>self.eval_dataset</em>. The dataset should yield tuples of <code>(features, labels)</code> where <code>features</code> is a dict of input features and <code>labels</code> is the labels. If <code>labels</code> is
a tensor, the loss is calculated by the model by calling <code>model(features, labels=labels)</code>. If
<code>labels</code> is a dict, such as when using a QuestionAnswering head model with multiple targets, the loss
is instead calculated by calling <code>model(features, **labels)</code>.`,name:"eval_dataset"}]}}),bi=new E({props:{name:"get_test_tfdataset",anchor:"transformers.TFTrainer.get_test_tfdataset",parameters:[{name:"test_dataset",val:": DatasetV2"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L197",parametersDescription:[{anchor:"transformers.TFTrainer.get_test_tfdataset.test_dataset",description:`<strong>test_dataset</strong> (<code>Dataset</code>) &#x2014;
The dataset to use. The dataset should yield tuples of <code>(features, labels)</code> where <code>features</code> is a
dict of input features and <code>labels</code> is the labels. If <code>labels</code> is a tensor, the loss is calculated
by the model by calling <code>model(features, labels=labels)</code>. If <code>labels</code> is a dict, such as when using
a QuestionAnswering head model with multiple targets, the loss is instead calculated by calling
<code>model(features, **labels)</code>.`,name:"test_dataset"}]}}),Ei=new E({props:{name:"get_train_tfdataset",anchor:"transformers.TFTrainer.get_train_tfdataset",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L140"}}),$i=new E({props:{name:"log",anchor:"transformers.TFTrainer.log",parameters:[{name:"logs",val:": typing.Dict[str, float]"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L383",parametersDescription:[{anchor:"transformers.TFTrainer.log.logs",description:`<strong>logs</strong> (<code>Dict[str, float]</code>) &#x2014;
The values to log.`,name:"logs"}]}}),ki=new E({props:{name:"predict",anchor:"transformers.TFTrainer.predict",parameters:[{name:"test_dataset",val:": DatasetV2"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L762",parametersDescription:[{anchor:"transformers.TFTrainer.predict.test_dataset",description:`<strong>test_dataset</strong> (<code>Dataset</code>) &#x2014;
Dataset to run the predictions on. The dataset should yield tuples of <code>(features, labels)</code> where
<code>features</code> is a dict of input features and <code>labels</code> is the labels. If <code>labels</code> is a tensor, the
loss is calculated by the model by calling <code>model(features, labels=labels)</code>. If <code>labels</code> is a dict,
such as when using a QuestionAnswering head model with multiple targets, the loss is instead calculated
by calling <code>model(features, **labels)</code>`,name:"test_dataset"}]}}),xi=new E({props:{name:"prediction_loop",anchor:"transformers.TFTrainer.prediction_loop",parameters:[{name:"dataset",val:": DatasetV2"},{name:"steps",val:": int"},{name:"num_examples",val:": int"},{name:"description",val:": str"},{name:"prediction_loss_only",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L296"}}),Si=new E({props:{name:"prediction_step",anchor:"transformers.TFTrainer.prediction_step",parameters:[{name:"features",val:": Tensor"},{name:"labels",val:": Tensor"},{name:"nb_instances_in_global_batch",val:": Tensor"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L443"}}),Ci=new E({props:{name:"run_model",anchor:"transformers.TFTrainer.run_model",parameters:[{name:"features",val:""},{name:"labels",val:""},{name:"training",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L732",parametersDescription:[{anchor:"transformers.TFTrainer.run_model.features",description:"<strong>features</strong> (<code>tf.Tensor</code>) &#x2014; A batch of input features.",name:"features"},{anchor:"transformers.TFTrainer.run_model.labels",description:"<strong>labels</strong> (<code>tf.Tensor</code>) &#x2014; A batch of labels.",name:"labels"},{anchor:"transformers.TFTrainer.run_model.training",description:"<strong>training</strong> (<code>bool</code>) &#x2014; Whether or not to run the model in training mode.",name:"training"}],returnDescription:`
<p>The loss and logits.</p>
`,returnType:`
<p>A tuple of two <code>tf.Tensor</code></p>
`}}),Ni=new E({props:{name:"save_model",anchor:"transformers.TFTrainer.save_model",parameters:[{name:"output_dir",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L788"}}),zi=new E({props:{name:"setup_comet",anchor:"transformers.TFTrainer.setup_comet",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L266"}}),qi=new E({props:{name:"setup_wandb",anchor:"transformers.TFTrainer.setup_wandb",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L247"}}),Mi=new E({props:{name:"train",anchor:"transformers.TFTrainer.train",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L468"}}),ji=new E({props:{name:"training_step",anchor:"transformers.TFTrainer.training_step",parameters:[{name:"features",val:""},{name:"labels",val:""},{name:"nb_instances_in_global_batch",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_tf.py#L625"}}),Hi=new $({}),Gi=new E({props:{name:"class transformers.TrainingArguments",anchor:"transformers.TrainingArguments",parameters:[{name:"output_dir",val:": str"},{name:"overwrite_output_dir",val:": bool = False"},{name:"do_train",val:": bool = False"},{name:"do_eval",val:": bool = False"},{name:"do_predict",val:": bool = False"},{name:"evaluation_strategy",val:": IntervalStrategy = 'no'"},{name:"prediction_loss_only",val:": bool = False"},{name:"per_device_train_batch_size",val:": int = 8"},{name:"per_device_eval_batch_size",val:": int = 8"},{name:"per_gpu_train_batch_size",val:": typing.Optional[int] = None"},{name:"per_gpu_eval_batch_size",val:": typing.Optional[int] = None"},{name:"gradient_accumulation_steps",val:": int = 1"},{name:"eval_accumulation_steps",val:": typing.Optional[int] = None"},{name:"learning_rate",val:": float = 5e-05"},{name:"weight_decay",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"max_grad_norm",val:": float = 1.0"},{name:"num_train_epochs",val:": float = 3.0"},{name:"max_steps",val:": int = -1"},{name:"lr_scheduler_type",val:": SchedulerType = 'linear'"},{name:"warmup_ratio",val:": float = 0.0"},{name:"warmup_steps",val:": int = 0"},{name:"log_level",val:": typing.Optional[str] = 'passive'"},{name:"log_level_replica",val:": typing.Optional[str] = 'passive'"},{name:"log_on_each_node",val:": bool = True"},{name:"logging_dir",val:": typing.Optional[str] = None"},{name:"logging_strategy",val:": IntervalStrategy = 'steps'"},{name:"logging_first_step",val:": bool = False"},{name:"logging_steps",val:": int = 500"},{name:"logging_nan_inf_filter",val:": str = True"},{name:"save_strategy",val:": IntervalStrategy = 'steps'"},{name:"save_steps",val:": int = 500"},{name:"save_total_limit",val:": typing.Optional[int] = None"},{name:"save_on_each_node",val:": bool = False"},{name:"no_cuda",val:": bool = False"},{name:"seed",val:": int = 42"},{name:"bf16",val:": bool = False"},{name:"fp16",val:": bool = False"},{name:"fp16_opt_level",val:": str = 'O1'"},{name:"half_precision_backend",val:": str = 'auto'"},{name:"bf16_full_eval",val:": bool = False"},{name:"fp16_full_eval",val:": bool = False"},{name:"tf32",val:": bool = None"},{name:"local_rank",val:": int = -1"},{name:"xpu_backend",val:": str = None"},{name:"tpu_num_cores",val:": typing.Optional[int] = None"},{name:"tpu_metrics_debug",val:": bool = False"},{name:"debug",val:": str = ''"},{name:"dataloader_drop_last",val:": bool = False"},{name:"eval_steps",val:": int = None"},{name:"dataloader_num_workers",val:": int = 0"},{name:"past_index",val:": int = -1"},{name:"run_name",val:": typing.Optional[str] = None"},{name:"disable_tqdm",val:": typing.Optional[bool] = None"},{name:"remove_unused_columns",val:": typing.Optional[bool] = True"},{name:"label_names",val:": typing.Optional[typing.List[str]] = None"},{name:"load_best_model_at_end",val:": typing.Optional[bool] = False"},{name:"metric_for_best_model",val:": typing.Optional[str] = None"},{name:"greater_is_better",val:": typing.Optional[bool] = None"},{name:"ignore_data_skip",val:": bool = False"},{name:"sharded_ddp",val:": str = ''"},{name:"deepspeed",val:": typing.Optional[str] = None"},{name:"label_smoothing_factor",val:": float = 0.0"},{name:"adafactor",val:": bool = False"},{name:"group_by_length",val:": bool = False"},{name:"length_column_name",val:": typing.Optional[str] = 'length'"},{name:"report_to",val:": typing.Optional[typing.List[str]] = None"},{name:"ddp_find_unused_parameters",val:": typing.Optional[bool] = None"},{name:"dataloader_pin_memory",val:": bool = True"},{name:"skip_memory_metrics",val:": bool = True"},{name:"use_legacy_prediction_loop",val:": bool = False"},{name:"push_to_hub",val:": bool = False"},{name:"resume_from_checkpoint",val:": typing.Optional[str] = None"},{name:"hub_model_id",val:": str = None"},{name:"hub_strategy",val:": HubStrategy = 'every_save'"},{name:"hub_token",val:": str = None"},{name:"gradient_checkpointing",val:": bool = False"},{name:"fp16_backend",val:": str = 'auto'"},{name:"push_to_hub_model_id",val:": str = None"},{name:"push_to_hub_organization",val:": str = None"},{name:"push_to_hub_token",val:": str = None"},{name:"mp_parameters",val:": str = ''"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L73",parametersDescription:[{anchor:"transformers.TrainingArguments.output_dir",description:`<strong>output_dir</strong> (<code>str</code>) &#x2014;
The output directory where the model predictions and checkpoints will be written.`,name:"output_dir"},{anchor:"transformers.TrainingArguments.overwrite_output_dir",description:`<strong>overwrite_output_dir</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>True</code>, overwrite the content of the output directory. Use this to continue training if
<code>output_dir</code> points to a checkpoint directory.`,name:"overwrite_output_dir"},{anchor:"transformers.TrainingArguments.do_train",description:`<strong>do_train</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to run training or not. This argument is not directly used by <a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>, it&#x2019;s
intended to be used by your training/evaluation scripts instead. See the <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="nofollow">example scripts</a> for more details.`,name:"do_train"},{anchor:"transformers.TrainingArguments.do_eval",description:`<strong>do_eval</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to run evaluation on the validation set or not. Will be set to <code>True</code> if
<code>evaluation_strategy</code> is different from <code>&quot;no&quot;</code>. This argument is not directly used by
<a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>, it&#x2019;s intended to be used by your training/evaluation scripts instead. See
the <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="nofollow">example scripts</a> for more
details.`,name:"do_eval"},{anchor:"transformers.TrainingArguments.do_predict",description:`<strong>do_predict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to run predictions on the test set or not. This argument is not directly used by
<a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>, it&#x2019;s intended to be used by your training/evaluation scripts instead. See
the <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="nofollow">example scripts</a> for more
details.`,name:"do_predict"},{anchor:"transformers.TrainingArguments.evaluation_strategy",description:`<strong>evaluation_strategy</strong> (<code>str</code> or <a href="/docs/transformers/master/en/internal/trainer_utils#transformers.IntervalStrategy">IntervalStrategy</a>, <em>optional</em>, defaults to <code>&quot;no&quot;</code>) &#x2014;
The evaluation strategy to adopt during training. Possible values are:</p>
<ul>
<li><code>&quot;no&quot;</code>: No evaluation is done during training.</li>
<li><code>&quot;steps&quot;</code>: Evaluation is done (and logged) every <code>eval_steps</code>.</li>
<li><code>&quot;epoch&quot;</code>: Evaluation is done at the end of each epoch.</li>
</ul>`,name:"evaluation_strategy"},{anchor:"transformers.TrainingArguments.prediction_loss_only",description:`<strong>prediction_loss_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <em>False</em>) &#x2014;
When performing evaluation and generating predictions, only returns the loss.`,name:"prediction_loss_only"},{anchor:"transformers.TrainingArguments.per_device_train_batch_size",description:`<strong>per_device_train_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The batch size per GPU/TPU core/CPU for training.`,name:"per_device_train_batch_size"},{anchor:"transformers.TrainingArguments.per_device_eval_batch_size",description:`<strong>per_device_eval_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The batch size per GPU/TPU core/CPU for evaluation.`,name:"per_device_eval_batch_size"},{anchor:"transformers.TrainingArguments.gradient_accumulation_steps",description:`<strong>gradient_accumulation_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of updates steps to accumulate the gradients for, before performing a backward/update pass.`,name:"gradient_accumulation_steps"}]}}),Vi=new E({props:{name:"get_process_log_level",anchor:"transformers.TrainingArguments.get_process_log_level",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L1154"}}),Zi=new E({props:{name:"get_warmup_steps",anchor:"transformers.TrainingArguments.get_warmup_steps",parameters:[{name:"num_training_steps",val:": int"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L1239"}}),Ji=new E({props:{name:"main_process_first",anchor:"transformers.TrainingArguments.main_process_first",parameters:[{name:"local",val:" = True"},{name:"desc",val:" = 'work'"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L1186",parametersDescription:[{anchor:"transformers.TrainingArguments.main_process_first.local",description:`<strong>local</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
if <code>True</code> first means process of rank 0 of each node if <code>False</code> first means process of rank 0
of node rank 0 In multi-node environment with a shared filesystem you most likely will want to use
<code>local=False</code> so that only the main process of the first node will do the processing. If however, the
filesystem is not shared, then the main process of each node will need to do the processing, which is
the default behavior.`,name:"local"},{anchor:"transformers.TrainingArguments.main_process_first.desc",description:`<strong>desc</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;work&quot;</code>) &#x2014;
a work description to be used in debug logs`,name:"desc"}]}}),Qi=new E({props:{name:"to_dict",anchor:"transformers.TrainingArguments.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L1248"}}),Xi=new E({props:{name:"to_json_string",anchor:"transformers.TrainingArguments.to_json_string",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L1263"}}),el=new E({props:{name:"to_sanitized_dict",anchor:"transformers.TrainingArguments.to_sanitized_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/training_args.py#L1269"}}),tl=new $({}),rl=new E({props:{name:"class transformers.Seq2SeqTrainingArguments",anchor:"transformers.Seq2SeqTrainingArguments",parameters:[{name:"output_dir",val:": str"},{name:"overwrite_output_dir",val:": bool = False"},{name:"do_train",val:": bool = False"},{name:"do_eval",val:": bool = False"},{name:"do_predict",val:": bool = False"},{name:"evaluation_strategy",val:": IntervalStrategy = 'no'"},{name:"prediction_loss_only",val:": bool = False"},{name:"per_device_train_batch_size",val:": int = 8"},{name:"per_device_eval_batch_size",val:": int = 8"},{name:"per_gpu_train_batch_size",val:": typing.Optional[int] = None"},{name:"per_gpu_eval_batch_size",val:": typing.Optional[int] = None"},{name:"gradient_accumulation_steps",val:": int = 1"},{name:"eval_accumulation_steps",val:": typing.Optional[int] = None"},{name:"learning_rate",val:": float = 5e-05"},{name:"weight_decay",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"max_grad_norm",val:": float = 1.0"},{name:"num_train_epochs",val:": float = 3.0"},{name:"max_steps",val:": int = -1"},{name:"lr_scheduler_type",val:": SchedulerType = 'linear'"},{name:"warmup_ratio",val:": float = 0.0"},{name:"warmup_steps",val:": int = 0"},{name:"log_level",val:": typing.Optional[str] = 'passive'"},{name:"log_level_replica",val:": typing.Optional[str] = 'passive'"},{name:"log_on_each_node",val:": bool = True"},{name:"logging_dir",val:": typing.Optional[str] = None"},{name:"logging_strategy",val:": IntervalStrategy = 'steps'"},{name:"logging_first_step",val:": bool = False"},{name:"logging_steps",val:": int = 500"},{name:"logging_nan_inf_filter",val:": str = True"},{name:"save_strategy",val:": IntervalStrategy = 'steps'"},{name:"save_steps",val:": int = 500"},{name:"save_total_limit",val:": typing.Optional[int] = None"},{name:"save_on_each_node",val:": bool = False"},{name:"no_cuda",val:": bool = False"},{name:"seed",val:": int = 42"},{name:"bf16",val:": bool = False"},{name:"fp16",val:": bool = False"},{name:"fp16_opt_level",val:": str = 'O1'"},{name:"half_precision_backend",val:": str = 'auto'"},{name:"bf16_full_eval",val:": bool = False"},{name:"fp16_full_eval",val:": bool = False"},{name:"tf32",val:": bool = None"},{name:"local_rank",val:": int = -1"},{name:"xpu_backend",val:": str = None"},{name:"tpu_num_cores",val:": typing.Optional[int] = None"},{name:"tpu_metrics_debug",val:": bool = False"},{name:"debug",val:": str = ''"},{name:"dataloader_drop_last",val:": bool = False"},{name:"eval_steps",val:": int = None"},{name:"dataloader_num_workers",val:": int = 0"},{name:"past_index",val:": int = -1"},{name:"run_name",val:": typing.Optional[str] = None"},{name:"disable_tqdm",val:": typing.Optional[bool] = None"},{name:"remove_unused_columns",val:": typing.Optional[bool] = True"},{name:"label_names",val:": typing.Optional[typing.List[str]] = None"},{name:"load_best_model_at_end",val:": typing.Optional[bool] = False"},{name:"metric_for_best_model",val:": typing.Optional[str] = None"},{name:"greater_is_better",val:": typing.Optional[bool] = None"},{name:"ignore_data_skip",val:": bool = False"},{name:"sharded_ddp",val:": str = ''"},{name:"deepspeed",val:": typing.Optional[str] = None"},{name:"label_smoothing_factor",val:": float = 0.0"},{name:"adafactor",val:": bool = False"},{name:"group_by_length",val:": bool = False"},{name:"length_column_name",val:": typing.Optional[str] = 'length'"},{name:"report_to",val:": typing.Optional[typing.List[str]] = None"},{name:"ddp_find_unused_parameters",val:": typing.Optional[bool] = None"},{name:"dataloader_pin_memory",val:": bool = True"},{name:"skip_memory_metrics",val:": bool = True"},{name:"use_legacy_prediction_loop",val:": bool = False"},{name:"push_to_hub",val:": bool = False"},{name:"resume_from_checkpoint",val:": typing.Optional[str] = None"},{name:"hub_model_id",val:": str = None"},{name:"hub_strategy",val:": HubStrategy = 'every_save'"},{name:"hub_token",val:": str = None"},{name:"gradient_checkpointing",val:": bool = False"},{name:"fp16_backend",val:": str = 'auto'"},{name:"push_to_hub_model_id",val:": str = None"},{name:"push_to_hub_organization",val:": str = None"},{name:"push_to_hub_token",val:": str = None"},{name:"mp_parameters",val:": str = ''"},{name:"sortish_sampler",val:": bool = False"},{name:"predict_with_generate",val:": bool = False"},{name:"generation_max_length",val:": typing.Optional[int] = None"},{name:"generation_num_beams",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/training_args_seq2seq.py#L28",parametersDescription:[{anchor:"transformers.Seq2SeqTrainingArguments.output_dir",description:`<strong>output_dir</strong> (<code>str</code>) &#x2014;
The output directory where the model predictions and checkpoints will be written.`,name:"output_dir"},{anchor:"transformers.Seq2SeqTrainingArguments.overwrite_output_dir",description:`<strong>overwrite_output_dir</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>True</code>, overwrite the content of the output directory. Use this to continue training if
<code>output_dir</code> points to a checkpoint directory.`,name:"overwrite_output_dir"},{anchor:"transformers.Seq2SeqTrainingArguments.do_train",description:`<strong>do_train</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to run training or not. This argument is not directly used by <a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>, it&#x2019;s
intended to be used by your training/evaluation scripts instead. See the <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="nofollow">example scripts</a> for more details.`,name:"do_train"},{anchor:"transformers.Seq2SeqTrainingArguments.do_eval",description:`<strong>do_eval</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to run evaluation on the validation set or not. Will be set to <code>True</code> if
<code>evaluation_strategy</code> is different from <code>&quot;no&quot;</code>. This argument is not directly used by
<a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>, it&#x2019;s intended to be used by your training/evaluation scripts instead. See
the <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="nofollow">example scripts</a> for more
details.`,name:"do_eval"},{anchor:"transformers.Seq2SeqTrainingArguments.do_predict",description:`<strong>do_predict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to run predictions on the test set or not. This argument is not directly used by
<a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>, it&#x2019;s intended to be used by your training/evaluation scripts instead. See
the <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="nofollow">example scripts</a> for more
details.`,name:"do_predict"},{anchor:"transformers.Seq2SeqTrainingArguments.evaluation_strategy",description:`<strong>evaluation_strategy</strong> (<code>str</code> or <a href="/docs/transformers/master/en/internal/trainer_utils#transformers.IntervalStrategy">IntervalStrategy</a>, <em>optional</em>, defaults to <code>&quot;no&quot;</code>) &#x2014;
The evaluation strategy to adopt during training. Possible values are:</p>
<ul>
<li><code>&quot;no&quot;</code>: No evaluation is done during training.</li>
<li><code>&quot;steps&quot;</code>: Evaluation is done (and logged) every <code>eval_steps</code>.</li>
<li><code>&quot;epoch&quot;</code>: Evaluation is done at the end of each epoch.</li>
</ul>`,name:"evaluation_strategy"},{anchor:"transformers.Seq2SeqTrainingArguments.prediction_loss_only",description:`<strong>prediction_loss_only</strong> (<code>bool</code>, <em>optional</em>, defaults to <em>False</em>) &#x2014;
When performing evaluation and generating predictions, only returns the loss.`,name:"prediction_loss_only"},{anchor:"transformers.Seq2SeqTrainingArguments.per_device_train_batch_size",description:`<strong>per_device_train_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The batch size per GPU/TPU core/CPU for training.`,name:"per_device_train_batch_size"},{anchor:"transformers.Seq2SeqTrainingArguments.per_device_eval_batch_size",description:`<strong>per_device_eval_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The batch size per GPU/TPU core/CPU for evaluation.`,name:"per_device_eval_batch_size"},{anchor:"transformers.Seq2SeqTrainingArguments.gradient_accumulation_steps",description:`<strong>gradient_accumulation_steps</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of updates steps to accumulate the gradients for, before performing a backward/update pass.`,name:"gradient_accumulation_steps"}]}}),dl=new $({}),cl=new E({props:{name:"class transformers.TFTrainingArguments",anchor:"transformers.TFTrainingArguments",parameters:[{name:"output_dir",val:": str"},{name:"overwrite_output_dir",val:": bool = False"},{name:"do_train",val:": bool = False"},{name:"do_eval",val:": bool = False"},{name:"do_predict",val:": bool = False"},{name:"evaluation_strategy",val:": IntervalStrategy = 'no'"},{name:"prediction_loss_only",val:": bool = False"},{name:"per_device_train_batch_size",val:": int = 8"},{name:"per_device_eval_batch_size",val:": int = 8"},{name:"per_gpu_train_batch_size",val:": typing.Optional[int] = None"},{name:"per_gpu_eval_batch_size",val:": typing.Optional[int] = None"},{name:"gradient_accumulation_steps",val:": int = 1"},{name:"eval_accumulation_steps",val:": typing.Optional[int] = None"},{name:"learning_rate",val:": float = 5e-05"},{name:"weight_decay",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"max_grad_norm",val:": float = 1.0"},{name:"num_train_epochs",val:": float = 3.0"},{name:"max_steps",val:": int = -1"},{name:"lr_scheduler_type",val:": SchedulerType = 'linear'"},{name:"warmup_ratio",val:": float = 0.0"},{name:"warmup_steps",val:": int = 0"},{name:"log_level",val:": typing.Optional[str] = 'passive'"},{name:"log_level_replica",val:": typing.Optional[str] = 'passive'"},{name:"log_on_each_node",val:": bool = True"},{name:"logging_dir",val:": typing.Optional[str] = None"},{name:"logging_strategy",val:": IntervalStrategy = 'steps'"},{name:"logging_first_step",val:": bool = False"},{name:"logging_steps",val:": int = 500"},{name:"logging_nan_inf_filter",val:": str = True"},{name:"save_strategy",val:": IntervalStrategy = 'steps'"},{name:"save_steps",val:": int = 500"},{name:"save_total_limit",val:": typing.Optional[int] = None"},{name:"save_on_each_node",val:": bool = False"},{name:"no_cuda",val:": bool = False"},{name:"seed",val:": int = 42"},{name:"bf16",val:": bool = False"},{name:"fp16",val:": bool = False"},{name:"fp16_opt_level",val:": str = 'O1'"},{name:"half_precision_backend",val:": str = 'auto'"},{name:"bf16_full_eval",val:": bool = False"},{name:"fp16_full_eval",val:": bool = False"},{name:"tf32",val:": bool = None"},{name:"local_rank",val:": int = -1"},{name:"xpu_backend",val:": str = None"},{name:"tpu_num_cores",val:": typing.Optional[int] = None"},{name:"tpu_metrics_debug",val:": bool = False"},{name:"debug",val:": str = ''"},{name:"dataloader_drop_last",val:": bool = False"},{name:"eval_steps",val:": int = None"},{name:"dataloader_num_workers",val:": int = 0"},{name:"past_index",val:": int = -1"},{name:"run_name",val:": typing.Optional[str] = None"},{name:"disable_tqdm",val:": typing.Optional[bool] = None"},{name:"remove_unused_columns",val:": typing.Optional[bool] = True"},{name:"label_names",val:": typing.Optional[typing.List[str]] = None"},{name:"load_best_model_at_end",val:": typing.Optional[bool] = False"},{name:"metric_for_best_model",val:": typing.Optional[str] = None"},{name:"greater_is_better",val:": typing.Optional[bool] = None"},{name:"ignore_data_skip",val:": bool = False"},{name:"sharded_ddp",val:": str = ''"},{name:"deepspeed",val:": typing.Optional[str] = None"},{name:"label_smoothing_factor",val:": float = 0.0"},{name:"adafactor",val:": bool = False"},{name:"group_by_length",val:": bool = False"},{name:"length_column_name",val:": typing.Optional[str] = 'length'"},{name:"report_to",val:": typing.Optional[typing.List[str]] = None"},{name:"ddp_find_unused_parameters",val:": typing.Optional[bool] = None"},{name:"dataloader_pin_memory",val:": bool = True"},{name:"skip_memory_metrics",val:": bool = True"},{name:"use_legacy_prediction_loop",val:": bool = False"},{name:"push_to_hub",val:": bool = False"},{name:"resume_from_checkpoint",val:": typing.Optional[str] = None"},{name:"hub_model_id",val:": str = None"},{name:"hub_strategy",val:": HubStrategy = 'every_save'"},{name:"hub_token",val:": str = None"},{name:"gradient_checkpointing",val:": bool = False"},{name:"fp16_backend",val:": str = 'auto'"},{name:"push_to_hub_model_id",val:": str = None"},{name:"push_to_hub_organization",val:": str = None"},{name:"push_to_hub_token",val:": str = None"},{name:"mp_parameters",val:": str = ''"},{name:"tpu_name",val:": str = None"},{name:"tpu_zone",val:": str = None"},{name:"gcp_project",val:": str = None"},{name:"poly_power",val:": float = 1.0"},{name:"xla",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/training_args_tf.py#L31",parametersDescription:[{anchor:"transformers.TFTrainingArguments.output_dir",description:`<strong>output_dir</strong> (<code>str</code>) &#x2014;
The output directory where the model predictions and checkpoints will be written.`,name:"output_dir"},{anchor:"transformers.TFTrainingArguments.overwrite_output_dir",description:`<strong>overwrite_output_dir</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>True</code>, overwrite the content of the output directory. Use this to continue training if
<code>output_dir</code> points to a checkpoint directory.`,name:"overwrite_output_dir"},{anchor:"transformers.TFTrainingArguments.do_train",description:`<strong>do_train</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to run training or not. This argument is not directly used by <a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>, it&#x2019;s
intended to be used by your training/evaluation scripts instead. See the <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="nofollow">example scripts</a> for more details.`,name:"do_train"},{anchor:"transformers.TFTrainingArguments.do_eval",description:`<strong>do_eval</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to run evaluation on the validation set or not. Will be set to <code>True</code> if
<code>evaluation_strategy</code> is different from <code>&quot;no&quot;</code>. This argument is not directly used by
<a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>, it&#x2019;s intended to be used by your training/evaluation scripts instead. See
the <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="nofollow">example scripts</a> for more
details.`,name:"do_eval"},{anchor:"transformers.TFTrainingArguments.do_predict",description:`<strong>do_predict</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to run predictions on the test set or not. This argument is not directly used by
<a href="/docs/transformers/master/en/main_classes/trainer#transformers.Trainer">Trainer</a>, it&#x2019;s intended to be used by your training/evaluation scripts instead. See
the <a href="https://github.com/huggingface/transformers/tree/master/examples" rel="nofollow">example scripts</a> for more
details.`,name:"do_predict"},{anchor:"transformers.TFTrainingArguments.evaluation_strategy",description:`<strong>evaluation_strategy</strong> (<code>str</code> or <a href="/docs/transformers/master/en/internal/trainer_utils#transformers.IntervalStrategy">IntervalStrategy</a>, <em>optional</em>, defaults to <code>&quot;no&quot;</code>) &#x2014;
The evaluation strategy to adopt during training. Possible values are:</p>
<ul>
<li><code>&quot;no&quot;</code>: No evaluation is done during training.</li>
<li><code>&quot;steps&quot;</code>: Evaluation is done (and logged) every <code>eval_steps</code>.</li>
<li><code>&quot;epoch&quot;</code>: Evaluation is done at the end of each epoch.</li>
</ul>`,name:"evaluation_strategy"},{anchor:"transformers.TFTrainingArguments.per_device_train_batch_size",description:`<strong>per_device_train_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The batch size per GPU/TPU core/CPU for training.`,name:"per_device_train_batch_size"},{anchor:"transformers.TFTrainingArguments.per_device_eval_batch_size",description:`<strong>per_device_eval_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The batch size per GPU/TPU core/CPU for evaluation.
gradient<em>accumulation_steps &#x2014; (<code>int</code>, _optional</em>, defaults to 1):
Number of updates steps to accumulate the gradients for, before performing a backward/update pass.`,name:"per_device_eval_batch_size"}]}}),hl=new $({}),fl=new $({}),ul=new j({props:{code:`[...]
logger = logging.getLogger(__name__)

# Setup logging
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)],
)

# set the main code and the modules it uses to the same log-level according to the node
log_level = training_args.get_process_log_level()
logger.setLevel(log_level)
datasets.utils.logging.set_verbosity(log_level)
transformers.utils.logging.set_verbosity(log_level)

trainer = Trainer(...),`,highlighted:`[...]
logger = logging.getLogger(__name__)

<span class="hljs-comment"># Setup logging</span>
logging.basicConfig(
    <span class="hljs-built_in">format</span>=<span class="hljs-string">&quot;%(asctime)s - %(levelname)s - %(name)s - %(message)s&quot;</span>,
    datefmt=<span class="hljs-string">&quot;%m/%d/%Y %H:%M:%S&quot;</span>,
    handlers=[logging.StreamHandler(sys.stdout)],
)

<span class="hljs-comment"># set the main code and the modules it uses to the same log-level according to the node</span>
log_level = training_args.get_process_log_level()
logger.setLevel(log_level)
datasets.utils.logging.set_verbosity(log_level)
transformers.utils.logging.set_verbosity(log_level)

trainer = Trainer(...)`}}),_l=new j({props:{code:"my_app.py ... --log_level warning --log_level_replica error,",highlighted:"my_app.py ... --log_level warning --log_level_replica error"}}),vl=new j({props:{code:"my_app.py ... --log_level warning --log_level_replica error --log_on_each_node 0,",highlighted:"my_app.py ... --log_level warning --log_level_replica error --log_on_each_node 0"}}),gl=new j({props:{code:"my_app.py ... --log_level error --log_level_replica error --log_on_each_node 0,",highlighted:"my_app.py ... --log_level error --log_level_replica error --log_on_each_node 0"}}),bl=new $({}),El=new $({}),kl=new $({}),Al=new j({props:{code:`pip install fairscale
pip install deepspeed,`,highlighted:`pip install fairscale
pip install deepspeed`}}),Ol=new $({}),xl=new j({props:{code:"which nvcc,",highlighted:'<span class="hljs-built_in">which</span> nvcc'}}),Cl=new $({}),Nl=new j({props:{code:`/usr/local/cuda-10.2
/usr/local/cuda-11.0,`,highlighted:`/usr/local/cuda-10.2
/usr/local/cuda-11.0`}}),Il=new j({props:{code:`echo $PATH
echo $LD_LIBRARY_PATH,`,highlighted:`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$PATH</span>
<span class="hljs-built_in">echo</span> <span class="hljs-variable">$LD_LIBRARY_PATH</span>`}}),zl=new j({props:{code:`export PATH=/usr/local/cuda-10.2/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:$LD_LIBRARY_PATH,`,highlighted:`<span class="hljs-built_in">export</span> PATH=/usr/local/cuda-10.2/bin:<span class="hljs-variable">$PATH</span>
<span class="hljs-built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:<span class="hljs-variable">$LD_LIBRARY_PATH</span>`}}),Fl=new $({}),Ll=new j({props:{code:`sudo ln -s /usr/bin/gcc-7  /usr/local/cuda-10.2/bin/gcc
sudo ln -s /usr/bin/g++-7  /usr/local/cuda-10.2/bin/g++,`,highlighted:`sudo <span class="hljs-built_in">ln</span> -s /usr/bin/gcc-7  /usr/local/cuda-10.2/bin/gcc
sudo <span class="hljs-built_in">ln</span> -s /usr/bin/g++-7  /usr/local/cuda-10.2/bin/g++`}}),ql=new $({}),jl=new j({props:{code:"pip install fairscale,",highlighted:"pip install fairscale"}}),Hl=new j({props:{code:"pip install transformers[fairscale],",highlighted:"pip install transformers[fairscale]"}}),Bl=new j({props:{code:"pip install fairscale --no-build-isolation .,",highlighted:"pip install fairscale --no-build-isolation ."}}),Vl=new j({props:{code:`git clone https://github.com/facebookresearch/fairscale/
cd fairscale
rm -r dist build
python setup.py bdist_wheel
pip uninstall -y fairscale
pip install dist/fairscale-*.whl,`,highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/facebookresearch/fairscale/
<span class="hljs-built_in">cd</span> fairscale
<span class="hljs-built_in">rm</span> -r dist build
python setup.py bdist_wheel
pip uninstall -y fairscale
pip install dist/fairscale-*.whl`}}),Zl=new j({props:{code:`pip uninstall -y fairscale; pip install fairscale --pre \\
-f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html \\
--no-cache --no-build-isolation,`,highlighted:`pip uninstall -y fairscale; pip install fairscale --pre \\
-f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html \\
--no-cache --no-build-isolation`}}),Jl=new j({props:{code:`pip install -v --disable-pip-version-check . \\
-f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html --pre,`,highlighted:`pip install -v --disable-pip-version-check . \\
-f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html --pre`}}),Xl=new j({props:{code:`python -m torch.distributed.launch --nproc_per_node=2 examples/pytorch/translation/run_translation.py \\
--model_name_or_path t5-small --per_device_train_batch_size 1   \\
--output_dir output_dir --overwrite_output_dir \\
--do_train --max_train_samples 500 --num_train_epochs 1 \\
--dataset_name wmt16 --dataset_config "ro-en" \\
--source_lang en --target_lang ro \\
--fp16 --sharded_ddp simple,`,highlighted:`python -m torch.distributed.launch --nproc_per_node=2 examples/pytorch/translation/run_translation.py \\
--model_name_or_path t5-small --per_device_train_batch_size 1   \\
--output_dir output_dir --overwrite_output_dir \\
--do_train --max_train_samples 500 --num_train_epochs 1 \\
--dataset_name wmt16 --dataset_config <span class="hljs-string">&quot;ro-en&quot;</span> \\
--source_lang en --target_lang ro \\
--fp16 --sharded_ddp simple`}}),ad=new j({props:{code:`python -m torch.distributed.launch --nproc_per_node=2 examples/pytorch/translation/run_translation.py \\
--model_name_or_path t5-small --per_device_train_batch_size 1   \\
--output_dir output_dir --overwrite_output_dir \\
--do_train --max_train_samples 500 --num_train_epochs 1 \\
--dataset_name wmt16 --dataset_config "ro-en" \\
--source_lang en --target_lang ro \\
--fp16 --sharded_ddp zero_dp_2,`,highlighted:`python -m torch.distributed.launch --nproc_per_node=2 examples/pytorch/translation/run_translation.py \\
--model_name_or_path t5-small --per_device_train_batch_size 1   \\
--output_dir output_dir --overwrite_output_dir \\
--do_train --max_train_samples 500 --num_train_epochs 1 \\
--dataset_name wmt16 --dataset_config <span class="hljs-string">&quot;ro-en&quot;</span> \\
--source_lang en --target_lang ro \\
--fp16 --sharded_ddp zero_dp_2`}}),nd=new $({}),sd=new $({}),id=new $({}),ld=new $({}),dd=new $({}),cd=new $({}),pd=new $({}),md=new $({}),hd=new $({}),fd=new $({}),ud=new $({}),_d=new $({}),vd=new $({}),gd=new $({}),bd=new $({}),yd=new $({}),Ed=new $({}),wd=new $({}),$d=new $({}),Td=new $({}),kd=new $({}),Dd=new $({}),Pd=new $({}),Ad=new $({}),{c(){w=n("meta"),I=l(),k=n("h1"),A=n("a"),ve=n("span"),h(H.$$.fragment),G=l(),J=n("span"),ge=a("Trainer"),re=l(),R=n("p"),ie=a("The "),le=n("a"),ae=a("Trainer"),de=a(" and "),Q=n("a"),tt=a("TFTrainer"),be=a(` classes provide an API for feature-complete
training in most standard use cases. It\u2019s used in most of the `),x=n("a"),M=a("example scripts"),ct=a("."),ye=l(),q=n("p"),pt=a("Before instantiating your "),Ee=n("a"),Wn=a("Trainer"),Bn=a("/"),we=n("a"),Vn=a("TFTrainer"),Yn=a(`, create a
`),xe=n("a"),Zn=a("TrainingArguments"),Jn=a("/"),W=n("a"),Se=a("TFTrainingArguments"),Nd=a(` to access all the points of
customization during training.`),rt=l(),oe=n("p"),Id=a("The API supports distributed training on multiple GPUs/TPUs, mixed precision through "),Ce=n("a"),zd=a("NVIDIA Apex"),ce=a(" and Native AMP for PyTorch and "),Ta=n("code"),Fd=a("tf.keras.mixed_precision"),Ld=a(" for TensorFlow."),Qn=l(),mt=n("p"),K0=a("Both "),qd=n("a"),X0=a("Trainer"),e$=a(" and "),Ud=n("a"),t$=a("TFTrainer"),r$=a(` contain the basic training loop which supports
the above features. To inject custom behavior you can subclass them and override the following methods:`),rb=l(),P=n("ul"),ka=n("li"),Fm=n("strong"),a$=a("get_train_dataloader"),o$=a("/"),Lm=n("strong"),n$=a("get_train_tfdataset"),s$=a(" \u2014 Creates the training DataLoader (PyTorch) or TF Dataset."),i$=l(),Da=n("li"),qm=n("strong"),l$=a("get_eval_dataloader"),d$=a("/"),Um=n("strong"),c$=a("get_eval_tfdataset"),p$=a(" \u2014 Creates the evaluation DataLoader (PyTorch) or TF Dataset."),m$=l(),Pa=n("li"),Rm=n("strong"),h$=a("get_test_dataloader"),f$=a("/"),Mm=n("strong"),u$=a("get_test_tfdataset"),_$=a(" \u2014 Creates the test DataLoader (PyTorch) or TF Dataset."),v$=l(),Rd=n("li"),jm=n("strong"),g$=a("log"),b$=a(" \u2014 Logs information on the various objects watching training."),y$=l(),ht=n("li"),Hm=n("strong"),E$=a("create_optimizer_and_scheduler"),w$=a(` \u2014 Sets up the optimizer and learning rate scheduler if they were not passed at
init. Note, that you can also subclass or override the `),Gm=n("code"),$$=a("create_optimizer"),T$=a(" and "),Wm=n("code"),k$=a("create_scheduler"),D$=a(` methods
separately.`),P$=l(),Md=n("li"),Bm=n("strong"),A$=a("create_optimizer"),O$=a(" \u2014 Sets up the optimizer if it wasn\u2019t passed at init."),x$=l(),jd=n("li"),Vm=n("strong"),S$=a("create_scheduler"),C$=a(" \u2014 Sets up the learning rate scheduler if it wasn\u2019t passed at init."),N$=l(),Hd=n("li"),Ym=n("strong"),I$=a("compute_loss"),z$=a(" - Computes the loss on a batch of training inputs."),F$=l(),Gd=n("li"),Zm=n("strong"),L$=a("training_step"),q$=a(" \u2014 Performs a training step."),U$=l(),Wd=n("li"),Jm=n("strong"),R$=a("prediction_step"),M$=a(" \u2014 Performs an evaluation/test step."),j$=l(),Bd=n("li"),Qm=n("strong"),H$=a("run_model"),G$=a(" (TensorFlow only) \u2014 Basic pass through the model."),W$=l(),Vd=n("li"),Km=n("strong"),B$=a("evaluate"),V$=a(" \u2014 Runs an evaluation loop and returns metrics."),Y$=l(),Yd=n("li"),Xm=n("strong"),Z$=a("predict"),J$=a(" \u2014 Returns predictions (with metrics if labels are available) on a test set."),ab=l(),h(Aa.$$.fragment),ob=l(),Oa=n("p"),Q$=a("Here is an example of how to customize "),Zd=n("a"),K$=a("Trainer"),X$=a(` using a custom loss function for multi-label
classification:`),nb=l(),h(Kn.$$.fragment),sb=l(),ft=n("p"),e4=a("Another way to customize the training loop behavior for the PyTorch "),Jd=n("a"),t4=a("Trainer"),r4=a(` is to use
`),Qd=n("a"),a4=a("callbacks"),o4=a(` that can inspect the training loop state (for progress reporting, logging on TensorBoard or
other ML platforms\u2026) and take decisions (like early stopping).`),ib=l(),pr=n("h2"),xa=n("a"),eh=n("span"),h(Xn.$$.fragment),n4=l(),th=n("span"),s4=a("Trainer"),lb=l(),b=n("div"),h(es.$$.fragment),i4=l(),rh=n("p"),l4=a("Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for \u{1F917} Transformers."),d4=l(),ah=n("p"),c4=a("Important attributes:"),p4=l(),$e=n("ul"),Sa=n("li"),oh=n("strong"),m4=a("model"),h4=a(` \u2014 Always points to the core model. If using a transformers model, it will be a
`),Kd=n("a"),f4=a("PreTrainedModel"),u4=a(" subclass."),_4=l(),K=n("li"),nh=n("strong"),v4=a("model_wrapped"),g4=a(` \u2014 Always points to the most external model in case one or more other modules wrap the
original model. This is the model that should be used for the forward pass. For example, under `),sh=n("code"),b4=a("DeepSpeed"),y4=a(`,
the inner model is wrapped in `),ih=n("code"),E4=a("DeepSpeed"),w4=a(" and then again in "),lh=n("code"),$4=a("torch.nn.DistributedDataParallel"),T4=a(`. If the
inner model hasn\u2019t been wrapped, then `),dh=n("code"),k4=a("self.model_wrapped"),D4=a(" is the same as "),ch=n("code"),P4=a("self.model"),A4=a("."),O4=l(),Xd=n("li"),ph=n("strong"),x4=a("is_model_parallel"),S4=a(` \u2014 Whether or not a model has been switched to a model parallel mode (different from
data parallelism, this means some of the model layers are split on different GPUs).`),C4=l(),Ne=n("li"),mh=n("strong"),N4=a("place_model_on_device"),I4=a(` \u2014 Whether or not to automatically place the model on the device - it will be set
to `),hh=n("code"),z4=a("False"),F4=a(` if model parallel or deepspeed is used, or if the default
`),fh=n("code"),L4=a("TrainingArguments.place_model_on_device"),q4=a(" is overridden to return "),uh=n("code"),U4=a("False"),R4=a(" ."),M4=l(),Ie=n("li"),_h=n("strong"),j4=a("is_in_train"),H4=a(" \u2014 Whether or not a model is currently running "),vh=n("code"),G4=a("train"),W4=a(" (e.g. when "),gh=n("code"),B4=a("evaluate"),V4=a(` is called
while in `),bh=n("code"),Y4=a("train"),Z4=a(")"),J4=l(),Ca=n("div"),h(ts.$$.fragment),Q4=l(),rs=n("p"),K4=a("Add a callback to the current list of "),yh=n("code"),X4=a("TrainerCallback"),e3=a("."),t3=l(),Na=n("div"),h(as.$$.fragment),r3=l(),os=n("p"),a3=a("A helper wrapper that creates an appropriate context manager for "),Eh=n("code"),o3=a("autocast"),n3=a(` while feeding it the desired
arguments, depending on the situation.`),s3=l(),ut=n("div"),h(ns.$$.fragment),i3=l(),wh=n("p"),l3=a("How the loss is computed by Trainer. By default, all models return the loss in the first element."),d3=l(),$h=n("p"),c3=a("Subclass and override for custom behavior."),p3=l(),_t=n("div"),h(ss.$$.fragment),m3=l(),Th=n("p"),h3=a("Setup the optimizer."),f3=l(),is=n("p"),u3=a(`We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
Trainer\u2019s init through `),kh=n("code"),_3=a("optimizers"),v3=a(", or subclass and override this method in a subclass."),g3=l(),vt=n("div"),h(ls.$$.fragment),b3=l(),Dh=n("p"),y3=a("Setup the optimizer and the learning rate scheduler."),E3=l(),at=n("p"),w3=a(`We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
Trainer\u2019s init through `),Ph=n("code"),$3=a("optimizers"),T3=a(", or subclass and override this method (or "),Ah=n("code"),k3=a("create_optimizer"),D3=a(`
and/or `),Oh=n("code"),P3=a("create_scheduler"),A3=a(") in a subclass."),O3=l(),Ia=n("div"),h(ds.$$.fragment),x3=l(),xh=n("p"),S3=a(`Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or
passed as an argument.`),C3=l(),ze=n("div"),h(cs.$$.fragment),N3=l(),Sh=n("p"),I3=a("Run evaluation and returns metrics."),z3=l(),ps=n("p"),F3=a(`The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
(pass it to the init `),Ch=n("code"),L3=a("compute_metrics"),q3=a(" argument)."),U3=l(),Nh=n("p"),R3=a("You can also subclass and override this method to inject custom behavior."),M3=l(),gt=n("div"),h(ms.$$.fragment),j3=l(),mr=n("p"),H3=a("Prediction/evaluation loop, shared by "),Ih=n("code"),G3=a("Trainer.evaluate()"),W3=a(" and "),zh=n("code"),B3=a("Trainer.predict()"),V3=a("."),Y3=l(),Fh=n("p"),Z3=a("Works both with or without labels."),J3=l(),za=n("div"),h(hs.$$.fragment),Q3=l(),fs=n("p"),K3=a("For models that inherit from "),ec=n("a"),X3=a("PreTrainedModel"),e5=a(`, uses that method to compute the number of
floating point operations for every backward + forward pass. If using another model, either implement such a
method in the model or subclass and override this method.`),t5=l(),bt=n("div"),h(us.$$.fragment),r5=l(),_s=n("p"),a5=a("Returns the evaluation "),Lh=n("code"),o5=a("DataLoader"),n5=a("."),s5=l(),qh=n("p"),i5=a("Subclass and override this method if you want to inject some custom behavior."),l5=l(),yt=n("div"),h(vs.$$.fragment),d5=l(),gs=n("p"),c5=a("Returns the test "),Uh=n("code"),p5=a("DataLoader"),m5=a("."),h5=l(),Rh=n("p"),f5=a("Subclass and override this method if you want to inject some custom behavior."),u5=l(),Fe=n("div"),h(bs.$$.fragment),_5=l(),ys=n("p"),v5=a("Returns the training "),Mh=n("code"),g5=a("DataLoader"),b5=a("."),y5=l(),hr=n("p"),E5=a("Will use no sampler if "),jh=n("code"),w5=a("self.train_dataset"),$5=a(" does not implement "),Hh=n("code"),T5=a("__len__"),k5=a(`, a random sampler (adapted
to distributed training if necessary) otherwise.`),D5=l(),Gh=n("p"),P5=a("Subclass and override this method if you want to inject some custom behavior."),A5=l(),Et=n("div"),h(Es.$$.fragment),O5=l(),Te=n("p"),x5=a("Launch an hyperparameter search using "),Wh=n("code"),S5=a("optuna"),C5=a(" or "),Bh=n("code"),N5=a("Ray Tune"),I5=a(" or "),Vh=n("code"),z5=a("SigOpt"),F5=a(`. The optimized quantity is
determined by `),Yh=n("code"),L5=a("compute_objective"),q5=a(`, which defaults to a function returning the evaluation loss when no
metric is provided, the sum of all metrics otherwise.`),U5=l(),h(Fa.$$.fragment),R5=l(),La=n("div"),h(ws.$$.fragment),M5=l(),$s=n("p"),j5=a("Initializes a git repo in "),Zh=n("code"),H5=a("self.args.hub_model_id"),G5=a("."),W5=l(),qa=n("div"),h(Ts.$$.fragment),B5=l(),Jh=n("p"),V5=a(`Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several
machines) main process.`),Y5=l(),Ua=n("div"),h(ks.$$.fragment),Z5=l(),Ds=n("p"),J5=a(`Whether or not this process is the global main process (when training in a distributed fashion on several
machines, this is only going to be `),Qh=n("code"),Q5=a("True"),K5=a(" for one process)."),X5=l(),wt=n("div"),h(Ps.$$.fragment),eT=l(),As=n("p"),tT=a("Log "),Kh=n("code"),rT=a("logs"),aT=a(" on the various objects watching training."),oT=l(),Xh=n("p"),nT=a("Subclass and override this method to inject custom behavior."),sT=l(),z=n("div"),h(Os.$$.fragment),iT=l(),ef=n("p"),lT=a("Log metrics in a specially formatted way"),dT=l(),tf=n("p"),cT=a("Under distributed environment this is done only for a process with rank 0."),pT=l(),rf=n("p"),mT=a("Notes on memory reports:"),hT=l(),fr=n("p"),fT=a("In order to get memory usage report you need to install "),af=n("code"),uT=a("psutil"),_T=a(". You can do that with "),of=n("code"),vT=a("pip install psutil"),gT=a("."),bT=l(),nf=n("p"),yT=a("Now when this method is run, you will see a report that will include: :"),ET=l(),h(xs.$$.fragment),wT=l(),sf=n("p"),lf=n("strong"),$T=a("Understanding the reports:"),TT=l(),ot=n("ul"),df=n("li"),ke=n("p"),kT=a("the first segment, e.g., "),cf=n("code"),DT=a("train__"),PT=a(", tells you which stage the metrics are for. Reports starting with "),pf=n("code"),AT=a("init_"),OT=a(`
will be added to the first stage that gets run. So that if only evaluation is run, the memory usage for the
`),mf=n("code"),xT=a("__init__"),ST=a(" will be reported along with the "),hf=n("code"),CT=a("eval_"),NT=a(" metrics."),IT=l(),ff=n("li"),ur=n("p"),zT=a("the third segment, is either "),uf=n("code"),FT=a("cpu"),LT=a(" or "),_f=n("code"),qT=a("gpu"),UT=a(`, tells you whether it\u2019s the general RAM or the gpu0 memory
metric.`),RT=l(),vf=n("li"),tc=n("p"),gf=n("code"),MT=a("*_alloc_delta"),jT=a(` - is the difference in the used/allocated memory counter between the end and the start of the
stage - it can be negative if a function released more memory than it allocated.`),HT=l(),U=n("li"),$t=n("p"),bf=n("code"),GT=a("*_peaked_delta"),WT=a(` - is any extra memory that was consumed and then freed - relative to the current allocated
memory counter - it is never negative. When you look at the metrics of any stage you add up `),yf=n("code"),BT=a("alloc_delta"),VT=a(` +
`),Ef=n("code"),YT=a("peaked_delta"),ZT=a(" and you know how much memory was needed to complete that stage."),JT=l(),wf=n("p"),QT=a(`The reporting happens only for process of rank 0 and gpu 0 (if there is a gpu). Typically this is enough since the
main process does the bulk of work, but it could be not quite so if model parallel is used and then other GPUs may
use a different amount of gpu memory. This is also not the same under DataParallel where gpu0 may require much more
memory than the rest since it stores the gradient and optimizer states for all participating GPUS. Perhaps in the
future these reports will evolve to measure those too.`),KT=l(),$f=n("p"),XT=a(`The CPU RAM metric measures RSS (Resident Set Size) includes both the memory which is unique to the process and the
memory shared with other processes. It is important to note that it does not include swapped out memory, so the
reports could be imprecise.`),e6=l(),Ss=n("p"),t6=a(`The CPU peak memory is measured using a sampling thread. Due to python\u2019s GIL it may miss some of the peak memory if
that thread didn\u2019t get a chance to run when the highest memory was used. Therefore this report can be less than
reality. Using `),Tf=n("code"),r6=a("tracemalloc"),a6=a(` would have reported the exact peak memory, but it doesn\u2019t report memory allocations
outside of python. So if some C++ CUDA extension allocated its own memory it won\u2019t be reported. And therefore it
was dropped in favor of the memory sampling approach, which reads the current process memory usage.`),o6=l(),nt=n("p"),n6=a("The GPU allocated and peak memory reporting is done with "),kf=n("code"),s6=a("torch.cuda.memory_allocated()"),i6=a(` and
`),Df=n("code"),l6=a("torch.cuda.max_memory_allocated()"),d6=a(`. This metric reports only \u201Cdeltas\u201D for pytorch-specific allocations, as
`),Pf=n("code"),c6=a("torch.cuda"),p6=a(` memory management system doesn\u2019t track any memory allocated outside of pytorch. For example, the
very first cuda call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory.`),m6=l(),ne=n("p"),h6=a("Note that this tracker doesn\u2019t account for memory allocations outside of "),rc=n("a"),f6=a("Trainer"),u6=a(`\u2019s
`),Af=n("code"),_6=a("__init__"),v6=a(", "),Of=n("code"),g6=a("train"),b6=a(", "),xf=n("code"),y6=a("evaluate"),E6=a(" and "),Sf=n("code"),w6=a("predict"),$6=a(" calls."),T6=l(),S=n("p"),k6=a("Because "),Cf=n("code"),D6=a("evaluation"),P6=a(" calls may happen during "),Nf=n("code"),A6=a("train"),O6=a(`, we can\u2019t handle nested invocations because
`),If=n("code"),x6=a("torch.cuda.max_memory_allocated"),S6=a(" is a single counter, so if it gets reset by a nested eval call, "),zf=n("code"),C6=a("train"),N6=a(`\u2019s
tracker will report incorrect info. If this `),Cs=n("a"),I6=a("pytorch issue"),z6=a(`
gets resolved it will be possible to change this class to be re-entrant. Until then we will only track the outer
level of `),Ff=n("code"),F6=a("train"),L6=a(", "),Lf=n("code"),q6=a("evaluate"),U6=a(" and "),qf=n("code"),R6=a("predict"),M6=a(" methods. Which means that if "),Uf=n("code"),j6=a("eval"),H6=a(" is called during "),Rf=n("code"),G6=a("train"),W6=a(`,
it\u2019s the latter that will account for its memory usage and that of the former.`),B6=l(),De=n("p"),V6=a("This also means that if any other tool that is used along the "),ac=n("a"),Y6=a("Trainer"),Z6=a(` calls
`),Mf=n("code"),J6=a("torch.cuda.reset_peak_memory_stats"),Q6=a(`, the gpu peak memory stats could be invalid. And the
`),oc=n("a"),K6=a("Trainer"),X6=a(` will disrupt the normal behavior of any such tools that rely on calling
`),jf=n("code"),e7=a("torch.cuda.reset_peak_memory_stats"),t7=a(" themselves."),r7=l(),Hf=n("p"),a7=a("For best performance you may want to consider turning the memory profiling off for production runs."),o7=l(),Ra=n("div"),h(Ns.$$.fragment),n7=l(),Gf=n("p"),s7=a("Reformat Trainer metrics values to a human-readable format"),i7=l(),Tt=n("div"),h(Is.$$.fragment),l7=l(),zs=n("p"),d7=a("Helper to get number of samples in a "),Wf=n("code"),c7=a("DataLoader"),p7=a(" by accessing its dataset."),m7=l(),nc=n("p"),h7=a("Will raise an exception if the underlying dataset does not implement method "),Bf=n("code"),f7=a("__len__"),u7=l(),kt=n("div"),h(Fs.$$.fragment),_7=l(),Ls=n("p"),v7=a("Remove a callback from the current list of "),Vf=n("code"),g7=a("TrainerCallback"),b7=a(" and returns it."),y7=l(),qs=n("p"),E7=a("If the callback is not found, returns "),Yf=n("code"),w7=a("None"),$7=a(" (and no error is raised)."),T7=l(),X=n("div"),h(Us.$$.fragment),k7=l(),Zf=n("p"),D7=a("Run prediction and returns predictions and potential metrics."),P7=l(),Rs=n("p"),A7=a(`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Jf=n("code"),O7=a("evaluate()"),x7=a("."),S7=l(),h(Ma.$$.fragment),C7=l(),Ms=n("p"),N7=a("Returns: "),Qf=n("em"),I7=a("NamedTuple"),z7=a(" A namedtuple with the following keys:"),F7=l(),_r=n("ul"),vr=n("li"),L7=a("predictions ("),Kf=n("code"),q7=a("np.ndarray"),U7=a("): The predictions on "),Xf=n("code"),R7=a("test_dataset"),M7=a("."),j7=l(),js=n("li"),H7=a("label"),Hs=n("em"),G7=a("ids ("),eu=n("code"),W7=a("np.ndarray"),B7=a(", _optional"),V7=a("): The labels (if the dataset contained some)."),Y7=l(),gr=n("li"),Z7=a("metrics ("),tu=n("code"),J7=a("Dict[str, float]"),Q7=a(", "),ru=n("em"),K7=a("optional"),X7=a(`): The potential dictionary of metrics (if the dataset
contained labels).`),e8=l(),Dt=n("div"),h(Gs.$$.fragment),t8=l(),br=n("p"),r8=a("Prediction/evaluation loop, shared by "),au=n("code"),a8=a("Trainer.evaluate()"),o8=a(" and "),ou=n("code"),n8=a("Trainer.predict()"),s8=a("."),i8=l(),nu=n("p"),l8=a("Works both with or without labels."),d8=l(),Pt=n("div"),h(Ws.$$.fragment),c8=l(),yr=n("p"),p8=a("Perform an evaluation step on "),su=n("code"),m8=a("model"),h8=a(" using obj:"),iu=n("em"),f8=a("inputs"),u8=a("."),_8=l(),lu=n("p"),v8=a("Subclass and override to inject custom behavior."),g8=l(),ja=n("div"),h(Bs.$$.fragment),b8=l(),st=n("p"),y8=a("Upload "),du=n("em"),E8=a("self.model"),w8=a(" and "),cu=n("em"),$8=a("self.tokenizer"),T8=a(" to the \u{1F917} model hub on the repo "),pu=n("em"),k8=a("self.args.hub_model_id"),D8=a("."),P8=l(),Ha=n("div"),h(Vs.$$.fragment),A8=l(),Ys=n("p"),O8=a("Remove a callback from the current list of "),mu=n("code"),x8=a("TrainerCallback"),S8=a("."),C8=l(),Le=n("div"),h(Zs.$$.fragment),N8=l(),Js=n("p"),I8=a("Save metrics into a json file for that split, e.g. "),hu=n("code"),z8=a("train_results.json"),F8=a("."),L8=l(),fu=n("p"),q8=a("Under distributed environment this is done only for a process with rank 0."),U8=l(),Qs=n("p"),R8=a("To understand the metrics please read the docstring of "),uu=n("code"),M8=a("log_metrics()"),j8=a(`. The only
difference is that raw unformatted numbers are saved in the current method.`),H8=l(),At=n("div"),h(Ks.$$.fragment),G8=l(),Xs=n("p"),W8=a("Will save the model, so you can reload it using "),_u=n("code"),B8=a("from_pretrained()"),V8=a("."),Y8=l(),vu=n("p"),Z8=a("Will only save from the main process."),J8=l(),Ot=n("div"),h(ei.$$.fragment),Q8=l(),gu=n("p"),K8=a("Saves the Trainer state, since Trainer.save_model saves only the tokenizer with the model"),X8=l(),bu=n("p"),e9=a("Under distributed environment this is done only for a process with rank 0."),t9=l(),Ga=n("div"),h(ti.$$.fragment),r9=l(),yu=n("p"),a9=a("Main training entry point."),o9=l(),xt=n("div"),h(ri.$$.fragment),n9=l(),Eu=n("p"),s9=a("Perform a training step on a batch of inputs."),i9=l(),wu=n("p"),l9=a("Subclass and override to inject custom behavior."),db=l(),Er=n("h2"),Wa=n("a"),$u=n("span"),h(ai.$$.fragment),d9=l(),Tu=n("span"),c9=a("Seq2SeqTrainer"),cb=l(),wr=n("div"),qe=n("div"),h(oi.$$.fragment),p9=l(),ku=n("p"),m9=a("Run evaluation and returns metrics."),h9=l(),ni=n("p"),f9=a(`The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
(pass it to the init `),Du=n("code"),u9=a("compute_metrics"),_9=a(" argument)."),v9=l(),Pu=n("p"),g9=a("You can also subclass and override this method to inject custom behavior."),b9=l(),ee=n("div"),h(si.$$.fragment),y9=l(),Au=n("p"),E9=a("Run prediction and returns predictions and potential metrics."),w9=l(),ii=n("p"),$9=a(`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Ou=n("code"),T9=a("evaluate()"),k9=a("."),D9=l(),h(Ba.$$.fragment),P9=l(),li=n("p"),A9=a("Returns: "),xu=n("em"),O9=a("NamedTuple"),x9=a(" A namedtuple with the following keys:"),S9=l(),$r=n("ul"),Tr=n("li"),C9=a("predictions ("),Su=n("code"),N9=a("np.ndarray"),I9=a("): The predictions on "),Cu=n("code"),z9=a("test_dataset"),F9=a("."),L9=l(),di=n("li"),q9=a("label"),ci=n("em"),U9=a("ids ("),Nu=n("code"),R9=a("np.ndarray"),M9=a(", _optional"),j9=a("): The labels (if the dataset contained some)."),H9=l(),kr=n("li"),G9=a("metrics ("),Iu=n("code"),W9=a("Dict[str, float]"),B9=a(", "),zu=n("em"),V9=a("optional"),Y9=a(`): The potential dictionary of metrics (if the dataset
contained labels).`),pb=l(),Dr=n("h2"),Va=n("a"),Fu=n("span"),h(pi.$$.fragment),Z9=l(),Lu=n("span"),J9=a("TFTrainer"),mb=l(),T=n("div"),h(mi.$$.fragment),Q9=l(),qu=n("p"),K9=a("TFTrainer is a simple but feature-complete training and eval loop for TensorFlow, optimized for \u{1F917} Transformers."),X9=l(),St=n("div"),h(hi.$$.fragment),ek=l(),Uu=n("p"),tk=a("Setup the optimizer and the learning rate scheduler."),rk=l(),fi=n("p"),ak=a(`We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
TFTrainer\u2019s init through `),Ru=n("code"),ok=a("optimizers"),nk=a(", or subclass and override this method."),sk=l(),Ct=n("div"),h(ui.$$.fragment),ik=l(),Mu=n("p"),lk=a("Run evaluation and returns metrics."),dk=l(),_i=n("p"),ck=a(`The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
(pass it to the init `),ju=n("code"),pk=a("compute_metrics"),mk=a(" argument)."),hk=l(),Nt=n("div"),h(vi.$$.fragment),fk=l(),gi=n("p"),uk=a("Returns the evaluation "),Hu=n("code"),_k=a("Dataset"),vk=a("."),gk=l(),Gu=n("p"),bk=a("Subclass and override this method if you want to inject some custom behavior."),yk=l(),It=n("div"),h(bi.$$.fragment),Ek=l(),yi=n("p"),wk=a("Returns a test "),Wu=n("code"),$k=a("Dataset"),Tk=a("."),kk=l(),Bu=n("p"),Dk=a("Subclass and override this method if you want to inject some custom behavior."),Pk=l(),zt=n("div"),h(Ei.$$.fragment),Ak=l(),wi=n("p"),Ok=a("Returns the training "),Vu=n("code"),xk=a("Dataset"),Sk=a("."),Ck=l(),Yu=n("p"),Nk=a("Subclass and override this method if you want to inject some custom behavior."),Ik=l(),Ft=n("div"),h($i.$$.fragment),zk=l(),Ti=n("p"),Fk=a("Log "),Zu=n("code"),Lk=a("logs"),qk=a(" on the various objects watching training."),Uk=l(),Ju=n("p"),Rk=a("Subclass and override this method to inject custom behavior."),Mk=l(),pe=n("div"),h(ki.$$.fragment),jk=l(),Qu=n("p"),Hk=a("Run prediction and returns predictions and potential metrics."),Gk=l(),Di=n("p"),Wk=a(`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Ku=n("code"),Bk=a("evaluate()"),Vk=a("."),Yk=l(),Pi=n("p"),Zk=a("Returns: "),Xu=n("em"),Jk=a("NamedTuple"),Qk=a(" A namedtuple with the following keys:"),Kk=l(),Pr=n("ul"),Ar=n("li"),Xk=a("predictions ("),e_=n("code"),eD=a("np.ndarray"),tD=a("): The predictions on "),t_=n("code"),rD=a("test_dataset"),aD=a("."),oD=l(),Ai=n("li"),nD=a("label"),Oi=n("em"),sD=a("ids ("),r_=n("code"),iD=a("np.ndarray"),lD=a(", _optional"),dD=a("): The labels (if the dataset contained some)."),cD=l(),Or=n("li"),pD=a("metrics ("),a_=n("code"),mD=a("Dict[str, float]"),hD=a(", "),o_=n("em"),fD=a("optional"),uD=a(`): The potential dictionary of metrics (if the dataset
contained labels).`),_D=l(),Lt=n("div"),h(xi.$$.fragment),vD=l(),xr=n("p"),gD=a("Prediction/evaluation loop, shared by "),sc=n("a"),bD=a("evaluate()"),yD=a(` and
`),ic=n("a"),ED=a("predict()"),wD=a("."),$D=l(),n_=n("p"),TD=a("Works both with or without labels."),kD=l(),qt=n("div"),h(Si.$$.fragment),DD=l(),s_=n("p"),PD=a("Compute the prediction on features and update the loss with labels."),AD=l(),i_=n("p"),OD=a("Subclass and override to inject some custom behavior."),xD=l(),Ut=n("div"),h(Ci.$$.fragment),SD=l(),l_=n("p"),CD=a("Computes the loss of the given features and labels pair."),ND=l(),d_=n("p"),ID=a("Subclass and override this method if you want to inject some custom behavior."),zD=l(),Ya=n("div"),h(Ni.$$.fragment),FD=l(),Ii=n("p"),LD=a("Will save the model, so you can reload it using "),c_=n("code"),qD=a("from_pretrained()"),UD=a("."),RD=l(),Ue=n("div"),h(zi.$$.fragment),MD=l(),p_=n("p"),jD=a("Setup the optional Comet.ml integration."),HD=l(),Fi=n("p"),GD=a(`Environment:
COMET_MODE:
(Optional): str - \u201COFFLINE\u201D, \u201CONLINE\u201D, or \u201CDISABLED\u201D
COMET_PROJECT_NAME:
(Optional): str - Comet.ml project name for experiments
COMET_OFFLINE_DIRECTORY:
(Optional): str - folder to use for saving offline experiments when `),m_=n("code"),WD=a("COMET_MODE"),BD=a(" is \u201COFFLINE\u201D"),VD=l(),Li=n("p"),YD=a("For a number of configurable items in the environment, see "),h_=n("code"),ZD=a("here <https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables>"),JD=a("__"),QD=l(),Re=n("div"),h(qi.$$.fragment),KD=l(),Ui=n("p"),XD=a("Setup the optional Weights & Biases ("),f_=n("code"),eP=a("wandb"),tP=a(") integration."),rP=l(),Ri=n("p"),aP=a("One can subclass and override this method to customize the setup if needed. Find more information "),u_=n("code"),oP=a("here <https://docs.wandb.com/huggingface>"),nP=a("__. You can also override the following environment variables:"),sP=l(),__=n("p"),iP=a(`Environment:
WANDB_PROJECT:
(Optional): str - \u201Chuggingface\u201D by default, set this to a custom string to store results in a different
project.
WANDB_DISABLED:
(Optional): boolean - defaults to false, set to \u201Ctrue\u201D to disable wandb entirely.`),lP=l(),Za=n("div"),h(Mi.$$.fragment),dP=l(),v_=n("p"),cP=a("Train method to train the model."),pP=l(),Rt=n("div"),h(ji.$$.fragment),mP=l(),g_=n("p"),hP=a("Perform a training step on features and labels."),fP=l(),b_=n("p"),uP=a("Subclass and override to inject some custom behavior."),hb=l(),Sr=n("h2"),Ja=n("a"),y_=n("span"),h(Hi.$$.fragment),_P=l(),E_=n("span"),vP=a("TrainingArguments"),fb=l(),N=n("div"),h(Gi.$$.fragment),gP=l(),Wi=n("p"),bP=a("TrainingArguments is the subset of the arguments we use in our example scripts "),w_=n("strong"),yP=a(`which relate to the training loop
itself`),EP=a("."),wP=l(),Cr=n("p"),$P=a("Using "),lc=n("a"),TP=a("HfArgumentParser"),kP=a(" we can turn this class into "),Bi=n("a"),DP=a("argparse"),PP=a(` arguments that can be specified on the command
line.`),AP=l(),me=n("div"),h(Vi.$$.fragment),OP=l(),$_=n("p"),xP=a(`Returns the log level to be used depending on whether this process is the main process of node 0, main process
of node non-0, or a non-main process.`),SP=l(),Nr=n("p"),CP=a("For the main process the log level defaults to "),T_=n("code"),NP=a("logging.INFO"),IP=a(" unless overridden by "),k_=n("code"),zP=a("log_level"),FP=a(" argument."),LP=l(),Ir=n("p"),qP=a("For the replica processes the log level defaults to "),D_=n("code"),UP=a("logging.WARNING"),RP=a(` unless overridden by
`),P_=n("code"),MP=a("log_level_replica"),jP=a(" argument."),HP=l(),Yi=n("p"),GP=a(`The choice between the main and replica process settings is made according to the return value of
`),A_=n("code"),WP=a("should_log"),BP=a("."),VP=l(),Qa=n("div"),h(Zi.$$.fragment),YP=l(),O_=n("p"),ZP=a("Get number of steps used for a linear warmup."),JP=l(),Mt=n("div"),h(Ji.$$.fragment),QP=l(),x_=n("p"),KP=a(`A context manager for torch distributed environment where on needs to do something on the main process,
while blocking replicas, and when it\u2019s finished releasing the replicas.`),XP=l(),zr=n("p"),eA=a("One such use is for "),S_=n("code"),tA=a("datasets"),rA=a("\u2019s "),C_=n("code"),aA=a("map"),oA=a(` feature which to be efficient should be run once on the main
process, which upon completion saves a cached version of results and which then automatically gets loaded
by the replicas.`),nA=l(),Ka=n("div"),h(Qi.$$.fragment),sA=l(),Ki=n("p"),iA=a("Serializes this instance while replace "),N_=n("code"),lA=a("Enum"),dA=a(` by their values (for JSON serialization support). It obfuscates
the token values by removing their value.`),cA=l(),Xa=n("div"),h(Xi.$$.fragment),pA=l(),I_=n("p"),mA=a("Serializes this instance to a JSON string."),hA=l(),eo=n("div"),h(el.$$.fragment),fA=l(),z_=n("p"),uA=a("Sanitized serialization to use with TensorBoard\u2019s hparams"),ub=l(),Fr=n("h2"),to=n("a"),F_=n("span"),h(tl.$$.fragment),_A=l(),L_=n("span"),vA=a("Seq2SeqTrainingArguments"),_b=l(),se=n("div"),h(rl.$$.fragment),gA=l(),al=n("p"),bA=a("TrainingArguments is the subset of the arguments we use in our example scripts "),q_=n("strong"),yA=a(`which relate to the training loop
itself`),EA=a("."),wA=l(),Lr=n("p"),$A=a("Using "),dc=n("a"),TA=a("HfArgumentParser"),kA=a(" we can turn this class into "),ol=n("a"),DA=a("argparse"),PA=a(` arguments that can be specified on the command
line.`),AA=l(),Pe=n("p"),OA=a("sortish"),nl=n("em"),xA=a("sampler ("),U_=n("code"),SA=a("bool"),CA=a(", _optional"),NA=a(", defaults to "),R_=n("code"),IA=a("False"),zA=a(`):
Whether to use a `),M_=n("em"),FA=a("sortish sampler"),LA=a(" or not. Only possible if the underlying datasets are "),j_=n("em"),qA=a("Seq2SeqDataset"),UA=a(` for
now but will become generally available in the near future.`),RA=l(),C=n("p"),MA=a(`It sorts the inputs according to lengths in order to minimize the padding size, with a bit of randomness for
the training set.
predict`),sl=n("em"),jA=a("with_generate ("),H_=n("code"),HA=a("bool"),GA=a(", _optional"),WA=a(", defaults to "),G_=n("code"),BA=a("False"),VA=a(`):
Whether to use generate to calculate generative metrics (ROUGE, BLEU).
generation`),il=n("em"),YA=a("max_length ("),W_=n("code"),ZA=a("int"),JA=a(", _optional"),QA=a(`):
The `),B_=n("code"),KA=a("max_length"),XA=a(" to use on each evaluation loop when "),V_=n("code"),eO=a("predict_with_generate=True"),tO=a(`. Will default to
the `),Y_=n("code"),rO=a("max_length"),aO=a(` value of the model configuration.
generation`),ll=n("em"),oO=a("num_beams ("),Z_=n("code"),nO=a("int"),sO=a(", _optional"),iO=a(`):
The `),J_=n("code"),lO=a("num_beams"),dO=a(" to use on each evaluation loop when "),Q_=n("code"),cO=a("predict_with_generate=True"),pO=a(`. Will default to the
`),K_=n("code"),mO=a("num_beams"),hO=a(" value of the model configuration."),vb=l(),qr=n("h2"),ro=n("a"),X_=n("span"),h(dl.$$.fragment),fO=l(),ev=n("span"),uO=a("TFTrainingArguments"),gb=l(),it=n("div"),h(cl.$$.fragment),_O=l(),pl=n("p"),vO=a("TrainingArguments is the subset of the arguments we use in our example scripts "),tv=n("strong"),gO=a(`which relate to the training loop
itself`),bO=a("."),yO=l(),Ur=n("p"),EO=a("Using "),cc=n("a"),wO=a("HfArgumentParser"),$O=a(" we can turn this class into "),ml=n("a"),TO=a("argparse"),kO=a(` arguments that can be specified on the command
line.`),bb=l(),Rr=n("h2"),ao=n("a"),rv=n("span"),h(hl.$$.fragment),DO=l(),av=n("span"),PO=a("Checkpoints"),yb=l(),he=n("p"),AO=a("By default, "),pc=n("a"),OO=a("Trainer"),xO=a(" will save all checkpoints in the "),ov=n("code"),SO=a("output_dir"),CO=a(` you set in the
`),mc=n("a"),NO=a("TrainingArguments"),IO=a(" you are using. Those will go in subfolder named "),nv=n("code"),zO=a("checkpoint-xxx"),FO=a(` with xxx
being the step at which the training was at.`),Eb=l(),oo=n("p"),LO=a("Resuming training from a checkpoint can be done when calling "),hc=n("a"),qO=a("train()"),UO=a(" with either:"),wb=l(),no=n("ul"),fc=n("li"),sv=n("code"),RO=a("resume_from_checkpoint=True"),MO=a(" which will resume training from the latest checkpoint"),jO=l(),uc=n("li"),iv=n("code"),HO=a("resume_from_checkpoint=checkpoint_dir"),GO=a(` which will resume training from the specific checkpoint in the directory
passed.`),$b=l(),Me=n("p"),WO=a("In addition, you can easily save your checkpoints on the Model Hub when using "),lv=n("code"),BO=a("push_to_hub=True"),VO=a(`. By default, all
the models saved in intermediate checkpoints are saved in different commits, but not the optimizer state. You can adapt
the `),dv=n("code"),YO=a("hub-strategy"),ZO=a(" value of your "),_c=n("a"),JO=a("TrainingArguments"),QO=a(" to either:"),Tb=l(),so=n("ul"),io=n("li"),cv=n("code"),KO=a('"checkpoint"'),XO=a(`: the latest checkpoint is also pushed in a subfolder named last-checkpoint, allowing you to
resume training easily with `),pv=n("code"),ex=a('trainer.train(resume_from_checkpoint="output_dir/last-checkpoint")'),tx=a("."),rx=l(),vc=n("li"),mv=n("code"),ax=a('"all_checkpoints"'),ox=a(`: all checkpoints are pushed like they appear in the output folder (so you will get one
checkpoint folder per folder in your final repository)`),kb=l(),Mr=n("h2"),lo=n("a"),hv=n("span"),h(fl.$$.fragment),nx=l(),fv=n("span"),sx=a("Logging"),Db=l(),je=n("p"),ix=a("By default "),gc=n("a"),lx=a("Trainer"),dx=a(" will use "),uv=n("code"),cx=a("logging.INFO"),px=a(" for the main process and "),_v=n("code"),mx=a("logging.WARNING"),hx=a(` for
the replicas if any.`),Pb=l(),jt=n("p"),fx=a("These defaults can be overridden to use any of the 5 "),vv=n("code"),ux=a("logging"),_x=a(" levels with "),bc=n("a"),vx=a("TrainingArguments"),gx=a(`\u2019s
arguments:`),Ab=l(),co=n("ul"),yc=n("li"),gv=n("code"),bx=a("log_level"),yx=a(" - for the main process"),Ex=l(),Ec=n("li"),bv=n("code"),wx=a("log_level_replica"),$x=a(" - for the replicas"),Ob=l(),He=n("p"),Tx=a("Further, if "),wc=n("a"),kx=a("TrainingArguments"),Dx=a("\u2019s "),yv=n("code"),Px=a("log_on_each_node"),Ax=a(" is set to "),Ev=n("code"),Ox=a("False"),xx=a(` only the main node will
use the log level settings for its main process, all other nodes will use the log level settings for replicas.`),xb=l(),te=n("p"),Sx=a("Note that "),$c=n("a"),Cx=a("Trainer"),Nx=a(" is going to set "),wv=n("code"),Ix=a("transformers"),zx=a(`\u2019s log level separately for each node in its
`),$v=n("code"),Fx=a("__init__()"),Lx=a(`. So you may want to set this sooner (see the next example) if you tap into other
`),Tv=n("code"),qx=a("transformers"),Ux=a(" functionality before creating the "),Tc=n("a"),Rx=a("Trainer"),Mx=a(" object."),Sb=l(),kc=n("p"),jx=a("Here is an example of how this can be used in an application:"),Cb=l(),h(ul.$$.fragment),Nb=l(),Dc=n("p"),Hx=a(`And then if you only want to see warnings on the main node and all other nodes to not print any most likely duplicated
warnings you could run it as:`),Ib=l(),h(_l.$$.fragment),zb=l(),Pc=n("p"),Gx=a(`In the multi-node environment if you also don\u2019t want the logs to repeat for each node\u2019s main process, you will want to
change the above to:`),Fb=l(),h(vl.$$.fragment),Lb=l(),Ac=n("p"),Wx=a(`and then only the main process of the first node will log at the \u201Cwarning\u201D level, and all other processes on the main
node and all processes on other nodes will log at the \u201Cerror\u201D level.`),qb=l(),Oc=n("p"),Bx=a("If you need your application to be as quiet as possible you could do:"),Ub=l(),h(gl.$$.fragment),Rb=l(),po=n("p"),Vx=a("(add "),kv=n("code"),Yx=a("--log_on_each_node 0"),Zx=a(" if on multi-node environment)"),Mb=l(),jr=n("h2"),mo=n("a"),Dv=n("span"),h(bl.$$.fragment),Jx=l(),Pv=n("span"),Qx=a("Randomness"),jb=l(),fe=n("p"),Kx=a("When resuming from a checkpoint generated by "),xc=n("a"),Xx=a("Trainer"),eS=a(` all efforts are made to restore the
`),Av=n("em"),tS=a("python"),rS=a(", "),Ov=n("em"),aS=a("numpy"),oS=a(" and "),xv=n("em"),nS=a("pytorch"),sS=a(` RNG states to the same states as they were at the moment of saving that checkpoint,
which should make the \u201Cstop and resume\u201D style of training as close as possible to non-stop training.`),Hb=l(),Ht=n("p"),iS=a(`However, due to various default non-deterministic pytorch settings this might not fully work. If you want full
determinism please refer to `),yl=n("a"),lS=a("Controlling sources of randomness"),dS=a(`. As explained in the document, that some of those settings
that make things deterministic (.e.g., `),Sv=n("code"),cS=a("torch.backends.cudnn.deterministic"),pS=a(`) may slow things down, therefore this
can\u2019t be done by default, but you can enable those yourself if needed.`),Gb=l(),Hr=n("h2"),ho=n("a"),Cv=n("span"),h(El.$$.fragment),mS=l(),Nv=n("span"),hS=a("Trainer Integrations"),Wb=l(),fo=n("p"),fS=a("The "),Sc=n("a"),uS=a("Trainer"),_S=a(` has been extended to support libraries that may dramatically improve your training
time and fit much bigger models.`),Bb=l(),Ge=n("p"),vS=a("Currently it supports third party solutions, "),wl=n("a"),gS=a("DeepSpeed"),bS=a(" and "),$l=n("a"),yS=a("FairScale"),ES=a(", which implement parts of the paper "),Tl=n("a"),wS=a(`ZeRO: Memory Optimizations
Toward Training Trillion Parameter Models, by Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He`),$S=a("."),Vb=l(),Cc=n("p"),TS=a("This provided support is new and experimental as of this writing."),Yb=l(),Nc=n("a"),Zb=l(),Gr=n("h3"),uo=n("a"),Iv=n("span"),h(kl.$$.fragment),kS=l(),zv=n("span"),DS=a("CUDA Extension Installation Notes"),Jb=l(),Ic=n("p"),PS=a("As of this writing, both FairScale and Deepspeed require compilation of CUDA C++ code, before they can be used."),Qb=l(),Gt=n("p"),AS=a("While all installation issues should be dealt with through the corresponding GitHub Issues of "),Dl=n("a"),OS=a("FairScale"),xS=a(" and "),Pl=n("a"),SS=a("Deepspeed"),CS=a(`, there are a few common issues that one may encounter while building
any PyTorch extension that needs to build CUDA extensions.`),Kb=l(),zc=n("p"),NS=a("Therefore, if you encounter a CUDA-related build issue while doing one of the following or both:"),Xb=l(),h(Al.$$.fragment),ey=l(),Fc=n("p"),IS=a("please, read the following notes first."),ty=l(),Wt=n("p"),zS=a("In these notes we give examples for what to do when "),Fv=n("code"),FS=a("pytorch"),LS=a(" has been built with CUDA "),Lv=n("code"),qS=a("10.2"),US=a(`. If your situation is
different remember to adjust the version number to the one you are after.`),ry=l(),Wr=n("h4"),_o=n("a"),qv=n("span"),h(Ol.$$.fragment),RS=l(),Uv=n("span"),MS=a("Possible problem #1"),ay=l(),Lc=n("p"),jS=a(`While, Pytorch comes with its own CUDA toolkit, to build these two projects you must have an identical version of CUDA
installed system-wide.`),oy=l(),We=n("p"),HS=a("For example, if you installed "),Rv=n("code"),GS=a("pytorch"),WS=a(" with "),Mv=n("code"),BS=a("cudatoolkit==10.2"),VS=a(` in the Python environment, you also need to have
CUDA `),jv=n("code"),YS=a("10.2"),ZS=a(" installed system-wide."),ny=l(),Bt=n("p"),JS=a("The exact location may vary from system to system, but "),Hv=n("code"),QS=a("/usr/local/cuda-10.2"),KS=a(` is the most common location on many
Unix systems. When CUDA is correctly set up and added to the `),Gv=n("code"),XS=a("PATH"),eC=a(` environment variable, one can find the
installation location by doing:`),sy=l(),h(xl.$$.fragment),iy=l(),vo=n("p"),tC=a(`If you don\u2019t have CUDA installed system-wide, install it first. You will find the instructions by using your favorite
search engine. For example, if you\u2019re on Ubuntu you may want to search for: `),Sl=n("a"),rC=a("ubuntu cuda 10.2 install"),aC=a("."),ly=l(),Br=n("h4"),go=n("a"),Wv=n("span"),h(Cl.$$.fragment),oC=l(),Bv=n("span"),nC=a("Possible problem #2"),dy=l(),qc=n("p"),sC=a(`Another possible common problem is that you may have more than one CUDA toolkit installed system-wide. For example you
may have:`),cy=l(),h(Nl.$$.fragment),py=l(),Vt=n("p"),iC=a("Now, in this situation you need to make sure that your "),Vv=n("code"),lC=a("PATH"),dC=a(" and "),Yv=n("code"),cC=a("LD_LIBRARY_PATH"),pC=a(` environment variables contain
the correct paths to the desired CUDA version. Typically, package installers will set these to contain whatever the
last version was installed. If you encounter the problem, where the package build fails because it can\u2019t find the right
CUDA version despite you having it installed system-wide, it means that you need to adjust the 2 aforementioned
environment variables.`),my=l(),Uc=n("p"),mC=a("First, you may look at their contents:"),hy=l(),h(Il.$$.fragment),fy=l(),Rc=n("p"),hC=a("so you get an idea of what is inside."),uy=l(),bo=n("p"),fC=a("It\u2019s possible that "),Zv=n("code"),uC=a("LD_LIBRARY_PATH"),_C=a(" is empty."),_y=l(),lt=n("p"),Jv=n("code"),vC=a("PATH"),gC=a(" lists the locations of where executables can be found and "),Qv=n("code"),bC=a("LD_LIBRARY_PATH"),yC=a(` is for where shared libraries
are to looked for. In both cases, earlier entries have priority over the later ones. `),Kv=n("code"),EC=a(":"),wC=a(` is used to separate multiple
entries.`),vy=l(),Mc=n("p"),$C=a(`Now, to tell the build program where to find the specific CUDA toolkit, insert the desired paths to be listed first by
doing:`),gy=l(),h(zl.$$.fragment),by=l(),jc=n("p"),TC=a("Note that we aren\u2019t overwriting the existing values, but prepending instead."),yy=l(),Be=n("p"),kC=a(`Of course, adjust the version number, the full path if need be. Check that the directories you assign actually do
exist. `),Xv=n("code"),DC=a("lib64"),PC=a(" sub-directory is where the various CUDA "),eg=n("code"),AC=a(".so"),OC=a(" objects, like "),tg=n("code"),xC=a("libcudart.so"),SC=a(` reside, it\u2019s unlikely
that your system will have it named differently, but if it is adjust it to reflect your reality.`),Ey=l(),Vr=n("h4"),yo=n("a"),rg=n("span"),h(Fl.$$.fragment),CC=l(),ag=n("span"),NC=a("Possible problem #3"),wy=l(),Yt=n("p"),IC=a("Some older CUDA versions may refuse to build with newer compilers. For example, you my have "),og=n("code"),zC=a("gcc-9"),FC=a(` but it wants
`),ng=n("code"),LC=a("gcc-7"),qC=a("."),$y=l(),Hc=n("p"),UC=a("There are various ways to go about it."),Ty=l(),Gc=n("p"),RC=a("If you can install the latest CUDA toolkit it typically should support the newer compiler."),ky=l(),Eo=n("p"),MC=a(`Alternatively, you could install the lower version of the compiler in addition to the one you already have, or you may
already have it but it\u2019s not the default one, so the build system can\u2019t see it. If you have `),sg=n("code"),jC=a("gcc-7"),HC=a(` installed but the
build system complains it can\u2019t find it, the following might do the trick:`),Dy=l(),h(Ll.$$.fragment),Py=l(),B=n("p"),GC=a("Here, we are making a symlink to "),ig=n("code"),WC=a("gcc-7"),BC=a(" from "),lg=n("code"),VC=a("/usr/local/cuda-10.2/bin/gcc"),YC=a(` and since
`),dg=n("code"),ZC=a("/usr/local/cuda-10.2/bin/"),JC=a(" should be in the "),cg=n("code"),QC=a("PATH"),KC=a(` environment variable (see the previous problem\u2019s solution), it
should find `),pg=n("code"),XC=a("gcc-7"),eN=a(" (and "),mg=n("code"),tN=a("g++7"),rN=a(") and then the build will succeed."),Ay=l(),Wc=n("p"),aN=a("As always make sure to edit the paths in the example to match your situation."),Oy=l(),Yr=n("h3"),wo=n("a"),hg=n("span"),h(ql.$$.fragment),oN=l(),fg=n("span"),nN=a("FairScale"),xy=l(),Ve=n("p"),sN=a("By integrating "),Ul=n("a"),iN=a("FairScale"),lN=a(" the "),Bc=n("a"),dN=a("Trainer"),cN=a(`
provides support for the following features from `),Rl=n("a"),pN=a("the ZeRO paper"),mN=a(":"),Sy=l(),Ye=n("ol"),ug=n("li"),hN=a("Optimizer State Sharding"),fN=l(),_g=n("li"),uN=a("Gradient Sharding"),_N=l(),vg=n("li"),vN=a("Model Parameters Sharding (new and very experimental)"),gN=l(),gg=n("li"),bN=a("CPU offload (new and very experimental)"),Cy=l(),Vc=n("p"),yN=a("You will need at least two GPUs to use this feature."),Ny=l(),Ml=n("p"),bg=n("strong"),EN=a("Installation"),wN=a(":"),Iy=l(),Yc=n("p"),$N=a("Install the library via pypi:"),zy=l(),h(jl.$$.fragment),Fy=l(),Zt=n("p"),TN=a("or via "),yg=n("code"),kN=a("transformers"),DN=a("\u2019 "),Eg=n("code"),PN=a("extras"),AN=a(":"),Ly=l(),h(Hl.$$.fragment),qy=l(),$o=n("p"),ON=a("(will become available starting from "),wg=n("code"),xN=a("transformers==4.6.0"),SN=a(")"),Uy=l(),To=n("p"),CN=a("or find more details on "),Gl=n("a"),NN=a("the FairScale\u2019s GitHub page"),IN=a("."),Ry=l(),ko=n("p"),zN=a("If you\u2019re still struggling with the build, first make sure to read "),Zc=n("a"),FN=a("zero-install-notes"),LN=a("."),My=l(),Jc=n("p"),qN=a("If it\u2019s still not resolved the build issue, here are a few more ideas."),jy=l(),Wl=n("p"),$g=n("code"),UN=a("fairscale"),RN=a(` seems to have an issue with the recently introduced by pip build isolation feature. If you have a problem
with it, you may want to try one of:`),Hy=l(),h(Bl.$$.fragment),Gy=l(),Qc=n("p"),MN=a("or:"),Wy=l(),h(Vl.$$.fragment),By=l(),Yl=n("p"),Tg=n("code"),jN=a("fairscale"),HN=a(" also has issues with building against pytorch-nightly, so if you use it you may have to try one of:"),Vy=l(),h(Zl.$$.fragment),Yy=l(),Kc=n("p"),GN=a("or:"),Zy=l(),h(Jl.$$.fragment),Jy=l(),Xc=n("p"),WN=a("Of course, adjust the urls to match the cuda version you use."),Qy=l(),Do=n("p"),BN=a(`If after trying everything suggested you still encounter build issues, please, proceed with the GitHub Issue of
`),Ql=n("a"),VN=a("FairScale"),YN=a("."),Ky=l(),Kl=n("p"),kg=n("strong"),ZN=a("Usage"),JN=a(":"),Xy=l(),Jt=n("p"),QN=a("To use the first version of Sharded data-parallelism, add "),Dg=n("code"),KN=a("--sharded_ddp simple"),XN=a(` to the command line arguments, and
make sure you have added the distributed launcher `),Pg=n("code"),eI=a("-m torch.distributed.launch --nproc_per_node=NUMBER_OF_GPUS_YOU_HAVE"),tI=a(" if you haven\u2019t been using it already."),e2=l(),Po=n("p"),rI=a("For example here is how you could use it for "),Ag=n("code"),aI=a("run_translation.py"),oI=a(" with 2 GPUs:"),t2=l(),h(Xl.$$.fragment),r2=l(),ep=n("p"),nI=a("Notes:"),a2=l(),Ze=n("ul"),Og=n("li"),sI=a("This feature requires distributed training (so multiple GPUs)."),iI=l(),xg=n("li"),lI=a("It is not implemented for TPUs."),dI=l(),ed=n("li"),cI=a("It works with "),Sg=n("code"),pI=a("--fp16"),mI=a(" too, to make things even faster."),hI=l(),td=n("li"),fI=a("One of the main benefits of enabling "),Cg=n("code"),uI=a("--sharded_ddp simple"),_I=a(` is that it uses a lot less GPU memory, so you should be
able to use significantly larger batch sizes using the same hardware (e.g. 3x and even bigger) which should lead to
significantly shorter training time.`),o2=l(),rd=n("ol"),dt=n("li"),vI=a("To use the second version of Sharded data-parallelism, add "),Ng=n("code"),gI=a("--sharded_ddp zero_dp_2"),bI=a(" or "),Ig=n("code"),yI=a("--sharded_ddp zero_dp_3"),EI=a(" to the command line arguments, and make sure you have added the distributed launcher "),zg=n("code"),wI=a("-m torch.distributed.launch --nproc_per_node=NUMBER_OF_GPUS_YOU_HAVE"),$I=a(" if you haven\u2019t been using it already."),n2=l(),Ao=n("p"),TI=a("For example here is how you could use it for "),Fg=n("code"),kI=a("run_translation.py"),DI=a(" with 2 GPUs:"),s2=l(),h(ad.$$.fragment),i2=l(),Zr=n("p"),Lg=n("code"),PI=a("zero_dp_2"),AI=a(" is an optimized version of the simple wrapper, while "),qg=n("code"),OI=a("zero_dp_3"),xI=a(` fully shards model weights,
gradients and optimizer states.`),l2=l(),Qt=n("p"),SI=a("Both are compatible with adding "),Ug=n("code"),CI=a("cpu_offload"),NI=a(" to enable ZeRO-offload (activate it like this: "),Rg=n("code"),II=a('--sharded_ddp "zero_dp_2 cpu_offload"'),zI=a(")."),d2=l(),tp=n("p"),FI=a("Notes:"),c2=l(),ue=n("ul"),Mg=n("li"),LI=a("This feature requires distributed training (so multiple GPUs)."),qI=l(),jg=n("li"),UI=a("It is not implemented for TPUs."),RI=l(),od=n("li"),MI=a("It works with "),Hg=n("code"),jI=a("--fp16"),HI=a(" too, to make things even faster."),GI=l(),Jr=n("li"),WI=a("The "),Gg=n("code"),BI=a("cpu_offload"),VI=a(" additional option requires "),Wg=n("code"),YI=a("--fp16"),ZI=a("."),JI=l(),Bg=n("li"),QI=a(`This is an area of active development, so make sure you have a source install of fairscale to use this feature as
some bugs you encounter may have been fixed there already.`),p2=l(),rp=n("p"),KI=a("Known caveats:"),m2=l(),Oo=n("ul"),Qr=n("li"),XI=a("This feature is incompatible with "),Vg=n("code"),ez=a("--predict_with_generate"),tz=a(" in the "),Yg=n("em"),rz=a("run_translation.py"),az=a(" script."),oz=l(),Ae=n("li"),nz=a("Using "),Zg=n("code"),sz=a("--sharded_ddp zero_dp_3"),iz=a(` requires wrapping each layer of the model in the special container
`),Jg=n("code"),lz=a("FullyShardedDataParallelism"),dz=a(" of fairscale. It should be used with the option "),Qg=n("code"),cz=a("auto_wrap"),pz=a(` if you are not
doing this yourself: `),Kg=n("code"),mz=a('--sharded_ddp "zero_dp_3 auto_wrap"'),hz=a("."),h2=l(),Kr=n("h3"),xo=n("a"),Xg=n("span"),h(nd.$$.fragment),fz=l(),e1=n("span"),uz=a("DeepSpeed"),f2=l(),So=n("p"),_z=a("Moved to "),ap=n("a"),vz=a("deepspeed-trainer-integration"),gz=a("."),u2=l(),Xr=n("h4"),Co=n("a"),t1=n("span"),h(sd.$$.fragment),bz=l(),r1=n("span"),yz=a("Installation"),_2=l(),No=n("p"),Ez=a("Moved to "),op=n("a"),wz=a("deepspeed-installation"),$z=a("."),v2=l(),ea=n("h4"),Io=n("a"),a1=n("span"),h(id.$$.fragment),Tz=l(),o1=n("span"),kz=a("Deployment with multiple GPUs"),g2=l(),zo=n("p"),Dz=a("Moved to "),np=n("a"),Pz=a("deepspeed-multi-gpu"),Az=a("."),b2=l(),ta=n("h4"),Fo=n("a"),n1=n("span"),h(ld.$$.fragment),Oz=l(),s1=n("span"),xz=a("Deployment with one GPU"),y2=l(),Lo=n("p"),Sz=a("Moved to "),sp=n("a"),Cz=a("deepspeed-one-gpu"),Nz=a("."),E2=l(),ra=n("h4"),qo=n("a"),i1=n("span"),h(dd.$$.fragment),Iz=l(),l1=n("span"),zz=a("Deployment in Notebooks"),w2=l(),Uo=n("p"),Fz=a("Moved to "),ip=n("a"),Lz=a("deepspeed-notebook"),qz=a("."),$2=l(),aa=n("h4"),Ro=n("a"),d1=n("span"),h(cd.$$.fragment),Uz=l(),c1=n("span"),Rz=a("Configuration"),T2=l(),Mo=n("p"),Mz=a("Moved to "),lp=n("a"),jz=a("deepspeed-config"),Hz=a("."),k2=l(),oa=n("h4"),jo=n("a"),p1=n("span"),h(pd.$$.fragment),Gz=l(),m1=n("span"),Wz=a("Passing Configuration"),D2=l(),Ho=n("p"),Bz=a("Moved to "),dp=n("a"),Vz=a("deepspeed-config-passing"),Yz=a("."),P2=l(),na=n("h4"),Go=n("a"),h1=n("span"),h(md.$$.fragment),Zz=l(),f1=n("span"),Jz=a("Shared Configuration"),A2=l(),Wo=n("p"),Qz=a("Moved to "),cp=n("a"),Kz=a("deepspeed-config-shared"),Xz=a("."),O2=l(),sa=n("h4"),Bo=n("a"),u1=n("span"),h(hd.$$.fragment),eF=l(),_1=n("span"),tF=a("ZeRO"),x2=l(),Vo=n("p"),rF=a("Moved to "),pp=n("a"),aF=a("deepspeed-zero"),oF=a("."),S2=l(),ia=n("h5"),Yo=n("a"),v1=n("span"),h(fd.$$.fragment),nF=l(),g1=n("span"),sF=a("ZeRO-2 Config"),C2=l(),Zo=n("p"),iF=a("Moved to "),mp=n("a"),lF=a("deepspeed-zero2-config"),dF=a("."),N2=l(),la=n("h5"),Jo=n("a"),b1=n("span"),h(ud.$$.fragment),cF=l(),y1=n("span"),pF=a("ZeRO-3 Config"),I2=l(),Qo=n("p"),mF=a("Moved to "),hp=n("a"),hF=a("deepspeed-zero3-config"),fF=a("."),z2=l(),da=n("h4"),Ko=n("a"),E1=n("span"),h(_d.$$.fragment),uF=l(),w1=n("span"),_F=a("NVMe Support"),F2=l(),Xo=n("p"),vF=a("Moved to "),fp=n("a"),gF=a("deepspeed-nvme"),bF=a("."),L2=l(),ca=n("h5"),en=n("a"),$1=n("span"),h(vd.$$.fragment),yF=l(),T1=n("span"),EF=a("ZeRO-2 vs ZeRO-3 Performance"),q2=l(),tn=n("p"),wF=a("Moved to "),up=n("a"),$F=a("deepspeed-zero2-zero3-performance"),TF=a("."),U2=l(),pa=n("h5"),rn=n("a"),k1=n("span"),h(gd.$$.fragment),kF=l(),D1=n("span"),DF=a("ZeRO-2 Example"),R2=l(),an=n("p"),PF=a("Moved to "),_p=n("a"),AF=a("deepspeed-zero2-example"),OF=a("."),M2=l(),ma=n("h5"),on=n("a"),P1=n("span"),h(bd.$$.fragment),xF=l(),A1=n("span"),SF=a("ZeRO-3 Example"),j2=l(),nn=n("p"),CF=a("Moved to "),vp=n("a"),NF=a("deepspeed-zero3-example"),IF=a("."),H2=l(),ha=n("h4"),sn=n("a"),O1=n("span"),h(yd.$$.fragment),zF=l(),x1=n("span"),FF=a("Optimizer and Scheduler"),G2=l(),fa=n("h5"),ln=n("a"),S1=n("span"),h(Ed.$$.fragment),LF=l(),C1=n("span"),qF=a("Optimizer"),W2=l(),dn=n("p"),UF=a("Moved to "),gp=n("a"),RF=a("deepspeed-optimizer"),MF=a("."),B2=l(),ua=n("h5"),cn=n("a"),N1=n("span"),h(wd.$$.fragment),jF=l(),I1=n("span"),HF=a("Scheduler"),V2=l(),pn=n("p"),GF=a("Moved to "),bp=n("a"),WF=a("deepspeed-scheduler"),BF=a("."),Y2=l(),_a=n("h4"),mn=n("a"),z1=n("span"),h($d.$$.fragment),VF=l(),F1=n("span"),YF=a("fp32 Precision"),Z2=l(),hn=n("p"),ZF=a("Moved to "),yp=n("a"),JF=a("deepspeed-fp32"),QF=a("."),J2=l(),va=n("h4"),fn=n("a"),L1=n("span"),h(Td.$$.fragment),KF=l(),q1=n("span"),XF=a("Automatic Mixed Precision"),Q2=l(),un=n("p"),eL=a("Moved to "),Ep=n("a"),tL=a("deepspeed-amp"),rL=a("."),K2=l(),ga=n("h4"),_n=n("a"),U1=n("span"),h(kd.$$.fragment),aL=l(),R1=n("span"),oL=a("Batch Size"),X2=l(),vn=n("p"),nL=a("Moved to "),wp=n("a"),sL=a("deepspeed-bs"),iL=a("."),eE=l(),ba=n("h4"),gn=n("a"),M1=n("span"),h(Dd.$$.fragment),lL=l(),j1=n("span"),dL=a("Gradient Accumulation"),tE=l(),bn=n("p"),cL=a("Moved to "),$p=n("a"),pL=a("deepspeed-grad-acc"),mL=a("."),rE=l(),ya=n("h4"),yn=n("a"),H1=n("span"),h(Pd.$$.fragment),hL=l(),G1=n("span"),fL=a("Gradient Clipping"),aE=l(),En=n("p"),uL=a("Moved to "),Tp=n("a"),_L=a("deepspeed-grad-clip"),vL=a("."),oE=l(),Ea=n("h4"),wn=n("a"),W1=n("span"),h(Ad.$$.fragment),gL=l(),B1=n("span"),bL=a("Getting The Model Weights Out"),nE=l(),$n=n("p"),yL=a("Moved to "),kp=n("a"),EL=a("deepspeed-weight-extraction"),wL=a("."),this.h()},l(t){const c=LV('[data-svelte="svelte-1phssyn"]',document.head);w=s(c,"META",{name:!0,content:!0}),c.forEach(r),I=d(t),k=s(t,"H1",{class:!0});var Od=i(k);A=s(Od,"A",{id:!0,class:!0,href:!0});var V1=i(A);ve=s(V1,"SPAN",{});var Y1=i(ve);f(H.$$.fragment,Y1),Y1.forEach(r),V1.forEach(r),G=d(Od),J=s(Od,"SPAN",{});var Z1=i(J);ge=o(Z1,"Trainer"),Z1.forEach(r),Od.forEach(r),re=d(t),R=s(t,"P",{});var Tn=i(R);ie=o(Tn,"The "),le=s(Tn,"A",{href:!0});var GL=i(le);ae=o(GL,"Trainer"),GL.forEach(r),de=o(Tn," and "),Q=s(Tn,"A",{href:!0});var WL=i(Q);tt=o(WL,"TFTrainer"),WL.forEach(r),be=o(Tn,` classes provide an API for feature-complete
training in most standard use cases. It\u2019s used in most of the `),x=s(Tn,"A",{href:!0});var BL=i(x);M=o(BL,"example scripts"),BL.forEach(r),ct=o(Tn,"."),Tn.forEach(r),ye=d(t),q=s(t,"P",{});var Kt=i(q);pt=o(Kt,"Before instantiating your "),Ee=s(Kt,"A",{href:!0});var VL=i(Ee);Wn=o(VL,"Trainer"),VL.forEach(r),Bn=o(Kt,"/"),we=s(Kt,"A",{href:!0});var YL=i(we);Vn=o(YL,"TFTrainer"),YL.forEach(r),Yn=o(Kt,`, create a
`),xe=s(Kt,"A",{href:!0});var ZL=i(xe);Zn=o(ZL,"TrainingArguments"),ZL.forEach(r),Jn=o(Kt,"/"),W=s(Kt,"A",{href:!0});var JL=i(W);Se=o(JL,"TFTrainingArguments"),JL.forEach(r),Nd=o(Kt,` to access all the points of
customization during training.`),Kt.forEach(r),rt=d(t),oe=s(t,"P",{});var Dp=i(oe);Id=o(Dp,"The API supports distributed training on multiple GPUs/TPUs, mixed precision through "),Ce=s(Dp,"A",{href:!0,rel:!0});var QL=i(Ce);zd=o(QL,"NVIDIA Apex"),QL.forEach(r),ce=o(Dp," and Native AMP for PyTorch and "),Ta=s(Dp,"CODE",{});var KL=i(Ta);Fd=o(KL,"tf.keras.mixed_precision"),KL.forEach(r),Ld=o(Dp," for TensorFlow."),Dp.forEach(r),Qn=d(t),mt=s(t,"P",{});var Pp=i(mt);K0=o(Pp,"Both "),qd=s(Pp,"A",{href:!0});var XL=i(qd);X0=o(XL,"Trainer"),XL.forEach(r),e$=o(Pp," and "),Ud=s(Pp,"A",{href:!0});var eq=i(Ud);t$=o(eq,"TFTrainer"),eq.forEach(r),r$=o(Pp,` contain the basic training loop which supports
the above features. To inject custom behavior you can subclass them and override the following methods:`),Pp.forEach(r),rb=d(t),P=s(t,"UL",{});var O=i(P);ka=s(O,"LI",{});var J1=i(ka);Fm=s(J1,"STRONG",{});var tq=i(Fm);a$=o(tq,"get_train_dataloader"),tq.forEach(r),o$=o(J1,"/"),Lm=s(J1,"STRONG",{});var rq=i(Lm);n$=o(rq,"get_train_tfdataset"),rq.forEach(r),s$=o(J1," \u2014 Creates the training DataLoader (PyTorch) or TF Dataset."),J1.forEach(r),i$=d(O),Da=s(O,"LI",{});var Q1=i(Da);qm=s(Q1,"STRONG",{});var aq=i(qm);l$=o(aq,"get_eval_dataloader"),aq.forEach(r),d$=o(Q1,"/"),Um=s(Q1,"STRONG",{});var oq=i(Um);c$=o(oq,"get_eval_tfdataset"),oq.forEach(r),p$=o(Q1," \u2014 Creates the evaluation DataLoader (PyTorch) or TF Dataset."),Q1.forEach(r),m$=d(O),Pa=s(O,"LI",{});var K1=i(Pa);Rm=s(K1,"STRONG",{});var nq=i(Rm);h$=o(nq,"get_test_dataloader"),nq.forEach(r),f$=o(K1,"/"),Mm=s(K1,"STRONG",{});var sq=i(Mm);u$=o(sq,"get_test_tfdataset"),sq.forEach(r),_$=o(K1," \u2014 Creates the test DataLoader (PyTorch) or TF Dataset."),K1.forEach(r),v$=d(O),Rd=s(O,"LI",{});var $L=i(Rd);jm=s($L,"STRONG",{});var iq=i(jm);g$=o(iq,"log"),iq.forEach(r),b$=o($L," \u2014 Logs information on the various objects watching training."),$L.forEach(r),y$=d(O),ht=s(O,"LI",{});var xd=i(ht);Hm=s(xd,"STRONG",{});var lq=i(Hm);E$=o(lq,"create_optimizer_and_scheduler"),lq.forEach(r),w$=o(xd,` \u2014 Sets up the optimizer and learning rate scheduler if they were not passed at
init. Note, that you can also subclass or override the `),Gm=s(xd,"CODE",{});var dq=i(Gm);$$=o(dq,"create_optimizer"),dq.forEach(r),T$=o(xd," and "),Wm=s(xd,"CODE",{});var cq=i(Wm);k$=o(cq,"create_scheduler"),cq.forEach(r),D$=o(xd,` methods
separately.`),xd.forEach(r),P$=d(O),Md=s(O,"LI",{});var TL=i(Md);Bm=s(TL,"STRONG",{});var pq=i(Bm);A$=o(pq,"create_optimizer"),pq.forEach(r),O$=o(TL," \u2014 Sets up the optimizer if it wasn\u2019t passed at init."),TL.forEach(r),x$=d(O),jd=s(O,"LI",{});var kL=i(jd);Vm=s(kL,"STRONG",{});var mq=i(Vm);S$=o(mq,"create_scheduler"),mq.forEach(r),C$=o(kL," \u2014 Sets up the learning rate scheduler if it wasn\u2019t passed at init."),kL.forEach(r),N$=d(O),Hd=s(O,"LI",{});var DL=i(Hd);Ym=s(DL,"STRONG",{});var hq=i(Ym);I$=o(hq,"compute_loss"),hq.forEach(r),z$=o(DL," - Computes the loss on a batch of training inputs."),DL.forEach(r),F$=d(O),Gd=s(O,"LI",{});var PL=i(Gd);Zm=s(PL,"STRONG",{});var fq=i(Zm);L$=o(fq,"training_step"),fq.forEach(r),q$=o(PL," \u2014 Performs a training step."),PL.forEach(r),U$=d(O),Wd=s(O,"LI",{});var AL=i(Wd);Jm=s(AL,"STRONG",{});var uq=i(Jm);R$=o(uq,"prediction_step"),uq.forEach(r),M$=o(AL," \u2014 Performs an evaluation/test step."),AL.forEach(r),j$=d(O),Bd=s(O,"LI",{});var OL=i(Bd);Qm=s(OL,"STRONG",{});var _q=i(Qm);H$=o(_q,"run_model"),_q.forEach(r),G$=o(OL," (TensorFlow only) \u2014 Basic pass through the model."),OL.forEach(r),W$=d(O),Vd=s(O,"LI",{});var xL=i(Vd);Km=s(xL,"STRONG",{});var vq=i(Km);B$=o(vq,"evaluate"),vq.forEach(r),V$=o(xL," \u2014 Runs an evaluation loop and returns metrics."),xL.forEach(r),Y$=d(O),Yd=s(O,"LI",{});var SL=i(Yd);Xm=s(SL,"STRONG",{});var gq=i(Xm);Z$=o(gq,"predict"),gq.forEach(r),J$=o(SL," \u2014 Returns predictions (with metrics if labels are available) on a test set."),SL.forEach(r),O.forEach(r),ab=d(t),f(Aa.$$.fragment,t),ob=d(t),Oa=s(t,"P",{});var iE=i(Oa);Q$=o(iE,"Here is an example of how to customize "),Zd=s(iE,"A",{href:!0});var bq=i(Zd);K$=o(bq,"Trainer"),bq.forEach(r),X$=o(iE,` using a custom loss function for multi-label
classification:`),iE.forEach(r),nb=d(t),f(Kn.$$.fragment,t),sb=d(t),ft=s(t,"P",{});var Ap=i(ft);e4=o(Ap,"Another way to customize the training loop behavior for the PyTorch "),Jd=s(Ap,"A",{href:!0});var yq=i(Jd);t4=o(yq,"Trainer"),yq.forEach(r),r4=o(Ap,` is to use
`),Qd=s(Ap,"A",{href:!0});var Eq=i(Qd);a4=o(Eq,"callbacks"),Eq.forEach(r),o4=o(Ap,` that can inspect the training loop state (for progress reporting, logging on TensorBoard or
other ML platforms\u2026) and take decisions (like early stopping).`),Ap.forEach(r),ib=d(t),pr=s(t,"H2",{class:!0});var lE=i(pr);xa=s(lE,"A",{id:!0,class:!0,href:!0});var wq=i(xa);eh=s(wq,"SPAN",{});var $q=i(eh);f(Xn.$$.fragment,$q),$q.forEach(r),wq.forEach(r),n4=d(lE),th=s(lE,"SPAN",{});var Tq=i(th);s4=o(Tq,"Trainer"),Tq.forEach(r),lE.forEach(r),lb=d(t),b=s(t,"DIV",{class:!0});var y=i(b);f(es.$$.fragment,y),i4=d(y),rh=s(y,"P",{});var kq=i(rh);l4=o(kq,"Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for \u{1F917} Transformers."),kq.forEach(r),d4=d(y),ah=s(y,"P",{});var Dq=i(ah);c4=o(Dq,"Important attributes:"),Dq.forEach(r),p4=d(y),$e=s(y,"UL",{});var Xt=i($e);Sa=s(Xt,"LI",{});var X1=i(Sa);oh=s(X1,"STRONG",{});var Pq=i(oh);m4=o(Pq,"model"),Pq.forEach(r),h4=o(X1,` \u2014 Always points to the core model. If using a transformers model, it will be a
`),Kd=s(X1,"A",{href:!0});var Aq=i(Kd);f4=o(Aq,"PreTrainedModel"),Aq.forEach(r),u4=o(X1," subclass."),X1.forEach(r),_4=d(Xt),K=s(Xt,"LI",{});var Oe=i(K);nh=s(Oe,"STRONG",{});var Oq=i(nh);v4=o(Oq,"model_wrapped"),Oq.forEach(r),g4=o(Oe,` \u2014 Always points to the most external model in case one or more other modules wrap the
original model. This is the model that should be used for the forward pass. For example, under `),sh=s(Oe,"CODE",{});var xq=i(sh);b4=o(xq,"DeepSpeed"),xq.forEach(r),y4=o(Oe,`,
the inner model is wrapped in `),ih=s(Oe,"CODE",{});var Sq=i(ih);E4=o(Sq,"DeepSpeed"),Sq.forEach(r),w4=o(Oe," and then again in "),lh=s(Oe,"CODE",{});var Cq=i(lh);$4=o(Cq,"torch.nn.DistributedDataParallel"),Cq.forEach(r),T4=o(Oe,`. If the
inner model hasn\u2019t been wrapped, then `),dh=s(Oe,"CODE",{});var Nq=i(dh);k4=o(Nq,"self.model_wrapped"),Nq.forEach(r),D4=o(Oe," is the same as "),ch=s(Oe,"CODE",{});var Iq=i(ch);P4=o(Iq,"self.model"),Iq.forEach(r),A4=o(Oe,"."),Oe.forEach(r),O4=d(Xt),Xd=s(Xt,"LI",{});var CL=i(Xd);ph=s(CL,"STRONG",{});var zq=i(ph);x4=o(zq,"is_model_parallel"),zq.forEach(r),S4=o(CL,` \u2014 Whether or not a model has been switched to a model parallel mode (different from
data parallelism, this means some of the model layers are split on different GPUs).`),CL.forEach(r),C4=d(Xt),Ne=s(Xt,"LI",{});var wa=i(Ne);mh=s(wa,"STRONG",{});var Fq=i(mh);N4=o(Fq,"place_model_on_device"),Fq.forEach(r),I4=o(wa,` \u2014 Whether or not to automatically place the model on the device - it will be set
to `),hh=s(wa,"CODE",{});var Lq=i(hh);z4=o(Lq,"False"),Lq.forEach(r),F4=o(wa,` if model parallel or deepspeed is used, or if the default
`),fh=s(wa,"CODE",{});var qq=i(fh);L4=o(qq,"TrainingArguments.place_model_on_device"),qq.forEach(r),q4=o(wa," is overridden to return "),uh=s(wa,"CODE",{});var Uq=i(uh);U4=o(Uq,"False"),Uq.forEach(r),R4=o(wa," ."),wa.forEach(r),M4=d(Xt),Ie=s(Xt,"LI",{});var $a=i(Ie);_h=s($a,"STRONG",{});var Rq=i(_h);j4=o(Rq,"is_in_train"),Rq.forEach(r),H4=o($a," \u2014 Whether or not a model is currently running "),vh=s($a,"CODE",{});var Mq=i(vh);G4=o(Mq,"train"),Mq.forEach(r),W4=o($a," (e.g. when "),gh=s($a,"CODE",{});var jq=i(gh);B4=o(jq,"evaluate"),jq.forEach(r),V4=o($a,` is called
while in `),bh=s($a,"CODE",{});var Hq=i(bh);Y4=o(Hq,"train"),Hq.forEach(r),Z4=o($a,")"),$a.forEach(r),Xt.forEach(r),J4=d(y),Ca=s(y,"DIV",{class:!0});var dE=i(Ca);f(ts.$$.fragment,dE),Q4=d(dE),rs=s(dE,"P",{});var cE=i(rs);K4=o(cE,"Add a callback to the current list of "),yh=s(cE,"CODE",{});var Gq=i(yh);X4=o(Gq,"TrainerCallback"),Gq.forEach(r),e3=o(cE,"."),cE.forEach(r),dE.forEach(r),t3=d(y),Na=s(y,"DIV",{class:!0});var pE=i(Na);f(as.$$.fragment,pE),r3=d(pE),os=s(pE,"P",{});var mE=i(os);a3=o(mE,"A helper wrapper that creates an appropriate context manager for "),Eh=s(mE,"CODE",{});var Wq=i(Eh);o3=o(Wq,"autocast"),Wq.forEach(r),n3=o(mE,` while feeding it the desired
arguments, depending on the situation.`),mE.forEach(r),pE.forEach(r),s3=d(y),ut=s(y,"DIV",{class:!0});var Op=i(ut);f(ns.$$.fragment,Op),i3=d(Op),wh=s(Op,"P",{});var Bq=i(wh);l3=o(Bq,"How the loss is computed by Trainer. By default, all models return the loss in the first element."),Bq.forEach(r),d3=d(Op),$h=s(Op,"P",{});var Vq=i($h);c3=o(Vq,"Subclass and override for custom behavior."),Vq.forEach(r),Op.forEach(r),p3=d(y),_t=s(y,"DIV",{class:!0});var xp=i(_t);f(ss.$$.fragment,xp),m3=d(xp),Th=s(xp,"P",{});var Yq=i(Th);h3=o(Yq,"Setup the optimizer."),Yq.forEach(r),f3=d(xp),is=s(xp,"P",{});var hE=i(is);u3=o(hE,`We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
Trainer\u2019s init through `),kh=s(hE,"CODE",{});var Zq=i(kh);_3=o(Zq,"optimizers"),Zq.forEach(r),v3=o(hE,", or subclass and override this method in a subclass."),hE.forEach(r),xp.forEach(r),g3=d(y),vt=s(y,"DIV",{class:!0});var Sp=i(vt);f(ls.$$.fragment,Sp),b3=d(Sp),Dh=s(Sp,"P",{});var Jq=i(Dh);y3=o(Jq,"Setup the optimizer and the learning rate scheduler."),Jq.forEach(r),E3=d(Sp),at=s(Sp,"P",{});var kn=i(at);w3=o(kn,`We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
Trainer\u2019s init through `),Ph=s(kn,"CODE",{});var Qq=i(Ph);$3=o(Qq,"optimizers"),Qq.forEach(r),T3=o(kn,", or subclass and override this method (or "),Ah=s(kn,"CODE",{});var Kq=i(Ah);k3=o(Kq,"create_optimizer"),Kq.forEach(r),D3=o(kn,`
and/or `),Oh=s(kn,"CODE",{});var Xq=i(Oh);P3=o(Xq,"create_scheduler"),Xq.forEach(r),A3=o(kn,") in a subclass."),kn.forEach(r),Sp.forEach(r),O3=d(y),Ia=s(y,"DIV",{class:!0});var fE=i(Ia);f(ds.$$.fragment,fE),x3=d(fE),xh=s(fE,"P",{});var eU=i(xh);S3=o(eU,`Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or
passed as an argument.`),eU.forEach(r),fE.forEach(r),C3=d(y),ze=s(y,"DIV",{class:!0});var Dn=i(ze);f(cs.$$.fragment,Dn),N3=d(Dn),Sh=s(Dn,"P",{});var tU=i(Sh);I3=o(tU,"Run evaluation and returns metrics."),tU.forEach(r),z3=d(Dn),ps=s(Dn,"P",{});var uE=i(ps);F3=o(uE,`The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
(pass it to the init `),Ch=s(uE,"CODE",{});var rU=i(Ch);L3=o(rU,"compute_metrics"),rU.forEach(r),q3=o(uE," argument)."),uE.forEach(r),U3=d(Dn),Nh=s(Dn,"P",{});var aU=i(Nh);R3=o(aU,"You can also subclass and override this method to inject custom behavior."),aU.forEach(r),Dn.forEach(r),M3=d(y),gt=s(y,"DIV",{class:!0});var Cp=i(gt);f(ms.$$.fragment,Cp),j3=d(Cp),mr=s(Cp,"P",{});var Np=i(mr);H3=o(Np,"Prediction/evaluation loop, shared by "),Ih=s(Np,"CODE",{});var oU=i(Ih);G3=o(oU,"Trainer.evaluate()"),oU.forEach(r),W3=o(Np," and "),zh=s(Np,"CODE",{});var nU=i(zh);B3=o(nU,"Trainer.predict()"),nU.forEach(r),V3=o(Np,"."),Np.forEach(r),Y3=d(Cp),Fh=s(Cp,"P",{});var sU=i(Fh);Z3=o(sU,"Works both with or without labels."),sU.forEach(r),Cp.forEach(r),J3=d(y),za=s(y,"DIV",{class:!0});var _E=i(za);f(hs.$$.fragment,_E),Q3=d(_E),fs=s(_E,"P",{});var vE=i(fs);K3=o(vE,"For models that inherit from "),ec=s(vE,"A",{href:!0});var iU=i(ec);X3=o(iU,"PreTrainedModel"),iU.forEach(r),e5=o(vE,`, uses that method to compute the number of
floating point operations for every backward + forward pass. If using another model, either implement such a
method in the model or subclass and override this method.`),vE.forEach(r),_E.forEach(r),t5=d(y),bt=s(y,"DIV",{class:!0});var Ip=i(bt);f(us.$$.fragment,Ip),r5=d(Ip),_s=s(Ip,"P",{});var gE=i(_s);a5=o(gE,"Returns the evaluation "),Lh=s(gE,"CODE",{});var lU=i(Lh);o5=o(lU,"DataLoader"),lU.forEach(r),n5=o(gE,"."),gE.forEach(r),s5=d(Ip),qh=s(Ip,"P",{});var dU=i(qh);i5=o(dU,"Subclass and override this method if you want to inject some custom behavior."),dU.forEach(r),Ip.forEach(r),l5=d(y),yt=s(y,"DIV",{class:!0});var zp=i(yt);f(vs.$$.fragment,zp),d5=d(zp),gs=s(zp,"P",{});var bE=i(gs);c5=o(bE,"Returns the test "),Uh=s(bE,"CODE",{});var cU=i(Uh);p5=o(cU,"DataLoader"),cU.forEach(r),m5=o(bE,"."),bE.forEach(r),h5=d(zp),Rh=s(zp,"P",{});var pU=i(Rh);f5=o(pU,"Subclass and override this method if you want to inject some custom behavior."),pU.forEach(r),zp.forEach(r),u5=d(y),Fe=s(y,"DIV",{class:!0});var Pn=i(Fe);f(bs.$$.fragment,Pn),_5=d(Pn),ys=s(Pn,"P",{});var yE=i(ys);v5=o(yE,"Returns the training "),Mh=s(yE,"CODE",{});var mU=i(Mh);g5=o(mU,"DataLoader"),mU.forEach(r),b5=o(yE,"."),yE.forEach(r),y5=d(Pn),hr=s(Pn,"P",{});var Fp=i(hr);E5=o(Fp,"Will use no sampler if "),jh=s(Fp,"CODE",{});var hU=i(jh);w5=o(hU,"self.train_dataset"),hU.forEach(r),$5=o(Fp," does not implement "),Hh=s(Fp,"CODE",{});var fU=i(Hh);T5=o(fU,"__len__"),fU.forEach(r),k5=o(Fp,`, a random sampler (adapted
to distributed training if necessary) otherwise.`),Fp.forEach(r),D5=d(Pn),Gh=s(Pn,"P",{});var uU=i(Gh);P5=o(uU,"Subclass and override this method if you want to inject some custom behavior."),uU.forEach(r),Pn.forEach(r),A5=d(y),Et=s(y,"DIV",{class:!0});var Lp=i(Et);f(Es.$$.fragment,Lp),O5=d(Lp),Te=s(Lp,"P",{});var er=i(Te);x5=o(er,"Launch an hyperparameter search using "),Wh=s(er,"CODE",{});var _U=i(Wh);S5=o(_U,"optuna"),_U.forEach(r),C5=o(er," or "),Bh=s(er,"CODE",{});var vU=i(Bh);N5=o(vU,"Ray Tune"),vU.forEach(r),I5=o(er," or "),Vh=s(er,"CODE",{});var gU=i(Vh);z5=o(gU,"SigOpt"),gU.forEach(r),F5=o(er,`. The optimized quantity is
determined by `),Yh=s(er,"CODE",{});var bU=i(Yh);L5=o(bU,"compute_objective"),bU.forEach(r),q5=o(er,`, which defaults to a function returning the evaluation loss when no
metric is provided, the sum of all metrics otherwise.`),er.forEach(r),U5=d(Lp),f(Fa.$$.fragment,Lp),Lp.forEach(r),R5=d(y),La=s(y,"DIV",{class:!0});var EE=i(La);f(ws.$$.fragment,EE),M5=d(EE),$s=s(EE,"P",{});var wE=i($s);j5=o(wE,"Initializes a git repo in "),Zh=s(wE,"CODE",{});var yU=i(Zh);H5=o(yU,"self.args.hub_model_id"),yU.forEach(r),G5=o(wE,"."),wE.forEach(r),EE.forEach(r),W5=d(y),qa=s(y,"DIV",{class:!0});var $E=i(qa);f(Ts.$$.fragment,$E),B5=d($E),Jh=s($E,"P",{});var EU=i(Jh);V5=o(EU,`Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several
machines) main process.`),EU.forEach(r),$E.forEach(r),Y5=d(y),Ua=s(y,"DIV",{class:!0});var TE=i(Ua);f(ks.$$.fragment,TE),Z5=d(TE),Ds=s(TE,"P",{});var kE=i(Ds);J5=o(kE,`Whether or not this process is the global main process (when training in a distributed fashion on several
machines, this is only going to be `),Qh=s(kE,"CODE",{});var wU=i(Qh);Q5=o(wU,"True"),wU.forEach(r),K5=o(kE," for one process)."),kE.forEach(r),TE.forEach(r),X5=d(y),wt=s(y,"DIV",{class:!0});var qp=i(wt);f(Ps.$$.fragment,qp),eT=d(qp),As=s(qp,"P",{});var DE=i(As);tT=o(DE,"Log "),Kh=s(DE,"CODE",{});var $U=i(Kh);rT=o($U,"logs"),$U.forEach(r),aT=o(DE," on the various objects watching training."),DE.forEach(r),oT=d(qp),Xh=s(qp,"P",{});var TU=i(Xh);nT=o(TU,"Subclass and override this method to inject custom behavior."),TU.forEach(r),qp.forEach(r),sT=d(y),z=s(y,"DIV",{class:!0});var V=i(z);f(Os.$$.fragment,V),iT=d(V),ef=s(V,"P",{});var kU=i(ef);lT=o(kU,"Log metrics in a specially formatted way"),kU.forEach(r),dT=d(V),tf=s(V,"P",{});var DU=i(tf);cT=o(DU,"Under distributed environment this is done only for a process with rank 0."),DU.forEach(r),pT=d(V),rf=s(V,"P",{});var PU=i(rf);mT=o(PU,"Notes on memory reports:"),PU.forEach(r),hT=d(V),fr=s(V,"P",{});var Up=i(fr);fT=o(Up,"In order to get memory usage report you need to install "),af=s(Up,"CODE",{});var AU=i(af);uT=o(AU,"psutil"),AU.forEach(r),_T=o(Up,". You can do that with "),of=s(Up,"CODE",{});var OU=i(of);vT=o(OU,"pip install psutil"),OU.forEach(r),gT=o(Up,"."),Up.forEach(r),bT=d(V),nf=s(V,"P",{});var xU=i(nf);yT=o(xU,"Now when this method is run, you will see a report that will include: :"),xU.forEach(r),ET=d(V),f(xs.$$.fragment,V),wT=d(V),sf=s(V,"P",{});var SU=i(sf);lf=s(SU,"STRONG",{});var CU=i(lf);$T=o(CU,"Understanding the reports:"),CU.forEach(r),SU.forEach(r),TT=d(V),ot=s(V,"UL",{});var An=i(ot);df=s(An,"LI",{});var NU=i(df);ke=s(NU,"P",{});var tr=i(ke);kT=o(tr,"the first segment, e.g., "),cf=s(tr,"CODE",{});var IU=i(cf);DT=o(IU,"train__"),IU.forEach(r),PT=o(tr,", tells you which stage the metrics are for. Reports starting with "),pf=s(tr,"CODE",{});var zU=i(pf);AT=o(zU,"init_"),zU.forEach(r),OT=o(tr,`
will be added to the first stage that gets run. So that if only evaluation is run, the memory usage for the
`),mf=s(tr,"CODE",{});var FU=i(mf);xT=o(FU,"__init__"),FU.forEach(r),ST=o(tr," will be reported along with the "),hf=s(tr,"CODE",{});var LU=i(hf);CT=o(LU,"eval_"),LU.forEach(r),NT=o(tr," metrics."),tr.forEach(r),NU.forEach(r),IT=d(An),ff=s(An,"LI",{});var qU=i(ff);ur=s(qU,"P",{});var Rp=i(ur);zT=o(Rp,"the third segment, is either "),uf=s(Rp,"CODE",{});var UU=i(uf);FT=o(UU,"cpu"),UU.forEach(r),LT=o(Rp," or "),_f=s(Rp,"CODE",{});var RU=i(_f);qT=o(RU,"gpu"),RU.forEach(r),UT=o(Rp,`, tells you whether it\u2019s the general RAM or the gpu0 memory
metric.`),Rp.forEach(r),qU.forEach(r),RT=d(An),vf=s(An,"LI",{});var MU=i(vf);tc=s(MU,"P",{});var NL=i(tc);gf=s(NL,"CODE",{});var jU=i(gf);MT=o(jU,"*_alloc_delta"),jU.forEach(r),jT=o(NL,` - is the difference in the used/allocated memory counter between the end and the start of the
stage - it can be negative if a function released more memory than it allocated.`),NL.forEach(r),MU.forEach(r),HT=d(An),U=s(An,"LI",{});var Y=i(U);$t=s(Y,"P",{});var Sd=i($t);bf=s(Sd,"CODE",{});var HU=i(bf);GT=o(HU,"*_peaked_delta"),HU.forEach(r),WT=o(Sd,` - is any extra memory that was consumed and then freed - relative to the current allocated
memory counter - it is never negative. When you look at the metrics of any stage you add up `),yf=s(Sd,"CODE",{});var GU=i(yf);BT=o(GU,"alloc_delta"),GU.forEach(r),VT=o(Sd,` +
`),Ef=s(Sd,"CODE",{});var WU=i(Ef);YT=o(WU,"peaked_delta"),WU.forEach(r),ZT=o(Sd," and you know how much memory was needed to complete that stage."),Sd.forEach(r),JT=d(Y),wf=s(Y,"P",{});var BU=i(wf);QT=o(BU,`The reporting happens only for process of rank 0 and gpu 0 (if there is a gpu). Typically this is enough since the
main process does the bulk of work, but it could be not quite so if model parallel is used and then other GPUs may
use a different amount of gpu memory. This is also not the same under DataParallel where gpu0 may require much more
memory than the rest since it stores the gradient and optimizer states for all participating GPUS. Perhaps in the
future these reports will evolve to measure those too.`),BU.forEach(r),KT=d(Y),$f=s(Y,"P",{});var VU=i($f);XT=o(VU,`The CPU RAM metric measures RSS (Resident Set Size) includes both the memory which is unique to the process and the
memory shared with other processes. It is important to note that it does not include swapped out memory, so the
reports could be imprecise.`),VU.forEach(r),e6=d(Y),Ss=s(Y,"P",{});var PE=i(Ss);t6=o(PE,`The CPU peak memory is measured using a sampling thread. Due to python\u2019s GIL it may miss some of the peak memory if
that thread didn\u2019t get a chance to run when the highest memory was used. Therefore this report can be less than
reality. Using `),Tf=s(PE,"CODE",{});var YU=i(Tf);r6=o(YU,"tracemalloc"),YU.forEach(r),a6=o(PE,` would have reported the exact peak memory, but it doesn\u2019t report memory allocations
outside of python. So if some C++ CUDA extension allocated its own memory it won\u2019t be reported. And therefore it
was dropped in favor of the memory sampling approach, which reads the current process memory usage.`),PE.forEach(r),o6=d(Y),nt=s(Y,"P",{});var On=i(nt);n6=o(On,"The GPU allocated and peak memory reporting is done with "),kf=s(On,"CODE",{});var ZU=i(kf);s6=o(ZU,"torch.cuda.memory_allocated()"),ZU.forEach(r),i6=o(On,` and
`),Df=s(On,"CODE",{});var JU=i(Df);l6=o(JU,"torch.cuda.max_memory_allocated()"),JU.forEach(r),d6=o(On,`. This metric reports only \u201Cdeltas\u201D for pytorch-specific allocations, as
`),Pf=s(On,"CODE",{});var QU=i(Pf);c6=o(QU,"torch.cuda"),QU.forEach(r),p6=o(On,` memory management system doesn\u2019t track any memory allocated outside of pytorch. For example, the
very first cuda call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory.`),On.forEach(r),m6=d(Y),ne=s(Y,"P",{});var Je=i(ne);h6=o(Je,"Note that this tracker doesn\u2019t account for memory allocations outside of "),rc=s(Je,"A",{href:!0});var KU=i(rc);f6=o(KU,"Trainer"),KU.forEach(r),u6=o(Je,`\u2019s
`),Af=s(Je,"CODE",{});var XU=i(Af);_6=o(XU,"__init__"),XU.forEach(r),v6=o(Je,", "),Of=s(Je,"CODE",{});var eR=i(Of);g6=o(eR,"train"),eR.forEach(r),b6=o(Je,", "),xf=s(Je,"CODE",{});var tR=i(xf);y6=o(tR,"evaluate"),tR.forEach(r),E6=o(Je," and "),Sf=s(Je,"CODE",{});var rR=i(Sf);w6=o(rR,"predict"),rR.forEach(r),$6=o(Je," calls."),Je.forEach(r),T6=d(Y),S=s(Y,"P",{});var F=i(S);k6=o(F,"Because "),Cf=s(F,"CODE",{});var aR=i(Cf);D6=o(aR,"evaluation"),aR.forEach(r),P6=o(F," calls may happen during "),Nf=s(F,"CODE",{});var oR=i(Nf);A6=o(oR,"train"),oR.forEach(r),O6=o(F,`, we can\u2019t handle nested invocations because
`),If=s(F,"CODE",{});var nR=i(If);x6=o(nR,"torch.cuda.max_memory_allocated"),nR.forEach(r),S6=o(F," is a single counter, so if it gets reset by a nested eval call, "),zf=s(F,"CODE",{});var sR=i(zf);C6=o(sR,"train"),sR.forEach(r),N6=o(F,`\u2019s
tracker will report incorrect info. If this `),Cs=s(F,"A",{href:!0,rel:!0});var iR=i(Cs);I6=o(iR,"pytorch issue"),iR.forEach(r),z6=o(F,`
gets resolved it will be possible to change this class to be re-entrant. Until then we will only track the outer
level of `),Ff=s(F,"CODE",{});var lR=i(Ff);F6=o(lR,"train"),lR.forEach(r),L6=o(F,", "),Lf=s(F,"CODE",{});var dR=i(Lf);q6=o(dR,"evaluate"),dR.forEach(r),U6=o(F," and "),qf=s(F,"CODE",{});var cR=i(qf);R6=o(cR,"predict"),cR.forEach(r),M6=o(F," methods. Which means that if "),Uf=s(F,"CODE",{});var pR=i(Uf);j6=o(pR,"eval"),pR.forEach(r),H6=o(F," is called during "),Rf=s(F,"CODE",{});var mR=i(Rf);G6=o(mR,"train"),mR.forEach(r),W6=o(F,`,
it\u2019s the latter that will account for its memory usage and that of the former.`),F.forEach(r),B6=d(Y),De=s(Y,"P",{});var rr=i(De);V6=o(rr,"This also means that if any other tool that is used along the "),ac=s(rr,"A",{href:!0});var hR=i(ac);Y6=o(hR,"Trainer"),hR.forEach(r),Z6=o(rr,` calls
`),Mf=s(rr,"CODE",{});var fR=i(Mf);J6=o(fR,"torch.cuda.reset_peak_memory_stats"),fR.forEach(r),Q6=o(rr,`, the gpu peak memory stats could be invalid. And the
`),oc=s(rr,"A",{href:!0});var uR=i(oc);K6=o(uR,"Trainer"),uR.forEach(r),X6=o(rr,` will disrupt the normal behavior of any such tools that rely on calling
`),jf=s(rr,"CODE",{});var _R=i(jf);e7=o(_R,"torch.cuda.reset_peak_memory_stats"),_R.forEach(r),t7=o(rr," themselves."),rr.forEach(r),r7=d(Y),Hf=s(Y,"P",{});var vR=i(Hf);a7=o(vR,"For best performance you may want to consider turning the memory profiling off for production runs."),vR.forEach(r),Y.forEach(r),An.forEach(r),V.forEach(r),o7=d(y),Ra=s(y,"DIV",{class:!0});var AE=i(Ra);f(Ns.$$.fragment,AE),n7=d(AE),Gf=s(AE,"P",{});var gR=i(Gf);s7=o(gR,"Reformat Trainer metrics values to a human-readable format"),gR.forEach(r),AE.forEach(r),i7=d(y),Tt=s(y,"DIV",{class:!0});var Mp=i(Tt);f(Is.$$.fragment,Mp),l7=d(Mp),zs=s(Mp,"P",{});var OE=i(zs);d7=o(OE,"Helper to get number of samples in a "),Wf=s(OE,"CODE",{});var bR=i(Wf);c7=o(bR,"DataLoader"),bR.forEach(r),p7=o(OE," by accessing its dataset."),OE.forEach(r),m7=d(Mp),nc=s(Mp,"P",{});var IL=i(nc);h7=o(IL,"Will raise an exception if the underlying dataset does not implement method "),Bf=s(IL,"CODE",{});var yR=i(Bf);f7=o(yR,"__len__"),yR.forEach(r),IL.forEach(r),Mp.forEach(r),u7=d(y),kt=s(y,"DIV",{class:!0});var jp=i(kt);f(Fs.$$.fragment,jp),_7=d(jp),Ls=s(jp,"P",{});var xE=i(Ls);v7=o(xE,"Remove a callback from the current list of "),Vf=s(xE,"CODE",{});var ER=i(Vf);g7=o(ER,"TrainerCallback"),ER.forEach(r),b7=o(xE," and returns it."),xE.forEach(r),y7=d(jp),qs=s(jp,"P",{});var SE=i(qs);E7=o(SE,"If the callback is not found, returns "),Yf=s(SE,"CODE",{});var wR=i(Yf);w7=o(wR,"None"),wR.forEach(r),$7=o(SE," (and no error is raised)."),SE.forEach(r),jp.forEach(r),T7=d(y),X=s(y,"DIV",{class:!0});var Qe=i(X);f(Us.$$.fragment,Qe),k7=d(Qe),Zf=s(Qe,"P",{});var $R=i(Zf);D7=o($R,"Run prediction and returns predictions and potential metrics."),$R.forEach(r),P7=d(Qe),Rs=s(Qe,"P",{});var CE=i(Rs);A7=o(CE,`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Jf=s(CE,"CODE",{});var TR=i(Jf);O7=o(TR,"evaluate()"),TR.forEach(r),x7=o(CE,"."),CE.forEach(r),S7=d(Qe),f(Ma.$$.fragment,Qe),C7=d(Qe),Ms=s(Qe,"P",{});var NE=i(Ms);N7=o(NE,"Returns: "),Qf=s(NE,"EM",{});var kR=i(Qf);I7=o(kR,"NamedTuple"),kR.forEach(r),z7=o(NE," A namedtuple with the following keys:"),NE.forEach(r),F7=d(Qe),_r=s(Qe,"UL",{});var Hp=i(_r);vr=s(Hp,"LI",{});var Gp=i(vr);L7=o(Gp,"predictions ("),Kf=s(Gp,"CODE",{});var DR=i(Kf);q7=o(DR,"np.ndarray"),DR.forEach(r),U7=o(Gp,"): The predictions on "),Xf=s(Gp,"CODE",{});var PR=i(Xf);R7=o(PR,"test_dataset"),PR.forEach(r),M7=o(Gp,"."),Gp.forEach(r),j7=d(Hp),js=s(Hp,"LI",{});var IE=i(js);H7=o(IE,"label"),Hs=s(IE,"EM",{});var zE=i(Hs);G7=o(zE,"ids ("),eu=s(zE,"CODE",{});var AR=i(eu);W7=o(AR,"np.ndarray"),AR.forEach(r),B7=o(zE,", _optional"),zE.forEach(r),V7=o(IE,"): The labels (if the dataset contained some)."),IE.forEach(r),Y7=d(Hp),gr=s(Hp,"LI",{});var Wp=i(gr);Z7=o(Wp,"metrics ("),tu=s(Wp,"CODE",{});var OR=i(tu);J7=o(OR,"Dict[str, float]"),OR.forEach(r),Q7=o(Wp,", "),ru=s(Wp,"EM",{});var xR=i(ru);K7=o(xR,"optional"),xR.forEach(r),X7=o(Wp,`): The potential dictionary of metrics (if the dataset
contained labels).`),Wp.forEach(r),Hp.forEach(r),Qe.forEach(r),e8=d(y),Dt=s(y,"DIV",{class:!0});var Bp=i(Dt);f(Gs.$$.fragment,Bp),t8=d(Bp),br=s(Bp,"P",{});var Vp=i(br);r8=o(Vp,"Prediction/evaluation loop, shared by "),au=s(Vp,"CODE",{});var SR=i(au);a8=o(SR,"Trainer.evaluate()"),SR.forEach(r),o8=o(Vp," and "),ou=s(Vp,"CODE",{});var CR=i(ou);n8=o(CR,"Trainer.predict()"),CR.forEach(r),s8=o(Vp,"."),Vp.forEach(r),i8=d(Bp),nu=s(Bp,"P",{});var NR=i(nu);l8=o(NR,"Works both with or without labels."),NR.forEach(r),Bp.forEach(r),d8=d(y),Pt=s(y,"DIV",{class:!0});var Yp=i(Pt);f(Ws.$$.fragment,Yp),c8=d(Yp),yr=s(Yp,"P",{});var Zp=i(yr);p8=o(Zp,"Perform an evaluation step on "),su=s(Zp,"CODE",{});var IR=i(su);m8=o(IR,"model"),IR.forEach(r),h8=o(Zp," using obj:"),iu=s(Zp,"EM",{});var zR=i(iu);f8=o(zR,"inputs"),zR.forEach(r),u8=o(Zp,"."),Zp.forEach(r),_8=d(Yp),lu=s(Yp,"P",{});var FR=i(lu);v8=o(FR,"Subclass and override to inject custom behavior."),FR.forEach(r),Yp.forEach(r),g8=d(y),ja=s(y,"DIV",{class:!0});var FE=i(ja);f(Bs.$$.fragment,FE),b8=d(FE),st=s(FE,"P",{});var xn=i(st);y8=o(xn,"Upload "),du=s(xn,"EM",{});var LR=i(du);E8=o(LR,"self.model"),LR.forEach(r),w8=o(xn," and "),cu=s(xn,"EM",{});var qR=i(cu);$8=o(qR,"self.tokenizer"),qR.forEach(r),T8=o(xn," to the \u{1F917} model hub on the repo "),pu=s(xn,"EM",{});var UR=i(pu);k8=o(UR,"self.args.hub_model_id"),UR.forEach(r),D8=o(xn,"."),xn.forEach(r),FE.forEach(r),P8=d(y),Ha=s(y,"DIV",{class:!0});var LE=i(Ha);f(Vs.$$.fragment,LE),A8=d(LE),Ys=s(LE,"P",{});var qE=i(Ys);O8=o(qE,"Remove a callback from the current list of "),mu=s(qE,"CODE",{});var RR=i(mu);x8=o(RR,"TrainerCallback"),RR.forEach(r),S8=o(qE,"."),qE.forEach(r),LE.forEach(r),C8=d(y),Le=s(y,"DIV",{class:!0});var Sn=i(Le);f(Zs.$$.fragment,Sn),N8=d(Sn),Js=s(Sn,"P",{});var UE=i(Js);I8=o(UE,"Save metrics into a json file for that split, e.g. "),hu=s(UE,"CODE",{});var MR=i(hu);z8=o(MR,"train_results.json"),MR.forEach(r),F8=o(UE,"."),UE.forEach(r),L8=d(Sn),fu=s(Sn,"P",{});var jR=i(fu);q8=o(jR,"Under distributed environment this is done only for a process with rank 0."),jR.forEach(r),U8=d(Sn),Qs=s(Sn,"P",{});var RE=i(Qs);R8=o(RE,"To understand the metrics please read the docstring of "),uu=s(RE,"CODE",{});var HR=i(uu);M8=o(HR,"log_metrics()"),HR.forEach(r),j8=o(RE,`. The only
difference is that raw unformatted numbers are saved in the current method.`),RE.forEach(r),Sn.forEach(r),H8=d(y),At=s(y,"DIV",{class:!0});var Jp=i(At);f(Ks.$$.fragment,Jp),G8=d(Jp),Xs=s(Jp,"P",{});var ME=i(Xs);W8=o(ME,"Will save the model, so you can reload it using "),_u=s(ME,"CODE",{});var GR=i(_u);B8=o(GR,"from_pretrained()"),GR.forEach(r),V8=o(ME,"."),ME.forEach(r),Y8=d(Jp),vu=s(Jp,"P",{});var WR=i(vu);Z8=o(WR,"Will only save from the main process."),WR.forEach(r),Jp.forEach(r),J8=d(y),Ot=s(y,"DIV",{class:!0});var Qp=i(Ot);f(ei.$$.fragment,Qp),Q8=d(Qp),gu=s(Qp,"P",{});var BR=i(gu);K8=o(BR,"Saves the Trainer state, since Trainer.save_model saves only the tokenizer with the model"),BR.forEach(r),X8=d(Qp),bu=s(Qp,"P",{});var VR=i(bu);e9=o(VR,"Under distributed environment this is done only for a process with rank 0."),VR.forEach(r),Qp.forEach(r),t9=d(y),Ga=s(y,"DIV",{class:!0});var jE=i(Ga);f(ti.$$.fragment,jE),r9=d(jE),yu=s(jE,"P",{});var YR=i(yu);a9=o(YR,"Main training entry point."),YR.forEach(r),jE.forEach(r),o9=d(y),xt=s(y,"DIV",{class:!0});var Kp=i(xt);f(ri.$$.fragment,Kp),n9=d(Kp),Eu=s(Kp,"P",{});var ZR=i(Eu);s9=o(ZR,"Perform a training step on a batch of inputs."),ZR.forEach(r),i9=d(Kp),wu=s(Kp,"P",{});var JR=i(wu);l9=o(JR,"Subclass and override to inject custom behavior."),JR.forEach(r),Kp.forEach(r),y.forEach(r),db=d(t),Er=s(t,"H2",{class:!0});var HE=i(Er);Wa=s(HE,"A",{id:!0,class:!0,href:!0});var QR=i(Wa);$u=s(QR,"SPAN",{});var KR=i($u);f(ai.$$.fragment,KR),KR.forEach(r),QR.forEach(r),d9=d(HE),Tu=s(HE,"SPAN",{});var XR=i(Tu);c9=o(XR,"Seq2SeqTrainer"),XR.forEach(r),HE.forEach(r),cb=d(t),wr=s(t,"DIV",{class:!0});var GE=i(wr);qe=s(GE,"DIV",{class:!0});var Cn=i(qe);f(oi.$$.fragment,Cn),p9=d(Cn),ku=s(Cn,"P",{});var eM=i(ku);m9=o(eM,"Run evaluation and returns metrics."),eM.forEach(r),h9=d(Cn),ni=s(Cn,"P",{});var WE=i(ni);f9=o(WE,`The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
(pass it to the init `),Du=s(WE,"CODE",{});var tM=i(Du);u9=o(tM,"compute_metrics"),tM.forEach(r),_9=o(WE," argument)."),WE.forEach(r),v9=d(Cn),Pu=s(Cn,"P",{});var rM=i(Pu);g9=o(rM,"You can also subclass and override this method to inject custom behavior."),rM.forEach(r),Cn.forEach(r),b9=d(GE),ee=s(GE,"DIV",{class:!0});var Ke=i(ee);f(si.$$.fragment,Ke),y9=d(Ke),Au=s(Ke,"P",{});var aM=i(Au);E9=o(aM,"Run prediction and returns predictions and potential metrics."),aM.forEach(r),w9=d(Ke),ii=s(Ke,"P",{});var BE=i(ii);$9=o(BE,`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Ou=s(BE,"CODE",{});var oM=i(Ou);T9=o(oM,"evaluate()"),oM.forEach(r),k9=o(BE,"."),BE.forEach(r),D9=d(Ke),f(Ba.$$.fragment,Ke),P9=d(Ke),li=s(Ke,"P",{});var VE=i(li);A9=o(VE,"Returns: "),xu=s(VE,"EM",{});var nM=i(xu);O9=o(nM,"NamedTuple"),nM.forEach(r),x9=o(VE," A namedtuple with the following keys:"),VE.forEach(r),S9=d(Ke),$r=s(Ke,"UL",{});var Xp=i($r);Tr=s(Xp,"LI",{});var em=i(Tr);C9=o(em,"predictions ("),Su=s(em,"CODE",{});var sM=i(Su);N9=o(sM,"np.ndarray"),sM.forEach(r),I9=o(em,"): The predictions on "),Cu=s(em,"CODE",{});var iM=i(Cu);z9=o(iM,"test_dataset"),iM.forEach(r),F9=o(em,"."),em.forEach(r),L9=d(Xp),di=s(Xp,"LI",{});var YE=i(di);q9=o(YE,"label"),ci=s(YE,"EM",{});var ZE=i(ci);U9=o(ZE,"ids ("),Nu=s(ZE,"CODE",{});var lM=i(Nu);R9=o(lM,"np.ndarray"),lM.forEach(r),M9=o(ZE,", _optional"),ZE.forEach(r),j9=o(YE,"): The labels (if the dataset contained some)."),YE.forEach(r),H9=d(Xp),kr=s(Xp,"LI",{});var tm=i(kr);G9=o(tm,"metrics ("),Iu=s(tm,"CODE",{});var dM=i(Iu);W9=o(dM,"Dict[str, float]"),dM.forEach(r),B9=o(tm,", "),zu=s(tm,"EM",{});var cM=i(zu);V9=o(cM,"optional"),cM.forEach(r),Y9=o(tm,`): The potential dictionary of metrics (if the dataset
contained labels).`),tm.forEach(r),Xp.forEach(r),Ke.forEach(r),GE.forEach(r),pb=d(t),Dr=s(t,"H2",{class:!0});var JE=i(Dr);Va=s(JE,"A",{id:!0,class:!0,href:!0});var pM=i(Va);Fu=s(pM,"SPAN",{});var mM=i(Fu);f(pi.$$.fragment,mM),mM.forEach(r),pM.forEach(r),Z9=d(JE),Lu=s(JE,"SPAN",{});var hM=i(Lu);J9=o(hM,"TFTrainer"),hM.forEach(r),JE.forEach(r),mb=d(t),T=s(t,"DIV",{class:!0});var D=i(T);f(mi.$$.fragment,D),Q9=d(D),qu=s(D,"P",{});var fM=i(qu);K9=o(fM,"TFTrainer is a simple but feature-complete training and eval loop for TensorFlow, optimized for \u{1F917} Transformers."),fM.forEach(r),X9=d(D),St=s(D,"DIV",{class:!0});var rm=i(St);f(hi.$$.fragment,rm),ek=d(rm),Uu=s(rm,"P",{});var uM=i(Uu);tk=o(uM,"Setup the optimizer and the learning rate scheduler."),uM.forEach(r),rk=d(rm),fi=s(rm,"P",{});var QE=i(fi);ak=o(QE,`We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
TFTrainer\u2019s init through `),Ru=s(QE,"CODE",{});var _M=i(Ru);ok=o(_M,"optimizers"),_M.forEach(r),nk=o(QE,", or subclass and override this method."),QE.forEach(r),rm.forEach(r),sk=d(D),Ct=s(D,"DIV",{class:!0});var am=i(Ct);f(ui.$$.fragment,am),ik=d(am),Mu=s(am,"P",{});var vM=i(Mu);lk=o(vM,"Run evaluation and returns metrics."),vM.forEach(r),dk=d(am),_i=s(am,"P",{});var KE=i(_i);ck=o(KE,`The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
(pass it to the init `),ju=s(KE,"CODE",{});var gM=i(ju);pk=o(gM,"compute_metrics"),gM.forEach(r),mk=o(KE," argument)."),KE.forEach(r),am.forEach(r),hk=d(D),Nt=s(D,"DIV",{class:!0});var om=i(Nt);f(vi.$$.fragment,om),fk=d(om),gi=s(om,"P",{});var XE=i(gi);uk=o(XE,"Returns the evaluation "),Hu=s(XE,"CODE",{});var bM=i(Hu);_k=o(bM,"Dataset"),bM.forEach(r),vk=o(XE,"."),XE.forEach(r),gk=d(om),Gu=s(om,"P",{});var yM=i(Gu);bk=o(yM,"Subclass and override this method if you want to inject some custom behavior."),yM.forEach(r),om.forEach(r),yk=d(D),It=s(D,"DIV",{class:!0});var nm=i(It);f(bi.$$.fragment,nm),Ek=d(nm),yi=s(nm,"P",{});var ew=i(yi);wk=o(ew,"Returns a test "),Wu=s(ew,"CODE",{});var EM=i(Wu);$k=o(EM,"Dataset"),EM.forEach(r),Tk=o(ew,"."),ew.forEach(r),kk=d(nm),Bu=s(nm,"P",{});var wM=i(Bu);Dk=o(wM,"Subclass and override this method if you want to inject some custom behavior."),wM.forEach(r),nm.forEach(r),Pk=d(D),zt=s(D,"DIV",{class:!0});var sm=i(zt);f(Ei.$$.fragment,sm),Ak=d(sm),wi=s(sm,"P",{});var tw=i(wi);Ok=o(tw,"Returns the training "),Vu=s(tw,"CODE",{});var $M=i(Vu);xk=o($M,"Dataset"),$M.forEach(r),Sk=o(tw,"."),tw.forEach(r),Ck=d(sm),Yu=s(sm,"P",{});var TM=i(Yu);Nk=o(TM,"Subclass and override this method if you want to inject some custom behavior."),TM.forEach(r),sm.forEach(r),Ik=d(D),Ft=s(D,"DIV",{class:!0});var im=i(Ft);f($i.$$.fragment,im),zk=d(im),Ti=s(im,"P",{});var rw=i(Ti);Fk=o(rw,"Log "),Zu=s(rw,"CODE",{});var kM=i(Zu);Lk=o(kM,"logs"),kM.forEach(r),qk=o(rw," on the various objects watching training."),rw.forEach(r),Uk=d(im),Ju=s(im,"P",{});var DM=i(Ju);Rk=o(DM,"Subclass and override this method to inject custom behavior."),DM.forEach(r),im.forEach(r),Mk=d(D),pe=s(D,"DIV",{class:!0});var ar=i(pe);f(ki.$$.fragment,ar),jk=d(ar),Qu=s(ar,"P",{});var PM=i(Qu);Hk=o(PM,"Run prediction and returns predictions and potential metrics."),PM.forEach(r),Gk=d(ar),Di=s(ar,"P",{});var aw=i(Di);Wk=o(aw,`Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
will also return metrics, like in `),Ku=s(aw,"CODE",{});var AM=i(Ku);Bk=o(AM,"evaluate()"),AM.forEach(r),Vk=o(aw,"."),aw.forEach(r),Yk=d(ar),Pi=s(ar,"P",{});var ow=i(Pi);Zk=o(ow,"Returns: "),Xu=s(ow,"EM",{});var OM=i(Xu);Jk=o(OM,"NamedTuple"),OM.forEach(r),Qk=o(ow," A namedtuple with the following keys:"),ow.forEach(r),Kk=d(ar),Pr=s(ar,"UL",{});var lm=i(Pr);Ar=s(lm,"LI",{});var dm=i(Ar);Xk=o(dm,"predictions ("),e_=s(dm,"CODE",{});var xM=i(e_);eD=o(xM,"np.ndarray"),xM.forEach(r),tD=o(dm,"): The predictions on "),t_=s(dm,"CODE",{});var SM=i(t_);rD=o(SM,"test_dataset"),SM.forEach(r),aD=o(dm,"."),dm.forEach(r),oD=d(lm),Ai=s(lm,"LI",{});var nw=i(Ai);nD=o(nw,"label"),Oi=s(nw,"EM",{});var sw=i(Oi);sD=o(sw,"ids ("),r_=s(sw,"CODE",{});var CM=i(r_);iD=o(CM,"np.ndarray"),CM.forEach(r),lD=o(sw,", _optional"),sw.forEach(r),dD=o(nw,"): The labels (if the dataset contained some)."),nw.forEach(r),cD=d(lm),Or=s(lm,"LI",{});var cm=i(Or);pD=o(cm,"metrics ("),a_=s(cm,"CODE",{});var NM=i(a_);mD=o(NM,"Dict[str, float]"),NM.forEach(r),hD=o(cm,", "),o_=s(cm,"EM",{});var IM=i(o_);fD=o(IM,"optional"),IM.forEach(r),uD=o(cm,`): The potential dictionary of metrics (if the dataset
contained labels).`),cm.forEach(r),lm.forEach(r),ar.forEach(r),_D=d(D),Lt=s(D,"DIV",{class:!0});var pm=i(Lt);f(xi.$$.fragment,pm),vD=d(pm),xr=s(pm,"P",{});var mm=i(xr);gD=o(mm,"Prediction/evaluation loop, shared by "),sc=s(mm,"A",{href:!0});var zM=i(sc);bD=o(zM,"evaluate()"),zM.forEach(r),yD=o(mm,` and
`),ic=s(mm,"A",{href:!0});var FM=i(ic);ED=o(FM,"predict()"),FM.forEach(r),wD=o(mm,"."),mm.forEach(r),$D=d(pm),n_=s(pm,"P",{});var LM=i(n_);TD=o(LM,"Works both with or without labels."),LM.forEach(r),pm.forEach(r),kD=d(D),qt=s(D,"DIV",{class:!0});var hm=i(qt);f(Si.$$.fragment,hm),DD=d(hm),s_=s(hm,"P",{});var qM=i(s_);PD=o(qM,"Compute the prediction on features and update the loss with labels."),qM.forEach(r),AD=d(hm),i_=s(hm,"P",{});var UM=i(i_);OD=o(UM,"Subclass and override to inject some custom behavior."),UM.forEach(r),hm.forEach(r),xD=d(D),Ut=s(D,"DIV",{class:!0});var fm=i(Ut);f(Ci.$$.fragment,fm),SD=d(fm),l_=s(fm,"P",{});var RM=i(l_);CD=o(RM,"Computes the loss of the given features and labels pair."),RM.forEach(r),ND=d(fm),d_=s(fm,"P",{});var MM=i(d_);ID=o(MM,"Subclass and override this method if you want to inject some custom behavior."),MM.forEach(r),fm.forEach(r),zD=d(D),Ya=s(D,"DIV",{class:!0});var iw=i(Ya);f(Ni.$$.fragment,iw),FD=d(iw),Ii=s(iw,"P",{});var lw=i(Ii);LD=o(lw,"Will save the model, so you can reload it using "),c_=s(lw,"CODE",{});var jM=i(c_);qD=o(jM,"from_pretrained()"),jM.forEach(r),UD=o(lw,"."),lw.forEach(r),iw.forEach(r),RD=d(D),Ue=s(D,"DIV",{class:!0});var Nn=i(Ue);f(zi.$$.fragment,Nn),MD=d(Nn),p_=s(Nn,"P",{});var HM=i(p_);jD=o(HM,"Setup the optional Comet.ml integration."),HM.forEach(r),HD=d(Nn),Fi=s(Nn,"P",{});var dw=i(Fi);GD=o(dw,`Environment:
COMET_MODE:
(Optional): str - \u201COFFLINE\u201D, \u201CONLINE\u201D, or \u201CDISABLED\u201D
COMET_PROJECT_NAME:
(Optional): str - Comet.ml project name for experiments
COMET_OFFLINE_DIRECTORY:
(Optional): str - folder to use for saving offline experiments when `),m_=s(dw,"CODE",{});var GM=i(m_);WD=o(GM,"COMET_MODE"),GM.forEach(r),BD=o(dw," is \u201COFFLINE\u201D"),dw.forEach(r),VD=d(Nn),Li=s(Nn,"P",{});var cw=i(Li);YD=o(cw,"For a number of configurable items in the environment, see "),h_=s(cw,"CODE",{});var WM=i(h_);ZD=o(WM,"here <https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables>"),WM.forEach(r),JD=o(cw,"__"),cw.forEach(r),Nn.forEach(r),QD=d(D),Re=s(D,"DIV",{class:!0});var In=i(Re);f(qi.$$.fragment,In),KD=d(In),Ui=s(In,"P",{});var pw=i(Ui);XD=o(pw,"Setup the optional Weights & Biases ("),f_=s(pw,"CODE",{});var BM=i(f_);eP=o(BM,"wandb"),BM.forEach(r),tP=o(pw,") integration."),pw.forEach(r),rP=d(In),Ri=s(In,"P",{});var mw=i(Ri);aP=o(mw,"One can subclass and override this method to customize the setup if needed. Find more information "),u_=s(mw,"CODE",{});var VM=i(u_);oP=o(VM,"here <https://docs.wandb.com/huggingface>"),VM.forEach(r),nP=o(mw,"__. You can also override the following environment variables:"),mw.forEach(r),sP=d(In),__=s(In,"P",{});var YM=i(__);iP=o(YM,`Environment:
WANDB_PROJECT:
(Optional): str - \u201Chuggingface\u201D by default, set this to a custom string to store results in a different
project.
WANDB_DISABLED:
(Optional): boolean - defaults to false, set to \u201Ctrue\u201D to disable wandb entirely.`),YM.forEach(r),In.forEach(r),lP=d(D),Za=s(D,"DIV",{class:!0});var hw=i(Za);f(Mi.$$.fragment,hw),dP=d(hw),v_=s(hw,"P",{});var ZM=i(v_);cP=o(ZM,"Train method to train the model."),ZM.forEach(r),hw.forEach(r),pP=d(D),Rt=s(D,"DIV",{class:!0});var um=i(Rt);f(ji.$$.fragment,um),mP=d(um),g_=s(um,"P",{});var JM=i(g_);hP=o(JM,"Perform a training step on features and labels."),JM.forEach(r),fP=d(um),b_=s(um,"P",{});var QM=i(b_);uP=o(QM,"Subclass and override to inject some custom behavior."),QM.forEach(r),um.forEach(r),D.forEach(r),hb=d(t),Sr=s(t,"H2",{class:!0});var fw=i(Sr);Ja=s(fw,"A",{id:!0,class:!0,href:!0});var KM=i(Ja);y_=s(KM,"SPAN",{});var XM=i(y_);f(Hi.$$.fragment,XM),XM.forEach(r),KM.forEach(r),_P=d(fw),E_=s(fw,"SPAN",{});var ej=i(E_);vP=o(ej,"TrainingArguments"),ej.forEach(r),fw.forEach(r),fb=d(t),N=s(t,"DIV",{class:!0});var Z=i(N);f(Gi.$$.fragment,Z),gP=d(Z),Wi=s(Z,"P",{});var uw=i(Wi);bP=o(uw,"TrainingArguments is the subset of the arguments we use in our example scripts "),w_=s(uw,"STRONG",{});var tj=i(w_);yP=o(tj,`which relate to the training loop
itself`),tj.forEach(r),EP=o(uw,"."),uw.forEach(r),wP=d(Z),Cr=s(Z,"P",{});var _m=i(Cr);$P=o(_m,"Using "),lc=s(_m,"A",{href:!0});var rj=i(lc);TP=o(rj,"HfArgumentParser"),rj.forEach(r),kP=o(_m," we can turn this class into "),Bi=s(_m,"A",{href:!0,rel:!0});var aj=i(Bi);DP=o(aj,"argparse"),aj.forEach(r),PP=o(_m,` arguments that can be specified on the command
line.`),_m.forEach(r),AP=d(Z),me=s(Z,"DIV",{class:!0});var or=i(me);f(Vi.$$.fragment,or),OP=d(or),$_=s(or,"P",{});var oj=i($_);xP=o(oj,`Returns the log level to be used depending on whether this process is the main process of node 0, main process
of node non-0, or a non-main process.`),oj.forEach(r),SP=d(or),Nr=s(or,"P",{});var vm=i(Nr);CP=o(vm,"For the main process the log level defaults to "),T_=s(vm,"CODE",{});var nj=i(T_);NP=o(nj,"logging.INFO"),nj.forEach(r),IP=o(vm," unless overridden by "),k_=s(vm,"CODE",{});var sj=i(k_);zP=o(sj,"log_level"),sj.forEach(r),FP=o(vm," argument."),vm.forEach(r),LP=d(or),Ir=s(or,"P",{});var gm=i(Ir);qP=o(gm,"For the replica processes the log level defaults to "),D_=s(gm,"CODE",{});var ij=i(D_);UP=o(ij,"logging.WARNING"),ij.forEach(r),RP=o(gm,` unless overridden by
`),P_=s(gm,"CODE",{});var lj=i(P_);MP=o(lj,"log_level_replica"),lj.forEach(r),jP=o(gm," argument."),gm.forEach(r),HP=d(or),Yi=s(or,"P",{});var _w=i(Yi);GP=o(_w,`The choice between the main and replica process settings is made according to the return value of
`),A_=s(_w,"CODE",{});var dj=i(A_);WP=o(dj,"should_log"),dj.forEach(r),BP=o(_w,"."),_w.forEach(r),or.forEach(r),VP=d(Z),Qa=s(Z,"DIV",{class:!0});var vw=i(Qa);f(Zi.$$.fragment,vw),YP=d(vw),O_=s(vw,"P",{});var cj=i(O_);ZP=o(cj,"Get number of steps used for a linear warmup."),cj.forEach(r),vw.forEach(r),JP=d(Z),Mt=s(Z,"DIV",{class:!0});var bm=i(Mt);f(Ji.$$.fragment,bm),QP=d(bm),x_=s(bm,"P",{});var pj=i(x_);KP=o(pj,`A context manager for torch distributed environment where on needs to do something on the main process,
while blocking replicas, and when it\u2019s finished releasing the replicas.`),pj.forEach(r),XP=d(bm),zr=s(bm,"P",{});var ym=i(zr);eA=o(ym,"One such use is for "),S_=s(ym,"CODE",{});var mj=i(S_);tA=o(mj,"datasets"),mj.forEach(r),rA=o(ym,"\u2019s "),C_=s(ym,"CODE",{});var hj=i(C_);aA=o(hj,"map"),hj.forEach(r),oA=o(ym,` feature which to be efficient should be run once on the main
process, which upon completion saves a cached version of results and which then automatically gets loaded
by the replicas.`),ym.forEach(r),bm.forEach(r),nA=d(Z),Ka=s(Z,"DIV",{class:!0});var gw=i(Ka);f(Qi.$$.fragment,gw),sA=d(gw),Ki=s(gw,"P",{});var bw=i(Ki);iA=o(bw,"Serializes this instance while replace "),N_=s(bw,"CODE",{});var fj=i(N_);lA=o(fj,"Enum"),fj.forEach(r),dA=o(bw,` by their values (for JSON serialization support). It obfuscates
the token values by removing their value.`),bw.forEach(r),gw.forEach(r),cA=d(Z),Xa=s(Z,"DIV",{class:!0});var yw=i(Xa);f(Xi.$$.fragment,yw),pA=d(yw),I_=s(yw,"P",{});var uj=i(I_);mA=o(uj,"Serializes this instance to a JSON string."),uj.forEach(r),yw.forEach(r),hA=d(Z),eo=s(Z,"DIV",{class:!0});var Ew=i(eo);f(el.$$.fragment,Ew),fA=d(Ew),z_=s(Ew,"P",{});var _j=i(z_);uA=o(_j,"Sanitized serialization to use with TensorBoard\u2019s hparams"),_j.forEach(r),Ew.forEach(r),Z.forEach(r),ub=d(t),Fr=s(t,"H2",{class:!0});var ww=i(Fr);to=s(ww,"A",{id:!0,class:!0,href:!0});var vj=i(to);F_=s(vj,"SPAN",{});var gj=i(F_);f(tl.$$.fragment,gj),gj.forEach(r),vj.forEach(r),_A=d(ww),L_=s(ww,"SPAN",{});var bj=i(L_);vA=o(bj,"Seq2SeqTrainingArguments"),bj.forEach(r),ww.forEach(r),_b=d(t),se=s(t,"DIV",{class:!0});var nr=i(se);f(rl.$$.fragment,nr),gA=d(nr),al=s(nr,"P",{});var $w=i(al);bA=o($w,"TrainingArguments is the subset of the arguments we use in our example scripts "),q_=s($w,"STRONG",{});var yj=i(q_);yA=o(yj,`which relate to the training loop
itself`),yj.forEach(r),EA=o($w,"."),$w.forEach(r),wA=d(nr),Lr=s(nr,"P",{});var Em=i(Lr);$A=o(Em,"Using "),dc=s(Em,"A",{href:!0});var Ej=i(dc);TA=o(Ej,"HfArgumentParser"),Ej.forEach(r),kA=o(Em," we can turn this class into "),ol=s(Em,"A",{href:!0,rel:!0});var wj=i(ol);DA=o(wj,"argparse"),wj.forEach(r),PA=o(Em,` arguments that can be specified on the command
line.`),Em.forEach(r),AA=d(nr),Pe=s(nr,"P",{});var sr=i(Pe);OA=o(sr,"sortish"),nl=s(sr,"EM",{});var Tw=i(nl);xA=o(Tw,"sampler ("),U_=s(Tw,"CODE",{});var $j=i(U_);SA=o($j,"bool"),$j.forEach(r),CA=o(Tw,", _optional"),Tw.forEach(r),NA=o(sr,", defaults to "),R_=s(sr,"CODE",{});var Tj=i(R_);IA=o(Tj,"False"),Tj.forEach(r),zA=o(sr,`):
Whether to use a `),M_=s(sr,"EM",{});var kj=i(M_);FA=o(kj,"sortish sampler"),kj.forEach(r),LA=o(sr," or not. Only possible if the underlying datasets are "),j_=s(sr,"EM",{});var Dj=i(j_);qA=o(Dj,"Seq2SeqDataset"),Dj.forEach(r),UA=o(sr,` for
now but will become generally available in the near future.`),sr.forEach(r),RA=d(nr),C=s(nr,"P",{});var L=i(C);MA=o(L,`It sorts the inputs according to lengths in order to minimize the padding size, with a bit of randomness for
the training set.
predict`),sl=s(L,"EM",{});var kw=i(sl);jA=o(kw,"with_generate ("),H_=s(kw,"CODE",{});var Pj=i(H_);HA=o(Pj,"bool"),Pj.forEach(r),GA=o(kw,", _optional"),kw.forEach(r),WA=o(L,", defaults to "),G_=s(L,"CODE",{});var Aj=i(G_);BA=o(Aj,"False"),Aj.forEach(r),VA=o(L,`):
Whether to use generate to calculate generative metrics (ROUGE, BLEU).
generation`),il=s(L,"EM",{});var Dw=i(il);YA=o(Dw,"max_length ("),W_=s(Dw,"CODE",{});var Oj=i(W_);ZA=o(Oj,"int"),Oj.forEach(r),JA=o(Dw,", _optional"),Dw.forEach(r),QA=o(L,`):
The `),B_=s(L,"CODE",{});var xj=i(B_);KA=o(xj,"max_length"),xj.forEach(r),XA=o(L," to use on each evaluation loop when "),V_=s(L,"CODE",{});var Sj=i(V_);eO=o(Sj,"predict_with_generate=True"),Sj.forEach(r),tO=o(L,`. Will default to
the `),Y_=s(L,"CODE",{});var Cj=i(Y_);rO=o(Cj,"max_length"),Cj.forEach(r),aO=o(L,` value of the model configuration.
generation`),ll=s(L,"EM",{});var Pw=i(ll);oO=o(Pw,"num_beams ("),Z_=s(Pw,"CODE",{});var Nj=i(Z_);nO=o(Nj,"int"),Nj.forEach(r),sO=o(Pw,", _optional"),Pw.forEach(r),iO=o(L,`):
The `),J_=s(L,"CODE",{});var Ij=i(J_);lO=o(Ij,"num_beams"),Ij.forEach(r),dO=o(L," to use on each evaluation loop when "),Q_=s(L,"CODE",{});var zj=i(Q_);cO=o(zj,"predict_with_generate=True"),zj.forEach(r),pO=o(L,`. Will default to the
`),K_=s(L,"CODE",{});var Fj=i(K_);mO=o(Fj,"num_beams"),Fj.forEach(r),hO=o(L," value of the model configuration."),L.forEach(r),nr.forEach(r),vb=d(t),qr=s(t,"H2",{class:!0});var Aw=i(qr);ro=s(Aw,"A",{id:!0,class:!0,href:!0});var Lj=i(ro);X_=s(Lj,"SPAN",{});var qj=i(X_);f(dl.$$.fragment,qj),qj.forEach(r),Lj.forEach(r),fO=d(Aw),ev=s(Aw,"SPAN",{});var Uj=i(ev);uO=o(Uj,"TFTrainingArguments"),Uj.forEach(r),Aw.forEach(r),gb=d(t),it=s(t,"DIV",{class:!0});var wm=i(it);f(cl.$$.fragment,wm),_O=d(wm),pl=s(wm,"P",{});var Ow=i(pl);vO=o(Ow,"TrainingArguments is the subset of the arguments we use in our example scripts "),tv=s(Ow,"STRONG",{});var Rj=i(tv);gO=o(Rj,`which relate to the training loop
itself`),Rj.forEach(r),bO=o(Ow,"."),Ow.forEach(r),yO=d(wm),Ur=s(wm,"P",{});var $m=i(Ur);EO=o($m,"Using "),cc=s($m,"A",{href:!0});var Mj=i(cc);wO=o(Mj,"HfArgumentParser"),Mj.forEach(r),$O=o($m," we can turn this class into "),ml=s($m,"A",{href:!0,rel:!0});var jj=i(ml);TO=o(jj,"argparse"),jj.forEach(r),kO=o($m,` arguments that can be specified on the command
line.`),$m.forEach(r),wm.forEach(r),bb=d(t),Rr=s(t,"H2",{class:!0});var xw=i(Rr);ao=s(xw,"A",{id:!0,class:!0,href:!0});var Hj=i(ao);rv=s(Hj,"SPAN",{});var Gj=i(rv);f(hl.$$.fragment,Gj),Gj.forEach(r),Hj.forEach(r),DO=d(xw),av=s(xw,"SPAN",{});var Wj=i(av);PO=o(Wj,"Checkpoints"),Wj.forEach(r),xw.forEach(r),yb=d(t),he=s(t,"P",{});var ir=i(he);AO=o(ir,"By default, "),pc=s(ir,"A",{href:!0});var Bj=i(pc);OO=o(Bj,"Trainer"),Bj.forEach(r),xO=o(ir," will save all checkpoints in the "),ov=s(ir,"CODE",{});var Vj=i(ov);SO=o(Vj,"output_dir"),Vj.forEach(r),CO=o(ir,` you set in the
`),mc=s(ir,"A",{href:!0});var Yj=i(mc);NO=o(Yj,"TrainingArguments"),Yj.forEach(r),IO=o(ir," you are using. Those will go in subfolder named "),nv=s(ir,"CODE",{});var Zj=i(nv);zO=o(Zj,"checkpoint-xxx"),Zj.forEach(r),FO=o(ir,` with xxx
being the step at which the training was at.`),ir.forEach(r),Eb=d(t),oo=s(t,"P",{});var Sw=i(oo);LO=o(Sw,"Resuming training from a checkpoint can be done when calling "),hc=s(Sw,"A",{href:!0});var Jj=i(hc);qO=o(Jj,"train()"),Jj.forEach(r),UO=o(Sw," with either:"),Sw.forEach(r),wb=d(t),no=s(t,"UL",{});var Cw=i(no);fc=s(Cw,"LI",{});var zL=i(fc);sv=s(zL,"CODE",{});var Qj=i(sv);RO=o(Qj,"resume_from_checkpoint=True"),Qj.forEach(r),MO=o(zL," which will resume training from the latest checkpoint"),zL.forEach(r),jO=d(Cw),uc=s(Cw,"LI",{});var FL=i(uc);iv=s(FL,"CODE",{});var Kj=i(iv);HO=o(Kj,"resume_from_checkpoint=checkpoint_dir"),Kj.forEach(r),GO=o(FL,` which will resume training from the specific checkpoint in the directory
passed.`),FL.forEach(r),Cw.forEach(r),$b=d(t),Me=s(t,"P",{});var zn=i(Me);WO=o(zn,"In addition, you can easily save your checkpoints on the Model Hub when using "),lv=s(zn,"CODE",{});var Xj=i(lv);BO=o(Xj,"push_to_hub=True"),Xj.forEach(r),VO=o(zn,`. By default, all
the models saved in intermediate checkpoints are saved in different commits, but not the optimizer state. You can adapt
the `),dv=s(zn,"CODE",{});var eH=i(dv);YO=o(eH,"hub-strategy"),eH.forEach(r),ZO=o(zn," value of your "),_c=s(zn,"A",{href:!0});var tH=i(_c);JO=o(tH,"TrainingArguments"),tH.forEach(r),QO=o(zn," to either:"),zn.forEach(r),Tb=d(t),so=s(t,"UL",{});var Nw=i(so);io=s(Nw,"LI",{});var eb=i(io);cv=s(eb,"CODE",{});var rH=i(cv);KO=o(rH,'"checkpoint"'),rH.forEach(r),XO=o(eb,`: the latest checkpoint is also pushed in a subfolder named last-checkpoint, allowing you to
resume training easily with `),pv=s(eb,"CODE",{});var aH=i(pv);ex=o(aH,'trainer.train(resume_from_checkpoint="output_dir/last-checkpoint")'),aH.forEach(r),tx=o(eb,"."),eb.forEach(r),rx=d(Nw),vc=s(Nw,"LI",{});var LL=i(vc);mv=s(LL,"CODE",{});var oH=i(mv);ax=o(oH,'"all_checkpoints"'),oH.forEach(r),ox=o(LL,`: all checkpoints are pushed like they appear in the output folder (so you will get one
checkpoint folder per folder in your final repository)`),LL.forEach(r),Nw.forEach(r),kb=d(t),Mr=s(t,"H2",{class:!0});var Iw=i(Mr);lo=s(Iw,"A",{id:!0,class:!0,href:!0});var nH=i(lo);hv=s(nH,"SPAN",{});var sH=i(hv);f(fl.$$.fragment,sH),sH.forEach(r),nH.forEach(r),nx=d(Iw),fv=s(Iw,"SPAN",{});var iH=i(fv);sx=o(iH,"Logging"),iH.forEach(r),Iw.forEach(r),Db=d(t),je=s(t,"P",{});var Fn=i(je);ix=o(Fn,"By default "),gc=s(Fn,"A",{href:!0});var lH=i(gc);lx=o(lH,"Trainer"),lH.forEach(r),dx=o(Fn," will use "),uv=s(Fn,"CODE",{});var dH=i(uv);cx=o(dH,"logging.INFO"),dH.forEach(r),px=o(Fn," for the main process and "),_v=s(Fn,"CODE",{});var cH=i(_v);mx=o(cH,"logging.WARNING"),cH.forEach(r),hx=o(Fn,` for
the replicas if any.`),Fn.forEach(r),Pb=d(t),jt=s(t,"P",{});var Tm=i(jt);fx=o(Tm,"These defaults can be overridden to use any of the 5 "),vv=s(Tm,"CODE",{});var pH=i(vv);ux=o(pH,"logging"),pH.forEach(r),_x=o(Tm," levels with "),bc=s(Tm,"A",{href:!0});var mH=i(bc);vx=o(mH,"TrainingArguments"),mH.forEach(r),gx=o(Tm,`\u2019s
arguments:`),Tm.forEach(r),Ab=d(t),co=s(t,"UL",{});var zw=i(co);yc=s(zw,"LI",{});var qL=i(yc);gv=s(qL,"CODE",{});var hH=i(gv);bx=o(hH,"log_level"),hH.forEach(r),yx=o(qL," - for the main process"),qL.forEach(r),Ex=d(zw),Ec=s(zw,"LI",{});var UL=i(Ec);bv=s(UL,"CODE",{});var fH=i(bv);wx=o(fH,"log_level_replica"),fH.forEach(r),$x=o(UL," - for the replicas"),UL.forEach(r),zw.forEach(r),Ob=d(t),He=s(t,"P",{});var Ln=i(He);Tx=o(Ln,"Further, if "),wc=s(Ln,"A",{href:!0});var uH=i(wc);kx=o(uH,"TrainingArguments"),uH.forEach(r),Dx=o(Ln,"\u2019s "),yv=s(Ln,"CODE",{});var _H=i(yv);Px=o(_H,"log_on_each_node"),_H.forEach(r),Ax=o(Ln," is set to "),Ev=s(Ln,"CODE",{});var vH=i(Ev);Ox=o(vH,"False"),vH.forEach(r),xx=o(Ln,` only the main node will
use the log level settings for its main process, all other nodes will use the log level settings for replicas.`),Ln.forEach(r),xb=d(t),te=s(t,"P",{});var Xe=i(te);Sx=o(Xe,"Note that "),$c=s(Xe,"A",{href:!0});var gH=i($c);Cx=o(gH,"Trainer"),gH.forEach(r),Nx=o(Xe," is going to set "),wv=s(Xe,"CODE",{});var bH=i(wv);Ix=o(bH,"transformers"),bH.forEach(r),zx=o(Xe,`\u2019s log level separately for each node in its
`),$v=s(Xe,"CODE",{});var yH=i($v);Fx=o(yH,"__init__()"),yH.forEach(r),Lx=o(Xe,`. So you may want to set this sooner (see the next example) if you tap into other
`),Tv=s(Xe,"CODE",{});var EH=i(Tv);qx=o(EH,"transformers"),EH.forEach(r),Ux=o(Xe," functionality before creating the "),Tc=s(Xe,"A",{href:!0});var wH=i(Tc);Rx=o(wH,"Trainer"),wH.forEach(r),Mx=o(Xe," object."),Xe.forEach(r),Sb=d(t),kc=s(t,"P",{});var $H=i(kc);jx=o($H,"Here is an example of how this can be used in an application:"),$H.forEach(r),Cb=d(t),f(ul.$$.fragment,t),Nb=d(t),Dc=s(t,"P",{});var TH=i(Dc);Hx=o(TH,`And then if you only want to see warnings on the main node and all other nodes to not print any most likely duplicated
warnings you could run it as:`),TH.forEach(r),Ib=d(t),f(_l.$$.fragment,t),zb=d(t),Pc=s(t,"P",{});var kH=i(Pc);Gx=o(kH,`In the multi-node environment if you also don\u2019t want the logs to repeat for each node\u2019s main process, you will want to
change the above to:`),kH.forEach(r),Fb=d(t),f(vl.$$.fragment,t),Lb=d(t),Ac=s(t,"P",{});var DH=i(Ac);Wx=o(DH,`and then only the main process of the first node will log at the \u201Cwarning\u201D level, and all other processes on the main
node and all processes on other nodes will log at the \u201Cerror\u201D level.`),DH.forEach(r),qb=d(t),Oc=s(t,"P",{});var PH=i(Oc);Bx=o(PH,"If you need your application to be as quiet as possible you could do:"),PH.forEach(r),Ub=d(t),f(gl.$$.fragment,t),Rb=d(t),po=s(t,"P",{});var Fw=i(po);Vx=o(Fw,"(add "),kv=s(Fw,"CODE",{});var AH=i(kv);Yx=o(AH,"--log_on_each_node 0"),AH.forEach(r),Zx=o(Fw," if on multi-node environment)"),Fw.forEach(r),Mb=d(t),jr=s(t,"H2",{class:!0});var Lw=i(jr);mo=s(Lw,"A",{id:!0,class:!0,href:!0});var OH=i(mo);Dv=s(OH,"SPAN",{});var xH=i(Dv);f(bl.$$.fragment,xH),xH.forEach(r),OH.forEach(r),Jx=d(Lw),Pv=s(Lw,"SPAN",{});var SH=i(Pv);Qx=o(SH,"Randomness"),SH.forEach(r),Lw.forEach(r),jb=d(t),fe=s(t,"P",{});var lr=i(fe);Kx=o(lr,"When resuming from a checkpoint generated by "),xc=s(lr,"A",{href:!0});var CH=i(xc);Xx=o(CH,"Trainer"),CH.forEach(r),eS=o(lr,` all efforts are made to restore the
`),Av=s(lr,"EM",{});var NH=i(Av);tS=o(NH,"python"),NH.forEach(r),rS=o(lr,", "),Ov=s(lr,"EM",{});var IH=i(Ov);aS=o(IH,"numpy"),IH.forEach(r),oS=o(lr," and "),xv=s(lr,"EM",{});var zH=i(xv);nS=o(zH,"pytorch"),zH.forEach(r),sS=o(lr,` RNG states to the same states as they were at the moment of saving that checkpoint,
which should make the \u201Cstop and resume\u201D style of training as close as possible to non-stop training.`),lr.forEach(r),Hb=d(t),Ht=s(t,"P",{});var km=i(Ht);iS=o(km,`However, due to various default non-deterministic pytorch settings this might not fully work. If you want full
determinism please refer to `),yl=s(km,"A",{href:!0,rel:!0});var FH=i(yl);lS=o(FH,"Controlling sources of randomness"),FH.forEach(r),dS=o(km,`. As explained in the document, that some of those settings
that make things deterministic (.e.g., `),Sv=s(km,"CODE",{});var LH=i(Sv);cS=o(LH,"torch.backends.cudnn.deterministic"),LH.forEach(r),pS=o(km,`) may slow things down, therefore this
can\u2019t be done by default, but you can enable those yourself if needed.`),km.forEach(r),Gb=d(t),Hr=s(t,"H2",{class:!0});var qw=i(Hr);ho=s(qw,"A",{id:!0,class:!0,href:!0});var qH=i(ho);Cv=s(qH,"SPAN",{});var UH=i(Cv);f(El.$$.fragment,UH),UH.forEach(r),qH.forEach(r),mS=d(qw),Nv=s(qw,"SPAN",{});var RH=i(Nv);hS=o(RH,"Trainer Integrations"),RH.forEach(r),qw.forEach(r),Wb=d(t),fo=s(t,"P",{});var Uw=i(fo);fS=o(Uw,"The "),Sc=s(Uw,"A",{href:!0});var MH=i(Sc);uS=o(MH,"Trainer"),MH.forEach(r),_S=o(Uw,` has been extended to support libraries that may dramatically improve your training
time and fit much bigger models.`),Uw.forEach(r),Bb=d(t),Ge=s(t,"P",{});var qn=i(Ge);vS=o(qn,"Currently it supports third party solutions, "),wl=s(qn,"A",{href:!0,rel:!0});var jH=i(wl);gS=o(jH,"DeepSpeed"),jH.forEach(r),bS=o(qn," and "),$l=s(qn,"A",{href:!0,rel:!0});var HH=i($l);yS=o(HH,"FairScale"),HH.forEach(r),ES=o(qn,", which implement parts of the paper "),Tl=s(qn,"A",{href:!0,rel:!0});var GH=i(Tl);wS=o(GH,`ZeRO: Memory Optimizations
Toward Training Trillion Parameter Models, by Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He`),GH.forEach(r),$S=o(qn,"."),qn.forEach(r),Vb=d(t),Cc=s(t,"P",{});var WH=i(Cc);TS=o(WH,"This provided support is new and experimental as of this writing."),WH.forEach(r),Yb=d(t),Nc=s(t,"A",{id:!0}),i(Nc).forEach(r),Zb=d(t),Gr=s(t,"H3",{class:!0});var Rw=i(Gr);uo=s(Rw,"A",{id:!0,class:!0,href:!0});var BH=i(uo);Iv=s(BH,"SPAN",{});var VH=i(Iv);f(kl.$$.fragment,VH),VH.forEach(r),BH.forEach(r),kS=d(Rw),zv=s(Rw,"SPAN",{});var YH=i(zv);DS=o(YH,"CUDA Extension Installation Notes"),YH.forEach(r),Rw.forEach(r),Jb=d(t),Ic=s(t,"P",{});var ZH=i(Ic);PS=o(ZH,"As of this writing, both FairScale and Deepspeed require compilation of CUDA C++ code, before they can be used."),ZH.forEach(r),Qb=d(t),Gt=s(t,"P",{});var Dm=i(Gt);AS=o(Dm,"While all installation issues should be dealt with through the corresponding GitHub Issues of "),Dl=s(Dm,"A",{href:!0,rel:!0});var JH=i(Dl);OS=o(JH,"FairScale"),JH.forEach(r),xS=o(Dm," and "),Pl=s(Dm,"A",{href:!0,rel:!0});var QH=i(Pl);SS=o(QH,"Deepspeed"),QH.forEach(r),CS=o(Dm,`, there are a few common issues that one may encounter while building
any PyTorch extension that needs to build CUDA extensions.`),Dm.forEach(r),Kb=d(t),zc=s(t,"P",{});var KH=i(zc);NS=o(KH,"Therefore, if you encounter a CUDA-related build issue while doing one of the following or both:"),KH.forEach(r),Xb=d(t),f(Al.$$.fragment,t),ey=d(t),Fc=s(t,"P",{});var XH=i(Fc);IS=o(XH,"please, read the following notes first."),XH.forEach(r),ty=d(t),Wt=s(t,"P",{});var Pm=i(Wt);zS=o(Pm,"In these notes we give examples for what to do when "),Fv=s(Pm,"CODE",{});var eG=i(Fv);FS=o(eG,"pytorch"),eG.forEach(r),LS=o(Pm," has been built with CUDA "),Lv=s(Pm,"CODE",{});var tG=i(Lv);qS=o(tG,"10.2"),tG.forEach(r),US=o(Pm,`. If your situation is
different remember to adjust the version number to the one you are after.`),Pm.forEach(r),ry=d(t),Wr=s(t,"H4",{class:!0});var Mw=i(Wr);_o=s(Mw,"A",{id:!0,class:!0,href:!0});var rG=i(_o);qv=s(rG,"SPAN",{});var aG=i(qv);f(Ol.$$.fragment,aG),aG.forEach(r),rG.forEach(r),RS=d(Mw),Uv=s(Mw,"SPAN",{});var oG=i(Uv);MS=o(oG,"Possible problem #1"),oG.forEach(r),Mw.forEach(r),ay=d(t),Lc=s(t,"P",{});var nG=i(Lc);jS=o(nG,`While, Pytorch comes with its own CUDA toolkit, to build these two projects you must have an identical version of CUDA
installed system-wide.`),nG.forEach(r),oy=d(t),We=s(t,"P",{});var Un=i(We);HS=o(Un,"For example, if you installed "),Rv=s(Un,"CODE",{});var sG=i(Rv);GS=o(sG,"pytorch"),sG.forEach(r),WS=o(Un," with "),Mv=s(Un,"CODE",{});var iG=i(Mv);BS=o(iG,"cudatoolkit==10.2"),iG.forEach(r),VS=o(Un,` in the Python environment, you also need to have
CUDA `),jv=s(Un,"CODE",{});var lG=i(jv);YS=o(lG,"10.2"),lG.forEach(r),ZS=o(Un," installed system-wide."),Un.forEach(r),ny=d(t),Bt=s(t,"P",{});var Am=i(Bt);JS=o(Am,"The exact location may vary from system to system, but "),Hv=s(Am,"CODE",{});var dG=i(Hv);QS=o(dG,"/usr/local/cuda-10.2"),dG.forEach(r),KS=o(Am,` is the most common location on many
Unix systems. When CUDA is correctly set up and added to the `),Gv=s(Am,"CODE",{});var cG=i(Gv);XS=o(cG,"PATH"),cG.forEach(r),eC=o(Am,` environment variable, one can find the
installation location by doing:`),Am.forEach(r),sy=d(t),f(xl.$$.fragment,t),iy=d(t),vo=s(t,"P",{});var jw=i(vo);tC=o(jw,`If you don\u2019t have CUDA installed system-wide, install it first. You will find the instructions by using your favorite
search engine. For example, if you\u2019re on Ubuntu you may want to search for: `),Sl=s(jw,"A",{href:!0,rel:!0});var pG=i(Sl);rC=o(pG,"ubuntu cuda 10.2 install"),pG.forEach(r),aC=o(jw,"."),jw.forEach(r),ly=d(t),Br=s(t,"H4",{class:!0});var Hw=i(Br);go=s(Hw,"A",{id:!0,class:!0,href:!0});var mG=i(go);Wv=s(mG,"SPAN",{});var hG=i(Wv);f(Cl.$$.fragment,hG),hG.forEach(r),mG.forEach(r),oC=d(Hw),Bv=s(Hw,"SPAN",{});var fG=i(Bv);nC=o(fG,"Possible problem #2"),fG.forEach(r),Hw.forEach(r),dy=d(t),qc=s(t,"P",{});var uG=i(qc);sC=o(uG,`Another possible common problem is that you may have more than one CUDA toolkit installed system-wide. For example you
may have:`),uG.forEach(r),cy=d(t),f(Nl.$$.fragment,t),py=d(t),Vt=s(t,"P",{});var Om=i(Vt);iC=o(Om,"Now, in this situation you need to make sure that your "),Vv=s(Om,"CODE",{});var _G=i(Vv);lC=o(_G,"PATH"),_G.forEach(r),dC=o(Om," and "),Yv=s(Om,"CODE",{});var vG=i(Yv);cC=o(vG,"LD_LIBRARY_PATH"),vG.forEach(r),pC=o(Om,` environment variables contain
the correct paths to the desired CUDA version. Typically, package installers will set these to contain whatever the
last version was installed. If you encounter the problem, where the package build fails because it can\u2019t find the right
CUDA version despite you having it installed system-wide, it means that you need to adjust the 2 aforementioned
environment variables.`),Om.forEach(r),my=d(t),Uc=s(t,"P",{});var gG=i(Uc);mC=o(gG,"First, you may look at their contents:"),gG.forEach(r),hy=d(t),f(Il.$$.fragment,t),fy=d(t),Rc=s(t,"P",{});var bG=i(Rc);hC=o(bG,"so you get an idea of what is inside."),bG.forEach(r),uy=d(t),bo=s(t,"P",{});var Gw=i(bo);fC=o(Gw,"It\u2019s possible that "),Zv=s(Gw,"CODE",{});var yG=i(Zv);uC=o(yG,"LD_LIBRARY_PATH"),yG.forEach(r),_C=o(Gw," is empty."),Gw.forEach(r),_y=d(t),lt=s(t,"P",{});var Cd=i(lt);Jv=s(Cd,"CODE",{});var EG=i(Jv);vC=o(EG,"PATH"),EG.forEach(r),gC=o(Cd," lists the locations of where executables can be found and "),Qv=s(Cd,"CODE",{});var wG=i(Qv);bC=o(wG,"LD_LIBRARY_PATH"),wG.forEach(r),yC=o(Cd,` is for where shared libraries
are to looked for. In both cases, earlier entries have priority over the later ones. `),Kv=s(Cd,"CODE",{});var $G=i(Kv);EC=o($G,":"),$G.forEach(r),wC=o(Cd,` is used to separate multiple
entries.`),Cd.forEach(r),vy=d(t),Mc=s(t,"P",{});var TG=i(Mc);$C=o(TG,`Now, to tell the build program where to find the specific CUDA toolkit, insert the desired paths to be listed first by
doing:`),TG.forEach(r),gy=d(t),f(zl.$$.fragment,t),by=d(t),jc=s(t,"P",{});var kG=i(jc);TC=o(kG,"Note that we aren\u2019t overwriting the existing values, but prepending instead."),kG.forEach(r),yy=d(t),Be=s(t,"P",{});var Rn=i(Be);kC=o(Rn,`Of course, adjust the version number, the full path if need be. Check that the directories you assign actually do
exist. `),Xv=s(Rn,"CODE",{});var DG=i(Xv);DC=o(DG,"lib64"),DG.forEach(r),PC=o(Rn," sub-directory is where the various CUDA "),eg=s(Rn,"CODE",{});var PG=i(eg);AC=o(PG,".so"),PG.forEach(r),OC=o(Rn," objects, like "),tg=s(Rn,"CODE",{});var AG=i(tg);xC=o(AG,"libcudart.so"),AG.forEach(r),SC=o(Rn,` reside, it\u2019s unlikely
that your system will have it named differently, but if it is adjust it to reflect your reality.`),Rn.forEach(r),Ey=d(t),Vr=s(t,"H4",{class:!0});var Ww=i(Vr);yo=s(Ww,"A",{id:!0,class:!0,href:!0});var OG=i(yo);rg=s(OG,"SPAN",{});var xG=i(rg);f(Fl.$$.fragment,xG),xG.forEach(r),OG.forEach(r),CC=d(Ww),ag=s(Ww,"SPAN",{});var SG=i(ag);NC=o(SG,"Possible problem #3"),SG.forEach(r),Ww.forEach(r),wy=d(t),Yt=s(t,"P",{});var xm=i(Yt);IC=o(xm,"Some older CUDA versions may refuse to build with newer compilers. For example, you my have "),og=s(xm,"CODE",{});var CG=i(og);zC=o(CG,"gcc-9"),CG.forEach(r),FC=o(xm,` but it wants
`),ng=s(xm,"CODE",{});var NG=i(ng);LC=o(NG,"gcc-7"),NG.forEach(r),qC=o(xm,"."),xm.forEach(r),$y=d(t),Hc=s(t,"P",{});var IG=i(Hc);UC=o(IG,"There are various ways to go about it."),IG.forEach(r),Ty=d(t),Gc=s(t,"P",{});var zG=i(Gc);RC=o(zG,"If you can install the latest CUDA toolkit it typically should support the newer compiler."),zG.forEach(r),ky=d(t),Eo=s(t,"P",{});var Bw=i(Eo);MC=o(Bw,`Alternatively, you could install the lower version of the compiler in addition to the one you already have, or you may
already have it but it\u2019s not the default one, so the build system can\u2019t see it. If you have `),sg=s(Bw,"CODE",{});var FG=i(sg);jC=o(FG,"gcc-7"),FG.forEach(r),HC=o(Bw,` installed but the
build system complains it can\u2019t find it, the following might do the trick:`),Bw.forEach(r),Dy=d(t),f(Ll.$$.fragment,t),Py=d(t),B=s(t,"P",{});var _e=i(B);GC=o(_e,"Here, we are making a symlink to "),ig=s(_e,"CODE",{});var LG=i(ig);WC=o(LG,"gcc-7"),LG.forEach(r),BC=o(_e," from "),lg=s(_e,"CODE",{});var qG=i(lg);VC=o(qG,"/usr/local/cuda-10.2/bin/gcc"),qG.forEach(r),YC=o(_e,` and since
`),dg=s(_e,"CODE",{});var UG=i(dg);ZC=o(UG,"/usr/local/cuda-10.2/bin/"),UG.forEach(r),JC=o(_e," should be in the "),cg=s(_e,"CODE",{});var RG=i(cg);QC=o(RG,"PATH"),RG.forEach(r),KC=o(_e,` environment variable (see the previous problem\u2019s solution), it
should find `),pg=s(_e,"CODE",{});var MG=i(pg);XC=o(MG,"gcc-7"),MG.forEach(r),eN=o(_e," (and "),mg=s(_e,"CODE",{});var jG=i(mg);tN=o(jG,"g++7"),jG.forEach(r),rN=o(_e,") and then the build will succeed."),_e.forEach(r),Ay=d(t),Wc=s(t,"P",{});var HG=i(Wc);aN=o(HG,"As always make sure to edit the paths in the example to match your situation."),HG.forEach(r),Oy=d(t),Yr=s(t,"H3",{class:!0});var Vw=i(Yr);wo=s(Vw,"A",{id:!0,class:!0,href:!0});var GG=i(wo);hg=s(GG,"SPAN",{});var WG=i(hg);f(ql.$$.fragment,WG),WG.forEach(r),GG.forEach(r),oN=d(Vw),fg=s(Vw,"SPAN",{});var BG=i(fg);nN=o(BG,"FairScale"),BG.forEach(r),Vw.forEach(r),xy=d(t),Ve=s(t,"P",{});var Mn=i(Ve);sN=o(Mn,"By integrating "),Ul=s(Mn,"A",{href:!0,rel:!0});var VG=i(Ul);iN=o(VG,"FairScale"),VG.forEach(r),lN=o(Mn," the "),Bc=s(Mn,"A",{href:!0});var YG=i(Bc);dN=o(YG,"Trainer"),YG.forEach(r),cN=o(Mn,`
provides support for the following features from `),Rl=s(Mn,"A",{href:!0,rel:!0});var ZG=i(Rl);pN=o(ZG,"the ZeRO paper"),ZG.forEach(r),mN=o(Mn,":"),Mn.forEach(r),Sy=d(t),Ye=s(t,"OL",{});var jn=i(Ye);ug=s(jn,"LI",{});var JG=i(ug);hN=o(JG,"Optimizer State Sharding"),JG.forEach(r),fN=d(jn),_g=s(jn,"LI",{});var QG=i(_g);uN=o(QG,"Gradient Sharding"),QG.forEach(r),_N=d(jn),vg=s(jn,"LI",{});var KG=i(vg);vN=o(KG,"Model Parameters Sharding (new and very experimental)"),KG.forEach(r),gN=d(jn),gg=s(jn,"LI",{});var XG=i(gg);bN=o(XG,"CPU offload (new and very experimental)"),XG.forEach(r),jn.forEach(r),Cy=d(t),Vc=s(t,"P",{});var eW=i(Vc);yN=o(eW,"You will need at least two GPUs to use this feature."),eW.forEach(r),Ny=d(t),Ml=s(t,"P",{});var RL=i(Ml);bg=s(RL,"STRONG",{});var tW=i(bg);EN=o(tW,"Installation"),tW.forEach(r),wN=o(RL,":"),RL.forEach(r),Iy=d(t),Yc=s(t,"P",{});var rW=i(Yc);$N=o(rW,"Install the library via pypi:"),rW.forEach(r),zy=d(t),f(jl.$$.fragment,t),Fy=d(t),Zt=s(t,"P",{});var Sm=i(Zt);TN=o(Sm,"or via "),yg=s(Sm,"CODE",{});var aW=i(yg);kN=o(aW,"transformers"),aW.forEach(r),DN=o(Sm,"\u2019 "),Eg=s(Sm,"CODE",{});var oW=i(Eg);PN=o(oW,"extras"),oW.forEach(r),AN=o(Sm,":"),Sm.forEach(r),Ly=d(t),f(Hl.$$.fragment,t),qy=d(t),$o=s(t,"P",{});var Yw=i($o);ON=o(Yw,"(will become available starting from "),wg=s(Yw,"CODE",{});var nW=i(wg);xN=o(nW,"transformers==4.6.0"),nW.forEach(r),SN=o(Yw,")"),Yw.forEach(r),Uy=d(t),To=s(t,"P",{});var Zw=i(To);CN=o(Zw,"or find more details on "),Gl=s(Zw,"A",{href:!0,rel:!0});var sW=i(Gl);NN=o(sW,"the FairScale\u2019s GitHub page"),sW.forEach(r),IN=o(Zw,"."),Zw.forEach(r),Ry=d(t),ko=s(t,"P",{});var Jw=i(ko);zN=o(Jw,"If you\u2019re still struggling with the build, first make sure to read "),Zc=s(Jw,"A",{href:!0});var iW=i(Zc);FN=o(iW,"zero-install-notes"),iW.forEach(r),LN=o(Jw,"."),Jw.forEach(r),My=d(t),Jc=s(t,"P",{});var lW=i(Jc);qN=o(lW,"If it\u2019s still not resolved the build issue, here are a few more ideas."),lW.forEach(r),jy=d(t),Wl=s(t,"P",{});var ML=i(Wl);$g=s(ML,"CODE",{});var dW=i($g);UN=o(dW,"fairscale"),dW.forEach(r),RN=o(ML,` seems to have an issue with the recently introduced by pip build isolation feature. If you have a problem
with it, you may want to try one of:`),ML.forEach(r),Hy=d(t),f(Bl.$$.fragment,t),Gy=d(t),Qc=s(t,"P",{});var cW=i(Qc);MN=o(cW,"or:"),cW.forEach(r),Wy=d(t),f(Vl.$$.fragment,t),By=d(t),Yl=s(t,"P",{});var jL=i(Yl);Tg=s(jL,"CODE",{});var pW=i(Tg);jN=o(pW,"fairscale"),pW.forEach(r),HN=o(jL," also has issues with building against pytorch-nightly, so if you use it you may have to try one of:"),jL.forEach(r),Vy=d(t),f(Zl.$$.fragment,t),Yy=d(t),Kc=s(t,"P",{});var mW=i(Kc);GN=o(mW,"or:"),mW.forEach(r),Zy=d(t),f(Jl.$$.fragment,t),Jy=d(t),Xc=s(t,"P",{});var hW=i(Xc);WN=o(hW,"Of course, adjust the urls to match the cuda version you use."),hW.forEach(r),Qy=d(t),Do=s(t,"P",{});var Qw=i(Do);BN=o(Qw,`If after trying everything suggested you still encounter build issues, please, proceed with the GitHub Issue of
`),Ql=s(Qw,"A",{href:!0,rel:!0});var fW=i(Ql);VN=o(fW,"FairScale"),fW.forEach(r),YN=o(Qw,"."),Qw.forEach(r),Ky=d(t),Kl=s(t,"P",{});var HL=i(Kl);kg=s(HL,"STRONG",{});var uW=i(kg);ZN=o(uW,"Usage"),uW.forEach(r),JN=o(HL,":"),HL.forEach(r),Xy=d(t),Jt=s(t,"P",{});var Cm=i(Jt);QN=o(Cm,"To use the first version of Sharded data-parallelism, add "),Dg=s(Cm,"CODE",{});var _W=i(Dg);KN=o(_W,"--sharded_ddp simple"),_W.forEach(r),XN=o(Cm,` to the command line arguments, and
make sure you have added the distributed launcher `),Pg=s(Cm,"CODE",{});var vW=i(Pg);eI=o(vW,"-m torch.distributed.launch --nproc_per_node=NUMBER_OF_GPUS_YOU_HAVE"),vW.forEach(r),tI=o(Cm," if you haven\u2019t been using it already."),Cm.forEach(r),e2=d(t),Po=s(t,"P",{});var Kw=i(Po);rI=o(Kw,"For example here is how you could use it for "),Ag=s(Kw,"CODE",{});var gW=i(Ag);aI=o(gW,"run_translation.py"),gW.forEach(r),oI=o(Kw," with 2 GPUs:"),Kw.forEach(r),t2=d(t),f(Xl.$$.fragment,t),r2=d(t),ep=s(t,"P",{});var bW=i(ep);nI=o(bW,"Notes:"),bW.forEach(r),a2=d(t),Ze=s(t,"UL",{});var Hn=i(Ze);Og=s(Hn,"LI",{});var yW=i(Og);sI=o(yW,"This feature requires distributed training (so multiple GPUs)."),yW.forEach(r),iI=d(Hn),xg=s(Hn,"LI",{});var EW=i(xg);lI=o(EW,"It is not implemented for TPUs."),EW.forEach(r),dI=d(Hn),ed=s(Hn,"LI",{});var Xw=i(ed);cI=o(Xw,"It works with "),Sg=s(Xw,"CODE",{});var wW=i(Sg);pI=o(wW,"--fp16"),wW.forEach(r),mI=o(Xw," too, to make things even faster."),Xw.forEach(r),hI=d(Hn),td=s(Hn,"LI",{});var e0=i(td);fI=o(e0,"One of the main benefits of enabling "),Cg=s(e0,"CODE",{});var $W=i(Cg);uI=o($W,"--sharded_ddp simple"),$W.forEach(r),_I=o(e0,` is that it uses a lot less GPU memory, so you should be
able to use significantly larger batch sizes using the same hardware (e.g. 3x and even bigger) which should lead to
significantly shorter training time.`),e0.forEach(r),Hn.forEach(r),o2=d(t),rd=s(t,"OL",{start:!0});var TW=i(rd);dt=s(TW,"LI",{});var Gn=i(dt);vI=o(Gn,"To use the second version of Sharded data-parallelism, add "),Ng=s(Gn,"CODE",{});var kW=i(Ng);gI=o(kW,"--sharded_ddp zero_dp_2"),kW.forEach(r),bI=o(Gn," or "),Ig=s(Gn,"CODE",{});var DW=i(Ig);yI=o(DW,"--sharded_ddp zero_dp_3"),DW.forEach(r),EI=o(Gn," to the command line arguments, and make sure you have added the distributed launcher "),zg=s(Gn,"CODE",{});var PW=i(zg);wI=o(PW,"-m torch.distributed.launch --nproc_per_node=NUMBER_OF_GPUS_YOU_HAVE"),PW.forEach(r),$I=o(Gn," if you haven\u2019t been using it already."),Gn.forEach(r),TW.forEach(r),n2=d(t),Ao=s(t,"P",{});var t0=i(Ao);TI=o(t0,"For example here is how you could use it for "),Fg=s(t0,"CODE",{});var AW=i(Fg);kI=o(AW,"run_translation.py"),AW.forEach(r),DI=o(t0," with 2 GPUs:"),t0.forEach(r),s2=d(t),f(ad.$$.fragment,t),i2=d(t),Zr=s(t,"P",{});var tb=i(Zr);Lg=s(tb,"CODE",{});var OW=i(Lg);PI=o(OW,"zero_dp_2"),OW.forEach(r),AI=o(tb," is an optimized version of the simple wrapper, while "),qg=s(tb,"CODE",{});var xW=i(qg);OI=o(xW,"zero_dp_3"),xW.forEach(r),xI=o(tb,` fully shards model weights,
gradients and optimizer states.`),tb.forEach(r),l2=d(t),Qt=s(t,"P",{});var Nm=i(Qt);SI=o(Nm,"Both are compatible with adding "),Ug=s(Nm,"CODE",{});var SW=i(Ug);CI=o(SW,"cpu_offload"),SW.forEach(r),NI=o(Nm," to enable ZeRO-offload (activate it like this: "),Rg=s(Nm,"CODE",{});var CW=i(Rg);II=o(CW,'--sharded_ddp "zero_dp_2 cpu_offload"'),CW.forEach(r),zI=o(Nm,")."),Nm.forEach(r),d2=d(t),tp=s(t,"P",{});var NW=i(tp);FI=o(NW,"Notes:"),NW.forEach(r),c2=d(t),ue=s(t,"UL",{});var dr=i(ue);Mg=s(dr,"LI",{});var IW=i(Mg);LI=o(IW,"This feature requires distributed training (so multiple GPUs)."),IW.forEach(r),qI=d(dr),jg=s(dr,"LI",{});var zW=i(jg);UI=o(zW,"It is not implemented for TPUs."),zW.forEach(r),RI=d(dr),od=s(dr,"LI",{});var r0=i(od);MI=o(r0,"It works with "),Hg=s(r0,"CODE",{});var FW=i(Hg);jI=o(FW,"--fp16"),FW.forEach(r),HI=o(r0," too, to make things even faster."),r0.forEach(r),GI=d(dr),Jr=s(dr,"LI",{});var Im=i(Jr);WI=o(Im,"The "),Gg=s(Im,"CODE",{});var LW=i(Gg);BI=o(LW,"cpu_offload"),LW.forEach(r),VI=o(Im," additional option requires "),Wg=s(Im,"CODE",{});var qW=i(Wg);YI=o(qW,"--fp16"),qW.forEach(r),ZI=o(Im,"."),Im.forEach(r),JI=d(dr),Bg=s(dr,"LI",{});var UW=i(Bg);QI=o(UW,`This is an area of active development, so make sure you have a source install of fairscale to use this feature as
some bugs you encounter may have been fixed there already.`),UW.forEach(r),dr.forEach(r),p2=d(t),rp=s(t,"P",{});var RW=i(rp);KI=o(RW,"Known caveats:"),RW.forEach(r),m2=d(t),Oo=s(t,"UL",{});var a0=i(Oo);Qr=s(a0,"LI",{});var zm=i(Qr);XI=o(zm,"This feature is incompatible with "),Vg=s(zm,"CODE",{});var MW=i(Vg);ez=o(MW,"--predict_with_generate"),MW.forEach(r),tz=o(zm," in the "),Yg=s(zm,"EM",{});var jW=i(Yg);rz=o(jW,"run_translation.py"),jW.forEach(r),az=o(zm," script."),zm.forEach(r),oz=d(a0),Ae=s(a0,"LI",{});var cr=i(Ae);nz=o(cr,"Using "),Zg=s(cr,"CODE",{});var HW=i(Zg);sz=o(HW,"--sharded_ddp zero_dp_3"),HW.forEach(r),iz=o(cr,` requires wrapping each layer of the model in the special container
`),Jg=s(cr,"CODE",{});var GW=i(Jg);lz=o(GW,"FullyShardedDataParallelism"),GW.forEach(r),dz=o(cr," of fairscale. It should be used with the option "),Qg=s(cr,"CODE",{});var WW=i(Qg);cz=o(WW,"auto_wrap"),WW.forEach(r),pz=o(cr,` if you are not
doing this yourself: `),Kg=s(cr,"CODE",{});var BW=i(Kg);mz=o(BW,'--sharded_ddp "zero_dp_3 auto_wrap"'),BW.forEach(r),hz=o(cr,"."),cr.forEach(r),a0.forEach(r),h2=d(t),Kr=s(t,"H3",{class:!0});var o0=i(Kr);xo=s(o0,"A",{id:!0,class:!0,href:!0});var VW=i(xo);Xg=s(VW,"SPAN",{});var YW=i(Xg);f(nd.$$.fragment,YW),YW.forEach(r),VW.forEach(r),fz=d(o0),e1=s(o0,"SPAN",{});var ZW=i(e1);uz=o(ZW,"DeepSpeed"),ZW.forEach(r),o0.forEach(r),f2=d(t),So=s(t,"P",{});var n0=i(So);_z=o(n0,"Moved to "),ap=s(n0,"A",{href:!0});var JW=i(ap);vz=o(JW,"deepspeed-trainer-integration"),JW.forEach(r),gz=o(n0,"."),n0.forEach(r),u2=d(t),Xr=s(t,"H4",{class:!0});var s0=i(Xr);Co=s(s0,"A",{id:!0,class:!0,href:!0});var QW=i(Co);t1=s(QW,"SPAN",{});var KW=i(t1);f(sd.$$.fragment,KW),KW.forEach(r),QW.forEach(r),bz=d(s0),r1=s(s0,"SPAN",{});var XW=i(r1);yz=o(XW,"Installation"),XW.forEach(r),s0.forEach(r),_2=d(t),No=s(t,"P",{});var i0=i(No);Ez=o(i0,"Moved to "),op=s(i0,"A",{href:!0});var eB=i(op);wz=o(eB,"deepspeed-installation"),eB.forEach(r),$z=o(i0,"."),i0.forEach(r),v2=d(t),ea=s(t,"H4",{class:!0});var l0=i(ea);Io=s(l0,"A",{id:!0,class:!0,href:!0});var tB=i(Io);a1=s(tB,"SPAN",{});var rB=i(a1);f(id.$$.fragment,rB),rB.forEach(r),tB.forEach(r),Tz=d(l0),o1=s(l0,"SPAN",{});var aB=i(o1);kz=o(aB,"Deployment with multiple GPUs"),aB.forEach(r),l0.forEach(r),g2=d(t),zo=s(t,"P",{});var d0=i(zo);Dz=o(d0,"Moved to "),np=s(d0,"A",{href:!0});var oB=i(np);Pz=o(oB,"deepspeed-multi-gpu"),oB.forEach(r),Az=o(d0,"."),d0.forEach(r),b2=d(t),ta=s(t,"H4",{class:!0});var c0=i(ta);Fo=s(c0,"A",{id:!0,class:!0,href:!0});var nB=i(Fo);n1=s(nB,"SPAN",{});var sB=i(n1);f(ld.$$.fragment,sB),sB.forEach(r),nB.forEach(r),Oz=d(c0),s1=s(c0,"SPAN",{});var iB=i(s1);xz=o(iB,"Deployment with one GPU"),iB.forEach(r),c0.forEach(r),y2=d(t),Lo=s(t,"P",{});var p0=i(Lo);Sz=o(p0,"Moved to "),sp=s(p0,"A",{href:!0});var lB=i(sp);Cz=o(lB,"deepspeed-one-gpu"),lB.forEach(r),Nz=o(p0,"."),p0.forEach(r),E2=d(t),ra=s(t,"H4",{class:!0});var m0=i(ra);qo=s(m0,"A",{id:!0,class:!0,href:!0});var dB=i(qo);i1=s(dB,"SPAN",{});var cB=i(i1);f(dd.$$.fragment,cB),cB.forEach(r),dB.forEach(r),Iz=d(m0),l1=s(m0,"SPAN",{});var pB=i(l1);zz=o(pB,"Deployment in Notebooks"),pB.forEach(r),m0.forEach(r),w2=d(t),Uo=s(t,"P",{});var h0=i(Uo);Fz=o(h0,"Moved to "),ip=s(h0,"A",{href:!0});var mB=i(ip);Lz=o(mB,"deepspeed-notebook"),mB.forEach(r),qz=o(h0,"."),h0.forEach(r),$2=d(t),aa=s(t,"H4",{class:!0});var f0=i(aa);Ro=s(f0,"A",{id:!0,class:!0,href:!0});var hB=i(Ro);d1=s(hB,"SPAN",{});var fB=i(d1);f(cd.$$.fragment,fB),fB.forEach(r),hB.forEach(r),Uz=d(f0),c1=s(f0,"SPAN",{});var uB=i(c1);Rz=o(uB,"Configuration"),uB.forEach(r),f0.forEach(r),T2=d(t),Mo=s(t,"P",{});var u0=i(Mo);Mz=o(u0,"Moved to "),lp=s(u0,"A",{href:!0});var _B=i(lp);jz=o(_B,"deepspeed-config"),_B.forEach(r),Hz=o(u0,"."),u0.forEach(r),k2=d(t),oa=s(t,"H4",{class:!0});var _0=i(oa);jo=s(_0,"A",{id:!0,class:!0,href:!0});var vB=i(jo);p1=s(vB,"SPAN",{});var gB=i(p1);f(pd.$$.fragment,gB),gB.forEach(r),vB.forEach(r),Gz=d(_0),m1=s(_0,"SPAN",{});var bB=i(m1);Wz=o(bB,"Passing Configuration"),bB.forEach(r),_0.forEach(r),D2=d(t),Ho=s(t,"P",{});var v0=i(Ho);Bz=o(v0,"Moved to "),dp=s(v0,"A",{href:!0});var yB=i(dp);Vz=o(yB,"deepspeed-config-passing"),yB.forEach(r),Yz=o(v0,"."),v0.forEach(r),P2=d(t),na=s(t,"H4",{class:!0});var g0=i(na);Go=s(g0,"A",{id:!0,class:!0,href:!0});var EB=i(Go);h1=s(EB,"SPAN",{});var wB=i(h1);f(md.$$.fragment,wB),wB.forEach(r),EB.forEach(r),Zz=d(g0),f1=s(g0,"SPAN",{});var $B=i(f1);Jz=o($B,"Shared Configuration"),$B.forEach(r),g0.forEach(r),A2=d(t),Wo=s(t,"P",{});var b0=i(Wo);Qz=o(b0,"Moved to "),cp=s(b0,"A",{href:!0});var TB=i(cp);Kz=o(TB,"deepspeed-config-shared"),TB.forEach(r),Xz=o(b0,"."),b0.forEach(r),O2=d(t),sa=s(t,"H4",{class:!0});var y0=i(sa);Bo=s(y0,"A",{id:!0,class:!0,href:!0});var kB=i(Bo);u1=s(kB,"SPAN",{});var DB=i(u1);f(hd.$$.fragment,DB),DB.forEach(r),kB.forEach(r),eF=d(y0),_1=s(y0,"SPAN",{});var PB=i(_1);tF=o(PB,"ZeRO"),PB.forEach(r),y0.forEach(r),x2=d(t),Vo=s(t,"P",{});var E0=i(Vo);rF=o(E0,"Moved to "),pp=s(E0,"A",{href:!0});var AB=i(pp);aF=o(AB,"deepspeed-zero"),AB.forEach(r),oF=o(E0,"."),E0.forEach(r),S2=d(t),ia=s(t,"H5",{class:!0});var w0=i(ia);Yo=s(w0,"A",{id:!0,class:!0,href:!0});var OB=i(Yo);v1=s(OB,"SPAN",{});var xB=i(v1);f(fd.$$.fragment,xB),xB.forEach(r),OB.forEach(r),nF=d(w0),g1=s(w0,"SPAN",{});var SB=i(g1);sF=o(SB,"ZeRO-2 Config"),SB.forEach(r),w0.forEach(r),C2=d(t),Zo=s(t,"P",{});var $0=i(Zo);iF=o($0,"Moved to "),mp=s($0,"A",{href:!0});var CB=i(mp);lF=o(CB,"deepspeed-zero2-config"),CB.forEach(r),dF=o($0,"."),$0.forEach(r),N2=d(t),la=s(t,"H5",{class:!0});var T0=i(la);Jo=s(T0,"A",{id:!0,class:!0,href:!0});var NB=i(Jo);b1=s(NB,"SPAN",{});var IB=i(b1);f(ud.$$.fragment,IB),IB.forEach(r),NB.forEach(r),cF=d(T0),y1=s(T0,"SPAN",{});var zB=i(y1);pF=o(zB,"ZeRO-3 Config"),zB.forEach(r),T0.forEach(r),I2=d(t),Qo=s(t,"P",{});var k0=i(Qo);mF=o(k0,"Moved to "),hp=s(k0,"A",{href:!0});var FB=i(hp);hF=o(FB,"deepspeed-zero3-config"),FB.forEach(r),fF=o(k0,"."),k0.forEach(r),z2=d(t),da=s(t,"H4",{class:!0});var D0=i(da);Ko=s(D0,"A",{id:!0,class:!0,href:!0});var LB=i(Ko);E1=s(LB,"SPAN",{});var qB=i(E1);f(_d.$$.fragment,qB),qB.forEach(r),LB.forEach(r),uF=d(D0),w1=s(D0,"SPAN",{});var UB=i(w1);_F=o(UB,"NVMe Support"),UB.forEach(r),D0.forEach(r),F2=d(t),Xo=s(t,"P",{});var P0=i(Xo);vF=o(P0,"Moved to "),fp=s(P0,"A",{href:!0});var RB=i(fp);gF=o(RB,"deepspeed-nvme"),RB.forEach(r),bF=o(P0,"."),P0.forEach(r),L2=d(t),ca=s(t,"H5",{class:!0});var A0=i(ca);en=s(A0,"A",{id:!0,class:!0,href:!0});var MB=i(en);$1=s(MB,"SPAN",{});var jB=i($1);f(vd.$$.fragment,jB),jB.forEach(r),MB.forEach(r),yF=d(A0),T1=s(A0,"SPAN",{});var HB=i(T1);EF=o(HB,"ZeRO-2 vs ZeRO-3 Performance"),HB.forEach(r),A0.forEach(r),q2=d(t),tn=s(t,"P",{});var O0=i(tn);wF=o(O0,"Moved to "),up=s(O0,"A",{href:!0});var GB=i(up);$F=o(GB,"deepspeed-zero2-zero3-performance"),GB.forEach(r),TF=o(O0,"."),O0.forEach(r),U2=d(t),pa=s(t,"H5",{class:!0});var x0=i(pa);rn=s(x0,"A",{id:!0,class:!0,href:!0});var WB=i(rn);k1=s(WB,"SPAN",{});var BB=i(k1);f(gd.$$.fragment,BB),BB.forEach(r),WB.forEach(r),kF=d(x0),D1=s(x0,"SPAN",{});var VB=i(D1);DF=o(VB,"ZeRO-2 Example"),VB.forEach(r),x0.forEach(r),R2=d(t),an=s(t,"P",{});var S0=i(an);PF=o(S0,"Moved to "),_p=s(S0,"A",{href:!0});var YB=i(_p);AF=o(YB,"deepspeed-zero2-example"),YB.forEach(r),OF=o(S0,"."),S0.forEach(r),M2=d(t),ma=s(t,"H5",{class:!0});var C0=i(ma);on=s(C0,"A",{id:!0,class:!0,href:!0});var ZB=i(on);P1=s(ZB,"SPAN",{});var JB=i(P1);f(bd.$$.fragment,JB),JB.forEach(r),ZB.forEach(r),xF=d(C0),A1=s(C0,"SPAN",{});var QB=i(A1);SF=o(QB,"ZeRO-3 Example"),QB.forEach(r),C0.forEach(r),j2=d(t),nn=s(t,"P",{});var N0=i(nn);CF=o(N0,"Moved to "),vp=s(N0,"A",{href:!0});var KB=i(vp);NF=o(KB,"deepspeed-zero3-example"),KB.forEach(r),IF=o(N0,"."),N0.forEach(r),H2=d(t),ha=s(t,"H4",{class:!0});var I0=i(ha);sn=s(I0,"A",{id:!0,class:!0,href:!0});var XB=i(sn);O1=s(XB,"SPAN",{});var eV=i(O1);f(yd.$$.fragment,eV),eV.forEach(r),XB.forEach(r),zF=d(I0),x1=s(I0,"SPAN",{});var tV=i(x1);FF=o(tV,"Optimizer and Scheduler"),tV.forEach(r),I0.forEach(r),G2=d(t),fa=s(t,"H5",{class:!0});var z0=i(fa);ln=s(z0,"A",{id:!0,class:!0,href:!0});var rV=i(ln);S1=s(rV,"SPAN",{});var aV=i(S1);f(Ed.$$.fragment,aV),aV.forEach(r),rV.forEach(r),LF=d(z0),C1=s(z0,"SPAN",{});var oV=i(C1);qF=o(oV,"Optimizer"),oV.forEach(r),z0.forEach(r),W2=d(t),dn=s(t,"P",{});var F0=i(dn);UF=o(F0,"Moved to "),gp=s(F0,"A",{href:!0});var nV=i(gp);RF=o(nV,"deepspeed-optimizer"),nV.forEach(r),MF=o(F0,"."),F0.forEach(r),B2=d(t),ua=s(t,"H5",{class:!0});var L0=i(ua);cn=s(L0,"A",{id:!0,class:!0,href:!0});var sV=i(cn);N1=s(sV,"SPAN",{});var iV=i(N1);f(wd.$$.fragment,iV),iV.forEach(r),sV.forEach(r),jF=d(L0),I1=s(L0,"SPAN",{});var lV=i(I1);HF=o(lV,"Scheduler"),lV.forEach(r),L0.forEach(r),V2=d(t),pn=s(t,"P",{});var q0=i(pn);GF=o(q0,"Moved to "),bp=s(q0,"A",{href:!0});var dV=i(bp);WF=o(dV,"deepspeed-scheduler"),dV.forEach(r),BF=o(q0,"."),q0.forEach(r),Y2=d(t),_a=s(t,"H4",{class:!0});var U0=i(_a);mn=s(U0,"A",{id:!0,class:!0,href:!0});var cV=i(mn);z1=s(cV,"SPAN",{});var pV=i(z1);f($d.$$.fragment,pV),pV.forEach(r),cV.forEach(r),VF=d(U0),F1=s(U0,"SPAN",{});var mV=i(F1);YF=o(mV,"fp32 Precision"),mV.forEach(r),U0.forEach(r),Z2=d(t),hn=s(t,"P",{});var R0=i(hn);ZF=o(R0,"Moved to "),yp=s(R0,"A",{href:!0});var hV=i(yp);JF=o(hV,"deepspeed-fp32"),hV.forEach(r),QF=o(R0,"."),R0.forEach(r),J2=d(t),va=s(t,"H4",{class:!0});var M0=i(va);fn=s(M0,"A",{id:!0,class:!0,href:!0});var fV=i(fn);L1=s(fV,"SPAN",{});var uV=i(L1);f(Td.$$.fragment,uV),uV.forEach(r),fV.forEach(r),KF=d(M0),q1=s(M0,"SPAN",{});var _V=i(q1);XF=o(_V,"Automatic Mixed Precision"),_V.forEach(r),M0.forEach(r),Q2=d(t),un=s(t,"P",{});var j0=i(un);eL=o(j0,"Moved to "),Ep=s(j0,"A",{href:!0});var vV=i(Ep);tL=o(vV,"deepspeed-amp"),vV.forEach(r),rL=o(j0,"."),j0.forEach(r),K2=d(t),ga=s(t,"H4",{class:!0});var H0=i(ga);_n=s(H0,"A",{id:!0,class:!0,href:!0});var gV=i(_n);U1=s(gV,"SPAN",{});var bV=i(U1);f(kd.$$.fragment,bV),bV.forEach(r),gV.forEach(r),aL=d(H0),R1=s(H0,"SPAN",{});var yV=i(R1);oL=o(yV,"Batch Size"),yV.forEach(r),H0.forEach(r),X2=d(t),vn=s(t,"P",{});var G0=i(vn);nL=o(G0,"Moved to "),wp=s(G0,"A",{href:!0});var EV=i(wp);sL=o(EV,"deepspeed-bs"),EV.forEach(r),iL=o(G0,"."),G0.forEach(r),eE=d(t),ba=s(t,"H4",{class:!0});var W0=i(ba);gn=s(W0,"A",{id:!0,class:!0,href:!0});var wV=i(gn);M1=s(wV,"SPAN",{});var $V=i(M1);f(Dd.$$.fragment,$V),$V.forEach(r),wV.forEach(r),lL=d(W0),j1=s(W0,"SPAN",{});var TV=i(j1);dL=o(TV,"Gradient Accumulation"),TV.forEach(r),W0.forEach(r),tE=d(t),bn=s(t,"P",{});var B0=i(bn);cL=o(B0,"Moved to "),$p=s(B0,"A",{href:!0});var kV=i($p);pL=o(kV,"deepspeed-grad-acc"),kV.forEach(r),mL=o(B0,"."),B0.forEach(r),rE=d(t),ya=s(t,"H4",{class:!0});var V0=i(ya);yn=s(V0,"A",{id:!0,class:!0,href:!0});var DV=i(yn);H1=s(DV,"SPAN",{});var PV=i(H1);f(Pd.$$.fragment,PV),PV.forEach(r),DV.forEach(r),hL=d(V0),G1=s(V0,"SPAN",{});var AV=i(G1);fL=o(AV,"Gradient Clipping"),AV.forEach(r),V0.forEach(r),aE=d(t),En=s(t,"P",{});var Y0=i(En);uL=o(Y0,"Moved to "),Tp=s(Y0,"A",{href:!0});var OV=i(Tp);_L=o(OV,"deepspeed-grad-clip"),OV.forEach(r),vL=o(Y0,"."),Y0.forEach(r),oE=d(t),Ea=s(t,"H4",{class:!0});var Z0=i(Ea);wn=s(Z0,"A",{id:!0,class:!0,href:!0});var xV=i(wn);W1=s(xV,"SPAN",{});var SV=i(W1);f(Ad.$$.fragment,SV),SV.forEach(r),xV.forEach(r),gL=d(Z0),B1=s(Z0,"SPAN",{});var CV=i(B1);bL=o(CV,"Getting The Model Weights Out"),CV.forEach(r),Z0.forEach(r),nE=d(t),$n=s(t,"P",{});var J0=i($n);yL=o(J0,"Moved to "),kp=s(J0,"A",{href:!0});var NV=i(kp);EL=o(NV,"deepspeed-weight-extraction"),NV.forEach(r),wL=o(J0,"."),J0.forEach(r),this.h()},h(){m(w,"name","hf:doc:metadata"),m(w,"content",JSON.stringify(HV)),m(A,"id","trainer"),m(A,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(A,"href","#trainer"),m(k,"class","relative group"),m(le,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(Q,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TFTrainer"),m(x,"href","/docs/transformers/master/en/../examples"),m(Ee,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(we,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TFTrainer"),m(xe,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments"),m(W,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TFTrainingArguments"),m(Ce,"href","https://github.com/NVIDIA/apex"),m(Ce,"rel","nofollow"),m(qd,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(Ud,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TFTrainer"),m(Zd,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(Jd,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(Qd,"href","/docs/transformers/master/en/callback"),m(xa,"id","transformers.Trainer"),m(xa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(xa,"href","#transformers.Trainer"),m(pr,"class","relative group"),m(Kd,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),m(Ca,"class","docstring"),m(Na,"class","docstring"),m(ut,"class","docstring"),m(_t,"class","docstring"),m(vt,"class","docstring"),m(Ia,"class","docstring"),m(ze,"class","docstring"),m(gt,"class","docstring"),m(ec,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),m(za,"class","docstring"),m(bt,"class","docstring"),m(yt,"class","docstring"),m(Fe,"class","docstring"),m(Et,"class","docstring"),m(La,"class","docstring"),m(qa,"class","docstring"),m(Ua,"class","docstring"),m(wt,"class","docstring"),m(rc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(Cs,"href","https://github.com/pytorch/pytorch/issues/16266"),m(Cs,"rel","nofollow"),m(ac,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(oc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(z,"class","docstring"),m(Ra,"class","docstring"),m(Tt,"class","docstring"),m(kt,"class","docstring"),m(X,"class","docstring"),m(Dt,"class","docstring"),m(Pt,"class","docstring"),m(ja,"class","docstring"),m(Ha,"class","docstring"),m(Le,"class","docstring"),m(At,"class","docstring"),m(Ot,"class","docstring"),m(Ga,"class","docstring"),m(xt,"class","docstring"),m(b,"class","docstring"),m(Wa,"id","transformers.Seq2SeqTrainer"),m(Wa,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Wa,"href","#transformers.Seq2SeqTrainer"),m(Er,"class","relative group"),m(qe,"class","docstring"),m(ee,"class","docstring"),m(wr,"class","docstring"),m(Va,"id","transformers.TFTrainer"),m(Va,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Va,"href","#transformers.TFTrainer"),m(Dr,"class","relative group"),m(St,"class","docstring"),m(Ct,"class","docstring"),m(Nt,"class","docstring"),m(It,"class","docstring"),m(zt,"class","docstring"),m(Ft,"class","docstring"),m(pe,"class","docstring"),m(sc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TFTrainer.evaluate"),m(ic,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TFTrainer.predict"),m(Lt,"class","docstring"),m(qt,"class","docstring"),m(Ut,"class","docstring"),m(Ya,"class","docstring"),m(Ue,"class","docstring"),m(Re,"class","docstring"),m(Za,"class","docstring"),m(Rt,"class","docstring"),m(T,"class","docstring"),m(Ja,"id","transformers.TrainingArguments"),m(Ja,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ja,"href","#transformers.TrainingArguments"),m(Sr,"class","relative group"),m(lc,"href","/docs/transformers/master/en/internal/trainer_utils#transformers.HfArgumentParser"),m(Bi,"href","https://docs.python.org/3/library/argparse.html#module-argparse"),m(Bi,"rel","nofollow"),m(me,"class","docstring"),m(Qa,"class","docstring"),m(Mt,"class","docstring"),m(Ka,"class","docstring"),m(Xa,"class","docstring"),m(eo,"class","docstring"),m(N,"class","docstring"),m(to,"id","transformers.Seq2SeqTrainingArguments"),m(to,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(to,"href","#transformers.Seq2SeqTrainingArguments"),m(Fr,"class","relative group"),m(dc,"href","/docs/transformers/master/en/internal/trainer_utils#transformers.HfArgumentParser"),m(ol,"href","https://docs.python.org/3/library/argparse.html#module-argparse"),m(ol,"rel","nofollow"),m(se,"class","docstring"),m(ro,"id","transformers.TFTrainingArguments"),m(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ro,"href","#transformers.TFTrainingArguments"),m(qr,"class","relative group"),m(cc,"href","/docs/transformers/master/en/internal/trainer_utils#transformers.HfArgumentParser"),m(ml,"href","https://docs.python.org/3/library/argparse.html#module-argparse"),m(ml,"rel","nofollow"),m(it,"class","docstring"),m(ao,"id","checkpoints"),m(ao,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ao,"href","#checkpoints"),m(Rr,"class","relative group"),m(pc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(mc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments"),m(hc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer.train"),m(_c,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments"),m(lo,"id","logging"),m(lo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(lo,"href","#logging"),m(Mr,"class","relative group"),m(gc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(bc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments"),m(wc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.TrainingArguments"),m($c,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(Tc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(mo,"id","randomness"),m(mo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(mo,"href","#randomness"),m(jr,"class","relative group"),m(xc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(yl,"href","https://pytorch.org/docs/stable/notes/randomness.html"),m(yl,"rel","nofollow"),m(ho,"id","trainer-integrations"),m(ho,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ho,"href","#trainer-integrations"),m(Hr,"class","relative group"),m(Sc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(wl,"href","https://github.com/microsoft/DeepSpeed"),m(wl,"rel","nofollow"),m($l,"href","https://github.com/facebookresearch/fairscale/"),m($l,"rel","nofollow"),m(Tl,"href","https://arxiv.org/abs/1910.02054"),m(Tl,"rel","nofollow"),m(Nc,"id","zero-install-notes"),m(uo,"id","cuda-extension-installation-notes"),m(uo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(uo,"href","#cuda-extension-installation-notes"),m(Gr,"class","relative group"),m(Dl,"href","https://github.com/facebookresearch/fairscale/issues"),m(Dl,"rel","nofollow"),m(Pl,"href","https://github.com/microsoft/DeepSpeed/issues"),m(Pl,"rel","nofollow"),m(_o,"id","possible-problem-1"),m(_o,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_o,"href","#possible-problem-1"),m(Wr,"class","relative group"),m(Sl,"href","https://www.google.com/search?q=ubuntu+cuda+10.2+install"),m(Sl,"rel","nofollow"),m(go,"id","possible-problem-2"),m(go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(go,"href","#possible-problem-2"),m(Br,"class","relative group"),m(yo,"id","possible-problem-3"),m(yo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(yo,"href","#possible-problem-3"),m(Vr,"class","relative group"),m(wo,"id","fairscale"),m(wo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(wo,"href","#fairscale"),m(Yr,"class","relative group"),m(Ul,"href","https://github.com/facebookresearch/fairscale/"),m(Ul,"rel","nofollow"),m(Bc,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(Rl,"href","https://arxiv.org/abs/1910.02054"),m(Rl,"rel","nofollow"),m(Gl,"href","https://github.com/facebookresearch/fairscale/#installation"),m(Gl,"rel","nofollow"),m(Zc,"href","/docs/transformers/master/en/main_classes/trainer#zero-install-notes"),m(Ql,"href","https://github.com/facebookresearch/fairscale/issues"),m(Ql,"rel","nofollow"),m(rd,"start","3"),m(xo,"id","deepspeed"),m(xo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(xo,"href","#deepspeed"),m(Kr,"class","relative group"),m(ap,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-trainer-integration"),m(Co,"id","installation"),m(Co,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Co,"href","#installation"),m(Xr,"class","relative group"),m(op,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-installation"),m(Io,"id","deployment-with-multiple-gpus"),m(Io,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Io,"href","#deployment-with-multiple-gpus"),m(ea,"class","relative group"),m(np,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-multi-gpu"),m(Fo,"id","deployment-with-one-gpu"),m(Fo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Fo,"href","#deployment-with-one-gpu"),m(ta,"class","relative group"),m(sp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-one-gpu"),m(qo,"id","deployment-in-notebooks"),m(qo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(qo,"href","#deployment-in-notebooks"),m(ra,"class","relative group"),m(ip,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-notebook"),m(Ro,"id","configuration"),m(Ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ro,"href","#configuration"),m(aa,"class","relative group"),m(lp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-config"),m(jo,"id","passing-configuration"),m(jo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(jo,"href","#passing-configuration"),m(oa,"class","relative group"),m(dp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-config-passing"),m(Go,"id","shared-configuration"),m(Go,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Go,"href","#shared-configuration"),m(na,"class","relative group"),m(cp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-config-shared"),m(Bo,"id","zero"),m(Bo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Bo,"href","#zero"),m(sa,"class","relative group"),m(pp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-zero"),m(Yo,"id","zero2-config"),m(Yo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Yo,"href","#zero2-config"),m(ia,"class","relative group"),m(mp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-zero2-config"),m(Jo,"id","zero3-config"),m(Jo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Jo,"href","#zero3-config"),m(la,"class","relative group"),m(hp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-zero3-config"),m(Ko,"id","nvme-support"),m(Ko,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Ko,"href","#nvme-support"),m(da,"class","relative group"),m(fp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-nvme"),m(en,"id","zero2-vs-zero3-performance"),m(en,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(en,"href","#zero2-vs-zero3-performance"),m(ca,"class","relative group"),m(up,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-zero2-zero3-performance"),m(rn,"id","zero2-example"),m(rn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(rn,"href","#zero2-example"),m(pa,"class","relative group"),m(_p,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-zero2-example"),m(on,"id","zero3-example"),m(on,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(on,"href","#zero3-example"),m(ma,"class","relative group"),m(vp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-zero3-example"),m(sn,"id","optimizer-and-scheduler"),m(sn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(sn,"href","#optimizer-and-scheduler"),m(ha,"class","relative group"),m(ln,"id","optimizer"),m(ln,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ln,"href","#optimizer"),m(fa,"class","relative group"),m(gp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-optimizer"),m(cn,"id","scheduler"),m(cn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(cn,"href","#scheduler"),m(ua,"class","relative group"),m(bp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-scheduler"),m(mn,"id","fp32-precision"),m(mn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(mn,"href","#fp32-precision"),m(_a,"class","relative group"),m(yp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-fp32"),m(fn,"id","automatic-mixed-precision"),m(fn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(fn,"href","#automatic-mixed-precision"),m(va,"class","relative group"),m(Ep,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-amp"),m(_n,"id","batch-size"),m(_n,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_n,"href","#batch-size"),m(ga,"class","relative group"),m(wp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-bs"),m(gn,"id","gradient-accumulation"),m(gn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(gn,"href","#gradient-accumulation"),m(ba,"class","relative group"),m($p,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-grad-acc"),m(yn,"id","gradient-clipping"),m(yn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(yn,"href","#gradient-clipping"),m(ya,"class","relative group"),m(Tp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-grad-clip"),m(wn,"id","getting-the-model-weights-out"),m(wn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(wn,"href","#getting-the-model-weights-out"),m(Ea,"class","relative group"),m(kp,"href","/docs/transformers/master/en/main_classes/trainer#deepspeed-weight-extraction")},m(t,c){e(document.head,w),p(t,I,c),p(t,k,c),e(k,A),e(A,ve),u(H,ve,null),e(k,G),e(k,J),e(J,ge),p(t,re,c),p(t,R,c),e(R,ie),e(R,le),e(le,ae),e(R,de),e(R,Q),e(Q,tt),e(R,be),e(R,x),e(x,M),e(R,ct),p(t,ye,c),p(t,q,c),e(q,pt),e(q,Ee),e(Ee,Wn),e(q,Bn),e(q,we),e(we,Vn),e(q,Yn),e(q,xe),e(xe,Zn),e(q,Jn),e(q,W),e(W,Se),e(q,Nd),p(t,rt,c),p(t,oe,c),e(oe,Id),e(oe,Ce),e(Ce,zd),e(oe,ce),e(oe,Ta),e(Ta,Fd),e(oe,Ld),p(t,Qn,c),p(t,mt,c),e(mt,K0),e(mt,qd),e(qd,X0),e(mt,e$),e(mt,Ud),e(Ud,t$),e(mt,r$),p(t,rb,c),p(t,P,c),e(P,ka),e(ka,Fm),e(Fm,a$),e(ka,o$),e(ka,Lm),e(Lm,n$),e(ka,s$),e(P,i$),e(P,Da),e(Da,qm),e(qm,l$),e(Da,d$),e(Da,Um),e(Um,c$),e(Da,p$),e(P,m$),e(P,Pa),e(Pa,Rm),e(Rm,h$),e(Pa,f$),e(Pa,Mm),e(Mm,u$),e(Pa,_$),e(P,v$),e(P,Rd),e(Rd,jm),e(jm,g$),e(Rd,b$),e(P,y$),e(P,ht),e(ht,Hm),e(Hm,E$),e(ht,w$),e(ht,Gm),e(Gm,$$),e(ht,T$),e(ht,Wm),e(Wm,k$),e(ht,D$),e(P,P$),e(P,Md),e(Md,Bm),e(Bm,A$),e(Md,O$),e(P,x$),e(P,jd),e(jd,Vm),e(Vm,S$),e(jd,C$),e(P,N$),e(P,Hd),e(Hd,Ym),e(Ym,I$),e(Hd,z$),e(P,F$),e(P,Gd),e(Gd,Zm),e(Zm,L$),e(Gd,q$),e(P,U$),e(P,Wd),e(Wd,Jm),e(Jm,R$),e(Wd,M$),e(P,j$),e(P,Bd),e(Bd,Qm),e(Qm,H$),e(Bd,G$),e(P,W$),e(P,Vd),e(Vd,Km),e(Km,B$),e(Vd,V$),e(P,Y$),e(P,Yd),e(Yd,Xm),e(Xm,Z$),e(Yd,J$),p(t,ab,c),u(Aa,t,c),p(t,ob,c),p(t,Oa,c),e(Oa,Q$),e(Oa,Zd),e(Zd,K$),e(Oa,X$),p(t,nb,c),u(Kn,t,c),p(t,sb,c),p(t,ft,c),e(ft,e4),e(ft,Jd),e(Jd,t4),e(ft,r4),e(ft,Qd),e(Qd,a4),e(ft,o4),p(t,ib,c),p(t,pr,c),e(pr,xa),e(xa,eh),u(Xn,eh,null),e(pr,n4),e(pr,th),e(th,s4),p(t,lb,c),p(t,b,c),u(es,b,null),e(b,i4),e(b,rh),e(rh,l4),e(b,d4),e(b,ah),e(ah,c4),e(b,p4),e(b,$e),e($e,Sa),e(Sa,oh),e(oh,m4),e(Sa,h4),e(Sa,Kd),e(Kd,f4),e(Sa,u4),e($e,_4),e($e,K),e(K,nh),e(nh,v4),e(K,g4),e(K,sh),e(sh,b4),e(K,y4),e(K,ih),e(ih,E4),e(K,w4),e(K,lh),e(lh,$4),e(K,T4),e(K,dh),e(dh,k4),e(K,D4),e(K,ch),e(ch,P4),e(K,A4),e($e,O4),e($e,Xd),e(Xd,ph),e(ph,x4),e(Xd,S4),e($e,C4),e($e,Ne),e(Ne,mh),e(mh,N4),e(Ne,I4),e(Ne,hh),e(hh,z4),e(Ne,F4),e(Ne,fh),e(fh,L4),e(Ne,q4),e(Ne,uh),e(uh,U4),e(Ne,R4),e($e,M4),e($e,Ie),e(Ie,_h),e(_h,j4),e(Ie,H4),e(Ie,vh),e(vh,G4),e(Ie,W4),e(Ie,gh),e(gh,B4),e(Ie,V4),e(Ie,bh),e(bh,Y4),e(Ie,Z4),e(b,J4),e(b,Ca),u(ts,Ca,null),e(Ca,Q4),e(Ca,rs),e(rs,K4),e(rs,yh),e(yh,X4),e(rs,e3),e(b,t3),e(b,Na),u(as,Na,null),e(Na,r3),e(Na,os),e(os,a3),e(os,Eh),e(Eh,o3),e(os,n3),e(b,s3),e(b,ut),u(ns,ut,null),e(ut,i3),e(ut,wh),e(wh,l3),e(ut,d3),e(ut,$h),e($h,c3),e(b,p3),e(b,_t),u(ss,_t,null),e(_t,m3),e(_t,Th),e(Th,h3),e(_t,f3),e(_t,is),e(is,u3),e(is,kh),e(kh,_3),e(is,v3),e(b,g3),e(b,vt),u(ls,vt,null),e(vt,b3),e(vt,Dh),e(Dh,y3),e(vt,E3),e(vt,at),e(at,w3),e(at,Ph),e(Ph,$3),e(at,T3),e(at,Ah),e(Ah,k3),e(at,D3),e(at,Oh),e(Oh,P3),e(at,A3),e(b,O3),e(b,Ia),u(ds,Ia,null),e(Ia,x3),e(Ia,xh),e(xh,S3),e(b,C3),e(b,ze),u(cs,ze,null),e(ze,N3),e(ze,Sh),e(Sh,I3),e(ze,z3),e(ze,ps),e(ps,F3),e(ps,Ch),e(Ch,L3),e(ps,q3),e(ze,U3),e(ze,Nh),e(Nh,R3),e(b,M3),e(b,gt),u(ms,gt,null),e(gt,j3),e(gt,mr),e(mr,H3),e(mr,Ih),e(Ih,G3),e(mr,W3),e(mr,zh),e(zh,B3),e(mr,V3),e(gt,Y3),e(gt,Fh),e(Fh,Z3),e(b,J3),e(b,za),u(hs,za,null),e(za,Q3),e(za,fs),e(fs,K3),e(fs,ec),e(ec,X3),e(fs,e5),e(b,t5),e(b,bt),u(us,bt,null),e(bt,r5),e(bt,_s),e(_s,a5),e(_s,Lh),e(Lh,o5),e(_s,n5),e(bt,s5),e(bt,qh),e(qh,i5),e(b,l5),e(b,yt),u(vs,yt,null),e(yt,d5),e(yt,gs),e(gs,c5),e(gs,Uh),e(Uh,p5),e(gs,m5),e(yt,h5),e(yt,Rh),e(Rh,f5),e(b,u5),e(b,Fe),u(bs,Fe,null),e(Fe,_5),e(Fe,ys),e(ys,v5),e(ys,Mh),e(Mh,g5),e(ys,b5),e(Fe,y5),e(Fe,hr),e(hr,E5),e(hr,jh),e(jh,w5),e(hr,$5),e(hr,Hh),e(Hh,T5),e(hr,k5),e(Fe,D5),e(Fe,Gh),e(Gh,P5),e(b,A5),e(b,Et),u(Es,Et,null),e(Et,O5),e(Et,Te),e(Te,x5),e(Te,Wh),e(Wh,S5),e(Te,C5),e(Te,Bh),e(Bh,N5),e(Te,I5),e(Te,Vh),e(Vh,z5),e(Te,F5),e(Te,Yh),e(Yh,L5),e(Te,q5),e(Et,U5),u(Fa,Et,null),e(b,R5),e(b,La),u(ws,La,null),e(La,M5),e(La,$s),e($s,j5),e($s,Zh),e(Zh,H5),e($s,G5),e(b,W5),e(b,qa),u(Ts,qa,null),e(qa,B5),e(qa,Jh),e(Jh,V5),e(b,Y5),e(b,Ua),u(ks,Ua,null),e(Ua,Z5),e(Ua,Ds),e(Ds,J5),e(Ds,Qh),e(Qh,Q5),e(Ds,K5),e(b,X5),e(b,wt),u(Ps,wt,null),e(wt,eT),e(wt,As),e(As,tT),e(As,Kh),e(Kh,rT),e(As,aT),e(wt,oT),e(wt,Xh),e(Xh,nT),e(b,sT),e(b,z),u(Os,z,null),e(z,iT),e(z,ef),e(ef,lT),e(z,dT),e(z,tf),e(tf,cT),e(z,pT),e(z,rf),e(rf,mT),e(z,hT),e(z,fr),e(fr,fT),e(fr,af),e(af,uT),e(fr,_T),e(fr,of),e(of,vT),e(fr,gT),e(z,bT),e(z,nf),e(nf,yT),e(z,ET),u(xs,z,null),e(z,wT),e(z,sf),e(sf,lf),e(lf,$T),e(z,TT),e(z,ot),e(ot,df),e(df,ke),e(ke,kT),e(ke,cf),e(cf,DT),e(ke,PT),e(ke,pf),e(pf,AT),e(ke,OT),e(ke,mf),e(mf,xT),e(ke,ST),e(ke,hf),e(hf,CT),e(ke,NT),e(ot,IT),e(ot,ff),e(ff,ur),e(ur,zT),e(ur,uf),e(uf,FT),e(ur,LT),e(ur,_f),e(_f,qT),e(ur,UT),e(ot,RT),e(ot,vf),e(vf,tc),e(tc,gf),e(gf,MT),e(tc,jT),e(ot,HT),e(ot,U),e(U,$t),e($t,bf),e(bf,GT),e($t,WT),e($t,yf),e(yf,BT),e($t,VT),e($t,Ef),e(Ef,YT),e($t,ZT),e(U,JT),e(U,wf),e(wf,QT),e(U,KT),e(U,$f),e($f,XT),e(U,e6),e(U,Ss),e(Ss,t6),e(Ss,Tf),e(Tf,r6),e(Ss,a6),e(U,o6),e(U,nt),e(nt,n6),e(nt,kf),e(kf,s6),e(nt,i6),e(nt,Df),e(Df,l6),e(nt,d6),e(nt,Pf),e(Pf,c6),e(nt,p6),e(U,m6),e(U,ne),e(ne,h6),e(ne,rc),e(rc,f6),e(ne,u6),e(ne,Af),e(Af,_6),e(ne,v6),e(ne,Of),e(Of,g6),e(ne,b6),e(ne,xf),e(xf,y6),e(ne,E6),e(ne,Sf),e(Sf,w6),e(ne,$6),e(U,T6),e(U,S),e(S,k6),e(S,Cf),e(Cf,D6),e(S,P6),e(S,Nf),e(Nf,A6),e(S,O6),e(S,If),e(If,x6),e(S,S6),e(S,zf),e(zf,C6),e(S,N6),e(S,Cs),e(Cs,I6),e(S,z6),e(S,Ff),e(Ff,F6),e(S,L6),e(S,Lf),e(Lf,q6),e(S,U6),e(S,qf),e(qf,R6),e(S,M6),e(S,Uf),e(Uf,j6),e(S,H6),e(S,Rf),e(Rf,G6),e(S,W6),e(U,B6),e(U,De),e(De,V6),e(De,ac),e(ac,Y6),e(De,Z6),e(De,Mf),e(Mf,J6),e(De,Q6),e(De,oc),e(oc,K6),e(De,X6),e(De,jf),e(jf,e7),e(De,t7),e(U,r7),e(U,Hf),e(Hf,a7),e(b,o7),e(b,Ra),u(Ns,Ra,null),e(Ra,n7),e(Ra,Gf),e(Gf,s7),e(b,i7),e(b,Tt),u(Is,Tt,null),e(Tt,l7),e(Tt,zs),e(zs,d7),e(zs,Wf),e(Wf,c7),e(zs,p7),e(Tt,m7),e(Tt,nc),e(nc,h7),e(nc,Bf),e(Bf,f7),e(b,u7),e(b,kt),u(Fs,kt,null),e(kt,_7),e(kt,Ls),e(Ls,v7),e(Ls,Vf),e(Vf,g7),e(Ls,b7),e(kt,y7),e(kt,qs),e(qs,E7),e(qs,Yf),e(Yf,w7),e(qs,$7),e(b,T7),e(b,X),u(Us,X,null),e(X,k7),e(X,Zf),e(Zf,D7),e(X,P7),e(X,Rs),e(Rs,A7),e(Rs,Jf),e(Jf,O7),e(Rs,x7),e(X,S7),u(Ma,X,null),e(X,C7),e(X,Ms),e(Ms,N7),e(Ms,Qf),e(Qf,I7),e(Ms,z7),e(X,F7),e(X,_r),e(_r,vr),e(vr,L7),e(vr,Kf),e(Kf,q7),e(vr,U7),e(vr,Xf),e(Xf,R7),e(vr,M7),e(_r,j7),e(_r,js),e(js,H7),e(js,Hs),e(Hs,G7),e(Hs,eu),e(eu,W7),e(Hs,B7),e(js,V7),e(_r,Y7),e(_r,gr),e(gr,Z7),e(gr,tu),e(tu,J7),e(gr,Q7),e(gr,ru),e(ru,K7),e(gr,X7),e(b,e8),e(b,Dt),u(Gs,Dt,null),e(Dt,t8),e(Dt,br),e(br,r8),e(br,au),e(au,a8),e(br,o8),e(br,ou),e(ou,n8),e(br,s8),e(Dt,i8),e(Dt,nu),e(nu,l8),e(b,d8),e(b,Pt),u(Ws,Pt,null),e(Pt,c8),e(Pt,yr),e(yr,p8),e(yr,su),e(su,m8),e(yr,h8),e(yr,iu),e(iu,f8),e(yr,u8),e(Pt,_8),e(Pt,lu),e(lu,v8),e(b,g8),e(b,ja),u(Bs,ja,null),e(ja,b8),e(ja,st),e(st,y8),e(st,du),e(du,E8),e(st,w8),e(st,cu),e(cu,$8),e(st,T8),e(st,pu),e(pu,k8),e(st,D8),e(b,P8),e(b,Ha),u(Vs,Ha,null),e(Ha,A8),e(Ha,Ys),e(Ys,O8),e(Ys,mu),e(mu,x8),e(Ys,S8),e(b,C8),e(b,Le),u(Zs,Le,null),e(Le,N8),e(Le,Js),e(Js,I8),e(Js,hu),e(hu,z8),e(Js,F8),e(Le,L8),e(Le,fu),e(fu,q8),e(Le,U8),e(Le,Qs),e(Qs,R8),e(Qs,uu),e(uu,M8),e(Qs,j8),e(b,H8),e(b,At),u(Ks,At,null),e(At,G8),e(At,Xs),e(Xs,W8),e(Xs,_u),e(_u,B8),e(Xs,V8),e(At,Y8),e(At,vu),e(vu,Z8),e(b,J8),e(b,Ot),u(ei,Ot,null),e(Ot,Q8),e(Ot,gu),e(gu,K8),e(Ot,X8),e(Ot,bu),e(bu,e9),e(b,t9),e(b,Ga),u(ti,Ga,null),e(Ga,r9),e(Ga,yu),e(yu,a9),e(b,o9),e(b,xt),u(ri,xt,null),e(xt,n9),e(xt,Eu),e(Eu,s9),e(xt,i9),e(xt,wu),e(wu,l9),p(t,db,c),p(t,Er,c),e(Er,Wa),e(Wa,$u),u(ai,$u,null),e(Er,d9),e(Er,Tu),e(Tu,c9),p(t,cb,c),p(t,wr,c),e(wr,qe),u(oi,qe,null),e(qe,p9),e(qe,ku),e(ku,m9),e(qe,h9),e(qe,ni),e(ni,f9),e(ni,Du),e(Du,u9),e(ni,_9),e(qe,v9),e(qe,Pu),e(Pu,g9),e(wr,b9),e(wr,ee),u(si,ee,null),e(ee,y9),e(ee,Au),e(Au,E9),e(ee,w9),e(ee,ii),e(ii,$9),e(ii,Ou),e(Ou,T9),e(ii,k9),e(ee,D9),u(Ba,ee,null),e(ee,P9),e(ee,li),e(li,A9),e(li,xu),e(xu,O9),e(li,x9),e(ee,S9),e(ee,$r),e($r,Tr),e(Tr,C9),e(Tr,Su),e(Su,N9),e(Tr,I9),e(Tr,Cu),e(Cu,z9),e(Tr,F9),e($r,L9),e($r,di),e(di,q9),e(di,ci),e(ci,U9),e(ci,Nu),e(Nu,R9),e(ci,M9),e(di,j9),e($r,H9),e($r,kr),e(kr,G9),e(kr,Iu),e(Iu,W9),e(kr,B9),e(kr,zu),e(zu,V9),e(kr,Y9),p(t,pb,c),p(t,Dr,c),e(Dr,Va),e(Va,Fu),u(pi,Fu,null),e(Dr,Z9),e(Dr,Lu),e(Lu,J9),p(t,mb,c),p(t,T,c),u(mi,T,null),e(T,Q9),e(T,qu),e(qu,K9),e(T,X9),e(T,St),u(hi,St,null),e(St,ek),e(St,Uu),e(Uu,tk),e(St,rk),e(St,fi),e(fi,ak),e(fi,Ru),e(Ru,ok),e(fi,nk),e(T,sk),e(T,Ct),u(ui,Ct,null),e(Ct,ik),e(Ct,Mu),e(Mu,lk),e(Ct,dk),e(Ct,_i),e(_i,ck),e(_i,ju),e(ju,pk),e(_i,mk),e(T,hk),e(T,Nt),u(vi,Nt,null),e(Nt,fk),e(Nt,gi),e(gi,uk),e(gi,Hu),e(Hu,_k),e(gi,vk),e(Nt,gk),e(Nt,Gu),e(Gu,bk),e(T,yk),e(T,It),u(bi,It,null),e(It,Ek),e(It,yi),e(yi,wk),e(yi,Wu),e(Wu,$k),e(yi,Tk),e(It,kk),e(It,Bu),e(Bu,Dk),e(T,Pk),e(T,zt),u(Ei,zt,null),e(zt,Ak),e(zt,wi),e(wi,Ok),e(wi,Vu),e(Vu,xk),e(wi,Sk),e(zt,Ck),e(zt,Yu),e(Yu,Nk),e(T,Ik),e(T,Ft),u($i,Ft,null),e(Ft,zk),e(Ft,Ti),e(Ti,Fk),e(Ti,Zu),e(Zu,Lk),e(Ti,qk),e(Ft,Uk),e(Ft,Ju),e(Ju,Rk),e(T,Mk),e(T,pe),u(ki,pe,null),e(pe,jk),e(pe,Qu),e(Qu,Hk),e(pe,Gk),e(pe,Di),e(Di,Wk),e(Di,Ku),e(Ku,Bk),e(Di,Vk),e(pe,Yk),e(pe,Pi),e(Pi,Zk),e(Pi,Xu),e(Xu,Jk),e(Pi,Qk),e(pe,Kk),e(pe,Pr),e(Pr,Ar),e(Ar,Xk),e(Ar,e_),e(e_,eD),e(Ar,tD),e(Ar,t_),e(t_,rD),e(Ar,aD),e(Pr,oD),e(Pr,Ai),e(Ai,nD),e(Ai,Oi),e(Oi,sD),e(Oi,r_),e(r_,iD),e(Oi,lD),e(Ai,dD),e(Pr,cD),e(Pr,Or),e(Or,pD),e(Or,a_),e(a_,mD),e(Or,hD),e(Or,o_),e(o_,fD),e(Or,uD),e(T,_D),e(T,Lt),u(xi,Lt,null),e(Lt,vD),e(Lt,xr),e(xr,gD),e(xr,sc),e(sc,bD),e(xr,yD),e(xr,ic),e(ic,ED),e(xr,wD),e(Lt,$D),e(Lt,n_),e(n_,TD),e(T,kD),e(T,qt),u(Si,qt,null),e(qt,DD),e(qt,s_),e(s_,PD),e(qt,AD),e(qt,i_),e(i_,OD),e(T,xD),e(T,Ut),u(Ci,Ut,null),e(Ut,SD),e(Ut,l_),e(l_,CD),e(Ut,ND),e(Ut,d_),e(d_,ID),e(T,zD),e(T,Ya),u(Ni,Ya,null),e(Ya,FD),e(Ya,Ii),e(Ii,LD),e(Ii,c_),e(c_,qD),e(Ii,UD),e(T,RD),e(T,Ue),u(zi,Ue,null),e(Ue,MD),e(Ue,p_),e(p_,jD),e(Ue,HD),e(Ue,Fi),e(Fi,GD),e(Fi,m_),e(m_,WD),e(Fi,BD),e(Ue,VD),e(Ue,Li),e(Li,YD),e(Li,h_),e(h_,ZD),e(Li,JD),e(T,QD),e(T,Re),u(qi,Re,null),e(Re,KD),e(Re,Ui),e(Ui,XD),e(Ui,f_),e(f_,eP),e(Ui,tP),e(Re,rP),e(Re,Ri),e(Ri,aP),e(Ri,u_),e(u_,oP),e(Ri,nP),e(Re,sP),e(Re,__),e(__,iP),e(T,lP),e(T,Za),u(Mi,Za,null),e(Za,dP),e(Za,v_),e(v_,cP),e(T,pP),e(T,Rt),u(ji,Rt,null),e(Rt,mP),e(Rt,g_),e(g_,hP),e(Rt,fP),e(Rt,b_),e(b_,uP),p(t,hb,c),p(t,Sr,c),e(Sr,Ja),e(Ja,y_),u(Hi,y_,null),e(Sr,_P),e(Sr,E_),e(E_,vP),p(t,fb,c),p(t,N,c),u(Gi,N,null),e(N,gP),e(N,Wi),e(Wi,bP),e(Wi,w_),e(w_,yP),e(Wi,EP),e(N,wP),e(N,Cr),e(Cr,$P),e(Cr,lc),e(lc,TP),e(Cr,kP),e(Cr,Bi),e(Bi,DP),e(Cr,PP),e(N,AP),e(N,me),u(Vi,me,null),e(me,OP),e(me,$_),e($_,xP),e(me,SP),e(me,Nr),e(Nr,CP),e(Nr,T_),e(T_,NP),e(Nr,IP),e(Nr,k_),e(k_,zP),e(Nr,FP),e(me,LP),e(me,Ir),e(Ir,qP),e(Ir,D_),e(D_,UP),e(Ir,RP),e(Ir,P_),e(P_,MP),e(Ir,jP),e(me,HP),e(me,Yi),e(Yi,GP),e(Yi,A_),e(A_,WP),e(Yi,BP),e(N,VP),e(N,Qa),u(Zi,Qa,null),e(Qa,YP),e(Qa,O_),e(O_,ZP),e(N,JP),e(N,Mt),u(Ji,Mt,null),e(Mt,QP),e(Mt,x_),e(x_,KP),e(Mt,XP),e(Mt,zr),e(zr,eA),e(zr,S_),e(S_,tA),e(zr,rA),e(zr,C_),e(C_,aA),e(zr,oA),e(N,nA),e(N,Ka),u(Qi,Ka,null),e(Ka,sA),e(Ka,Ki),e(Ki,iA),e(Ki,N_),e(N_,lA),e(Ki,dA),e(N,cA),e(N,Xa),u(Xi,Xa,null),e(Xa,pA),e(Xa,I_),e(I_,mA),e(N,hA),e(N,eo),u(el,eo,null),e(eo,fA),e(eo,z_),e(z_,uA),p(t,ub,c),p(t,Fr,c),e(Fr,to),e(to,F_),u(tl,F_,null),e(Fr,_A),e(Fr,L_),e(L_,vA),p(t,_b,c),p(t,se,c),u(rl,se,null),e(se,gA),e(se,al),e(al,bA),e(al,q_),e(q_,yA),e(al,EA),e(se,wA),e(se,Lr),e(Lr,$A),e(Lr,dc),e(dc,TA),e(Lr,kA),e(Lr,ol),e(ol,DA),e(Lr,PA),e(se,AA),e(se,Pe),e(Pe,OA),e(Pe,nl),e(nl,xA),e(nl,U_),e(U_,SA),e(nl,CA),e(Pe,NA),e(Pe,R_),e(R_,IA),e(Pe,zA),e(Pe,M_),e(M_,FA),e(Pe,LA),e(Pe,j_),e(j_,qA),e(Pe,UA),e(se,RA),e(se,C),e(C,MA),e(C,sl),e(sl,jA),e(sl,H_),e(H_,HA),e(sl,GA),e(C,WA),e(C,G_),e(G_,BA),e(C,VA),e(C,il),e(il,YA),e(il,W_),e(W_,ZA),e(il,JA),e(C,QA),e(C,B_),e(B_,KA),e(C,XA),e(C,V_),e(V_,eO),e(C,tO),e(C,Y_),e(Y_,rO),e(C,aO),e(C,ll),e(ll,oO),e(ll,Z_),e(Z_,nO),e(ll,sO),e(C,iO),e(C,J_),e(J_,lO),e(C,dO),e(C,Q_),e(Q_,cO),e(C,pO),e(C,K_),e(K_,mO),e(C,hO),p(t,vb,c),p(t,qr,c),e(qr,ro),e(ro,X_),u(dl,X_,null),e(qr,fO),e(qr,ev),e(ev,uO),p(t,gb,c),p(t,it,c),u(cl,it,null),e(it,_O),e(it,pl),e(pl,vO),e(pl,tv),e(tv,gO),e(pl,bO),e(it,yO),e(it,Ur),e(Ur,EO),e(Ur,cc),e(cc,wO),e(Ur,$O),e(Ur,ml),e(ml,TO),e(Ur,kO),p(t,bb,c),p(t,Rr,c),e(Rr,ao),e(ao,rv),u(hl,rv,null),e(Rr,DO),e(Rr,av),e(av,PO),p(t,yb,c),p(t,he,c),e(he,AO),e(he,pc),e(pc,OO),e(he,xO),e(he,ov),e(ov,SO),e(he,CO),e(he,mc),e(mc,NO),e(he,IO),e(he,nv),e(nv,zO),e(he,FO),p(t,Eb,c),p(t,oo,c),e(oo,LO),e(oo,hc),e(hc,qO),e(oo,UO),p(t,wb,c),p(t,no,c),e(no,fc),e(fc,sv),e(sv,RO),e(fc,MO),e(no,jO),e(no,uc),e(uc,iv),e(iv,HO),e(uc,GO),p(t,$b,c),p(t,Me,c),e(Me,WO),e(Me,lv),e(lv,BO),e(Me,VO),e(Me,dv),e(dv,YO),e(Me,ZO),e(Me,_c),e(_c,JO),e(Me,QO),p(t,Tb,c),p(t,so,c),e(so,io),e(io,cv),e(cv,KO),e(io,XO),e(io,pv),e(pv,ex),e(io,tx),e(so,rx),e(so,vc),e(vc,mv),e(mv,ax),e(vc,ox),p(t,kb,c),p(t,Mr,c),e(Mr,lo),e(lo,hv),u(fl,hv,null),e(Mr,nx),e(Mr,fv),e(fv,sx),p(t,Db,c),p(t,je,c),e(je,ix),e(je,gc),e(gc,lx),e(je,dx),e(je,uv),e(uv,cx),e(je,px),e(je,_v),e(_v,mx),e(je,hx),p(t,Pb,c),p(t,jt,c),e(jt,fx),e(jt,vv),e(vv,ux),e(jt,_x),e(jt,bc),e(bc,vx),e(jt,gx),p(t,Ab,c),p(t,co,c),e(co,yc),e(yc,gv),e(gv,bx),e(yc,yx),e(co,Ex),e(co,Ec),e(Ec,bv),e(bv,wx),e(Ec,$x),p(t,Ob,c),p(t,He,c),e(He,Tx),e(He,wc),e(wc,kx),e(He,Dx),e(He,yv),e(yv,Px),e(He,Ax),e(He,Ev),e(Ev,Ox),e(He,xx),p(t,xb,c),p(t,te,c),e(te,Sx),e(te,$c),e($c,Cx),e(te,Nx),e(te,wv),e(wv,Ix),e(te,zx),e(te,$v),e($v,Fx),e(te,Lx),e(te,Tv),e(Tv,qx),e(te,Ux),e(te,Tc),e(Tc,Rx),e(te,Mx),p(t,Sb,c),p(t,kc,c),e(kc,jx),p(t,Cb,c),u(ul,t,c),p(t,Nb,c),p(t,Dc,c),e(Dc,Hx),p(t,Ib,c),u(_l,t,c),p(t,zb,c),p(t,Pc,c),e(Pc,Gx),p(t,Fb,c),u(vl,t,c),p(t,Lb,c),p(t,Ac,c),e(Ac,Wx),p(t,qb,c),p(t,Oc,c),e(Oc,Bx),p(t,Ub,c),u(gl,t,c),p(t,Rb,c),p(t,po,c),e(po,Vx),e(po,kv),e(kv,Yx),e(po,Zx),p(t,Mb,c),p(t,jr,c),e(jr,mo),e(mo,Dv),u(bl,Dv,null),e(jr,Jx),e(jr,Pv),e(Pv,Qx),p(t,jb,c),p(t,fe,c),e(fe,Kx),e(fe,xc),e(xc,Xx),e(fe,eS),e(fe,Av),e(Av,tS),e(fe,rS),e(fe,Ov),e(Ov,aS),e(fe,oS),e(fe,xv),e(xv,nS),e(fe,sS),p(t,Hb,c),p(t,Ht,c),e(Ht,iS),e(Ht,yl),e(yl,lS),e(Ht,dS),e(Ht,Sv),e(Sv,cS),e(Ht,pS),p(t,Gb,c),p(t,Hr,c),e(Hr,ho),e(ho,Cv),u(El,Cv,null),e(Hr,mS),e(Hr,Nv),e(Nv,hS),p(t,Wb,c),p(t,fo,c),e(fo,fS),e(fo,Sc),e(Sc,uS),e(fo,_S),p(t,Bb,c),p(t,Ge,c),e(Ge,vS),e(Ge,wl),e(wl,gS),e(Ge,bS),e(Ge,$l),e($l,yS),e(Ge,ES),e(Ge,Tl),e(Tl,wS),e(Ge,$S),p(t,Vb,c),p(t,Cc,c),e(Cc,TS),p(t,Yb,c),p(t,Nc,c),p(t,Zb,c),p(t,Gr,c),e(Gr,uo),e(uo,Iv),u(kl,Iv,null),e(Gr,kS),e(Gr,zv),e(zv,DS),p(t,Jb,c),p(t,Ic,c),e(Ic,PS),p(t,Qb,c),p(t,Gt,c),e(Gt,AS),e(Gt,Dl),e(Dl,OS),e(Gt,xS),e(Gt,Pl),e(Pl,SS),e(Gt,CS),p(t,Kb,c),p(t,zc,c),e(zc,NS),p(t,Xb,c),u(Al,t,c),p(t,ey,c),p(t,Fc,c),e(Fc,IS),p(t,ty,c),p(t,Wt,c),e(Wt,zS),e(Wt,Fv),e(Fv,FS),e(Wt,LS),e(Wt,Lv),e(Lv,qS),e(Wt,US),p(t,ry,c),p(t,Wr,c),e(Wr,_o),e(_o,qv),u(Ol,qv,null),e(Wr,RS),e(Wr,Uv),e(Uv,MS),p(t,ay,c),p(t,Lc,c),e(Lc,jS),p(t,oy,c),p(t,We,c),e(We,HS),e(We,Rv),e(Rv,GS),e(We,WS),e(We,Mv),e(Mv,BS),e(We,VS),e(We,jv),e(jv,YS),e(We,ZS),p(t,ny,c),p(t,Bt,c),e(Bt,JS),e(Bt,Hv),e(Hv,QS),e(Bt,KS),e(Bt,Gv),e(Gv,XS),e(Bt,eC),p(t,sy,c),u(xl,t,c),p(t,iy,c),p(t,vo,c),e(vo,tC),e(vo,Sl),e(Sl,rC),e(vo,aC),p(t,ly,c),p(t,Br,c),e(Br,go),e(go,Wv),u(Cl,Wv,null),e(Br,oC),e(Br,Bv),e(Bv,nC),p(t,dy,c),p(t,qc,c),e(qc,sC),p(t,cy,c),u(Nl,t,c),p(t,py,c),p(t,Vt,c),e(Vt,iC),e(Vt,Vv),e(Vv,lC),e(Vt,dC),e(Vt,Yv),e(Yv,cC),e(Vt,pC),p(t,my,c),p(t,Uc,c),e(Uc,mC),p(t,hy,c),u(Il,t,c),p(t,fy,c),p(t,Rc,c),e(Rc,hC),p(t,uy,c),p(t,bo,c),e(bo,fC),e(bo,Zv),e(Zv,uC),e(bo,_C),p(t,_y,c),p(t,lt,c),e(lt,Jv),e(Jv,vC),e(lt,gC),e(lt,Qv),e(Qv,bC),e(lt,yC),e(lt,Kv),e(Kv,EC),e(lt,wC),p(t,vy,c),p(t,Mc,c),e(Mc,$C),p(t,gy,c),u(zl,t,c),p(t,by,c),p(t,jc,c),e(jc,TC),p(t,yy,c),p(t,Be,c),e(Be,kC),e(Be,Xv),e(Xv,DC),e(Be,PC),e(Be,eg),e(eg,AC),e(Be,OC),e(Be,tg),e(tg,xC),e(Be,SC),p(t,Ey,c),p(t,Vr,c),e(Vr,yo),e(yo,rg),u(Fl,rg,null),e(Vr,CC),e(Vr,ag),e(ag,NC),p(t,wy,c),p(t,Yt,c),e(Yt,IC),e(Yt,og),e(og,zC),e(Yt,FC),e(Yt,ng),e(ng,LC),e(Yt,qC),p(t,$y,c),p(t,Hc,c),e(Hc,UC),p(t,Ty,c),p(t,Gc,c),e(Gc,RC),p(t,ky,c),p(t,Eo,c),e(Eo,MC),e(Eo,sg),e(sg,jC),e(Eo,HC),p(t,Dy,c),u(Ll,t,c),p(t,Py,c),p(t,B,c),e(B,GC),e(B,ig),e(ig,WC),e(B,BC),e(B,lg),e(lg,VC),e(B,YC),e(B,dg),e(dg,ZC),e(B,JC),e(B,cg),e(cg,QC),e(B,KC),e(B,pg),e(pg,XC),e(B,eN),e(B,mg),e(mg,tN),e(B,rN),p(t,Ay,c),p(t,Wc,c),e(Wc,aN),p(t,Oy,c),p(t,Yr,c),e(Yr,wo),e(wo,hg),u(ql,hg,null),e(Yr,oN),e(Yr,fg),e(fg,nN),p(t,xy,c),p(t,Ve,c),e(Ve,sN),e(Ve,Ul),e(Ul,iN),e(Ve,lN),e(Ve,Bc),e(Bc,dN),e(Ve,cN),e(Ve,Rl),e(Rl,pN),e(Ve,mN),p(t,Sy,c),p(t,Ye,c),e(Ye,ug),e(ug,hN),e(Ye,fN),e(Ye,_g),e(_g,uN),e(Ye,_N),e(Ye,vg),e(vg,vN),e(Ye,gN),e(Ye,gg),e(gg,bN),p(t,Cy,c),p(t,Vc,c),e(Vc,yN),p(t,Ny,c),p(t,Ml,c),e(Ml,bg),e(bg,EN),e(Ml,wN),p(t,Iy,c),p(t,Yc,c),e(Yc,$N),p(t,zy,c),u(jl,t,c),p(t,Fy,c),p(t,Zt,c),e(Zt,TN),e(Zt,yg),e(yg,kN),e(Zt,DN),e(Zt,Eg),e(Eg,PN),e(Zt,AN),p(t,Ly,c),u(Hl,t,c),p(t,qy,c),p(t,$o,c),e($o,ON),e($o,wg),e(wg,xN),e($o,SN),p(t,Uy,c),p(t,To,c),e(To,CN),e(To,Gl),e(Gl,NN),e(To,IN),p(t,Ry,c),p(t,ko,c),e(ko,zN),e(ko,Zc),e(Zc,FN),e(ko,LN),p(t,My,c),p(t,Jc,c),e(Jc,qN),p(t,jy,c),p(t,Wl,c),e(Wl,$g),e($g,UN),e(Wl,RN),p(t,Hy,c),u(Bl,t,c),p(t,Gy,c),p(t,Qc,c),e(Qc,MN),p(t,Wy,c),u(Vl,t,c),p(t,By,c),p(t,Yl,c),e(Yl,Tg),e(Tg,jN),e(Yl,HN),p(t,Vy,c),u(Zl,t,c),p(t,Yy,c),p(t,Kc,c),e(Kc,GN),p(t,Zy,c),u(Jl,t,c),p(t,Jy,c),p(t,Xc,c),e(Xc,WN),p(t,Qy,c),p(t,Do,c),e(Do,BN),e(Do,Ql),e(Ql,VN),e(Do,YN),p(t,Ky,c),p(t,Kl,c),e(Kl,kg),e(kg,ZN),e(Kl,JN),p(t,Xy,c),p(t,Jt,c),e(Jt,QN),e(Jt,Dg),e(Dg,KN),e(Jt,XN),e(Jt,Pg),e(Pg,eI),e(Jt,tI),p(t,e2,c),p(t,Po,c),e(Po,rI),e(Po,Ag),e(Ag,aI),e(Po,oI),p(t,t2,c),u(Xl,t,c),p(t,r2,c),p(t,ep,c),e(ep,nI),p(t,a2,c),p(t,Ze,c),e(Ze,Og),e(Og,sI),e(Ze,iI),e(Ze,xg),e(xg,lI),e(Ze,dI),e(Ze,ed),e(ed,cI),e(ed,Sg),e(Sg,pI),e(ed,mI),e(Ze,hI),e(Ze,td),e(td,fI),e(td,Cg),e(Cg,uI),e(td,_I),p(t,o2,c),p(t,rd,c),e(rd,dt),e(dt,vI),e(dt,Ng),e(Ng,gI),e(dt,bI),e(dt,Ig),e(Ig,yI),e(dt,EI),e(dt,zg),e(zg,wI),e(dt,$I),p(t,n2,c),p(t,Ao,c),e(Ao,TI),e(Ao,Fg),e(Fg,kI),e(Ao,DI),p(t,s2,c),u(ad,t,c),p(t,i2,c),p(t,Zr,c),e(Zr,Lg),e(Lg,PI),e(Zr,AI),e(Zr,qg),e(qg,OI),e(Zr,xI),p(t,l2,c),p(t,Qt,c),e(Qt,SI),e(Qt,Ug),e(Ug,CI),e(Qt,NI),e(Qt,Rg),e(Rg,II),e(Qt,zI),p(t,d2,c),p(t,tp,c),e(tp,FI),p(t,c2,c),p(t,ue,c),e(ue,Mg),e(Mg,LI),e(ue,qI),e(ue,jg),e(jg,UI),e(ue,RI),e(ue,od),e(od,MI),e(od,Hg),e(Hg,jI),e(od,HI),e(ue,GI),e(ue,Jr),e(Jr,WI),e(Jr,Gg),e(Gg,BI),e(Jr,VI),e(Jr,Wg),e(Wg,YI),e(Jr,ZI),e(ue,JI),e(ue,Bg),e(Bg,QI),p(t,p2,c),p(t,rp,c),e(rp,KI),p(t,m2,c),p(t,Oo,c),e(Oo,Qr),e(Qr,XI),e(Qr,Vg),e(Vg,ez),e(Qr,tz),e(Qr,Yg),e(Yg,rz),e(Qr,az),e(Oo,oz),e(Oo,Ae),e(Ae,nz),e(Ae,Zg),e(Zg,sz),e(Ae,iz),e(Ae,Jg),e(Jg,lz),e(Ae,dz),e(Ae,Qg),e(Qg,cz),e(Ae,pz),e(Ae,Kg),e(Kg,mz),e(Ae,hz),p(t,h2,c),p(t,Kr,c),e(Kr,xo),e(xo,Xg),u(nd,Xg,null),e(Kr,fz),e(Kr,e1),e(e1,uz),p(t,f2,c),p(t,So,c),e(So,_z),e(So,ap),e(ap,vz),e(So,gz),p(t,u2,c),p(t,Xr,c),e(Xr,Co),e(Co,t1),u(sd,t1,null),e(Xr,bz),e(Xr,r1),e(r1,yz),p(t,_2,c),p(t,No,c),e(No,Ez),e(No,op),e(op,wz),e(No,$z),p(t,v2,c),p(t,ea,c),e(ea,Io),e(Io,a1),u(id,a1,null),e(ea,Tz),e(ea,o1),e(o1,kz),p(t,g2,c),p(t,zo,c),e(zo,Dz),e(zo,np),e(np,Pz),e(zo,Az),p(t,b2,c),p(t,ta,c),e(ta,Fo),e(Fo,n1),u(ld,n1,null),e(ta,Oz),e(ta,s1),e(s1,xz),p(t,y2,c),p(t,Lo,c),e(Lo,Sz),e(Lo,sp),e(sp,Cz),e(Lo,Nz),p(t,E2,c),p(t,ra,c),e(ra,qo),e(qo,i1),u(dd,i1,null),e(ra,Iz),e(ra,l1),e(l1,zz),p(t,w2,c),p(t,Uo,c),e(Uo,Fz),e(Uo,ip),e(ip,Lz),e(Uo,qz),p(t,$2,c),p(t,aa,c),e(aa,Ro),e(Ro,d1),u(cd,d1,null),e(aa,Uz),e(aa,c1),e(c1,Rz),p(t,T2,c),p(t,Mo,c),e(Mo,Mz),e(Mo,lp),e(lp,jz),e(Mo,Hz),p(t,k2,c),p(t,oa,c),e(oa,jo),e(jo,p1),u(pd,p1,null),e(oa,Gz),e(oa,m1),e(m1,Wz),p(t,D2,c),p(t,Ho,c),e(Ho,Bz),e(Ho,dp),e(dp,Vz),e(Ho,Yz),p(t,P2,c),p(t,na,c),e(na,Go),e(Go,h1),u(md,h1,null),e(na,Zz),e(na,f1),e(f1,Jz),p(t,A2,c),p(t,Wo,c),e(Wo,Qz),e(Wo,cp),e(cp,Kz),e(Wo,Xz),p(t,O2,c),p(t,sa,c),e(sa,Bo),e(Bo,u1),u(hd,u1,null),e(sa,eF),e(sa,_1),e(_1,tF),p(t,x2,c),p(t,Vo,c),e(Vo,rF),e(Vo,pp),e(pp,aF),e(Vo,oF),p(t,S2,c),p(t,ia,c),e(ia,Yo),e(Yo,v1),u(fd,v1,null),e(ia,nF),e(ia,g1),e(g1,sF),p(t,C2,c),p(t,Zo,c),e(Zo,iF),e(Zo,mp),e(mp,lF),e(Zo,dF),p(t,N2,c),p(t,la,c),e(la,Jo),e(Jo,b1),u(ud,b1,null),e(la,cF),e(la,y1),e(y1,pF),p(t,I2,c),p(t,Qo,c),e(Qo,mF),e(Qo,hp),e(hp,hF),e(Qo,fF),p(t,z2,c),p(t,da,c),e(da,Ko),e(Ko,E1),u(_d,E1,null),e(da,uF),e(da,w1),e(w1,_F),p(t,F2,c),p(t,Xo,c),e(Xo,vF),e(Xo,fp),e(fp,gF),e(Xo,bF),p(t,L2,c),p(t,ca,c),e(ca,en),e(en,$1),u(vd,$1,null),e(ca,yF),e(ca,T1),e(T1,EF),p(t,q2,c),p(t,tn,c),e(tn,wF),e(tn,up),e(up,$F),e(tn,TF),p(t,U2,c),p(t,pa,c),e(pa,rn),e(rn,k1),u(gd,k1,null),e(pa,kF),e(pa,D1),e(D1,DF),p(t,R2,c),p(t,an,c),e(an,PF),e(an,_p),e(_p,AF),e(an,OF),p(t,M2,c),p(t,ma,c),e(ma,on),e(on,P1),u(bd,P1,null),e(ma,xF),e(ma,A1),e(A1,SF),p(t,j2,c),p(t,nn,c),e(nn,CF),e(nn,vp),e(vp,NF),e(nn,IF),p(t,H2,c),p(t,ha,c),e(ha,sn),e(sn,O1),u(yd,O1,null),e(ha,zF),e(ha,x1),e(x1,FF),p(t,G2,c),p(t,fa,c),e(fa,ln),e(ln,S1),u(Ed,S1,null),e(fa,LF),e(fa,C1),e(C1,qF),p(t,W2,c),p(t,dn,c),e(dn,UF),e(dn,gp),e(gp,RF),e(dn,MF),p(t,B2,c),p(t,ua,c),e(ua,cn),e(cn,N1),u(wd,N1,null),e(ua,jF),e(ua,I1),e(I1,HF),p(t,V2,c),p(t,pn,c),e(pn,GF),e(pn,bp),e(bp,WF),e(pn,BF),p(t,Y2,c),p(t,_a,c),e(_a,mn),e(mn,z1),u($d,z1,null),e(_a,VF),e(_a,F1),e(F1,YF),p(t,Z2,c),p(t,hn,c),e(hn,ZF),e(hn,yp),e(yp,JF),e(hn,QF),p(t,J2,c),p(t,va,c),e(va,fn),e(fn,L1),u(Td,L1,null),e(va,KF),e(va,q1),e(q1,XF),p(t,Q2,c),p(t,un,c),e(un,eL),e(un,Ep),e(Ep,tL),e(un,rL),p(t,K2,c),p(t,ga,c),e(ga,_n),e(_n,U1),u(kd,U1,null),e(ga,aL),e(ga,R1),e(R1,oL),p(t,X2,c),p(t,vn,c),e(vn,nL),e(vn,wp),e(wp,sL),e(vn,iL),p(t,eE,c),p(t,ba,c),e(ba,gn),e(gn,M1),u(Dd,M1,null),e(ba,lL),e(ba,j1),e(j1,dL),p(t,tE,c),p(t,bn,c),e(bn,cL),e(bn,$p),e($p,pL),e(bn,mL),p(t,rE,c),p(t,ya,c),e(ya,yn),e(yn,H1),u(Pd,H1,null),e(ya,hL),e(ya,G1),e(G1,fL),p(t,aE,c),p(t,En,c),e(En,uL),e(En,Tp),e(Tp,_L),e(En,vL),p(t,oE,c),p(t,Ea,c),e(Ea,wn),e(wn,W1),u(Ad,W1,null),e(Ea,gL),e(Ea,B1),e(B1,bL),p(t,nE,c),p(t,$n,c),e($n,yL),e($n,kp),e(kp,EL),e($n,wL),sE=!0},p(t,[c]){const Od={};c&2&&(Od.$$scope={dirty:c,ctx:t}),Aa.$set(Od);const V1={};c&2&&(V1.$$scope={dirty:c,ctx:t}),Fa.$set(V1);const Y1={};c&2&&(Y1.$$scope={dirty:c,ctx:t}),Ma.$set(Y1);const Z1={};c&2&&(Z1.$$scope={dirty:c,ctx:t}),Ba.$set(Z1)},i(t){sE||(_(H.$$.fragment,t),_(Aa.$$.fragment,t),_(Kn.$$.fragment,t),_(Xn.$$.fragment,t),_(es.$$.fragment,t),_(ts.$$.fragment,t),_(as.$$.fragment,t),_(ns.$$.fragment,t),_(ss.$$.fragment,t),_(ls.$$.fragment,t),_(ds.$$.fragment,t),_(cs.$$.fragment,t),_(ms.$$.fragment,t),_(hs.$$.fragment,t),_(us.$$.fragment,t),_(vs.$$.fragment,t),_(bs.$$.fragment,t),_(Es.$$.fragment,t),_(Fa.$$.fragment,t),_(ws.$$.fragment,t),_(Ts.$$.fragment,t),_(ks.$$.fragment,t),_(Ps.$$.fragment,t),_(Os.$$.fragment,t),_(xs.$$.fragment,t),_(Ns.$$.fragment,t),_(Is.$$.fragment,t),_(Fs.$$.fragment,t),_(Us.$$.fragment,t),_(Ma.$$.fragment,t),_(Gs.$$.fragment,t),_(Ws.$$.fragment,t),_(Bs.$$.fragment,t),_(Vs.$$.fragment,t),_(Zs.$$.fragment,t),_(Ks.$$.fragment,t),_(ei.$$.fragment,t),_(ti.$$.fragment,t),_(ri.$$.fragment,t),_(ai.$$.fragment,t),_(oi.$$.fragment,t),_(si.$$.fragment,t),_(Ba.$$.fragment,t),_(pi.$$.fragment,t),_(mi.$$.fragment,t),_(hi.$$.fragment,t),_(ui.$$.fragment,t),_(vi.$$.fragment,t),_(bi.$$.fragment,t),_(Ei.$$.fragment,t),_($i.$$.fragment,t),_(ki.$$.fragment,t),_(xi.$$.fragment,t),_(Si.$$.fragment,t),_(Ci.$$.fragment,t),_(Ni.$$.fragment,t),_(zi.$$.fragment,t),_(qi.$$.fragment,t),_(Mi.$$.fragment,t),_(ji.$$.fragment,t),_(Hi.$$.fragment,t),_(Gi.$$.fragment,t),_(Vi.$$.fragment,t),_(Zi.$$.fragment,t),_(Ji.$$.fragment,t),_(Qi.$$.fragment,t),_(Xi.$$.fragment,t),_(el.$$.fragment,t),_(tl.$$.fragment,t),_(rl.$$.fragment,t),_(dl.$$.fragment,t),_(cl.$$.fragment,t),_(hl.$$.fragment,t),_(fl.$$.fragment,t),_(ul.$$.fragment,t),_(_l.$$.fragment,t),_(vl.$$.fragment,t),_(gl.$$.fragment,t),_(bl.$$.fragment,t),_(El.$$.fragment,t),_(kl.$$.fragment,t),_(Al.$$.fragment,t),_(Ol.$$.fragment,t),_(xl.$$.fragment,t),_(Cl.$$.fragment,t),_(Nl.$$.fragment,t),_(Il.$$.fragment,t),_(zl.$$.fragment,t),_(Fl.$$.fragment,t),_(Ll.$$.fragment,t),_(ql.$$.fragment,t),_(jl.$$.fragment,t),_(Hl.$$.fragment,t),_(Bl.$$.fragment,t),_(Vl.$$.fragment,t),_(Zl.$$.fragment,t),_(Jl.$$.fragment,t),_(Xl.$$.fragment,t),_(ad.$$.fragment,t),_(nd.$$.fragment,t),_(sd.$$.fragment,t),_(id.$$.fragment,t),_(ld.$$.fragment,t),_(dd.$$.fragment,t),_(cd.$$.fragment,t),_(pd.$$.fragment,t),_(md.$$.fragment,t),_(hd.$$.fragment,t),_(fd.$$.fragment,t),_(ud.$$.fragment,t),_(_d.$$.fragment,t),_(vd.$$.fragment,t),_(gd.$$.fragment,t),_(bd.$$.fragment,t),_(yd.$$.fragment,t),_(Ed.$$.fragment,t),_(wd.$$.fragment,t),_($d.$$.fragment,t),_(Td.$$.fragment,t),_(kd.$$.fragment,t),_(Dd.$$.fragment,t),_(Pd.$$.fragment,t),_(Ad.$$.fragment,t),sE=!0)},o(t){v(H.$$.fragment,t),v(Aa.$$.fragment,t),v(Kn.$$.fragment,t),v(Xn.$$.fragment,t),v(es.$$.fragment,t),v(ts.$$.fragment,t),v(as.$$.fragment,t),v(ns.$$.fragment,t),v(ss.$$.fragment,t),v(ls.$$.fragment,t),v(ds.$$.fragment,t),v(cs.$$.fragment,t),v(ms.$$.fragment,t),v(hs.$$.fragment,t),v(us.$$.fragment,t),v(vs.$$.fragment,t),v(bs.$$.fragment,t),v(Es.$$.fragment,t),v(Fa.$$.fragment,t),v(ws.$$.fragment,t),v(Ts.$$.fragment,t),v(ks.$$.fragment,t),v(Ps.$$.fragment,t),v(Os.$$.fragment,t),v(xs.$$.fragment,t),v(Ns.$$.fragment,t),v(Is.$$.fragment,t),v(Fs.$$.fragment,t),v(Us.$$.fragment,t),v(Ma.$$.fragment,t),v(Gs.$$.fragment,t),v(Ws.$$.fragment,t),v(Bs.$$.fragment,t),v(Vs.$$.fragment,t),v(Zs.$$.fragment,t),v(Ks.$$.fragment,t),v(ei.$$.fragment,t),v(ti.$$.fragment,t),v(ri.$$.fragment,t),v(ai.$$.fragment,t),v(oi.$$.fragment,t),v(si.$$.fragment,t),v(Ba.$$.fragment,t),v(pi.$$.fragment,t),v(mi.$$.fragment,t),v(hi.$$.fragment,t),v(ui.$$.fragment,t),v(vi.$$.fragment,t),v(bi.$$.fragment,t),v(Ei.$$.fragment,t),v($i.$$.fragment,t),v(ki.$$.fragment,t),v(xi.$$.fragment,t),v(Si.$$.fragment,t),v(Ci.$$.fragment,t),v(Ni.$$.fragment,t),v(zi.$$.fragment,t),v(qi.$$.fragment,t),v(Mi.$$.fragment,t),v(ji.$$.fragment,t),v(Hi.$$.fragment,t),v(Gi.$$.fragment,t),v(Vi.$$.fragment,t),v(Zi.$$.fragment,t),v(Ji.$$.fragment,t),v(Qi.$$.fragment,t),v(Xi.$$.fragment,t),v(el.$$.fragment,t),v(tl.$$.fragment,t),v(rl.$$.fragment,t),v(dl.$$.fragment,t),v(cl.$$.fragment,t),v(hl.$$.fragment,t),v(fl.$$.fragment,t),v(ul.$$.fragment,t),v(_l.$$.fragment,t),v(vl.$$.fragment,t),v(gl.$$.fragment,t),v(bl.$$.fragment,t),v(El.$$.fragment,t),v(kl.$$.fragment,t),v(Al.$$.fragment,t),v(Ol.$$.fragment,t),v(xl.$$.fragment,t),v(Cl.$$.fragment,t),v(Nl.$$.fragment,t),v(Il.$$.fragment,t),v(zl.$$.fragment,t),v(Fl.$$.fragment,t),v(Ll.$$.fragment,t),v(ql.$$.fragment,t),v(jl.$$.fragment,t),v(Hl.$$.fragment,t),v(Bl.$$.fragment,t),v(Vl.$$.fragment,t),v(Zl.$$.fragment,t),v(Jl.$$.fragment,t),v(Xl.$$.fragment,t),v(ad.$$.fragment,t),v(nd.$$.fragment,t),v(sd.$$.fragment,t),v(id.$$.fragment,t),v(ld.$$.fragment,t),v(dd.$$.fragment,t),v(cd.$$.fragment,t),v(pd.$$.fragment,t),v(md.$$.fragment,t),v(hd.$$.fragment,t),v(fd.$$.fragment,t),v(ud.$$.fragment,t),v(_d.$$.fragment,t),v(vd.$$.fragment,t),v(gd.$$.fragment,t),v(bd.$$.fragment,t),v(yd.$$.fragment,t),v(Ed.$$.fragment,t),v(wd.$$.fragment,t),v($d.$$.fragment,t),v(Td.$$.fragment,t),v(kd.$$.fragment,t),v(Dd.$$.fragment,t),v(Pd.$$.fragment,t),v(Ad.$$.fragment,t),sE=!1},d(t){r(w),t&&r(I),t&&r(k),g(H),t&&r(re),t&&r(R),t&&r(ye),t&&r(q),t&&r(rt),t&&r(oe),t&&r(Qn),t&&r(mt),t&&r(rb),t&&r(P),t&&r(ab),g(Aa,t),t&&r(ob),t&&r(Oa),t&&r(nb),g(Kn,t),t&&r(sb),t&&r(ft),t&&r(ib),t&&r(pr),g(Xn),t&&r(lb),t&&r(b),g(es),g(ts),g(as),g(ns),g(ss),g(ls),g(ds),g(cs),g(ms),g(hs),g(us),g(vs),g(bs),g(Es),g(Fa),g(ws),g(Ts),g(ks),g(Ps),g(Os),g(xs),g(Ns),g(Is),g(Fs),g(Us),g(Ma),g(Gs),g(Ws),g(Bs),g(Vs),g(Zs),g(Ks),g(ei),g(ti),g(ri),t&&r(db),t&&r(Er),g(ai),t&&r(cb),t&&r(wr),g(oi),g(si),g(Ba),t&&r(pb),t&&r(Dr),g(pi),t&&r(mb),t&&r(T),g(mi),g(hi),g(ui),g(vi),g(bi),g(Ei),g($i),g(ki),g(xi),g(Si),g(Ci),g(Ni),g(zi),g(qi),g(Mi),g(ji),t&&r(hb),t&&r(Sr),g(Hi),t&&r(fb),t&&r(N),g(Gi),g(Vi),g(Zi),g(Ji),g(Qi),g(Xi),g(el),t&&r(ub),t&&r(Fr),g(tl),t&&r(_b),t&&r(se),g(rl),t&&r(vb),t&&r(qr),g(dl),t&&r(gb),t&&r(it),g(cl),t&&r(bb),t&&r(Rr),g(hl),t&&r(yb),t&&r(he),t&&r(Eb),t&&r(oo),t&&r(wb),t&&r(no),t&&r($b),t&&r(Me),t&&r(Tb),t&&r(so),t&&r(kb),t&&r(Mr),g(fl),t&&r(Db),t&&r(je),t&&r(Pb),t&&r(jt),t&&r(Ab),t&&r(co),t&&r(Ob),t&&r(He),t&&r(xb),t&&r(te),t&&r(Sb),t&&r(kc),t&&r(Cb),g(ul,t),t&&r(Nb),t&&r(Dc),t&&r(Ib),g(_l,t),t&&r(zb),t&&r(Pc),t&&r(Fb),g(vl,t),t&&r(Lb),t&&r(Ac),t&&r(qb),t&&r(Oc),t&&r(Ub),g(gl,t),t&&r(Rb),t&&r(po),t&&r(Mb),t&&r(jr),g(bl),t&&r(jb),t&&r(fe),t&&r(Hb),t&&r(Ht),t&&r(Gb),t&&r(Hr),g(El),t&&r(Wb),t&&r(fo),t&&r(Bb),t&&r(Ge),t&&r(Vb),t&&r(Cc),t&&r(Yb),t&&r(Nc),t&&r(Zb),t&&r(Gr),g(kl),t&&r(Jb),t&&r(Ic),t&&r(Qb),t&&r(Gt),t&&r(Kb),t&&r(zc),t&&r(Xb),g(Al,t),t&&r(ey),t&&r(Fc),t&&r(ty),t&&r(Wt),t&&r(ry),t&&r(Wr),g(Ol),t&&r(ay),t&&r(Lc),t&&r(oy),t&&r(We),t&&r(ny),t&&r(Bt),t&&r(sy),g(xl,t),t&&r(iy),t&&r(vo),t&&r(ly),t&&r(Br),g(Cl),t&&r(dy),t&&r(qc),t&&r(cy),g(Nl,t),t&&r(py),t&&r(Vt),t&&r(my),t&&r(Uc),t&&r(hy),g(Il,t),t&&r(fy),t&&r(Rc),t&&r(uy),t&&r(bo),t&&r(_y),t&&r(lt),t&&r(vy),t&&r(Mc),t&&r(gy),g(zl,t),t&&r(by),t&&r(jc),t&&r(yy),t&&r(Be),t&&r(Ey),t&&r(Vr),g(Fl),t&&r(wy),t&&r(Yt),t&&r($y),t&&r(Hc),t&&r(Ty),t&&r(Gc),t&&r(ky),t&&r(Eo),t&&r(Dy),g(Ll,t),t&&r(Py),t&&r(B),t&&r(Ay),t&&r(Wc),t&&r(Oy),t&&r(Yr),g(ql),t&&r(xy),t&&r(Ve),t&&r(Sy),t&&r(Ye),t&&r(Cy),t&&r(Vc),t&&r(Ny),t&&r(Ml),t&&r(Iy),t&&r(Yc),t&&r(zy),g(jl,t),t&&r(Fy),t&&r(Zt),t&&r(Ly),g(Hl,t),t&&r(qy),t&&r($o),t&&r(Uy),t&&r(To),t&&r(Ry),t&&r(ko),t&&r(My),t&&r(Jc),t&&r(jy),t&&r(Wl),t&&r(Hy),g(Bl,t),t&&r(Gy),t&&r(Qc),t&&r(Wy),g(Vl,t),t&&r(By),t&&r(Yl),t&&r(Vy),g(Zl,t),t&&r(Yy),t&&r(Kc),t&&r(Zy),g(Jl,t),t&&r(Jy),t&&r(Xc),t&&r(Qy),t&&r(Do),t&&r(Ky),t&&r(Kl),t&&r(Xy),t&&r(Jt),t&&r(e2),t&&r(Po),t&&r(t2),g(Xl,t),t&&r(r2),t&&r(ep),t&&r(a2),t&&r(Ze),t&&r(o2),t&&r(rd),t&&r(n2),t&&r(Ao),t&&r(s2),g(ad,t),t&&r(i2),t&&r(Zr),t&&r(l2),t&&r(Qt),t&&r(d2),t&&r(tp),t&&r(c2),t&&r(ue),t&&r(p2),t&&r(rp),t&&r(m2),t&&r(Oo),t&&r(h2),t&&r(Kr),g(nd),t&&r(f2),t&&r(So),t&&r(u2),t&&r(Xr),g(sd),t&&r(_2),t&&r(No),t&&r(v2),t&&r(ea),g(id),t&&r(g2),t&&r(zo),t&&r(b2),t&&r(ta),g(ld),t&&r(y2),t&&r(Lo),t&&r(E2),t&&r(ra),g(dd),t&&r(w2),t&&r(Uo),t&&r($2),t&&r(aa),g(cd),t&&r(T2),t&&r(Mo),t&&r(k2),t&&r(oa),g(pd),t&&r(D2),t&&r(Ho),t&&r(P2),t&&r(na),g(md),t&&r(A2),t&&r(Wo),t&&r(O2),t&&r(sa),g(hd),t&&r(x2),t&&r(Vo),t&&r(S2),t&&r(ia),g(fd),t&&r(C2),t&&r(Zo),t&&r(N2),t&&r(la),g(ud),t&&r(I2),t&&r(Qo),t&&r(z2),t&&r(da),g(_d),t&&r(F2),t&&r(Xo),t&&r(L2),t&&r(ca),g(vd),t&&r(q2),t&&r(tn),t&&r(U2),t&&r(pa),g(gd),t&&r(R2),t&&r(an),t&&r(M2),t&&r(ma),g(bd),t&&r(j2),t&&r(nn),t&&r(H2),t&&r(ha),g(yd),t&&r(G2),t&&r(fa),g(Ed),t&&r(W2),t&&r(dn),t&&r(B2),t&&r(ua),g(wd),t&&r(V2),t&&r(pn),t&&r(Y2),t&&r(_a),g($d),t&&r(Z2),t&&r(hn),t&&r(J2),t&&r(va),g(Td),t&&r(Q2),t&&r(un),t&&r(K2),t&&r(ga),g(kd),t&&r(X2),t&&r(vn),t&&r(eE),t&&r(ba),g(Dd),t&&r(tE),t&&r(bn),t&&r(rE),t&&r(ya),g(Pd),t&&r(aE),t&&r(En),t&&r(oE),t&&r(Ea),g(Ad),t&&r(nE),t&&r($n)}}}const HV={local:"trainer",sections:[{local:"transformers.Trainer",title:"Trainer"},{local:"transformers.Seq2SeqTrainer",title:"Seq2SeqTrainer"},{local:"transformers.TFTrainer",title:"TFTrainer"},{local:"transformers.TrainingArguments",title:"TrainingArguments"},{local:"transformers.Seq2SeqTrainingArguments",title:"Seq2SeqTrainingArguments"},{local:"transformers.TFTrainingArguments",title:"TFTrainingArguments"},{local:"checkpoints",title:"Checkpoints"},{local:"logging",title:"Logging"},{local:"randomness",title:"Randomness"},{local:"trainer-integrations",sections:[{local:"cuda-extension-installation-notes",sections:[{local:"possible-problem-1",title:"Possible problem #1"},{local:"possible-problem-2",title:"Possible problem #2"},{local:"possible-problem-3",title:"Possible problem #3"}],title:"CUDA Extension Installation Notes"},{local:"fairscale",title:"FairScale"},{local:"deepspeed",sections:[{local:"installation",title:"Installation"},{local:"deployment-with-multiple-gpus",title:"Deployment with multiple GPUs"},{local:"deployment-with-one-gpu",title:"Deployment with one GPU"},{local:"deployment-in-notebooks",title:"Deployment in Notebooks"},{local:"configuration",title:"Configuration"},{local:"passing-configuration",title:"Passing Configuration"},{local:"shared-configuration",title:"Shared Configuration"},{local:"zero",sections:[{local:"zero2-config",title:"ZeRO-2 Config"},{local:"zero3-config",title:"ZeRO-3 Config"}],title:"ZeRO"},{local:"nvme-support",sections:[{local:"zero2-vs-zero3-performance",title:"ZeRO-2 vs ZeRO-3 Performance"},{local:"zero2-example",title:"ZeRO-2 Example"},{local:"zero3-example",title:"ZeRO-3 Example"}],title:"NVMe Support"},{local:"optimizer-and-scheduler",sections:[{local:"optimizer",title:"Optimizer"},{local:"scheduler",title:"Scheduler"}],title:"Optimizer and Scheduler"},{local:"fp32-precision",title:"fp32 Precision"},{local:"automatic-mixed-precision",title:"Automatic Mixed Precision"},{local:"batch-size",title:"Batch Size"},{local:"gradient-accumulation",title:"Gradient Accumulation"},{local:"gradient-clipping",title:"Gradient Clipping"},{local:"getting-the-model-weights-out",title:"Getting The Model Weights Out"}],title:"DeepSpeed"}],title:"Trainer Integrations"}],title:"Trainer"};function GV(et,w,I){let{fw:k}=w;return et.$$set=A=>{"fw"in A&&I(0,k=A.fw)},[k]}class QV extends IV{constructor(w){super();zV(this,w,GV,jV,FV,{fw:0})}}export{QV as default,HV as metadata};
