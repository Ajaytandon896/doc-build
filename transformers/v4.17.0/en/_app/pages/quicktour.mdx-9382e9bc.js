import{S as eu,i as tu,s as su,e as n,k as f,w as d,t as a,M as au,c as l,d as s,m as u,a as i,x as _,h as o,b as h,F as t,g as p,y as g,q as v,o as $,B as y}from"../chunks/vendor-4833417e.js";import{T as Ea}from"../chunks/Tip-fffd6df1.js";import{Y as Xf}from"../chunks/Youtube-27813aed.js";import{I as Oe}from"../chunks/IconCopyLink-4b81c553.js";import{C as R}from"../chunks/CodeBlock-6a3d1b46.js";import{C as fe}from"../chunks/CodeBlockFw-27a176a0.js";import{D as ou}from"../chunks/DocNotebookDropdown-ecff2a90.js";import"../chunks/CopyButton-dacfbfaf.js";function ru(N){let m,k;return{c(){m=n("p"),k=a(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(c){m=l(c,"P",{});var w=i(m);k=o(w,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),w.forEach(s)},m(c,w){p(c,m,w),t(m,k)},d(c){c&&s(m)}}}function nu(N){let m,k,c,w,A,b,E,x;return{c(){m=n("p"),k=a("For more details about the "),c=n("a"),w=a("pipeline()"),A=a(" and associated tasks, refer to the documentation "),b=n("a"),E=a("here"),x=a("."),this.h()},l(P){m=l(P,"P",{});var j=i(m);k=o(j,"For more details about the "),c=l(j,"A",{href:!0});var O=i(c);w=o(O,"pipeline()"),O.forEach(s),A=o(j," and associated tasks, refer to the documentation "),b=l(j,"A",{href:!0});var H=i(b);E=o(H,"here"),H.forEach(s),x=o(j,"."),j.forEach(s),this.h()},h(){h(c,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(b,"href","./main_classes/pipelines")},m(P,j){p(P,m,j),t(m,k),t(m,c),t(c,w),t(m,A),t(m,b),t(b,E),t(m,x)},d(P){P&&s(m)}}}function lu(N){let m,k,c,w,A,b,E,x;return{c(){m=n("p"),k=a("See the "),c=n("a"),w=a("task summary"),A=a(" for which "),b=n("a"),E=a("AutoModel"),x=a(" class to use for which task."),this.h()},l(P){m=l(P,"P",{});var j=i(m);k=o(j,"See the "),c=l(j,"A",{href:!0});var O=i(c);w=o(O,"task summary"),O.forEach(s),A=o(j," for which "),b=l(j,"A",{href:!0});var H=i(b);E=o(H,"AutoModel"),H.forEach(s),x=o(j," class to use for which task."),j.forEach(s),this.h()},h(){h(c,"href","./task_summary"),h(b,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModel")},m(P,j){p(P,m,j),t(m,k),t(m,c),t(c,w),t(m,A),t(m,b),t(b,E),t(m,x)},d(P){P&&s(m)}}}function iu(N){let m,k,c,w,A;return{c(){m=n("p"),k=a("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=n("em"),w=a("before"),A=a(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(b){m=l(b,"P",{});var E=i(m);k=o(E,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=l(E,"EM",{});var x=i(c);w=o(x,"before"),x.forEach(s),A=o(E,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),E.forEach(s)},m(b,E){p(b,m,E),t(m,k),t(m,c),t(c,w),t(m,A)},d(b){b&&s(m)}}}function pu(N){let m,k,c,w,A;return{c(){m=n("p"),k=a(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=n("code"),w=a("None"),A=a(" are ignored.")},l(b){m=l(b,"P",{});var E=i(m);k=o(E,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=l(E,"CODE",{});var x=i(c);w=o(x,"None"),x.forEach(s),A=o(E," are ignored."),E.forEach(s)},m(b,E){p(b,m,E),t(m,k),t(m,c),t(c,w),t(m,A)},d(b){b&&s(m)}}}function fu(N){let m,k,c,w,A,b,E,x,P,j,O,H,U,mr,xt,cr,dr,qt,_r,gr,ja,ue,Aa,se,he,xs,De,vr,qs,$r,Ta,Le,zt,yr,br,xa,Re,qa,me,wr,Ft,kr,Er,za,He,zs,jr,Ar,Fa,T,Fs,Tr,xr,Ss,qr,zr,Ps,Fr,Sr,Ms,Pr,Mr,Cs,Cr,Ir,Is,Nr,Or,Ns,Dr,Lr,Os,Rr,Sa,Ue,Ds,Hr,Ur,Pa,W,Ls,Wr,Br,Rs,Yr,Gr,Hs,Qr,Ma,We,Us,Jr,Kr,Ca,ce,Ws,Vr,Zr,Bs,Xr,Ia,de,Na,ae,_e,Ys,Be,en,Gs,tn,Oa,ge,sn,St,an,on,Da,Pt,rn,La,Ye,Ra,ve,nn,Mt,ln,pn,Ha,Ge,Ua,B,fn,Qe,un,hn,Qs,mn,cn,Wa,Je,Ba,$e,dn,Ct,_n,gn,Ya,Ke,Ga,Y,vn,It,$n,yn,Ve,bn,wn,Qa,Ze,Ja,D,kn,Nt,En,jn,Js,An,Tn,Ks,xn,qn,Ka,Xe,Va,G,zn,et,Fn,Sn,tt,Pn,Mn,Za,st,Xa,Ot,Cn,eo,at,to,ye,In,Dt,Nn,On,so,oe,be,Vs,ot,Dn,Zs,Ln,ao,M,Rn,Lt,Hn,Un,rt,Wn,Bn,Rt,Yn,Gn,nt,Qn,Jn,oo,lt,ro,Q,Kn,Ht,Vn,Zn,Xs,Xn,el,no,it,lo,J,tl,Ut,sl,al,ea,ol,rl,io,pt,po,K,nl,Wt,ll,il,Bt,pl,fl,fo,re,we,ta,ft,ul,sa,hl,uo,ut,ho,q,ml,Yt,cl,dl,Gt,_l,gl,Qt,vl,$l,Jt,yl,bl,aa,wl,kl,Kt,El,jl,mo,V,Al,oa,Tl,xl,Vt,ql,zl,co,ne,ke,ra,ht,Fl,na,Sl,_o,Z,Pl,la,Ml,Cl,Zt,Il,Nl,go,Ee,Ol,Xt,Dl,Ll,vo,mt,$o,je,Rl,ia,Hl,Ul,yo,es,Wl,bo,ct,wo,ts,Bl,ko,Ae,ss,as,Yl,Gl,Ql,os,rs,Jl,Kl,Eo,Te,Vl,ns,Zl,Xl,jo,dt,Ao,xe,ei,ls,ti,si,To,le,qe,pa,_t,ai,fa,oi,xo,F,ri,is,ni,li,ps,ii,pi,fs,fi,ui,us,hi,mi,hs,ci,di,qo,gt,zo,ze,Fo,Fe,_i,ua,gi,vi,So,vt,Po,X,$i,ha,yi,bi,ma,wi,ki,Mo,$t,Co,Se,Io,z,Ei,yt,ca,ji,Ai,bt,da,Ti,xi,ms,qi,zi,_a,Fi,Si,wt,Pi,Mi,cs,Ci,Ii,No,Pe,Oo,ie,Me,ga,kt,Ni,va,Oi,Do,Ce,Di,ds,Li,Ri,Lo,Et,Ro,Ie,Hi,_s,Ui,Wi,Ho,jt,Uo,ee,Bi,$a,Yi,Gi,ya,Qi,Ji,Wo,At,Bo;return b=new Oe({}),O=new ou({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"}]}}),ue=new Ea({props:{$$slots:{default:[ru]},$$scope:{ctx:N}}}),De=new Oe({}),Re=new Xf({props:{id:"tiZFewofSLM"}}),de=new Ea({props:{$$slots:{default:[nu]},$$scope:{ctx:N}}}),Be=new Oe({}),Ye=new fe({props:{group1:{id:"pt",code:"pip install torch",highlighted:"pip install torch"},group2:{id:"tf",code:"pip install tensorflow",highlighted:"pip install tensorflow"}}}),Ge=new R({props:{codee:`from transformers import pipeline

classifier = pipeline("sentiment-analysis"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),Je=new R({props:{codee:'classifier("We are very happy to show you the \u{1F917} Transformers library."),',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),Ke=new R({props:{codee:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),Ze=new R({props:{codee:"pip install datasets ,",highlighted:"pip install datasets "}}),Xe=new R({props:{codee:`from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h", device=0),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, device=<span class="hljs-number">0</span>)`}}),st=new R({props:{codee:`import datasets

dataset = datasets.load_dataset("superb", name="asr", split="test"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = datasets.load_dataset(<span class="hljs-string">&quot;superb&quot;</span>, name=<span class="hljs-string">&quot;asr&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)`}}),at=new R({props:{codee:`files = dataset["file"]
speech_recognizer(files[:4]),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dataset[<span class="hljs-string">&quot;file&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer(files[:<span class="hljs-number">4</span>])
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FAT AND SAUCE&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;STUFFERED INTO YOU HIS BELLY COUNSELLED HIM&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HO BERTIE ANY GOOD IN YOUR MIND&#x27;</span>}]`}}),ot=new Oe({}),lt=new R({props:{codee:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment",',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),it=new fe({props:{group1:{id:"pt",code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`},group2:{id:"tf",code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}}),pt=new R({props:{codee:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers."),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),ft=new Oe({}),ut=new Xf({props:{id:"AhChOFRegn4"}}),ht=new Oe({}),mt=new R({props:{codee:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),ct=new R({props:{codee:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),dt=new fe({props:{group1:{id:"pt",code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`},group2:{id:"tf",code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}}),_t=new Oe({}),gt=new fe({props:{group1:{id:"pt",code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`},group2:{id:"tf",code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}}),ze=new Ea({props:{$$slots:{default:[lu]},$$scope:{ctx:N}}}),vt=new fe({props:{group1:{id:"pt",code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'},group2:{id:"tf",code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}}),$t=new fe({props:{group1:{id:"pt",code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`},group2:{id:"tf",code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
print(tf_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf_predictions)
tf.Tensor(
[[<span class="hljs-number">0.00206</span> <span class="hljs-number">0.00177</span> <span class="hljs-number">0.01155</span> <span class="hljs-number">0.21209</span> <span class="hljs-number">0.77253</span>]
 [<span class="hljs-number">0.20842</span> <span class="hljs-number">0.18262</span> <span class="hljs-number">0.19693</span> <span class="hljs-number">0.1755</span>  <span class="hljs-number">0.23652</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), dtype=float32)`}}}),Se=new Ea({props:{$$slots:{default:[iu]},$$scope:{ctx:N}}}),Pe=new Ea({props:{$$slots:{default:[pu]},$$scope:{ctx:N}}}),kt=new Oe({}),Et=new fe({props:{group1:{id:"pt",code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`},group2:{id:"tf",code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}}),jt=new fe({props:{group1:{id:"pt",code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'},group2:{id:"tf",code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}}),At=new fe({props:{group1:{id:"pt",code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`},group2:{id:"tf",code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}}),{c(){m=n("meta"),k=f(),c=n("h1"),w=n("a"),A=n("span"),d(b.$$.fragment),E=f(),x=n("span"),P=a("Quick tour"),j=f(),d(O.$$.fragment),H=f(),U=n("p"),mr=a("Get up and running with \u{1F917} Transformers! Start using the "),xt=n("a"),cr=a("pipeline()"),dr=a(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),qt=n("a"),_r=a("AutoClass"),gr=a(" to solve your text, vision or audio task."),ja=f(),d(ue.$$.fragment),Aa=f(),se=n("h2"),he=n("a"),xs=n("span"),d(De.$$.fragment),vr=f(),qs=n("span"),$r=a("Pipeline"),Ta=f(),Le=n("p"),zt=n("a"),yr=a("pipeline()"),br=a(" is the easiest way to use a pretrained model for a given task."),xa=f(),d(Re.$$.fragment),qa=f(),me=n("p"),wr=a("The "),Ft=n("a"),kr=a("pipeline()"),Er=a(" supports many common tasks out-of-the-box:"),za=f(),He=n("p"),zs=n("strong"),jr=a("Text"),Ar=a(":"),Fa=f(),T=n("ul"),Fs=n("li"),Tr=a("Sentiment analysis: classify the polarity of a given text."),xr=f(),Ss=n("li"),qr=a("Text generation (in English): generate text from a given input."),zr=f(),Ps=n("li"),Fr=a("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),Sr=f(),Ms=n("li"),Pr=a("Question answering: extract the answer from the context, given some context and a question."),Mr=f(),Cs=n("li"),Cr=a("Fill-mask: fill in the blank given a text with masked words."),Ir=f(),Is=n("li"),Nr=a("Summarization: generate a summary of a long sequence of text or document."),Or=f(),Ns=n("li"),Dr=a("Translation: translate text into another language."),Lr=f(),Os=n("li"),Rr=a("Feature extraction: create a tensor representation of the text."),Sa=f(),Ue=n("p"),Ds=n("strong"),Hr=a("Image"),Ur=a(":"),Pa=f(),W=n("ul"),Ls=n("li"),Wr=a("Image classification: classify an image."),Br=f(),Rs=n("li"),Yr=a("Image segmentation: classify every pixel in an image."),Gr=f(),Hs=n("li"),Qr=a("Object detection: detect objects within an image."),Ma=f(),We=n("p"),Us=n("strong"),Jr=a("Audio"),Kr=a(":"),Ca=f(),ce=n("ul"),Ws=n("li"),Vr=a("Audio classification: assign a label to a given segment of audio."),Zr=f(),Bs=n("li"),Xr=a("Automatic speech recognition (ASR): transcribe audio data into text."),Ia=f(),d(de.$$.fragment),Na=f(),ae=n("h3"),_e=n("a"),Ys=n("span"),d(Be.$$.fragment),en=f(),Gs=n("span"),tn=a("Pipeline usage"),Oa=f(),ge=n("p"),sn=a("In the following example, you will use the "),St=n("a"),an=a("pipeline()"),on=a(" for sentiment analysis."),Da=f(),Pt=n("p"),rn=a("Install the following dependencies if you haven\u2019t already:"),La=f(),d(Ye.$$.fragment),Ra=f(),ve=n("p"),nn=a("Import "),Mt=n("a"),ln=a("pipeline()"),pn=a(" and specify the task you want to complete:"),Ha=f(),d(Ge.$$.fragment),Ua=f(),B=n("p"),fn=a("The pipeline downloads and caches a default "),Qe=n("a"),un=a("pretrained model"),hn=a(" and tokenizer for sentiment analysis. Now you can use the "),Qs=n("code"),mn=a("classifier"),cn=a(" on your target text:"),Wa=f(),d(Je.$$.fragment),Ba=f(),$e=n("p"),dn=a("For more than one sentence, pass a list of sentences to the "),Ct=n("a"),_n=a("pipeline()"),gn=a(" which returns a list of dictionaries:"),Ya=f(),d(Ke.$$.fragment),Ga=f(),Y=n("p"),vn=a("The "),It=n("a"),$n=a("pipeline()"),yn=a(" can also iterate over an entire dataset. Start by installing the "),Ve=n("a"),bn=a("\u{1F917} Datasets"),wn=a(" library:"),Qa=f(),d(Ze.$$.fragment),Ja=f(),D=n("p"),kn=a("Create a "),Nt=n("a"),En=a("pipeline()"),jn=a(" with the task you want to solve for and the model you want to use. Set the "),Js=n("code"),An=a("device"),Tn=a(" parameter to "),Ks=n("code"),xn=a("0"),qn=a(" to place the tensors on a CUDA device:"),Ka=f(),d(Xe.$$.fragment),Va=f(),G=n("p"),zn=a("Next, load a dataset (see the \u{1F917} Datasets "),et=n("a"),Fn=a("Quick Start"),Sn=a(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),tt=n("a"),Pn=a("SUPERB"),Mn=a(" dataset:"),Za=f(),d(st.$$.fragment),Xa=f(),Ot=n("p"),Cn=a("You can pass a whole dataset pipeline:"),eo=f(),d(at.$$.fragment),to=f(),ye=n("p"),In=a("For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Dt=n("a"),Nn=a("pipeline documentation"),On=a(" for more information."),so=f(),oe=n("h3"),be=n("a"),Vs=n("span"),d(ot.$$.fragment),Dn=f(),Zs=n("span"),Ln=a("Use another model and tokenizer in the pipeline"),ao=f(),M=n("p"),Rn=a("The "),Lt=n("a"),Hn=a("pipeline()"),Un=a(" can accommodate any model from the "),rt=n("a"),Wn=a("Model Hub"),Bn=a(", making it easy to adapt the "),Rt=n("a"),Yn=a("pipeline()"),Gn=a(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),nt=n("a"),Qn=a("BERT model"),Jn=a(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),oo=f(),d(lt.$$.fragment),ro=f(),Q=n("p"),Kn=a("Use the "),Ht=n("a"),Vn=a("AutoModelForSequenceClassification"),Zn=a(" and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Xs=n("code"),Xn=a("AutoClass"),el=a(" below):"),no=f(),d(it.$$.fragment),lo=f(),J=n("p"),tl=a("Then you can specify the model and tokenizer in the "),Ut=n("a"),sl=a("pipeline()"),al=a(", and apply the "),ea=n("code"),ol=a("classifier"),rl=a(" on your target text:"),io=f(),d(pt.$$.fragment),po=f(),K=n("p"),nl=a("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Wt=n("a"),ll=a("fine-tuning tutorial"),il=a(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Bt=n("a"),pl=a("here"),fl=a(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),fo=f(),re=n("h2"),we=n("a"),ta=n("span"),d(ft.$$.fragment),ul=f(),sa=n("span"),hl=a("AutoClass"),uo=f(),d(ut.$$.fragment),ho=f(),q=n("p"),ml=a("Under the hood, the "),Yt=n("a"),cl=a("AutoModelForSequenceClassification"),dl=a(" and "),Gt=n("a"),_l=a("AutoTokenizer"),gl=a(" classes work together to power the "),Qt=n("a"),vl=a("pipeline()"),$l=a(". An "),Jt=n("a"),yl=a("AutoClass"),bl=a(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),aa=n("code"),wl=a("AutoClass"),kl=a(" for your task and it\u2019s associated tokenizer with "),Kt=n("a"),El=a("AutoTokenizer"),jl=a("."),mo=f(),V=n("p"),Al=a("Let\u2019s return to our example and see how you can use the "),oa=n("code"),Tl=a("AutoClass"),xl=a(" to replicate the results of the "),Vt=n("a"),ql=a("pipeline()"),zl=a("."),co=f(),ne=n("h3"),ke=n("a"),ra=n("span"),d(ht.$$.fragment),Fl=f(),na=n("span"),Sl=a("AutoTokenizer"),_o=f(),Z=n("p"),Pl=a("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=n("em"),Ml=a("tokens"),Cl=a(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Zt=n("a"),Il=a("here"),Nl=a("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),go=f(),Ee=n("p"),Ol=a("Load a tokenizer with "),Xt=n("a"),Dl=a("AutoTokenizer"),Ll=a(":"),vo=f(),d(mt.$$.fragment),$o=f(),je=n("p"),Rl=a("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ia=n("em"),Hl=a("vocabulary"),Ul=a("."),yo=f(),es=n("p"),Wl=a("Pass your text to the tokenizer:"),bo=f(),d(ct.$$.fragment),wo=f(),ts=n("p"),Bl=a("The tokenizer will return a dictionary containing:"),ko=f(),Ae=n("ul"),ss=n("li"),as=n("a"),Yl=a("input_ids"),Gl=a(": numerical representions of your tokens."),Ql=f(),os=n("li"),rs=n("a"),Jl=a("atttention_mask"),Kl=a(": indicates which tokens should be attended to."),Eo=f(),Te=n("p"),Vl=a("Just like the "),ns=n("a"),Zl=a("pipeline()"),Xl=a(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),jo=f(),d(dt.$$.fragment),Ao=f(),xe=n("p"),ei=a("Read the "),ls=n("a"),ti=a("preprocessing"),si=a(" tutorial for more details about tokenization."),To=f(),le=n("h3"),qe=n("a"),pa=n("span"),d(_t.$$.fragment),ai=f(),fa=n("span"),oi=a("AutoModel"),xo=f(),F=n("p"),ri=a("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),is=n("a"),ni=a("AutoModel"),li=a(" like you would load an "),ps=n("a"),ii=a("AutoTokenizer"),pi=a(". The only difference is selecting the correct "),fs=n("a"),fi=a("AutoModel"),ui=a(" for the task. Since you are doing text - or sequence - classification, load "),us=n("a"),hi=a("AutoModelForSequenceClassification"),mi=a(". The TensorFlow equivalent is simply "),hs=n("a"),ci=a("TFAutoModelForSequenceClassification"),di=a(":"),qo=f(),d(gt.$$.fragment),zo=f(),d(ze.$$.fragment),Fo=f(),Fe=n("p"),_i=a("Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),ua=n("code"),gi=a("**"),vi=a(". For TensorFlow models, pass the dictionary keys directly to the tensors:"),So=f(),d(vt.$$.fragment),Po=f(),X=n("p"),$i=a("The model outputs the final activations in the "),ha=n("code"),yi=a("logits"),bi=a(" attribute. Apply the softmax function to the "),ma=n("code"),wi=a("logits"),ki=a(" to retrieve the probabilities:"),Mo=f(),d($t.$$.fragment),Co=f(),d(Se.$$.fragment),Io=f(),z=n("p"),Ei=a("Models are a standard "),yt=n("a"),ca=n("code"),ji=a("torch.nn.Module"),Ai=a(" or a "),bt=n("a"),da=n("code"),Ti=a("tf.keras.Model"),xi=a(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),ms=n("a"),qi=a("Trainer"),zi=a(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),_a=n("code"),Fi=a("fit"),Si=a(" method from "),wt=n("a"),Pi=a("Keras"),Mi=a(". Refer to the "),cs=n("a"),Ci=a("training tutorial"),Ii=a(" for more details."),No=f(),d(Pe.$$.fragment),Oo=f(),ie=n("h3"),Me=n("a"),ga=n("span"),d(kt.$$.fragment),Ni=f(),va=n("span"),Oi=a("Save a model"),Do=f(),Ce=n("p"),Di=a("Once your model is fine-tuned, you can save it with its tokenizer using "),ds=n("a"),Li=a("PreTrainedModel.save_pretrained()"),Ri=a(":"),Lo=f(),d(Et.$$.fragment),Ro=f(),Ie=n("p"),Hi=a("When you are ready to use the model again, reload it with "),_s=n("a"),Ui=a("PreTrainedModel.from_pretrained()"),Wi=a(":"),Ho=f(),d(jt.$$.fragment),Uo=f(),ee=n("p"),Bi=a("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),$a=n("code"),Yi=a("from_pt"),Gi=a(" or "),ya=n("code"),Qi=a("from_tf"),Ji=a(" parameter can convert the model from one framework to the other:"),Wo=f(),d(At.$$.fragment),this.h()},l(e){const r=au('[data-svelte="svelte-1phssyn"]',document.head);m=l(r,"META",{name:!0,content:!0}),r.forEach(s),k=u(e),c=l(e,"H1",{class:!0});var Tt=i(c);w=l(Tt,"A",{id:!0,class:!0,href:!0});var ba=i(w);A=l(ba,"SPAN",{});var wa=i(A);_(b.$$.fragment,wa),wa.forEach(s),ba.forEach(s),E=u(Tt),x=l(Tt,"SPAN",{});var ka=i(x);P=o(ka,"Quick tour"),ka.forEach(s),Tt.forEach(s),j=u(e),_(O.$$.fragment,e),H=u(e),U=l(e,"P",{});var pe=i(U);mr=o(pe,"Get up and running with \u{1F917} Transformers! Start using the "),xt=l(pe,"A",{href:!0});var sp=i(xt);cr=o(sp,"pipeline()"),sp.forEach(s),dr=o(pe," for rapid inference, and quickly load a pretrained model and tokenizer with an "),qt=l(pe,"A",{href:!0});var ap=i(qt);_r=o(ap,"AutoClass"),ap.forEach(s),gr=o(pe," to solve your text, vision or audio task."),pe.forEach(s),ja=u(e),_(ue.$$.fragment,e),Aa=u(e),se=l(e,"H2",{class:!0});var Yo=i(se);he=l(Yo,"A",{id:!0,class:!0,href:!0});var op=i(he);xs=l(op,"SPAN",{});var rp=i(xs);_(De.$$.fragment,rp),rp.forEach(s),op.forEach(s),vr=u(Yo),qs=l(Yo,"SPAN",{});var np=i(qs);$r=o(np,"Pipeline"),np.forEach(s),Yo.forEach(s),Ta=u(e),Le=l(e,"P",{});var Ki=i(Le);zt=l(Ki,"A",{href:!0});var lp=i(zt);yr=o(lp,"pipeline()"),lp.forEach(s),br=o(Ki," is the easiest way to use a pretrained model for a given task."),Ki.forEach(s),xa=u(e),_(Re.$$.fragment,e),qa=u(e),me=l(e,"P",{});var Go=i(me);wr=o(Go,"The "),Ft=l(Go,"A",{href:!0});var ip=i(Ft);kr=o(ip,"pipeline()"),ip.forEach(s),Er=o(Go," supports many common tasks out-of-the-box:"),Go.forEach(s),za=u(e),He=l(e,"P",{});var Vi=i(He);zs=l(Vi,"STRONG",{});var pp=i(zs);jr=o(pp,"Text"),pp.forEach(s),Ar=o(Vi,":"),Vi.forEach(s),Fa=u(e),T=l(e,"UL",{});var S=i(T);Fs=l(S,"LI",{});var fp=i(Fs);Tr=o(fp,"Sentiment analysis: classify the polarity of a given text."),fp.forEach(s),xr=u(S),Ss=l(S,"LI",{});var up=i(Ss);qr=o(up,"Text generation (in English): generate text from a given input."),up.forEach(s),zr=u(S),Ps=l(S,"LI",{});var hp=i(Ps);Fr=o(hp,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),hp.forEach(s),Sr=u(S),Ms=l(S,"LI",{});var mp=i(Ms);Pr=o(mp,"Question answering: extract the answer from the context, given some context and a question."),mp.forEach(s),Mr=u(S),Cs=l(S,"LI",{});var cp=i(Cs);Cr=o(cp,"Fill-mask: fill in the blank given a text with masked words."),cp.forEach(s),Ir=u(S),Is=l(S,"LI",{});var dp=i(Is);Nr=o(dp,"Summarization: generate a summary of a long sequence of text or document."),dp.forEach(s),Or=u(S),Ns=l(S,"LI",{});var _p=i(Ns);Dr=o(_p,"Translation: translate text into another language."),_p.forEach(s),Lr=u(S),Os=l(S,"LI",{});var gp=i(Os);Rr=o(gp,"Feature extraction: create a tensor representation of the text."),gp.forEach(s),S.forEach(s),Sa=u(e),Ue=l(e,"P",{});var Zi=i(Ue);Ds=l(Zi,"STRONG",{});var vp=i(Ds);Hr=o(vp,"Image"),vp.forEach(s),Ur=o(Zi,":"),Zi.forEach(s),Pa=u(e),W=l(e,"UL",{});var gs=i(W);Ls=l(gs,"LI",{});var $p=i(Ls);Wr=o($p,"Image classification: classify an image."),$p.forEach(s),Br=u(gs),Rs=l(gs,"LI",{});var yp=i(Rs);Yr=o(yp,"Image segmentation: classify every pixel in an image."),yp.forEach(s),Gr=u(gs),Hs=l(gs,"LI",{});var bp=i(Hs);Qr=o(bp,"Object detection: detect objects within an image."),bp.forEach(s),gs.forEach(s),Ma=u(e),We=l(e,"P",{});var Xi=i(We);Us=l(Xi,"STRONG",{});var wp=i(Us);Jr=o(wp,"Audio"),wp.forEach(s),Kr=o(Xi,":"),Xi.forEach(s),Ca=u(e),ce=l(e,"UL",{});var Qo=i(ce);Ws=l(Qo,"LI",{});var kp=i(Ws);Vr=o(kp,"Audio classification: assign a label to a given segment of audio."),kp.forEach(s),Zr=u(Qo),Bs=l(Qo,"LI",{});var Ep=i(Bs);Xr=o(Ep,"Automatic speech recognition (ASR): transcribe audio data into text."),Ep.forEach(s),Qo.forEach(s),Ia=u(e),_(de.$$.fragment,e),Na=u(e),ae=l(e,"H3",{class:!0});var Jo=i(ae);_e=l(Jo,"A",{id:!0,class:!0,href:!0});var jp=i(_e);Ys=l(jp,"SPAN",{});var Ap=i(Ys);_(Be.$$.fragment,Ap),Ap.forEach(s),jp.forEach(s),en=u(Jo),Gs=l(Jo,"SPAN",{});var Tp=i(Gs);tn=o(Tp,"Pipeline usage"),Tp.forEach(s),Jo.forEach(s),Oa=u(e),ge=l(e,"P",{});var Ko=i(ge);sn=o(Ko,"In the following example, you will use the "),St=l(Ko,"A",{href:!0});var xp=i(St);an=o(xp,"pipeline()"),xp.forEach(s),on=o(Ko," for sentiment analysis."),Ko.forEach(s),Da=u(e),Pt=l(e,"P",{});var qp=i(Pt);rn=o(qp,"Install the following dependencies if you haven\u2019t already:"),qp.forEach(s),La=u(e),_(Ye.$$.fragment,e),Ra=u(e),ve=l(e,"P",{});var Vo=i(ve);nn=o(Vo,"Import "),Mt=l(Vo,"A",{href:!0});var zp=i(Mt);ln=o(zp,"pipeline()"),zp.forEach(s),pn=o(Vo," and specify the task you want to complete:"),Vo.forEach(s),Ha=u(e),_(Ge.$$.fragment,e),Ua=u(e),B=l(e,"P",{});var vs=i(B);fn=o(vs,"The pipeline downloads and caches a default "),Qe=l(vs,"A",{href:!0,rel:!0});var Fp=i(Qe);un=o(Fp,"pretrained model"),Fp.forEach(s),hn=o(vs," and tokenizer for sentiment analysis. Now you can use the "),Qs=l(vs,"CODE",{});var Sp=i(Qs);mn=o(Sp,"classifier"),Sp.forEach(s),cn=o(vs," on your target text:"),vs.forEach(s),Wa=u(e),_(Je.$$.fragment,e),Ba=u(e),$e=l(e,"P",{});var Zo=i($e);dn=o(Zo,"For more than one sentence, pass a list of sentences to the "),Ct=l(Zo,"A",{href:!0});var Pp=i(Ct);_n=o(Pp,"pipeline()"),Pp.forEach(s),gn=o(Zo," which returns a list of dictionaries:"),Zo.forEach(s),Ya=u(e),_(Ke.$$.fragment,e),Ga=u(e),Y=l(e,"P",{});var $s=i(Y);vn=o($s,"The "),It=l($s,"A",{href:!0});var Mp=i(It);$n=o(Mp,"pipeline()"),Mp.forEach(s),yn=o($s," can also iterate over an entire dataset. Start by installing the "),Ve=l($s,"A",{href:!0,rel:!0});var Cp=i(Ve);bn=o(Cp,"\u{1F917} Datasets"),Cp.forEach(s),wn=o($s," library:"),$s.forEach(s),Qa=u(e),_(Ze.$$.fragment,e),Ja=u(e),D=l(e,"P",{});var Ne=i(D);kn=o(Ne,"Create a "),Nt=l(Ne,"A",{href:!0});var Ip=i(Nt);En=o(Ip,"pipeline()"),Ip.forEach(s),jn=o(Ne," with the task you want to solve for and the model you want to use. Set the "),Js=l(Ne,"CODE",{});var Np=i(Js);An=o(Np,"device"),Np.forEach(s),Tn=o(Ne," parameter to "),Ks=l(Ne,"CODE",{});var Op=i(Ks);xn=o(Op,"0"),Op.forEach(s),qn=o(Ne," to place the tensors on a CUDA device:"),Ne.forEach(s),Ka=u(e),_(Xe.$$.fragment,e),Va=u(e),G=l(e,"P",{});var ys=i(G);zn=o(ys,"Next, load a dataset (see the \u{1F917} Datasets "),et=l(ys,"A",{href:!0,rel:!0});var Dp=i(et);Fn=o(Dp,"Quick Start"),Dp.forEach(s),Sn=o(ys," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),tt=l(ys,"A",{href:!0,rel:!0});var Lp=i(tt);Pn=o(Lp,"SUPERB"),Lp.forEach(s),Mn=o(ys," dataset:"),ys.forEach(s),Za=u(e),_(st.$$.fragment,e),Xa=u(e),Ot=l(e,"P",{});var Rp=i(Ot);Cn=o(Rp,"You can pass a whole dataset pipeline:"),Rp.forEach(s),eo=u(e),_(at.$$.fragment,e),to=u(e),ye=l(e,"P",{});var Xo=i(ye);In=o(Xo,"For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Dt=l(Xo,"A",{href:!0});var Hp=i(Dt);Nn=o(Hp,"pipeline documentation"),Hp.forEach(s),On=o(Xo," for more information."),Xo.forEach(s),so=u(e),oe=l(e,"H3",{class:!0});var er=i(oe);be=l(er,"A",{id:!0,class:!0,href:!0});var Up=i(be);Vs=l(Up,"SPAN",{});var Wp=i(Vs);_(ot.$$.fragment,Wp),Wp.forEach(s),Up.forEach(s),Dn=u(er),Zs=l(er,"SPAN",{});var Bp=i(Zs);Ln=o(Bp,"Use another model and tokenizer in the pipeline"),Bp.forEach(s),er.forEach(s),ao=u(e),M=l(e,"P",{});var te=i(M);Rn=o(te,"The "),Lt=l(te,"A",{href:!0});var Yp=i(Lt);Hn=o(Yp,"pipeline()"),Yp.forEach(s),Un=o(te," can accommodate any model from the "),rt=l(te,"A",{href:!0,rel:!0});var Gp=i(rt);Wn=o(Gp,"Model Hub"),Gp.forEach(s),Bn=o(te,", making it easy to adapt the "),Rt=l(te,"A",{href:!0});var Qp=i(Rt);Yn=o(Qp,"pipeline()"),Qp.forEach(s),Gn=o(te," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),nt=l(te,"A",{href:!0,rel:!0});var Jp=i(nt);Qn=o(Jp,"BERT model"),Jp.forEach(s),Jn=o(te," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),te.forEach(s),oo=u(e),_(lt.$$.fragment,e),ro=u(e),Q=l(e,"P",{});var bs=i(Q);Kn=o(bs,"Use the "),Ht=l(bs,"A",{href:!0});var Kp=i(Ht);Vn=o(Kp,"AutoModelForSequenceClassification"),Kp.forEach(s),Zn=o(bs," and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Xs=l(bs,"CODE",{});var Vp=i(Xs);Xn=o(Vp,"AutoClass"),Vp.forEach(s),el=o(bs," below):"),bs.forEach(s),no=u(e),_(it.$$.fragment,e),lo=u(e),J=l(e,"P",{});var ws=i(J);tl=o(ws,"Then you can specify the model and tokenizer in the "),Ut=l(ws,"A",{href:!0});var Zp=i(Ut);sl=o(Zp,"pipeline()"),Zp.forEach(s),al=o(ws,", and apply the "),ea=l(ws,"CODE",{});var Xp=i(ea);ol=o(Xp,"classifier"),Xp.forEach(s),rl=o(ws," on your target text:"),ws.forEach(s),io=u(e),_(pt.$$.fragment,e),po=u(e),K=l(e,"P",{});var ks=i(K);nl=o(ks,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Wt=l(ks,"A",{href:!0});var ef=i(Wt);ll=o(ef,"fine-tuning tutorial"),ef.forEach(s),il=o(ks," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Bt=l(ks,"A",{href:!0});var tf=i(Bt);pl=o(tf,"here"),tf.forEach(s),fl=o(ks,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),ks.forEach(s),fo=u(e),re=l(e,"H2",{class:!0});var tr=i(re);we=l(tr,"A",{id:!0,class:!0,href:!0});var sf=i(we);ta=l(sf,"SPAN",{});var af=i(ta);_(ft.$$.fragment,af),af.forEach(s),sf.forEach(s),ul=u(tr),sa=l(tr,"SPAN",{});var of=i(sa);hl=o(of,"AutoClass"),of.forEach(s),tr.forEach(s),uo=u(e),_(ut.$$.fragment,e),ho=u(e),q=l(e,"P",{});var C=i(q);ml=o(C,"Under the hood, the "),Yt=l(C,"A",{href:!0});var rf=i(Yt);cl=o(rf,"AutoModelForSequenceClassification"),rf.forEach(s),dl=o(C," and "),Gt=l(C,"A",{href:!0});var nf=i(Gt);_l=o(nf,"AutoTokenizer"),nf.forEach(s),gl=o(C," classes work together to power the "),Qt=l(C,"A",{href:!0});var lf=i(Qt);vl=o(lf,"pipeline()"),lf.forEach(s),$l=o(C,". An "),Jt=l(C,"A",{href:!0});var pf=i(Jt);yl=o(pf,"AutoClass"),pf.forEach(s),bl=o(C," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),aa=l(C,"CODE",{});var ff=i(aa);wl=o(ff,"AutoClass"),ff.forEach(s),kl=o(C," for your task and it\u2019s associated tokenizer with "),Kt=l(C,"A",{href:!0});var uf=i(Kt);El=o(uf,"AutoTokenizer"),uf.forEach(s),jl=o(C,"."),C.forEach(s),mo=u(e),V=l(e,"P",{});var Es=i(V);Al=o(Es,"Let\u2019s return to our example and see how you can use the "),oa=l(Es,"CODE",{});var hf=i(oa);Tl=o(hf,"AutoClass"),hf.forEach(s),xl=o(Es," to replicate the results of the "),Vt=l(Es,"A",{href:!0});var mf=i(Vt);ql=o(mf,"pipeline()"),mf.forEach(s),zl=o(Es,"."),Es.forEach(s),co=u(e),ne=l(e,"H3",{class:!0});var sr=i(ne);ke=l(sr,"A",{id:!0,class:!0,href:!0});var cf=i(ke);ra=l(cf,"SPAN",{});var df=i(ra);_(ht.$$.fragment,df),df.forEach(s),cf.forEach(s),Fl=u(sr),na=l(sr,"SPAN",{});var _f=i(na);Sl=o(_f,"AutoTokenizer"),_f.forEach(s),sr.forEach(s),_o=u(e),Z=l(e,"P",{});var js=i(Z);Pl=o(js,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),la=l(js,"EM",{});var gf=i(la);Ml=o(gf,"tokens"),gf.forEach(s),Cl=o(js,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Zt=l(js,"A",{href:!0});var vf=i(Zt);Il=o(vf,"here"),vf.forEach(s),Nl=o(js,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),js.forEach(s),go=u(e),Ee=l(e,"P",{});var ar=i(Ee);Ol=o(ar,"Load a tokenizer with "),Xt=l(ar,"A",{href:!0});var $f=i(Xt);Dl=o($f,"AutoTokenizer"),$f.forEach(s),Ll=o(ar,":"),ar.forEach(s),vo=u(e),_(mt.$$.fragment,e),$o=u(e),je=l(e,"P",{});var or=i(je);Rl=o(or,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ia=l(or,"EM",{});var yf=i(ia);Hl=o(yf,"vocabulary"),yf.forEach(s),Ul=o(or,"."),or.forEach(s),yo=u(e),es=l(e,"P",{});var bf=i(es);Wl=o(bf,"Pass your text to the tokenizer:"),bf.forEach(s),bo=u(e),_(ct.$$.fragment,e),wo=u(e),ts=l(e,"P",{});var wf=i(ts);Bl=o(wf,"The tokenizer will return a dictionary containing:"),wf.forEach(s),ko=u(e),Ae=l(e,"UL",{});var rr=i(Ae);ss=l(rr,"LI",{});var ep=i(ss);as=l(ep,"A",{href:!0});var kf=i(as);Yl=o(kf,"input_ids"),kf.forEach(s),Gl=o(ep,": numerical representions of your tokens."),ep.forEach(s),Ql=u(rr),os=l(rr,"LI",{});var tp=i(os);rs=l(tp,"A",{href:!0});var Ef=i(rs);Jl=o(Ef,"atttention_mask"),Ef.forEach(s),Kl=o(tp,": indicates which tokens should be attended to."),tp.forEach(s),rr.forEach(s),Eo=u(e),Te=l(e,"P",{});var nr=i(Te);Vl=o(nr,"Just like the "),ns=l(nr,"A",{href:!0});var jf=i(ns);Zl=o(jf,"pipeline()"),jf.forEach(s),Xl=o(nr,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),nr.forEach(s),jo=u(e),_(dt.$$.fragment,e),Ao=u(e),xe=l(e,"P",{});var lr=i(xe);ei=o(lr,"Read the "),ls=l(lr,"A",{href:!0});var Af=i(ls);ti=o(Af,"preprocessing"),Af.forEach(s),si=o(lr," tutorial for more details about tokenization."),lr.forEach(s),To=u(e),le=l(e,"H3",{class:!0});var ir=i(le);qe=l(ir,"A",{id:!0,class:!0,href:!0});var Tf=i(qe);pa=l(Tf,"SPAN",{});var xf=i(pa);_(_t.$$.fragment,xf),xf.forEach(s),Tf.forEach(s),ai=u(ir),fa=l(ir,"SPAN",{});var qf=i(fa);oi=o(qf,"AutoModel"),qf.forEach(s),ir.forEach(s),xo=u(e),F=l(e,"P",{});var L=i(F);ri=o(L,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),is=l(L,"A",{href:!0});var zf=i(is);ni=o(zf,"AutoModel"),zf.forEach(s),li=o(L," like you would load an "),ps=l(L,"A",{href:!0});var Ff=i(ps);ii=o(Ff,"AutoTokenizer"),Ff.forEach(s),pi=o(L,". The only difference is selecting the correct "),fs=l(L,"A",{href:!0});var Sf=i(fs);fi=o(Sf,"AutoModel"),Sf.forEach(s),ui=o(L," for the task. Since you are doing text - or sequence - classification, load "),us=l(L,"A",{href:!0});var Pf=i(us);hi=o(Pf,"AutoModelForSequenceClassification"),Pf.forEach(s),mi=o(L,". The TensorFlow equivalent is simply "),hs=l(L,"A",{href:!0});var Mf=i(hs);ci=o(Mf,"TFAutoModelForSequenceClassification"),Mf.forEach(s),di=o(L,":"),L.forEach(s),qo=u(e),_(gt.$$.fragment,e),zo=u(e),_(ze.$$.fragment,e),Fo=u(e),Fe=l(e,"P",{});var pr=i(Fe);_i=o(pr,"Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),ua=l(pr,"CODE",{});var Cf=i(ua);gi=o(Cf,"**"),Cf.forEach(s),vi=o(pr,". For TensorFlow models, pass the dictionary keys directly to the tensors:"),pr.forEach(s),So=u(e),_(vt.$$.fragment,e),Po=u(e),X=l(e,"P",{});var As=i(X);$i=o(As,"The model outputs the final activations in the "),ha=l(As,"CODE",{});var If=i(ha);yi=o(If,"logits"),If.forEach(s),bi=o(As," attribute. Apply the softmax function to the "),ma=l(As,"CODE",{});var Nf=i(ma);wi=o(Nf,"logits"),Nf.forEach(s),ki=o(As," to retrieve the probabilities:"),As.forEach(s),Mo=u(e),_($t.$$.fragment,e),Co=u(e),_(Se.$$.fragment,e),Io=u(e),z=l(e,"P",{});var I=i(z);Ei=o(I,"Models are a standard "),yt=l(I,"A",{href:!0,rel:!0});var Of=i(yt);ca=l(Of,"CODE",{});var Df=i(ca);ji=o(Df,"torch.nn.Module"),Df.forEach(s),Of.forEach(s),Ai=o(I," or a "),bt=l(I,"A",{href:!0,rel:!0});var Lf=i(bt);da=l(Lf,"CODE",{});var Rf=i(da);Ti=o(Rf,"tf.keras.Model"),Rf.forEach(s),Lf.forEach(s),xi=o(I," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),ms=l(I,"A",{href:!0});var Hf=i(ms);qi=o(Hf,"Trainer"),Hf.forEach(s),zi=o(I," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),_a=l(I,"CODE",{});var Uf=i(_a);Fi=o(Uf,"fit"),Uf.forEach(s),Si=o(I," method from "),wt=l(I,"A",{href:!0,rel:!0});var Wf=i(wt);Pi=o(Wf,"Keras"),Wf.forEach(s),Mi=o(I,". Refer to the "),cs=l(I,"A",{href:!0});var Bf=i(cs);Ci=o(Bf,"training tutorial"),Bf.forEach(s),Ii=o(I," for more details."),I.forEach(s),No=u(e),_(Pe.$$.fragment,e),Oo=u(e),ie=l(e,"H3",{class:!0});var fr=i(ie);Me=l(fr,"A",{id:!0,class:!0,href:!0});var Yf=i(Me);ga=l(Yf,"SPAN",{});var Gf=i(ga);_(kt.$$.fragment,Gf),Gf.forEach(s),Yf.forEach(s),Ni=u(fr),va=l(fr,"SPAN",{});var Qf=i(va);Oi=o(Qf,"Save a model"),Qf.forEach(s),fr.forEach(s),Do=u(e),Ce=l(e,"P",{});var ur=i(Ce);Di=o(ur,"Once your model is fine-tuned, you can save it with its tokenizer using "),ds=l(ur,"A",{href:!0});var Jf=i(ds);Li=o(Jf,"PreTrainedModel.save_pretrained()"),Jf.forEach(s),Ri=o(ur,":"),ur.forEach(s),Lo=u(e),_(Et.$$.fragment,e),Ro=u(e),Ie=l(e,"P",{});var hr=i(Ie);Hi=o(hr,"When you are ready to use the model again, reload it with "),_s=l(hr,"A",{href:!0});var Kf=i(_s);Ui=o(Kf,"PreTrainedModel.from_pretrained()"),Kf.forEach(s),Wi=o(hr,":"),hr.forEach(s),Ho=u(e),_(jt.$$.fragment,e),Uo=u(e),ee=l(e,"P",{});var Ts=i(ee);Bi=o(Ts,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),$a=l(Ts,"CODE",{});var Vf=i($a);Yi=o(Vf,"from_pt"),Vf.forEach(s),Gi=o(Ts," or "),ya=l(Ts,"CODE",{});var Zf=i(ya);Qi=o(Zf,"from_tf"),Zf.forEach(s),Ji=o(Ts," parameter can convert the model from one framework to the other:"),Ts.forEach(s),Wo=u(e),_(At.$$.fragment,e),this.h()},h(){h(m,"name","hf:doc:metadata"),h(m,"content",JSON.stringify(uu)),h(w,"id","quick-tour"),h(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(w,"href","#quick-tour"),h(c,"class","relative group"),h(xt,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(qt,"href","./model_doc/auto"),h(he,"id","pipeline"),h(he,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(he,"href","#pipeline"),h(se,"class","relative group"),h(zt,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(Ft,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(_e,"id","pipeline-usage"),h(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(_e,"href","#pipeline-usage"),h(ae,"class","relative group"),h(St,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(Mt,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(Qe,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),h(Qe,"rel","nofollow"),h(Ct,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(It,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(Ve,"href","https://huggingface.co/docs/datasets/"),h(Ve,"rel","nofollow"),h(Nt,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(et,"href","https://huggingface.co/docs/datasets/quickstart.html"),h(et,"rel","nofollow"),h(tt,"href","https://huggingface.co/datasets/superb"),h(tt,"rel","nofollow"),h(Dt,"href","main_classes/pipeline"),h(be,"id","use-another-model-and-tokenizer-in-the-pipeline"),h(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(be,"href","#use-another-model-and-tokenizer-in-the-pipeline"),h(oe,"class","relative group"),h(Lt,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(rt,"href","https://huggingface.co/models"),h(rt,"rel","nofollow"),h(Rt,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(nt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),h(nt,"rel","nofollow"),h(Ht,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),h(Ut,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(Wt,"href","./training"),h(Bt,"href","./model_sharing"),h(we,"id","autoclass"),h(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(we,"href","#autoclass"),h(re,"class","relative group"),h(Yt,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),h(Gt,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoTokenizer"),h(Qt,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(Jt,"href","./model_doc/auto"),h(Kt,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoTokenizer"),h(Vt,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(ke,"id","autotokenizer"),h(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ke,"href","#autotokenizer"),h(ne,"class","relative group"),h(Zt,"href","./tokenizer_summary"),h(Xt,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoTokenizer"),h(as,"href","./glossary#input-ids"),h(rs,"href",".glossary#attention-mask"),h(ns,"href","/docs/transformers/v4.17.0/en/main_classes/pipelines#transformers.pipeline"),h(ls,"href","./preprocessing"),h(qe,"id","automodel"),h(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(qe,"href","#automodel"),h(le,"class","relative group"),h(is,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModel"),h(ps,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoTokenizer"),h(fs,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModel"),h(us,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),h(hs,"href","/docs/transformers/v4.17.0/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),h(yt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(yt,"rel","nofollow"),h(bt,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),h(bt,"rel","nofollow"),h(ms,"href","/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.Trainer"),h(wt,"href","https://keras.io/"),h(wt,"rel","nofollow"),h(cs,"href","./training"),h(Me,"id","save-a-model"),h(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(Me,"href","#save-a-model"),h(ie,"class","relative group"),h(ds,"href","/docs/transformers/v4.17.0/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),h(_s,"href","/docs/transformers/v4.17.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(e,r){t(document.head,m),p(e,k,r),p(e,c,r),t(c,w),t(w,A),g(b,A,null),t(c,E),t(c,x),t(x,P),p(e,j,r),g(O,e,r),p(e,H,r),p(e,U,r),t(U,mr),t(U,xt),t(xt,cr),t(U,dr),t(U,qt),t(qt,_r),t(U,gr),p(e,ja,r),g(ue,e,r),p(e,Aa,r),p(e,se,r),t(se,he),t(he,xs),g(De,xs,null),t(se,vr),t(se,qs),t(qs,$r),p(e,Ta,r),p(e,Le,r),t(Le,zt),t(zt,yr),t(Le,br),p(e,xa,r),g(Re,e,r),p(e,qa,r),p(e,me,r),t(me,wr),t(me,Ft),t(Ft,kr),t(me,Er),p(e,za,r),p(e,He,r),t(He,zs),t(zs,jr),t(He,Ar),p(e,Fa,r),p(e,T,r),t(T,Fs),t(Fs,Tr),t(T,xr),t(T,Ss),t(Ss,qr),t(T,zr),t(T,Ps),t(Ps,Fr),t(T,Sr),t(T,Ms),t(Ms,Pr),t(T,Mr),t(T,Cs),t(Cs,Cr),t(T,Ir),t(T,Is),t(Is,Nr),t(T,Or),t(T,Ns),t(Ns,Dr),t(T,Lr),t(T,Os),t(Os,Rr),p(e,Sa,r),p(e,Ue,r),t(Ue,Ds),t(Ds,Hr),t(Ue,Ur),p(e,Pa,r),p(e,W,r),t(W,Ls),t(Ls,Wr),t(W,Br),t(W,Rs),t(Rs,Yr),t(W,Gr),t(W,Hs),t(Hs,Qr),p(e,Ma,r),p(e,We,r),t(We,Us),t(Us,Jr),t(We,Kr),p(e,Ca,r),p(e,ce,r),t(ce,Ws),t(Ws,Vr),t(ce,Zr),t(ce,Bs),t(Bs,Xr),p(e,Ia,r),g(de,e,r),p(e,Na,r),p(e,ae,r),t(ae,_e),t(_e,Ys),g(Be,Ys,null),t(ae,en),t(ae,Gs),t(Gs,tn),p(e,Oa,r),p(e,ge,r),t(ge,sn),t(ge,St),t(St,an),t(ge,on),p(e,Da,r),p(e,Pt,r),t(Pt,rn),p(e,La,r),g(Ye,e,r),p(e,Ra,r),p(e,ve,r),t(ve,nn),t(ve,Mt),t(Mt,ln),t(ve,pn),p(e,Ha,r),g(Ge,e,r),p(e,Ua,r),p(e,B,r),t(B,fn),t(B,Qe),t(Qe,un),t(B,hn),t(B,Qs),t(Qs,mn),t(B,cn),p(e,Wa,r),g(Je,e,r),p(e,Ba,r),p(e,$e,r),t($e,dn),t($e,Ct),t(Ct,_n),t($e,gn),p(e,Ya,r),g(Ke,e,r),p(e,Ga,r),p(e,Y,r),t(Y,vn),t(Y,It),t(It,$n),t(Y,yn),t(Y,Ve),t(Ve,bn),t(Y,wn),p(e,Qa,r),g(Ze,e,r),p(e,Ja,r),p(e,D,r),t(D,kn),t(D,Nt),t(Nt,En),t(D,jn),t(D,Js),t(Js,An),t(D,Tn),t(D,Ks),t(Ks,xn),t(D,qn),p(e,Ka,r),g(Xe,e,r),p(e,Va,r),p(e,G,r),t(G,zn),t(G,et),t(et,Fn),t(G,Sn),t(G,tt),t(tt,Pn),t(G,Mn),p(e,Za,r),g(st,e,r),p(e,Xa,r),p(e,Ot,r),t(Ot,Cn),p(e,eo,r),g(at,e,r),p(e,to,r),p(e,ye,r),t(ye,In),t(ye,Dt),t(Dt,Nn),t(ye,On),p(e,so,r),p(e,oe,r),t(oe,be),t(be,Vs),g(ot,Vs,null),t(oe,Dn),t(oe,Zs),t(Zs,Ln),p(e,ao,r),p(e,M,r),t(M,Rn),t(M,Lt),t(Lt,Hn),t(M,Un),t(M,rt),t(rt,Wn),t(M,Bn),t(M,Rt),t(Rt,Yn),t(M,Gn),t(M,nt),t(nt,Qn),t(M,Jn),p(e,oo,r),g(lt,e,r),p(e,ro,r),p(e,Q,r),t(Q,Kn),t(Q,Ht),t(Ht,Vn),t(Q,Zn),t(Q,Xs),t(Xs,Xn),t(Q,el),p(e,no,r),g(it,e,r),p(e,lo,r),p(e,J,r),t(J,tl),t(J,Ut),t(Ut,sl),t(J,al),t(J,ea),t(ea,ol),t(J,rl),p(e,io,r),g(pt,e,r),p(e,po,r),p(e,K,r),t(K,nl),t(K,Wt),t(Wt,ll),t(K,il),t(K,Bt),t(Bt,pl),t(K,fl),p(e,fo,r),p(e,re,r),t(re,we),t(we,ta),g(ft,ta,null),t(re,ul),t(re,sa),t(sa,hl),p(e,uo,r),g(ut,e,r),p(e,ho,r),p(e,q,r),t(q,ml),t(q,Yt),t(Yt,cl),t(q,dl),t(q,Gt),t(Gt,_l),t(q,gl),t(q,Qt),t(Qt,vl),t(q,$l),t(q,Jt),t(Jt,yl),t(q,bl),t(q,aa),t(aa,wl),t(q,kl),t(q,Kt),t(Kt,El),t(q,jl),p(e,mo,r),p(e,V,r),t(V,Al),t(V,oa),t(oa,Tl),t(V,xl),t(V,Vt),t(Vt,ql),t(V,zl),p(e,co,r),p(e,ne,r),t(ne,ke),t(ke,ra),g(ht,ra,null),t(ne,Fl),t(ne,na),t(na,Sl),p(e,_o,r),p(e,Z,r),t(Z,Pl),t(Z,la),t(la,Ml),t(Z,Cl),t(Z,Zt),t(Zt,Il),t(Z,Nl),p(e,go,r),p(e,Ee,r),t(Ee,Ol),t(Ee,Xt),t(Xt,Dl),t(Ee,Ll),p(e,vo,r),g(mt,e,r),p(e,$o,r),p(e,je,r),t(je,Rl),t(je,ia),t(ia,Hl),t(je,Ul),p(e,yo,r),p(e,es,r),t(es,Wl),p(e,bo,r),g(ct,e,r),p(e,wo,r),p(e,ts,r),t(ts,Bl),p(e,ko,r),p(e,Ae,r),t(Ae,ss),t(ss,as),t(as,Yl),t(ss,Gl),t(Ae,Ql),t(Ae,os),t(os,rs),t(rs,Jl),t(os,Kl),p(e,Eo,r),p(e,Te,r),t(Te,Vl),t(Te,ns),t(ns,Zl),t(Te,Xl),p(e,jo,r),g(dt,e,r),p(e,Ao,r),p(e,xe,r),t(xe,ei),t(xe,ls),t(ls,ti),t(xe,si),p(e,To,r),p(e,le,r),t(le,qe),t(qe,pa),g(_t,pa,null),t(le,ai),t(le,fa),t(fa,oi),p(e,xo,r),p(e,F,r),t(F,ri),t(F,is),t(is,ni),t(F,li),t(F,ps),t(ps,ii),t(F,pi),t(F,fs),t(fs,fi),t(F,ui),t(F,us),t(us,hi),t(F,mi),t(F,hs),t(hs,ci),t(F,di),p(e,qo,r),g(gt,e,r),p(e,zo,r),g(ze,e,r),p(e,Fo,r),p(e,Fe,r),t(Fe,_i),t(Fe,ua),t(ua,gi),t(Fe,vi),p(e,So,r),g(vt,e,r),p(e,Po,r),p(e,X,r),t(X,$i),t(X,ha),t(ha,yi),t(X,bi),t(X,ma),t(ma,wi),t(X,ki),p(e,Mo,r),g($t,e,r),p(e,Co,r),g(Se,e,r),p(e,Io,r),p(e,z,r),t(z,Ei),t(z,yt),t(yt,ca),t(ca,ji),t(z,Ai),t(z,bt),t(bt,da),t(da,Ti),t(z,xi),t(z,ms),t(ms,qi),t(z,zi),t(z,_a),t(_a,Fi),t(z,Si),t(z,wt),t(wt,Pi),t(z,Mi),t(z,cs),t(cs,Ci),t(z,Ii),p(e,No,r),g(Pe,e,r),p(e,Oo,r),p(e,ie,r),t(ie,Me),t(Me,ga),g(kt,ga,null),t(ie,Ni),t(ie,va),t(va,Oi),p(e,Do,r),p(e,Ce,r),t(Ce,Di),t(Ce,ds),t(ds,Li),t(Ce,Ri),p(e,Lo,r),g(Et,e,r),p(e,Ro,r),p(e,Ie,r),t(Ie,Hi),t(Ie,_s),t(_s,Ui),t(Ie,Wi),p(e,Ho,r),g(jt,e,r),p(e,Uo,r),p(e,ee,r),t(ee,Bi),t(ee,$a),t($a,Yi),t(ee,Gi),t(ee,ya),t(ya,Qi),t(ee,Ji),p(e,Wo,r),g(At,e,r),Bo=!0},p(e,[r]){const Tt={};r&2&&(Tt.$$scope={dirty:r,ctx:e}),ue.$set(Tt);const ba={};r&2&&(ba.$$scope={dirty:r,ctx:e}),de.$set(ba);const wa={};r&2&&(wa.$$scope={dirty:r,ctx:e}),ze.$set(wa);const ka={};r&2&&(ka.$$scope={dirty:r,ctx:e}),Se.$set(ka);const pe={};r&2&&(pe.$$scope={dirty:r,ctx:e}),Pe.$set(pe)},i(e){Bo||(v(b.$$.fragment,e),v(O.$$.fragment,e),v(ue.$$.fragment,e),v(De.$$.fragment,e),v(Re.$$.fragment,e),v(de.$$.fragment,e),v(Be.$$.fragment,e),v(Ye.$$.fragment,e),v(Ge.$$.fragment,e),v(Je.$$.fragment,e),v(Ke.$$.fragment,e),v(Ze.$$.fragment,e),v(Xe.$$.fragment,e),v(st.$$.fragment,e),v(at.$$.fragment,e),v(ot.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ft.$$.fragment,e),v(ut.$$.fragment,e),v(ht.$$.fragment,e),v(mt.$$.fragment,e),v(ct.$$.fragment,e),v(dt.$$.fragment,e),v(_t.$$.fragment,e),v(gt.$$.fragment,e),v(ze.$$.fragment,e),v(vt.$$.fragment,e),v($t.$$.fragment,e),v(Se.$$.fragment,e),v(Pe.$$.fragment,e),v(kt.$$.fragment,e),v(Et.$$.fragment,e),v(jt.$$.fragment,e),v(At.$$.fragment,e),Bo=!0)},o(e){$(b.$$.fragment,e),$(O.$$.fragment,e),$(ue.$$.fragment,e),$(De.$$.fragment,e),$(Re.$$.fragment,e),$(de.$$.fragment,e),$(Be.$$.fragment,e),$(Ye.$$.fragment,e),$(Ge.$$.fragment,e),$(Je.$$.fragment,e),$(Ke.$$.fragment,e),$(Ze.$$.fragment,e),$(Xe.$$.fragment,e),$(st.$$.fragment,e),$(at.$$.fragment,e),$(ot.$$.fragment,e),$(lt.$$.fragment,e),$(it.$$.fragment,e),$(pt.$$.fragment,e),$(ft.$$.fragment,e),$(ut.$$.fragment,e),$(ht.$$.fragment,e),$(mt.$$.fragment,e),$(ct.$$.fragment,e),$(dt.$$.fragment,e),$(_t.$$.fragment,e),$(gt.$$.fragment,e),$(ze.$$.fragment,e),$(vt.$$.fragment,e),$($t.$$.fragment,e),$(Se.$$.fragment,e),$(Pe.$$.fragment,e),$(kt.$$.fragment,e),$(Et.$$.fragment,e),$(jt.$$.fragment,e),$(At.$$.fragment,e),Bo=!1},d(e){s(m),e&&s(k),e&&s(c),y(b),e&&s(j),y(O,e),e&&s(H),e&&s(U),e&&s(ja),y(ue,e),e&&s(Aa),e&&s(se),y(De),e&&s(Ta),e&&s(Le),e&&s(xa),y(Re,e),e&&s(qa),e&&s(me),e&&s(za),e&&s(He),e&&s(Fa),e&&s(T),e&&s(Sa),e&&s(Ue),e&&s(Pa),e&&s(W),e&&s(Ma),e&&s(We),e&&s(Ca),e&&s(ce),e&&s(Ia),y(de,e),e&&s(Na),e&&s(ae),y(Be),e&&s(Oa),e&&s(ge),e&&s(Da),e&&s(Pt),e&&s(La),y(Ye,e),e&&s(Ra),e&&s(ve),e&&s(Ha),y(Ge,e),e&&s(Ua),e&&s(B),e&&s(Wa),y(Je,e),e&&s(Ba),e&&s($e),e&&s(Ya),y(Ke,e),e&&s(Ga),e&&s(Y),e&&s(Qa),y(Ze,e),e&&s(Ja),e&&s(D),e&&s(Ka),y(Xe,e),e&&s(Va),e&&s(G),e&&s(Za),y(st,e),e&&s(Xa),e&&s(Ot),e&&s(eo),y(at,e),e&&s(to),e&&s(ye),e&&s(so),e&&s(oe),y(ot),e&&s(ao),e&&s(M),e&&s(oo),y(lt,e),e&&s(ro),e&&s(Q),e&&s(no),y(it,e),e&&s(lo),e&&s(J),e&&s(io),y(pt,e),e&&s(po),e&&s(K),e&&s(fo),e&&s(re),y(ft),e&&s(uo),y(ut,e),e&&s(ho),e&&s(q),e&&s(mo),e&&s(V),e&&s(co),e&&s(ne),y(ht),e&&s(_o),e&&s(Z),e&&s(go),e&&s(Ee),e&&s(vo),y(mt,e),e&&s($o),e&&s(je),e&&s(yo),e&&s(es),e&&s(bo),y(ct,e),e&&s(wo),e&&s(ts),e&&s(ko),e&&s(Ae),e&&s(Eo),e&&s(Te),e&&s(jo),y(dt,e),e&&s(Ao),e&&s(xe),e&&s(To),e&&s(le),y(_t),e&&s(xo),e&&s(F),e&&s(qo),y(gt,e),e&&s(zo),y(ze,e),e&&s(Fo),e&&s(Fe),e&&s(So),y(vt,e),e&&s(Po),e&&s(X),e&&s(Mo),y($t,e),e&&s(Co),y(Se,e),e&&s(Io),e&&s(z),e&&s(No),y(Pe,e),e&&s(Oo),e&&s(ie),y(kt),e&&s(Do),e&&s(Ce),e&&s(Lo),y(Et,e),e&&s(Ro),e&&s(Ie),e&&s(Ho),y(jt,e),e&&s(Uo),e&&s(ee),e&&s(Wo),y(At,e)}}}const uu={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function hu(N,m,k){let{fw:c}=m;return N.$$set=w=>{"fw"in w&&k(0,c=w.fw)},[c]}class bu extends eu{constructor(m){super();tu(this,m,hu,fu,su,{fw:0})}}export{bu as default,uu as metadata};
