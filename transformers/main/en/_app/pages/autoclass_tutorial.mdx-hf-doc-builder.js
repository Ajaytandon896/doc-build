import{S as ma,i as da,s as $a,e as n,k as $,w as N,t as s,M as va,c as l,d as a,m as v,a as i,x as O,h as o,b as d,G as t,g as p,y as D,q as H,o as G,B as R,v as _a,L as ka}from"../chunks/vendor-hf-doc-builder.js";import{T as ua}from"../chunks/Tip-hf-doc-builder.js";import{I as Ve}from"../chunks/IconCopyLink-hf-doc-builder.js";import{C as we}from"../chunks/CodeBlock-hf-doc-builder.js";import{F as wa,M as ha}from"../chunks/Markdown-hf-doc-builder.js";function ba(te){let r,g,c,_,A,w,L,C;return{c(){r=n("p"),g=s("Remember, architecture refers to the skeleton of the model and checkpoints are the weights for a given architecture. For example, "),c=n("a"),_=s("BERT"),A=s(" is an architecture, while "),w=n("code"),L=s("bert-base-uncased"),C=s(" is a checkpoint. Model is a general term that can mean either architecture or checkpoint."),this.h()},l(y){r=l(y,"P",{});var F=i(r);g=o(F,"Remember, architecture refers to the skeleton of the model and checkpoints are the weights for a given architecture. For example, "),c=l(F,"A",{href:!0,rel:!0});var x=i(c);_=o(x,"BERT"),x.forEach(a),A=o(F," is an architecture, while "),w=l(F,"CODE",{});var M=i(w);L=o(M,"bert-base-uncased"),M.forEach(a),C=o(F," is a checkpoint. Model is a general term that can mean either architecture or checkpoint."),F.forEach(a),this.h()},h(){d(c,"href","https://huggingface.co/bert-base-uncased"),d(c,"rel","nofollow")},m(y,F){p(y,r,F),t(r,g),t(r,c),t(c,_),t(r,A),t(r,w),t(w,L),t(r,C)},d(y){y&&a(r)}}}function ga(te){let r,g,c,_,A,w,L,C,y,F,x,M,T,B,P,V,U,E,I,k;return{c(){r=n("p"),g=s("For PyTorch models, the "),c=n("code"),_=s("from_pretrained()"),A=s(" method uses "),w=n("code"),L=s("torch.load()"),C=s(" which internally uses "),y=n("code"),F=s("pickle"),x=s(" and is known to be insecure. In general, never load a model that could have come from an untrusted source, or that could have been tampered with. This security risk is partially mitigated for public models hosted on the Hugging Face Hub, which are "),M=n("a"),T=s("scanned for malware"),B=s(" at each commit. See the "),P=n("a"),V=s("Hub documentation"),U=s(" for best practices like "),E=n("a"),I=s("signed commit verification"),k=s(" with GPG."),this.h()},l(S){r=l(S,"P",{});var m=i(r);g=o(m,"For PyTorch models, the "),c=l(m,"CODE",{});var Y=i(c);_=o(Y,"from_pretrained()"),Y.forEach(a),A=o(m," method uses "),w=l(m,"CODE",{});var W=i(w);L=o(W,"torch.load()"),W.forEach(a),C=o(m," which internally uses "),y=l(m,"CODE",{});var q=i(y);F=o(q,"pickle"),q.forEach(a),x=o(m," and is known to be insecure. In general, never load a model that could have come from an untrusted source, or that could have been tampered with. This security risk is partially mitigated for public models hosted on the Hugging Face Hub, which are "),M=l(m,"A",{href:!0,rel:!0});var Q=i(M);T=o(Q,"scanned for malware"),Q.forEach(a),B=o(m," at each commit. See the "),P=l(m,"A",{href:!0,rel:!0});var X=i(P);V=o(X,"Hub documentation"),X.forEach(a),U=o(m," for best practices like "),E=l(m,"A",{href:!0,rel:!0});var J=i(E);I=o(J,"signed commit verification"),J.forEach(a),k=o(m," with GPG."),m.forEach(a),this.h()},h(){d(M,"href","https://huggingface.co/docs/hub/security-malware"),d(M,"rel","nofollow"),d(P,"href","https://huggingface.co/docs/hub/security"),d(P,"rel","nofollow"),d(E,"href","https://huggingface.co/docs/hub/security-gpg#signing-commits-with-gpg"),d(E,"rel","nofollow")},m(S,m){p(S,r,m),t(r,g),t(r,c),t(c,_),t(r,A),t(r,w),t(w,L),t(r,C),t(r,y),t(y,F),t(r,x),t(r,M),t(M,T),t(r,B),t(r,P),t(P,V),t(r,U),t(r,E),t(E,I),t(r,k)},d(S){S&&a(r)}}}function ya(te){let r,g,c,_,A,w,L,C,y,F,x,M,T,B,P,V,U,E,I,k,S,m,Y,W,q,Q,X,J,ae,Z,se,h,j;return T=new we({props:{code:`from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),E=new we({props:{code:`from transformers import AutoModelForTokenClassification

model = AutoModelForTokenClassification.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),k=new ua({props:{warning:!0,$$slots:{default:[ga]},$$scope:{ctx:te}}}),{c(){r=n("p"),g=s("Finally, the "),c=n("code"),_=s("AutoModelFor"),A=s(" classes let you load a pretrained model for a given task (see "),w=n("a"),L=s("here"),C=s(" for a complete list of available tasks). For example, load a model for sequence classification with "),y=n("a"),F=s("AutoModelForSequenceClassification.from_pretrained()"),x=s(":"),M=$(),N(T.$$.fragment),B=$(),P=n("p"),V=s("Easily reuse the same checkpoint to load an architecture for a different task:"),U=$(),N(E.$$.fragment),I=$(),N(k.$$.fragment),S=$(),m=n("p"),Y=s("Generally, we recommend using the "),W=n("code"),q=s("AutoTokenizer"),Q=s(" class and the "),X=n("code"),J=s("AutoModelFor"),ae=s(" class to load pretrained instances of models. This will ensure you load the correct architecture every time. In the next "),Z=n("a"),se=s("tutorial"),h=s(", learn how to use your newly loaded tokenizer, feature extractor and processor to preprocess a dataset for fine-tuning."),this.h()},l(f){r=l(f,"P",{});var b=i(r);g=o(b,"Finally, the "),c=l(b,"CODE",{});var oe=i(c);_=o(oe,"AutoModelFor"),oe.forEach(a),A=o(b," classes let you load a pretrained model for a given task (see "),w=l(b,"A",{href:!0});var ne=i(w);L=o(ne,"here"),ne.forEach(a),C=o(b," for a complete list of available tasks). For example, load a model for sequence classification with "),y=l(b,"A",{href:!0});var z=i(y);F=o(z,"AutoModelForSequenceClassification.from_pretrained()"),z.forEach(a),x=o(b,":"),b.forEach(a),M=v(f),O(T.$$.fragment,f),B=v(f),P=l(f,"P",{});var ee=i(P);V=o(ee,"Easily reuse the same checkpoint to load an architecture for a different task:"),ee.forEach(a),U=v(f),O(E.$$.fragment,f),I=v(f),O(k.$$.fragment,f),S=v(f),m=l(f,"P",{});var K=i(m);Y=o(K,"Generally, we recommend using the "),W=l(K,"CODE",{});var re=i(W);q=o(re,"AutoTokenizer"),re.forEach(a),Q=o(K," class and the "),X=l(K,"CODE",{});var Te=i(X);J=o(Te,"AutoModelFor"),Te.forEach(a),ae=o(K," class to load pretrained instances of models. This will ensure you load the correct architecture every time. In the next "),Z=l(K,"A",{href:!0});var pe=i(Z);se=o(pe,"tutorial"),pe.forEach(a),h=o(K,", learn how to use your newly loaded tokenizer, feature extractor and processor to preprocess a dataset for fine-tuning."),K.forEach(a),this.h()},h(){d(w,"href","model_doc/auto"),d(y,"href","/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained"),d(Z,"href","preprocessing")},m(f,b){p(f,r,b),t(r,g),t(r,c),t(c,_),t(r,A),t(r,w),t(w,L),t(r,C),t(r,y),t(y,F),t(r,x),p(f,M,b),D(T,f,b),p(f,B,b),p(f,P,b),t(P,V),p(f,U,b),D(E,f,b),p(f,I,b),D(k,f,b),p(f,S,b),p(f,m,b),t(m,Y),t(m,W),t(W,q),t(m,Q),t(m,X),t(X,J),t(m,ae),t(m,Z),t(Z,se),t(m,h),j=!0},p(f,b){const oe={};b&2&&(oe.$$scope={dirty:b,ctx:f}),k.$set(oe)},i(f){j||(H(T.$$.fragment,f),H(E.$$.fragment,f),H(k.$$.fragment,f),j=!0)},o(f){G(T.$$.fragment,f),G(E.$$.fragment,f),G(k.$$.fragment,f),j=!1},d(f){f&&a(r),f&&a(M),R(T,f),f&&a(B),f&&a(P),f&&a(U),R(E,f),f&&a(I),R(k,f),f&&a(S),f&&a(m)}}}function Aa(te){let r,g;return r=new ha({props:{$$slots:{default:[ya]},$$scope:{ctx:te}}}),{c(){N(r.$$.fragment)},l(c){O(r.$$.fragment,c)},m(c,_){D(r,c,_),g=!0},p(c,_){const A={};_&2&&(A.$$scope={dirty:_,ctx:c}),r.$set(A)},i(c){g||(H(r.$$.fragment,c),g=!0)},o(c){G(r.$$.fragment,c),g=!1},d(c){R(r,c)}}}function Ea(te){let r,g,c,_,A,w,L,C,y,F,x,M,T,B,P,V,U,E,I,k,S,m,Y,W,q,Q,X,J,ae,Z,se;return T=new we({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),E=new we({props:{code:`from transformers import TFAutoModelForTokenClassification

model = TFAutoModelForTokenClassification.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),{c(){r=n("p"),g=s("Finally, the "),c=n("code"),_=s("TFAutoModelFor"),A=s(" classes let you load a pretrained model for a given task (see "),w=n("a"),L=s("here"),C=s(" for a complete list of available tasks). For example, load a model for sequence classification with "),y=n("a"),F=s("TFAutoModelForSequenceClassification.from_pretrained()"),x=s(":"),M=$(),N(T.$$.fragment),B=$(),P=n("p"),V=s("Easily reuse the same checkpoint to load an architecture for a different task:"),U=$(),N(E.$$.fragment),I=$(),k=n("p"),S=s("Generally, we recommend using the "),m=n("code"),Y=s("AutoTokenizer"),W=s(" class and the "),q=n("code"),Q=s("TFAutoModelFor"),X=s(" class to load pretrained instances of models. This will ensure you load the correct architecture every time. In the next "),J=n("a"),ae=s("tutorial"),Z=s(", learn how to use your newly loaded tokenizer, feature extractor and processor to preprocess a dataset for fine-tuning."),this.h()},l(h){r=l(h,"P",{});var j=i(r);g=o(j,"Finally, the "),c=l(j,"CODE",{});var f=i(c);_=o(f,"TFAutoModelFor"),f.forEach(a),A=o(j," classes let you load a pretrained model for a given task (see "),w=l(j,"A",{href:!0});var b=i(w);L=o(b,"here"),b.forEach(a),C=o(j," for a complete list of available tasks). For example, load a model for sequence classification with "),y=l(j,"A",{href:!0});var oe=i(y);F=o(oe,"TFAutoModelForSequenceClassification.from_pretrained()"),oe.forEach(a),x=o(j,":"),j.forEach(a),M=v(h),O(T.$$.fragment,h),B=v(h),P=l(h,"P",{});var ne=i(P);V=o(ne,"Easily reuse the same checkpoint to load an architecture for a different task:"),ne.forEach(a),U=v(h),O(E.$$.fragment,h),I=v(h),k=l(h,"P",{});var z=i(k);S=o(z,"Generally, we recommend using the "),m=l(z,"CODE",{});var ee=i(m);Y=o(ee,"AutoTokenizer"),ee.forEach(a),W=o(z," class and the "),q=l(z,"CODE",{});var K=i(q);Q=o(K,"TFAutoModelFor"),K.forEach(a),X=o(z," class to load pretrained instances of models. This will ensure you load the correct architecture every time. In the next "),J=l(z,"A",{href:!0});var re=i(J);ae=o(re,"tutorial"),re.forEach(a),Z=o(z,", learn how to use your newly loaded tokenizer, feature extractor and processor to preprocess a dataset for fine-tuning."),z.forEach(a),this.h()},h(){d(w,"href","model_doc/auto"),d(y,"href","/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained"),d(J,"href","preprocessing")},m(h,j){p(h,r,j),t(r,g),t(r,c),t(c,_),t(r,A),t(r,w),t(w,L),t(r,C),t(r,y),t(y,F),t(r,x),p(h,M,j),D(T,h,j),p(h,B,j),p(h,P,j),t(P,V),p(h,U,j),D(E,h,j),p(h,I,j),p(h,k,j),t(k,S),t(k,m),t(m,Y),t(k,W),t(k,q),t(q,Q),t(k,X),t(k,J),t(J,ae),t(k,Z),se=!0},p:ka,i(h){se||(H(T.$$.fragment,h),H(E.$$.fragment,h),se=!0)},o(h){G(T.$$.fragment,h),G(E.$$.fragment,h),se=!1},d(h){h&&a(r),h&&a(M),R(T,h),h&&a(B),h&&a(P),h&&a(U),R(E,h),h&&a(I),h&&a(k)}}}function ja(te){let r,g;return r=new ha({props:{$$slots:{default:[Ea]},$$scope:{ctx:te}}}),{c(){N(r.$$.fragment)},l(c){O(r.$$.fragment,c)},m(c,_){D(r,c,_),g=!0},p(c,_){const A={};_&2&&(A.$$scope={dirty:_,ctx:c}),r.$set(A)},i(c){g||(H(r.$$.fragment,c),g=!0)},o(c){G(r.$$.fragment,c),g=!1},d(c){R(r,c)}}}function Fa(te){let r,g,c,_,A,w,L,C,y,F,x,M,T,B,P,V,U,E,I,k,S,m,Y,W,q,Q,X,J,ae,Z,se,h,j,f,b,oe,ne,z,ee,K,re,Te,pe,$t,Ue,Pe,vt,We,fe,_t,qe,kt,wt,Je,be,Ke,Me,bt,Qe,ge,Xe,le,ue,Ne,ye,gt,Oe,yt,Ye,Ce,At,Ze,he,Et,ze,jt,Ft,et,Ae,tt,ie,me,De,Ee,xt,He,Tt,at,de,Pt,Le,qt,Mt,rt,$e,Ct,Se,zt,Lt,st,je,ot,ce,ve,Ge,Fe,St,Re,It,nt,_e,lt;return w=new Ve({}),k=new ua({props:{$$slots:{default:[ba]},$$scope:{ctx:te}}}),re=new Ve({}),be=new we({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)`}}),ge=new we({props:{code:`sequence = "In a hole in the ground there lived a hobbit."
print(tokenizer(sequence))`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>sequence = <span class="hljs-string">&quot;In a hole in the ground there lived a hobbit.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer(sequence))
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">4920</span>, <span class="hljs-number">1999</span>, <span class="hljs-number">1996</span>, <span class="hljs-number">2598</span>, <span class="hljs-number">2045</span>, <span class="hljs-number">2973</span>, <span class="hljs-number">1037</span>, <span class="hljs-number">7570</span>, <span class="hljs-number">10322</span>, <span class="hljs-number">4183</span>, <span class="hljs-number">1012</span>, <span class="hljs-number">102</span>], 
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], 
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),ye=new Ve({}),Ae=new we({props:{code:`from transformers import AutoFeatureExtractor

feature_extractor = AutoFeatureExtractor.from_pretrained(
    "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition&quot;</span>
<span class="hljs-meta">... </span>)`}}),Ee=new Ve({}),je=new we({props:{code:`from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("microsoft/layoutlmv2-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/layoutlmv2-base-uncased&quot;</span>)`}}),Fe=new Ve({}),_e=new wa({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ja],pytorch:[Aa]},$$scope:{ctx:te}}}),{c(){r=n("meta"),g=$(),c=n("h1"),_=n("a"),A=n("span"),N(w.$$.fragment),L=$(),C=n("span"),y=s("Load pretrained instances with an AutoClass"),F=$(),x=n("p"),M=s("With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As a part of \u{1F917} Transformers core philosophy to make the library easy, simple and flexible to use, an "),T=n("code"),B=s("AutoClass"),P=s(" automatically infer and load the correct architecture from a given checkpoint. The "),V=n("code"),U=s("from_pretrained()"),E=s(" method lets you quickly load a pretrained model for any architecture so you don\u2019t have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different."),I=$(),N(k.$$.fragment),S=$(),m=n("p"),Y=s("In this tutorial, learn to:"),W=$(),q=n("ul"),Q=n("li"),X=s("Load a pretrained tokenizer."),J=$(),ae=n("li"),Z=s("Load a pretrained feature extractor."),se=$(),h=n("li"),j=s("Load a pretrained processor."),f=$(),b=n("li"),oe=s("Load a pretrained model."),ne=$(),z=n("h2"),ee=n("a"),K=n("span"),N(re.$$.fragment),Te=$(),pe=n("span"),$t=s("AutoTokenizer"),Ue=$(),Pe=n("p"),vt=s("Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model."),We=$(),fe=n("p"),_t=s("Load a tokenizer with "),qe=n("a"),kt=s("AutoTokenizer.from_pretrained()"),wt=s(":"),Je=$(),N(be.$$.fragment),Ke=$(),Me=n("p"),bt=s("Then tokenize your input as shown below:"),Qe=$(),N(ge.$$.fragment),Xe=$(),le=n("h2"),ue=n("a"),Ne=n("span"),N(ye.$$.fragment),gt=$(),Oe=n("span"),yt=s("AutoFeatureExtractor"),Ye=$(),Ce=n("p"),At=s("For audio and vision tasks, a feature extractor processes the audio signal or image into the correct input format."),Ze=$(),he=n("p"),Et=s("Load a feature extractor with "),ze=n("a"),jt=s("AutoFeatureExtractor.from_pretrained()"),Ft=s(":"),et=$(),N(Ae.$$.fragment),tt=$(),ie=n("h2"),me=n("a"),De=n("span"),N(Ee.$$.fragment),xt=$(),He=n("span"),Tt=s("AutoProcessor"),at=$(),de=n("p"),Pt=s("Multimodal tasks require a processor that combines two types of preprocessing tools. For example, the "),Le=n("a"),qt=s("LayoutLMV2"),Mt=s(" model requires a feature extractor to handle images and a tokenizer to handle text; a processor combines both of them."),rt=$(),$e=n("p"),Ct=s("Load a processor with "),Se=n("a"),zt=s("AutoProcessor.from_pretrained()"),Lt=s(":"),st=$(),N(je.$$.fragment),ot=$(),ce=n("h2"),ve=n("a"),Ge=n("span"),N(Fe.$$.fragment),St=$(),Re=n("span"),It=s("AutoModel"),nt=$(),N(_e.$$.fragment),this.h()},l(e){const u=va('[data-svelte="svelte-1phssyn"]',document.head);r=l(u,"META",{name:!0,content:!0}),u.forEach(a),g=v(e),c=l(e,"H1",{class:!0});var xe=i(c);_=l(xe,"A",{id:!0,class:!0,href:!0});var Be=i(_);A=l(Be,"SPAN",{});var Nt=i(A);O(w.$$.fragment,Nt),Nt.forEach(a),Be.forEach(a),L=v(xe),C=l(xe,"SPAN",{});var Ot=i(C);y=o(Ot,"Load pretrained instances with an AutoClass"),Ot.forEach(a),xe.forEach(a),F=v(e),x=l(e,"P",{});var Ie=i(x);M=o(Ie,"With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As a part of \u{1F917} Transformers core philosophy to make the library easy, simple and flexible to use, an "),T=l(Ie,"CODE",{});var Dt=i(T);B=o(Dt,"AutoClass"),Dt.forEach(a),P=o(Ie," automatically infer and load the correct architecture from a given checkpoint. The "),V=l(Ie,"CODE",{});var Ht=i(V);U=o(Ht,"from_pretrained()"),Ht.forEach(a),E=o(Ie," method lets you quickly load a pretrained model for any architecture so you don\u2019t have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different."),Ie.forEach(a),I=v(e),O(k.$$.fragment,e),S=v(e),m=l(e,"P",{});var Gt=i(m);Y=o(Gt,"In this tutorial, learn to:"),Gt.forEach(a),W=v(e),q=l(e,"UL",{});var ke=i(q);Q=l(ke,"LI",{});var Rt=i(Q);X=o(Rt,"Load a pretrained tokenizer."),Rt.forEach(a),J=v(ke),ae=l(ke,"LI",{});var Bt=i(ae);Z=o(Bt,"Load a pretrained feature extractor."),Bt.forEach(a),se=v(ke),h=l(ke,"LI",{});var Vt=i(h);j=o(Vt,"Load a pretrained processor."),Vt.forEach(a),f=v(ke),b=l(ke,"LI",{});var Ut=i(b);oe=o(Ut,"Load a pretrained model."),Ut.forEach(a),ke.forEach(a),ne=v(e),z=l(e,"H2",{class:!0});var it=i(z);ee=l(it,"A",{id:!0,class:!0,href:!0});var Wt=i(ee);K=l(Wt,"SPAN",{});var Jt=i(K);O(re.$$.fragment,Jt),Jt.forEach(a),Wt.forEach(a),Te=v(it),pe=l(it,"SPAN",{});var Kt=i(pe);$t=o(Kt,"AutoTokenizer"),Kt.forEach(a),it.forEach(a),Ue=v(e),Pe=l(e,"P",{});var Qt=i(Pe);vt=o(Qt,"Nearly every NLP task begins with a tokenizer. A tokenizer converts your input into a format that can be processed by the model."),Qt.forEach(a),We=v(e),fe=l(e,"P",{});var ct=i(fe);_t=o(ct,"Load a tokenizer with "),qe=l(ct,"A",{href:!0});var Xt=i(qe);kt=o(Xt,"AutoTokenizer.from_pretrained()"),Xt.forEach(a),wt=o(ct,":"),ct.forEach(a),Je=v(e),O(be.$$.fragment,e),Ke=v(e),Me=l(e,"P",{});var Yt=i(Me);bt=o(Yt,"Then tokenize your input as shown below:"),Yt.forEach(a),Qe=v(e),O(ge.$$.fragment,e),Xe=v(e),le=l(e,"H2",{class:!0});var pt=i(le);ue=l(pt,"A",{id:!0,class:!0,href:!0});var Zt=i(ue);Ne=l(Zt,"SPAN",{});var ea=i(Ne);O(ye.$$.fragment,ea),ea.forEach(a),Zt.forEach(a),gt=v(pt),Oe=l(pt,"SPAN",{});var ta=i(Oe);yt=o(ta,"AutoFeatureExtractor"),ta.forEach(a),pt.forEach(a),Ye=v(e),Ce=l(e,"P",{});var aa=i(Ce);At=o(aa,"For audio and vision tasks, a feature extractor processes the audio signal or image into the correct input format."),aa.forEach(a),Ze=v(e),he=l(e,"P",{});var ft=i(he);Et=o(ft,"Load a feature extractor with "),ze=l(ft,"A",{href:!0});var ra=i(ze);jt=o(ra,"AutoFeatureExtractor.from_pretrained()"),ra.forEach(a),Ft=o(ft,":"),ft.forEach(a),et=v(e),O(Ae.$$.fragment,e),tt=v(e),ie=l(e,"H2",{class:!0});var ut=i(ie);me=l(ut,"A",{id:!0,class:!0,href:!0});var sa=i(me);De=l(sa,"SPAN",{});var oa=i(De);O(Ee.$$.fragment,oa),oa.forEach(a),sa.forEach(a),xt=v(ut),He=l(ut,"SPAN",{});var na=i(He);Tt=o(na,"AutoProcessor"),na.forEach(a),ut.forEach(a),at=v(e),de=l(e,"P",{});var ht=i(de);Pt=o(ht,"Multimodal tasks require a processor that combines two types of preprocessing tools. For example, the "),Le=l(ht,"A",{href:!0});var la=i(Le);qt=o(la,"LayoutLMV2"),la.forEach(a),Mt=o(ht," model requires a feature extractor to handle images and a tokenizer to handle text; a processor combines both of them."),ht.forEach(a),rt=v(e),$e=l(e,"P",{});var mt=i($e);Ct=o(mt,"Load a processor with "),Se=l(mt,"A",{href:!0});var ia=i(Se);zt=o(ia,"AutoProcessor.from_pretrained()"),ia.forEach(a),Lt=o(mt,":"),mt.forEach(a),st=v(e),O(je.$$.fragment,e),ot=v(e),ce=l(e,"H2",{class:!0});var dt=i(ce);ve=l(dt,"A",{id:!0,class:!0,href:!0});var ca=i(ve);Ge=l(ca,"SPAN",{});var pa=i(Ge);O(Fe.$$.fragment,pa),pa.forEach(a),ca.forEach(a),St=v(dt),Re=l(dt,"SPAN",{});var fa=i(Re);It=o(fa,"AutoModel"),fa.forEach(a),dt.forEach(a),nt=v(e),O(_e.$$.fragment,e),this.h()},h(){d(r,"name","hf:doc:metadata"),d(r,"content",JSON.stringify(xa)),d(_,"id","load-pretrained-instances-with-an-autoclass"),d(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_,"href","#load-pretrained-instances-with-an-autoclass"),d(c,"class","relative group"),d(ee,"id","autotokenizer"),d(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ee,"href","#autotokenizer"),d(z,"class","relative group"),d(qe,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),d(ue,"id","autofeatureextractor"),d(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ue,"href","#autofeatureextractor"),d(le,"class","relative group"),d(ze,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),d(me,"id","autoprocessor"),d(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(me,"href","#autoprocessor"),d(ie,"class","relative group"),d(Le,"href","model_doc/layoutlmv2"),d(Se,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),d(ve,"id","automodel"),d(ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ve,"href","#automodel"),d(ce,"class","relative group")},m(e,u){t(document.head,r),p(e,g,u),p(e,c,u),t(c,_),t(_,A),D(w,A,null),t(c,L),t(c,C),t(C,y),p(e,F,u),p(e,x,u),t(x,M),t(x,T),t(T,B),t(x,P),t(x,V),t(V,U),t(x,E),p(e,I,u),D(k,e,u),p(e,S,u),p(e,m,u),t(m,Y),p(e,W,u),p(e,q,u),t(q,Q),t(Q,X),t(q,J),t(q,ae),t(ae,Z),t(q,se),t(q,h),t(h,j),t(q,f),t(q,b),t(b,oe),p(e,ne,u),p(e,z,u),t(z,ee),t(ee,K),D(re,K,null),t(z,Te),t(z,pe),t(pe,$t),p(e,Ue,u),p(e,Pe,u),t(Pe,vt),p(e,We,u),p(e,fe,u),t(fe,_t),t(fe,qe),t(qe,kt),t(fe,wt),p(e,Je,u),D(be,e,u),p(e,Ke,u),p(e,Me,u),t(Me,bt),p(e,Qe,u),D(ge,e,u),p(e,Xe,u),p(e,le,u),t(le,ue),t(ue,Ne),D(ye,Ne,null),t(le,gt),t(le,Oe),t(Oe,yt),p(e,Ye,u),p(e,Ce,u),t(Ce,At),p(e,Ze,u),p(e,he,u),t(he,Et),t(he,ze),t(ze,jt),t(he,Ft),p(e,et,u),D(Ae,e,u),p(e,tt,u),p(e,ie,u),t(ie,me),t(me,De),D(Ee,De,null),t(ie,xt),t(ie,He),t(He,Tt),p(e,at,u),p(e,de,u),t(de,Pt),t(de,Le),t(Le,qt),t(de,Mt),p(e,rt,u),p(e,$e,u),t($e,Ct),t($e,Se),t(Se,zt),t($e,Lt),p(e,st,u),D(je,e,u),p(e,ot,u),p(e,ce,u),t(ce,ve),t(ve,Ge),D(Fe,Ge,null),t(ce,St),t(ce,Re),t(Re,It),p(e,nt,u),D(_e,e,u),lt=!0},p(e,[u]){const xe={};u&2&&(xe.$$scope={dirty:u,ctx:e}),k.$set(xe);const Be={};u&2&&(Be.$$scope={dirty:u,ctx:e}),_e.$set(Be)},i(e){lt||(H(w.$$.fragment,e),H(k.$$.fragment,e),H(re.$$.fragment,e),H(be.$$.fragment,e),H(ge.$$.fragment,e),H(ye.$$.fragment,e),H(Ae.$$.fragment,e),H(Ee.$$.fragment,e),H(je.$$.fragment,e),H(Fe.$$.fragment,e),H(_e.$$.fragment,e),lt=!0)},o(e){G(w.$$.fragment,e),G(k.$$.fragment,e),G(re.$$.fragment,e),G(be.$$.fragment,e),G(ge.$$.fragment,e),G(ye.$$.fragment,e),G(Ae.$$.fragment,e),G(Ee.$$.fragment,e),G(je.$$.fragment,e),G(Fe.$$.fragment,e),G(_e.$$.fragment,e),lt=!1},d(e){a(r),e&&a(g),e&&a(c),R(w),e&&a(F),e&&a(x),e&&a(I),R(k,e),e&&a(S),e&&a(m),e&&a(W),e&&a(q),e&&a(ne),e&&a(z),R(re),e&&a(Ue),e&&a(Pe),e&&a(We),e&&a(fe),e&&a(Je),R(be,e),e&&a(Ke),e&&a(Me),e&&a(Qe),R(ge,e),e&&a(Xe),e&&a(le),R(ye),e&&a(Ye),e&&a(Ce),e&&a(Ze),e&&a(he),e&&a(et),R(Ae,e),e&&a(tt),e&&a(ie),R(Ee),e&&a(at),e&&a(de),e&&a(rt),e&&a($e),e&&a(st),R(je,e),e&&a(ot),e&&a(ce),R(Fe),e&&a(nt),R(_e,e)}}}const xa={local:"load-pretrained-instances-with-an-autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"autofeatureextractor",title:"AutoFeatureExtractor"},{local:"autoprocessor",title:"AutoProcessor"},{local:"automodel",title:"AutoModel"}],title:"Load pretrained instances with an AutoClass"};function Ta(te){return _a(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class La extends ma{constructor(r){super();da(this,r,Ta,Fa,$a,{})}}export{La as default,xa as metadata};
