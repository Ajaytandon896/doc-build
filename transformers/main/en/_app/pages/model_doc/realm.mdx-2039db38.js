import{S as Jm,i as Zm,s as Ym,e as o,k as d,w as f,t as s,M as eh,c as r,d as n,m as l,a,x as u,h as i,b as c,F as e,g as p,y as g,q as _,o as k,B as v}from"../../chunks/vendor-4833417e.js";import{T as Rr}from"../../chunks/Tip-fffd6df1.js";import{D as z}from"../../chunks/Docstring-4f315ed9.js";import{C as Xe}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as J}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function th(N){let h,$,w,T,y;return{c(){h=o("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=o("code"),T=s("Module"),y=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);$=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(n),y=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(n)},m(R,b){p(R,h,b),e(h,$),e(h,w),e(w,T),e(h,y)},d(R){R&&n(h)}}}function nh(N){let h,$,w,T,y;return{c(){h=o("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=o("code"),T=s("Module"),y=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);$=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(n),y=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(n)},m(R,b){p(R,h,b),e(h,$),e(h,w),e(w,T),e(h,y)},d(R){R&&n(h)}}}function oh(N){let h,$,w,T,y;return{c(){h=o("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=o("code"),T=s("Module"),y=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);$=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(n),y=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(n)},m(R,b){p(R,h,b),e(h,$),e(h,w),e(w,T),e(h,y)},d(R){R&&n(h)}}}function rh(N){let h,$,w,T,y;return{c(){h=o("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=o("code"),T=s("Module"),y=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);$=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(n),y=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(n)},m(R,b){p(R,h,b),e(h,$),e(h,w),e(w,T),e(h,y)},d(R){R&&n(h)}}}function ah(N){let h,$,w,T,y;return{c(){h=o("p"),$=s("Although the recipe for forward pass needs to be defined within this function, one should call the "),w=o("code"),T=s("Module"),y=s(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(R){h=r(R,"P",{});var b=a(h);$=i(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),w=r(b,"CODE",{});var q=a(w);T=i(q,"Module"),q.forEach(n),y=i(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(n)},m(R,b){p(R,h,b),e(h,$),e(h,w),e(w,T),e(h,y)},d(R){R&&n(h)}}}function sh(N){let h,$,w,T,y,R,b,q,ya,Tr,se,ye,ro,Ge,Ea,ao,za,$r,Ee,qa,Je,xa,Aa,yr,pn,Pa,Er,fn,so,ja,zr,Z,La,Ze,Ma,Sa,Ye,Fa,Ca,qr,ie,ze,io,et,Ia,lo,Da,xr,x,tt,Na,co,Oa,Wa,L,mo,un,Ka,Ba,ho,gn,Qa,Ha,po,_n,Va,Ua,fo,kn,Xa,Ga,uo,vn,Ja,Za,go,wn,Ya,es,nt,ts,ot,ns,os,rs,de,as,bn,ss,is,Rn,ds,ls,cs,_o,ms,hs,rt,Ar,le,qe,ko,at,ps,vo,fs,Pr,E,st,us,wo,gs,_s,xe,Tn,ks,vs,$n,ws,bs,Rs,it,Ts,yn,$s,ys,Es,Y,dt,zs,bo,qs,xs,lt,En,As,Ro,Ps,js,zn,Ls,To,Ms,Ss,Ae,ct,Fs,mt,Cs,$o,Is,Ds,Ns,W,ht,Os,yo,Ws,Ks,pt,Bs,ce,Qs,Eo,Hs,Vs,zo,Us,Xs,Gs,qn,ft,Js,P,ut,Zs,gt,Ys,qo,ei,ti,ni,me,xo,oi,ri,_t,ai,Ao,si,ii,di,kt,li,Po,ci,mi,hi,vt,xn,pi,jo,fi,ui,An,gi,Lo,_i,ki,Mo,vi,wi,wt,jr,he,Pe,So,bt,bi,Fo,Ri,Lr,M,Rt,Ti,Tt,$i,Co,yi,Ei,zi,je,Pn,qi,xi,jn,Ai,Pi,ji,$t,Li,Ln,Mi,Si,Fi,j,yt,Ci,Et,Ii,Io,Di,Ni,Oi,pe,Do,Wi,Ki,zt,Bi,No,Qi,Hi,Vi,qt,Ui,Oo,Xi,Gi,Ji,xt,Mn,Zi,Wo,Yi,ed,Sn,td,Ko,nd,od,Bo,rd,ad,At,Mr,fe,Le,Qo,Pt,sd,Ho,id,Sr,H,jt,dd,Vo,ld,cd,Me,Lt,md,Uo,hd,Fr,ue,Se,Xo,Mt,pd,Go,fd,Cr,V,St,ud,Ft,gd,Ct,_d,kd,vd,S,It,wd,ge,bd,Fn,Rd,Td,Jo,$d,yd,Ed,Fe,zd,Zo,qd,xd,Dt,Ir,_e,Ce,Yo,Nt,Ad,er,Pd,Dr,U,Ot,jd,Wt,Ld,Kt,Md,Sd,Fd,F,Bt,Cd,ke,Id,Cn,Dd,Nd,tr,Od,Wd,Kd,Ie,Bd,nr,Qd,Hd,Qt,Nr,ve,De,or,Ht,Vd,rr,Ud,Or,X,Vt,Xd,Ut,Gd,Xt,Jd,Zd,Yd,C,Gt,el,we,tl,In,nl,ol,ar,rl,al,sl,Ne,il,sr,dl,ll,Jt,Wr,be,Oe,ir,Zt,cl,dr,ml,Kr,G,Yt,hl,en,pl,tn,fl,ul,gl,ee,nn,_l,Re,kl,Dn,vl,wl,lr,bl,Rl,Tl,We,Br,Te,Ke,cr,on,$l,mr,yl,Qr,O,rn,El,Be,hr,zl,ql,an,xl,Al,Pl,Qe,sn,jl,dn,Ll,pr,Ml,Sl,Fl,I,ln,Cl,$e,Il,Nn,Dl,Nl,fr,Ol,Wl,Kl,He,Bl,ur,Ql,Hl,cn,Hr;return R=new J({}),Ge=new J({}),et=new J({}),tt=new z({props:{name:"class transformers.RealmConfig",anchor:"transformers.RealmConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"retriever_proj_size",val:" = 128"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_candidates",val:" = 8"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu_new'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"span_hidden_size",val:" = 256"},{name:"max_span_width",val:" = 10"},{name:"reader_layer_norm_eps",val:" = 0.001"},{name:"reader_beam_size",val:" = 5"},{name:"reader_seq_len",val:" = 320"},{name:"num_block_records",val:" = 13353718"},{name:"searcher_beam_size",val:" = 5000"},{name:"searcher_seq_len",val:" = 64"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/configuration_realm.py#L36",parametersDescription:[{anchor:"transformers.RealmConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the REALM model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>, <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmScorer">RealmScorer</a>, <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder">RealmKnowledgeAugEncoder</a>, or
<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmReader">RealmReader</a>.`,name:"vocab_size"},{anchor:"transformers.RealmConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimension of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.RealmConfig.retriever_proj_size",description:`<strong>retriever_proj_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Dimension of the retriever(embedder) projection.`,name:"retriever_proj_size"},{anchor:"transformers.RealmConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.RealmConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.RealmConfig.num_candidates",description:`<strong>num_candidates</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of candidates inputted to the RealmScorer or RealmKnowledgeAugEncoder.`,name:"num_candidates"},{anchor:"transformers.RealmConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.RealmConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu_new&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.RealmConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.RealmConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.RealmConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.RealmConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>, <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmScorer">RealmScorer</a>,
<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder">RealmKnowledgeAugEncoder</a>, or <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmReader">RealmReader</a>.`,name:"type_vocab_size"},{anchor:"transformers.RealmConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.RealmConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.RealmConfig.span_hidden_size",description:`<strong>span_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Dimension of the reader&#x2019;s spans.`,name:"span_hidden_size"},{anchor:"transformers.RealmConfig.max_span_width",description:`<strong>max_span_width</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Max span width of the reader.`,name:"max_span_width"},{anchor:"transformers.RealmConfig.reader_layer_norm_eps",description:`<strong>reader_layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The epsilon used by the reader&#x2019;s layer normalization layers.`,name:"reader_layer_norm_eps"},{anchor:"transformers.RealmConfig.reader_beam_size",description:`<strong>reader_beam_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Beam size of the reader.`,name:"reader_beam_size"},{anchor:"transformers.RealmConfig.reader_seq_len",description:`<strong>reader_seq_len</strong> (<code>int</code>, <em>optional</em>, defaults to 288+32) &#x2014;
Maximum sequence length of the reader.`,name:"reader_seq_len"},{anchor:"transformers.RealmConfig.num_block_records",description:`<strong>num_block_records</strong> (<code>int</code>, <em>optional</em>, defaults to 13353718) &#x2014;
Number of block records.`,name:"num_block_records"},{anchor:"transformers.RealmConfig.searcher_beam_size",description:`<strong>searcher_beam_size</strong> (<code>int</code>, <em>optional</em>, defaults to 5000) &#x2014;
Beam size of the searcher. Note that when eval mode is enabled, <em>searcher_beam_size</em> will be the same as
<em>reader_beam_size</em>.`,name:"searcher_beam_size"},{anchor:"transformers.RealmConfig.searcher_seq_len",description:`<strong>searcher_seq_len</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Maximum sequence length of the searcher.`,name:"searcher_seq_len"}]}}),rt=new Xe({props:{code:`from transformers import RealmEmbedder, RealmConfig

# Initializing a REALM realm-cc-news-pretrained-* style configuration
configuration = RealmConfig()

# Initializing a model from the google/realm-cc-news-pretrained-embedder style configuration
model = RealmEmbedder(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmEmbedder, RealmConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a REALM realm-cc-news-pretrained-* style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = RealmConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the google/realm-cc-news-pretrained-embedder style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmEmbedder(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),at=new J({}),st=new z({props:{name:"class transformers.RealmTokenizer",anchor:"transformers.RealmTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L88",parametersDescription:[{anchor:"transformers.RealmTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RealmTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RealmTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.RealmTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.RealmTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RealmTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RealmTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RealmTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RealmTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RealmTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).
strip_accents &#x2014; (<code>bool</code>, <em>optional</em>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"tokenize_chinese_chars"}]}}),dt=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L295",parametersDescription:[{anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new z({props:{name:"get_special_tokens_mask",anchor:"transformers.RealmTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L320",parametersDescription:[{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.RealmTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ht=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L348",parametersDescription:[{anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RealmTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pt=new Xe({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),ft=new z({props:{name:"save_vocabulary",anchor:"transformers.RealmTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L377"}}),ut=new z({props:{name:"batch_encode_candidates",anchor:"transformers.RealmTokenizer.batch_encode_candidates",parameters:[{name:"text",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm.py#L222",parametersDescription:[{anchor:"transformers.RealmTokenizer.batch_encode_candidates.text",description:`<strong>text</strong> (<code>List[List[str]]</code>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).`,name:"text"},{anchor:"transformers.RealmTokenizer.batch_encode_candidates.text_pair",description:`<strong>text_pair</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).
**kwargs &#x2014;
Keyword arguments of the <strong>call</strong> method.`,name:"text_pair"}],returnDescription:`
<p>Encoded text or text pair.</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),wt=new Xe({props:{code:`from transformers import RealmTokenizer

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_text = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`}}),bt=new J({}),Rt=new z({props:{name:"class transformers.RealmTokenizerFast",anchor:"transformers.RealmTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm_fast.py#L79",parametersDescription:[{anchor:"transformers.RealmTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RealmTokenizerFast.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RealmTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RealmTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RealmTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RealmTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RealmTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RealmTokenizerFast.clean_text",description:`<strong>clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean the text before tokenization by removing any control characters and replacing all
whitespaces by the classic one.`,name:"clean_text"},{anchor:"transformers.RealmTokenizerFast.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see <a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">this
issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RealmTokenizerFast.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"},{anchor:"transformers.RealmTokenizerFast.wordpieces_prefix",description:`<strong>wordpieces_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;##&quot;</code>) &#x2014;
The prefix for subwords.`,name:"wordpieces_prefix"}]}}),yt=new z({props:{name:"batch_encode_candidates",anchor:"transformers.RealmTokenizerFast.batch_encode_candidates",parameters:[{name:"text",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/tokenization_realm_fast.py#L170",parametersDescription:[{anchor:"transformers.RealmTokenizerFast.batch_encode_candidates.text",description:`<strong>text</strong> (<code>List[List[str]]</code>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).`,name:"text"},{anchor:"transformers.RealmTokenizerFast.batch_encode_candidates.text_pair",description:`<strong>text_pair</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
The batch of sequences to be encoded. Each sequence must be in this format: (batch_size,
num_candidates, text).
**kwargs &#x2014;
Keyword arguments of the <strong>call</strong> method.`,name:"text_pair"}],returnDescription:`
<p>Encoded text or text pair.</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),At=new Xe({props:{code:`from transformers import RealmTokenizerFast

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

tokenizer = RealmTokenizerFast.from_pretrained("google/realm-cc-news-pretrained-encoder")
tokenized_text = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizerFast.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_text = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`}}),Pt=new J({}),jt=new z({props:{name:"class transformers.RealmRetriever",anchor:"transformers.RealmRetriever",parameters:[{name:"block_records",val:""},{name:"tokenizer",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/retrieval_realm.py#L73",parametersDescription:[{anchor:"transformers.RealmRetriever.block_records",description:`<strong>block_records</strong> (<code>np.ndarray</code>) &#x2014;
A numpy array which cantains evidence texts.`,name:"block_records"},{anchor:"transformers.RealmRetriever.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>) &#x2014;
The tokenizer to encode retrieved texts.`,name:"tokenizer"}]}}),Lt=new z({props:{name:"block_has_answer",anchor:"transformers.RealmRetriever.block_has_answer",parameters:[{name:"concat_inputs",val:""},{name:"answer_ids",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/retrieval_realm.py#L130"}}),Mt=new J({}),St=new z({props:{name:"class transformers.RealmEmbedder",anchor:"transformers.RealmEmbedder",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1145",parametersDescription:[{anchor:"transformers.RealmEmbedder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),It=new z({props:{name:"forward",anchor:"transformers.RealmEmbedder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1159",parametersDescription:[{anchor:"transformers.RealmEmbedder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmEmbedder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmEmbedder.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmEmbedder.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmEmbedder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmEmbedder.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmEmbedder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmEmbedder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmEmbedder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmEmbedderOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>projected_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.retriever_proj_size)</code>) \u2014 Projected score.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Fe=new Rr({props:{$$slots:{default:[th]},$$scope:{ctx:N}}}),Dt=new Xe({props:{code:`from transformers import RealmTokenizer, RealmEmbedder
import torch

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-embedder")
model = RealmEmbedder.from_pretrained("google/realm-cc-news-pretrained-embedder")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

projected_score = outputs.projected_score`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmEmbedder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-embedder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmEmbedder.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-embedder&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>projected_score = outputs.projected_score`}}),Nt=new J({}),Ot=new z({props:{name:"class transformers.RealmScorer",anchor:"transformers.RealmScorer",parameters:[{name:"config",val:""},{name:"query_embedder",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1225",parametersDescription:[{anchor:"transformers.RealmScorer.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.RealmScorer.query_embedder",description:`<strong>query_embedder</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmEmbedder">RealmEmbedder</a>) &#x2014;
Embedder for input sequences. If not specified, it will use the same embedder as candidate sequences.`,name:"query_embedder"}]}}),Bt=new z({props:{name:"forward",anchor:"transformers.RealmScorer.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"candidate_input_ids",val:" = None"},{name:"candidate_attention_mask",val:" = None"},{name:"candidate_token_type_ids",val:" = None"},{name:"candidate_inputs_embeds",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1241",parametersDescription:[{anchor:"transformers.RealmScorer.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmScorer.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmScorer.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmScorer.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmScorer.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmScorer.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmScorer.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmScorer.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmScorer.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmScorer.forward.candidate_input_ids",description:`<strong>candidate_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>) &#x2014;
Indices of candidate input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"candidate_input_ids"},{anchor:"transformers.RealmScorer.forward.candidate_attention_mask",description:`<strong>candidate_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"candidate_attention_mask"},{anchor:"transformers.RealmScorer.forward.candidate_token_type_ids",description:`<strong>candidate_token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"candidate_token_type_ids"},{anchor:"transformers.RealmScorer.forward.candidate_inputs_embeds",description:`<strong>candidate_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size * num_candidates, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>candidate_input_ids</code> you can choose to directly pass an embedded
representation. This is useful if you want more control over how to convert <em>candidate_input_ids</em> indices
into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"candidate_inputs_embeds"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmScorerOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li><strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_candidates)</code>) \u2014 The relevance score of document candidates (before softmax).</li>
<li><strong>query_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.retriever_proj_size)</code>) \u2014 Query score derived from the query embedder.</li>
<li><strong>candidate_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_candidates, config.retriever_proj_size)</code>) \u2014 Candidate score derived from the embedder.</li>
</ul>
`}}),Ie=new Rr({props:{$$slots:{default:[nh]},$$scope:{ctx:N}}}),Qt=new Xe({props:{code:`import torch
from transformers import RealmTokenizer, RealmScorer

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-scorer")
model = RealmScorer.from_pretrained("google/realm-cc-news-pretrained-scorer", num_candidates=2)

# batch_size = 2, num_candidates = 2
input_texts = ["How are you?", "What is the item in the picture?"]
candidates_texts = [["Hello world!", "Nice to meet you!"], ["A cute cat.", "An adorable dog."]]

inputs = tokenizer(input_texts, return_tensors="pt")
candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=10, return_tensors="pt")

outputs = model(
    **inputs,
    candidate_input_ids=candidates_inputs.input_ids,
    candidate_attention_mask=candidates_inputs.attention_mask,
    candidate_token_type_ids=candidates_inputs.token_type_ids,
)
relevance_score = outputs.relevance_score`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmScorer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-scorer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmScorer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-scorer&quot;</span>, num_candidates=<span class="hljs-number">2</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_texts = [<span class="hljs-string">&quot;How are you?&quot;</span>, <span class="hljs-string">&quot;What is the item in the picture?&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>candidates_texts = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;A cute cat.&quot;</span>, <span class="hljs-string">&quot;An adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(input_texts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(
<span class="hljs-meta">... </span>    **inputs,
<span class="hljs-meta">... </span>    candidate_input_ids=candidates_inputs.input_ids,
<span class="hljs-meta">... </span>    candidate_attention_mask=candidates_inputs.attention_mask,
<span class="hljs-meta">... </span>    candidate_token_type_ids=candidates_inputs.token_type_ids,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_score = outputs.relevance_score`}}),Ht=new J({}),Vt=new z({props:{name:"class transformers.RealmKnowledgeAugEncoder",anchor:"transformers.RealmKnowledgeAugEncoder",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1372",parametersDescription:[{anchor:"transformers.RealmKnowledgeAugEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Gt=new z({props:{name:"forward",anchor:"transformers.RealmKnowledgeAugEncoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"relevance_score",val:" = None"},{name:"labels",val:" = None"},{name:"mlm_mask",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1391",parametersDescription:[{anchor:"transformers.RealmKnowledgeAugEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_candidates, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.relevance_score",description:`<strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_candidates)</code>, <em>optional</em>) &#x2014;
Relevance score derived from RealmScorer, must be specified if you want to compute the masked language
modeling loss.`,name:"relevance_score"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.RealmKnowledgeAugEncoder.forward.mlm_mask",description:`<strong>mlm_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid calculating joint loss on certain positions. If not specified, the loss will not be masked.
Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"mlm_mask"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Ne=new Rr({props:{$$slots:{default:[oh]},$$scope:{ctx:N}}}),Jt=new Xe({props:{code:`import torch
from transformers import RealmTokenizer, RealmKnowledgeAugEncoder

tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
model = RealmKnowledgeAugEncoder.from_pretrained(
    "google/realm-cc-news-pretrained-encoder", num_candidates=2
)

# batch_size = 2, num_candidates = 2
text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]

inputs = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmTokenizer, RealmKnowledgeAugEncoder

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmKnowledgeAugEncoder.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google/realm-cc-news-pretrained-encoder&quot;</span>, num_candidates=<span class="hljs-number">2</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># batch_size = 2, num_candidates = 2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text = [[<span class="hljs-string">&quot;Hello world!&quot;</span>, <span class="hljs-string">&quot;Nice to meet you!&quot;</span>], [<span class="hljs-string">&quot;The cute cat.&quot;</span>, <span class="hljs-string">&quot;The adorable dog.&quot;</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer.batch_encode_candidates(text, max_length=<span class="hljs-number">10</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Zt=new J({}),Yt=new z({props:{name:"class transformers.RealmReader",anchor:"transformers.RealmReader",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1520",parametersDescription:[{anchor:"transformers.RealmReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),nn=new z({props:{name:"forward",anchor:"transformers.RealmReader.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"position_ids",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"relevance_score",val:" = None"},{name:"block_mask",val:" = None"},{name:"start_positions",val:" = None"},{name:"end_positions",val:" = None"},{name:"has_answers",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1534",parametersDescription:[{anchor:"transformers.RealmReader.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmReader.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmReader.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(reader_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.RealmReader.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.RealmReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(reader_beam_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.RealmReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.RealmReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.RealmReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.RealmReader.forward.relevance_score",description:`<strong>relevance_score</strong> (<code>torch.FloatTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Relevance score, which must be specified if you want to compute the logits and marginal log loss.`,name:"relevance_score"},{anchor:"transformers.RealmReader.forward.block_mask",description:`<strong>block_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(searcher_beam_size, sequence_length)</code>, <em>optional</em>) &#x2014;
The mask of the evidence block, which must be specified if you want to compute the logits and marginal log
loss.`,name:"block_mask"},{anchor:"transformers.RealmReader.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.RealmReader.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"},{anchor:"transformers.RealmReader.forward.has_answers",description:`<strong>has_answers</strong> (<code>torch.BoolTensor</code> of shape <code>(searcher_beam_size,)</code>, <em>optional</em>) &#x2014;
Whether or not the evidence block has answer(s).`,name:"has_answers"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmReaderOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Total loss.</p>
</li>
<li>
<p><strong>retriever_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Retriever loss.</p>
</li>
<li>
<p><strong>reader_loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>start_positions</code>, <code>end_positions</code>, <code>has_answers</code> are provided) \u2014 Reader loss.</p>
</li>
<li>
<p><strong>retriever_correct</strong> (<code>torch.BoolTensor</code> of shape <code>(config.searcher_beam_size,)</code>, <em>optional</em>) \u2014 Whether or not an evidence block contains answer.</p>
</li>
<li>
<p><strong>reader_correct</strong> (<code>torch.BoolTensor</code> of shape <code>(config.reader_beam_size, num_candidates)</code>, <em>optional</em>) \u2014 Whether or not a span candidate contains answer.</p>
</li>
<li>
<p><strong>block_idx</strong> (<code>torch.LongTensor</code> of shape <code>()</code>) \u2014 The index of the retrieved evidence block in which the predicted answer is most likely.</p>
</li>
<li>
<p><strong>candidate</strong> (<code>torch.LongTensor</code> of shape <code>()</code>) \u2014 The index of the retrieved span candidates in which the predicted answer is most likely.</p>
</li>
<li>
<p><strong>start_pos</strong> (<code>torch.IntTensor</code> of shape <code>()</code>) \u2014 Predicted answer starting position in <em>RealmReader</em>\u2019s inputs.
*<em> (<code>torch.IntTensor</code> of shape <code>()</code>) \u2014 Predicted answer ending position in </em>RealmReader*\u2018s inputs.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),We=new Rr({props:{$$slots:{default:[rh]},$$scope:{ctx:N}}}),on=new J({}),rn=new z({props:{name:"class transformers.RealmForOpenQA",anchor:"transformers.RealmForOpenQA",parameters:[{name:"config",val:""},{name:"retriever",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1727",parametersDescription:[{anchor:"transformers.RealmForOpenQA.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),sn=new z({props:{name:"block_embedding_to",anchor:"transformers.RealmForOpenQA.block_embedding_to",parameters:[{name:"device",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1750",parametersDescription:[{anchor:"transformers.RealmForOpenQA.block_embedding_to.device",description:`<strong>device</strong> (<code>str</code> or <code>torch.device</code>) &#x2014;
The device to which <code>self.block_emb</code> will be sent.`,name:"device"}]}}),ln=new z({props:{name:"forward",anchor:"transformers.RealmForOpenQA.forward",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"token_type_ids",val:" = None"},{name:"answer_ids",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/realm/modeling_realm.py#L1760",parametersDescription:[{anchor:"transformers.RealmForOpenQA.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(1, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a>. See <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.RealmForOpenQA.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(1, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.RealmForOpenQA.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(1, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token (should not be used in this model by design).</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.RealmForOpenQA.forward.answer_ids",description:`<strong>answer_ids</strong> (<code>list</code> of shape <code>(num_answers, answer_length)</code>, <em>optional</em>) &#x2014;
Answer ids for computing the marginal log-likelihood loss. Indices should be in <code>[-1, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-1</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"answer_ids"},{anchor:"transformers.RealmForOpenQA.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <code>transformers.models.realm.modeling_realm.RealmForOpenQAOutput</code>or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig"
>RealmConfig</a>) and inputs.</p>
<ul>
<li><strong>reader_output</strong> (<code>dict</code>) \u2014 Reader output.</li>
<li><strong>predicted_answer_ids</strong> (<code>torch.LongTensor</code> of shape <code>(answer_sequence_length)</code>) \u2014 Predicted answer ids.</li>
</ul>
`}}),He=new Rr({props:{$$slots:{default:[ah]},$$scope:{ctx:N}}}),cn=new Xe({props:{code:`import torch
from transformers import RealmForOpenQA, RealmRetriever, RealmTokenizer

retriever = RealmRetriever.from_pretrained("google/realm-orqa-nq-openqa")
tokenizer = RealmTokenizer.from_pretrained("google/realm-orqa-nq-openqa")
model = RealmForOpenQA.from_pretrained("google/realm-orqa-nq-openqa", retriever=retriever)

question = "Who is the pioneer in modern computer science?"
question_ids = tokenizer([question], return_tensors="pt")
answer_ids = tokenizer(
    ["alan mathison turing"],
    add_special_tokens=False,
    return_token_type_ids=False,
    return_attention_mask=False,
).input_ids

reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=False)
predicted_answer = tokenizer.decode(predicted_answer_ids)
loss = reader_output.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> RealmForOpenQA, RealmRetriever, RealmTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>retriever = RealmRetriever.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = RealmTokenizer.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = RealmForOpenQA.from_pretrained(<span class="hljs-string">&quot;google/realm-orqa-nq-openqa&quot;</span>, retriever=retriever)

<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;Who is the pioneer in modern computer science?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>question_ids = tokenizer([question], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_ids = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;alan mathison turing&quot;</span>],
<span class="hljs-meta">... </span>    add_special_tokens=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    return_token_type_ids=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    return_attention_mask=<span class="hljs-literal">False</span>,
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_answer = tokenizer.decode(predicted_answer_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = reader_output.loss`}}),{c(){h=o("meta"),$=d(),w=o("h1"),T=o("a"),y=o("span"),f(R.$$.fragment),b=d(),q=o("span"),ya=s("REALM"),Tr=d(),se=o("h2"),ye=o("a"),ro=o("span"),f(Ge.$$.fragment),Ea=d(),ao=o("span"),za=s("Overview"),$r=d(),Ee=o("p"),qa=s("The REALM model was proposed in "),Je=o("a"),xa=s("REALM: Retrieval-Augmented Language Model Pre-Training"),Aa=s(` by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It\u2019s a
retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then
utilizes retrieved documents to process question answering tasks.`),yr=d(),pn=o("p"),Pa=s("The abstract from the paper is the following:"),Er=d(),fn=o("p"),so=o("em"),ja=s(`Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks
such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we
augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend
over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the
first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language
modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We
demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the
challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both
explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous
methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as
interpretability and modularity.`),zr=d(),Z=o("p"),La=s("This model was contributed by "),Ze=o("a"),Ma=s("qqaatw"),Sa=s(`. The original code can be found
`),Ye=o("a"),Fa=s("here"),Ca=s("."),qr=d(),ie=o("h2"),ze=o("a"),io=o("span"),f(et.$$.fragment),Ia=d(),lo=o("span"),Da=s("RealmConfig"),xr=d(),x=o("div"),f(tt.$$.fragment),Na=d(),co=o("p"),Oa=s("This is the configuration class to store the configuration of"),Wa=d(),L=o("ol"),mo=o("li"),un=o("a"),Ka=s("RealmEmbedder"),Ba=d(),ho=o("li"),gn=o("a"),Qa=s("RealmScorer"),Ha=d(),po=o("li"),_n=o("a"),Va=s("RealmKnowledgeAugEncoder"),Ua=d(),fo=o("li"),kn=o("a"),Xa=s("RealmRetriever"),Ga=d(),uo=o("li"),vn=o("a"),Ja=s("RealmReader"),Za=d(),go=o("li"),wn=o("a"),Ya=s("RealmForOpenQA"),es=d(),nt=o("p"),ts=s(`It is used to instantiate an REALM model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the REALM
`),ot=o("a"),ns=s("realm-cc-news-pretrained"),os=s(" architecture."),rs=d(),de=o("p"),as=s("Configuration objects inherit from "),bn=o("a"),ss=s("PretrainedConfig"),is=s(` and can be used to control the model outputs. Read the
documentation from `),Rn=o("a"),ds=s("PretrainedConfig"),ls=s(" for more information."),cs=d(),_o=o("p"),ms=s("Example:"),hs=d(),f(rt.$$.fragment),Ar=d(),le=o("h2"),qe=o("a"),ko=o("span"),f(at.$$.fragment),ps=d(),vo=o("span"),fs=s("RealmTokenizer"),Pr=d(),E=o("div"),f(st.$$.fragment),us=d(),wo=o("p"),gs=s("Construct a REALM tokenizer."),_s=d(),xe=o("p"),Tn=o("a"),ks=s("RealmTokenizer"),vs=s(" is identical to "),$n=o("a"),ws=s("BertTokenizer"),bs=s(` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),Rs=d(),it=o("p"),Ts=s("This tokenizer inherits from "),yn=o("a"),$s=s("PreTrainedTokenizer"),ys=s(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Es=d(),Y=o("div"),f(dt.$$.fragment),zs=d(),bo=o("p"),qs=s(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A REALM sequence has the following format:`),xs=d(),lt=o("ul"),En=o("li"),As=s("single sequence: "),Ro=o("code"),Ps=s("[CLS] X [SEP]"),js=d(),zn=o("li"),Ls=s("pair of sequences: "),To=o("code"),Ms=s("[CLS] A [SEP] B [SEP]"),Ss=d(),Ae=o("div"),f(ct.$$.fragment),Fs=d(),mt=o("p"),Cs=s(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),$o=o("code"),Is=s("prepare_for_model"),Ds=s(" method."),Ns=d(),W=o("div"),f(ht.$$.fragment),Os=d(),yo=o("p"),Ws=s(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence
pair mask has the following format:`),Ks=d(),f(pt.$$.fragment),Bs=d(),ce=o("p"),Qs=s("If "),Eo=o("code"),Hs=s("token_ids_1"),Vs=s(" is "),zo=o("code"),Us=s("None"),Xs=s(", this method only returns the first portion of the mask (0s)."),Gs=d(),qn=o("div"),f(ft.$$.fragment),Js=d(),P=o("div"),f(ut.$$.fragment),Zs=d(),gt=o("p"),Ys=s("Encode a batch of text or text pair. This method is similar to regular "),qo=o("strong"),ei=s("call"),ti=s(` method but has the following
differences:`),ni=d(),me=o("ol"),xo=o("li"),oi=s("Handle additional num_candidate axis. (batch_size, num_candidates, text)"),ri=d(),_t=o("li"),ai=s("Always pad the sequences to "),Ao=o("em"),si=s("max_length"),ii=s("."),di=d(),kt=o("li"),li=s("Must specify "),Po=o("em"),ci=s("max_length"),mi=s(" in order to stack packs of candidates into a batch."),hi=d(),vt=o("ul"),xn=o("li"),pi=s("single sequence: "),jo=o("code"),fi=s("[CLS] X [SEP]"),ui=d(),An=o("li"),gi=s("pair of sequences: "),Lo=o("code"),_i=s("[CLS] A [SEP] B [SEP]"),ki=d(),Mo=o("p"),vi=s("Example:"),wi=d(),f(wt.$$.fragment),jr=d(),he=o("h2"),Pe=o("a"),So=o("span"),f(bt.$$.fragment),bi=d(),Fo=o("span"),Ri=s("RealmTokenizerFast"),Lr=d(),M=o("div"),f(Rt.$$.fragment),Ti=d(),Tt=o("p"),$i=s("Construct a \u201Cfast\u201D REALM tokenizer (backed by HuggingFace\u2019s "),Co=o("em"),yi=s("tokenizers"),Ei=s(" library). Based on WordPiece."),zi=d(),je=o("p"),Pn=o("a"),qi=s("RealmTokenizerFast"),xi=s(" is identical to "),jn=o("a"),Ai=s("BertTokenizerFast"),Pi=s(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),ji=d(),$t=o("p"),Li=s("This tokenizer inherits from "),Ln=o("a"),Mi=s("PreTrainedTokenizerFast"),Si=s(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Fi=d(),j=o("div"),f(yt.$$.fragment),Ci=d(),Et=o("p"),Ii=s("Encode a batch of text or text pair. This method is similar to regular "),Io=o("strong"),Di=s("call"),Ni=s(` method but has the following
differences:`),Oi=d(),pe=o("ol"),Do=o("li"),Wi=s("Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Ki=d(),zt=o("li"),Bi=s("Always pad the sequences to "),No=o("em"),Qi=s("max_length"),Hi=s("."),Vi=d(),qt=o("li"),Ui=s("Must specify "),Oo=o("em"),Xi=s("max_length"),Gi=s(" in order to stack packs of candidates into a batch."),Ji=d(),xt=o("ul"),Mn=o("li"),Zi=s("single sequence: "),Wo=o("code"),Yi=s("[CLS] X [SEP]"),ed=d(),Sn=o("li"),td=s("pair of sequences: "),Ko=o("code"),nd=s("[CLS] A [SEP] B [SEP]"),od=d(),Bo=o("p"),rd=s("Example:"),ad=d(),f(At.$$.fragment),Mr=d(),fe=o("h2"),Le=o("a"),Qo=o("span"),f(Pt.$$.fragment),sd=d(),Ho=o("span"),id=s("RealmRetriever"),Sr=d(),H=o("div"),f(jt.$$.fragment),dd=d(),Vo=o("p"),ld=s(`The retriever of REALM outputting the retrieved evidence block and whether the block has answers as well as answer
positions.\u201D`),cd=d(),Me=o("div"),f(Lt.$$.fragment),md=d(),Uo=o("p"),hd=s("check if retrieved_blocks has answers."),Fr=d(),ue=o("h2"),Se=o("a"),Xo=o("span"),f(Mt.$$.fragment),pd=d(),Go=o("span"),fd=s("RealmEmbedder"),Cr=d(),V=o("div"),f(St.$$.fragment),ud=d(),Ft=o("p"),gd=s(`The embedder of REALM outputting projected score that will be used to calculate relevance score.
This model is a PyTorch `),Ct=o("a"),_d=s("torch.nn.Module"),kd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),vd=d(),S=o("div"),f(It.$$.fragment),wd=d(),ge=o("p"),bd=s("The "),Fn=o("a"),Rd=s("RealmEmbedder"),Td=s(" forward method, overrides the "),Jo=o("code"),$d=s("__call__"),yd=s(" special method."),Ed=d(),f(Fe.$$.fragment),zd=d(),Zo=o("p"),qd=s("Example:"),xd=d(),f(Dt.$$.fragment),Ir=d(),_e=o("h2"),Ce=o("a"),Yo=o("span"),f(Nt.$$.fragment),Ad=d(),er=o("span"),Pd=s("RealmScorer"),Dr=d(),U=o("div"),f(Ot.$$.fragment),jd=d(),Wt=o("p"),Ld=s(`The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).
This model is a PyTorch `),Kt=o("a"),Md=s("torch.nn.Module"),Sd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Fd=d(),F=o("div"),f(Bt.$$.fragment),Cd=d(),ke=o("p"),Id=s("The "),Cn=o("a"),Dd=s("RealmScorer"),Nd=s(" forward method, overrides the "),tr=o("code"),Od=s("__call__"),Wd=s(" special method."),Kd=d(),f(Ie.$$.fragment),Bd=d(),nr=o("p"),Qd=s("Example:"),Hd=d(),f(Qt.$$.fragment),Nr=d(),ve=o("h2"),De=o("a"),or=o("span"),f(Ht.$$.fragment),Vd=d(),rr=o("span"),Ud=s("RealmKnowledgeAugEncoder"),Or=d(),X=o("div"),f(Vt.$$.fragment),Xd=d(),Ut=o("p"),Gd=s(`The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood loss.
This model is a PyTorch `),Xt=o("a"),Jd=s("torch.nn.Module"),Zd=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Yd=d(),C=o("div"),f(Gt.$$.fragment),el=d(),we=o("p"),tl=s("The "),In=o("a"),nl=s("RealmKnowledgeAugEncoder"),ol=s(" forward method, overrides the "),ar=o("code"),rl=s("__call__"),al=s(" special method."),sl=d(),f(Ne.$$.fragment),il=d(),sr=o("p"),dl=s("Example:"),ll=d(),f(Jt.$$.fragment),Wr=d(),be=o("h2"),Oe=o("a"),ir=o("span"),f(Zt.$$.fragment),cl=d(),dr=o("span"),ml=s("RealmReader"),Kr=d(),G=o("div"),f(Yt.$$.fragment),hl=d(),en=o("p"),pl=s(`The reader of REALM.
This model is a PyTorch `),tn=o("a"),fl=s("torch.nn.Module"),ul=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),gl=d(),ee=o("div"),f(nn.$$.fragment),_l=d(),Re=o("p"),kl=s("The "),Dn=o("a"),vl=s("RealmReader"),wl=s(" forward method, overrides the "),lr=o("code"),bl=s("__call__"),Rl=s(" special method."),Tl=d(),f(We.$$.fragment),Br=d(),Te=o("h2"),Ke=o("a"),cr=o("span"),f(on.$$.fragment),$l=d(),mr=o("span"),yl=s("RealmForOpenQA"),Qr=d(),O=o("div"),f(rn.$$.fragment),El=d(),Be=o("p"),hr=o("code"),zl=s("RealmForOpenQA"),ql=s(` for end-to-end open domain question answering.
This model is a PyTorch `),an=o("a"),xl=s("torch.nn.Module"),Al=s(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),Pl=d(),Qe=o("div"),f(sn.$$.fragment),jl=d(),dn=o("p"),Ll=s("Send "),pr=o("code"),Ml=s("self.block_emb"),Sl=s(" to a specific device."),Fl=d(),I=o("div"),f(ln.$$.fragment),Cl=d(),$e=o("p"),Il=s("The "),Nn=o("a"),Dl=s("RealmForOpenQA"),Nl=s(" forward method, overrides the "),fr=o("code"),Ol=s("__call__"),Wl=s(" special method."),Kl=d(),f(He.$$.fragment),Bl=d(),ur=o("p"),Ql=s("Example:"),Hl=d(),f(cn.$$.fragment),this.h()},l(t){const m=eh('[data-svelte="svelte-1phssyn"]',document.head);h=r(m,"META",{name:!0,content:!0}),m.forEach(n),$=l(t),w=r(t,"H1",{class:!0});var mn=a(w);T=r(mn,"A",{id:!0,class:!0,href:!0});var gr=a(T);y=r(gr,"SPAN",{});var _r=a(y);u(R.$$.fragment,_r),_r.forEach(n),gr.forEach(n),b=l(mn),q=r(mn,"SPAN",{});var kr=a(q);ya=i(kr,"REALM"),kr.forEach(n),mn.forEach(n),Tr=l(t),se=r(t,"H2",{class:!0});var hn=a(se);ye=r(hn,"A",{id:!0,class:!0,href:!0});var Yl=a(ye);ro=r(Yl,"SPAN",{});var ec=a(ro);u(Ge.$$.fragment,ec),ec.forEach(n),Yl.forEach(n),Ea=l(hn),ao=r(hn,"SPAN",{});var tc=a(ao);za=i(tc,"Overview"),tc.forEach(n),hn.forEach(n),$r=l(t),Ee=r(t,"P",{});var Vr=a(Ee);qa=i(Vr,"The REALM model was proposed in "),Je=r(Vr,"A",{href:!0,rel:!0});var nc=a(Je);xa=i(nc,"REALM: Retrieval-Augmented Language Model Pre-Training"),nc.forEach(n),Aa=i(Vr,` by Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat and Ming-Wei Chang. It\u2019s a
retrieval-augmented language model that firstly retrieves documents from a textual knowledge corpus and then
utilizes retrieved documents to process question answering tasks.`),Vr.forEach(n),yr=l(t),pn=r(t,"P",{});var oc=a(pn);Pa=i(oc,"The abstract from the paper is the following:"),oc.forEach(n),Er=l(t),fn=r(t,"P",{});var rc=a(fn);so=r(rc,"EM",{});var ac=a(so);ja=i(ac,`Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks
such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network,
requiring ever-larger networks to cover more facts. To capture knowledge in a more modular and interpretable way, we
augment language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend
over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the
first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language
modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents. We
demonstrate the effectiveness of Retrieval-Augmented Language Model pre-training (REALM) by fine-tuning on the
challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both
explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous
methods by a significant margin (4-16% absolute accuracy), while also providing qualitative benefits such as
interpretability and modularity.`),ac.forEach(n),rc.forEach(n),zr=l(t),Z=r(t,"P",{});var On=a(Z);La=i(On,"This model was contributed by "),Ze=r(On,"A",{href:!0,rel:!0});var sc=a(Ze);Ma=i(sc,"qqaatw"),sc.forEach(n),Sa=i(On,`. The original code can be found
`),Ye=r(On,"A",{href:!0,rel:!0});var ic=a(Ye);Fa=i(ic,"here"),ic.forEach(n),Ca=i(On,"."),On.forEach(n),qr=l(t),ie=r(t,"H2",{class:!0});var Ur=a(ie);ze=r(Ur,"A",{id:!0,class:!0,href:!0});var dc=a(ze);io=r(dc,"SPAN",{});var lc=a(io);u(et.$$.fragment,lc),lc.forEach(n),dc.forEach(n),Ia=l(Ur),lo=r(Ur,"SPAN",{});var cc=a(lo);Da=i(cc,"RealmConfig"),cc.forEach(n),Ur.forEach(n),xr=l(t),x=r(t,"DIV",{class:!0});var D=a(x);u(tt.$$.fragment,D),Na=l(D),co=r(D,"P",{});var mc=a(co);Oa=i(mc,"This is the configuration class to store the configuration of"),mc.forEach(n),Wa=l(D),L=r(D,"OL",{});var K=a(L);mo=r(K,"LI",{});var hc=a(mo);un=r(hc,"A",{href:!0});var pc=a(un);Ka=i(pc,"RealmEmbedder"),pc.forEach(n),hc.forEach(n),Ba=l(K),ho=r(K,"LI",{});var fc=a(ho);gn=r(fc,"A",{href:!0});var uc=a(gn);Qa=i(uc,"RealmScorer"),uc.forEach(n),fc.forEach(n),Ha=l(K),po=r(K,"LI",{});var gc=a(po);_n=r(gc,"A",{href:!0});var _c=a(_n);Va=i(_c,"RealmKnowledgeAugEncoder"),_c.forEach(n),gc.forEach(n),Ua=l(K),fo=r(K,"LI",{});var kc=a(fo);kn=r(kc,"A",{href:!0});var vc=a(kn);Xa=i(vc,"RealmRetriever"),vc.forEach(n),kc.forEach(n),Ga=l(K),uo=r(K,"LI",{});var wc=a(uo);vn=r(wc,"A",{href:!0});var bc=a(vn);Ja=i(bc,"RealmReader"),bc.forEach(n),wc.forEach(n),Za=l(K),go=r(K,"LI",{});var Rc=a(go);wn=r(Rc,"A",{href:!0});var Tc=a(wn);Ya=i(Tc,"RealmForOpenQA"),Tc.forEach(n),Rc.forEach(n),K.forEach(n),es=l(D),nt=r(D,"P",{});var Xr=a(nt);ts=i(Xr,`It is used to instantiate an REALM model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the REALM
`),ot=r(Xr,"A",{href:!0,rel:!0});var $c=a(ot);ns=i($c,"realm-cc-news-pretrained"),$c.forEach(n),os=i(Xr," architecture."),Xr.forEach(n),rs=l(D),de=r(D,"P",{});var Wn=a(de);as=i(Wn,"Configuration objects inherit from "),bn=r(Wn,"A",{href:!0});var yc=a(bn);ss=i(yc,"PretrainedConfig"),yc.forEach(n),is=i(Wn,` and can be used to control the model outputs. Read the
documentation from `),Rn=r(Wn,"A",{href:!0});var Ec=a(Rn);ds=i(Ec,"PretrainedConfig"),Ec.forEach(n),ls=i(Wn," for more information."),Wn.forEach(n),cs=l(D),_o=r(D,"P",{});var zc=a(_o);ms=i(zc,"Example:"),zc.forEach(n),hs=l(D),u(rt.$$.fragment,D),D.forEach(n),Ar=l(t),le=r(t,"H2",{class:!0});var Gr=a(le);qe=r(Gr,"A",{id:!0,class:!0,href:!0});var qc=a(qe);ko=r(qc,"SPAN",{});var xc=a(ko);u(at.$$.fragment,xc),xc.forEach(n),qc.forEach(n),ps=l(Gr),vo=r(Gr,"SPAN",{});var Ac=a(vo);fs=i(Ac,"RealmTokenizer"),Ac.forEach(n),Gr.forEach(n),Pr=l(t),E=r(t,"DIV",{class:!0});var A=a(E);u(st.$$.fragment,A),us=l(A),wo=r(A,"P",{});var Pc=a(wo);gs=i(Pc,"Construct a REALM tokenizer."),Pc.forEach(n),_s=l(A),xe=r(A,"P",{});var vr=a(xe);Tn=r(vr,"A",{href:!0});var jc=a(Tn);ks=i(jc,"RealmTokenizer"),jc.forEach(n),vs=i(vr," is identical to "),$n=r(vr,"A",{href:!0});var Lc=a($n);ws=i(Lc,"BertTokenizer"),Lc.forEach(n),bs=i(vr,` and runs end-to-end tokenization: punctuation splitting and
wordpiece.`),vr.forEach(n),Rs=l(A),it=r(A,"P",{});var Jr=a(it);Ts=i(Jr,"This tokenizer inherits from "),yn=r(Jr,"A",{href:!0});var Mc=a(yn);$s=i(Mc,"PreTrainedTokenizer"),Mc.forEach(n),ys=i(Jr,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Jr.forEach(n),Es=l(A),Y=r(A,"DIV",{class:!0});var Kn=a(Y);u(dt.$$.fragment,Kn),zs=l(Kn),bo=r(Kn,"P",{});var Sc=a(bo);qs=i(Sc,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A REALM sequence has the following format:`),Sc.forEach(n),xs=l(Kn),lt=r(Kn,"UL",{});var Zr=a(lt);En=r(Zr,"LI",{});var Vl=a(En);As=i(Vl,"single sequence: "),Ro=r(Vl,"CODE",{});var Fc=a(Ro);Ps=i(Fc,"[CLS] X [SEP]"),Fc.forEach(n),Vl.forEach(n),js=l(Zr),zn=r(Zr,"LI",{});var Ul=a(zn);Ls=i(Ul,"pair of sequences: "),To=r(Ul,"CODE",{});var Cc=a(To);Ms=i(Cc,"[CLS] A [SEP] B [SEP]"),Cc.forEach(n),Ul.forEach(n),Zr.forEach(n),Kn.forEach(n),Ss=l(A),Ae=r(A,"DIV",{class:!0});var Yr=a(Ae);u(ct.$$.fragment,Yr),Fs=l(Yr),mt=r(Yr,"P",{});var ea=a(mt);Cs=i(ea,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),$o=r(ea,"CODE",{});var Ic=a($o);Is=i(Ic,"prepare_for_model"),Ic.forEach(n),Ds=i(ea," method."),ea.forEach(n),Yr.forEach(n),Ns=l(A),W=r(A,"DIV",{class:!0});var Ve=a(W);u(ht.$$.fragment,Ve),Os=l(Ve),yo=r(Ve,"P",{});var Dc=a(yo);Ws=i(Dc,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A REALM sequence
pair mask has the following format:`),Dc.forEach(n),Ks=l(Ve),u(pt.$$.fragment,Ve),Bs=l(Ve),ce=r(Ve,"P",{});var Bn=a(ce);Qs=i(Bn,"If "),Eo=r(Bn,"CODE",{});var Nc=a(Eo);Hs=i(Nc,"token_ids_1"),Nc.forEach(n),Vs=i(Bn," is "),zo=r(Bn,"CODE",{});var Oc=a(zo);Us=i(Oc,"None"),Oc.forEach(n),Xs=i(Bn,", this method only returns the first portion of the mask (0s)."),Bn.forEach(n),Ve.forEach(n),Gs=l(A),qn=r(A,"DIV",{class:!0});var Wc=a(qn);u(ft.$$.fragment,Wc),Wc.forEach(n),Js=l(A),P=r(A,"DIV",{class:!0});var B=a(P);u(ut.$$.fragment,B),Zs=l(B),gt=r(B,"P",{});var ta=a(gt);Ys=i(ta,"Encode a batch of text or text pair. This method is similar to regular "),qo=r(ta,"STRONG",{});var Kc=a(qo);ei=i(Kc,"call"),Kc.forEach(n),ti=i(ta,` method but has the following
differences:`),ta.forEach(n),ni=l(B),me=r(B,"OL",{});var Qn=a(me);xo=r(Qn,"LI",{});var Bc=a(xo);oi=i(Bc,"Handle additional num_candidate axis. (batch_size, num_candidates, text)"),Bc.forEach(n),ri=l(Qn),_t=r(Qn,"LI",{});var na=a(_t);ai=i(na,"Always pad the sequences to "),Ao=r(na,"EM",{});var Qc=a(Ao);si=i(Qc,"max_length"),Qc.forEach(n),ii=i(na,"."),na.forEach(n),di=l(Qn),kt=r(Qn,"LI",{});var oa=a(kt);li=i(oa,"Must specify "),Po=r(oa,"EM",{});var Hc=a(Po);ci=i(Hc,"max_length"),Hc.forEach(n),mi=i(oa," in order to stack packs of candidates into a batch."),oa.forEach(n),Qn.forEach(n),hi=l(B),vt=r(B,"UL",{});var ra=a(vt);xn=r(ra,"LI",{});var Xl=a(xn);pi=i(Xl,"single sequence: "),jo=r(Xl,"CODE",{});var Vc=a(jo);fi=i(Vc,"[CLS] X [SEP]"),Vc.forEach(n),Xl.forEach(n),ui=l(ra),An=r(ra,"LI",{});var Gl=a(An);gi=i(Gl,"pair of sequences: "),Lo=r(Gl,"CODE",{});var Uc=a(Lo);_i=i(Uc,"[CLS] A [SEP] B [SEP]"),Uc.forEach(n),Gl.forEach(n),ra.forEach(n),ki=l(B),Mo=r(B,"P",{});var Xc=a(Mo);vi=i(Xc,"Example:"),Xc.forEach(n),wi=l(B),u(wt.$$.fragment,B),B.forEach(n),A.forEach(n),jr=l(t),he=r(t,"H2",{class:!0});var aa=a(he);Pe=r(aa,"A",{id:!0,class:!0,href:!0});var Gc=a(Pe);So=r(Gc,"SPAN",{});var Jc=a(So);u(bt.$$.fragment,Jc),Jc.forEach(n),Gc.forEach(n),bi=l(aa),Fo=r(aa,"SPAN",{});var Zc=a(Fo);Ri=i(Zc,"RealmTokenizerFast"),Zc.forEach(n),aa.forEach(n),Lr=l(t),M=r(t,"DIV",{class:!0});var te=a(M);u(Rt.$$.fragment,te),Ti=l(te),Tt=r(te,"P",{});var sa=a(Tt);$i=i(sa,"Construct a \u201Cfast\u201D REALM tokenizer (backed by HuggingFace\u2019s "),Co=r(sa,"EM",{});var Yc=a(Co);yi=i(Yc,"tokenizers"),Yc.forEach(n),Ei=i(sa," library). Based on WordPiece."),sa.forEach(n),zi=l(te),je=r(te,"P",{});var wr=a(je);Pn=r(wr,"A",{href:!0});var em=a(Pn);qi=i(em,"RealmTokenizerFast"),em.forEach(n),xi=i(wr," is identical to "),jn=r(wr,"A",{href:!0});var tm=a(jn);Ai=i(tm,"BertTokenizerFast"),tm.forEach(n),Pi=i(wr,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),wr.forEach(n),ji=l(te),$t=r(te,"P",{});var ia=a($t);Li=i(ia,"This tokenizer inherits from "),Ln=r(ia,"A",{href:!0});var nm=a(Ln);Mi=i(nm,"PreTrainedTokenizerFast"),nm.forEach(n),Si=i(ia,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),ia.forEach(n),Fi=l(te),j=r(te,"DIV",{class:!0});var Q=a(j);u(yt.$$.fragment,Q),Ci=l(Q),Et=r(Q,"P",{});var da=a(Et);Ii=i(da,"Encode a batch of text or text pair. This method is similar to regular "),Io=r(da,"STRONG",{});var om=a(Io);Di=i(om,"call"),om.forEach(n),Ni=i(da,` method but has the following
differences:`),da.forEach(n),Oi=l(Q),pe=r(Q,"OL",{});var Hn=a(pe);Do=r(Hn,"LI",{});var rm=a(Do);Wi=i(rm,"Handle additional num_candidate axis. (batch_size, num_candidates, text)"),rm.forEach(n),Ki=l(Hn),zt=r(Hn,"LI",{});var la=a(zt);Bi=i(la,"Always pad the sequences to "),No=r(la,"EM",{});var am=a(No);Qi=i(am,"max_length"),am.forEach(n),Hi=i(la,"."),la.forEach(n),Vi=l(Hn),qt=r(Hn,"LI",{});var ca=a(qt);Ui=i(ca,"Must specify "),Oo=r(ca,"EM",{});var sm=a(Oo);Xi=i(sm,"max_length"),sm.forEach(n),Gi=i(ca," in order to stack packs of candidates into a batch."),ca.forEach(n),Hn.forEach(n),Ji=l(Q),xt=r(Q,"UL",{});var ma=a(xt);Mn=r(ma,"LI",{});var Jl=a(Mn);Zi=i(Jl,"single sequence: "),Wo=r(Jl,"CODE",{});var im=a(Wo);Yi=i(im,"[CLS] X [SEP]"),im.forEach(n),Jl.forEach(n),ed=l(ma),Sn=r(ma,"LI",{});var Zl=a(Sn);td=i(Zl,"pair of sequences: "),Ko=r(Zl,"CODE",{});var dm=a(Ko);nd=i(dm,"[CLS] A [SEP] B [SEP]"),dm.forEach(n),Zl.forEach(n),ma.forEach(n),od=l(Q),Bo=r(Q,"P",{});var lm=a(Bo);rd=i(lm,"Example:"),lm.forEach(n),ad=l(Q),u(At.$$.fragment,Q),Q.forEach(n),te.forEach(n),Mr=l(t),fe=r(t,"H2",{class:!0});var ha=a(fe);Le=r(ha,"A",{id:!0,class:!0,href:!0});var cm=a(Le);Qo=r(cm,"SPAN",{});var mm=a(Qo);u(Pt.$$.fragment,mm),mm.forEach(n),cm.forEach(n),sd=l(ha),Ho=r(ha,"SPAN",{});var hm=a(Ho);id=i(hm,"RealmRetriever"),hm.forEach(n),ha.forEach(n),Sr=l(t),H=r(t,"DIV",{class:!0});var Vn=a(H);u(jt.$$.fragment,Vn),dd=l(Vn),Vo=r(Vn,"P",{});var pm=a(Vo);ld=i(pm,`The retriever of REALM outputting the retrieved evidence block and whether the block has answers as well as answer
positions.\u201D`),pm.forEach(n),cd=l(Vn),Me=r(Vn,"DIV",{class:!0});var pa=a(Me);u(Lt.$$.fragment,pa),md=l(pa),Uo=r(pa,"P",{});var fm=a(Uo);hd=i(fm,"check if retrieved_blocks has answers."),fm.forEach(n),pa.forEach(n),Vn.forEach(n),Fr=l(t),ue=r(t,"H2",{class:!0});var fa=a(ue);Se=r(fa,"A",{id:!0,class:!0,href:!0});var um=a(Se);Xo=r(um,"SPAN",{});var gm=a(Xo);u(Mt.$$.fragment,gm),gm.forEach(n),um.forEach(n),pd=l(fa),Go=r(fa,"SPAN",{});var _m=a(Go);fd=i(_m,"RealmEmbedder"),_m.forEach(n),fa.forEach(n),Cr=l(t),V=r(t,"DIV",{class:!0});var Un=a(V);u(St.$$.fragment,Un),ud=l(Un),Ft=r(Un,"P",{});var ua=a(Ft);gd=i(ua,`The embedder of REALM outputting projected score that will be used to calculate relevance score.
This model is a PyTorch `),Ct=r(ua,"A",{href:!0,rel:!0});var km=a(Ct);_d=i(km,"torch.nn.Module"),km.forEach(n),kd=i(ua,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ua.forEach(n),vd=l(Un),S=r(Un,"DIV",{class:!0});var ne=a(S);u(It.$$.fragment,ne),wd=l(ne),ge=r(ne,"P",{});var Xn=a(ge);bd=i(Xn,"The "),Fn=r(Xn,"A",{href:!0});var vm=a(Fn);Rd=i(vm,"RealmEmbedder"),vm.forEach(n),Td=i(Xn," forward method, overrides the "),Jo=r(Xn,"CODE",{});var wm=a(Jo);$d=i(wm,"__call__"),wm.forEach(n),yd=i(Xn," special method."),Xn.forEach(n),Ed=l(ne),u(Fe.$$.fragment,ne),zd=l(ne),Zo=r(ne,"P",{});var bm=a(Zo);qd=i(bm,"Example:"),bm.forEach(n),xd=l(ne),u(Dt.$$.fragment,ne),ne.forEach(n),Un.forEach(n),Ir=l(t),_e=r(t,"H2",{class:!0});var ga=a(_e);Ce=r(ga,"A",{id:!0,class:!0,href:!0});var Rm=a(Ce);Yo=r(Rm,"SPAN",{});var Tm=a(Yo);u(Nt.$$.fragment,Tm),Tm.forEach(n),Rm.forEach(n),Ad=l(ga),er=r(ga,"SPAN",{});var $m=a(er);Pd=i($m,"RealmScorer"),$m.forEach(n),ga.forEach(n),Dr=l(t),U=r(t,"DIV",{class:!0});var Gn=a(U);u(Ot.$$.fragment,Gn),jd=l(Gn),Wt=r(Gn,"P",{});var _a=a(Wt);Ld=i(_a,`The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).
This model is a PyTorch `),Kt=r(_a,"A",{href:!0,rel:!0});var ym=a(Kt);Md=i(ym,"torch.nn.Module"),ym.forEach(n),Sd=i(_a,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),_a.forEach(n),Fd=l(Gn),F=r(Gn,"DIV",{class:!0});var oe=a(F);u(Bt.$$.fragment,oe),Cd=l(oe),ke=r(oe,"P",{});var Jn=a(ke);Id=i(Jn,"The "),Cn=r(Jn,"A",{href:!0});var Em=a(Cn);Dd=i(Em,"RealmScorer"),Em.forEach(n),Nd=i(Jn," forward method, overrides the "),tr=r(Jn,"CODE",{});var zm=a(tr);Od=i(zm,"__call__"),zm.forEach(n),Wd=i(Jn," special method."),Jn.forEach(n),Kd=l(oe),u(Ie.$$.fragment,oe),Bd=l(oe),nr=r(oe,"P",{});var qm=a(nr);Qd=i(qm,"Example:"),qm.forEach(n),Hd=l(oe),u(Qt.$$.fragment,oe),oe.forEach(n),Gn.forEach(n),Nr=l(t),ve=r(t,"H2",{class:!0});var ka=a(ve);De=r(ka,"A",{id:!0,class:!0,href:!0});var xm=a(De);or=r(xm,"SPAN",{});var Am=a(or);u(Ht.$$.fragment,Am),Am.forEach(n),xm.forEach(n),Vd=l(ka),rr=r(ka,"SPAN",{});var Pm=a(rr);Ud=i(Pm,"RealmKnowledgeAugEncoder"),Pm.forEach(n),ka.forEach(n),Or=l(t),X=r(t,"DIV",{class:!0});var Zn=a(X);u(Vt.$$.fragment,Zn),Xd=l(Zn),Ut=r(Zn,"P",{});var va=a(Ut);Gd=i(va,`The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood loss.
This model is a PyTorch `),Xt=r(va,"A",{href:!0,rel:!0});var jm=a(Xt);Jd=i(jm,"torch.nn.Module"),jm.forEach(n),Zd=i(va,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),va.forEach(n),Yd=l(Zn),C=r(Zn,"DIV",{class:!0});var re=a(C);u(Gt.$$.fragment,re),el=l(re),we=r(re,"P",{});var Yn=a(we);tl=i(Yn,"The "),In=r(Yn,"A",{href:!0});var Lm=a(In);nl=i(Lm,"RealmKnowledgeAugEncoder"),Lm.forEach(n),ol=i(Yn," forward method, overrides the "),ar=r(Yn,"CODE",{});var Mm=a(ar);rl=i(Mm,"__call__"),Mm.forEach(n),al=i(Yn," special method."),Yn.forEach(n),sl=l(re),u(Ne.$$.fragment,re),il=l(re),sr=r(re,"P",{});var Sm=a(sr);dl=i(Sm,"Example:"),Sm.forEach(n),ll=l(re),u(Jt.$$.fragment,re),re.forEach(n),Zn.forEach(n),Wr=l(t),be=r(t,"H2",{class:!0});var wa=a(be);Oe=r(wa,"A",{id:!0,class:!0,href:!0});var Fm=a(Oe);ir=r(Fm,"SPAN",{});var Cm=a(ir);u(Zt.$$.fragment,Cm),Cm.forEach(n),Fm.forEach(n),cl=l(wa),dr=r(wa,"SPAN",{});var Im=a(dr);ml=i(Im,"RealmReader"),Im.forEach(n),wa.forEach(n),Kr=l(t),G=r(t,"DIV",{class:!0});var eo=a(G);u(Yt.$$.fragment,eo),hl=l(eo),en=r(eo,"P",{});var ba=a(en);pl=i(ba,`The reader of REALM.
This model is a PyTorch `),tn=r(ba,"A",{href:!0,rel:!0});var Dm=a(tn);fl=i(Dm,"torch.nn.Module"),Dm.forEach(n),ul=i(ba,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),ba.forEach(n),gl=l(eo),ee=r(eo,"DIV",{class:!0});var to=a(ee);u(nn.$$.fragment,to),_l=l(to),Re=r(to,"P",{});var no=a(Re);kl=i(no,"The "),Dn=r(no,"A",{href:!0});var Nm=a(Dn);vl=i(Nm,"RealmReader"),Nm.forEach(n),wl=i(no," forward method, overrides the "),lr=r(no,"CODE",{});var Om=a(lr);bl=i(Om,"__call__"),Om.forEach(n),Rl=i(no," special method."),no.forEach(n),Tl=l(to),u(We.$$.fragment,to),to.forEach(n),eo.forEach(n),Br=l(t),Te=r(t,"H2",{class:!0});var Ra=a(Te);Ke=r(Ra,"A",{id:!0,class:!0,href:!0});var Wm=a(Ke);cr=r(Wm,"SPAN",{});var Km=a(cr);u(on.$$.fragment,Km),Km.forEach(n),Wm.forEach(n),$l=l(Ra),mr=r(Ra,"SPAN",{});var Bm=a(mr);yl=i(Bm,"RealmForOpenQA"),Bm.forEach(n),Ra.forEach(n),Qr=l(t),O=r(t,"DIV",{class:!0});var Ue=a(O);u(rn.$$.fragment,Ue),El=l(Ue),Be=r(Ue,"P",{});var br=a(Be);hr=r(br,"CODE",{});var Qm=a(hr);zl=i(Qm,"RealmForOpenQA"),Qm.forEach(n),ql=i(br,` for end-to-end open domain question answering.
This model is a PyTorch `),an=r(br,"A",{href:!0,rel:!0});var Hm=a(an);xl=i(Hm,"torch.nn.Module"),Hm.forEach(n),Al=i(br,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`),br.forEach(n),Pl=l(Ue),Qe=r(Ue,"DIV",{class:!0});var Ta=a(Qe);u(sn.$$.fragment,Ta),jl=l(Ta),dn=r(Ta,"P",{});var $a=a(dn);Ll=i($a,"Send "),pr=r($a,"CODE",{});var Vm=a(pr);Ml=i(Vm,"self.block_emb"),Vm.forEach(n),Sl=i($a," to a specific device."),$a.forEach(n),Ta.forEach(n),Fl=l(Ue),I=r(Ue,"DIV",{class:!0});var ae=a(I);u(ln.$$.fragment,ae),Cl=l(ae),$e=r(ae,"P",{});var oo=a($e);Il=i(oo,"The "),Nn=r(oo,"A",{href:!0});var Um=a(Nn);Dl=i(Um,"RealmForOpenQA"),Um.forEach(n),Nl=i(oo," forward method, overrides the "),fr=r(oo,"CODE",{});var Xm=a(fr);Ol=i(Xm,"__call__"),Xm.forEach(n),Wl=i(oo," special method."),oo.forEach(n),Kl=l(ae),u(He.$$.fragment,ae),Bl=l(ae),ur=r(ae,"P",{});var Gm=a(ur);Ql=i(Gm,"Example:"),Gm.forEach(n),Hl=l(ae),u(cn.$$.fragment,ae),ae.forEach(n),Ue.forEach(n),this.h()},h(){c(h,"name","hf:doc:metadata"),c(h,"content",JSON.stringify(ih)),c(T,"id","realm"),c(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T,"href","#realm"),c(w,"class","relative group"),c(ye,"id","overview"),c(ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ye,"href","#overview"),c(se,"class","relative group"),c(Je,"href","https://arxiv.org/abs/2002.08909"),c(Je,"rel","nofollow"),c(Ze,"href","https://huggingface.co/qqaatw"),c(Ze,"rel","nofollow"),c(Ye,"href","https://github.com/google-research/language/tree/master/language/realm"),c(Ye,"rel","nofollow"),c(ze,"id","transformers.RealmConfig"),c(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(ze,"href","#transformers.RealmConfig"),c(ie,"class","relative group"),c(un,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmEmbedder"),c(gn,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmScorer"),c(_n,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder"),c(kn,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmRetriever"),c(vn,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmReader"),c(wn,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmForOpenQA"),c(ot,"href","https://huggingface.co/google/realm-cc-news-pretrained-embedder"),c(ot,"rel","nofollow"),c(bn,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(Rn,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(x,"class","docstring"),c(qe,"id","transformers.RealmTokenizer"),c(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(qe,"href","#transformers.RealmTokenizer"),c(le,"class","relative group"),c(Tn,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer"),c($n,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(yn,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),c(Y,"class","docstring"),c(Ae,"class","docstring"),c(W,"class","docstring"),c(qn,"class","docstring"),c(P,"class","docstring"),c(E,"class","docstring"),c(Pe,"id","transformers.RealmTokenizerFast"),c(Pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pe,"href","#transformers.RealmTokenizerFast"),c(he,"class","relative group"),c(Pn,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizerFast"),c(jn,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(Ln,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),c(j,"class","docstring"),c(M,"class","docstring"),c(Le,"id","transformers.RealmRetriever"),c(Le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Le,"href","#transformers.RealmRetriever"),c(fe,"class","relative group"),c(Me,"class","docstring"),c(H,"class","docstring"),c(Se,"id","transformers.RealmEmbedder"),c(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Se,"href","#transformers.RealmEmbedder"),c(ue,"class","relative group"),c(Ct,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ct,"rel","nofollow"),c(Fn,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmEmbedder"),c(S,"class","docstring"),c(V,"class","docstring"),c(Ce,"id","transformers.RealmScorer"),c(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ce,"href","#transformers.RealmScorer"),c(_e,"class","relative group"),c(Kt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Kt,"rel","nofollow"),c(Cn,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmScorer"),c(F,"class","docstring"),c(U,"class","docstring"),c(De,"id","transformers.RealmKnowledgeAugEncoder"),c(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(De,"href","#transformers.RealmKnowledgeAugEncoder"),c(ve,"class","relative group"),c(Xt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Xt,"rel","nofollow"),c(In,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmKnowledgeAugEncoder"),c(C,"class","docstring"),c(X,"class","docstring"),c(Oe,"id","transformers.RealmReader"),c(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Oe,"href","#transformers.RealmReader"),c(be,"class","relative group"),c(tn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(tn,"rel","nofollow"),c(Dn,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmReader"),c(ee,"class","docstring"),c(G,"class","docstring"),c(Ke,"id","transformers.RealmForOpenQA"),c(Ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ke,"href","#transformers.RealmForOpenQA"),c(Te,"class","relative group"),c(an,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(an,"rel","nofollow"),c(Qe,"class","docstring"),c(Nn,"href","/docs/transformers/main/en/model_doc/realm#transformers.RealmForOpenQA"),c(I,"class","docstring"),c(O,"class","docstring")},m(t,m){e(document.head,h),p(t,$,m),p(t,w,m),e(w,T),e(T,y),g(R,y,null),e(w,b),e(w,q),e(q,ya),p(t,Tr,m),p(t,se,m),e(se,ye),e(ye,ro),g(Ge,ro,null),e(se,Ea),e(se,ao),e(ao,za),p(t,$r,m),p(t,Ee,m),e(Ee,qa),e(Ee,Je),e(Je,xa),e(Ee,Aa),p(t,yr,m),p(t,pn,m),e(pn,Pa),p(t,Er,m),p(t,fn,m),e(fn,so),e(so,ja),p(t,zr,m),p(t,Z,m),e(Z,La),e(Z,Ze),e(Ze,Ma),e(Z,Sa),e(Z,Ye),e(Ye,Fa),e(Z,Ca),p(t,qr,m),p(t,ie,m),e(ie,ze),e(ze,io),g(et,io,null),e(ie,Ia),e(ie,lo),e(lo,Da),p(t,xr,m),p(t,x,m),g(tt,x,null),e(x,Na),e(x,co),e(co,Oa),e(x,Wa),e(x,L),e(L,mo),e(mo,un),e(un,Ka),e(L,Ba),e(L,ho),e(ho,gn),e(gn,Qa),e(L,Ha),e(L,po),e(po,_n),e(_n,Va),e(L,Ua),e(L,fo),e(fo,kn),e(kn,Xa),e(L,Ga),e(L,uo),e(uo,vn),e(vn,Ja),e(L,Za),e(L,go),e(go,wn),e(wn,Ya),e(x,es),e(x,nt),e(nt,ts),e(nt,ot),e(ot,ns),e(nt,os),e(x,rs),e(x,de),e(de,as),e(de,bn),e(bn,ss),e(de,is),e(de,Rn),e(Rn,ds),e(de,ls),e(x,cs),e(x,_o),e(_o,ms),e(x,hs),g(rt,x,null),p(t,Ar,m),p(t,le,m),e(le,qe),e(qe,ko),g(at,ko,null),e(le,ps),e(le,vo),e(vo,fs),p(t,Pr,m),p(t,E,m),g(st,E,null),e(E,us),e(E,wo),e(wo,gs),e(E,_s),e(E,xe),e(xe,Tn),e(Tn,ks),e(xe,vs),e(xe,$n),e($n,ws),e(xe,bs),e(E,Rs),e(E,it),e(it,Ts),e(it,yn),e(yn,$s),e(it,ys),e(E,Es),e(E,Y),g(dt,Y,null),e(Y,zs),e(Y,bo),e(bo,qs),e(Y,xs),e(Y,lt),e(lt,En),e(En,As),e(En,Ro),e(Ro,Ps),e(lt,js),e(lt,zn),e(zn,Ls),e(zn,To),e(To,Ms),e(E,Ss),e(E,Ae),g(ct,Ae,null),e(Ae,Fs),e(Ae,mt),e(mt,Cs),e(mt,$o),e($o,Is),e(mt,Ds),e(E,Ns),e(E,W),g(ht,W,null),e(W,Os),e(W,yo),e(yo,Ws),e(W,Ks),g(pt,W,null),e(W,Bs),e(W,ce),e(ce,Qs),e(ce,Eo),e(Eo,Hs),e(ce,Vs),e(ce,zo),e(zo,Us),e(ce,Xs),e(E,Gs),e(E,qn),g(ft,qn,null),e(E,Js),e(E,P),g(ut,P,null),e(P,Zs),e(P,gt),e(gt,Ys),e(gt,qo),e(qo,ei),e(gt,ti),e(P,ni),e(P,me),e(me,xo),e(xo,oi),e(me,ri),e(me,_t),e(_t,ai),e(_t,Ao),e(Ao,si),e(_t,ii),e(me,di),e(me,kt),e(kt,li),e(kt,Po),e(Po,ci),e(kt,mi),e(P,hi),e(P,vt),e(vt,xn),e(xn,pi),e(xn,jo),e(jo,fi),e(vt,ui),e(vt,An),e(An,gi),e(An,Lo),e(Lo,_i),e(P,ki),e(P,Mo),e(Mo,vi),e(P,wi),g(wt,P,null),p(t,jr,m),p(t,he,m),e(he,Pe),e(Pe,So),g(bt,So,null),e(he,bi),e(he,Fo),e(Fo,Ri),p(t,Lr,m),p(t,M,m),g(Rt,M,null),e(M,Ti),e(M,Tt),e(Tt,$i),e(Tt,Co),e(Co,yi),e(Tt,Ei),e(M,zi),e(M,je),e(je,Pn),e(Pn,qi),e(je,xi),e(je,jn),e(jn,Ai),e(je,Pi),e(M,ji),e(M,$t),e($t,Li),e($t,Ln),e(Ln,Mi),e($t,Si),e(M,Fi),e(M,j),g(yt,j,null),e(j,Ci),e(j,Et),e(Et,Ii),e(Et,Io),e(Io,Di),e(Et,Ni),e(j,Oi),e(j,pe),e(pe,Do),e(Do,Wi),e(pe,Ki),e(pe,zt),e(zt,Bi),e(zt,No),e(No,Qi),e(zt,Hi),e(pe,Vi),e(pe,qt),e(qt,Ui),e(qt,Oo),e(Oo,Xi),e(qt,Gi),e(j,Ji),e(j,xt),e(xt,Mn),e(Mn,Zi),e(Mn,Wo),e(Wo,Yi),e(xt,ed),e(xt,Sn),e(Sn,td),e(Sn,Ko),e(Ko,nd),e(j,od),e(j,Bo),e(Bo,rd),e(j,ad),g(At,j,null),p(t,Mr,m),p(t,fe,m),e(fe,Le),e(Le,Qo),g(Pt,Qo,null),e(fe,sd),e(fe,Ho),e(Ho,id),p(t,Sr,m),p(t,H,m),g(jt,H,null),e(H,dd),e(H,Vo),e(Vo,ld),e(H,cd),e(H,Me),g(Lt,Me,null),e(Me,md),e(Me,Uo),e(Uo,hd),p(t,Fr,m),p(t,ue,m),e(ue,Se),e(Se,Xo),g(Mt,Xo,null),e(ue,pd),e(ue,Go),e(Go,fd),p(t,Cr,m),p(t,V,m),g(St,V,null),e(V,ud),e(V,Ft),e(Ft,gd),e(Ft,Ct),e(Ct,_d),e(Ft,kd),e(V,vd),e(V,S),g(It,S,null),e(S,wd),e(S,ge),e(ge,bd),e(ge,Fn),e(Fn,Rd),e(ge,Td),e(ge,Jo),e(Jo,$d),e(ge,yd),e(S,Ed),g(Fe,S,null),e(S,zd),e(S,Zo),e(Zo,qd),e(S,xd),g(Dt,S,null),p(t,Ir,m),p(t,_e,m),e(_e,Ce),e(Ce,Yo),g(Nt,Yo,null),e(_e,Ad),e(_e,er),e(er,Pd),p(t,Dr,m),p(t,U,m),g(Ot,U,null),e(U,jd),e(U,Wt),e(Wt,Ld),e(Wt,Kt),e(Kt,Md),e(Wt,Sd),e(U,Fd),e(U,F),g(Bt,F,null),e(F,Cd),e(F,ke),e(ke,Id),e(ke,Cn),e(Cn,Dd),e(ke,Nd),e(ke,tr),e(tr,Od),e(ke,Wd),e(F,Kd),g(Ie,F,null),e(F,Bd),e(F,nr),e(nr,Qd),e(F,Hd),g(Qt,F,null),p(t,Nr,m),p(t,ve,m),e(ve,De),e(De,or),g(Ht,or,null),e(ve,Vd),e(ve,rr),e(rr,Ud),p(t,Or,m),p(t,X,m),g(Vt,X,null),e(X,Xd),e(X,Ut),e(Ut,Gd),e(Ut,Xt),e(Xt,Jd),e(Ut,Zd),e(X,Yd),e(X,C),g(Gt,C,null),e(C,el),e(C,we),e(we,tl),e(we,In),e(In,nl),e(we,ol),e(we,ar),e(ar,rl),e(we,al),e(C,sl),g(Ne,C,null),e(C,il),e(C,sr),e(sr,dl),e(C,ll),g(Jt,C,null),p(t,Wr,m),p(t,be,m),e(be,Oe),e(Oe,ir),g(Zt,ir,null),e(be,cl),e(be,dr),e(dr,ml),p(t,Kr,m),p(t,G,m),g(Yt,G,null),e(G,hl),e(G,en),e(en,pl),e(en,tn),e(tn,fl),e(en,ul),e(G,gl),e(G,ee),g(nn,ee,null),e(ee,_l),e(ee,Re),e(Re,kl),e(Re,Dn),e(Dn,vl),e(Re,wl),e(Re,lr),e(lr,bl),e(Re,Rl),e(ee,Tl),g(We,ee,null),p(t,Br,m),p(t,Te,m),e(Te,Ke),e(Ke,cr),g(on,cr,null),e(Te,$l),e(Te,mr),e(mr,yl),p(t,Qr,m),p(t,O,m),g(rn,O,null),e(O,El),e(O,Be),e(Be,hr),e(hr,zl),e(Be,ql),e(Be,an),e(an,xl),e(Be,Al),e(O,Pl),e(O,Qe),g(sn,Qe,null),e(Qe,jl),e(Qe,dn),e(dn,Ll),e(dn,pr),e(pr,Ml),e(dn,Sl),e(O,Fl),e(O,I),g(ln,I,null),e(I,Cl),e(I,$e),e($e,Il),e($e,Nn),e(Nn,Dl),e($e,Nl),e($e,fr),e(fr,Ol),e($e,Wl),e(I,Kl),g(He,I,null),e(I,Bl),e(I,ur),e(ur,Ql),e(I,Hl),g(cn,I,null),Hr=!0},p(t,[m]){const mn={};m&2&&(mn.$$scope={dirty:m,ctx:t}),Fe.$set(mn);const gr={};m&2&&(gr.$$scope={dirty:m,ctx:t}),Ie.$set(gr);const _r={};m&2&&(_r.$$scope={dirty:m,ctx:t}),Ne.$set(_r);const kr={};m&2&&(kr.$$scope={dirty:m,ctx:t}),We.$set(kr);const hn={};m&2&&(hn.$$scope={dirty:m,ctx:t}),He.$set(hn)},i(t){Hr||(_(R.$$.fragment,t),_(Ge.$$.fragment,t),_(et.$$.fragment,t),_(tt.$$.fragment,t),_(rt.$$.fragment,t),_(at.$$.fragment,t),_(st.$$.fragment,t),_(dt.$$.fragment,t),_(ct.$$.fragment,t),_(ht.$$.fragment,t),_(pt.$$.fragment,t),_(ft.$$.fragment,t),_(ut.$$.fragment,t),_(wt.$$.fragment,t),_(bt.$$.fragment,t),_(Rt.$$.fragment,t),_(yt.$$.fragment,t),_(At.$$.fragment,t),_(Pt.$$.fragment,t),_(jt.$$.fragment,t),_(Lt.$$.fragment,t),_(Mt.$$.fragment,t),_(St.$$.fragment,t),_(It.$$.fragment,t),_(Fe.$$.fragment,t),_(Dt.$$.fragment,t),_(Nt.$$.fragment,t),_(Ot.$$.fragment,t),_(Bt.$$.fragment,t),_(Ie.$$.fragment,t),_(Qt.$$.fragment,t),_(Ht.$$.fragment,t),_(Vt.$$.fragment,t),_(Gt.$$.fragment,t),_(Ne.$$.fragment,t),_(Jt.$$.fragment,t),_(Zt.$$.fragment,t),_(Yt.$$.fragment,t),_(nn.$$.fragment,t),_(We.$$.fragment,t),_(on.$$.fragment,t),_(rn.$$.fragment,t),_(sn.$$.fragment,t),_(ln.$$.fragment,t),_(He.$$.fragment,t),_(cn.$$.fragment,t),Hr=!0)},o(t){k(R.$$.fragment,t),k(Ge.$$.fragment,t),k(et.$$.fragment,t),k(tt.$$.fragment,t),k(rt.$$.fragment,t),k(at.$$.fragment,t),k(st.$$.fragment,t),k(dt.$$.fragment,t),k(ct.$$.fragment,t),k(ht.$$.fragment,t),k(pt.$$.fragment,t),k(ft.$$.fragment,t),k(ut.$$.fragment,t),k(wt.$$.fragment,t),k(bt.$$.fragment,t),k(Rt.$$.fragment,t),k(yt.$$.fragment,t),k(At.$$.fragment,t),k(Pt.$$.fragment,t),k(jt.$$.fragment,t),k(Lt.$$.fragment,t),k(Mt.$$.fragment,t),k(St.$$.fragment,t),k(It.$$.fragment,t),k(Fe.$$.fragment,t),k(Dt.$$.fragment,t),k(Nt.$$.fragment,t),k(Ot.$$.fragment,t),k(Bt.$$.fragment,t),k(Ie.$$.fragment,t),k(Qt.$$.fragment,t),k(Ht.$$.fragment,t),k(Vt.$$.fragment,t),k(Gt.$$.fragment,t),k(Ne.$$.fragment,t),k(Jt.$$.fragment,t),k(Zt.$$.fragment,t),k(Yt.$$.fragment,t),k(nn.$$.fragment,t),k(We.$$.fragment,t),k(on.$$.fragment,t),k(rn.$$.fragment,t),k(sn.$$.fragment,t),k(ln.$$.fragment,t),k(He.$$.fragment,t),k(cn.$$.fragment,t),Hr=!1},d(t){n(h),t&&n($),t&&n(w),v(R),t&&n(Tr),t&&n(se),v(Ge),t&&n($r),t&&n(Ee),t&&n(yr),t&&n(pn),t&&n(Er),t&&n(fn),t&&n(zr),t&&n(Z),t&&n(qr),t&&n(ie),v(et),t&&n(xr),t&&n(x),v(tt),v(rt),t&&n(Ar),t&&n(le),v(at),t&&n(Pr),t&&n(E),v(st),v(dt),v(ct),v(ht),v(pt),v(ft),v(ut),v(wt),t&&n(jr),t&&n(he),v(bt),t&&n(Lr),t&&n(M),v(Rt),v(yt),v(At),t&&n(Mr),t&&n(fe),v(Pt),t&&n(Sr),t&&n(H),v(jt),v(Lt),t&&n(Fr),t&&n(ue),v(Mt),t&&n(Cr),t&&n(V),v(St),v(It),v(Fe),v(Dt),t&&n(Ir),t&&n(_e),v(Nt),t&&n(Dr),t&&n(U),v(Ot),v(Bt),v(Ie),v(Qt),t&&n(Nr),t&&n(ve),v(Ht),t&&n(Or),t&&n(X),v(Vt),v(Gt),v(Ne),v(Jt),t&&n(Wr),t&&n(be),v(Zt),t&&n(Kr),t&&n(G),v(Yt),v(nn),v(We),t&&n(Br),t&&n(Te),v(on),t&&n(Qr),t&&n(O),v(rn),v(sn),v(ln),v(He),v(cn)}}}const ih={local:"realm",sections:[{local:"overview",title:"Overview"},{local:"transformers.RealmConfig",title:"RealmConfig"},{local:"transformers.RealmTokenizer",title:"RealmTokenizer"},{local:"transformers.RealmTokenizerFast",title:"RealmTokenizerFast"},{local:"transformers.RealmRetriever",title:"RealmRetriever"},{local:"transformers.RealmEmbedder",title:"RealmEmbedder"},{local:"transformers.RealmScorer",title:"RealmScorer"},{local:"transformers.RealmKnowledgeAugEncoder",title:"RealmKnowledgeAugEncoder"},{local:"transformers.RealmReader",title:"RealmReader"},{local:"transformers.RealmForOpenQA",title:"RealmForOpenQA"}],title:"REALM"};function dh(N,h,$){let{fw:w}=h;return N.$$set=T=>{"fw"in T&&$(0,w=T.fw)},[w]}class uh extends Jm{constructor(h){super();Zm(this,h,dh,sh,Ym,{fw:0})}}export{uh as default,ih as metadata};
