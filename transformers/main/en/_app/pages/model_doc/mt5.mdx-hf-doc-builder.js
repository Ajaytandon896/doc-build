import{S as Wd,i as Bd,s as Hd,e as r,k as p,w as T,t as l,M as Rd,c as n,d as s,m,a,x as v,h as i,b as d,G as t,g as f,y as $,q as b,o as w,B as y,v as Xd,L as me}from"../../chunks/vendor-hf-doc-builder.js";import{D as E}from"../../chunks/Docstring-hf-doc-builder.js";import{C as ce}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as P}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as pe}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Kd(M){let h,x,_,u,k;return u=new ce({props:{code:`from transformers import MT5Model, T5Tokenizer

model = MT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(o){h=n(o,"P",{});var g=a(h);x=i(g,"Examples:"),g.forEach(s),_=m(o),v(u.$$.fragment,o)},m(o,g){f(o,h,g),t(h,x),f(o,_,g),$(u,o,g),k=!0},p:me,i(o){k||(b(u.$$.fragment,o),k=!0)},o(o){w(u.$$.fragment,o),k=!1},d(o){o&&s(h),o&&s(_),y(u,o)}}}function Jd(M){let h,x,_,u,k;return u=new ce({props:{code:`from transformers import MT5ForConditionalGeneration, T5Tokenizer

model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="pt")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="pt")

outputs = model(**inputs, labels=labels["input_ids"])
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(o){h=n(o,"P",{});var g=a(h);x=i(g,"Examples:"),g.forEach(s),_=m(o),v(u.$$.fragment,o)},m(o,g){f(o,h,g),t(h,x),f(o,_,g),$(u,o,g),k=!0},p:me,i(o){k||(b(u.$$.fragment,o),k=!0)},o(o){w(u.$$.fragment,o),k=!1},d(o){o&&s(h),o&&s(_),y(u,o)}}}function Qd(M){let h,x,_,u,k;return u=new ce({props:{code:`from transformers import MT5EncoderModel, T5Tokenizer

model = MT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="pt").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(o){h=n(o,"P",{});var g=a(h);x=i(g,"Examples:"),g.forEach(s),_=m(o),v(u.$$.fragment,o)},m(o,g){f(o,h,g),t(h,x),f(o,_,g),$(u,o,g),k=!0},p:me,i(o){k||(b(u.$$.fragment,o),k=!0)},o(o){w(u.$$.fragment,o),k=!1},d(o){o&&s(h),o&&s(_),y(u,o)}}}function Yd(M){let h,x,_,u,k;return u=new ce({props:{code:`from transformers import TFMT5Model, T5Tokenizer

model = TFMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(o){h=n(o,"P",{});var g=a(h);x=i(g,"Examples:"),g.forEach(s),_=m(o),v(u.$$.fragment,o)},m(o,g){f(o,h,g),t(h,x),f(o,_,g),$(u,o,g),k=!0},p:me,i(o){k||(b(u.$$.fragment,o),k=!0)},o(o){w(u.$$.fragment,o),k=!1},d(o){o&&s(h),o&&s(_),y(u,o)}}}function Zd(M){let h,x,_,u,k;return u=new ce({props:{code:`from transformers import TFMT5ForConditionalGeneration, T5Tokenizer

model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="tf")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(summary, return_tensors="tf")

outputs = model(**inputs, labels=labels["input_ids"])
loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    labels = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(o){h=n(o,"P",{});var g=a(h);x=i(g,"Examples:"),g.forEach(s),_=m(o),v(u.$$.fragment,o)},m(o,g){f(o,h,g),t(h,x),f(o,_,g),$(u,o,g),k=!0},p:me,i(o){k||(b(u.$$.fragment,o),k=!0)},o(o){w(u.$$.fragment,o),k=!1},d(o){o&&s(h),o&&s(_),y(u,o)}}}function ep(M){let h,x,_,u,k;return u=new ce({props:{code:`from transformers import TFMT5EncoderModel, T5Tokenizer

model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
input_ids = tokenizer(article, return_tensors="tf").input_ids
outputs = model(input_ids)
hidden_state = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFMT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFMT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(article, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_state = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(o){h=n(o,"P",{});var g=a(h);x=i(g,"Examples:"),g.forEach(s),_=m(o),v(u.$$.fragment,o)},m(o,g){f(o,h,g),t(h,x),f(o,_,g),$(u,o,g),k=!0},p:me,i(o){k||(b(u.$$.fragment,o),k=!0)},o(o){w(u.$$.fragment,o),k=!1},d(o){o&&s(h),o&&s(_),y(u,o)}}}function tp(M){let h,x,_,u,k;return u=new ce({props:{code:`from transformers import FlaxMT5Model, T5Tokenizer

model = FlaxMT5Model.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5Model, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5Model.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(o){h=n(o,"P",{});var g=a(h);x=i(g,"Examples:"),g.forEach(s),_=m(o),v(u.$$.fragment,o)},m(o,g){f(o,h,g),t(h,x),f(o,_,g),$(u,o,g),k=!0},p:me,i(o){k||(b(u.$$.fragment,o),k=!0)},o(o){w(u.$$.fragment,o),k=!1},d(o){o&&s(h),o&&s(_),y(u,o)}}}function sp(M){let h,x,_,u,k;return u=new ce({props:{code:`from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer

model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxMT5ForConditionalGeneration, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMT5ForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(o){h=n(o,"P",{});var g=a(h);x=i(g,"Examples:"),g.forEach(s),_=m(o),v(u.$$.fragment,o)},m(o,g){f(o,h,g),t(h,x),f(o,_,g),$(u,o,g),k=!0},p:me,i(o){k||(b(u.$$.fragment,o),k=!0)},o(o){w(u.$$.fragment,o),k=!1},d(o){o&&s(h),o&&s(_),y(u,o)}}}function rp(M){let h,x,_,u,k;return u=new ce({props:{code:`from transformers import FlaxT5EncoderModel, T5Tokenizer

model = FlaxT5EncoderModel.from_pretrained("google/mt5-small")
tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
summary = "Weiter Verhandlung in Syrien."
inputs = tokenizer(article, return_tensors="np")

with tokenizer.as_target_tokenizer():
    decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

outputs = model(input_ids=inputs["input_ids"])
hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlaxT5EncoderModel, T5Tokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxT5EncoderModel.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = T5Tokenizer.from_pretrained(<span class="hljs-string">&quot;google/mt5-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Offizier sagt, dass weiter verhandelt werden muss in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>summary = <span class="hljs-string">&quot;Weiter Verhandlung in Syrien.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>    decoder_input_ids = tokenizer(summary, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=inputs[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>hidden_states = outputs.last_hidden_state`}}),{c(){h=r("p"),x=l("Examples:"),_=p(),T(u.$$.fragment)},l(o){h=n(o,"P",{});var g=a(h);x=i(g,"Examples:"),g.forEach(s),_=m(o),v(u.$$.fragment,o)},m(o,g){f(o,h,g),t(h,x),f(o,_,g),$(u,o,g),k=!0},p:me,i(o){k||(b(u.$$.fragment,o),k=!0)},o(o){w(u.$$.fragment,o),k=!1},d(o){o&&s(h),o&&s(_),y(u,o)}}}function np(M){let h,x,_,u,k,o,g,Ns,Zn,Nr,K,fe,Ds,We,ea,Is,ta,Dr,ue,sa,Be,ra,na,Ir,Qt,aa,Gr,Yt,Gs,oa,Or,he,la,He,ia,da,Vr,Zt,pa,Ur,F,Os,Vs,Re,ma,ca,Us,Ws,Xe,fa,ua,Bs,Hs,Ke,ha,ga,Rs,Xs,Je,_a,ka,Ks,es,Qe,Ta,va,Wr,W,$a,Ye,ba,wa,Ze,ya,xa,Br,J,ge,Js,et,Ma,Qs,za,Hr,C,tt,Ea,A,qa,ts,Fa,ja,ss,Pa,Ca,st,Aa,Sa,La,Q,Na,rs,Da,Ia,ns,Ga,Oa,Rr,Y,_e,Ys,rt,Va,Zs,Ua,Xr,z,nt,Wa,at,Ba,ot,Ha,Ra,Xa,lt,Ka,as,Ja,Qa,Ya,B,it,Za,er,eo,to,dt,os,so,tr,ro,no,ls,ao,sr,oo,lo,ke,pt,io,rr,po,mo,Te,mt,co,nr,fo,uo,ve,ct,ho,ft,go,ar,_o,ko,Kr,$e,To,is,vo,$o,Jr,Z,be,or,ut,bo,lr,wo,Qr,q,ht,yo,ee,xo,ir,Mo,zo,gt,Eo,qo,Fo,_t,jo,ds,Po,Co,Ao,H,kt,So,dr,Lo,No,Tt,ps,Do,pr,Io,Go,ms,Oo,mr,Vo,Uo,we,vt,Wo,cr,Bo,Yr,ye,Ho,cs,Ro,Xo,Zr,te,xe,fr,$t,Ko,ur,Jo,en,S,bt,Qo,wt,Yo,fs,Zo,el,tl,Me,tn,se,ze,hr,yt,sl,gr,rl,sn,L,xt,nl,Mt,al,us,ol,ll,il,Ee,rn,re,qe,_r,zt,dl,kr,pl,nn,N,Et,ml,qt,cl,hs,fl,ul,hl,Fe,an,ne,je,Tr,Ft,gl,vr,_l,on,D,jt,kl,Pt,Tl,gs,vl,$l,bl,Pe,ln,ae,Ce,$r,Ct,wl,br,yl,dn,I,At,xl,St,Ml,_s,zl,El,ql,Ae,pn,oe,Se,wr,Lt,Fl,yr,jl,mn,G,Nt,Pl,Dt,Cl,ks,Al,Sl,Ll,Le,cn,le,Ne,xr,It,Nl,Mr,Dl,fn,O,Gt,Il,Ot,Gl,Ts,Ol,Vl,Ul,De,un,ie,Ie,zr,Vt,Wl,Er,Bl,hn,V,Ut,Hl,Wt,Rl,vs,Xl,Kl,Jl,Ge,gn,de,Oe,qr,Bt,Ql,Fr,Yl,_n,U,Ht,Zl,Rt,ei,$s,ti,si,ri,Ve,kn;return o=new P({}),We=new P({}),et=new P({}),tt=new E({props:{name:"class transformers.MT5Config",anchor:"transformers.MT5Config",parameters:[{name:"vocab_size",val:" = 250112"},{name:"d_model",val:" = 512"},{name:"d_kv",val:" = 64"},{name:"d_ff",val:" = 1024"},{name:"num_layers",val:" = 8"},{name:"num_decoder_layers",val:" = None"},{name:"num_heads",val:" = 6"},{name:"relative_attention_num_buckets",val:" = 32"},{name:"relative_attention_max_distance",val:" = 128"},{name:"dropout_rate",val:" = 0.1"},{name:"layer_norm_epsilon",val:" = 1e-06"},{name:"initializer_factor",val:" = 1.0"},{name:"feed_forward_proj",val:" = 'gated-gelu'"},{name:"is_encoder_decoder",val:" = True"},{name:"use_cache",val:" = True"},{name:"tokenizer_class",val:" = 'T5Tokenizer'"},{name:"tie_word_embeddings",val:" = False"},{name:"pad_token_id",val:" = 0"},{name:"eos_token_id",val:" = 1"},{name:"decoder_start_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MT5Config.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250112) &#x2014;
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Model">T5Model</a> or <a href="/docs/transformers/main/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a>.`,name:"vocab_size"},{anchor:"transformers.MT5Config.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.MT5Config.d_kv",description:`<strong>d_kv</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Size of the key, query, value projections per attention head. <code>d_kv</code> has to be equal to <code>d_model // num_heads</code>.`,name:"d_kv"},{anchor:"transformers.MT5Config.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the intermediate feed forward layer in each <code>T5Block</code>.`,name:"d_ff"},{anchor:"transformers.MT5Config.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_layers"},{anchor:"transformers.MT5Config.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Number of hidden layers in the Transformer decoder. Will use the same value as <code>num_layers</code> if not set.`,name:"num_decoder_layers"},{anchor:"transformers.MT5Config.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.MT5Config.relative_attention_num_buckets",description:`<strong>relative_attention_num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer.`,name:"relative_attention_num_buckets"},{anchor:"transformers.MT5Config.relative_attention_max_distance",description:`<strong>relative_attention_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The maximum distance of the longer sequences for the bucket separation.`,name:"relative_attention_max_distance"},{anchor:"transformers.MT5Config.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.MT5Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MT5Config.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.MT5Config.feed_forward_proj",description:`<strong>feed_forward_proj</strong> (<code>string</code>, <em>optional</em>, defaults to <code>&quot;gated-gelu&quot;</code>) &#x2014;
Type of feed forward layer to be used. Should be one of <code>&quot;relu&quot;</code> or <code>&quot;gated-gelu&quot;</code>.`,name:"feed_forward_proj"},{anchor:"transformers.MT5Config.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/configuration_mt5.py#L24"}}),rt=new P({}),nt=new E({props:{name:"class transformers.T5Tokenizer",anchor:"transformers.T5Tokenizer",parameters:[{name:"vocab_file",val:""},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5Tokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5Tokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5Tokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5Tokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5Tokenizer.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5Tokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.T5Tokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.T5Tokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py#L55"}}),it=new E({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py#L249",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),pt=new E({props:{name:"convert_tokens_to_string",anchor:"transformers.T5Tokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py#L310"}}),mt=new E({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py#L227",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new E({props:{name:"get_special_tokens_mask",anchor:"transformers.T5Tokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.T5Tokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5.py#L188",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ut=new P({}),ht=new E({props:{name:"class transformers.T5TokenizerFast",anchor:"transformers.T5TokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"eos_token",val:" = '</s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"extra_ids",val:" = 100"},{name:"additional_special_tokens",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a <em>.spm</em> extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.T5TokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.T5TokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.T5TokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.T5TokenizerFast.extra_ids",description:`<strong>extra_ids</strong> (<code>int</code>, <em>optional</em>, defaults to 100) &#x2014;
Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
accessible as &#x201C;<extra<em>id{%d}&gt;&#x201D; where &#x201D;{%d}&#x201D; is a number between 0 and extra_ids-1. Extra tokens are
indexed from the end of the vocabulary up to beginning (&#x201C;<extra_id_0>&#x201D; is the last token in the vocabulary
like in T5 preprocessing see
<a href="https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117" rel="nofollow">here</a>).</extra_id_0></extra<em>`,name:"extra_ids"},{anchor:"transformers.T5TokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5_fast.py#L65"}}),kt=new E({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5_fast.py#L191",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),vt=new E({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.T5TokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/tokenization_t5_fast.py#L217",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),$t=new P({}),bt=new E({props:{name:"class transformers.MT5Model",anchor:"transformers.MT5Model",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_mt5.py#L28"}}),Me=new pe({props:{anchor:"transformers.MT5Model.example",$$slots:{default:[Kd]},$$scope:{ctx:M}}}),yt=new P({}),xt=new E({props:{name:"class transformers.MT5ForConditionalGeneration",anchor:"transformers.MT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_mt5.py#L62"}}),Ee=new pe({props:{anchor:"transformers.MT5ForConditionalGeneration.example",$$slots:{default:[Jd]},$$scope:{ctx:M}}}),zt=new P({}),Et=new E({props:{name:"class transformers.MT5EncoderModel",anchor:"transformers.MT5EncoderModel",parameters:[{name:"config",val:": T5Config"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_mt5.py#L94"}}),Fe=new pe({props:{anchor:"transformers.MT5EncoderModel.example",$$slots:{default:[Qd]},$$scope:{ctx:M}}}),Ft=new P({}),jt=new E({props:{name:"class transformers.TFMT5Model",anchor:"transformers.TFMT5Model",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_tf_mt5.py#L28"}}),Pe=new pe({props:{anchor:"transformers.TFMT5Model.example",$$slots:{default:[Yd]},$$scope:{ctx:M}}}),Ct=new P({}),At=new E({props:{name:"class transformers.TFMT5ForConditionalGeneration",anchor:"transformers.TFMT5ForConditionalGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_tf_mt5.py#L53"}}),Ae=new pe({props:{anchor:"transformers.TFMT5ForConditionalGeneration.example",$$slots:{default:[Zd]},$$scope:{ctx:M}}}),Lt=new P({}),Nt=new E({props:{name:"class transformers.TFMT5EncoderModel",anchor:"transformers.TFMT5EncoderModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_tf_mt5.py#L79"}}),Le=new pe({props:{anchor:"transformers.TFMT5EncoderModel.example",$$slots:{default:[ep]},$$scope:{ctx:M}}}),It=new P({}),Gt=new E({props:{name:"class transformers.FlaxMT5Model",anchor:"transformers.FlaxMT5Model",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_flax_mt5.py#L43"}}),De=new pe({props:{anchor:"transformers.FlaxMT5Model.example",$$slots:{default:[tp]},$$scope:{ctx:M}}}),Vt=new P({}),Ut=new E({props:{name:"class transformers.FlaxMT5ForConditionalGeneration",anchor:"transformers.FlaxMT5ForConditionalGeneration",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_flax_mt5.py#L97"}}),Ge=new pe({props:{anchor:"transformers.FlaxMT5ForConditionalGeneration.example",$$slots:{default:[sp]},$$scope:{ctx:M}}}),Bt=new P({}),Ht=new E({props:{name:"class transformers.FlaxMT5EncoderModel",anchor:"transformers.FlaxMT5EncoderModel",parameters:[{name:"config",val:": T5Config"},{name:"input_shape",val:": typing.Tuple[int] = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/mt5/modeling_flax_mt5.py#L70"}}),Ve=new pe({props:{anchor:"transformers.FlaxMT5EncoderModel.example",$$slots:{default:[rp]},$$scope:{ctx:M}}}),{c(){h=r("meta"),x=p(),_=r("h1"),u=r("a"),k=r("span"),T(o.$$.fragment),g=p(),Ns=r("span"),Zn=l("mT5"),Nr=p(),K=r("h2"),fe=r("a"),Ds=r("span"),T(We.$$.fragment),ea=p(),Is=r("span"),ta=l("Overview"),Dr=p(),ue=r("p"),sa=l("The mT5 model was presented in "),Be=r("a"),ra=l("mT5: A massively multilingual pre-trained text-to-text transformer"),na=l(` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),Ir=p(),Qt=r("p"),aa=l("The abstract from the paper is the following:"),Gr=p(),Yt=r("p"),Gs=r("em"),oa=l(`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),Or=p(),he=r("p"),la=l("Note: mT5 was only pre-trained on "),He=r("a"),ia=l("mC4"),da=l(` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),Vr=p(),Zt=r("p"),pa=l("Google has released the following variants:"),Ur=p(),F=r("ul"),Os=r("li"),Vs=r("p"),Re=r("a"),ma=l("google/mt5-small"),ca=p(),Us=r("li"),Ws=r("p"),Xe=r("a"),fa=l("google/mt5-base"),ua=p(),Bs=r("li"),Hs=r("p"),Ke=r("a"),ha=l("google/mt5-large"),ga=p(),Rs=r("li"),Xs=r("p"),Je=r("a"),_a=l("google/mt5-xl"),ka=p(),Ks=r("li"),es=r("p"),Qe=r("a"),Ta=l("google/mt5-xxl"),va=l("."),Wr=p(),W=r("p"),$a=l("This model was contributed by "),Ye=r("a"),ba=l("patrickvonplaten"),wa=l(`. The original code can be
found `),Ze=r("a"),ya=l("here"),xa=l("."),Br=p(),J=r("h2"),ge=r("a"),Js=r("span"),T(et.$$.fragment),Ma=p(),Qs=r("span"),za=l("MT5Config"),Hr=p(),C=r("div"),T(tt.$$.fragment),Ea=p(),A=r("p"),qa=l("This is the configuration class to store the configuration of a "),ts=r("a"),Fa=l("MT5Model"),ja=l(" or a "),ss=r("a"),Pa=l("TFMT5Model"),Ca=l(`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),st=r("a"),Aa=l("google/mt5-small"),Sa=l(" architecture."),La=p(),Q=r("p"),Na=l("Configuration objects inherit from "),rs=r("a"),Da=l("PretrainedConfig"),Ia=l(` and can be used to control the model outputs. Read the
documentation from `),ns=r("a"),Ga=l("PretrainedConfig"),Oa=l(" for more information."),Rr=p(),Y=r("h2"),_e=r("a"),Ys=r("span"),T(rt.$$.fragment),Va=p(),Zs=r("span"),Ua=l("MT5Tokenizer"),Xr=p(),z=r("div"),T(nt.$$.fragment),Wa=p(),at=r("p"),Ba=l("Construct a T5 tokenizer. Based on "),ot=r("a"),Ha=l("SentencePiece"),Ra=l("."),Xa=p(),lt=r("p"),Ka=l("This tokenizer inherits from "),as=r("a"),Ja=l("PreTrainedTokenizer"),Qa=l(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Ya=p(),B=r("div"),T(it.$$.fragment),Za=p(),er=r("p"),eo=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),to=p(),dt=r("ul"),os=r("li"),so=l("single sequence: "),tr=r("code"),ro=l("X </s>"),no=p(),ls=r("li"),ao=l("pair of sequences: "),sr=r("code"),oo=l("A </s> B </s>"),lo=p(),ke=r("div"),T(pt.$$.fragment),io=p(),rr=r("p"),po=l("Converts a sequence of tokens (string) in a single string."),mo=p(),Te=r("div"),T(mt.$$.fragment),co=p(),nr=r("p"),fo=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),uo=p(),ve=r("div"),T(ct.$$.fragment),ho=p(),ft=r("p"),go=l(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),ar=r("code"),_o=l("prepare_for_model"),ko=l(" method."),Kr=p(),$e=r("p"),To=l("See "),is=r("a"),vo=l("T5Tokenizer"),$o=l(" for all details."),Jr=p(),Z=r("h2"),be=r("a"),or=r("span"),T(ut.$$.fragment),bo=p(),lr=r("span"),wo=l("MT5TokenizerFast"),Qr=p(),q=r("div"),T(ht.$$.fragment),yo=p(),ee=r("p"),xo=l("Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),ir=r("em"),Mo=l("tokenizers"),zo=l(` library). Based on
`),gt=r("a"),Eo=l("Unigram"),qo=l("."),Fo=p(),_t=r("p"),jo=l("This tokenizer inherits from "),ds=r("a"),Po=l("PreTrainedTokenizerFast"),Co=l(` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),Ao=p(),H=r("div"),T(kt.$$.fragment),So=p(),dr=r("p"),Lo=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),No=p(),Tt=r("ul"),ps=r("li"),Do=l("single sequence: "),pr=r("code"),Io=l("X </s>"),Go=p(),ms=r("li"),Oo=l("pair of sequences: "),mr=r("code"),Vo=l("A </s> B </s>"),Uo=p(),we=r("div"),T(vt.$$.fragment),Wo=p(),cr=r("p"),Bo=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Yr=p(),ye=r("p"),Ho=l("See "),cs=r("a"),Ro=l("T5TokenizerFast"),Xo=l(" for all details."),Zr=p(),te=r("h2"),xe=r("a"),fr=r("span"),T($t.$$.fragment),Ko=p(),ur=r("span"),Jo=l("MT5Model"),en=p(),S=r("div"),T(bt.$$.fragment),Qo=p(),wt=r("p"),Yo=l("This class overrides "),fs=r("a"),Zo=l("T5Model"),el=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),tl=p(),T(Me.$$.fragment),tn=p(),se=r("h2"),ze=r("a"),hr=r("span"),T(yt.$$.fragment),sl=p(),gr=r("span"),rl=l("MT5ForConditionalGeneration"),sn=p(),L=r("div"),T(xt.$$.fragment),nl=p(),Mt=r("p"),al=l("This class overrides "),us=r("a"),ol=l("T5ForConditionalGeneration"),ll=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),il=p(),T(Ee.$$.fragment),rn=p(),re=r("h2"),qe=r("a"),_r=r("span"),T(zt.$$.fragment),dl=p(),kr=r("span"),pl=l("MT5EncoderModel"),nn=p(),N=r("div"),T(Et.$$.fragment),ml=p(),qt=r("p"),cl=l("This class overrides "),hs=r("a"),fl=l("T5EncoderModel"),ul=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),hl=p(),T(Fe.$$.fragment),an=p(),ne=r("h2"),je=r("a"),Tr=r("span"),T(Ft.$$.fragment),gl=p(),vr=r("span"),_l=l("TFMT5Model"),on=p(),D=r("div"),T(jt.$$.fragment),kl=p(),Pt=r("p"),Tl=l("This class overrides "),gs=r("a"),vl=l("TFT5Model"),$l=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),bl=p(),T(Pe.$$.fragment),ln=p(),ae=r("h2"),Ce=r("a"),$r=r("span"),T(Ct.$$.fragment),wl=p(),br=r("span"),yl=l("TFMT5ForConditionalGeneration"),dn=p(),I=r("div"),T(At.$$.fragment),xl=p(),St=r("p"),Ml=l("This class overrides "),_s=r("a"),zl=l("TFT5ForConditionalGeneration"),El=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),ql=p(),T(Ae.$$.fragment),pn=p(),oe=r("h2"),Se=r("a"),wr=r("span"),T(Lt.$$.fragment),Fl=p(),yr=r("span"),jl=l("TFMT5EncoderModel"),mn=p(),G=r("div"),T(Nt.$$.fragment),Pl=p(),Dt=r("p"),Cl=l("This class overrides "),ks=r("a"),Al=l("TFT5EncoderModel"),Sl=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Ll=p(),T(Le.$$.fragment),cn=p(),le=r("h2"),Ne=r("a"),xr=r("span"),T(It.$$.fragment),Nl=p(),Mr=r("span"),Dl=l("FlaxMT5Model"),fn=p(),O=r("div"),T(Gt.$$.fragment),Il=p(),Ot=r("p"),Gl=l("This class overrides "),Ts=r("a"),Ol=l("FlaxT5Model"),Vl=l(`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ul=p(),T(De.$$.fragment),un=p(),ie=r("h2"),Ie=r("a"),zr=r("span"),T(Vt.$$.fragment),Wl=p(),Er=r("span"),Bl=l("FlaxMT5ForConditionalGeneration"),hn=p(),V=r("div"),T(Ut.$$.fragment),Hl=p(),Wt=r("p"),Rl=l("This class overrides "),vs=r("a"),Xl=l("FlaxT5ForConditionalGeneration"),Kl=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Jl=p(),T(Ge.$$.fragment),gn=p(),de=r("h2"),Oe=r("a"),qr=r("span"),T(Bt.$$.fragment),Ql=p(),Fr=r("span"),Yl=l("FlaxMT5EncoderModel"),_n=p(),U=r("div"),T(Ht.$$.fragment),Zl=p(),Rt=r("p"),ei=l("This class overrides "),$s=r("a"),ti=l("FlaxT5EncoderModel"),si=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),ri=p(),T(Ve.$$.fragment),this.h()},l(e){const c=Rd('[data-svelte="svelte-1phssyn"]',document.head);h=n(c,"META",{name:!0,content:!0}),c.forEach(s),x=m(e),_=n(e,"H1",{class:!0});var Xt=a(_);u=n(Xt,"A",{id:!0,class:!0,href:!0});var jr=a(u);k=n(jr,"SPAN",{});var Pr=a(k);v(o.$$.fragment,Pr),Pr.forEach(s),jr.forEach(s),g=m(Xt),Ns=n(Xt,"SPAN",{});var Cr=a(Ns);Zn=i(Cr,"mT5"),Cr.forEach(s),Xt.forEach(s),Nr=m(e),K=n(e,"H2",{class:!0});var Kt=a(K);fe=n(Kt,"A",{id:!0,class:!0,href:!0});var Ar=a(fe);Ds=n(Ar,"SPAN",{});var Sr=a(Ds);v(We.$$.fragment,Sr),Sr.forEach(s),Ar.forEach(s),ea=m(Kt),Is=n(Kt,"SPAN",{});var Lr=a(Is);ta=i(Lr,"Overview"),Lr.forEach(s),Kt.forEach(s),Dr=m(e),ue=n(e,"P",{});var Jt=a(ue);sa=i(Jt,"The mT5 model was presented in "),Be=n(Jt,"A",{href:!0,rel:!0});var di=a(Be);ra=i(di,"mT5: A massively multilingual pre-trained text-to-text transformer"),di.forEach(s),na=i(Jt,` by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.`),Jt.forEach(s),Ir=m(e),Qt=n(e,"P",{});var pi=a(Qt);aa=i(pi,"The abstract from the paper is the following:"),pi.forEach(s),Gr=m(e),Yt=n(e,"P",{});var mi=a(Yt);Gs=n(mi,"EM",{});var ci=a(Gs);oa=i(ci,`The recent \u201CText-to-Text Transfer Transformer\u201D (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent \u201Caccidental translation\u201D in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.`),ci.forEach(s),mi.forEach(s),Or=m(e),he=n(e,"P",{});var Tn=a(he);la=i(Tn,"Note: mT5 was only pre-trained on "),He=n(Tn,"A",{href:!0,rel:!0});var fi=a(He);ia=i(fi,"mC4"),fi.forEach(s),da=i(Tn,` excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there\u2019s no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.`),Tn.forEach(s),Vr=m(e),Zt=n(e,"P",{});var ui=a(Zt);pa=i(ui,"Google has released the following variants:"),ui.forEach(s),Ur=m(e),F=n(e,"UL",{});var R=a(F);Os=n(R,"LI",{});var hi=a(Os);Vs=n(hi,"P",{});var gi=a(Vs);Re=n(gi,"A",{href:!0,rel:!0});var _i=a(Re);ma=i(_i,"google/mt5-small"),_i.forEach(s),gi.forEach(s),hi.forEach(s),ca=m(R),Us=n(R,"LI",{});var ki=a(Us);Ws=n(ki,"P",{});var Ti=a(Ws);Xe=n(Ti,"A",{href:!0,rel:!0});var vi=a(Xe);fa=i(vi,"google/mt5-base"),vi.forEach(s),Ti.forEach(s),ki.forEach(s),ua=m(R),Bs=n(R,"LI",{});var $i=a(Bs);Hs=n($i,"P",{});var bi=a(Hs);Ke=n(bi,"A",{href:!0,rel:!0});var wi=a(Ke);ha=i(wi,"google/mt5-large"),wi.forEach(s),bi.forEach(s),$i.forEach(s),ga=m(R),Rs=n(R,"LI",{});var yi=a(Rs);Xs=n(yi,"P",{});var xi=a(Xs);Je=n(xi,"A",{href:!0,rel:!0});var Mi=a(Je);_a=i(Mi,"google/mt5-xl"),Mi.forEach(s),xi.forEach(s),yi.forEach(s),ka=m(R),Ks=n(R,"LI",{});var zi=a(Ks);es=n(zi,"P",{});var ni=a(es);Qe=n(ni,"A",{href:!0,rel:!0});var Ei=a(Qe);Ta=i(Ei,"google/mt5-xxl"),Ei.forEach(s),va=i(ni,"."),ni.forEach(s),zi.forEach(s),R.forEach(s),Wr=m(e),W=n(e,"P",{});var bs=a(W);$a=i(bs,"This model was contributed by "),Ye=n(bs,"A",{href:!0,rel:!0});var qi=a(Ye);ba=i(qi,"patrickvonplaten"),qi.forEach(s),wa=i(bs,`. The original code can be
found `),Ze=n(bs,"A",{href:!0,rel:!0});var Fi=a(Ze);ya=i(Fi,"here"),Fi.forEach(s),xa=i(bs,"."),bs.forEach(s),Br=m(e),J=n(e,"H2",{class:!0});var vn=a(J);ge=n(vn,"A",{id:!0,class:!0,href:!0});var ji=a(ge);Js=n(ji,"SPAN",{});var Pi=a(Js);v(et.$$.fragment,Pi),Pi.forEach(s),ji.forEach(s),Ma=m(vn),Qs=n(vn,"SPAN",{});var Ci=a(Qs);za=i(Ci,"MT5Config"),Ci.forEach(s),vn.forEach(s),Hr=m(e),C=n(e,"DIV",{class:!0});var ws=a(C);v(tt.$$.fragment,ws),Ea=m(ws),A=n(ws,"P",{});var Ue=a(A);qa=i(Ue,"This is the configuration class to store the configuration of a "),ts=n(Ue,"A",{href:!0});var Ai=a(ts);Fa=i(Ai,"MT5Model"),Ai.forEach(s),ja=i(Ue," or a "),ss=n(Ue,"A",{href:!0});var Si=a(ss);Pa=i(Si,"TFMT5Model"),Si.forEach(s),Ca=i(Ue,`. It is used to
instantiate a mT5 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the mT5
`),st=n(Ue,"A",{href:!0,rel:!0});var Li=a(st);Aa=i(Li,"google/mt5-small"),Li.forEach(s),Sa=i(Ue," architecture."),Ue.forEach(s),La=m(ws),Q=n(ws,"P",{});var ys=a(Q);Na=i(ys,"Configuration objects inherit from "),rs=n(ys,"A",{href:!0});var Ni=a(rs);Da=i(Ni,"PretrainedConfig"),Ni.forEach(s),Ia=i(ys,` and can be used to control the model outputs. Read the
documentation from `),ns=n(ys,"A",{href:!0});var Di=a(ns);Ga=i(Di,"PretrainedConfig"),Di.forEach(s),Oa=i(ys," for more information."),ys.forEach(s),ws.forEach(s),Rr=m(e),Y=n(e,"H2",{class:!0});var $n=a(Y);_e=n($n,"A",{id:!0,class:!0,href:!0});var Ii=a(_e);Ys=n(Ii,"SPAN",{});var Gi=a(Ys);v(rt.$$.fragment,Gi),Gi.forEach(s),Ii.forEach(s),Va=m($n),Zs=n($n,"SPAN",{});var Oi=a(Zs);Ua=i(Oi,"MT5Tokenizer"),Oi.forEach(s),$n.forEach(s),Xr=m(e),z=n(e,"DIV",{class:!0});var j=a(z);v(nt.$$.fragment,j),Wa=m(j),at=n(j,"P",{});var bn=a(at);Ba=i(bn,"Construct a T5 tokenizer. Based on "),ot=n(bn,"A",{href:!0,rel:!0});var Vi=a(ot);Ha=i(Vi,"SentencePiece"),Vi.forEach(s),Ra=i(bn,"."),bn.forEach(s),Xa=m(j),lt=n(j,"P",{});var wn=a(lt);Ka=i(wn,"This tokenizer inherits from "),as=n(wn,"A",{href:!0});var Ui=a(as);Ja=i(Ui,"PreTrainedTokenizer"),Ui.forEach(s),Qa=i(wn,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),wn.forEach(s),Ya=m(j),B=n(j,"DIV",{class:!0});var xs=a(B);v(it.$$.fragment,xs),Za=m(xs),er=n(xs,"P",{});var Wi=a(er);eo=i(Wi,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),Wi.forEach(s),to=m(xs),dt=n(xs,"UL",{});var yn=a(dt);os=n(yn,"LI",{});var ai=a(os);so=i(ai,"single sequence: "),tr=n(ai,"CODE",{});var Bi=a(tr);ro=i(Bi,"X </s>"),Bi.forEach(s),ai.forEach(s),no=m(yn),ls=n(yn,"LI",{});var oi=a(ls);ao=i(oi,"pair of sequences: "),sr=n(oi,"CODE",{});var Hi=a(sr);oo=i(Hi,"A </s> B </s>"),Hi.forEach(s),oi.forEach(s),yn.forEach(s),xs.forEach(s),lo=m(j),ke=n(j,"DIV",{class:!0});var xn=a(ke);v(pt.$$.fragment,xn),io=m(xn),rr=n(xn,"P",{});var Ri=a(rr);po=i(Ri,"Converts a sequence of tokens (string) in a single string."),Ri.forEach(s),xn.forEach(s),mo=m(j),Te=n(j,"DIV",{class:!0});var Mn=a(Te);v(mt.$$.fragment,Mn),co=m(Mn),nr=n(Mn,"P",{});var Xi=a(nr);fo=i(Xi,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),Xi.forEach(s),Mn.forEach(s),uo=m(j),ve=n(j,"DIV",{class:!0});var zn=a(ve);v(ct.$$.fragment,zn),ho=m(zn),ft=n(zn,"P",{});var En=a(ft);go=i(En,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),ar=n(En,"CODE",{});var Ki=a(ar);_o=i(Ki,"prepare_for_model"),Ki.forEach(s),ko=i(En," method."),En.forEach(s),zn.forEach(s),j.forEach(s),Kr=m(e),$e=n(e,"P",{});var qn=a($e);To=i(qn,"See "),is=n(qn,"A",{href:!0});var Ji=a(is);vo=i(Ji,"T5Tokenizer"),Ji.forEach(s),$o=i(qn," for all details."),qn.forEach(s),Jr=m(e),Z=n(e,"H2",{class:!0});var Fn=a(Z);be=n(Fn,"A",{id:!0,class:!0,href:!0});var Qi=a(be);or=n(Qi,"SPAN",{});var Yi=a(or);v(ut.$$.fragment,Yi),Yi.forEach(s),Qi.forEach(s),bo=m(Fn),lr=n(Fn,"SPAN",{});var Zi=a(lr);wo=i(Zi,"MT5TokenizerFast"),Zi.forEach(s),Fn.forEach(s),Qr=m(e),q=n(e,"DIV",{class:!0});var X=a(q);v(ht.$$.fragment,X),yo=m(X),ee=n(X,"P",{});var Ms=a(ee);xo=i(Ms,"Construct a \u201Cfast\u201D T5 tokenizer (backed by HuggingFace\u2019s "),ir=n(Ms,"EM",{});var ed=a(ir);Mo=i(ed,"tokenizers"),ed.forEach(s),zo=i(Ms,` library). Based on
`),gt=n(Ms,"A",{href:!0,rel:!0});var td=a(gt);Eo=i(td,"Unigram"),td.forEach(s),qo=i(Ms,"."),Ms.forEach(s),Fo=m(X),_t=n(X,"P",{});var jn=a(_t);jo=i(jn,"This tokenizer inherits from "),ds=n(jn,"A",{href:!0});var sd=a(ds);Po=i(sd,"PreTrainedTokenizerFast"),sd.forEach(s),Co=i(jn,` which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`),jn.forEach(s),Ao=m(X),H=n(X,"DIV",{class:!0});var zs=a(H);v(kt.$$.fragment,zs),So=m(zs),dr=n(zs,"P",{});var rd=a(dr);Lo=i(rd,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:`),rd.forEach(s),No=m(zs),Tt=n(zs,"UL",{});var Pn=a(Tt);ps=n(Pn,"LI",{});var li=a(ps);Do=i(li,"single sequence: "),pr=n(li,"CODE",{});var nd=a(pr);Io=i(nd,"X </s>"),nd.forEach(s),li.forEach(s),Go=m(Pn),ms=n(Pn,"LI",{});var ii=a(ms);Oo=i(ii,"pair of sequences: "),mr=n(ii,"CODE",{});var ad=a(mr);Vo=i(ad,"A </s> B </s>"),ad.forEach(s),ii.forEach(s),Pn.forEach(s),zs.forEach(s),Uo=m(X),we=n(X,"DIV",{class:!0});var Cn=a(we);v(vt.$$.fragment,Cn),Wo=m(Cn),cr=n(Cn,"P",{});var od=a(cr);Bo=i(od,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.`),od.forEach(s),Cn.forEach(s),X.forEach(s),Yr=m(e),ye=n(e,"P",{});var An=a(ye);Ho=i(An,"See "),cs=n(An,"A",{href:!0});var ld=a(cs);Ro=i(ld,"T5TokenizerFast"),ld.forEach(s),Xo=i(An," for all details."),An.forEach(s),Zr=m(e),te=n(e,"H2",{class:!0});var Sn=a(te);xe=n(Sn,"A",{id:!0,class:!0,href:!0});var id=a(xe);fr=n(id,"SPAN",{});var dd=a(fr);v($t.$$.fragment,dd),dd.forEach(s),id.forEach(s),Ko=m(Sn),ur=n(Sn,"SPAN",{});var pd=a(ur);Jo=i(pd,"MT5Model"),pd.forEach(s),Sn.forEach(s),en=m(e),S=n(e,"DIV",{class:!0});var Es=a(S);v(bt.$$.fragment,Es),Qo=m(Es),wt=n(Es,"P",{});var Ln=a(wt);Yo=i(Ln,"This class overrides "),fs=n(Ln,"A",{href:!0});var md=a(fs);Zo=i(md,"T5Model"),md.forEach(s),el=i(Ln,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Ln.forEach(s),tl=m(Es),v(Me.$$.fragment,Es),Es.forEach(s),tn=m(e),se=n(e,"H2",{class:!0});var Nn=a(se);ze=n(Nn,"A",{id:!0,class:!0,href:!0});var cd=a(ze);hr=n(cd,"SPAN",{});var fd=a(hr);v(yt.$$.fragment,fd),fd.forEach(s),cd.forEach(s),sl=m(Nn),gr=n(Nn,"SPAN",{});var ud=a(gr);rl=i(ud,"MT5ForConditionalGeneration"),ud.forEach(s),Nn.forEach(s),sn=m(e),L=n(e,"DIV",{class:!0});var qs=a(L);v(xt.$$.fragment,qs),nl=m(qs),Mt=n(qs,"P",{});var Dn=a(Mt);al=i(Dn,"This class overrides "),us=n(Dn,"A",{href:!0});var hd=a(us);ol=i(hd,"T5ForConditionalGeneration"),hd.forEach(s),ll=i(Dn,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Dn.forEach(s),il=m(qs),v(Ee.$$.fragment,qs),qs.forEach(s),rn=m(e),re=n(e,"H2",{class:!0});var In=a(re);qe=n(In,"A",{id:!0,class:!0,href:!0});var gd=a(qe);_r=n(gd,"SPAN",{});var _d=a(_r);v(zt.$$.fragment,_d),_d.forEach(s),gd.forEach(s),dl=m(In),kr=n(In,"SPAN",{});var kd=a(kr);pl=i(kd,"MT5EncoderModel"),kd.forEach(s),In.forEach(s),nn=m(e),N=n(e,"DIV",{class:!0});var Fs=a(N);v(Et.$$.fragment,Fs),ml=m(Fs),qt=n(Fs,"P",{});var Gn=a(qt);cl=i(Gn,"This class overrides "),hs=n(Gn,"A",{href:!0});var Td=a(hs);fl=i(Td,"T5EncoderModel"),Td.forEach(s),ul=i(Gn,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Gn.forEach(s),hl=m(Fs),v(Fe.$$.fragment,Fs),Fs.forEach(s),an=m(e),ne=n(e,"H2",{class:!0});var On=a(ne);je=n(On,"A",{id:!0,class:!0,href:!0});var vd=a(je);Tr=n(vd,"SPAN",{});var $d=a(Tr);v(Ft.$$.fragment,$d),$d.forEach(s),vd.forEach(s),gl=m(On),vr=n(On,"SPAN",{});var bd=a(vr);_l=i(bd,"TFMT5Model"),bd.forEach(s),On.forEach(s),on=m(e),D=n(e,"DIV",{class:!0});var js=a(D);v(jt.$$.fragment,js),kl=m(js),Pt=n(js,"P",{});var Vn=a(Pt);Tl=i(Vn,"This class overrides "),gs=n(Vn,"A",{href:!0});var wd=a(gs);vl=i(wd,"TFT5Model"),wd.forEach(s),$l=i(Vn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Vn.forEach(s),bl=m(js),v(Pe.$$.fragment,js),js.forEach(s),ln=m(e),ae=n(e,"H2",{class:!0});var Un=a(ae);Ce=n(Un,"A",{id:!0,class:!0,href:!0});var yd=a(Ce);$r=n(yd,"SPAN",{});var xd=a($r);v(Ct.$$.fragment,xd),xd.forEach(s),yd.forEach(s),wl=m(Un),br=n(Un,"SPAN",{});var Md=a(br);yl=i(Md,"TFMT5ForConditionalGeneration"),Md.forEach(s),Un.forEach(s),dn=m(e),I=n(e,"DIV",{class:!0});var Ps=a(I);v(At.$$.fragment,Ps),xl=m(Ps),St=n(Ps,"P",{});var Wn=a(St);Ml=i(Wn,"This class overrides "),_s=n(Wn,"A",{href:!0});var zd=a(_s);zl=i(zd,"TFT5ForConditionalGeneration"),zd.forEach(s),El=i(Wn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Wn.forEach(s),ql=m(Ps),v(Ae.$$.fragment,Ps),Ps.forEach(s),pn=m(e),oe=n(e,"H2",{class:!0});var Bn=a(oe);Se=n(Bn,"A",{id:!0,class:!0,href:!0});var Ed=a(Se);wr=n(Ed,"SPAN",{});var qd=a(wr);v(Lt.$$.fragment,qd),qd.forEach(s),Ed.forEach(s),Fl=m(Bn),yr=n(Bn,"SPAN",{});var Fd=a(yr);jl=i(Fd,"TFMT5EncoderModel"),Fd.forEach(s),Bn.forEach(s),mn=m(e),G=n(e,"DIV",{class:!0});var Cs=a(G);v(Nt.$$.fragment,Cs),Pl=m(Cs),Dt=n(Cs,"P",{});var Hn=a(Dt);Cl=i(Hn,"This class overrides "),ks=n(Hn,"A",{href:!0});var jd=a(ks);Al=i(jd,"TFT5EncoderModel"),jd.forEach(s),Sl=i(Hn,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Hn.forEach(s),Ll=m(Cs),v(Le.$$.fragment,Cs),Cs.forEach(s),cn=m(e),le=n(e,"H2",{class:!0});var Rn=a(le);Ne=n(Rn,"A",{id:!0,class:!0,href:!0});var Pd=a(Ne);xr=n(Pd,"SPAN",{});var Cd=a(xr);v(It.$$.fragment,Cd),Cd.forEach(s),Pd.forEach(s),Nl=m(Rn),Mr=n(Rn,"SPAN",{});var Ad=a(Mr);Dl=i(Ad,"FlaxMT5Model"),Ad.forEach(s),Rn.forEach(s),fn=m(e),O=n(e,"DIV",{class:!0});var As=a(O);v(Gt.$$.fragment,As),Il=m(As),Ot=n(As,"P",{});var Xn=a(Ot);Gl=i(Xn,"This class overrides "),Ts=n(Xn,"A",{href:!0});var Sd=a(Ts);Ol=i(Sd,"FlaxT5Model"),Sd.forEach(s),Vl=i(Xn,`. Please check the superclass for the appropriate documentation alongside usage
examples.`),Xn.forEach(s),Ul=m(As),v(De.$$.fragment,As),As.forEach(s),un=m(e),ie=n(e,"H2",{class:!0});var Kn=a(ie);Ie=n(Kn,"A",{id:!0,class:!0,href:!0});var Ld=a(Ie);zr=n(Ld,"SPAN",{});var Nd=a(zr);v(Vt.$$.fragment,Nd),Nd.forEach(s),Ld.forEach(s),Wl=m(Kn),Er=n(Kn,"SPAN",{});var Dd=a(Er);Bl=i(Dd,"FlaxMT5ForConditionalGeneration"),Dd.forEach(s),Kn.forEach(s),hn=m(e),V=n(e,"DIV",{class:!0});var Ss=a(V);v(Ut.$$.fragment,Ss),Hl=m(Ss),Wt=n(Ss,"P",{});var Jn=a(Wt);Rl=i(Jn,"This class overrides "),vs=n(Jn,"A",{href:!0});var Id=a(vs);Xl=i(Id,"FlaxT5ForConditionalGeneration"),Id.forEach(s),Kl=i(Jn,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),Jn.forEach(s),Jl=m(Ss),v(Ge.$$.fragment,Ss),Ss.forEach(s),gn=m(e),de=n(e,"H2",{class:!0});var Qn=a(de);Oe=n(Qn,"A",{id:!0,class:!0,href:!0});var Gd=a(Oe);qr=n(Gd,"SPAN",{});var Od=a(qr);v(Bt.$$.fragment,Od),Od.forEach(s),Gd.forEach(s),Ql=m(Qn),Fr=n(Qn,"SPAN",{});var Vd=a(Fr);Yl=i(Vd,"FlaxMT5EncoderModel"),Vd.forEach(s),Qn.forEach(s),_n=m(e),U=n(e,"DIV",{class:!0});var Ls=a(U);v(Ht.$$.fragment,Ls),Zl=m(Ls),Rt=n(Ls,"P",{});var Yn=a(Rt);ei=i(Yn,"This class overrides "),$s=n(Yn,"A",{href:!0});var Ud=a($s);ti=i(Ud,"FlaxT5EncoderModel"),Ud.forEach(s),si=i(Yn,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Yn.forEach(s),ri=m(Ls),v(Ve.$$.fragment,Ls),Ls.forEach(s),this.h()},h(){d(h,"name","hf:doc:metadata"),d(h,"content",JSON.stringify(ap)),d(u,"id","mt5"),d(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(u,"href","#mt5"),d(_,"class","relative group"),d(fe,"id","overview"),d(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fe,"href","#overview"),d(K,"class","relative group"),d(Be,"href","https://arxiv.org/abs/2010.11934"),d(Be,"rel","nofollow"),d(He,"href","https://huggingface.co/datasets/mc4"),d(He,"rel","nofollow"),d(Re,"href","https://huggingface.co/google/mt5-small"),d(Re,"rel","nofollow"),d(Xe,"href","https://huggingface.co/google/mt5-base"),d(Xe,"rel","nofollow"),d(Ke,"href","https://huggingface.co/google/mt5-large"),d(Ke,"rel","nofollow"),d(Je,"href","https://huggingface.co/google/mt5-xl"),d(Je,"rel","nofollow"),d(Qe,"href","https://huggingface.co/google/mt5-xxl"),d(Qe,"rel","nofollow"),d(Ye,"href","https://huggingface.co/patrickvonplaten"),d(Ye,"rel","nofollow"),d(Ze,"href","https://github.com/google-research/multilingual-t5"),d(Ze,"rel","nofollow"),d(ge,"id","transformers.MT5Config"),d(ge,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ge,"href","#transformers.MT5Config"),d(J,"class","relative group"),d(ts,"href","/docs/transformers/main/en/model_doc/mt5#transformers.MT5Model"),d(ss,"href","/docs/transformers/main/en/model_doc/mt5#transformers.TFMT5Model"),d(st,"href","https://huggingface.co/google/mt5-small"),d(st,"rel","nofollow"),d(rs,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(ns,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),d(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(_e,"id","transformers.T5Tokenizer"),d(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_e,"href","#transformers.T5Tokenizer"),d(Y,"class","relative group"),d(ot,"href","https://github.com/google/sentencepiece"),d(ot,"rel","nofollow"),d(as,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),d(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(is,"href","/docs/transformers/main/en/model_doc/t5#transformers.T5Tokenizer"),d(be,"id","transformers.T5TokenizerFast"),d(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(be,"href","#transformers.T5TokenizerFast"),d(Z,"class","relative group"),d(gt,"href","https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models"),d(gt,"rel","nofollow"),d(ds,"href","/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"),d(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(cs,"href","/docs/transformers/main/en/model_doc/t5#transformers.T5TokenizerFast"),d(xe,"id","transformers.MT5Model"),d(xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xe,"href","#transformers.MT5Model"),d(te,"class","relative group"),d(fs,"href","/docs/transformers/main/en/model_doc/t5#transformers.T5Model"),d(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(ze,"id","transformers.MT5ForConditionalGeneration"),d(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(ze,"href","#transformers.MT5ForConditionalGeneration"),d(se,"class","relative group"),d(us,"href","/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(qe,"id","transformers.MT5EncoderModel"),d(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(qe,"href","#transformers.MT5EncoderModel"),d(re,"class","relative group"),d(hs,"href","/docs/transformers/main/en/model_doc/t5#transformers.T5EncoderModel"),d(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(je,"id","transformers.TFMT5Model"),d(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(je,"href","#transformers.TFMT5Model"),d(ne,"class","relative group"),d(gs,"href","/docs/transformers/main/en/model_doc/t5#transformers.TFT5Model"),d(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ce,"id","transformers.TFMT5ForConditionalGeneration"),d(Ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ce,"href","#transformers.TFMT5ForConditionalGeneration"),d(ae,"class","relative group"),d(_s,"href","/docs/transformers/main/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Se,"id","transformers.TFMT5EncoderModel"),d(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Se,"href","#transformers.TFMT5EncoderModel"),d(oe,"class","relative group"),d(ks,"href","/docs/transformers/main/en/model_doc/t5#transformers.TFT5EncoderModel"),d(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ne,"id","transformers.FlaxMT5Model"),d(Ne,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ne,"href","#transformers.FlaxMT5Model"),d(le,"class","relative group"),d(Ts,"href","/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5Model"),d(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Ie,"id","transformers.FlaxMT5ForConditionalGeneration"),d(Ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ie,"href","#transformers.FlaxMT5ForConditionalGeneration"),d(ie,"class","relative group"),d(vs,"href","/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),d(Oe,"id","transformers.FlaxMT5EncoderModel"),d(Oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Oe,"href","#transformers.FlaxMT5EncoderModel"),d(de,"class","relative group"),d($s,"href","/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5EncoderModel"),d(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,c){t(document.head,h),f(e,x,c),f(e,_,c),t(_,u),t(u,k),$(o,k,null),t(_,g),t(_,Ns),t(Ns,Zn),f(e,Nr,c),f(e,K,c),t(K,fe),t(fe,Ds),$(We,Ds,null),t(K,ea),t(K,Is),t(Is,ta),f(e,Dr,c),f(e,ue,c),t(ue,sa),t(ue,Be),t(Be,ra),t(ue,na),f(e,Ir,c),f(e,Qt,c),t(Qt,aa),f(e,Gr,c),f(e,Yt,c),t(Yt,Gs),t(Gs,oa),f(e,Or,c),f(e,he,c),t(he,la),t(he,He),t(He,ia),t(he,da),f(e,Vr,c),f(e,Zt,c),t(Zt,pa),f(e,Ur,c),f(e,F,c),t(F,Os),t(Os,Vs),t(Vs,Re),t(Re,ma),t(F,ca),t(F,Us),t(Us,Ws),t(Ws,Xe),t(Xe,fa),t(F,ua),t(F,Bs),t(Bs,Hs),t(Hs,Ke),t(Ke,ha),t(F,ga),t(F,Rs),t(Rs,Xs),t(Xs,Je),t(Je,_a),t(F,ka),t(F,Ks),t(Ks,es),t(es,Qe),t(Qe,Ta),t(es,va),f(e,Wr,c),f(e,W,c),t(W,$a),t(W,Ye),t(Ye,ba),t(W,wa),t(W,Ze),t(Ze,ya),t(W,xa),f(e,Br,c),f(e,J,c),t(J,ge),t(ge,Js),$(et,Js,null),t(J,Ma),t(J,Qs),t(Qs,za),f(e,Hr,c),f(e,C,c),$(tt,C,null),t(C,Ea),t(C,A),t(A,qa),t(A,ts),t(ts,Fa),t(A,ja),t(A,ss),t(ss,Pa),t(A,Ca),t(A,st),t(st,Aa),t(A,Sa),t(C,La),t(C,Q),t(Q,Na),t(Q,rs),t(rs,Da),t(Q,Ia),t(Q,ns),t(ns,Ga),t(Q,Oa),f(e,Rr,c),f(e,Y,c),t(Y,_e),t(_e,Ys),$(rt,Ys,null),t(Y,Va),t(Y,Zs),t(Zs,Ua),f(e,Xr,c),f(e,z,c),$(nt,z,null),t(z,Wa),t(z,at),t(at,Ba),t(at,ot),t(ot,Ha),t(at,Ra),t(z,Xa),t(z,lt),t(lt,Ka),t(lt,as),t(as,Ja),t(lt,Qa),t(z,Ya),t(z,B),$(it,B,null),t(B,Za),t(B,er),t(er,eo),t(B,to),t(B,dt),t(dt,os),t(os,so),t(os,tr),t(tr,ro),t(dt,no),t(dt,ls),t(ls,ao),t(ls,sr),t(sr,oo),t(z,lo),t(z,ke),$(pt,ke,null),t(ke,io),t(ke,rr),t(rr,po),t(z,mo),t(z,Te),$(mt,Te,null),t(Te,co),t(Te,nr),t(nr,fo),t(z,uo),t(z,ve),$(ct,ve,null),t(ve,ho),t(ve,ft),t(ft,go),t(ft,ar),t(ar,_o),t(ft,ko),f(e,Kr,c),f(e,$e,c),t($e,To),t($e,is),t(is,vo),t($e,$o),f(e,Jr,c),f(e,Z,c),t(Z,be),t(be,or),$(ut,or,null),t(Z,bo),t(Z,lr),t(lr,wo),f(e,Qr,c),f(e,q,c),$(ht,q,null),t(q,yo),t(q,ee),t(ee,xo),t(ee,ir),t(ir,Mo),t(ee,zo),t(ee,gt),t(gt,Eo),t(ee,qo),t(q,Fo),t(q,_t),t(_t,jo),t(_t,ds),t(ds,Po),t(_t,Co),t(q,Ao),t(q,H),$(kt,H,null),t(H,So),t(H,dr),t(dr,Lo),t(H,No),t(H,Tt),t(Tt,ps),t(ps,Do),t(ps,pr),t(pr,Io),t(Tt,Go),t(Tt,ms),t(ms,Oo),t(ms,mr),t(mr,Vo),t(q,Uo),t(q,we),$(vt,we,null),t(we,Wo),t(we,cr),t(cr,Bo),f(e,Yr,c),f(e,ye,c),t(ye,Ho),t(ye,cs),t(cs,Ro),t(ye,Xo),f(e,Zr,c),f(e,te,c),t(te,xe),t(xe,fr),$($t,fr,null),t(te,Ko),t(te,ur),t(ur,Jo),f(e,en,c),f(e,S,c),$(bt,S,null),t(S,Qo),t(S,wt),t(wt,Yo),t(wt,fs),t(fs,Zo),t(wt,el),t(S,tl),$(Me,S,null),f(e,tn,c),f(e,se,c),t(se,ze),t(ze,hr),$(yt,hr,null),t(se,sl),t(se,gr),t(gr,rl),f(e,sn,c),f(e,L,c),$(xt,L,null),t(L,nl),t(L,Mt),t(Mt,al),t(Mt,us),t(us,ol),t(Mt,ll),t(L,il),$(Ee,L,null),f(e,rn,c),f(e,re,c),t(re,qe),t(qe,_r),$(zt,_r,null),t(re,dl),t(re,kr),t(kr,pl),f(e,nn,c),f(e,N,c),$(Et,N,null),t(N,ml),t(N,qt),t(qt,cl),t(qt,hs),t(hs,fl),t(qt,ul),t(N,hl),$(Fe,N,null),f(e,an,c),f(e,ne,c),t(ne,je),t(je,Tr),$(Ft,Tr,null),t(ne,gl),t(ne,vr),t(vr,_l),f(e,on,c),f(e,D,c),$(jt,D,null),t(D,kl),t(D,Pt),t(Pt,Tl),t(Pt,gs),t(gs,vl),t(Pt,$l),t(D,bl),$(Pe,D,null),f(e,ln,c),f(e,ae,c),t(ae,Ce),t(Ce,$r),$(Ct,$r,null),t(ae,wl),t(ae,br),t(br,yl),f(e,dn,c),f(e,I,c),$(At,I,null),t(I,xl),t(I,St),t(St,Ml),t(St,_s),t(_s,zl),t(St,El),t(I,ql),$(Ae,I,null),f(e,pn,c),f(e,oe,c),t(oe,Se),t(Se,wr),$(Lt,wr,null),t(oe,Fl),t(oe,yr),t(yr,jl),f(e,mn,c),f(e,G,c),$(Nt,G,null),t(G,Pl),t(G,Dt),t(Dt,Cl),t(Dt,ks),t(ks,Al),t(Dt,Sl),t(G,Ll),$(Le,G,null),f(e,cn,c),f(e,le,c),t(le,Ne),t(Ne,xr),$(It,xr,null),t(le,Nl),t(le,Mr),t(Mr,Dl),f(e,fn,c),f(e,O,c),$(Gt,O,null),t(O,Il),t(O,Ot),t(Ot,Gl),t(Ot,Ts),t(Ts,Ol),t(Ot,Vl),t(O,Ul),$(De,O,null),f(e,un,c),f(e,ie,c),t(ie,Ie),t(Ie,zr),$(Vt,zr,null),t(ie,Wl),t(ie,Er),t(Er,Bl),f(e,hn,c),f(e,V,c),$(Ut,V,null),t(V,Hl),t(V,Wt),t(Wt,Rl),t(Wt,vs),t(vs,Xl),t(Wt,Kl),t(V,Jl),$(Ge,V,null),f(e,gn,c),f(e,de,c),t(de,Oe),t(Oe,qr),$(Bt,qr,null),t(de,Ql),t(de,Fr),t(Fr,Yl),f(e,_n,c),f(e,U,c),$(Ht,U,null),t(U,Zl),t(U,Rt),t(Rt,ei),t(Rt,$s),t($s,ti),t(Rt,si),t(U,ri),$(Ve,U,null),kn=!0},p(e,[c]){const Xt={};c&2&&(Xt.$$scope={dirty:c,ctx:e}),Me.$set(Xt);const jr={};c&2&&(jr.$$scope={dirty:c,ctx:e}),Ee.$set(jr);const Pr={};c&2&&(Pr.$$scope={dirty:c,ctx:e}),Fe.$set(Pr);const Cr={};c&2&&(Cr.$$scope={dirty:c,ctx:e}),Pe.$set(Cr);const Kt={};c&2&&(Kt.$$scope={dirty:c,ctx:e}),Ae.$set(Kt);const Ar={};c&2&&(Ar.$$scope={dirty:c,ctx:e}),Le.$set(Ar);const Sr={};c&2&&(Sr.$$scope={dirty:c,ctx:e}),De.$set(Sr);const Lr={};c&2&&(Lr.$$scope={dirty:c,ctx:e}),Ge.$set(Lr);const Jt={};c&2&&(Jt.$$scope={dirty:c,ctx:e}),Ve.$set(Jt)},i(e){kn||(b(o.$$.fragment,e),b(We.$$.fragment,e),b(et.$$.fragment,e),b(tt.$$.fragment,e),b(rt.$$.fragment,e),b(nt.$$.fragment,e),b(it.$$.fragment,e),b(pt.$$.fragment,e),b(mt.$$.fragment,e),b(ct.$$.fragment,e),b(ut.$$.fragment,e),b(ht.$$.fragment,e),b(kt.$$.fragment,e),b(vt.$$.fragment,e),b($t.$$.fragment,e),b(bt.$$.fragment,e),b(Me.$$.fragment,e),b(yt.$$.fragment,e),b(xt.$$.fragment,e),b(Ee.$$.fragment,e),b(zt.$$.fragment,e),b(Et.$$.fragment,e),b(Fe.$$.fragment,e),b(Ft.$$.fragment,e),b(jt.$$.fragment,e),b(Pe.$$.fragment,e),b(Ct.$$.fragment,e),b(At.$$.fragment,e),b(Ae.$$.fragment,e),b(Lt.$$.fragment,e),b(Nt.$$.fragment,e),b(Le.$$.fragment,e),b(It.$$.fragment,e),b(Gt.$$.fragment,e),b(De.$$.fragment,e),b(Vt.$$.fragment,e),b(Ut.$$.fragment,e),b(Ge.$$.fragment,e),b(Bt.$$.fragment,e),b(Ht.$$.fragment,e),b(Ve.$$.fragment,e),kn=!0)},o(e){w(o.$$.fragment,e),w(We.$$.fragment,e),w(et.$$.fragment,e),w(tt.$$.fragment,e),w(rt.$$.fragment,e),w(nt.$$.fragment,e),w(it.$$.fragment,e),w(pt.$$.fragment,e),w(mt.$$.fragment,e),w(ct.$$.fragment,e),w(ut.$$.fragment,e),w(ht.$$.fragment,e),w(kt.$$.fragment,e),w(vt.$$.fragment,e),w($t.$$.fragment,e),w(bt.$$.fragment,e),w(Me.$$.fragment,e),w(yt.$$.fragment,e),w(xt.$$.fragment,e),w(Ee.$$.fragment,e),w(zt.$$.fragment,e),w(Et.$$.fragment,e),w(Fe.$$.fragment,e),w(Ft.$$.fragment,e),w(jt.$$.fragment,e),w(Pe.$$.fragment,e),w(Ct.$$.fragment,e),w(At.$$.fragment,e),w(Ae.$$.fragment,e),w(Lt.$$.fragment,e),w(Nt.$$.fragment,e),w(Le.$$.fragment,e),w(It.$$.fragment,e),w(Gt.$$.fragment,e),w(De.$$.fragment,e),w(Vt.$$.fragment,e),w(Ut.$$.fragment,e),w(Ge.$$.fragment,e),w(Bt.$$.fragment,e),w(Ht.$$.fragment,e),w(Ve.$$.fragment,e),kn=!1},d(e){s(h),e&&s(x),e&&s(_),y(o),e&&s(Nr),e&&s(K),y(We),e&&s(Dr),e&&s(ue),e&&s(Ir),e&&s(Qt),e&&s(Gr),e&&s(Yt),e&&s(Or),e&&s(he),e&&s(Vr),e&&s(Zt),e&&s(Ur),e&&s(F),e&&s(Wr),e&&s(W),e&&s(Br),e&&s(J),y(et),e&&s(Hr),e&&s(C),y(tt),e&&s(Rr),e&&s(Y),y(rt),e&&s(Xr),e&&s(z),y(nt),y(it),y(pt),y(mt),y(ct),e&&s(Kr),e&&s($e),e&&s(Jr),e&&s(Z),y(ut),e&&s(Qr),e&&s(q),y(ht),y(kt),y(vt),e&&s(Yr),e&&s(ye),e&&s(Zr),e&&s(te),y($t),e&&s(en),e&&s(S),y(bt),y(Me),e&&s(tn),e&&s(se),y(yt),e&&s(sn),e&&s(L),y(xt),y(Ee),e&&s(rn),e&&s(re),y(zt),e&&s(nn),e&&s(N),y(Et),y(Fe),e&&s(an),e&&s(ne),y(Ft),e&&s(on),e&&s(D),y(jt),y(Pe),e&&s(ln),e&&s(ae),y(Ct),e&&s(dn),e&&s(I),y(At),y(Ae),e&&s(pn),e&&s(oe),y(Lt),e&&s(mn),e&&s(G),y(Nt),y(Le),e&&s(cn),e&&s(le),y(It),e&&s(fn),e&&s(O),y(Gt),y(De),e&&s(un),e&&s(ie),y(Vt),e&&s(hn),e&&s(V),y(Ut),y(Ge),e&&s(gn),e&&s(de),y(Bt),e&&s(_n),e&&s(U),y(Ht),y(Ve)}}}const ap={local:"mt5",sections:[{local:"overview",title:"Overview"},{local:"transformers.MT5Config",title:"MT5Config"},{local:"transformers.T5Tokenizer",title:"MT5Tokenizer"},{local:"transformers.T5TokenizerFast",title:"MT5TokenizerFast"},{local:"transformers.MT5Model",title:"MT5Model"},{local:"transformers.MT5ForConditionalGeneration",title:"MT5ForConditionalGeneration"},{local:"transformers.MT5EncoderModel",title:"MT5EncoderModel"},{local:"transformers.TFMT5Model",title:"TFMT5Model"},{local:"transformers.TFMT5ForConditionalGeneration",title:"TFMT5ForConditionalGeneration"},{local:"transformers.TFMT5EncoderModel",title:"TFMT5EncoderModel"},{local:"transformers.FlaxMT5Model",title:"FlaxMT5Model"},{local:"transformers.FlaxMT5ForConditionalGeneration",title:"FlaxMT5ForConditionalGeneration"},{local:"transformers.FlaxMT5EncoderModel",title:"FlaxMT5EncoderModel"}],title:"mT5"};function op(M){return Xd(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cp extends Wd{constructor(h){super();Bd(this,h,op,np,Hd,{})}}export{cp as default,ap as metadata};
