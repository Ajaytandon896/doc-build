import{S as y2,i as $2,s as F2,e as n,k as l,w as v,t as a,M as D2,c as o,d as t,m as d,a as r,x as k,h as i,b as c,F as e,g as m,y as T,q as w,o as y,B as $,v as B2}from"../../chunks/vendor-6b77c823.js";import{T as ge}from"../../chunks/Tip-39098574.js";import{D as H}from"../../chunks/Docstring-abef54e3.js";import{C as re}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as ve}from"../../chunks/IconCopyLink-7a11ce68.js";function M2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function E2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function x2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function z2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function j2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function C2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function q2(j){let p,D,g,b,F,_,u,B,ce,K,E,G,S,X,pe,I,he,ae,N,P,Y,V,x,z,me,W,se,ue,R,ie,ee,A,le,L,ne,fe,q,te,U,de;return{c(){p=n("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=n("ul"),F=n("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),u=l(),B=n("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),E=n("p"),G=a("This second option is useful when using "),S=n("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=n("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=n("ul"),z=n("li"),me=a("a single Tensor with "),W=n("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=n("code"),ie=a("model(inputs_ids)"),ee=l(),A=n("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n("code"),ne=a("model([input_ids, attention_mask])"),fe=l(),q=n("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n("code"),de=a('model({"input_ids": input_ids})')},l(h){p=o(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),b=o(h,"UL",{});var J=r(b);F=o(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),u=d(J),B=o(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),K=d(h),E=o(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=o(O,"CODE",{});var be=r(S);X=i(be,"tf.keras.Model.fit"),be.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=o(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),V=d(h),x=o(h,"UL",{});var C=r(x);z=o(C,"LI",{});var Q=r(z);me=i(Q,"a single Tensor with "),W=o(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),ue=i(Q," only and nothing else: "),R=o(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=o(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o(Z,"CODE",{});var De=r(L);ne=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=o(C,"LI",{});var oe=r(q);te=i(oe,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o(oe,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),oe.forEach(t),C.forEach(t)},m(h,M){m(h,p,M),e(p,D),m(h,g,M),m(h,b,M),e(b,F),e(F,_),e(b,u),e(b,B),e(B,ce),m(h,K,M),m(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),m(h,N,M),m(h,P,M),e(P,Y),m(h,V,M),m(h,x,M),e(x,z),e(z,me),e(z,W),e(W,se),e(z,ue),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,ne),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(b),h&&t(K),h&&t(E),h&&t(N),h&&t(P),h&&t(V),h&&t(x)}}}function P2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function A2(j){let p,D,g,b,F,_,u,B,ce,K,E,G,S,X,pe,I,he,ae,N,P,Y,V,x,z,me,W,se,ue,R,ie,ee,A,le,L,ne,fe,q,te,U,de;return{c(){p=n("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=n("ul"),F=n("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),u=l(),B=n("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),E=n("p"),G=a("This second option is useful when using "),S=n("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=n("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=n("ul"),z=n("li"),me=a("a single Tensor with "),W=n("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=n("code"),ie=a("model(inputs_ids)"),ee=l(),A=n("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n("code"),ne=a("model([input_ids, attention_mask])"),fe=l(),q=n("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n("code"),de=a('model({"input_ids": input_ids})')},l(h){p=o(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),b=o(h,"UL",{});var J=r(b);F=o(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),u=d(J),B=o(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),K=d(h),E=o(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=o(O,"CODE",{});var be=r(S);X=i(be,"tf.keras.Model.fit"),be.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=o(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),V=d(h),x=o(h,"UL",{});var C=r(x);z=o(C,"LI",{});var Q=r(z);me=i(Q,"a single Tensor with "),W=o(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),ue=i(Q," only and nothing else: "),R=o(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=o(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o(Z,"CODE",{});var De=r(L);ne=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=o(C,"LI",{});var oe=r(q);te=i(oe,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o(oe,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),oe.forEach(t),C.forEach(t)},m(h,M){m(h,p,M),e(p,D),m(h,g,M),m(h,b,M),e(b,F),e(F,_),e(b,u),e(b,B),e(B,ce),m(h,K,M),m(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),m(h,N,M),m(h,P,M),e(P,Y),m(h,V,M),m(h,x,M),e(x,z),e(z,me),e(z,W),e(W,se),e(z,ue),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,ne),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(b),h&&t(K),h&&t(E),h&&t(N),h&&t(P),h&&t(V),h&&t(x)}}}function O2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function N2(j){let p,D,g,b,F,_,u,B,ce,K,E,G,S,X,pe,I,he,ae,N,P,Y,V,x,z,me,W,se,ue,R,ie,ee,A,le,L,ne,fe,q,te,U,de;return{c(){p=n("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=n("ul"),F=n("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),u=l(),B=n("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),E=n("p"),G=a("This second option is useful when using "),S=n("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=n("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=n("ul"),z=n("li"),me=a("a single Tensor with "),W=n("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=n("code"),ie=a("model(inputs_ids)"),ee=l(),A=n("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n("code"),ne=a("model([input_ids, attention_mask])"),fe=l(),q=n("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n("code"),de=a('model({"input_ids": input_ids})')},l(h){p=o(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),b=o(h,"UL",{});var J=r(b);F=o(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),u=d(J),B=o(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),K=d(h),E=o(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=o(O,"CODE",{});var be=r(S);X=i(be,"tf.keras.Model.fit"),be.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=o(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),V=d(h),x=o(h,"UL",{});var C=r(x);z=o(C,"LI",{});var Q=r(z);me=i(Q,"a single Tensor with "),W=o(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),ue=i(Q," only and nothing else: "),R=o(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=o(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o(Z,"CODE",{});var De=r(L);ne=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=o(C,"LI",{});var oe=r(q);te=i(oe,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o(oe,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),oe.forEach(t),C.forEach(t)},m(h,M){m(h,p,M),e(p,D),m(h,g,M),m(h,b,M),e(b,F),e(F,_),e(b,u),e(b,B),e(B,ce),m(h,K,M),m(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),m(h,N,M),m(h,P,M),e(P,Y),m(h,V,M),m(h,x,M),e(x,z),e(z,me),e(z,W),e(W,se),e(z,ue),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,ne),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(b),h&&t(K),h&&t(E),h&&t(N),h&&t(P),h&&t(V),h&&t(x)}}}function L2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function S2(j){let p,D,g,b,F,_,u,B,ce,K,E,G,S,X,pe,I,he,ae,N,P,Y,V,x,z,me,W,se,ue,R,ie,ee,A,le,L,ne,fe,q,te,U,de;return{c(){p=n("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=n("ul"),F=n("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),u=l(),B=n("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),E=n("p"),G=a("This second option is useful when using "),S=n("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=n("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=n("ul"),z=n("li"),me=a("a single Tensor with "),W=n("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=n("code"),ie=a("model(inputs_ids)"),ee=l(),A=n("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n("code"),ne=a("model([input_ids, attention_mask])"),fe=l(),q=n("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n("code"),de=a('model({"input_ids": input_ids})')},l(h){p=o(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),b=o(h,"UL",{});var J=r(b);F=o(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),u=d(J),B=o(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),K=d(h),E=o(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=o(O,"CODE",{});var be=r(S);X=i(be,"tf.keras.Model.fit"),be.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=o(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),V=d(h),x=o(h,"UL",{});var C=r(x);z=o(C,"LI",{});var Q=r(z);me=i(Q,"a single Tensor with "),W=o(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),ue=i(Q," only and nothing else: "),R=o(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=o(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o(Z,"CODE",{});var De=r(L);ne=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=o(C,"LI",{});var oe=r(q);te=i(oe,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o(oe,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),oe.forEach(t),C.forEach(t)},m(h,M){m(h,p,M),e(p,D),m(h,g,M),m(h,b,M),e(b,F),e(F,_),e(b,u),e(b,B),e(B,ce),m(h,K,M),m(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),m(h,N,M),m(h,P,M),e(P,Y),m(h,V,M),m(h,x,M),e(x,z),e(z,me),e(z,W),e(W,se),e(z,ue),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,ne),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(b),h&&t(K),h&&t(E),h&&t(N),h&&t(P),h&&t(V),h&&t(x)}}}function I2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function W2(j){let p,D,g,b,F,_,u,B,ce,K,E,G,S,X,pe,I,he,ae,N,P,Y,V,x,z,me,W,se,ue,R,ie,ee,A,le,L,ne,fe,q,te,U,de;return{c(){p=n("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=n("ul"),F=n("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),u=l(),B=n("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),E=n("p"),G=a("This second option is useful when using "),S=n("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=n("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=n("ul"),z=n("li"),me=a("a single Tensor with "),W=n("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=n("code"),ie=a("model(inputs_ids)"),ee=l(),A=n("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n("code"),ne=a("model([input_ids, attention_mask])"),fe=l(),q=n("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n("code"),de=a('model({"input_ids": input_ids})')},l(h){p=o(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),b=o(h,"UL",{});var J=r(b);F=o(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),u=d(J),B=o(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),K=d(h),E=o(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=o(O,"CODE",{});var be=r(S);X=i(be,"tf.keras.Model.fit"),be.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=o(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),V=d(h),x=o(h,"UL",{});var C=r(x);z=o(C,"LI",{});var Q=r(z);me=i(Q,"a single Tensor with "),W=o(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),ue=i(Q," only and nothing else: "),R=o(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=o(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o(Z,"CODE",{});var De=r(L);ne=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=o(C,"LI",{});var oe=r(q);te=i(oe,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o(oe,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),oe.forEach(t),C.forEach(t)},m(h,M){m(h,p,M),e(p,D),m(h,g,M),m(h,b,M),e(b,F),e(F,_),e(b,u),e(b,B),e(B,ce),m(h,K,M),m(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),m(h,N,M),m(h,P,M),e(P,Y),m(h,V,M),m(h,x,M),e(x,z),e(z,me),e(z,W),e(W,se),e(z,ue),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,ne),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(b),h&&t(K),h&&t(E),h&&t(N),h&&t(P),h&&t(V),h&&t(x)}}}function R2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function U2(j){let p,D,g,b,F,_,u,B,ce,K,E,G,S,X,pe,I,he,ae,N,P,Y,V,x,z,me,W,se,ue,R,ie,ee,A,le,L,ne,fe,q,te,U,de;return{c(){p=n("p"),D=a("TF 2.0 models accepts two formats as inputs:"),g=l(),b=n("ul"),F=n("li"),_=a("having all inputs as keyword arguments (like PyTorch models), or"),u=l(),B=n("li"),ce=a("having all inputs as a list, tuple or dict in the first positional arguments."),K=l(),E=n("p"),G=a("This second option is useful when using "),S=n("code"),X=a("tf.keras.Model.fit"),pe=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),I=n("code"),he=a("model(inputs)"),ae=a("."),N=l(),P=n("p"),Y=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),V=l(),x=n("ul"),z=n("li"),me=a("a single Tensor with "),W=n("code"),se=a("input_ids"),ue=a(" only and nothing else: "),R=n("code"),ie=a("model(inputs_ids)"),ee=l(),A=n("li"),le=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=n("code"),ne=a("model([input_ids, attention_mask])"),fe=l(),q=n("li"),te=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=n("code"),de=a('model({"input_ids": input_ids})')},l(h){p=o(h,"P",{});var M=r(p);D=i(M,"TF 2.0 models accepts two formats as inputs:"),M.forEach(t),g=d(h),b=o(h,"UL",{});var J=r(b);F=o(J,"LI",{});var _e=r(F);_=i(_e,"having all inputs as keyword arguments (like PyTorch models), or"),_e.forEach(t),u=d(J),B=o(J,"LI",{});var Te=r(B);ce=i(Te,"having all inputs as a list, tuple or dict in the first positional arguments."),Te.forEach(t),J.forEach(t),K=d(h),E=o(h,"P",{});var O=r(E);G=i(O,"This second option is useful when using "),S=o(O,"CODE",{});var be=r(S);X=i(be,"tf.keras.Model.fit"),be.forEach(t),pe=i(O,` method which currently requires having all the
tensors in the first argument of the model call function: `),I=o(O,"CODE",{});var we=r(I);he=i(we,"model(inputs)"),we.forEach(t),ae=i(O,"."),O.forEach(t),N=d(h),P=o(h,"P",{});var ye=r(P);Y=i(ye,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ye.forEach(t),V=d(h),x=o(h,"UL",{});var C=r(x);z=o(C,"LI",{});var Q=r(z);me=i(Q,"a single Tensor with "),W=o(Q,"CODE",{});var $e=r(W);se=i($e,"input_ids"),$e.forEach(t),ue=i(Q," only and nothing else: "),R=o(Q,"CODE",{});var Fe=r(R);ie=i(Fe,"model(inputs_ids)"),Fe.forEach(t),Q.forEach(t),ee=d(C),A=o(C,"LI",{});var Z=r(A);le=i(Z,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),L=o(Z,"CODE",{});var De=r(L);ne=i(De,"model([input_ids, attention_mask])"),De.forEach(t),Z.forEach(t),fe=d(C),q=o(C,"LI",{});var oe=r(q);te=i(oe,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),U=o(oe,"CODE",{});var Be=r(U);de=i(Be,'model({"input_ids": input_ids})'),Be.forEach(t),oe.forEach(t),C.forEach(t)},m(h,M){m(h,p,M),e(p,D),m(h,g,M),m(h,b,M),e(b,F),e(F,_),e(b,u),e(b,B),e(B,ce),m(h,K,M),m(h,E,M),e(E,G),e(E,S),e(S,X),e(E,pe),e(E,I),e(I,he),e(E,ae),m(h,N,M),m(h,P,M),e(P,Y),m(h,V,M),m(h,x,M),e(x,z),e(z,me),e(z,W),e(W,se),e(z,ue),e(z,R),e(R,ie),e(x,ee),e(x,A),e(A,le),e(A,L),e(L,ne),e(x,fe),e(x,q),e(q,te),e(q,U),e(U,de)},d(h){h&&t(p),h&&t(g),h&&t(b),h&&t(K),h&&t(E),h&&t(N),h&&t(P),h&&t(V),h&&t(x)}}}function Q2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function H2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function K2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function V2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function J2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function G2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function X2(j){let p,D,g,b,F;return{c(){p=n("p"),D=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),b=a("Module"),F=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(_){p=o(_,"P",{});var u=r(p);D=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=o(u,"CODE",{});var B=r(g);b=i(B,"Module"),B.forEach(t),F=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(t)},m(_,u){m(_,p,u),e(p,D),e(p,g),e(g,b),e(p,F)},d(_){_&&t(p)}}}function Y2(j){let p,D,g,b,F,_,u,B,ce,K,E,G,S,X,pe,I,he,ae,N,P,Y,V,x,z,me,W,se,ue,R,ie,ee,A,le,L,ne,fe,q,te,U,de,h,M,J,_e,Te,O,be,we,ye,C,Q,$e,Fe,Z,De,oe,Be,Tm,wp,vt,wm,Yn,ym,$m,Zn,Fm,Dm,eo,Bm,Mm,yp,as,Js,Dl,to,Em,Bl,xm,$p,Ve,so,zm,Ct,jm,bi,Cm,qm,vi,Pm,Am,no,Om,Nm,Lm,is,Sm,ki,Im,Wm,Ti,Rm,Um,Qm,Ml,Hm,Km,oo,Fp,ls,Gs,El,ro,Vm,xl,Jm,Dp,_t,ao,Gm,zl,Xm,Ym,Xs,wi,Zm,eu,yi,tu,su,nu,io,ou,$i,ru,au,Bp,ds,Ys,jl,lo,iu,Cl,lu,Mp,bt,co,du,po,cu,ql,pu,hu,mu,Zs,Fi,uu,fu,Di,gu,_u,bu,ho,vu,Bi,ku,Tu,Ep,cs,en,Pl,mo,wu,Al,yu,xp,Je,uo,$u,Ol,Fu,Du,fo,Bu,Mi,Mu,Eu,xu,go,zu,_o,ju,Cu,qu,tt,bo,Pu,ps,Au,Ei,Ou,Nu,Nl,Lu,Su,Iu,tn,Wu,Ll,Ru,Uu,vo,zp,hs,sn,Sl,ko,Qu,Il,Hu,jp,Ge,To,Ku,wo,Vu,Wl,Ju,Gu,Xu,yo,Yu,xi,Zu,ef,tf,$o,sf,Fo,nf,of,rf,Ie,Do,af,ms,lf,zi,df,cf,Rl,pf,hf,mf,nn,uf,Ul,ff,gf,Bo,_f,Mo,Cp,us,on,Ql,Eo,bf,Hl,vf,qp,Xe,xo,kf,Kl,Tf,wf,zo,yf,ji,$f,Ff,Df,jo,Bf,Co,Mf,Ef,xf,ke,qo,zf,fs,jf,Ci,Cf,qf,Vl,Pf,Af,Of,rn,Nf,Jl,Lf,Sf,Po,If,Ao,Wf,Gl,Rf,Uf,Oo,Qf,No,Pp,gs,an,Xl,Lo,Hf,Yl,Kf,Ap,Ye,So,Vf,Zl,Jf,Gf,Io,Xf,qi,Yf,Zf,eg,Wo,tg,Ro,sg,ng,og,st,Uo,rg,_s,ag,Pi,ig,lg,ed,dg,cg,pg,ln,hg,td,mg,ug,Qo,Op,bs,dn,sd,Ho,fg,nd,gg,Np,Ze,Ko,_g,od,bg,vg,Vo,kg,Ai,Tg,wg,yg,Jo,$g,Go,Fg,Dg,Bg,We,Xo,Mg,vs,Eg,Oi,xg,zg,rd,jg,Cg,qg,cn,Pg,ad,Ag,Og,Yo,Ng,Zo,Lp,ks,pn,id,er,Lg,ld,Sg,Sp,et,tr,Ig,Ts,Wg,dd,Rg,Ug,cd,Qg,Hg,Kg,sr,Vg,Ni,Jg,Gg,Xg,nr,Yg,or,Zg,e_,t_,Re,rr,s_,ws,n_,Li,o_,r_,pd,a_,i_,l_,hn,d_,hd,c_,p_,ar,h_,ir,Ip,ys,mn,md,lr,m_,ud,u_,Wp,Pe,dr,f_,fd,g_,__,cr,b_,Si,v_,k_,T_,pr,w_,hr,y_,$_,F_,un,D_,nt,mr,B_,$s,M_,Ii,E_,x_,gd,z_,j_,C_,fn,q_,_d,P_,A_,ur,Rp,Fs,gn,bd,fr,O_,vd,N_,Up,Ae,gr,L_,_r,S_,kd,I_,W_,R_,br,U_,Wi,Q_,H_,K_,vr,V_,kr,J_,G_,X_,_n,Y_,Ue,Tr,Z_,Ds,eb,Ri,tb,sb,Td,nb,ob,rb,bn,ab,wd,ib,lb,wr,db,yr,Qp,Bs,vn,yd,$r,cb,$d,pb,Hp,Oe,Fr,hb,Fd,mb,ub,Dr,fb,Ui,gb,_b,bb,Br,vb,Mr,kb,Tb,wb,kn,yb,Qe,Er,$b,Ms,Fb,Qi,Db,Bb,Dd,Mb,Eb,xb,Tn,zb,Bd,jb,Cb,xr,qb,zr,Kp,Es,wn,Md,jr,Pb,Ed,Ab,Vp,Ne,Cr,Ob,xd,Nb,Lb,qr,Sb,Hi,Ib,Wb,Rb,Pr,Ub,Ar,Qb,Hb,Kb,yn,Vb,ot,Or,Jb,xs,Gb,Ki,Xb,Yb,zd,Zb,ev,tv,$n,sv,jd,nv,ov,Nr,Jp,zs,Fn,Cd,Lr,rv,qd,av,Gp,Le,Sr,iv,Pd,lv,dv,Ir,cv,Vi,pv,hv,mv,Wr,uv,Rr,fv,gv,_v,Dn,bv,He,Ur,vv,js,kv,Ji,Tv,wv,Ad,yv,$v,Fv,Bn,Dv,Od,Bv,Mv,Qr,Ev,Hr,Xp,Cs,Mn,Nd,Kr,xv,Ld,zv,Yp,Se,Vr,jv,qs,Cv,Sd,qv,Pv,Id,Av,Ov,Nv,Jr,Lv,Gi,Sv,Iv,Wv,Gr,Rv,Xr,Uv,Qv,Hv,En,Kv,Ke,Yr,Vv,Ps,Jv,Xi,Gv,Xv,Wd,Yv,Zv,ek,xn,tk,Rd,sk,nk,Zr,ok,ea,Zp,As,zn,Ud,ta,rk,Qd,ak,eh,Me,sa,ik,Hd,lk,dk,na,ck,Yi,pk,hk,mk,oa,uk,ra,fk,gk,_k,Kd,bk,vk,qt,Vd,aa,kk,Tk,Jd,ia,wk,yk,Gd,la,$k,Fk,Xd,da,Dk,Bk,rt,ca,Mk,Os,Ek,Yd,xk,zk,Zd,jk,Ck,qk,jn,Pk,ec,Ak,Ok,pa,th,Ns,Cn,tc,ha,Nk,sc,Lk,sh,Ee,ma,Sk,ua,Ik,nc,Wk,Rk,Uk,fa,Qk,Zi,Hk,Kk,Vk,ga,Jk,_a,Gk,Xk,Yk,oc,Zk,eT,Pt,rc,ba,tT,sT,ac,va,nT,oT,ic,ka,rT,aT,lc,Ta,iT,lT,at,wa,dT,Ls,cT,dc,pT,hT,cc,mT,uT,fT,qn,gT,pc,_T,bT,ya,nh,Ss,Pn,hc,$a,vT,mc,kT,oh,xe,Fa,TT,uc,wT,yT,Da,$T,el,FT,DT,BT,Ba,MT,Ma,ET,xT,zT,fc,jT,CT,At,gc,Ea,qT,PT,_c,xa,AT,OT,bc,za,NT,LT,vc,ja,ST,IT,it,Ca,WT,Is,RT,kc,UT,QT,Tc,HT,KT,VT,An,JT,wc,GT,XT,qa,rh,Ws,On,yc,Pa,YT,$c,ZT,ah,ze,Aa,ew,Fc,tw,sw,Oa,nw,tl,ow,rw,aw,Na,iw,La,lw,dw,cw,Dc,pw,hw,Ot,Bc,Sa,mw,uw,Mc,Ia,fw,gw,Ec,Wa,_w,bw,xc,Ra,vw,kw,lt,Ua,Tw,Rs,ww,zc,yw,$w,jc,Fw,Dw,Bw,Nn,Mw,Cc,Ew,xw,Qa,ih,Us,Ln,qc,Ha,zw,Pc,jw,lh,je,Ka,Cw,Ac,qw,Pw,Va,Aw,sl,Ow,Nw,Lw,Ja,Sw,Ga,Iw,Ww,Rw,Oc,Uw,Qw,Nt,Nc,Xa,Hw,Kw,Lc,Ya,Vw,Jw,Sc,Za,Gw,Xw,Ic,ei,Yw,Zw,dt,ti,ey,Qs,ty,Wc,sy,ny,Rc,oy,ry,ay,Sn,iy,Uc,ly,dy,si,dh,Hs,In,Qc,ni,cy,Hc,py,ch,Ce,oi,hy,Ks,my,Kc,uy,fy,Vc,gy,_y,by,ri,vy,nl,ky,Ty,wy,ai,yy,ii,$y,Fy,Dy,Jc,By,My,Lt,Gc,li,Ey,xy,Xc,di,zy,jy,Yc,ci,Cy,qy,Zc,pi,Py,Ay,ct,hi,Oy,Vs,Ny,ep,Ly,Sy,tp,Iy,Wy,Ry,Wn,Uy,sp,Qy,Hy,mi,ph;return _=new ve({}),X=new ve({}),to=new ve({}),so=new H({props:{name:"class transformers.DistilBertConfig",anchor:"transformers.DistilBertConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"max_position_embeddings",val:" = 512"},{name:"sinusoidal_pos_embds",val:" = False"},{name:"n_layers",val:" = 6"},{name:"n_heads",val:" = 12"},{name:"dim",val:" = 768"},{name:"hidden_dim",val:" = 3072"},{name:"dropout",val:" = 0.1"},{name:"attention_dropout",val:" = 0.1"},{name:"activation",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"qa_dropout",val:" = 0.1"},{name:"seq_classif_dropout",val:" = 0.2"},{name:"pad_token_id",val:" = 0"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/configuration_distilbert.py#L37",parametersDescription:[{anchor:"transformers.DistilBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DistilBERT model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> or <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DistilBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DistilBertConfig.sinusoidal_pos_embds",description:`<strong>sinusoidal_pos_embds</strong> (<code>boolean</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use sinusoidal positional embeddings.`,name:"sinusoidal_pos_embds"},{anchor:"transformers.DistilBertConfig.n_layers",description:`<strong>n_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"n_layers"},{anchor:"transformers.DistilBertConfig.n_heads",description:`<strong>n_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"n_heads"},{anchor:"transformers.DistilBertConfig.dim",description:`<strong>dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"dim"},{anchor:"transformers.DistilBertConfig.hidden_dim",description:`<strong>hidden_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
The size of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"hidden_dim"},{anchor:"transformers.DistilBertConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.DistilBertConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.DistilBertConfig.activation",description:`<strong>activation</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation"},{anchor:"transformers.DistilBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DistilBertConfig.qa_dropout",description:`<strong>qa_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probabilities used in the question answering model <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a>.`,name:"qa_dropout"},{anchor:"transformers.DistilBertConfig.seq_classif_dropout",description:`<strong>seq_classif_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The dropout probabilities used in the sequence classification and the multiple choice model
<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a>.`,name:"seq_classif_dropout"}]}}),oo=new re({props:{code:`from transformers import DistilBertModel, DistilBertConfig

# Initializing a DistilBERT configuration
configuration = DistilBertConfig()

# Initializing a model from the configuration
model = DistilBertModel(configuration)

# Accessing the model configuration
configuration = model.config`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertModel, DistilBertConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DistilBERT configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DistilBertConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`}}),ro=new ve({}),ao=new H({props:{name:"class transformers.DistilBertTokenizer",anchor:"transformers.DistilBertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/tokenization_distilbert.py#L56"}}),lo=new ve({}),co=new H({props:{name:"class transformers.DistilBertTokenizerFast",anchor:"transformers.DistilBertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/tokenization_distilbert_fast.py#L65"}}),mo=new ve({}),uo=new H({props:{name:"class transformers.DistilBertModel",anchor:"transformers.DistilBertModel",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L457",parametersDescription:[{anchor:"transformers.DistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),bo=new H({props:{name:"forward",anchor:"transformers.DistilBertModel.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L529",parametersDescription:[{anchor:"transformers.DistilBertModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),tn=new ge({props:{$$slots:{default:[M2]},$$scope:{ctx:j}}}),vo=new re({props:{code:`from transformers import DistilBertTokenizer, DistilBertModel
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),ko=new ve({}),To=new H({props:{name:"class transformers.DistilBertForMaskedLM",anchor:"transformers.DistilBertForMaskedLM",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L585",parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Do=new H({props:{name:"forward",anchor:"transformers.DistilBertForMaskedLM.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L627",parametersDescription:[{anchor:"transformers.DistilBertForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),nn=new ge({props:{$$slots:{default:[E2]},$$scope:{ctx:j}}}),Bo=new re({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMaskedLM
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

# retrieve index of [MASK]
mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]

predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)
tokenizer.decode(predicted_token_id)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of [MASK]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>].nonzero(as_tuple=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = logits[<span class="hljs-number">0</span>, mask_token_index].argmax(axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
`}}),Mo=new re({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="pt")["input_ids"]
# mask labels of non-[MASK] tokens
labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(outputs.loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-[MASK] tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(outputs.loss.item(), <span class="hljs-number">2</span>)
`}}),Eo=new ve({}),xo=new H({props:{name:"class transformers.DistilBertForSequenceClassification",anchor:"transformers.DistilBertForSequenceClassification",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L691",parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),qo=new H({props:{name:"forward",anchor:"transformers.DistilBertForSequenceClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L725",parametersDescription:[{anchor:"transformers.DistilBertForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),rn=new ge({props:{$$slots:{default:[x2]},$$scope:{ctx:j}}}),Po=new re({props:{code:`import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),Ao=new re({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=num_labels)

labels = torch.tensor(1)
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),Oo=new re({props:{code:`import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", problem_type="multi_label_classification")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_class_id = logits.argmax().item()
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),No=new re({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = DistilBertForSequenceClassification.from_pretrained(
    "distilbert-base-uncased", num_labels=num_labels, problem_type="multi_label_classification"
)

labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
    torch.float
)
loss = model(**inputs, labels=labels).loss
loss.backward()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels, problem_type=<span class="hljs-string">&quot;multi_label_classification&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.nn.functional.one_hot(torch.tensor([predicted_class_id]), num_classes=num_labels).to(
<span class="hljs-meta">... </span>    torch.<span class="hljs-built_in">float</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span>loss.backward()`}}),Lo=new ve({}),So=new H({props:{name:"class transformers.DistilBertForMultipleChoice",anchor:"transformers.DistilBertForMultipleChoice",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L1021",parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Uo=new H({props:{name:"forward",anchor:"transformers.DistilBertForMultipleChoice.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L1053",parametersDescription:[{anchor:"transformers.DistilBertForMultipleChoice.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForMultipleChoice.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForMultipleChoice.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForMultipleChoice.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForMultipleChoice.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForMultipleChoice.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices-1]</code> where <code>num_choices</code> is the size of the second dimension of the input tensors. (See
<code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <em>(1,)</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.MultipleChoiceModelOutput"
>transformers.modeling_outputs.MultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ln=new ge({props:{$$slots:{default:[z2]},$$scope:{ctx:j}}}),Qo=new re({props:{code:`from transformers import DistilBertTokenizer, DistilBertForMultipleChoice
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-cased")
model = DistilBertForMultipleChoice.from_pretrained("distilbert-base-cased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."
labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1

encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors="pt", padding=True)
outputs = model(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1

# the linear classifier still needs to be trained
loss = outputs.loss
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = torch.tensor(<span class="hljs-number">0</span>).unsqueeze(<span class="hljs-number">0</span>)  <span class="hljs-comment"># choice0 is correct (according to Wikipedia ;)), batch size 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([[prompt, choice0], [prompt, choice1]], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v.unsqueeze(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}, labels=labels)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ho=new ve({}),Ko=new H({props:{name:"class transformers.DistilBertForTokenClassification",anchor:"transformers.DistilBertForTokenClassification",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L926",parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Xo=new H({props:{name:"forward",anchor:"transformers.DistilBertForTokenClassification.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"labels",val:": typing.Optional[torch.LongTensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L958",parametersDescription:[{anchor:"transformers.DistilBertForTokenClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForTokenClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForTokenClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForTokenClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForTokenClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForTokenClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.TokenClassifierOutput"
>transformers.modeling_outputs.TokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),cn=new ge({props:{$$slots:{default:[j2]},$$scope:{ctx:j}}}),Yo=new re({props:{code:`from transformers import DistilBertTokenizer, DistilBertForTokenClassification
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="pt"
)

with torch.no_grad():
    logits = model(**inputs).logits

predicted_token_class_ids = logits.argmax(-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]
predicted_tokens_classes
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = logits.argmax(-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t.item()] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>]]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
`}}),Zo=new re({props:{code:`labels = predicted_token_class_ids
loss = model(**inputs, labels=labels).loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),er=new ve({}),tr=new H({props:{name:"class transformers.DistilBertForQuestionAnswering",anchor:"transformers.DistilBertForQuestionAnswering",parameters:[{name:"config",val:": PretrainedConfig"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L809",parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),rr=new H({props:{name:"forward",anchor:"transformers.DistilBertForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"head_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"start_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"end_positions",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_distilbert.py#L841",parametersDescription:[{anchor:"transformers.DistilBertForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, num_choices)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_choices, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.DistilBertForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.QuestionAnsweringModelOutput"
>transformers.modeling_outputs.QuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),hn=new ge({props:{$$slots:{default:[C2]},$$scope:{ctx:j}}}),ar=new re({props:{code:`from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering
import torch

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = DistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)

answer_start_index = outputs.start_logits.argmax()
answer_end_index = outputs.end_logits.argmax()

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, DistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = outputs.start_logits.argmax()
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = outputs.end_logits.argmax()

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
`}}),ir=new re({props:{code:`# target is "nice puppet"
target_start_index, target_end_index = torch.tensor([14]), torch.tensor([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = outputs.loss
round(loss.item(), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index, target_end_index = torch.tensor([<span class="hljs-number">14</span>]), torch.tensor([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
`}}),lr=new ve({}),dr=new H({props:{name:"class transformers.TFDistilBertModel",anchor:"transformers.TFDistilBertModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L522",parametersDescription:[{anchor:"transformers.TFDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),un=new ge({props:{$$slots:{default:[q2]},$$scope:{ctx:j}}}),mr=new H({props:{name:"call",anchor:"transformers.TFDistilBertModel.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L527",parametersDescription:[{anchor:"transformers.TFDistilBertModel.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertModel.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertModel.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutput"
>transformers.modeling_tf_outputs.TFBaseModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),fn=new ge({props:{$$slots:{default:[P2]},$$scope:{ctx:j}}}),ur=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertModel
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")
outputs = model(inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),fr=new ve({}),gr=new H({props:{name:"class transformers.TFDistilBertForMaskedLM",anchor:"transformers.TFDistilBertForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L609",parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),_n=new ge({props:{$$slots:{default:[A2]},$$scope:{ctx:j}}}),Tr=new H({props:{name:"call",anchor:"transformers.TFDistilBertForMaskedLM.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L629",parametersDescription:[{anchor:"transformers.TFDistilBertForMaskedLM.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMaskedLM.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMaskedLM.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMaskedLM.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMaskedLM.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMaskedLM.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMaskedLM.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of non-masked labels, returned when <code>labels</code> is provided) \u2014 Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMaskedLMOutput"
>transformers.modeling_tf_outputs.TFMaskedLMOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),bn=new ge({props:{$$slots:{default:[O2]},$$scope:{ctx:j}}}),wr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMaskedLM
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="tf")
logits = model(**inputs).logits

# retrieve index of [MASK]
mask_token_index = tf.where(inputs.input_ids == tokenizer.mask_token_id)[0][1]

predicted_token_id = tf.math.argmax(logits[0, mask_token_index], axis=-1)
tokenizer.decode(predicted_token_id)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve index of [MASK]</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>mask_token_index = tf.where(inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_id = tf.math.argmax(logits[<span class="hljs-number">0</span>, mask_token_index], axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predicted_token_id)
`}}),yr=new re({props:{code:`labels = tokenizer("The capital of France is Paris.", return_tensors="tf")["input_ids"]
# mask labels of non-[MASK] tokens
labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)

outputs = model(**inputs, labels=labels)
round(float(outputs.loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer(<span class="hljs-string">&quot;The capital of France is Paris.&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># mask labels of non-[MASK] tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tf.where(inputs.input_ids == tokenizer.mask_token_id, labels, -<span class="hljs-number">100</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, labels=labels)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(outputs.loss), <span class="hljs-number">2</span>)
`}}),$r=new ve({}),Fr=new H({props:{name:"class transformers.TFDistilBertForSequenceClassification",anchor:"transformers.TFDistilBertForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L699",parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),kn=new ge({props:{$$slots:{default:[N2]},$$scope:{ctx:j}}}),Er=new H({props:{name:"call",anchor:"transformers.TFDistilBertForSequenceClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L716",parametersDescription:[{anchor:"transformers.TFDistilBertForSequenceClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForSequenceClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Tn=new ge({props:{$$slots:{default:[L2]},$$scope:{ctx:j}}}),xr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="tf")

logits = model(**inputs).logits

predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
model.config.id2label[predicted_class_id]
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
`}}),zr=new re({props:{code:`# To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`
num_labels = len(model.config.id2label)
model = TFDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=num_labels)

labels = tf.constant(1)
loss = model(**inputs, labels=labels).loss
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To train a model on \`num_labels\` classes, you can pass \`num_labels=num_labels\` to \`.from_pretrained(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(model.config.id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=num_labels)

<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tf.constant(<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, labels=labels).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),jr=new ve({}),Cr=new H({props:{name:"class transformers.TFDistilBertForMultipleChoice",anchor:"transformers.TFDistilBertForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L862",parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),yn=new ge({props:{$$slots:{default:[S2]},$$scope:{ctx:j}}}),Or=new H({props:{name:"call",anchor:"transformers.TFDistilBertForMultipleChoice.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L888",parametersDescription:[{anchor:"transformers.TFDistilBertForMultipleChoice.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForMultipleChoice.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the multiple choice classification loss. Indices should be in <code>[0, ..., num_choices]</code>
where <code>num_choices</code> is the size of the second dimension of the input tensors. (See <code>input_ids</code> above)`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <em>(batch_size, )</em>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput"
>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),$n=new ge({props:{$$slots:{default:[I2]},$$scope:{ctx:j}}}),Nr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForMultipleChoice
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="tf", padding=True)
inputs = {k: tf.expand_dims(v, 0) for k, v in encoding.items()}
outputs = model(inputs)  # batch size is 1

# the linear classifier still needs to be trained
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForMultipleChoice
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = {k: tf.expand_dims(v, <span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(inputs)  <span class="hljs-comment"># batch size is 1</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># the linear classifier still needs to be trained</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Lr=new ve({}),Sr=new H({props:{name:"class transformers.TFDistilBertForTokenClassification",anchor:"transformers.TFDistilBertForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L786",parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Dn=new ge({props:{$$slots:{default:[W2]},$$scope:{ctx:j}}}),Ur=new H({props:{name:"call",anchor:"transformers.TFDistilBertForTokenClassification.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"labels",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L797",parametersDescription:[{anchor:"transformers.TFDistilBertForTokenClassification.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForTokenClassification.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForTokenClassification.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForTokenClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForTokenClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForTokenClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForTokenClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the token classification loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>.`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(n,)</code>, <em>optional</em>, where n is the number of unmasked labels, returned when <code>labels</code> is provided)  \u2014 Classification loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFTokenClassifierOutput"
>transformers.modeling_tf_outputs.TFTokenClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),Bn=new ge({props:{$$slots:{default:[R2]},$$scope:{ctx:j}}}),Qr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForTokenClassification
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer(
    "HuggingFace is a company based in Paris and New York", add_special_tokens=False, return_tensors="tf"
)

logits = model(**inputs).logits
predicted_token_class_ids = tf.math.argmax(logits, axis=-1)

# Note that tokens are classified rather then input words which means that
# there might be more predicted token classes than words.
# Multiple token classes might account for the same word
predicted_tokens_classes = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]
predicted_tokens_classes
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForTokenClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;HuggingFace is a company based in Paris and New York&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_token_class_ids = tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Note that tokens are classified rather then input words which means that</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># there might be more predicted token classes than words.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multiple token classes might account for the same word</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes = [model.config.id2label[t] <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> predicted_token_class_ids[<span class="hljs-number">0</span>].numpy().tolist()]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_tokens_classes
`}}),Hr=new re({props:{code:`labels = predicted_token_class_ids
loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = predicted_token_class_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.math.reduce_mean(model(**inputs, labels=labels).loss)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),Kr=new ve({}),Vr=new H({props:{name:"class transformers.TFDistilBertForQuestionAnswering",anchor:"transformers.TFDistilBertForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L987",parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),En=new ge({props:{$$slots:{default:[U2]},$$scope:{ctx:j}}}),Yr=new H({props:{name:"call",anchor:"transformers.TFDistilBertForQuestionAnswering.call",parameters:[{name:"input_ids",val:": typing.Union[typing.List[tensorflow.python.framework.ops.Tensor], typing.List[numpy.ndarray], typing.List[tensorflow.python.keras.engine.keras_tensor.KerasTensor], typing.Dict[str, tensorflow.python.framework.ops.Tensor], typing.Dict[str, numpy.ndarray], typing.Dict[str, tensorflow.python.keras.engine.keras_tensor.KerasTensor], tensorflow.python.framework.ops.Tensor, numpy.ndarray, tensorflow.python.keras.engine.keras_tensor.KerasTensor, NoneType] = None"},{name:"attention_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"head_mask",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"inputs_embeds",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"},{name:"start_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"end_positions",val:": typing.Union[numpy.ndarray, tensorflow.python.framework.ops.Tensor, NoneType] = None"},{name:"training",val:": typing.Optional[bool] = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_tf_distilbert.py#L998",parametersDescription:[{anchor:"transformers.TFDistilBertForQuestionAnswering.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.head_mask",description:`<strong>head_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.start_positions",description:`<strong>start_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.TFDistilBertForQuestionAnswering.call.end_positions",description:`<strong>end_positions</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>start_positions</code> and <code>end_positions</code> are provided) \u2014 Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.</p>
</li>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput"
>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),xn=new ge({props:{$$slots:{default:[Q2]},$$scope:{ctx:j}}}),Zr=new re({props:{code:`from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering
import tensorflow as tf

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = TFDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"

inputs = tokenizer(question, text, return_tensors="tf")
outputs = model(**inputs)

answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])
answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])

predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]
tokenizer.decode(predict_answer_tokens)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, TFDistilBertForQuestionAnswering
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>answer_start_index = <span class="hljs-built_in">int</span>(tf.math.argmax(outputs.start_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>answer_end_index = <span class="hljs-built_in">int</span>(tf.math.argmax(outputs.end_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>predict_answer_tokens = inputs.input_ids[<span class="hljs-number">0</span>, answer_start_index : answer_end_index + <span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(predict_answer_tokens)
`}}),ea=new re({props:{code:`# target is "nice puppet"
target_start_index, target_end_index = tf.constant([14]), tf.constant([15])

outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
loss = tf.math.reduce_mean(outputs.loss)
round(float(loss), 2)
`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># target is &quot;nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_start_index, target_end_index = tf.constant([<span class="hljs-number">14</span>]), tf.constant([<span class="hljs-number">15</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs, start_positions=target_start_index, end_positions=target_end_index)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.math.reduce_mean(outputs.loss)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(<span class="hljs-built_in">float</span>(loss), <span class="hljs-number">2</span>)
`}}),ta=new ve({}),sa=new H({props:{name:"class transformers.FlaxDistilBertModel",anchor:"transformers.FlaxDistilBertModel",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L523",parametersDescription:[{anchor:"transformers.FlaxDistilBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ca=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}]}}),jn=new ge({props:{$$slots:{default:[H2]},$$scope:{ctx:j}}}),pa=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertModel.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertModel.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),ha=new ve({}),ma=new H({props:{name:"class transformers.FlaxDistilBertForMaskedLM",anchor:"transformers.FlaxDistilBertForMaskedLM",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L596",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),wa=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForMaskedLM.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForMaskedLM.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),qn=new ge({props:{$$slots:{default:[K2]},$$scope:{ctx:j}}}),ya=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMaskedLM

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMaskedLM.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("The capital of France is [MASK].", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;The capital of France is [MASK].&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),$a=new ve({}),Fa=new H({props:{name:"class transformers.FlaxDistilBertForSequenceClassification",anchor:"transformers.FlaxDistilBertForSequenceClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L665",parametersDescription:[{anchor:"transformers.FlaxDistilBertForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ca=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForSequenceClassification.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) \u2014 Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),An=new ge({props:{$$slots:{default:[V2]},$$scope:{ctx:j}}}),qa=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForSequenceClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForSequenceClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Pa=new ve({}),Aa=new H({props:{name:"class transformers.FlaxDistilBertForMultipleChoice",anchor:"transformers.FlaxDistilBertForMultipleChoice",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L745",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMultipleChoice.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ua=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_choices, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForMultipleChoice.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, num_choices)</code>) \u2014 <em>num_choices</em> is the second dimension of the input tensors. (see <em>input_ids</em> above).</p>
<p>Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput"
>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Nn=new ge({props:{$$slots:{default:[J2]},$$scope:{ctx:j}}}),Qa=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForMultipleChoice

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForMultipleChoice.from_pretrained("distilbert-base-uncased")

prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
choice0 = "It is eaten with a fork and a knife."
choice1 = "It is eaten while held in the hand."

encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors="jax", padding=True)
outputs = model(**{k: v[None, :] for k, v in encoding.items()})

logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice0 = <span class="hljs-string">&quot;It is eaten with a fork and a knife.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>choice1 = <span class="hljs-string">&quot;It is eaten while held in the hand.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=<span class="hljs-string">&quot;jax&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**{k: v[<span class="hljs-literal">None</span>, :] <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> encoding.items()})

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),Ha=new ve({}),Ka=new H({props:{name:"class transformers.FlaxDistilBertForTokenClassification",anchor:"transformers.FlaxDistilBertForTokenClassification",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L811",parametersDescription:[{anchor:"transformers.FlaxDistilBertForTokenClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),ti=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForTokenClassification.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForTokenClassification.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.num_labels)</code>) \u2014 Classification scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxTokenClassifierOutput"
>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Sn=new ge({props:{$$slots:{default:[G2]},$$scope:{ctx:j}}}),si=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForTokenClassification

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForTokenClassification.from_pretrained("distilbert-base-uncased")

inputs = tokenizer("Hello, my dog is cute", return_tensors="jax")

outputs = model(**inputs)
logits = outputs.logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForTokenClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`}}),ni=new ve({}),oi=new H({props:{name:"class transformers.FlaxDistilBertForQuestionAnswering",anchor:"transformers.FlaxDistilBertForQuestionAnswering",parameters:[{name:"config",val:": DistilBertConfig"},{name:"input_shape",val:": typing.Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax._src.numpy.lax_numpy.float32'>"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L881",parametersDescription:[{anchor:"transformers.FlaxDistilBertForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),hi=new H({props:{name:"__call__",anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"return_dict",val:": typing.Optional[bool] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/models/distilbert/modeling_flax_distilbert.py#L446",parametersDescription:[{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxDistilBertForQuestionAnswering.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig"
>DistilBertConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-start scores (before SoftMax).</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) \u2014 Span-end scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput"
>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Wn=new ge({props:{$$slots:{default:[X2]},$$scope:{ctx:j}}}),mi=new re({props:{code:`from transformers import DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
model = FlaxDistilBertForQuestionAnswering.from_pretrained("distilbert-base-uncased")

question, text = "Who was Jim Henson?", "Jim Henson was a nice puppet"
inputs = tokenizer(question, text, return_tensors="jax")

outputs = model(**inputs)
start_scores = outputs.start_logits
end_scores = outputs.end_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DistilBertTokenizer, FlaxDistilBertForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DistilBertTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxDistilBertForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>question, text = <span class="hljs-string">&quot;Who was Jim Henson?&quot;</span>, <span class="hljs-string">&quot;Jim Henson was a nice puppet&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(question, text, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_scores = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_scores = outputs.end_logits`}}),{c(){p=n("meta"),D=l(),g=n("h1"),b=n("a"),F=n("span"),v(_.$$.fragment),u=l(),B=n("span"),ce=a("DistilBERT"),K=l(),E=n("h2"),G=n("a"),S=n("span"),v(X.$$.fragment),pe=l(),I=n("span"),he=a("Overview"),ae=l(),N=n("p"),P=a("The DistilBERT model was proposed in the blog post "),Y=n("a"),V=a(`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),x=a(", and the paper "),z=n("a"),me=a(`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),W=a(`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),se=n("em"),ue=a("bert-base-uncased"),R=a(`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),ie=l(),ee=n("p"),A=a("The abstract from the paper is the following:"),le=l(),L=n("p"),ne=n("em"),fe=a(`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),q=l(),te=n("p"),U=a("Tips:"),de=l(),h=n("ul"),M=n("li"),J=a("DistilBERT doesn\u2019t have "),_e=n("code"),Te=a("token_type_ids"),O=a(`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),be=n("code"),we=a("tokenizer.sep_token"),ye=a(" (or "),C=n("code"),Q=a("[SEP]"),$e=a(")."),Fe=l(),Z=n("li"),De=a("DistilBERT doesn\u2019t have options to select the input positions ("),oe=n("code"),Be=a("position_ids"),Tm=a(` input). This could be added if
necessary though, just let us know if you need this option.`),wp=l(),vt=n("p"),wm=a("This model was contributed by "),Yn=n("a"),ym=a("victorsanh"),$m=a(`. This model jax version was
contributed by `),Zn=n("a"),Fm=a("kamalkraj"),Dm=a(". The original code can be found "),eo=n("a"),Bm=a("here"),Mm=a("."),yp=l(),as=n("h2"),Js=n("a"),Dl=n("span"),v(to.$$.fragment),Em=l(),Bl=n("span"),xm=a("DistilBertConfig"),$p=l(),Ve=n("div"),v(so.$$.fragment),zm=l(),Ct=n("p"),jm=a("This is the configuration class to store the configuration of a "),bi=n("a"),Cm=a("DistilBertModel"),qm=a(" or a "),vi=n("a"),Pm=a("TFDistilBertModel"),Am=a(`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),no=n("a"),Om=a("distilbert-base-uncased"),Nm=a(" architecture."),Lm=l(),is=n("p"),Sm=a("Configuration objects inherit from "),ki=n("a"),Im=a("PretrainedConfig"),Wm=a(` and can be used to control the model outputs. Read the
documentation from `),Ti=n("a"),Rm=a("PretrainedConfig"),Um=a(" for more information."),Qm=l(),Ml=n("p"),Hm=a("Examples:"),Km=l(),v(oo.$$.fragment),Fp=l(),ls=n("h2"),Gs=n("a"),El=n("span"),v(ro.$$.fragment),Vm=l(),xl=n("span"),Jm=a("DistilBertTokenizer"),Dp=l(),_t=n("div"),v(ao.$$.fragment),Gm=l(),zl=n("p"),Xm=a("Construct a DistilBERT tokenizer."),Ym=l(),Xs=n("p"),wi=n("a"),Zm=a("DistilBertTokenizer"),eu=a(" is identical to "),yi=n("a"),tu=a("BertTokenizer"),su=a(` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),nu=l(),io=n("p"),ou=a("Refer to superclass "),$i=n("a"),ru=a("BertTokenizer"),au=a(" for usage examples and documentation concerning parameters."),Bp=l(),ds=n("h2"),Ys=n("a"),jl=n("span"),v(lo.$$.fragment),iu=l(),Cl=n("span"),lu=a("DistilBertTokenizerFast"),Mp=l(),bt=n("div"),v(co.$$.fragment),du=l(),po=n("p"),cu=a("Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),ql=n("em"),pu=a("tokenizers"),hu=a(" library)."),mu=l(),Zs=n("p"),Fi=n("a"),uu=a("DistilBertTokenizerFast"),fu=a(" is identical to "),Di=n("a"),gu=a("BertTokenizerFast"),_u=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),bu=l(),ho=n("p"),vu=a("Refer to superclass "),Bi=n("a"),ku=a("BertTokenizerFast"),Tu=a(" for usage examples and documentation concerning parameters."),Ep=l(),cs=n("h2"),en=n("a"),Pl=n("span"),v(mo.$$.fragment),wu=l(),Al=n("span"),yu=a("DistilBertModel"),xp=l(),Je=n("div"),v(uo.$$.fragment),$u=l(),Ol=n("p"),Fu=a("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),Du=l(),fo=n("p"),Bu=a("This model inherits from "),Mi=n("a"),Mu=a("PreTrainedModel"),Eu=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xu=l(),go=n("p"),zu=a("This model is also a PyTorch "),_o=n("a"),ju=a("torch.nn.Module"),Cu=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),qu=l(),tt=n("div"),v(bo.$$.fragment),Pu=l(),ps=n("p"),Au=a("The "),Ei=n("a"),Ou=a("DistilBertModel"),Nu=a(" forward method, overrides the "),Nl=n("code"),Lu=a("__call__"),Su=a(" special method."),Iu=l(),v(tn.$$.fragment),Wu=l(),Ll=n("p"),Ru=a("Example:"),Uu=l(),v(vo.$$.fragment),zp=l(),hs=n("h2"),sn=n("a"),Sl=n("span"),v(ko.$$.fragment),Qu=l(),Il=n("span"),Hu=a("DistilBertForMaskedLM"),jp=l(),Ge=n("div"),v(To.$$.fragment),Ku=l(),wo=n("p"),Vu=a("DistilBert Model with a "),Wl=n("code"),Ju=a("masked language modeling"),Gu=a(" head on top."),Xu=l(),yo=n("p"),Yu=a("This model inherits from "),xi=n("a"),Zu=a("PreTrainedModel"),ef=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),tf=l(),$o=n("p"),sf=a("This model is also a PyTorch "),Fo=n("a"),nf=a("torch.nn.Module"),of=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),rf=l(),Ie=n("div"),v(Do.$$.fragment),af=l(),ms=n("p"),lf=a("The "),zi=n("a"),df=a("DistilBertForMaskedLM"),cf=a(" forward method, overrides the "),Rl=n("code"),pf=a("__call__"),hf=a(" special method."),mf=l(),v(nn.$$.fragment),uf=l(),Ul=n("p"),ff=a("Example:"),gf=l(),v(Bo.$$.fragment),_f=l(),v(Mo.$$.fragment),Cp=l(),us=n("h2"),on=n("a"),Ql=n("span"),v(Eo.$$.fragment),bf=l(),Hl=n("span"),vf=a("DistilBertForSequenceClassification"),qp=l(),Xe=n("div"),v(xo.$$.fragment),kf=l(),Kl=n("p"),Tf=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),wf=l(),zo=n("p"),yf=a("This model inherits from "),ji=n("a"),$f=a("PreTrainedModel"),Ff=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Df=l(),jo=n("p"),Bf=a("This model is also a PyTorch "),Co=n("a"),Mf=a("torch.nn.Module"),Ef=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),xf=l(),ke=n("div"),v(qo.$$.fragment),zf=l(),fs=n("p"),jf=a("The "),Ci=n("a"),Cf=a("DistilBertForSequenceClassification"),qf=a(" forward method, overrides the "),Vl=n("code"),Pf=a("__call__"),Af=a(" special method."),Of=l(),v(rn.$$.fragment),Nf=l(),Jl=n("p"),Lf=a("Example of single-label classification:"),Sf=l(),v(Po.$$.fragment),If=l(),v(Ao.$$.fragment),Wf=l(),Gl=n("p"),Rf=a("Example of multi-label classification:"),Uf=l(),v(Oo.$$.fragment),Qf=l(),v(No.$$.fragment),Pp=l(),gs=n("h2"),an=n("a"),Xl=n("span"),v(Lo.$$.fragment),Hf=l(),Yl=n("span"),Kf=a("DistilBertForMultipleChoice"),Ap=l(),Ye=n("div"),v(So.$$.fragment),Vf=l(),Zl=n("p"),Jf=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Gf=l(),Io=n("p"),Xf=a("This model inherits from "),qi=n("a"),Yf=a("PreTrainedModel"),Zf=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),eg=l(),Wo=n("p"),tg=a("This model is also a PyTorch "),Ro=n("a"),sg=a("torch.nn.Module"),ng=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),og=l(),st=n("div"),v(Uo.$$.fragment),rg=l(),_s=n("p"),ag=a("The "),Pi=n("a"),ig=a("DistilBertForMultipleChoice"),lg=a(" forward method, overrides the "),ed=n("code"),dg=a("__call__"),cg=a(" special method."),pg=l(),v(ln.$$.fragment),hg=l(),td=n("p"),mg=a("Examples:"),ug=l(),v(Qo.$$.fragment),Op=l(),bs=n("h2"),dn=n("a"),sd=n("span"),v(Ho.$$.fragment),fg=l(),nd=n("span"),gg=a("DistilBertForTokenClassification"),Np=l(),Ze=n("div"),v(Ko.$$.fragment),_g=l(),od=n("p"),bg=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),vg=l(),Vo=n("p"),kg=a("This model inherits from "),Ai=n("a"),Tg=a("PreTrainedModel"),wg=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),yg=l(),Jo=n("p"),$g=a("This model is also a PyTorch "),Go=n("a"),Fg=a("torch.nn.Module"),Dg=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bg=l(),We=n("div"),v(Xo.$$.fragment),Mg=l(),vs=n("p"),Eg=a("The "),Oi=n("a"),xg=a("DistilBertForTokenClassification"),zg=a(" forward method, overrides the "),rd=n("code"),jg=a("__call__"),Cg=a(" special method."),qg=l(),v(cn.$$.fragment),Pg=l(),ad=n("p"),Ag=a("Example:"),Og=l(),v(Yo.$$.fragment),Ng=l(),v(Zo.$$.fragment),Lp=l(),ks=n("h2"),pn=n("a"),id=n("span"),v(er.$$.fragment),Lg=l(),ld=n("span"),Sg=a("DistilBertForQuestionAnswering"),Sp=l(),et=n("div"),v(tr.$$.fragment),Ig=l(),Ts=n("p"),Wg=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),dd=n("code"),Rg=a("span start logits"),Ug=a(" and "),cd=n("code"),Qg=a("span end logits"),Hg=a(")."),Kg=l(),sr=n("p"),Vg=a("This model inherits from "),Ni=n("a"),Jg=a("PreTrainedModel"),Gg=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xg=l(),nr=n("p"),Yg=a("This model is also a PyTorch "),or=n("a"),Zg=a("torch.nn.Module"),e_=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),t_=l(),Re=n("div"),v(rr.$$.fragment),s_=l(),ws=n("p"),n_=a("The "),Li=n("a"),o_=a("DistilBertForQuestionAnswering"),r_=a(" forward method, overrides the "),pd=n("code"),a_=a("__call__"),i_=a(" special method."),l_=l(),v(hn.$$.fragment),d_=l(),hd=n("p"),c_=a("Example:"),p_=l(),v(ar.$$.fragment),h_=l(),v(ir.$$.fragment),Ip=l(),ys=n("h2"),mn=n("a"),md=n("span"),v(lr.$$.fragment),m_=l(),ud=n("span"),u_=a("TFDistilBertModel"),Wp=l(),Pe=n("div"),v(dr.$$.fragment),f_=l(),fd=n("p"),g_=a("The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),__=l(),cr=n("p"),b_=a("This model inherits from "),Si=n("a"),v_=a("TFPreTrainedModel"),k_=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),T_=l(),pr=n("p"),w_=a("This model is also a "),hr=n("a"),y_=a("tf.keras.Model"),$_=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),F_=l(),v(un.$$.fragment),D_=l(),nt=n("div"),v(mr.$$.fragment),B_=l(),$s=n("p"),M_=a("The "),Ii=n("a"),E_=a("TFDistilBertModel"),x_=a(" forward method, overrides the "),gd=n("code"),z_=a("__call__"),j_=a(" special method."),C_=l(),v(fn.$$.fragment),q_=l(),_d=n("p"),P_=a("Example:"),A_=l(),v(ur.$$.fragment),Rp=l(),Fs=n("h2"),gn=n("a"),bd=n("span"),v(fr.$$.fragment),O_=l(),vd=n("span"),N_=a("TFDistilBertForMaskedLM"),Up=l(),Ae=n("div"),v(gr.$$.fragment),L_=l(),_r=n("p"),S_=a("DistilBert Model with a "),kd=n("code"),I_=a("masked language modeling"),W_=a(" head on top."),R_=l(),br=n("p"),U_=a("This model inherits from "),Wi=n("a"),Q_=a("TFPreTrainedModel"),H_=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),K_=l(),vr=n("p"),V_=a("This model is also a "),kr=n("a"),J_=a("tf.keras.Model"),G_=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),X_=l(),v(_n.$$.fragment),Y_=l(),Ue=n("div"),v(Tr.$$.fragment),Z_=l(),Ds=n("p"),eb=a("The "),Ri=n("a"),tb=a("TFDistilBertForMaskedLM"),sb=a(" forward method, overrides the "),Td=n("code"),nb=a("__call__"),ob=a(" special method."),rb=l(),v(bn.$$.fragment),ab=l(),wd=n("p"),ib=a("Example:"),lb=l(),v(wr.$$.fragment),db=l(),v(yr.$$.fragment),Qp=l(),Bs=n("h2"),vn=n("a"),yd=n("span"),v($r.$$.fragment),cb=l(),$d=n("span"),pb=a("TFDistilBertForSequenceClassification"),Hp=l(),Oe=n("div"),v(Fr.$$.fragment),hb=l(),Fd=n("p"),mb=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),ub=l(),Dr=n("p"),fb=a("This model inherits from "),Ui=n("a"),gb=a("TFPreTrainedModel"),_b=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),bb=l(),Br=n("p"),vb=a("This model is also a "),Mr=n("a"),kb=a("tf.keras.Model"),Tb=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),wb=l(),v(kn.$$.fragment),yb=l(),Qe=n("div"),v(Er.$$.fragment),$b=l(),Ms=n("p"),Fb=a("The "),Qi=n("a"),Db=a("TFDistilBertForSequenceClassification"),Bb=a(" forward method, overrides the "),Dd=n("code"),Mb=a("__call__"),Eb=a(" special method."),xb=l(),v(Tn.$$.fragment),zb=l(),Bd=n("p"),jb=a("Example:"),Cb=l(),v(xr.$$.fragment),qb=l(),v(zr.$$.fragment),Kp=l(),Es=n("h2"),wn=n("a"),Md=n("span"),v(jr.$$.fragment),Pb=l(),Ed=n("span"),Ab=a("TFDistilBertForMultipleChoice"),Vp=l(),Ne=n("div"),v(Cr.$$.fragment),Ob=l(),xd=n("p"),Nb=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),Lb=l(),qr=n("p"),Sb=a("This model inherits from "),Hi=n("a"),Ib=a("TFPreTrainedModel"),Wb=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rb=l(),Pr=n("p"),Ub=a("This model is also a "),Ar=n("a"),Qb=a("tf.keras.Model"),Hb=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Kb=l(),v(yn.$$.fragment),Vb=l(),ot=n("div"),v(Or.$$.fragment),Jb=l(),xs=n("p"),Gb=a("The "),Ki=n("a"),Xb=a("TFDistilBertForMultipleChoice"),Yb=a(" forward method, overrides the "),zd=n("code"),Zb=a("__call__"),ev=a(" special method."),tv=l(),v($n.$$.fragment),sv=l(),jd=n("p"),nv=a("Example:"),ov=l(),v(Nr.$$.fragment),Jp=l(),zs=n("h2"),Fn=n("a"),Cd=n("span"),v(Lr.$$.fragment),rv=l(),qd=n("span"),av=a("TFDistilBertForTokenClassification"),Gp=l(),Le=n("div"),v(Sr.$$.fragment),iv=l(),Pd=n("p"),lv=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),dv=l(),Ir=n("p"),cv=a("This model inherits from "),Vi=n("a"),pv=a("TFPreTrainedModel"),hv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),mv=l(),Wr=n("p"),uv=a("This model is also a "),Rr=n("a"),fv=a("tf.keras.Model"),gv=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),_v=l(),v(Dn.$$.fragment),bv=l(),He=n("div"),v(Ur.$$.fragment),vv=l(),js=n("p"),kv=a("The "),Ji=n("a"),Tv=a("TFDistilBertForTokenClassification"),wv=a(" forward method, overrides the "),Ad=n("code"),yv=a("__call__"),$v=a(" special method."),Fv=l(),v(Bn.$$.fragment),Dv=l(),Od=n("p"),Bv=a("Example:"),Mv=l(),v(Qr.$$.fragment),Ev=l(),v(Hr.$$.fragment),Xp=l(),Cs=n("h2"),Mn=n("a"),Nd=n("span"),v(Kr.$$.fragment),xv=l(),Ld=n("span"),zv=a("TFDistilBertForQuestionAnswering"),Yp=l(),Se=n("div"),v(Vr.$$.fragment),jv=l(),qs=n("p"),Cv=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),Sd=n("code"),qv=a("span start logits"),Pv=a(" and "),Id=n("code"),Av=a("span end logits"),Ov=a(")."),Nv=l(),Jr=n("p"),Lv=a("This model inherits from "),Gi=n("a"),Sv=a("TFPreTrainedModel"),Iv=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wv=l(),Gr=n("p"),Rv=a("This model is also a "),Xr=n("a"),Uv=a("tf.keras.Model"),Qv=a(` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Hv=l(),v(En.$$.fragment),Kv=l(),Ke=n("div"),v(Yr.$$.fragment),Vv=l(),Ps=n("p"),Jv=a("The "),Xi=n("a"),Gv=a("TFDistilBertForQuestionAnswering"),Xv=a(" forward method, overrides the "),Wd=n("code"),Yv=a("__call__"),Zv=a(" special method."),ek=l(),v(xn.$$.fragment),tk=l(),Rd=n("p"),sk=a("Example:"),nk=l(),v(Zr.$$.fragment),ok=l(),v(ea.$$.fragment),Zp=l(),As=n("h2"),zn=n("a"),Ud=n("span"),v(ta.$$.fragment),rk=l(),Qd=n("span"),ak=a("FlaxDistilBertModel"),eh=l(),Me=n("div"),v(sa.$$.fragment),ik=l(),Hd=n("p"),lk=a("The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),dk=l(),na=n("p"),ck=a("This model inherits from "),Yi=n("a"),pk=a("FlaxPreTrainedModel"),hk=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),mk=l(),oa=n("p"),uk=a("This model is also a Flax Linen "),ra=n("a"),fk=a("flax.linen.Module"),gk=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),_k=l(),Kd=n("p"),bk=a("Finally, this model supports inherent JAX features such as:"),vk=l(),qt=n("ul"),Vd=n("li"),aa=n("a"),kk=a("Just-In-Time (JIT) compilation"),Tk=l(),Jd=n("li"),ia=n("a"),wk=a("Automatic Differentiation"),yk=l(),Gd=n("li"),la=n("a"),$k=a("Vectorization"),Fk=l(),Xd=n("li"),da=n("a"),Dk=a("Parallelization"),Bk=l(),rt=n("div"),v(ca.$$.fragment),Mk=l(),Os=n("p"),Ek=a("The "),Yd=n("code"),xk=a("FlaxDistilBertPreTrainedModel"),zk=a(" forward method, overrides the "),Zd=n("code"),jk=a("__call__"),Ck=a(" special method."),qk=l(),v(jn.$$.fragment),Pk=l(),ec=n("p"),Ak=a("Example:"),Ok=l(),v(pa.$$.fragment),th=l(),Ns=n("h2"),Cn=n("a"),tc=n("span"),v(ha.$$.fragment),Nk=l(),sc=n("span"),Lk=a("FlaxDistilBertForMaskedLM"),sh=l(),Ee=n("div"),v(ma.$$.fragment),Sk=l(),ua=n("p"),Ik=a("DistilBert Model with a "),nc=n("code"),Wk=a("language modeling"),Rk=a(" head on top."),Uk=l(),fa=n("p"),Qk=a("This model inherits from "),Zi=n("a"),Hk=a("FlaxPreTrainedModel"),Kk=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Vk=l(),ga=n("p"),Jk=a("This model is also a Flax Linen "),_a=n("a"),Gk=a("flax.linen.Module"),Xk=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Yk=l(),oc=n("p"),Zk=a("Finally, this model supports inherent JAX features such as:"),eT=l(),Pt=n("ul"),rc=n("li"),ba=n("a"),tT=a("Just-In-Time (JIT) compilation"),sT=l(),ac=n("li"),va=n("a"),nT=a("Automatic Differentiation"),oT=l(),ic=n("li"),ka=n("a"),rT=a("Vectorization"),aT=l(),lc=n("li"),Ta=n("a"),iT=a("Parallelization"),lT=l(),at=n("div"),v(wa.$$.fragment),dT=l(),Ls=n("p"),cT=a("The "),dc=n("code"),pT=a("FlaxDistilBertPreTrainedModel"),hT=a(" forward method, overrides the "),cc=n("code"),mT=a("__call__"),uT=a(" special method."),fT=l(),v(qn.$$.fragment),gT=l(),pc=n("p"),_T=a("Example:"),bT=l(),v(ya.$$.fragment),nh=l(),Ss=n("h2"),Pn=n("a"),hc=n("span"),v($a.$$.fragment),vT=l(),mc=n("span"),kT=a("FlaxDistilBertForSequenceClassification"),oh=l(),xe=n("div"),v(Fa.$$.fragment),TT=l(),uc=n("p"),wT=a(`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),yT=l(),Da=n("p"),$T=a("This model inherits from "),el=n("a"),FT=a("FlaxPreTrainedModel"),DT=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),BT=l(),Ba=n("p"),MT=a("This model is also a Flax Linen "),Ma=n("a"),ET=a("flax.linen.Module"),xT=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),zT=l(),fc=n("p"),jT=a("Finally, this model supports inherent JAX features such as:"),CT=l(),At=n("ul"),gc=n("li"),Ea=n("a"),qT=a("Just-In-Time (JIT) compilation"),PT=l(),_c=n("li"),xa=n("a"),AT=a("Automatic Differentiation"),OT=l(),bc=n("li"),za=n("a"),NT=a("Vectorization"),LT=l(),vc=n("li"),ja=n("a"),ST=a("Parallelization"),IT=l(),it=n("div"),v(Ca.$$.fragment),WT=l(),Is=n("p"),RT=a("The "),kc=n("code"),UT=a("FlaxDistilBertPreTrainedModel"),QT=a(" forward method, overrides the "),Tc=n("code"),HT=a("__call__"),KT=a(" special method."),VT=l(),v(An.$$.fragment),JT=l(),wc=n("p"),GT=a("Example:"),XT=l(),v(qa.$$.fragment),rh=l(),Ws=n("h2"),On=n("a"),yc=n("span"),v(Pa.$$.fragment),YT=l(),$c=n("span"),ZT=a("FlaxDistilBertForMultipleChoice"),ah=l(),ze=n("div"),v(Aa.$$.fragment),ew=l(),Fc=n("p"),tw=a(`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),sw=l(),Oa=n("p"),nw=a("This model inherits from "),tl=n("a"),ow=a("FlaxPreTrainedModel"),rw=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),aw=l(),Na=n("p"),iw=a("This model is also a Flax Linen "),La=n("a"),lw=a("flax.linen.Module"),dw=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),cw=l(),Dc=n("p"),pw=a("Finally, this model supports inherent JAX features such as:"),hw=l(),Ot=n("ul"),Bc=n("li"),Sa=n("a"),mw=a("Just-In-Time (JIT) compilation"),uw=l(),Mc=n("li"),Ia=n("a"),fw=a("Automatic Differentiation"),gw=l(),Ec=n("li"),Wa=n("a"),_w=a("Vectorization"),bw=l(),xc=n("li"),Ra=n("a"),vw=a("Parallelization"),kw=l(),lt=n("div"),v(Ua.$$.fragment),Tw=l(),Rs=n("p"),ww=a("The "),zc=n("code"),yw=a("FlaxDistilBertPreTrainedModel"),$w=a(" forward method, overrides the "),jc=n("code"),Fw=a("__call__"),Dw=a(" special method."),Bw=l(),v(Nn.$$.fragment),Mw=l(),Cc=n("p"),Ew=a("Example:"),xw=l(),v(Qa.$$.fragment),ih=l(),Us=n("h2"),Ln=n("a"),qc=n("span"),v(Ha.$$.fragment),zw=l(),Pc=n("span"),jw=a("FlaxDistilBertForTokenClassification"),lh=l(),je=n("div"),v(Ka.$$.fragment),Cw=l(),Ac=n("p"),qw=a(`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),Pw=l(),Va=n("p"),Aw=a("This model inherits from "),sl=n("a"),Ow=a("FlaxPreTrainedModel"),Nw=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),Lw=l(),Ja=n("p"),Sw=a("This model is also a Flax Linen "),Ga=n("a"),Iw=a("flax.linen.Module"),Ww=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Rw=l(),Oc=n("p"),Uw=a("Finally, this model supports inherent JAX features such as:"),Qw=l(),Nt=n("ul"),Nc=n("li"),Xa=n("a"),Hw=a("Just-In-Time (JIT) compilation"),Kw=l(),Lc=n("li"),Ya=n("a"),Vw=a("Automatic Differentiation"),Jw=l(),Sc=n("li"),Za=n("a"),Gw=a("Vectorization"),Xw=l(),Ic=n("li"),ei=n("a"),Yw=a("Parallelization"),Zw=l(),dt=n("div"),v(ti.$$.fragment),ey=l(),Qs=n("p"),ty=a("The "),Wc=n("code"),sy=a("FlaxDistilBertPreTrainedModel"),ny=a(" forward method, overrides the "),Rc=n("code"),oy=a("__call__"),ry=a(" special method."),ay=l(),v(Sn.$$.fragment),iy=l(),Uc=n("p"),ly=a("Example:"),dy=l(),v(si.$$.fragment),dh=l(),Hs=n("h2"),In=n("a"),Qc=n("span"),v(ni.$$.fragment),cy=l(),Hc=n("span"),py=a("FlaxDistilBertForQuestionAnswering"),ch=l(),Ce=n("div"),v(oi.$$.fragment),hy=l(),Ks=n("p"),my=a(`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Kc=n("code"),uy=a("span start logits"),fy=a(" and "),Vc=n("code"),gy=a("span end logits"),_y=a(")."),by=l(),ri=n("p"),vy=a("This model inherits from "),nl=n("a"),ky=a("FlaxPreTrainedModel"),Ty=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),wy=l(),ai=n("p"),yy=a("This model is also a Flax Linen "),ii=n("a"),$y=a("flax.linen.Module"),Fy=a(`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),Dy=l(),Jc=n("p"),By=a("Finally, this model supports inherent JAX features such as:"),My=l(),Lt=n("ul"),Gc=n("li"),li=n("a"),Ey=a("Just-In-Time (JIT) compilation"),xy=l(),Xc=n("li"),di=n("a"),zy=a("Automatic Differentiation"),jy=l(),Yc=n("li"),ci=n("a"),Cy=a("Vectorization"),qy=l(),Zc=n("li"),pi=n("a"),Py=a("Parallelization"),Ay=l(),ct=n("div"),v(hi.$$.fragment),Oy=l(),Vs=n("p"),Ny=a("The "),ep=n("code"),Ly=a("FlaxDistilBertPreTrainedModel"),Sy=a(" forward method, overrides the "),tp=n("code"),Iy=a("__call__"),Wy=a(" special method."),Ry=l(),v(Wn.$$.fragment),Uy=l(),sp=n("p"),Qy=a("Example:"),Hy=l(),v(mi.$$.fragment),this.h()},l(s){const f=D2('[data-svelte="svelte-1phssyn"]',document.head);p=o(f,"META",{name:!0,content:!0}),f.forEach(t),D=d(s),g=o(s,"H1",{class:!0});var ui=r(g);b=o(ui,"A",{id:!0,class:!0,href:!0});var np=r(b);F=o(np,"SPAN",{});var op=r(F);k(_.$$.fragment,op),op.forEach(t),np.forEach(t),u=d(ui),B=o(ui,"SPAN",{});var rp=r(B);ce=i(rp,"DistilBERT"),rp.forEach(t),ui.forEach(t),K=d(s),E=o(s,"H2",{class:!0});var fi=r(E);G=o(fi,"A",{id:!0,class:!0,href:!0});var ap=r(G);S=o(ap,"SPAN",{});var ip=r(S);k(X.$$.fragment,ip),ip.forEach(t),ap.forEach(t),pe=d(fi),I=o(fi,"SPAN",{});var lp=r(I);he=i(lp,"Overview"),lp.forEach(t),fi.forEach(t),ae=d(s),N=o(s,"P",{});var St=r(N);P=i(St,"The DistilBERT model was proposed in the blog post "),Y=o(St,"A",{href:!0,rel:!0});var dp=r(Y);V=i(dp,`Smaller, faster, cheaper, lighter: Introducing DistilBERT, a
distilled version of BERT`),dp.forEach(t),x=i(St,", and the paper "),z=o(St,"A",{href:!0,rel:!0});var cp=r(z);me=i(cp,`DistilBERT, a
distilled version of BERT: smaller, faster, cheaper and lighter`),cp.forEach(t),W=i(St,`. DistilBERT is a
small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
`),se=o(St,"EM",{});var pp=r(se);ue=i(pp,"bert-base-uncased"),pp.forEach(t),R=i(St,`, runs 60% faster while preserving over 95% of BERT\u2019s performances as measured on the GLUE language
understanding benchmark.`),St.forEach(t),ie=d(s),ee=o(s,"P",{});var hp=r(ee);A=i(hp,"The abstract from the paper is the following:"),hp.forEach(t),le=d(s),L=o(s,"P",{});var mp=r(L);ne=o(mp,"EM",{});var up=r(ne);fe=i(up,`As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.`),up.forEach(t),mp.forEach(t),q=d(s),te=o(s,"P",{});var fp=r(te);U=i(fp,"Tips:"),fp.forEach(t),de=d(s),h=o(s,"UL",{});var gi=r(h);M=o(gi,"LI",{});var It=r(M);J=i(It,"DistilBERT doesn\u2019t have "),_e=o(It,"CODE",{});var gp=r(_e);Te=i(gp,"token_type_ids"),gp.forEach(t),O=i(It,`, you don\u2019t need to indicate which token belongs to which segment. Just
separate your segments with the separation token `),be=o(It,"CODE",{});var _p=r(be);we=i(_p,"tokenizer.sep_token"),_p.forEach(t),ye=i(It," (or "),C=o(It,"CODE",{});var bp=r(C);Q=i(bp,"[SEP]"),bp.forEach(t),$e=i(It,")."),It.forEach(t),Fe=d(gi),Z=o(gi,"LI",{});var _i=r(Z);De=i(_i,"DistilBERT doesn\u2019t have options to select the input positions ("),oe=o(_i,"CODE",{});var vp=r(oe);Be=i(vp,"position_ids"),vp.forEach(t),Tm=i(_i,` input). This could be added if
necessary though, just let us know if you need this option.`),_i.forEach(t),gi.forEach(t),wp=d(s),vt=o(s,"P",{});var Wt=r(vt);wm=i(Wt,"This model was contributed by "),Yn=o(Wt,"A",{href:!0,rel:!0});var Ky=r(Yn);ym=i(Ky,"victorsanh"),Ky.forEach(t),$m=i(Wt,`. This model jax version was
contributed by `),Zn=o(Wt,"A",{href:!0,rel:!0});var Vy=r(Zn);Fm=i(Vy,"kamalkraj"),Vy.forEach(t),Dm=i(Wt,". The original code can be found "),eo=o(Wt,"A",{href:!0,rel:!0});var Jy=r(eo);Bm=i(Jy,"here"),Jy.forEach(t),Mm=i(Wt,"."),Wt.forEach(t),yp=d(s),as=o(s,"H2",{class:!0});var hh=r(as);Js=o(hh,"A",{id:!0,class:!0,href:!0});var Gy=r(Js);Dl=o(Gy,"SPAN",{});var Xy=r(Dl);k(to.$$.fragment,Xy),Xy.forEach(t),Gy.forEach(t),Em=d(hh),Bl=o(hh,"SPAN",{});var Yy=r(Bl);xm=i(Yy,"DistilBertConfig"),Yy.forEach(t),hh.forEach(t),$p=d(s),Ve=o(s,"DIV",{class:!0});var Rt=r(Ve);k(so.$$.fragment,Rt),zm=d(Rt),Ct=o(Rt,"P",{});var Rn=r(Ct);jm=i(Rn,"This is the configuration class to store the configuration of a "),bi=o(Rn,"A",{href:!0});var Zy=r(bi);Cm=i(Zy,"DistilBertModel"),Zy.forEach(t),qm=i(Rn," or a "),vi=o(Rn,"A",{href:!0});var e1=r(vi);Pm=i(e1,"TFDistilBertModel"),e1.forEach(t),Am=i(Rn,`. It
is used to instantiate a DistilBERT model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the DistilBERT
`),no=o(Rn,"A",{href:!0,rel:!0});var t1=r(no);Om=i(t1,"distilbert-base-uncased"),t1.forEach(t),Nm=i(Rn," architecture."),Rn.forEach(t),Lm=d(Rt),is=o(Rt,"P",{});var ol=r(is);Sm=i(ol,"Configuration objects inherit from "),ki=o(ol,"A",{href:!0});var s1=r(ki);Im=i(s1,"PretrainedConfig"),s1.forEach(t),Wm=i(ol,` and can be used to control the model outputs. Read the
documentation from `),Ti=o(ol,"A",{href:!0});var n1=r(Ti);Rm=i(n1,"PretrainedConfig"),n1.forEach(t),Um=i(ol," for more information."),ol.forEach(t),Qm=d(Rt),Ml=o(Rt,"P",{});var o1=r(Ml);Hm=i(o1,"Examples:"),o1.forEach(t),Km=d(Rt),k(oo.$$.fragment,Rt),Rt.forEach(t),Fp=d(s),ls=o(s,"H2",{class:!0});var mh=r(ls);Gs=o(mh,"A",{id:!0,class:!0,href:!0});var r1=r(Gs);El=o(r1,"SPAN",{});var a1=r(El);k(ro.$$.fragment,a1),a1.forEach(t),r1.forEach(t),Vm=d(mh),xl=o(mh,"SPAN",{});var i1=r(xl);Jm=i(i1,"DistilBertTokenizer"),i1.forEach(t),mh.forEach(t),Dp=d(s),_t=o(s,"DIV",{class:!0});var Un=r(_t);k(ao.$$.fragment,Un),Gm=d(Un),zl=o(Un,"P",{});var l1=r(zl);Xm=i(l1,"Construct a DistilBERT tokenizer."),l1.forEach(t),Ym=d(Un),Xs=o(Un,"P",{});var kp=r(Xs);wi=o(kp,"A",{href:!0});var d1=r(wi);Zm=i(d1,"DistilBertTokenizer"),d1.forEach(t),eu=i(kp," is identical to "),yi=o(kp,"A",{href:!0});var c1=r(yi);tu=i(c1,"BertTokenizer"),c1.forEach(t),su=i(kp,` and runs end-to-end tokenization: punctuation splitting
and wordpiece.`),kp.forEach(t),nu=d(Un),io=o(Un,"P",{});var uh=r(io);ou=i(uh,"Refer to superclass "),$i=o(uh,"A",{href:!0});var p1=r($i);ru=i(p1,"BertTokenizer"),p1.forEach(t),au=i(uh," for usage examples and documentation concerning parameters."),uh.forEach(t),Un.forEach(t),Bp=d(s),ds=o(s,"H2",{class:!0});var fh=r(ds);Ys=o(fh,"A",{id:!0,class:!0,href:!0});var h1=r(Ys);jl=o(h1,"SPAN",{});var m1=r(jl);k(lo.$$.fragment,m1),m1.forEach(t),h1.forEach(t),iu=d(fh),Cl=o(fh,"SPAN",{});var u1=r(Cl);lu=i(u1,"DistilBertTokenizerFast"),u1.forEach(t),fh.forEach(t),Mp=d(s),bt=o(s,"DIV",{class:!0});var Qn=r(bt);k(co.$$.fragment,Qn),du=d(Qn),po=o(Qn,"P",{});var gh=r(po);cu=i(gh,"Construct a \u201Cfast\u201D DistilBERT tokenizer (backed by HuggingFace\u2019s "),ql=o(gh,"EM",{});var f1=r(ql);pu=i(f1,"tokenizers"),f1.forEach(t),hu=i(gh," library)."),gh.forEach(t),mu=d(Qn),Zs=o(Qn,"P",{});var Tp=r(Zs);Fi=o(Tp,"A",{href:!0});var g1=r(Fi);uu=i(g1,"DistilBertTokenizerFast"),g1.forEach(t),fu=i(Tp," is identical to "),Di=o(Tp,"A",{href:!0});var _1=r(Di);gu=i(_1,"BertTokenizerFast"),_1.forEach(t),_u=i(Tp,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Tp.forEach(t),bu=d(Qn),ho=o(Qn,"P",{});var _h=r(ho);vu=i(_h,"Refer to superclass "),Bi=o(_h,"A",{href:!0});var b1=r(Bi);ku=i(b1,"BertTokenizerFast"),b1.forEach(t),Tu=i(_h," for usage examples and documentation concerning parameters."),_h.forEach(t),Qn.forEach(t),Ep=d(s),cs=o(s,"H2",{class:!0});var bh=r(cs);en=o(bh,"A",{id:!0,class:!0,href:!0});var v1=r(en);Pl=o(v1,"SPAN",{});var k1=r(Pl);k(mo.$$.fragment,k1),k1.forEach(t),v1.forEach(t),wu=d(bh),Al=o(bh,"SPAN",{});var T1=r(Al);yu=i(T1,"DistilBertModel"),T1.forEach(t),bh.forEach(t),xp=d(s),Je=o(s,"DIV",{class:!0});var Ut=r(Je);k(uo.$$.fragment,Ut),$u=d(Ut),Ol=o(Ut,"P",{});var w1=r(Ol);Fu=i(w1,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),w1.forEach(t),Du=d(Ut),fo=o(Ut,"P",{});var vh=r(fo);Bu=i(vh,"This model inherits from "),Mi=o(vh,"A",{href:!0});var y1=r(Mi);Mu=i(y1,"PreTrainedModel"),y1.forEach(t),Eu=i(vh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vh.forEach(t),xu=d(Ut),go=o(Ut,"P",{});var kh=r(go);zu=i(kh,"This model is also a PyTorch "),_o=o(kh,"A",{href:!0,rel:!0});var $1=r(_o);ju=i($1,"torch.nn.Module"),$1.forEach(t),Cu=i(kh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),kh.forEach(t),qu=d(Ut),tt=o(Ut,"DIV",{class:!0});var Qt=r(tt);k(bo.$$.fragment,Qt),Pu=d(Qt),ps=o(Qt,"P",{});var rl=r(ps);Au=i(rl,"The "),Ei=o(rl,"A",{href:!0});var F1=r(Ei);Ou=i(F1,"DistilBertModel"),F1.forEach(t),Nu=i(rl," forward method, overrides the "),Nl=o(rl,"CODE",{});var D1=r(Nl);Lu=i(D1,"__call__"),D1.forEach(t),Su=i(rl," special method."),rl.forEach(t),Iu=d(Qt),k(tn.$$.fragment,Qt),Wu=d(Qt),Ll=o(Qt,"P",{});var B1=r(Ll);Ru=i(B1,"Example:"),B1.forEach(t),Uu=d(Qt),k(vo.$$.fragment,Qt),Qt.forEach(t),Ut.forEach(t),zp=d(s),hs=o(s,"H2",{class:!0});var Th=r(hs);sn=o(Th,"A",{id:!0,class:!0,href:!0});var M1=r(sn);Sl=o(M1,"SPAN",{});var E1=r(Sl);k(ko.$$.fragment,E1),E1.forEach(t),M1.forEach(t),Qu=d(Th),Il=o(Th,"SPAN",{});var x1=r(Il);Hu=i(x1,"DistilBertForMaskedLM"),x1.forEach(t),Th.forEach(t),jp=d(s),Ge=o(s,"DIV",{class:!0});var Ht=r(Ge);k(To.$$.fragment,Ht),Ku=d(Ht),wo=o(Ht,"P",{});var wh=r(wo);Vu=i(wh,"DistilBert Model with a "),Wl=o(wh,"CODE",{});var z1=r(Wl);Ju=i(z1,"masked language modeling"),z1.forEach(t),Gu=i(wh," head on top."),wh.forEach(t),Xu=d(Ht),yo=o(Ht,"P",{});var yh=r(yo);Yu=i(yh,"This model inherits from "),xi=o(yh,"A",{href:!0});var j1=r(xi);Zu=i(j1,"PreTrainedModel"),j1.forEach(t),ef=i(yh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),yh.forEach(t),tf=d(Ht),$o=o(Ht,"P",{});var $h=r($o);sf=i($h,"This model is also a PyTorch "),Fo=o($h,"A",{href:!0,rel:!0});var C1=r(Fo);nf=i(C1,"torch.nn.Module"),C1.forEach(t),of=i($h,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),$h.forEach(t),rf=d(Ht),Ie=o(Ht,"DIV",{class:!0});var kt=r(Ie);k(Do.$$.fragment,kt),af=d(kt),ms=o(kt,"P",{});var al=r(ms);lf=i(al,"The "),zi=o(al,"A",{href:!0});var q1=r(zi);df=i(q1,"DistilBertForMaskedLM"),q1.forEach(t),cf=i(al," forward method, overrides the "),Rl=o(al,"CODE",{});var P1=r(Rl);pf=i(P1,"__call__"),P1.forEach(t),hf=i(al," special method."),al.forEach(t),mf=d(kt),k(nn.$$.fragment,kt),uf=d(kt),Ul=o(kt,"P",{});var A1=r(Ul);ff=i(A1,"Example:"),A1.forEach(t),gf=d(kt),k(Bo.$$.fragment,kt),_f=d(kt),k(Mo.$$.fragment,kt),kt.forEach(t),Ht.forEach(t),Cp=d(s),us=o(s,"H2",{class:!0});var Fh=r(us);on=o(Fh,"A",{id:!0,class:!0,href:!0});var O1=r(on);Ql=o(O1,"SPAN",{});var N1=r(Ql);k(Eo.$$.fragment,N1),N1.forEach(t),O1.forEach(t),bf=d(Fh),Hl=o(Fh,"SPAN",{});var L1=r(Hl);vf=i(L1,"DistilBertForSequenceClassification"),L1.forEach(t),Fh.forEach(t),qp=d(s),Xe=o(s,"DIV",{class:!0});var Kt=r(Xe);k(xo.$$.fragment,Kt),kf=d(Kt),Kl=o(Kt,"P",{});var S1=r(Kl);Tf=i(S1,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),S1.forEach(t),wf=d(Kt),zo=o(Kt,"P",{});var Dh=r(zo);yf=i(Dh,"This model inherits from "),ji=o(Dh,"A",{href:!0});var I1=r(ji);$f=i(I1,"PreTrainedModel"),I1.forEach(t),Ff=i(Dh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Dh.forEach(t),Df=d(Kt),jo=o(Kt,"P",{});var Bh=r(jo);Bf=i(Bh,"This model is also a PyTorch "),Co=o(Bh,"A",{href:!0,rel:!0});var W1=r(Co);Mf=i(W1,"torch.nn.Module"),W1.forEach(t),Ef=i(Bh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Bh.forEach(t),xf=d(Kt),ke=o(Kt,"DIV",{class:!0});var qe=r(ke);k(qo.$$.fragment,qe),zf=d(qe),fs=o(qe,"P",{});var il=r(fs);jf=i(il,"The "),Ci=o(il,"A",{href:!0});var R1=r(Ci);Cf=i(R1,"DistilBertForSequenceClassification"),R1.forEach(t),qf=i(il," forward method, overrides the "),Vl=o(il,"CODE",{});var U1=r(Vl);Pf=i(U1,"__call__"),U1.forEach(t),Af=i(il," special method."),il.forEach(t),Of=d(qe),k(rn.$$.fragment,qe),Nf=d(qe),Jl=o(qe,"P",{});var Q1=r(Jl);Lf=i(Q1,"Example of single-label classification:"),Q1.forEach(t),Sf=d(qe),k(Po.$$.fragment,qe),If=d(qe),k(Ao.$$.fragment,qe),Wf=d(qe),Gl=o(qe,"P",{});var H1=r(Gl);Rf=i(H1,"Example of multi-label classification:"),H1.forEach(t),Uf=d(qe),k(Oo.$$.fragment,qe),Qf=d(qe),k(No.$$.fragment,qe),qe.forEach(t),Kt.forEach(t),Pp=d(s),gs=o(s,"H2",{class:!0});var Mh=r(gs);an=o(Mh,"A",{id:!0,class:!0,href:!0});var K1=r(an);Xl=o(K1,"SPAN",{});var V1=r(Xl);k(Lo.$$.fragment,V1),V1.forEach(t),K1.forEach(t),Hf=d(Mh),Yl=o(Mh,"SPAN",{});var J1=r(Yl);Kf=i(J1,"DistilBertForMultipleChoice"),J1.forEach(t),Mh.forEach(t),Ap=d(s),Ye=o(s,"DIV",{class:!0});var Vt=r(Ye);k(So.$$.fragment,Vt),Vf=d(Vt),Zl=o(Vt,"P",{});var G1=r(Zl);Jf=i(G1,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),G1.forEach(t),Gf=d(Vt),Io=o(Vt,"P",{});var Eh=r(Io);Xf=i(Eh,"This model inherits from "),qi=o(Eh,"A",{href:!0});var X1=r(qi);Yf=i(X1,"PreTrainedModel"),X1.forEach(t),Zf=i(Eh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Eh.forEach(t),eg=d(Vt),Wo=o(Vt,"P",{});var xh=r(Wo);tg=i(xh,"This model is also a PyTorch "),Ro=o(xh,"A",{href:!0,rel:!0});var Y1=r(Ro);sg=i(Y1,"torch.nn.Module"),Y1.forEach(t),ng=i(xh,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),xh.forEach(t),og=d(Vt),st=o(Vt,"DIV",{class:!0});var Jt=r(st);k(Uo.$$.fragment,Jt),rg=d(Jt),_s=o(Jt,"P",{});var ll=r(_s);ag=i(ll,"The "),Pi=o(ll,"A",{href:!0});var Z1=r(Pi);ig=i(Z1,"DistilBertForMultipleChoice"),Z1.forEach(t),lg=i(ll," forward method, overrides the "),ed=o(ll,"CODE",{});var e$=r(ed);dg=i(e$,"__call__"),e$.forEach(t),cg=i(ll," special method."),ll.forEach(t),pg=d(Jt),k(ln.$$.fragment,Jt),hg=d(Jt),td=o(Jt,"P",{});var t$=r(td);mg=i(t$,"Examples:"),t$.forEach(t),ug=d(Jt),k(Qo.$$.fragment,Jt),Jt.forEach(t),Vt.forEach(t),Op=d(s),bs=o(s,"H2",{class:!0});var zh=r(bs);dn=o(zh,"A",{id:!0,class:!0,href:!0});var s$=r(dn);sd=o(s$,"SPAN",{});var n$=r(sd);k(Ho.$$.fragment,n$),n$.forEach(t),s$.forEach(t),fg=d(zh),nd=o(zh,"SPAN",{});var o$=r(nd);gg=i(o$,"DistilBertForTokenClassification"),o$.forEach(t),zh.forEach(t),Np=d(s),Ze=o(s,"DIV",{class:!0});var Gt=r(Ze);k(Ko.$$.fragment,Gt),_g=d(Gt),od=o(Gt,"P",{});var r$=r(od);bg=i(r$,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),r$.forEach(t),vg=d(Gt),Vo=o(Gt,"P",{});var jh=r(Vo);kg=i(jh,"This model inherits from "),Ai=o(jh,"A",{href:!0});var a$=r(Ai);Tg=i(a$,"PreTrainedModel"),a$.forEach(t),wg=i(jh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),jh.forEach(t),yg=d(Gt),Jo=o(Gt,"P",{});var Ch=r(Jo);$g=i(Ch,"This model is also a PyTorch "),Go=o(Ch,"A",{href:!0,rel:!0});var i$=r(Go);Fg=i(i$,"torch.nn.Module"),i$.forEach(t),Dg=i(Ch,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ch.forEach(t),Bg=d(Gt),We=o(Gt,"DIV",{class:!0});var Tt=r(We);k(Xo.$$.fragment,Tt),Mg=d(Tt),vs=o(Tt,"P",{});var dl=r(vs);Eg=i(dl,"The "),Oi=o(dl,"A",{href:!0});var l$=r(Oi);xg=i(l$,"DistilBertForTokenClassification"),l$.forEach(t),zg=i(dl," forward method, overrides the "),rd=o(dl,"CODE",{});var d$=r(rd);jg=i(d$,"__call__"),d$.forEach(t),Cg=i(dl," special method."),dl.forEach(t),qg=d(Tt),k(cn.$$.fragment,Tt),Pg=d(Tt),ad=o(Tt,"P",{});var c$=r(ad);Ag=i(c$,"Example:"),c$.forEach(t),Og=d(Tt),k(Yo.$$.fragment,Tt),Ng=d(Tt),k(Zo.$$.fragment,Tt),Tt.forEach(t),Gt.forEach(t),Lp=d(s),ks=o(s,"H2",{class:!0});var qh=r(ks);pn=o(qh,"A",{id:!0,class:!0,href:!0});var p$=r(pn);id=o(p$,"SPAN",{});var h$=r(id);k(er.$$.fragment,h$),h$.forEach(t),p$.forEach(t),Lg=d(qh),ld=o(qh,"SPAN",{});var m$=r(ld);Sg=i(m$,"DistilBertForQuestionAnswering"),m$.forEach(t),qh.forEach(t),Sp=d(s),et=o(s,"DIV",{class:!0});var Xt=r(et);k(tr.$$.fragment,Xt),Ig=d(Xt),Ts=o(Xt,"P",{});var cl=r(Ts);Wg=i(cl,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),dd=o(cl,"CODE",{});var u$=r(dd);Rg=i(u$,"span start logits"),u$.forEach(t),Ug=i(cl," and "),cd=o(cl,"CODE",{});var f$=r(cd);Qg=i(f$,"span end logits"),f$.forEach(t),Hg=i(cl,")."),cl.forEach(t),Kg=d(Xt),sr=o(Xt,"P",{});var Ph=r(sr);Vg=i(Ph,"This model inherits from "),Ni=o(Ph,"A",{href:!0});var g$=r(Ni);Jg=i(g$,"PreTrainedModel"),g$.forEach(t),Gg=i(Ph,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ph.forEach(t),Xg=d(Xt),nr=o(Xt,"P",{});var Ah=r(nr);Yg=i(Ah,"This model is also a PyTorch "),or=o(Ah,"A",{href:!0,rel:!0});var _$=r(or);Zg=i(_$,"torch.nn.Module"),_$.forEach(t),e_=i(Ah,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ah.forEach(t),t_=d(Xt),Re=o(Xt,"DIV",{class:!0});var wt=r(Re);k(rr.$$.fragment,wt),s_=d(wt),ws=o(wt,"P",{});var pl=r(ws);n_=i(pl,"The "),Li=o(pl,"A",{href:!0});var b$=r(Li);o_=i(b$,"DistilBertForQuestionAnswering"),b$.forEach(t),r_=i(pl," forward method, overrides the "),pd=o(pl,"CODE",{});var v$=r(pd);a_=i(v$,"__call__"),v$.forEach(t),i_=i(pl," special method."),pl.forEach(t),l_=d(wt),k(hn.$$.fragment,wt),d_=d(wt),hd=o(wt,"P",{});var k$=r(hd);c_=i(k$,"Example:"),k$.forEach(t),p_=d(wt),k(ar.$$.fragment,wt),h_=d(wt),k(ir.$$.fragment,wt),wt.forEach(t),Xt.forEach(t),Ip=d(s),ys=o(s,"H2",{class:!0});var Oh=r(ys);mn=o(Oh,"A",{id:!0,class:!0,href:!0});var T$=r(mn);md=o(T$,"SPAN",{});var w$=r(md);k(lr.$$.fragment,w$),w$.forEach(t),T$.forEach(t),m_=d(Oh),ud=o(Oh,"SPAN",{});var y$=r(ud);u_=i(y$,"TFDistilBertModel"),y$.forEach(t),Oh.forEach(t),Wp=d(s),Pe=o(s,"DIV",{class:!0});var yt=r(Pe);k(dr.$$.fragment,yt),f_=d(yt),fd=o(yt,"P",{});var $$=r(fd);g_=i($$,"The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."),$$.forEach(t),__=d(yt),cr=o(yt,"P",{});var Nh=r(cr);b_=i(Nh,"This model inherits from "),Si=o(Nh,"A",{href:!0});var F$=r(Si);v_=i(F$,"TFPreTrainedModel"),F$.forEach(t),k_=i(Nh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Nh.forEach(t),T_=d(yt),pr=o(yt,"P",{});var Lh=r(pr);w_=i(Lh,"This model is also a "),hr=o(Lh,"A",{href:!0,rel:!0});var D$=r(hr);y_=i(D$,"tf.keras.Model"),D$.forEach(t),$_=i(Lh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Lh.forEach(t),F_=d(yt),k(un.$$.fragment,yt),D_=d(yt),nt=o(yt,"DIV",{class:!0});var Yt=r(nt);k(mr.$$.fragment,Yt),B_=d(Yt),$s=o(Yt,"P",{});var hl=r($s);M_=i(hl,"The "),Ii=o(hl,"A",{href:!0});var B$=r(Ii);E_=i(B$,"TFDistilBertModel"),B$.forEach(t),x_=i(hl," forward method, overrides the "),gd=o(hl,"CODE",{});var M$=r(gd);z_=i(M$,"__call__"),M$.forEach(t),j_=i(hl," special method."),hl.forEach(t),C_=d(Yt),k(fn.$$.fragment,Yt),q_=d(Yt),_d=o(Yt,"P",{});var E$=r(_d);P_=i(E$,"Example:"),E$.forEach(t),A_=d(Yt),k(ur.$$.fragment,Yt),Yt.forEach(t),yt.forEach(t),Rp=d(s),Fs=o(s,"H2",{class:!0});var Sh=r(Fs);gn=o(Sh,"A",{id:!0,class:!0,href:!0});var x$=r(gn);bd=o(x$,"SPAN",{});var z$=r(bd);k(fr.$$.fragment,z$),z$.forEach(t),x$.forEach(t),O_=d(Sh),vd=o(Sh,"SPAN",{});var j$=r(vd);N_=i(j$,"TFDistilBertForMaskedLM"),j$.forEach(t),Sh.forEach(t),Up=d(s),Ae=o(s,"DIV",{class:!0});var $t=r(Ae);k(gr.$$.fragment,$t),L_=d($t),_r=o($t,"P",{});var Ih=r(_r);S_=i(Ih,"DistilBert Model with a "),kd=o(Ih,"CODE",{});var C$=r(kd);I_=i(C$,"masked language modeling"),C$.forEach(t),W_=i(Ih," head on top."),Ih.forEach(t),R_=d($t),br=o($t,"P",{});var Wh=r(br);U_=i(Wh,"This model inherits from "),Wi=o(Wh,"A",{href:!0});var q$=r(Wi);Q_=i(q$,"TFPreTrainedModel"),q$.forEach(t),H_=i(Wh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wh.forEach(t),K_=d($t),vr=o($t,"P",{});var Rh=r(vr);V_=i(Rh,"This model is also a "),kr=o(Rh,"A",{href:!0,rel:!0});var P$=r(kr);J_=i(P$,"tf.keras.Model"),P$.forEach(t),G_=i(Rh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Rh.forEach(t),X_=d($t),k(_n.$$.fragment,$t),Y_=d($t),Ue=o($t,"DIV",{class:!0});var Ft=r(Ue);k(Tr.$$.fragment,Ft),Z_=d(Ft),Ds=o(Ft,"P",{});var ml=r(Ds);eb=i(ml,"The "),Ri=o(ml,"A",{href:!0});var A$=r(Ri);tb=i(A$,"TFDistilBertForMaskedLM"),A$.forEach(t),sb=i(ml," forward method, overrides the "),Td=o(ml,"CODE",{});var O$=r(Td);nb=i(O$,"__call__"),O$.forEach(t),ob=i(ml," special method."),ml.forEach(t),rb=d(Ft),k(bn.$$.fragment,Ft),ab=d(Ft),wd=o(Ft,"P",{});var N$=r(wd);ib=i(N$,"Example:"),N$.forEach(t),lb=d(Ft),k(wr.$$.fragment,Ft),db=d(Ft),k(yr.$$.fragment,Ft),Ft.forEach(t),$t.forEach(t),Qp=d(s),Bs=o(s,"H2",{class:!0});var Uh=r(Bs);vn=o(Uh,"A",{id:!0,class:!0,href:!0});var L$=r(vn);yd=o(L$,"SPAN",{});var S$=r(yd);k($r.$$.fragment,S$),S$.forEach(t),L$.forEach(t),cb=d(Uh),$d=o(Uh,"SPAN",{});var I$=r($d);pb=i(I$,"TFDistilBertForSequenceClassification"),I$.forEach(t),Uh.forEach(t),Hp=d(s),Oe=o(s,"DIV",{class:!0});var Dt=r(Oe);k(Fr.$$.fragment,Dt),hb=d(Dt),Fd=o(Dt,"P",{});var W$=r(Fd);mb=i(W$,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),W$.forEach(t),ub=d(Dt),Dr=o(Dt,"P",{});var Qh=r(Dr);fb=i(Qh,"This model inherits from "),Ui=o(Qh,"A",{href:!0});var R$=r(Ui);gb=i(R$,"TFPreTrainedModel"),R$.forEach(t),_b=i(Qh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qh.forEach(t),bb=d(Dt),Br=o(Dt,"P",{});var Hh=r(Br);vb=i(Hh,"This model is also a "),Mr=o(Hh,"A",{href:!0,rel:!0});var U$=r(Mr);kb=i(U$,"tf.keras.Model"),U$.forEach(t),Tb=i(Hh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Hh.forEach(t),wb=d(Dt),k(kn.$$.fragment,Dt),yb=d(Dt),Qe=o(Dt,"DIV",{class:!0});var Bt=r(Qe);k(Er.$$.fragment,Bt),$b=d(Bt),Ms=o(Bt,"P",{});var ul=r(Ms);Fb=i(ul,"The "),Qi=o(ul,"A",{href:!0});var Q$=r(Qi);Db=i(Q$,"TFDistilBertForSequenceClassification"),Q$.forEach(t),Bb=i(ul," forward method, overrides the "),Dd=o(ul,"CODE",{});var H$=r(Dd);Mb=i(H$,"__call__"),H$.forEach(t),Eb=i(ul," special method."),ul.forEach(t),xb=d(Bt),k(Tn.$$.fragment,Bt),zb=d(Bt),Bd=o(Bt,"P",{});var K$=r(Bd);jb=i(K$,"Example:"),K$.forEach(t),Cb=d(Bt),k(xr.$$.fragment,Bt),qb=d(Bt),k(zr.$$.fragment,Bt),Bt.forEach(t),Dt.forEach(t),Kp=d(s),Es=o(s,"H2",{class:!0});var Kh=r(Es);wn=o(Kh,"A",{id:!0,class:!0,href:!0});var V$=r(wn);Md=o(V$,"SPAN",{});var J$=r(Md);k(jr.$$.fragment,J$),J$.forEach(t),V$.forEach(t),Pb=d(Kh),Ed=o(Kh,"SPAN",{});var G$=r(Ed);Ab=i(G$,"TFDistilBertForMultipleChoice"),G$.forEach(t),Kh.forEach(t),Vp=d(s),Ne=o(s,"DIV",{class:!0});var Mt=r(Ne);k(Cr.$$.fragment,Mt),Ob=d(Mt),xd=o(Mt,"P",{});var X$=r(xd);Nb=i(X$,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),X$.forEach(t),Lb=d(Mt),qr=o(Mt,"P",{});var Vh=r(qr);Sb=i(Vh,"This model inherits from "),Hi=o(Vh,"A",{href:!0});var Y$=r(Hi);Ib=i(Y$,"TFPreTrainedModel"),Y$.forEach(t),Wb=i(Vh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Vh.forEach(t),Rb=d(Mt),Pr=o(Mt,"P",{});var Jh=r(Pr);Ub=i(Jh,"This model is also a "),Ar=o(Jh,"A",{href:!0,rel:!0});var Z$=r(Ar);Qb=i(Z$,"tf.keras.Model"),Z$.forEach(t),Hb=i(Jh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Jh.forEach(t),Kb=d(Mt),k(yn.$$.fragment,Mt),Vb=d(Mt),ot=o(Mt,"DIV",{class:!0});var Zt=r(ot);k(Or.$$.fragment,Zt),Jb=d(Zt),xs=o(Zt,"P",{});var fl=r(xs);Gb=i(fl,"The "),Ki=o(fl,"A",{href:!0});var eF=r(Ki);Xb=i(eF,"TFDistilBertForMultipleChoice"),eF.forEach(t),Yb=i(fl," forward method, overrides the "),zd=o(fl,"CODE",{});var tF=r(zd);Zb=i(tF,"__call__"),tF.forEach(t),ev=i(fl," special method."),fl.forEach(t),tv=d(Zt),k($n.$$.fragment,Zt),sv=d(Zt),jd=o(Zt,"P",{});var sF=r(jd);nv=i(sF,"Example:"),sF.forEach(t),ov=d(Zt),k(Nr.$$.fragment,Zt),Zt.forEach(t),Mt.forEach(t),Jp=d(s),zs=o(s,"H2",{class:!0});var Gh=r(zs);Fn=o(Gh,"A",{id:!0,class:!0,href:!0});var nF=r(Fn);Cd=o(nF,"SPAN",{});var oF=r(Cd);k(Lr.$$.fragment,oF),oF.forEach(t),nF.forEach(t),rv=d(Gh),qd=o(Gh,"SPAN",{});var rF=r(qd);av=i(rF,"TFDistilBertForTokenClassification"),rF.forEach(t),Gh.forEach(t),Gp=d(s),Le=o(s,"DIV",{class:!0});var Et=r(Le);k(Sr.$$.fragment,Et),iv=d(Et),Pd=o(Et,"P",{});var aF=r(Pd);lv=i(aF,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),aF.forEach(t),dv=d(Et),Ir=o(Et,"P",{});var Xh=r(Ir);cv=i(Xh,"This model inherits from "),Vi=o(Xh,"A",{href:!0});var iF=r(Vi);pv=i(iF,"TFPreTrainedModel"),iF.forEach(t),hv=i(Xh,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xh.forEach(t),mv=d(Et),Wr=o(Et,"P",{});var Yh=r(Wr);uv=i(Yh,"This model is also a "),Rr=o(Yh,"A",{href:!0,rel:!0});var lF=r(Rr);fv=i(lF,"tf.keras.Model"),lF.forEach(t),gv=i(Yh,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),Yh.forEach(t),_v=d(Et),k(Dn.$$.fragment,Et),bv=d(Et),He=o(Et,"DIV",{class:!0});var xt=r(He);k(Ur.$$.fragment,xt),vv=d(xt),js=o(xt,"P",{});var gl=r(js);kv=i(gl,"The "),Ji=o(gl,"A",{href:!0});var dF=r(Ji);Tv=i(dF,"TFDistilBertForTokenClassification"),dF.forEach(t),wv=i(gl," forward method, overrides the "),Ad=o(gl,"CODE",{});var cF=r(Ad);yv=i(cF,"__call__"),cF.forEach(t),$v=i(gl," special method."),gl.forEach(t),Fv=d(xt),k(Bn.$$.fragment,xt),Dv=d(xt),Od=o(xt,"P",{});var pF=r(Od);Bv=i(pF,"Example:"),pF.forEach(t),Mv=d(xt),k(Qr.$$.fragment,xt),Ev=d(xt),k(Hr.$$.fragment,xt),xt.forEach(t),Et.forEach(t),Xp=d(s),Cs=o(s,"H2",{class:!0});var Zh=r(Cs);Mn=o(Zh,"A",{id:!0,class:!0,href:!0});var hF=r(Mn);Nd=o(hF,"SPAN",{});var mF=r(Nd);k(Kr.$$.fragment,mF),mF.forEach(t),hF.forEach(t),xv=d(Zh),Ld=o(Zh,"SPAN",{});var uF=r(Ld);zv=i(uF,"TFDistilBertForQuestionAnswering"),uF.forEach(t),Zh.forEach(t),Yp=d(s),Se=o(s,"DIV",{class:!0});var zt=r(Se);k(Vr.$$.fragment,zt),jv=d(zt),qs=o(zt,"P",{});var _l=r(qs);Cv=i(_l,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layer on top of the hidden-states output to compute `),Sd=o(_l,"CODE",{});var fF=r(Sd);qv=i(fF,"span start logits"),fF.forEach(t),Pv=i(_l," and "),Id=o(_l,"CODE",{});var gF=r(Id);Av=i(gF,"span end logits"),gF.forEach(t),Ov=i(_l,")."),_l.forEach(t),Nv=d(zt),Jr=o(zt,"P",{});var em=r(Jr);Lv=i(em,"This model inherits from "),Gi=o(em,"A",{href:!0});var _F=r(Gi);Sv=i(_F,"TFPreTrainedModel"),_F.forEach(t),Iv=i(em,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),em.forEach(t),Wv=d(zt),Gr=o(zt,"P",{});var tm=r(Gr);Rv=i(tm,"This model is also a "),Xr=o(tm,"A",{href:!0,rel:!0});var bF=r(Xr);Uv=i(bF,"tf.keras.Model"),bF.forEach(t),Qv=i(tm,` subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`),tm.forEach(t),Hv=d(zt),k(En.$$.fragment,zt),Kv=d(zt),Ke=o(zt,"DIV",{class:!0});var jt=r(Ke);k(Yr.$$.fragment,jt),Vv=d(jt),Ps=o(jt,"P",{});var bl=r(Ps);Jv=i(bl,"The "),Xi=o(bl,"A",{href:!0});var vF=r(Xi);Gv=i(vF,"TFDistilBertForQuestionAnswering"),vF.forEach(t),Xv=i(bl," forward method, overrides the "),Wd=o(bl,"CODE",{});var kF=r(Wd);Yv=i(kF,"__call__"),kF.forEach(t),Zv=i(bl," special method."),bl.forEach(t),ek=d(jt),k(xn.$$.fragment,jt),tk=d(jt),Rd=o(jt,"P",{});var TF=r(Rd);sk=i(TF,"Example:"),TF.forEach(t),nk=d(jt),k(Zr.$$.fragment,jt),ok=d(jt),k(ea.$$.fragment,jt),jt.forEach(t),zt.forEach(t),Zp=d(s),As=o(s,"H2",{class:!0});var sm=r(As);zn=o(sm,"A",{id:!0,class:!0,href:!0});var wF=r(zn);Ud=o(wF,"SPAN",{});var yF=r(Ud);k(ta.$$.fragment,yF),yF.forEach(t),wF.forEach(t),rk=d(sm),Qd=o(sm,"SPAN",{});var $F=r(Qd);ak=i($F,"FlaxDistilBertModel"),$F.forEach(t),sm.forEach(t),eh=d(s),Me=o(s,"DIV",{class:!0});var pt=r(Me);k(sa.$$.fragment,pt),ik=d(pt),Hd=o(pt,"P",{});var FF=r(Hd);lk=i(FF,"The bare DistilBert Model transformer outputting raw hidden-states without any specific head on top."),FF.forEach(t),dk=d(pt),na=o(pt,"P",{});var nm=r(na);ck=i(nm,"This model inherits from "),Yi=o(nm,"A",{href:!0});var DF=r(Yi);pk=i(DF,"FlaxPreTrainedModel"),DF.forEach(t),hk=i(nm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),nm.forEach(t),mk=d(pt),oa=o(pt,"P",{});var om=r(oa);uk=i(om,"This model is also a Flax Linen "),ra=o(om,"A",{href:!0,rel:!0});var BF=r(ra);fk=i(BF,"flax.linen.Module"),BF.forEach(t),gk=i(om,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),om.forEach(t),_k=d(pt),Kd=o(pt,"P",{});var MF=r(Kd);bk=i(MF,"Finally, this model supports inherent JAX features such as:"),MF.forEach(t),vk=d(pt),qt=o(pt,"UL",{});var Hn=r(qt);Vd=o(Hn,"LI",{});var EF=r(Vd);aa=o(EF,"A",{href:!0,rel:!0});var xF=r(aa);kk=i(xF,"Just-In-Time (JIT) compilation"),xF.forEach(t),EF.forEach(t),Tk=d(Hn),Jd=o(Hn,"LI",{});var zF=r(Jd);ia=o(zF,"A",{href:!0,rel:!0});var jF=r(ia);wk=i(jF,"Automatic Differentiation"),jF.forEach(t),zF.forEach(t),yk=d(Hn),Gd=o(Hn,"LI",{});var CF=r(Gd);la=o(CF,"A",{href:!0,rel:!0});var qF=r(la);$k=i(qF,"Vectorization"),qF.forEach(t),CF.forEach(t),Fk=d(Hn),Xd=o(Hn,"LI",{});var PF=r(Xd);da=o(PF,"A",{href:!0,rel:!0});var AF=r(da);Dk=i(AF,"Parallelization"),AF.forEach(t),PF.forEach(t),Hn.forEach(t),Bk=d(pt),rt=o(pt,"DIV",{class:!0});var es=r(rt);k(ca.$$.fragment,es),Mk=d(es),Os=o(es,"P",{});var vl=r(Os);Ek=i(vl,"The "),Yd=o(vl,"CODE",{});var OF=r(Yd);xk=i(OF,"FlaxDistilBertPreTrainedModel"),OF.forEach(t),zk=i(vl," forward method, overrides the "),Zd=o(vl,"CODE",{});var NF=r(Zd);jk=i(NF,"__call__"),NF.forEach(t),Ck=i(vl," special method."),vl.forEach(t),qk=d(es),k(jn.$$.fragment,es),Pk=d(es),ec=o(es,"P",{});var LF=r(ec);Ak=i(LF,"Example:"),LF.forEach(t),Ok=d(es),k(pa.$$.fragment,es),es.forEach(t),pt.forEach(t),th=d(s),Ns=o(s,"H2",{class:!0});var rm=r(Ns);Cn=o(rm,"A",{id:!0,class:!0,href:!0});var SF=r(Cn);tc=o(SF,"SPAN",{});var IF=r(tc);k(ha.$$.fragment,IF),IF.forEach(t),SF.forEach(t),Nk=d(rm),sc=o(rm,"SPAN",{});var WF=r(sc);Lk=i(WF,"FlaxDistilBertForMaskedLM"),WF.forEach(t),rm.forEach(t),sh=d(s),Ee=o(s,"DIV",{class:!0});var ht=r(Ee);k(ma.$$.fragment,ht),Sk=d(ht),ua=o(ht,"P",{});var am=r(ua);Ik=i(am,"DistilBert Model with a "),nc=o(am,"CODE",{});var RF=r(nc);Wk=i(RF,"language modeling"),RF.forEach(t),Rk=i(am," head on top."),am.forEach(t),Uk=d(ht),fa=o(ht,"P",{});var im=r(fa);Qk=i(im,"This model inherits from "),Zi=o(im,"A",{href:!0});var UF=r(Zi);Hk=i(UF,"FlaxPreTrainedModel"),UF.forEach(t),Kk=i(im,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),im.forEach(t),Vk=d(ht),ga=o(ht,"P",{});var lm=r(ga);Jk=i(lm,"This model is also a Flax Linen "),_a=o(lm,"A",{href:!0,rel:!0});var QF=r(_a);Gk=i(QF,"flax.linen.Module"),QF.forEach(t),Xk=i(lm,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),lm.forEach(t),Yk=d(ht),oc=o(ht,"P",{});var HF=r(oc);Zk=i(HF,"Finally, this model supports inherent JAX features such as:"),HF.forEach(t),eT=d(ht),Pt=o(ht,"UL",{});var Kn=r(Pt);rc=o(Kn,"LI",{});var KF=r(rc);ba=o(KF,"A",{href:!0,rel:!0});var VF=r(ba);tT=i(VF,"Just-In-Time (JIT) compilation"),VF.forEach(t),KF.forEach(t),sT=d(Kn),ac=o(Kn,"LI",{});var JF=r(ac);va=o(JF,"A",{href:!0,rel:!0});var GF=r(va);nT=i(GF,"Automatic Differentiation"),GF.forEach(t),JF.forEach(t),oT=d(Kn),ic=o(Kn,"LI",{});var XF=r(ic);ka=o(XF,"A",{href:!0,rel:!0});var YF=r(ka);rT=i(YF,"Vectorization"),YF.forEach(t),XF.forEach(t),aT=d(Kn),lc=o(Kn,"LI",{});var ZF=r(lc);Ta=o(ZF,"A",{href:!0,rel:!0});var eD=r(Ta);iT=i(eD,"Parallelization"),eD.forEach(t),ZF.forEach(t),Kn.forEach(t),lT=d(ht),at=o(ht,"DIV",{class:!0});var ts=r(at);k(wa.$$.fragment,ts),dT=d(ts),Ls=o(ts,"P",{});var kl=r(Ls);cT=i(kl,"The "),dc=o(kl,"CODE",{});var tD=r(dc);pT=i(tD,"FlaxDistilBertPreTrainedModel"),tD.forEach(t),hT=i(kl," forward method, overrides the "),cc=o(kl,"CODE",{});var sD=r(cc);mT=i(sD,"__call__"),sD.forEach(t),uT=i(kl," special method."),kl.forEach(t),fT=d(ts),k(qn.$$.fragment,ts),gT=d(ts),pc=o(ts,"P",{});var nD=r(pc);_T=i(nD,"Example:"),nD.forEach(t),bT=d(ts),k(ya.$$.fragment,ts),ts.forEach(t),ht.forEach(t),nh=d(s),Ss=o(s,"H2",{class:!0});var dm=r(Ss);Pn=o(dm,"A",{id:!0,class:!0,href:!0});var oD=r(Pn);hc=o(oD,"SPAN",{});var rD=r(hc);k($a.$$.fragment,rD),rD.forEach(t),oD.forEach(t),vT=d(dm),mc=o(dm,"SPAN",{});var aD=r(mc);kT=i(aD,"FlaxDistilBertForSequenceClassification"),aD.forEach(t),dm.forEach(t),oh=d(s),xe=o(s,"DIV",{class:!0});var mt=r(xe);k(Fa.$$.fragment,mt),TT=d(mt),uc=o(mt,"P",{});var iD=r(uc);wT=i(iD,`DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the
pooled output) e.g. for GLUE tasks.`),iD.forEach(t),yT=d(mt),Da=o(mt,"P",{});var cm=r(Da);$T=i(cm,"This model inherits from "),el=o(cm,"A",{href:!0});var lD=r(el);FT=i(lD,"FlaxPreTrainedModel"),lD.forEach(t),DT=i(cm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),cm.forEach(t),BT=d(mt),Ba=o(mt,"P",{});var pm=r(Ba);MT=i(pm,"This model is also a Flax Linen "),Ma=o(pm,"A",{href:!0,rel:!0});var dD=r(Ma);ET=i(dD,"flax.linen.Module"),dD.forEach(t),xT=i(pm,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),pm.forEach(t),zT=d(mt),fc=o(mt,"P",{});var cD=r(fc);jT=i(cD,"Finally, this model supports inherent JAX features such as:"),cD.forEach(t),CT=d(mt),At=o(mt,"UL",{});var Vn=r(At);gc=o(Vn,"LI",{});var pD=r(gc);Ea=o(pD,"A",{href:!0,rel:!0});var hD=r(Ea);qT=i(hD,"Just-In-Time (JIT) compilation"),hD.forEach(t),pD.forEach(t),PT=d(Vn),_c=o(Vn,"LI",{});var mD=r(_c);xa=o(mD,"A",{href:!0,rel:!0});var uD=r(xa);AT=i(uD,"Automatic Differentiation"),uD.forEach(t),mD.forEach(t),OT=d(Vn),bc=o(Vn,"LI",{});var fD=r(bc);za=o(fD,"A",{href:!0,rel:!0});var gD=r(za);NT=i(gD,"Vectorization"),gD.forEach(t),fD.forEach(t),LT=d(Vn),vc=o(Vn,"LI",{});var _D=r(vc);ja=o(_D,"A",{href:!0,rel:!0});var bD=r(ja);ST=i(bD,"Parallelization"),bD.forEach(t),_D.forEach(t),Vn.forEach(t),IT=d(mt),it=o(mt,"DIV",{class:!0});var ss=r(it);k(Ca.$$.fragment,ss),WT=d(ss),Is=o(ss,"P",{});var Tl=r(Is);RT=i(Tl,"The "),kc=o(Tl,"CODE",{});var vD=r(kc);UT=i(vD,"FlaxDistilBertPreTrainedModel"),vD.forEach(t),QT=i(Tl," forward method, overrides the "),Tc=o(Tl,"CODE",{});var kD=r(Tc);HT=i(kD,"__call__"),kD.forEach(t),KT=i(Tl," special method."),Tl.forEach(t),VT=d(ss),k(An.$$.fragment,ss),JT=d(ss),wc=o(ss,"P",{});var TD=r(wc);GT=i(TD,"Example:"),TD.forEach(t),XT=d(ss),k(qa.$$.fragment,ss),ss.forEach(t),mt.forEach(t),rh=d(s),Ws=o(s,"H2",{class:!0});var hm=r(Ws);On=o(hm,"A",{id:!0,class:!0,href:!0});var wD=r(On);yc=o(wD,"SPAN",{});var yD=r(yc);k(Pa.$$.fragment,yD),yD.forEach(t),wD.forEach(t),YT=d(hm),$c=o(hm,"SPAN",{});var $D=r($c);ZT=i($D,"FlaxDistilBertForMultipleChoice"),$D.forEach(t),hm.forEach(t),ah=d(s),ze=o(s,"DIV",{class:!0});var ut=r(ze);k(Aa.$$.fragment,ut),ew=d(ut),Fc=o(ut,"P",{});var FD=r(Fc);tw=i(FD,`DistilBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and
a softmax) e.g. for RocStories/SWAG tasks.`),FD.forEach(t),sw=d(ut),Oa=o(ut,"P",{});var mm=r(Oa);nw=i(mm,"This model inherits from "),tl=o(mm,"A",{href:!0});var DD=r(tl);ow=i(DD,"FlaxPreTrainedModel"),DD.forEach(t),rw=i(mm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),mm.forEach(t),aw=d(ut),Na=o(ut,"P",{});var um=r(Na);iw=i(um,"This model is also a Flax Linen "),La=o(um,"A",{href:!0,rel:!0});var BD=r(La);lw=i(BD,"flax.linen.Module"),BD.forEach(t),dw=i(um,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),um.forEach(t),cw=d(ut),Dc=o(ut,"P",{});var MD=r(Dc);pw=i(MD,"Finally, this model supports inherent JAX features such as:"),MD.forEach(t),hw=d(ut),Ot=o(ut,"UL",{});var Jn=r(Ot);Bc=o(Jn,"LI",{});var ED=r(Bc);Sa=o(ED,"A",{href:!0,rel:!0});var xD=r(Sa);mw=i(xD,"Just-In-Time (JIT) compilation"),xD.forEach(t),ED.forEach(t),uw=d(Jn),Mc=o(Jn,"LI",{});var zD=r(Mc);Ia=o(zD,"A",{href:!0,rel:!0});var jD=r(Ia);fw=i(jD,"Automatic Differentiation"),jD.forEach(t),zD.forEach(t),gw=d(Jn),Ec=o(Jn,"LI",{});var CD=r(Ec);Wa=o(CD,"A",{href:!0,rel:!0});var qD=r(Wa);_w=i(qD,"Vectorization"),qD.forEach(t),CD.forEach(t),bw=d(Jn),xc=o(Jn,"LI",{});var PD=r(xc);Ra=o(PD,"A",{href:!0,rel:!0});var AD=r(Ra);vw=i(AD,"Parallelization"),AD.forEach(t),PD.forEach(t),Jn.forEach(t),kw=d(ut),lt=o(ut,"DIV",{class:!0});var ns=r(lt);k(Ua.$$.fragment,ns),Tw=d(ns),Rs=o(ns,"P",{});var wl=r(Rs);ww=i(wl,"The "),zc=o(wl,"CODE",{});var OD=r(zc);yw=i(OD,"FlaxDistilBertPreTrainedModel"),OD.forEach(t),$w=i(wl," forward method, overrides the "),jc=o(wl,"CODE",{});var ND=r(jc);Fw=i(ND,"__call__"),ND.forEach(t),Dw=i(wl," special method."),wl.forEach(t),Bw=d(ns),k(Nn.$$.fragment,ns),Mw=d(ns),Cc=o(ns,"P",{});var LD=r(Cc);Ew=i(LD,"Example:"),LD.forEach(t),xw=d(ns),k(Qa.$$.fragment,ns),ns.forEach(t),ut.forEach(t),ih=d(s),Us=o(s,"H2",{class:!0});var fm=r(Us);Ln=o(fm,"A",{id:!0,class:!0,href:!0});var SD=r(Ln);qc=o(SD,"SPAN",{});var ID=r(qc);k(Ha.$$.fragment,ID),ID.forEach(t),SD.forEach(t),zw=d(fm),Pc=o(fm,"SPAN",{});var WD=r(Pc);jw=i(WD,"FlaxDistilBertForTokenClassification"),WD.forEach(t),fm.forEach(t),lh=d(s),je=o(s,"DIV",{class:!0});var ft=r(je);k(Ka.$$.fragment,ft),Cw=d(ft),Ac=o(ft,"P",{});var RD=r(Ac);qw=i(RD,`DistilBert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.
for Named-Entity-Recognition (NER) tasks.`),RD.forEach(t),Pw=d(ft),Va=o(ft,"P",{});var gm=r(Va);Aw=i(gm,"This model inherits from "),sl=o(gm,"A",{href:!0});var UD=r(sl);Ow=i(UD,"FlaxPreTrainedModel"),UD.forEach(t),Nw=i(gm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),gm.forEach(t),Lw=d(ft),Ja=o(ft,"P",{});var _m=r(Ja);Sw=i(_m,"This model is also a Flax Linen "),Ga=o(_m,"A",{href:!0,rel:!0});var QD=r(Ga);Iw=i(QD,"flax.linen.Module"),QD.forEach(t),Ww=i(_m,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),_m.forEach(t),Rw=d(ft),Oc=o(ft,"P",{});var HD=r(Oc);Uw=i(HD,"Finally, this model supports inherent JAX features such as:"),HD.forEach(t),Qw=d(ft),Nt=o(ft,"UL",{});var Gn=r(Nt);Nc=o(Gn,"LI",{});var KD=r(Nc);Xa=o(KD,"A",{href:!0,rel:!0});var VD=r(Xa);Hw=i(VD,"Just-In-Time (JIT) compilation"),VD.forEach(t),KD.forEach(t),Kw=d(Gn),Lc=o(Gn,"LI",{});var JD=r(Lc);Ya=o(JD,"A",{href:!0,rel:!0});var GD=r(Ya);Vw=i(GD,"Automatic Differentiation"),GD.forEach(t),JD.forEach(t),Jw=d(Gn),Sc=o(Gn,"LI",{});var XD=r(Sc);Za=o(XD,"A",{href:!0,rel:!0});var YD=r(Za);Gw=i(YD,"Vectorization"),YD.forEach(t),XD.forEach(t),Xw=d(Gn),Ic=o(Gn,"LI",{});var ZD=r(Ic);ei=o(ZD,"A",{href:!0,rel:!0});var e2=r(ei);Yw=i(e2,"Parallelization"),e2.forEach(t),ZD.forEach(t),Gn.forEach(t),Zw=d(ft),dt=o(ft,"DIV",{class:!0});var os=r(dt);k(ti.$$.fragment,os),ey=d(os),Qs=o(os,"P",{});var yl=r(Qs);ty=i(yl,"The "),Wc=o(yl,"CODE",{});var t2=r(Wc);sy=i(t2,"FlaxDistilBertPreTrainedModel"),t2.forEach(t),ny=i(yl," forward method, overrides the "),Rc=o(yl,"CODE",{});var s2=r(Rc);oy=i(s2,"__call__"),s2.forEach(t),ry=i(yl," special method."),yl.forEach(t),ay=d(os),k(Sn.$$.fragment,os),iy=d(os),Uc=o(os,"P",{});var n2=r(Uc);ly=i(n2,"Example:"),n2.forEach(t),dy=d(os),k(si.$$.fragment,os),os.forEach(t),ft.forEach(t),dh=d(s),Hs=o(s,"H2",{class:!0});var bm=r(Hs);In=o(bm,"A",{id:!0,class:!0,href:!0});var o2=r(In);Qc=o(o2,"SPAN",{});var r2=r(Qc);k(ni.$$.fragment,r2),r2.forEach(t),o2.forEach(t),cy=d(bm),Hc=o(bm,"SPAN",{});var a2=r(Hc);py=i(a2,"FlaxDistilBertForQuestionAnswering"),a2.forEach(t),bm.forEach(t),ch=d(s),Ce=o(s,"DIV",{class:!0});var gt=r(Ce);k(oi.$$.fragment,gt),hy=d(gt),Ks=o(gt,"P",{});var $l=r(Ks);my=i($l,`DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a
linear layers on top of the hidden-states output to compute `),Kc=o($l,"CODE",{});var i2=r(Kc);uy=i(i2,"span start logits"),i2.forEach(t),fy=i($l," and "),Vc=o($l,"CODE",{});var l2=r(Vc);gy=i(l2,"span end logits"),l2.forEach(t),_y=i($l,")."),$l.forEach(t),by=d(gt),ri=o(gt,"P",{});var vm=r(ri);vy=i(vm,"This model inherits from "),nl=o(vm,"A",{href:!0});var d2=r(nl);ky=i(d2,"FlaxPreTrainedModel"),d2.forEach(t),Ty=i(vm,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`),vm.forEach(t),wy=d(gt),ai=o(gt,"P",{});var km=r(ai);yy=i(km,"This model is also a Flax Linen "),ii=o(km,"A",{href:!0,rel:!0});var c2=r(ii);$y=i(c2,"flax.linen.Module"),c2.forEach(t),Fy=i(km,`
subclass. Use it as a regular Flax linen Module and refer to the Flax documentation for all matter related to
general usage and behavior.`),km.forEach(t),Dy=d(gt),Jc=o(gt,"P",{});var p2=r(Jc);By=i(p2,"Finally, this model supports inherent JAX features such as:"),p2.forEach(t),My=d(gt),Lt=o(gt,"UL",{});var Xn=r(Lt);Gc=o(Xn,"LI",{});var h2=r(Gc);li=o(h2,"A",{href:!0,rel:!0});var m2=r(li);Ey=i(m2,"Just-In-Time (JIT) compilation"),m2.forEach(t),h2.forEach(t),xy=d(Xn),Xc=o(Xn,"LI",{});var u2=r(Xc);di=o(u2,"A",{href:!0,rel:!0});var f2=r(di);zy=i(f2,"Automatic Differentiation"),f2.forEach(t),u2.forEach(t),jy=d(Xn),Yc=o(Xn,"LI",{});var g2=r(Yc);ci=o(g2,"A",{href:!0,rel:!0});var _2=r(ci);Cy=i(_2,"Vectorization"),_2.forEach(t),g2.forEach(t),qy=d(Xn),Zc=o(Xn,"LI",{});var b2=r(Zc);pi=o(b2,"A",{href:!0,rel:!0});var v2=r(pi);Py=i(v2,"Parallelization"),v2.forEach(t),b2.forEach(t),Xn.forEach(t),Ay=d(gt),ct=o(gt,"DIV",{class:!0});var rs=r(ct);k(hi.$$.fragment,rs),Oy=d(rs),Vs=o(rs,"P",{});var Fl=r(Vs);Ny=i(Fl,"The "),ep=o(Fl,"CODE",{});var k2=r(ep);Ly=i(k2,"FlaxDistilBertPreTrainedModel"),k2.forEach(t),Sy=i(Fl," forward method, overrides the "),tp=o(Fl,"CODE",{});var T2=r(tp);Iy=i(T2,"__call__"),T2.forEach(t),Wy=i(Fl," special method."),Fl.forEach(t),Ry=d(rs),k(Wn.$$.fragment,rs),Uy=d(rs),sp=o(rs,"P",{});var w2=r(sp);Qy=i(w2,"Example:"),w2.forEach(t),Hy=d(rs),k(mi.$$.fragment,rs),rs.forEach(t),gt.forEach(t),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Z2)),c(b,"id","distilbert"),c(b,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(b,"href","#distilbert"),c(g,"class","relative group"),c(G,"id","overview"),c(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(G,"href","#overview"),c(E,"class","relative group"),c(Y,"href","https://medium.com/huggingface/distilbert-8cf3380435b5"),c(Y,"rel","nofollow"),c(z,"href","https://arxiv.org/abs/1910.01108"),c(z,"rel","nofollow"),c(Yn,"href","https://huggingface.co/victorsanh"),c(Yn,"rel","nofollow"),c(Zn,"href","https://huggingface.co/kamalkraj"),c(Zn,"rel","nofollow"),c(eo,"href","https://github.com/huggingface/transformers/tree/main/examples/research_projects/distillation"),c(eo,"rel","nofollow"),c(Js,"id","transformers.DistilBertConfig"),c(Js,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Js,"href","#transformers.DistilBertConfig"),c(as,"class","relative group"),c(bi,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel"),c(vi,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(no,"href","https://huggingface.co/distilbert-base-uncased"),c(no,"rel","nofollow"),c(ki,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ti,"href","/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ve,"class","docstring"),c(Gs,"id","transformers.DistilBertTokenizer"),c(Gs,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Gs,"href","#transformers.DistilBertTokenizer"),c(ls,"class","relative group"),c(wi,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer"),c(yi,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c($i,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer"),c(_t,"class","docstring"),c(Ys,"id","transformers.DistilBertTokenizerFast"),c(Ys,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ys,"href","#transformers.DistilBertTokenizerFast"),c(ds,"class","relative group"),c(Fi,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),c(Di,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(Bi,"href","/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast"),c(bt,"class","docstring"),c(en,"id","transformers.DistilBertModel"),c(en,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(en,"href","#transformers.DistilBertModel"),c(cs,"class","relative group"),c(Mi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(_o,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(_o,"rel","nofollow"),c(Ei,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel"),c(tt,"class","docstring"),c(Je,"class","docstring"),c(sn,"id","transformers.DistilBertForMaskedLM"),c(sn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(sn,"href","#transformers.DistilBertForMaskedLM"),c(hs,"class","relative group"),c(xi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Fo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Fo,"rel","nofollow"),c(zi,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),c(Ie,"class","docstring"),c(Ge,"class","docstring"),c(on,"id","transformers.DistilBertForSequenceClassification"),c(on,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(on,"href","#transformers.DistilBertForSequenceClassification"),c(us,"class","relative group"),c(ji,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Co,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Co,"rel","nofollow"),c(Ci,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),c(ke,"class","docstring"),c(Xe,"class","docstring"),c(an,"id","transformers.DistilBertForMultipleChoice"),c(an,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(an,"href","#transformers.DistilBertForMultipleChoice"),c(gs,"class","relative group"),c(qi,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Ro,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Ro,"rel","nofollow"),c(Pi,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),c(st,"class","docstring"),c(Ye,"class","docstring"),c(dn,"id","transformers.DistilBertForTokenClassification"),c(dn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(dn,"href","#transformers.DistilBertForTokenClassification"),c(bs,"class","relative group"),c(Ai,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(Go,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(Go,"rel","nofollow"),c(Oi,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),c(We,"class","docstring"),c(Ze,"class","docstring"),c(pn,"id","transformers.DistilBertForQuestionAnswering"),c(pn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(pn,"href","#transformers.DistilBertForQuestionAnswering"),c(ks,"class","relative group"),c(Ni,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel"),c(or,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),c(or,"rel","nofollow"),c(Li,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),c(Re,"class","docstring"),c(et,"class","docstring"),c(mn,"id","transformers.TFDistilBertModel"),c(mn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(mn,"href","#transformers.TFDistilBertModel"),c(ys,"class","relative group"),c(Si,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(hr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(hr,"rel","nofollow"),c(Ii,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertModel"),c(nt,"class","docstring"),c(Pe,"class","docstring"),c(gn,"id","transformers.TFDistilBertForMaskedLM"),c(gn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(gn,"href","#transformers.TFDistilBertForMaskedLM"),c(Fs,"class","relative group"),c(Wi,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(kr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(kr,"rel","nofollow"),c(Ri,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),c(Ue,"class","docstring"),c(Ae,"class","docstring"),c(vn,"id","transformers.TFDistilBertForSequenceClassification"),c(vn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(vn,"href","#transformers.TFDistilBertForSequenceClassification"),c(Bs,"class","relative group"),c(Ui,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Mr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Mr,"rel","nofollow"),c(Qi,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),c(Qe,"class","docstring"),c(Oe,"class","docstring"),c(wn,"id","transformers.TFDistilBertForMultipleChoice"),c(wn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(wn,"href","#transformers.TFDistilBertForMultipleChoice"),c(Es,"class","relative group"),c(Hi,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Ar,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Ar,"rel","nofollow"),c(Ki,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),c(ot,"class","docstring"),c(Ne,"class","docstring"),c(Fn,"id","transformers.TFDistilBertForTokenClassification"),c(Fn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Fn,"href","#transformers.TFDistilBertForTokenClassification"),c(zs,"class","relative group"),c(Vi,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Rr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Rr,"rel","nofollow"),c(Ji,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),c(He,"class","docstring"),c(Le,"class","docstring"),c(Mn,"id","transformers.TFDistilBertForQuestionAnswering"),c(Mn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Mn,"href","#transformers.TFDistilBertForQuestionAnswering"),c(Cs,"class","relative group"),c(Gi,"href","/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel"),c(Xr,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),c(Xr,"rel","nofollow"),c(Xi,"href","/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),c(Ke,"class","docstring"),c(Se,"class","docstring"),c(zn,"id","transformers.FlaxDistilBertModel"),c(zn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(zn,"href","#transformers.FlaxDistilBertModel"),c(As,"class","relative group"),c(Yi,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(ra,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(ra,"rel","nofollow"),c(aa,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(aa,"rel","nofollow"),c(ia,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(ia,"rel","nofollow"),c(la,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(la,"rel","nofollow"),c(da,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(da,"rel","nofollow"),c(rt,"class","docstring"),c(Me,"class","docstring"),c(Cn,"id","transformers.FlaxDistilBertForMaskedLM"),c(Cn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Cn,"href","#transformers.FlaxDistilBertForMaskedLM"),c(Ns,"class","relative group"),c(Zi,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(_a,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(_a,"rel","nofollow"),c(ba,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(ba,"rel","nofollow"),c(va,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(va,"rel","nofollow"),c(ka,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(ka,"rel","nofollow"),c(Ta,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Ta,"rel","nofollow"),c(at,"class","docstring"),c(Ee,"class","docstring"),c(Pn,"id","transformers.FlaxDistilBertForSequenceClassification"),c(Pn,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Pn,"href","#transformers.FlaxDistilBertForSequenceClassification"),c(Ss,"class","relative group"),c(el,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ma,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Ma,"rel","nofollow"),c(Ea,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Ea,"rel","nofollow"),c(xa,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(xa,"rel","nofollow"),c(za,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(za,"rel","nofollow"),c(ja,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(ja,"rel","nofollow"),c(it,"class","docstring"),c(xe,"class","docstring"),c(On,"id","transformers.FlaxDistilBertForMultipleChoice"),c(On,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(On,"href","#transformers.FlaxDistilBertForMultipleChoice"),c(Ws,"class","relative group"),c(tl,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(La,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(La,"rel","nofollow"),c(Sa,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Sa,"rel","nofollow"),c(Ia,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Ia,"rel","nofollow"),c(Wa,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Wa,"rel","nofollow"),c(Ra,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(Ra,"rel","nofollow"),c(lt,"class","docstring"),c(ze,"class","docstring"),c(Ln,"id","transformers.FlaxDistilBertForTokenClassification"),c(Ln,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ln,"href","#transformers.FlaxDistilBertForTokenClassification"),c(Us,"class","relative group"),c(sl,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ga,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(Ga,"rel","nofollow"),c(Xa,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(Xa,"rel","nofollow"),c(Ya,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(Ya,"rel","nofollow"),c(Za,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(Za,"rel","nofollow"),c(ei,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(ei,"rel","nofollow"),c(dt,"class","docstring"),c(je,"class","docstring"),c(In,"id","transformers.FlaxDistilBertForQuestionAnswering"),c(In,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(In,"href","#transformers.FlaxDistilBertForQuestionAnswering"),c(Hs,"class","relative group"),c(nl,"href","/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(ii,"href","https://flax.readthedocs.io/en/latest/flax.linen.html#module"),c(ii,"rel","nofollow"),c(li,"href","https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit"),c(li,"rel","nofollow"),c(di,"href","https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation"),c(di,"rel","nofollow"),c(ci,"href","https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap"),c(ci,"rel","nofollow"),c(pi,"href","https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"),c(pi,"rel","nofollow"),c(ct,"class","docstring"),c(Ce,"class","docstring")},m(s,f){e(document.head,p),m(s,D,f),m(s,g,f),e(g,b),e(b,F),T(_,F,null),e(g,u),e(g,B),e(B,ce),m(s,K,f),m(s,E,f),e(E,G),e(G,S),T(X,S,null),e(E,pe),e(E,I),e(I,he),m(s,ae,f),m(s,N,f),e(N,P),e(N,Y),e(Y,V),e(N,x),e(N,z),e(z,me),e(N,W),e(N,se),e(se,ue),e(N,R),m(s,ie,f),m(s,ee,f),e(ee,A),m(s,le,f),m(s,L,f),e(L,ne),e(ne,fe),m(s,q,f),m(s,te,f),e(te,U),m(s,de,f),m(s,h,f),e(h,M),e(M,J),e(M,_e),e(_e,Te),e(M,O),e(M,be),e(be,we),e(M,ye),e(M,C),e(C,Q),e(M,$e),e(h,Fe),e(h,Z),e(Z,De),e(Z,oe),e(oe,Be),e(Z,Tm),m(s,wp,f),m(s,vt,f),e(vt,wm),e(vt,Yn),e(Yn,ym),e(vt,$m),e(vt,Zn),e(Zn,Fm),e(vt,Dm),e(vt,eo),e(eo,Bm),e(vt,Mm),m(s,yp,f),m(s,as,f),e(as,Js),e(Js,Dl),T(to,Dl,null),e(as,Em),e(as,Bl),e(Bl,xm),m(s,$p,f),m(s,Ve,f),T(so,Ve,null),e(Ve,zm),e(Ve,Ct),e(Ct,jm),e(Ct,bi),e(bi,Cm),e(Ct,qm),e(Ct,vi),e(vi,Pm),e(Ct,Am),e(Ct,no),e(no,Om),e(Ct,Nm),e(Ve,Lm),e(Ve,is),e(is,Sm),e(is,ki),e(ki,Im),e(is,Wm),e(is,Ti),e(Ti,Rm),e(is,Um),e(Ve,Qm),e(Ve,Ml),e(Ml,Hm),e(Ve,Km),T(oo,Ve,null),m(s,Fp,f),m(s,ls,f),e(ls,Gs),e(Gs,El),T(ro,El,null),e(ls,Vm),e(ls,xl),e(xl,Jm),m(s,Dp,f),m(s,_t,f),T(ao,_t,null),e(_t,Gm),e(_t,zl),e(zl,Xm),e(_t,Ym),e(_t,Xs),e(Xs,wi),e(wi,Zm),e(Xs,eu),e(Xs,yi),e(yi,tu),e(Xs,su),e(_t,nu),e(_t,io),e(io,ou),e(io,$i),e($i,ru),e(io,au),m(s,Bp,f),m(s,ds,f),e(ds,Ys),e(Ys,jl),T(lo,jl,null),e(ds,iu),e(ds,Cl),e(Cl,lu),m(s,Mp,f),m(s,bt,f),T(co,bt,null),e(bt,du),e(bt,po),e(po,cu),e(po,ql),e(ql,pu),e(po,hu),e(bt,mu),e(bt,Zs),e(Zs,Fi),e(Fi,uu),e(Zs,fu),e(Zs,Di),e(Di,gu),e(Zs,_u),e(bt,bu),e(bt,ho),e(ho,vu),e(ho,Bi),e(Bi,ku),e(ho,Tu),m(s,Ep,f),m(s,cs,f),e(cs,en),e(en,Pl),T(mo,Pl,null),e(cs,wu),e(cs,Al),e(Al,yu),m(s,xp,f),m(s,Je,f),T(uo,Je,null),e(Je,$u),e(Je,Ol),e(Ol,Fu),e(Je,Du),e(Je,fo),e(fo,Bu),e(fo,Mi),e(Mi,Mu),e(fo,Eu),e(Je,xu),e(Je,go),e(go,zu),e(go,_o),e(_o,ju),e(go,Cu),e(Je,qu),e(Je,tt),T(bo,tt,null),e(tt,Pu),e(tt,ps),e(ps,Au),e(ps,Ei),e(Ei,Ou),e(ps,Nu),e(ps,Nl),e(Nl,Lu),e(ps,Su),e(tt,Iu),T(tn,tt,null),e(tt,Wu),e(tt,Ll),e(Ll,Ru),e(tt,Uu),T(vo,tt,null),m(s,zp,f),m(s,hs,f),e(hs,sn),e(sn,Sl),T(ko,Sl,null),e(hs,Qu),e(hs,Il),e(Il,Hu),m(s,jp,f),m(s,Ge,f),T(To,Ge,null),e(Ge,Ku),e(Ge,wo),e(wo,Vu),e(wo,Wl),e(Wl,Ju),e(wo,Gu),e(Ge,Xu),e(Ge,yo),e(yo,Yu),e(yo,xi),e(xi,Zu),e(yo,ef),e(Ge,tf),e(Ge,$o),e($o,sf),e($o,Fo),e(Fo,nf),e($o,of),e(Ge,rf),e(Ge,Ie),T(Do,Ie,null),e(Ie,af),e(Ie,ms),e(ms,lf),e(ms,zi),e(zi,df),e(ms,cf),e(ms,Rl),e(Rl,pf),e(ms,hf),e(Ie,mf),T(nn,Ie,null),e(Ie,uf),e(Ie,Ul),e(Ul,ff),e(Ie,gf),T(Bo,Ie,null),e(Ie,_f),T(Mo,Ie,null),m(s,Cp,f),m(s,us,f),e(us,on),e(on,Ql),T(Eo,Ql,null),e(us,bf),e(us,Hl),e(Hl,vf),m(s,qp,f),m(s,Xe,f),T(xo,Xe,null),e(Xe,kf),e(Xe,Kl),e(Kl,Tf),e(Xe,wf),e(Xe,zo),e(zo,yf),e(zo,ji),e(ji,$f),e(zo,Ff),e(Xe,Df),e(Xe,jo),e(jo,Bf),e(jo,Co),e(Co,Mf),e(jo,Ef),e(Xe,xf),e(Xe,ke),T(qo,ke,null),e(ke,zf),e(ke,fs),e(fs,jf),e(fs,Ci),e(Ci,Cf),e(fs,qf),e(fs,Vl),e(Vl,Pf),e(fs,Af),e(ke,Of),T(rn,ke,null),e(ke,Nf),e(ke,Jl),e(Jl,Lf),e(ke,Sf),T(Po,ke,null),e(ke,If),T(Ao,ke,null),e(ke,Wf),e(ke,Gl),e(Gl,Rf),e(ke,Uf),T(Oo,ke,null),e(ke,Qf),T(No,ke,null),m(s,Pp,f),m(s,gs,f),e(gs,an),e(an,Xl),T(Lo,Xl,null),e(gs,Hf),e(gs,Yl),e(Yl,Kf),m(s,Ap,f),m(s,Ye,f),T(So,Ye,null),e(Ye,Vf),e(Ye,Zl),e(Zl,Jf),e(Ye,Gf),e(Ye,Io),e(Io,Xf),e(Io,qi),e(qi,Yf),e(Io,Zf),e(Ye,eg),e(Ye,Wo),e(Wo,tg),e(Wo,Ro),e(Ro,sg),e(Wo,ng),e(Ye,og),e(Ye,st),T(Uo,st,null),e(st,rg),e(st,_s),e(_s,ag),e(_s,Pi),e(Pi,ig),e(_s,lg),e(_s,ed),e(ed,dg),e(_s,cg),e(st,pg),T(ln,st,null),e(st,hg),e(st,td),e(td,mg),e(st,ug),T(Qo,st,null),m(s,Op,f),m(s,bs,f),e(bs,dn),e(dn,sd),T(Ho,sd,null),e(bs,fg),e(bs,nd),e(nd,gg),m(s,Np,f),m(s,Ze,f),T(Ko,Ze,null),e(Ze,_g),e(Ze,od),e(od,bg),e(Ze,vg),e(Ze,Vo),e(Vo,kg),e(Vo,Ai),e(Ai,Tg),e(Vo,wg),e(Ze,yg),e(Ze,Jo),e(Jo,$g),e(Jo,Go),e(Go,Fg),e(Jo,Dg),e(Ze,Bg),e(Ze,We),T(Xo,We,null),e(We,Mg),e(We,vs),e(vs,Eg),e(vs,Oi),e(Oi,xg),e(vs,zg),e(vs,rd),e(rd,jg),e(vs,Cg),e(We,qg),T(cn,We,null),e(We,Pg),e(We,ad),e(ad,Ag),e(We,Og),T(Yo,We,null),e(We,Ng),T(Zo,We,null),m(s,Lp,f),m(s,ks,f),e(ks,pn),e(pn,id),T(er,id,null),e(ks,Lg),e(ks,ld),e(ld,Sg),m(s,Sp,f),m(s,et,f),T(tr,et,null),e(et,Ig),e(et,Ts),e(Ts,Wg),e(Ts,dd),e(dd,Rg),e(Ts,Ug),e(Ts,cd),e(cd,Qg),e(Ts,Hg),e(et,Kg),e(et,sr),e(sr,Vg),e(sr,Ni),e(Ni,Jg),e(sr,Gg),e(et,Xg),e(et,nr),e(nr,Yg),e(nr,or),e(or,Zg),e(nr,e_),e(et,t_),e(et,Re),T(rr,Re,null),e(Re,s_),e(Re,ws),e(ws,n_),e(ws,Li),e(Li,o_),e(ws,r_),e(ws,pd),e(pd,a_),e(ws,i_),e(Re,l_),T(hn,Re,null),e(Re,d_),e(Re,hd),e(hd,c_),e(Re,p_),T(ar,Re,null),e(Re,h_),T(ir,Re,null),m(s,Ip,f),m(s,ys,f),e(ys,mn),e(mn,md),T(lr,md,null),e(ys,m_),e(ys,ud),e(ud,u_),m(s,Wp,f),m(s,Pe,f),T(dr,Pe,null),e(Pe,f_),e(Pe,fd),e(fd,g_),e(Pe,__),e(Pe,cr),e(cr,b_),e(cr,Si),e(Si,v_),e(cr,k_),e(Pe,T_),e(Pe,pr),e(pr,w_),e(pr,hr),e(hr,y_),e(pr,$_),e(Pe,F_),T(un,Pe,null),e(Pe,D_),e(Pe,nt),T(mr,nt,null),e(nt,B_),e(nt,$s),e($s,M_),e($s,Ii),e(Ii,E_),e($s,x_),e($s,gd),e(gd,z_),e($s,j_),e(nt,C_),T(fn,nt,null),e(nt,q_),e(nt,_d),e(_d,P_),e(nt,A_),T(ur,nt,null),m(s,Rp,f),m(s,Fs,f),e(Fs,gn),e(gn,bd),T(fr,bd,null),e(Fs,O_),e(Fs,vd),e(vd,N_),m(s,Up,f),m(s,Ae,f),T(gr,Ae,null),e(Ae,L_),e(Ae,_r),e(_r,S_),e(_r,kd),e(kd,I_),e(_r,W_),e(Ae,R_),e(Ae,br),e(br,U_),e(br,Wi),e(Wi,Q_),e(br,H_),e(Ae,K_),e(Ae,vr),e(vr,V_),e(vr,kr),e(kr,J_),e(vr,G_),e(Ae,X_),T(_n,Ae,null),e(Ae,Y_),e(Ae,Ue),T(Tr,Ue,null),e(Ue,Z_),e(Ue,Ds),e(Ds,eb),e(Ds,Ri),e(Ri,tb),e(Ds,sb),e(Ds,Td),e(Td,nb),e(Ds,ob),e(Ue,rb),T(bn,Ue,null),e(Ue,ab),e(Ue,wd),e(wd,ib),e(Ue,lb),T(wr,Ue,null),e(Ue,db),T(yr,Ue,null),m(s,Qp,f),m(s,Bs,f),e(Bs,vn),e(vn,yd),T($r,yd,null),e(Bs,cb),e(Bs,$d),e($d,pb),m(s,Hp,f),m(s,Oe,f),T(Fr,Oe,null),e(Oe,hb),e(Oe,Fd),e(Fd,mb),e(Oe,ub),e(Oe,Dr),e(Dr,fb),e(Dr,Ui),e(Ui,gb),e(Dr,_b),e(Oe,bb),e(Oe,Br),e(Br,vb),e(Br,Mr),e(Mr,kb),e(Br,Tb),e(Oe,wb),T(kn,Oe,null),e(Oe,yb),e(Oe,Qe),T(Er,Qe,null),e(Qe,$b),e(Qe,Ms),e(Ms,Fb),e(Ms,Qi),e(Qi,Db),e(Ms,Bb),e(Ms,Dd),e(Dd,Mb),e(Ms,Eb),e(Qe,xb),T(Tn,Qe,null),e(Qe,zb),e(Qe,Bd),e(Bd,jb),e(Qe,Cb),T(xr,Qe,null),e(Qe,qb),T(zr,Qe,null),m(s,Kp,f),m(s,Es,f),e(Es,wn),e(wn,Md),T(jr,Md,null),e(Es,Pb),e(Es,Ed),e(Ed,Ab),m(s,Vp,f),m(s,Ne,f),T(Cr,Ne,null),e(Ne,Ob),e(Ne,xd),e(xd,Nb),e(Ne,Lb),e(Ne,qr),e(qr,Sb),e(qr,Hi),e(Hi,Ib),e(qr,Wb),e(Ne,Rb),e(Ne,Pr),e(Pr,Ub),e(Pr,Ar),e(Ar,Qb),e(Pr,Hb),e(Ne,Kb),T(yn,Ne,null),e(Ne,Vb),e(Ne,ot),T(Or,ot,null),e(ot,Jb),e(ot,xs),e(xs,Gb),e(xs,Ki),e(Ki,Xb),e(xs,Yb),e(xs,zd),e(zd,Zb),e(xs,ev),e(ot,tv),T($n,ot,null),e(ot,sv),e(ot,jd),e(jd,nv),e(ot,ov),T(Nr,ot,null),m(s,Jp,f),m(s,zs,f),e(zs,Fn),e(Fn,Cd),T(Lr,Cd,null),e(zs,rv),e(zs,qd),e(qd,av),m(s,Gp,f),m(s,Le,f),T(Sr,Le,null),e(Le,iv),e(Le,Pd),e(Pd,lv),e(Le,dv),e(Le,Ir),e(Ir,cv),e(Ir,Vi),e(Vi,pv),e(Ir,hv),e(Le,mv),e(Le,Wr),e(Wr,uv),e(Wr,Rr),e(Rr,fv),e(Wr,gv),e(Le,_v),T(Dn,Le,null),e(Le,bv),e(Le,He),T(Ur,He,null),e(He,vv),e(He,js),e(js,kv),e(js,Ji),e(Ji,Tv),e(js,wv),e(js,Ad),e(Ad,yv),e(js,$v),e(He,Fv),T(Bn,He,null),e(He,Dv),e(He,Od),e(Od,Bv),e(He,Mv),T(Qr,He,null),e(He,Ev),T(Hr,He,null),m(s,Xp,f),m(s,Cs,f),e(Cs,Mn),e(Mn,Nd),T(Kr,Nd,null),e(Cs,xv),e(Cs,Ld),e(Ld,zv),m(s,Yp,f),m(s,Se,f),T(Vr,Se,null),e(Se,jv),e(Se,qs),e(qs,Cv),e(qs,Sd),e(Sd,qv),e(qs,Pv),e(qs,Id),e(Id,Av),e(qs,Ov),e(Se,Nv),e(Se,Jr),e(Jr,Lv),e(Jr,Gi),e(Gi,Sv),e(Jr,Iv),e(Se,Wv),e(Se,Gr),e(Gr,Rv),e(Gr,Xr),e(Xr,Uv),e(Gr,Qv),e(Se,Hv),T(En,Se,null),e(Se,Kv),e(Se,Ke),T(Yr,Ke,null),e(Ke,Vv),e(Ke,Ps),e(Ps,Jv),e(Ps,Xi),e(Xi,Gv),e(Ps,Xv),e(Ps,Wd),e(Wd,Yv),e(Ps,Zv),e(Ke,ek),T(xn,Ke,null),e(Ke,tk),e(Ke,Rd),e(Rd,sk),e(Ke,nk),T(Zr,Ke,null),e(Ke,ok),T(ea,Ke,null),m(s,Zp,f),m(s,As,f),e(As,zn),e(zn,Ud),T(ta,Ud,null),e(As,rk),e(As,Qd),e(Qd,ak),m(s,eh,f),m(s,Me,f),T(sa,Me,null),e(Me,ik),e(Me,Hd),e(Hd,lk),e(Me,dk),e(Me,na),e(na,ck),e(na,Yi),e(Yi,pk),e(na,hk),e(Me,mk),e(Me,oa),e(oa,uk),e(oa,ra),e(ra,fk),e(oa,gk),e(Me,_k),e(Me,Kd),e(Kd,bk),e(Me,vk),e(Me,qt),e(qt,Vd),e(Vd,aa),e(aa,kk),e(qt,Tk),e(qt,Jd),e(Jd,ia),e(ia,wk),e(qt,yk),e(qt,Gd),e(Gd,la),e(la,$k),e(qt,Fk),e(qt,Xd),e(Xd,da),e(da,Dk),e(Me,Bk),e(Me,rt),T(ca,rt,null),e(rt,Mk),e(rt,Os),e(Os,Ek),e(Os,Yd),e(Yd,xk),e(Os,zk),e(Os,Zd),e(Zd,jk),e(Os,Ck),e(rt,qk),T(jn,rt,null),e(rt,Pk),e(rt,ec),e(ec,Ak),e(rt,Ok),T(pa,rt,null),m(s,th,f),m(s,Ns,f),e(Ns,Cn),e(Cn,tc),T(ha,tc,null),e(Ns,Nk),e(Ns,sc),e(sc,Lk),m(s,sh,f),m(s,Ee,f),T(ma,Ee,null),e(Ee,Sk),e(Ee,ua),e(ua,Ik),e(ua,nc),e(nc,Wk),e(ua,Rk),e(Ee,Uk),e(Ee,fa),e(fa,Qk),e(fa,Zi),e(Zi,Hk),e(fa,Kk),e(Ee,Vk),e(Ee,ga),e(ga,Jk),e(ga,_a),e(_a,Gk),e(ga,Xk),e(Ee,Yk),e(Ee,oc),e(oc,Zk),e(Ee,eT),e(Ee,Pt),e(Pt,rc),e(rc,ba),e(ba,tT),e(Pt,sT),e(Pt,ac),e(ac,va),e(va,nT),e(Pt,oT),e(Pt,ic),e(ic,ka),e(ka,rT),e(Pt,aT),e(Pt,lc),e(lc,Ta),e(Ta,iT),e(Ee,lT),e(Ee,at),T(wa,at,null),e(at,dT),e(at,Ls),e(Ls,cT),e(Ls,dc),e(dc,pT),e(Ls,hT),e(Ls,cc),e(cc,mT),e(Ls,uT),e(at,fT),T(qn,at,null),e(at,gT),e(at,pc),e(pc,_T),e(at,bT),T(ya,at,null),m(s,nh,f),m(s,Ss,f),e(Ss,Pn),e(Pn,hc),T($a,hc,null),e(Ss,vT),e(Ss,mc),e(mc,kT),m(s,oh,f),m(s,xe,f),T(Fa,xe,null),e(xe,TT),e(xe,uc),e(uc,wT),e(xe,yT),e(xe,Da),e(Da,$T),e(Da,el),e(el,FT),e(Da,DT),e(xe,BT),e(xe,Ba),e(Ba,MT),e(Ba,Ma),e(Ma,ET),e(Ba,xT),e(xe,zT),e(xe,fc),e(fc,jT),e(xe,CT),e(xe,At),e(At,gc),e(gc,Ea),e(Ea,qT),e(At,PT),e(At,_c),e(_c,xa),e(xa,AT),e(At,OT),e(At,bc),e(bc,za),e(za,NT),e(At,LT),e(At,vc),e(vc,ja),e(ja,ST),e(xe,IT),e(xe,it),T(Ca,it,null),e(it,WT),e(it,Is),e(Is,RT),e(Is,kc),e(kc,UT),e(Is,QT),e(Is,Tc),e(Tc,HT),e(Is,KT),e(it,VT),T(An,it,null),e(it,JT),e(it,wc),e(wc,GT),e(it,XT),T(qa,it,null),m(s,rh,f),m(s,Ws,f),e(Ws,On),e(On,yc),T(Pa,yc,null),e(Ws,YT),e(Ws,$c),e($c,ZT),m(s,ah,f),m(s,ze,f),T(Aa,ze,null),e(ze,ew),e(ze,Fc),e(Fc,tw),e(ze,sw),e(ze,Oa),e(Oa,nw),e(Oa,tl),e(tl,ow),e(Oa,rw),e(ze,aw),e(ze,Na),e(Na,iw),e(Na,La),e(La,lw),e(Na,dw),e(ze,cw),e(ze,Dc),e(Dc,pw),e(ze,hw),e(ze,Ot),e(Ot,Bc),e(Bc,Sa),e(Sa,mw),e(Ot,uw),e(Ot,Mc),e(Mc,Ia),e(Ia,fw),e(Ot,gw),e(Ot,Ec),e(Ec,Wa),e(Wa,_w),e(Ot,bw),e(Ot,xc),e(xc,Ra),e(Ra,vw),e(ze,kw),e(ze,lt),T(Ua,lt,null),e(lt,Tw),e(lt,Rs),e(Rs,ww),e(Rs,zc),e(zc,yw),e(Rs,$w),e(Rs,jc),e(jc,Fw),e(Rs,Dw),e(lt,Bw),T(Nn,lt,null),e(lt,Mw),e(lt,Cc),e(Cc,Ew),e(lt,xw),T(Qa,lt,null),m(s,ih,f),m(s,Us,f),e(Us,Ln),e(Ln,qc),T(Ha,qc,null),e(Us,zw),e(Us,Pc),e(Pc,jw),m(s,lh,f),m(s,je,f),T(Ka,je,null),e(je,Cw),e(je,Ac),e(Ac,qw),e(je,Pw),e(je,Va),e(Va,Aw),e(Va,sl),e(sl,Ow),e(Va,Nw),e(je,Lw),e(je,Ja),e(Ja,Sw),e(Ja,Ga),e(Ga,Iw),e(Ja,Ww),e(je,Rw),e(je,Oc),e(Oc,Uw),e(je,Qw),e(je,Nt),e(Nt,Nc),e(Nc,Xa),e(Xa,Hw),e(Nt,Kw),e(Nt,Lc),e(Lc,Ya),e(Ya,Vw),e(Nt,Jw),e(Nt,Sc),e(Sc,Za),e(Za,Gw),e(Nt,Xw),e(Nt,Ic),e(Ic,ei),e(ei,Yw),e(je,Zw),e(je,dt),T(ti,dt,null),e(dt,ey),e(dt,Qs),e(Qs,ty),e(Qs,Wc),e(Wc,sy),e(Qs,ny),e(Qs,Rc),e(Rc,oy),e(Qs,ry),e(dt,ay),T(Sn,dt,null),e(dt,iy),e(dt,Uc),e(Uc,ly),e(dt,dy),T(si,dt,null),m(s,dh,f),m(s,Hs,f),e(Hs,In),e(In,Qc),T(ni,Qc,null),e(Hs,cy),e(Hs,Hc),e(Hc,py),m(s,ch,f),m(s,Ce,f),T(oi,Ce,null),e(Ce,hy),e(Ce,Ks),e(Ks,my),e(Ks,Kc),e(Kc,uy),e(Ks,fy),e(Ks,Vc),e(Vc,gy),e(Ks,_y),e(Ce,by),e(Ce,ri),e(ri,vy),e(ri,nl),e(nl,ky),e(ri,Ty),e(Ce,wy),e(Ce,ai),e(ai,yy),e(ai,ii),e(ii,$y),e(ai,Fy),e(Ce,Dy),e(Ce,Jc),e(Jc,By),e(Ce,My),e(Ce,Lt),e(Lt,Gc),e(Gc,li),e(li,Ey),e(Lt,xy),e(Lt,Xc),e(Xc,di),e(di,zy),e(Lt,jy),e(Lt,Yc),e(Yc,ci),e(ci,Cy),e(Lt,qy),e(Lt,Zc),e(Zc,pi),e(pi,Py),e(Ce,Ay),e(Ce,ct),T(hi,ct,null),e(ct,Oy),e(ct,Vs),e(Vs,Ny),e(Vs,ep),e(ep,Ly),e(Vs,Sy),e(Vs,tp),e(tp,Iy),e(Vs,Wy),e(ct,Ry),T(Wn,ct,null),e(ct,Uy),e(ct,sp),e(sp,Qy),e(ct,Hy),T(mi,ct,null),ph=!0},p(s,[f]){const ui={};f&2&&(ui.$$scope={dirty:f,ctx:s}),tn.$set(ui);const np={};f&2&&(np.$$scope={dirty:f,ctx:s}),nn.$set(np);const op={};f&2&&(op.$$scope={dirty:f,ctx:s}),rn.$set(op);const rp={};f&2&&(rp.$$scope={dirty:f,ctx:s}),ln.$set(rp);const fi={};f&2&&(fi.$$scope={dirty:f,ctx:s}),cn.$set(fi);const ap={};f&2&&(ap.$$scope={dirty:f,ctx:s}),hn.$set(ap);const ip={};f&2&&(ip.$$scope={dirty:f,ctx:s}),un.$set(ip);const lp={};f&2&&(lp.$$scope={dirty:f,ctx:s}),fn.$set(lp);const St={};f&2&&(St.$$scope={dirty:f,ctx:s}),_n.$set(St);const dp={};f&2&&(dp.$$scope={dirty:f,ctx:s}),bn.$set(dp);const cp={};f&2&&(cp.$$scope={dirty:f,ctx:s}),kn.$set(cp);const pp={};f&2&&(pp.$$scope={dirty:f,ctx:s}),Tn.$set(pp);const hp={};f&2&&(hp.$$scope={dirty:f,ctx:s}),yn.$set(hp);const mp={};f&2&&(mp.$$scope={dirty:f,ctx:s}),$n.$set(mp);const up={};f&2&&(up.$$scope={dirty:f,ctx:s}),Dn.$set(up);const fp={};f&2&&(fp.$$scope={dirty:f,ctx:s}),Bn.$set(fp);const gi={};f&2&&(gi.$$scope={dirty:f,ctx:s}),En.$set(gi);const It={};f&2&&(It.$$scope={dirty:f,ctx:s}),xn.$set(It);const gp={};f&2&&(gp.$$scope={dirty:f,ctx:s}),jn.$set(gp);const _p={};f&2&&(_p.$$scope={dirty:f,ctx:s}),qn.$set(_p);const bp={};f&2&&(bp.$$scope={dirty:f,ctx:s}),An.$set(bp);const _i={};f&2&&(_i.$$scope={dirty:f,ctx:s}),Nn.$set(_i);const vp={};f&2&&(vp.$$scope={dirty:f,ctx:s}),Sn.$set(vp);const Wt={};f&2&&(Wt.$$scope={dirty:f,ctx:s}),Wn.$set(Wt)},i(s){ph||(w(_.$$.fragment,s),w(X.$$.fragment,s),w(to.$$.fragment,s),w(so.$$.fragment,s),w(oo.$$.fragment,s),w(ro.$$.fragment,s),w(ao.$$.fragment,s),w(lo.$$.fragment,s),w(co.$$.fragment,s),w(mo.$$.fragment,s),w(uo.$$.fragment,s),w(bo.$$.fragment,s),w(tn.$$.fragment,s),w(vo.$$.fragment,s),w(ko.$$.fragment,s),w(To.$$.fragment,s),w(Do.$$.fragment,s),w(nn.$$.fragment,s),w(Bo.$$.fragment,s),w(Mo.$$.fragment,s),w(Eo.$$.fragment,s),w(xo.$$.fragment,s),w(qo.$$.fragment,s),w(rn.$$.fragment,s),w(Po.$$.fragment,s),w(Ao.$$.fragment,s),w(Oo.$$.fragment,s),w(No.$$.fragment,s),w(Lo.$$.fragment,s),w(So.$$.fragment,s),w(Uo.$$.fragment,s),w(ln.$$.fragment,s),w(Qo.$$.fragment,s),w(Ho.$$.fragment,s),w(Ko.$$.fragment,s),w(Xo.$$.fragment,s),w(cn.$$.fragment,s),w(Yo.$$.fragment,s),w(Zo.$$.fragment,s),w(er.$$.fragment,s),w(tr.$$.fragment,s),w(rr.$$.fragment,s),w(hn.$$.fragment,s),w(ar.$$.fragment,s),w(ir.$$.fragment,s),w(lr.$$.fragment,s),w(dr.$$.fragment,s),w(un.$$.fragment,s),w(mr.$$.fragment,s),w(fn.$$.fragment,s),w(ur.$$.fragment,s),w(fr.$$.fragment,s),w(gr.$$.fragment,s),w(_n.$$.fragment,s),w(Tr.$$.fragment,s),w(bn.$$.fragment,s),w(wr.$$.fragment,s),w(yr.$$.fragment,s),w($r.$$.fragment,s),w(Fr.$$.fragment,s),w(kn.$$.fragment,s),w(Er.$$.fragment,s),w(Tn.$$.fragment,s),w(xr.$$.fragment,s),w(zr.$$.fragment,s),w(jr.$$.fragment,s),w(Cr.$$.fragment,s),w(yn.$$.fragment,s),w(Or.$$.fragment,s),w($n.$$.fragment,s),w(Nr.$$.fragment,s),w(Lr.$$.fragment,s),w(Sr.$$.fragment,s),w(Dn.$$.fragment,s),w(Ur.$$.fragment,s),w(Bn.$$.fragment,s),w(Qr.$$.fragment,s),w(Hr.$$.fragment,s),w(Kr.$$.fragment,s),w(Vr.$$.fragment,s),w(En.$$.fragment,s),w(Yr.$$.fragment,s),w(xn.$$.fragment,s),w(Zr.$$.fragment,s),w(ea.$$.fragment,s),w(ta.$$.fragment,s),w(sa.$$.fragment,s),w(ca.$$.fragment,s),w(jn.$$.fragment,s),w(pa.$$.fragment,s),w(ha.$$.fragment,s),w(ma.$$.fragment,s),w(wa.$$.fragment,s),w(qn.$$.fragment,s),w(ya.$$.fragment,s),w($a.$$.fragment,s),w(Fa.$$.fragment,s),w(Ca.$$.fragment,s),w(An.$$.fragment,s),w(qa.$$.fragment,s),w(Pa.$$.fragment,s),w(Aa.$$.fragment,s),w(Ua.$$.fragment,s),w(Nn.$$.fragment,s),w(Qa.$$.fragment,s),w(Ha.$$.fragment,s),w(Ka.$$.fragment,s),w(ti.$$.fragment,s),w(Sn.$$.fragment,s),w(si.$$.fragment,s),w(ni.$$.fragment,s),w(oi.$$.fragment,s),w(hi.$$.fragment,s),w(Wn.$$.fragment,s),w(mi.$$.fragment,s),ph=!0)},o(s){y(_.$$.fragment,s),y(X.$$.fragment,s),y(to.$$.fragment,s),y(so.$$.fragment,s),y(oo.$$.fragment,s),y(ro.$$.fragment,s),y(ao.$$.fragment,s),y(lo.$$.fragment,s),y(co.$$.fragment,s),y(mo.$$.fragment,s),y(uo.$$.fragment,s),y(bo.$$.fragment,s),y(tn.$$.fragment,s),y(vo.$$.fragment,s),y(ko.$$.fragment,s),y(To.$$.fragment,s),y(Do.$$.fragment,s),y(nn.$$.fragment,s),y(Bo.$$.fragment,s),y(Mo.$$.fragment,s),y(Eo.$$.fragment,s),y(xo.$$.fragment,s),y(qo.$$.fragment,s),y(rn.$$.fragment,s),y(Po.$$.fragment,s),y(Ao.$$.fragment,s),y(Oo.$$.fragment,s),y(No.$$.fragment,s),y(Lo.$$.fragment,s),y(So.$$.fragment,s),y(Uo.$$.fragment,s),y(ln.$$.fragment,s),y(Qo.$$.fragment,s),y(Ho.$$.fragment,s),y(Ko.$$.fragment,s),y(Xo.$$.fragment,s),y(cn.$$.fragment,s),y(Yo.$$.fragment,s),y(Zo.$$.fragment,s),y(er.$$.fragment,s),y(tr.$$.fragment,s),y(rr.$$.fragment,s),y(hn.$$.fragment,s),y(ar.$$.fragment,s),y(ir.$$.fragment,s),y(lr.$$.fragment,s),y(dr.$$.fragment,s),y(un.$$.fragment,s),y(mr.$$.fragment,s),y(fn.$$.fragment,s),y(ur.$$.fragment,s),y(fr.$$.fragment,s),y(gr.$$.fragment,s),y(_n.$$.fragment,s),y(Tr.$$.fragment,s),y(bn.$$.fragment,s),y(wr.$$.fragment,s),y(yr.$$.fragment,s),y($r.$$.fragment,s),y(Fr.$$.fragment,s),y(kn.$$.fragment,s),y(Er.$$.fragment,s),y(Tn.$$.fragment,s),y(xr.$$.fragment,s),y(zr.$$.fragment,s),y(jr.$$.fragment,s),y(Cr.$$.fragment,s),y(yn.$$.fragment,s),y(Or.$$.fragment,s),y($n.$$.fragment,s),y(Nr.$$.fragment,s),y(Lr.$$.fragment,s),y(Sr.$$.fragment,s),y(Dn.$$.fragment,s),y(Ur.$$.fragment,s),y(Bn.$$.fragment,s),y(Qr.$$.fragment,s),y(Hr.$$.fragment,s),y(Kr.$$.fragment,s),y(Vr.$$.fragment,s),y(En.$$.fragment,s),y(Yr.$$.fragment,s),y(xn.$$.fragment,s),y(Zr.$$.fragment,s),y(ea.$$.fragment,s),y(ta.$$.fragment,s),y(sa.$$.fragment,s),y(ca.$$.fragment,s),y(jn.$$.fragment,s),y(pa.$$.fragment,s),y(ha.$$.fragment,s),y(ma.$$.fragment,s),y(wa.$$.fragment,s),y(qn.$$.fragment,s),y(ya.$$.fragment,s),y($a.$$.fragment,s),y(Fa.$$.fragment,s),y(Ca.$$.fragment,s),y(An.$$.fragment,s),y(qa.$$.fragment,s),y(Pa.$$.fragment,s),y(Aa.$$.fragment,s),y(Ua.$$.fragment,s),y(Nn.$$.fragment,s),y(Qa.$$.fragment,s),y(Ha.$$.fragment,s),y(Ka.$$.fragment,s),y(ti.$$.fragment,s),y(Sn.$$.fragment,s),y(si.$$.fragment,s),y(ni.$$.fragment,s),y(oi.$$.fragment,s),y(hi.$$.fragment,s),y(Wn.$$.fragment,s),y(mi.$$.fragment,s),ph=!1},d(s){t(p),s&&t(D),s&&t(g),$(_),s&&t(K),s&&t(E),$(X),s&&t(ae),s&&t(N),s&&t(ie),s&&t(ee),s&&t(le),s&&t(L),s&&t(q),s&&t(te),s&&t(de),s&&t(h),s&&t(wp),s&&t(vt),s&&t(yp),s&&t(as),$(to),s&&t($p),s&&t(Ve),$(so),$(oo),s&&t(Fp),s&&t(ls),$(ro),s&&t(Dp),s&&t(_t),$(ao),s&&t(Bp),s&&t(ds),$(lo),s&&t(Mp),s&&t(bt),$(co),s&&t(Ep),s&&t(cs),$(mo),s&&t(xp),s&&t(Je),$(uo),$(bo),$(tn),$(vo),s&&t(zp),s&&t(hs),$(ko),s&&t(jp),s&&t(Ge),$(To),$(Do),$(nn),$(Bo),$(Mo),s&&t(Cp),s&&t(us),$(Eo),s&&t(qp),s&&t(Xe),$(xo),$(qo),$(rn),$(Po),$(Ao),$(Oo),$(No),s&&t(Pp),s&&t(gs),$(Lo),s&&t(Ap),s&&t(Ye),$(So),$(Uo),$(ln),$(Qo),s&&t(Op),s&&t(bs),$(Ho),s&&t(Np),s&&t(Ze),$(Ko),$(Xo),$(cn),$(Yo),$(Zo),s&&t(Lp),s&&t(ks),$(er),s&&t(Sp),s&&t(et),$(tr),$(rr),$(hn),$(ar),$(ir),s&&t(Ip),s&&t(ys),$(lr),s&&t(Wp),s&&t(Pe),$(dr),$(un),$(mr),$(fn),$(ur),s&&t(Rp),s&&t(Fs),$(fr),s&&t(Up),s&&t(Ae),$(gr),$(_n),$(Tr),$(bn),$(wr),$(yr),s&&t(Qp),s&&t(Bs),$($r),s&&t(Hp),s&&t(Oe),$(Fr),$(kn),$(Er),$(Tn),$(xr),$(zr),s&&t(Kp),s&&t(Es),$(jr),s&&t(Vp),s&&t(Ne),$(Cr),$(yn),$(Or),$($n),$(Nr),s&&t(Jp),s&&t(zs),$(Lr),s&&t(Gp),s&&t(Le),$(Sr),$(Dn),$(Ur),$(Bn),$(Qr),$(Hr),s&&t(Xp),s&&t(Cs),$(Kr),s&&t(Yp),s&&t(Se),$(Vr),$(En),$(Yr),$(xn),$(Zr),$(ea),s&&t(Zp),s&&t(As),$(ta),s&&t(eh),s&&t(Me),$(sa),$(ca),$(jn),$(pa),s&&t(th),s&&t(Ns),$(ha),s&&t(sh),s&&t(Ee),$(ma),$(wa),$(qn),$(ya),s&&t(nh),s&&t(Ss),$($a),s&&t(oh),s&&t(xe),$(Fa),$(Ca),$(An),$(qa),s&&t(rh),s&&t(Ws),$(Pa),s&&t(ah),s&&t(ze),$(Aa),$(Ua),$(Nn),$(Qa),s&&t(ih),s&&t(Us),$(Ha),s&&t(lh),s&&t(je),$(Ka),$(ti),$(Sn),$(si),s&&t(dh),s&&t(Hs),$(ni),s&&t(ch),s&&t(Ce),$(oi),$(hi),$(Wn),$(mi)}}}const Z2={local:"distilbert",sections:[{local:"overview",title:"Overview"},{local:"transformers.DistilBertConfig",title:"DistilBertConfig"},{local:"transformers.DistilBertTokenizer",title:"DistilBertTokenizer"},{local:"transformers.DistilBertTokenizerFast",title:"DistilBertTokenizerFast"},{local:"transformers.DistilBertModel",title:"DistilBertModel"},{local:"transformers.DistilBertForMaskedLM",title:"DistilBertForMaskedLM"},{local:"transformers.DistilBertForSequenceClassification",title:"DistilBertForSequenceClassification"},{local:"transformers.DistilBertForMultipleChoice",title:"DistilBertForMultipleChoice"},{local:"transformers.DistilBertForTokenClassification",title:"DistilBertForTokenClassification"},{local:"transformers.DistilBertForQuestionAnswering",title:"DistilBertForQuestionAnswering"},{local:"transformers.TFDistilBertModel",title:"TFDistilBertModel"},{local:"transformers.TFDistilBertForMaskedLM",title:"TFDistilBertForMaskedLM"},{local:"transformers.TFDistilBertForSequenceClassification",title:"TFDistilBertForSequenceClassification"},{local:"transformers.TFDistilBertForMultipleChoice",title:"TFDistilBertForMultipleChoice"},{local:"transformers.TFDistilBertForTokenClassification",title:"TFDistilBertForTokenClassification"},{local:"transformers.TFDistilBertForQuestionAnswering",title:"TFDistilBertForQuestionAnswering"},{local:"transformers.FlaxDistilBertModel",title:"FlaxDistilBertModel"},{local:"transformers.FlaxDistilBertForMaskedLM",title:"FlaxDistilBertForMaskedLM"},{local:"transformers.FlaxDistilBertForSequenceClassification",title:"FlaxDistilBertForSequenceClassification"},{local:"transformers.FlaxDistilBertForMultipleChoice",title:"FlaxDistilBertForMultipleChoice"},{local:"transformers.FlaxDistilBertForTokenClassification",title:"FlaxDistilBertForTokenClassification"},{local:"transformers.FlaxDistilBertForQuestionAnswering",title:"FlaxDistilBertForQuestionAnswering"}],title:"DistilBERT"};function e0(j){return B2(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class a0 extends y2{constructor(p){super();$2(this,p,e0,Y2,F2,{})}}export{a0 as default,Z2 as metadata};
