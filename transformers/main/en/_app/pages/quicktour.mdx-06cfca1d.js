import{S as Yf,i as Gf,s as Qf,e as n,k as f,w as d,t as a,M as Jf,c as l,d as s,m as u,a as i,x as _,h as o,b as m,F as t,g as p,y as g,q as v,o as y,B as $}from"../chunks/vendor-4833417e.js";import{T as ba}from"../chunks/Tip-fffd6df1.js";import{Y as Bf}from"../chunks/Youtube-27813aed.js";import{I as Ne}from"../chunks/IconCopyLink-4b81c553.js";import{C as D}from"../chunks/CodeBlock-6a3d1b46.js";import{C as pe}from"../chunks/CodeBlockFw-27a176a0.js";import{D as Kf}from"../chunks/DocNotebookDropdown-ecff2a90.js";import"../chunks/CopyButton-dacfbfaf.js";function Vf(N){let h,k;return{c(){h=n("p"),k=a(`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`)},l(c){h=l(c,"P",{});var w=i(h);k=o(w,`All code examples presented in the documentation have a toggle on the top left for PyTorch and TensorFlow. If
not, the code is expected to work for both backends without any change.`),w.forEach(s)},m(c,w){p(c,h,w),t(h,k)},d(c){c&&s(h)}}}function Zf(N){let h,k,c,w,A,b,j,x;return{c(){h=n("p"),k=a("For more details about the "),c=n("a"),w=a("pipeline()"),A=a(" and associated tasks, refer to the documentation "),b=n("a"),j=a("here"),x=a("."),this.h()},l(S){h=l(S,"P",{});var E=i(h);k=o(E,"For more details about the "),c=l(E,"A",{href:!0});var O=i(c);w=o(O,"pipeline()"),O.forEach(s),A=o(E," and associated tasks, refer to the documentation "),b=l(E,"A",{href:!0});var R=i(b);j=o(R,"here"),R.forEach(s),x=o(E,"."),E.forEach(s),this.h()},h(){m(c,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(b,"href","./main_classes/pipelines")},m(S,E){p(S,h,E),t(h,k),t(h,c),t(c,w),t(h,A),t(h,b),t(b,j),t(h,x)},d(S){S&&s(h)}}}function Xf(N){let h,k,c,w,A,b,j,x;return{c(){h=n("p"),k=a("See the "),c=n("a"),w=a("task summary"),A=a(" for which "),b=n("a"),j=a("AutoModel"),x=a(" class to use for which task."),this.h()},l(S){h=l(S,"P",{});var E=i(h);k=o(E,"See the "),c=l(E,"A",{href:!0});var O=i(c);w=o(O,"task summary"),O.forEach(s),A=o(E," for which "),b=l(E,"A",{href:!0});var R=i(b);j=o(R,"AutoModel"),R.forEach(s),x=o(E," class to use for which task."),E.forEach(s),this.h()},h(){m(c,"href","./task_summary"),m(b,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModel")},m(S,E){p(S,h,E),t(h,k),t(h,c),t(c,w),t(h,A),t(h,b),t(b,j),t(h,x)},d(S){S&&s(h)}}}function eu(N){let h,k,c,w,A;return{c(){h=n("p"),k=a("All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=n("em"),w=a("before"),A=a(` the final activation
function (like softmax) because the final activation function is often fused with the loss.`)},l(b){h=l(b,"P",{});var j=i(h);k=o(j,"All \u{1F917} Transformers models (PyTorch or TensorFlow) outputs the tensors "),c=l(j,"EM",{});var x=i(c);w=o(x,"before"),x.forEach(s),A=o(j,` the final activation
function (like softmax) because the final activation function is often fused with the loss.`),j.forEach(s)},m(b,j){p(b,h,j),t(h,k),t(h,c),t(c,w),t(h,A)},d(b){b&&s(h)}}}function tu(N){let h,k,c,w,A;return{c(){h=n("p"),k=a(`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=n("code"),w=a("None"),A=a(" are ignored.")},l(b){h=l(b,"P",{});var j=i(h);k=o(j,`\u{1F917} Transformers model outputs are special dataclasses so their attributes are autocompleted in an IDE.
The model outputs also behave like a tuple or a dictionary (e.g., you can index with an integer, a slice or a string) in which case the attributes that are `),c=l(j,"CODE",{});var x=i(c);w=o(x,"None"),x.forEach(s),A=o(j," are ignored."),j.forEach(s)},m(b,j){p(b,h,j),t(h,k),t(h,c),t(c,w),t(h,A)},d(b){b&&s(h)}}}function su(N){let h,k,c,w,A,b,j,x,S,E,O,R,H,ur,Tt,mr,hr,xt,cr,dr,wa,fe,ka,te,ue,Ts,Oe,_r,xs,gr,ja,Le,qt,vr,yr,Ea,De,Aa,me,$r,zt,br,wr,Ta,Re,qs,kr,jr,xa,T,zs,Er,Ar,Fs,Tr,xr,Ps,qr,zr,Ss,Fr,Pr,Ms,Sr,Mr,Cs,Cr,Ir,Is,Nr,Or,Ns,Lr,qa,He,Os,Dr,Rr,za,U,Ls,Hr,Ur,Ds,Wr,Br,Rs,Yr,Fa,Ue,Hs,Gr,Qr,Pa,he,Us,Jr,Kr,Ws,Vr,Sa,ce,Ma,se,de,Bs,We,Zr,Ys,Xr,Ca,_e,en,Ft,tn,sn,Ia,Pt,an,Na,Be,Oa,ge,on,St,rn,nn,La,Ye,Da,W,ln,Ge,pn,fn,Gs,un,mn,Ra,Qe,Ha,ve,hn,Mt,cn,dn,Ua,Je,Wa,B,_n,Ct,gn,vn,Ke,yn,$n,Ba,Ve,Ya,ye,bn,It,wn,kn,Ga,Ze,Qa,Y,jn,Xe,En,An,et,Tn,xn,Ja,tt,Ka,Nt,qn,Va,st,Za,$e,zn,Ot,Fn,Pn,Xa,ae,be,Qs,at,Sn,Js,Mn,eo,M,Cn,Lt,In,Nn,ot,On,Ln,Dt,Dn,Rn,rt,Hn,Un,to,nt,so,G,Wn,Rt,Bn,Yn,Ks,Gn,Qn,ao,lt,oo,Q,Jn,Ht,Kn,Vn,Vs,Zn,Xn,ro,it,no,J,el,Ut,tl,sl,Wt,al,ol,lo,oe,we,Zs,pt,rl,Xs,nl,io,ft,po,q,ll,Bt,il,pl,Yt,fl,ul,Gt,ml,hl,Qt,cl,dl,ea,_l,gl,Jt,vl,yl,fo,K,$l,ta,bl,wl,Kt,kl,jl,uo,re,ke,sa,ut,El,aa,Al,mo,V,Tl,oa,xl,ql,Vt,zl,Fl,ho,je,Pl,Zt,Sl,Ml,co,mt,_o,Ee,Cl,ra,Il,Nl,go,Xt,Ol,vo,ht,yo,es,Ll,$o,Ae,ts,ss,Dl,Rl,Hl,as,os,Ul,Wl,bo,Te,Bl,rs,Yl,Gl,wo,ct,ko,xe,Ql,ns,Jl,Kl,jo,ne,qe,na,dt,Vl,la,Zl,Eo,F,Xl,ls,ei,ti,is,si,ai,ps,oi,ri,fs,ni,li,us,ii,pi,Ao,_t,To,ze,xo,Fe,fi,ia,ui,mi,qo,gt,zo,Z,hi,pa,ci,di,fa,_i,gi,Fo,vt,Po,Pe,So,z,vi,yt,ua,yi,$i,$t,ma,bi,wi,ms,ki,ji,ha,Ei,Ai,bt,Ti,xi,hs,qi,zi,Mo,Se,Co,le,Me,ca,wt,Fi,da,Pi,Io,Ce,Si,cs,Mi,Ci,No,kt,Oo,Ie,Ii,ds,Ni,Oi,Lo,jt,Do,X,Li,_a,Di,Ri,ga,Hi,Ui,Ro,Et,Ho;return b=new Ne({}),O=new Kf({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/transformers_doc/tensorflow/quicktour.ipynb"}]}}),fe=new ba({props:{$$slots:{default:[Vf]},$$scope:{ctx:N}}}),Oe=new Ne({}),De=new Bf({props:{id:"tiZFewofSLM"}}),ce=new ba({props:{$$slots:{default:[Zf]},$$scope:{ctx:N}}}),We=new Ne({}),Be=new pe({props:{group1:{id:"pt",code:"pip install torch",highlighted:"pip install torch"},group2:{id:"tf",code:"pip install tensorflow",highlighted:"pip install tensorflow"}}}),Ye=new D({props:{code:`from transformers import pipeline

classifier = pipeline("sentiment-analysis")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`}}),Qe=new D({props:{code:'classifier("We are very happy to show you the \u{1F917} Transformers library.")',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`}}),Je=new D({props:{code:`results = classifier(["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."])
for result in results:
    print(f"label: {result['label']}, with score: {round(result['score'], 4)}")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`}}),Ve=new D({props:{code:"pip install datasets ",highlighted:"pip install datasets "}}),Ze=new D({props:{code:`import torch
from transformers import pipeline

speech_recognizer = pipeline("automatic-speech-recognition", model="facebook/wav2vec2-base-960h")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`}}),tt=new D({props:{code:`import datasets

dataset = datasets.load_dataset("superb", name="asr", split="test")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> datasets

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = datasets.load_dataset(<span class="hljs-string">&quot;superb&quot;</span>, name=<span class="hljs-string">&quot;asr&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)`}}),st=new D({props:{code:`files = dataset["file"]
speech_recognizer(files[:4])`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>files = dataset[<span class="hljs-string">&quot;file&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer(files[:<span class="hljs-number">4</span>])
[{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FAT AND SAUCE&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;STUFFERED INTO YOU HIS BELLY COUNSELLED HIM&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;AFTER EARLY NIGHTFALL THE YELLOW LAMPS WOULD LIGHT UP HERE AND THERE THE SQUALID QUARTER OF THE BROTHELS&#x27;</span>},
 {<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;HO BERTIE ANY GOOD IN YOUR MIND&#x27;</span>}]`}}),at=new Ne({}),nt=new D({props:{code:'model_name = "nlptown/bert-base-multilingual-uncased-sentiment"',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>'}}),lt=new pe({props:{group1:{id:"pt",code:`from transformers import AutoTokenizer, AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`},group2:{id:"tf",code:`from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}}),it=new D({props:{code:`classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
classifier("Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr\xE8s heureux de vous pr\xE9senter la biblioth\xE8que \u{1F917} Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`}}),pt=new Ne({}),ft=new Bf({props:{id:"AhChOFRegn4"}}),ut=new Ne({}),mt=new D({props:{code:`from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`}}),ht=new D({props:{code:`encoding = tokenizer("We are very happy to show you the \u{1F917} Transformers library.")
print(encoding)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`}}),ct=new pe({props:{group1:{id:"pt",code:`pt_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`},group2:{id:"tf",code:`tf_batch = tokenizer(
    ["We are very happy to show you the \u{1F917} Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="tf",
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the \u{1F917} Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`}}}),dt=new Ne({}),_t=new pe({props:{group1:{id:"pt",code:`from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`},group2:{id:"tf",code:`from transformers import TFAutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`}}}),ze=new ba({props:{$$slots:{default:[Xf]},$$scope:{ctx:N}}}),gt=new pe({props:{group1:{id:"pt",code:"pt_outputs = pt_model(**pt_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)'},group2:{id:"tf",code:"tf_outputs = tf_model(tf_batch)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)'}}}),vt=new pe({props:{group1:{id:"pt",code:`from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)
print(pt_predictions)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`},group2:{id:"tf",code:`import tensorflow as tf

tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)
print(tf.math.round(tf_predictions * 10**4) / 10**4)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tf.math.<span class="hljs-built_in">round</span>(tf_predictions * <span class="hljs-number">10</span>**<span class="hljs-number">4</span>) / <span class="hljs-number">10</span>**<span class="hljs-number">4</span>)
tf.Tensor(
[[<span class="hljs-number">0.0021</span> <span class="hljs-number">0.0018</span> <span class="hljs-number">0.0116</span> <span class="hljs-number">0.2121</span> <span class="hljs-number">0.7725</span>]
 [<span class="hljs-number">0.2084</span> <span class="hljs-number">0.1826</span> <span class="hljs-number">0.1969</span> <span class="hljs-number">0.1755</span>  <span class="hljs-number">0.2365</span>]], shape=(<span class="hljs-number">2</span>, <span class="hljs-number">5</span>), dtype=float32)`}}}),Pe=new ba({props:{$$slots:{default:[eu]},$$scope:{ctx:N}}}),Se=new ba({props:{$$slots:{default:[tu]},$$scope:{ctx:N}}}),wt=new Ne({}),kt=new pe({props:{group1:{id:"pt",code:`pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`},group2:{id:"tf",code:`tf_save_directory = "./tf_save_pretrained"
tokenizer.save_pretrained(tf_save_directory)
tf_model.save_pretrained(tf_save_directory)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`}}}),jt=new pe({props:{group1:{id:"pt",code:'pt_model = AutoModelForSequenceClassification.from_pretrained("./pt_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)'},group2:{id:"tf",code:'tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")',highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)'}}}),Et=new pe({props:{group1:{id:"pt",code:`from transformers import AutoModel

tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`},group2:{id:"tf",code:`from transformers import TFAutoModel

tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`}}}),{c(){h=n("meta"),k=f(),c=n("h1"),w=n("a"),A=n("span"),d(b.$$.fragment),j=f(),x=n("span"),S=a("Quick tour"),E=f(),d(O.$$.fragment),R=f(),H=n("p"),ur=a("Get up and running with \u{1F917} Transformers! Start using the "),Tt=n("a"),mr=a("pipeline()"),hr=a(" for rapid inference, and quickly load a pretrained model and tokenizer with an "),xt=n("a"),cr=a("AutoClass"),dr=a(" to solve your text, vision or audio task."),wa=f(),d(fe.$$.fragment),ka=f(),te=n("h2"),ue=n("a"),Ts=n("span"),d(Oe.$$.fragment),_r=f(),xs=n("span"),gr=a("Pipeline"),ja=f(),Le=n("p"),qt=n("a"),vr=a("pipeline()"),yr=a(" is the easiest way to use a pretrained model for a given task."),Ea=f(),d(De.$$.fragment),Aa=f(),me=n("p"),$r=a("The "),zt=n("a"),br=a("pipeline()"),wr=a(" supports many common tasks out-of-the-box:"),Ta=f(),Re=n("p"),qs=n("strong"),kr=a("Text"),jr=a(":"),xa=f(),T=n("ul"),zs=n("li"),Er=a("Sentiment analysis: classify the polarity of a given text."),Ar=f(),Fs=n("li"),Tr=a("Text generation (in English): generate text from a given input."),xr=f(),Ps=n("li"),qr=a("Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),zr=f(),Ss=n("li"),Fr=a("Question answering: extract the answer from the context, given some context and a question."),Pr=f(),Ms=n("li"),Sr=a("Fill-mask: fill in the blank given a text with masked words."),Mr=f(),Cs=n("li"),Cr=a("Summarization: generate a summary of a long sequence of text or document."),Ir=f(),Is=n("li"),Nr=a("Translation: translate text into another language."),Or=f(),Ns=n("li"),Lr=a("Feature extraction: create a tensor representation of the text."),qa=f(),He=n("p"),Os=n("strong"),Dr=a("Image"),Rr=a(":"),za=f(),U=n("ul"),Ls=n("li"),Hr=a("Image classification: classify an image."),Ur=f(),Ds=n("li"),Wr=a("Image segmentation: classify every pixel in an image."),Br=f(),Rs=n("li"),Yr=a("Object detection: detect objects within an image."),Fa=f(),Ue=n("p"),Hs=n("strong"),Gr=a("Audio"),Qr=a(":"),Pa=f(),he=n("ul"),Us=n("li"),Jr=a("Audio classification: assign a label to a given segment of audio."),Kr=f(),Ws=n("li"),Vr=a("Automatic speech recognition (ASR): transcribe audio data into text."),Sa=f(),d(ce.$$.fragment),Ma=f(),se=n("h3"),de=n("a"),Bs=n("span"),d(We.$$.fragment),Zr=f(),Ys=n("span"),Xr=a("Pipeline usage"),Ca=f(),_e=n("p"),en=a("In the following example, you will use the "),Ft=n("a"),tn=a("pipeline()"),sn=a(" for sentiment analysis."),Ia=f(),Pt=n("p"),an=a("Install the following dependencies if you haven\u2019t already:"),Na=f(),d(Be.$$.fragment),Oa=f(),ge=n("p"),on=a("Import "),St=n("a"),rn=a("pipeline()"),nn=a(" and specify the task you want to complete:"),La=f(),d(Ye.$$.fragment),Da=f(),W=n("p"),ln=a("The pipeline downloads and caches a default "),Ge=n("a"),pn=a("pretrained model"),fn=a(" and tokenizer for sentiment analysis. Now you can use the "),Gs=n("code"),un=a("classifier"),mn=a(" on your target text:"),Ra=f(),d(Qe.$$.fragment),Ha=f(),ve=n("p"),hn=a("For more than one sentence, pass a list of sentences to the "),Mt=n("a"),cn=a("pipeline()"),dn=a(" which returns a list of dictionaries:"),Ua=f(),d(Je.$$.fragment),Wa=f(),B=n("p"),_n=a("The "),Ct=n("a"),gn=a("pipeline()"),vn=a(" can also iterate over an entire dataset. Start by installing the "),Ke=n("a"),yn=a("\u{1F917} Datasets"),$n=a(" library:"),Ba=f(),d(Ve.$$.fragment),Ya=f(),ye=n("p"),bn=a("Create a "),It=n("a"),wn=a("pipeline()"),kn=a(" with the task you want to solve for and the model you want to use."),Ga=f(),d(Ze.$$.fragment),Qa=f(),Y=n("p"),jn=a("Next, load a dataset (see the \u{1F917} Datasets "),Xe=n("a"),En=a("Quick Start"),An=a(" for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),et=n("a"),Tn=a("SUPERB"),xn=a(" dataset:"),Ja=f(),d(tt.$$.fragment),Ka=f(),Nt=n("p"),qn=a("You can pass a whole dataset pipeline:"),Va=f(),d(st.$$.fragment),Za=f(),$e=n("p"),zn=a("For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Ot=n("a"),Fn=a("pipeline documentation"),Pn=a(" for more information."),Xa=f(),ae=n("h3"),be=n("a"),Qs=n("span"),d(at.$$.fragment),Sn=f(),Js=n("span"),Mn=a("Use another model and tokenizer in the pipeline"),eo=f(),M=n("p"),Cn=a("The "),Lt=n("a"),In=a("pipeline()"),Nn=a(" can accommodate any model from the "),ot=n("a"),On=a("Model Hub"),Ln=a(", making it easy to adapt the "),Dt=n("a"),Dn=a("pipeline()"),Rn=a(" for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),rt=n("a"),Hn=a("BERT model"),Un=a(" fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),to=f(),d(nt.$$.fragment),so=f(),G=n("p"),Wn=a("Use the "),Rt=n("a"),Bn=a("AutoModelForSequenceClassification"),Yn=a(" and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Ks=n("code"),Gn=a("AutoClass"),Qn=a(" below):"),ao=f(),d(lt.$$.fragment),oo=f(),Q=n("p"),Jn=a("Then you can specify the model and tokenizer in the "),Ht=n("a"),Kn=a("pipeline()"),Vn=a(", and apply the "),Vs=n("code"),Zn=a("classifier"),Xn=a(" on your target text:"),ro=f(),d(it.$$.fragment),no=f(),J=n("p"),el=a("If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Ut=n("a"),tl=a("fine-tuning tutorial"),sl=a(" to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Wt=n("a"),al=a("here"),ol=a(") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),lo=f(),oe=n("h2"),we=n("a"),Zs=n("span"),d(pt.$$.fragment),rl=f(),Xs=n("span"),nl=a("AutoClass"),io=f(),d(ft.$$.fragment),po=f(),q=n("p"),ll=a("Under the hood, the "),Bt=n("a"),il=a("AutoModelForSequenceClassification"),pl=a(" and "),Yt=n("a"),fl=a("AutoTokenizer"),ul=a(" classes work together to power the "),Gt=n("a"),ml=a("pipeline()"),hl=a(". An "),Qt=n("a"),cl=a("AutoClass"),dl=a(" is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),ea=n("code"),_l=a("AutoClass"),gl=a(" for your task and it\u2019s associated tokenizer with "),Jt=n("a"),vl=a("AutoTokenizer"),yl=a("."),fo=f(),K=n("p"),$l=a("Let\u2019s return to our example and see how you can use the "),ta=n("code"),bl=a("AutoClass"),wl=a(" to replicate the results of the "),Kt=n("a"),kl=a("pipeline()"),jl=a("."),uo=f(),re=n("h3"),ke=n("a"),sa=n("span"),d(ut.$$.fragment),El=f(),aa=n("span"),Al=a("AutoTokenizer"),mo=f(),V=n("p"),Tl=a("A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),oa=n("em"),xl=a("tokens"),ql=a(". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Vt=n("a"),zl=a("here"),Fl=a("). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),ho=f(),je=n("p"),Pl=a("Load a tokenizer with "),Zt=n("a"),Sl=a("AutoTokenizer"),Ml=a(":"),co=f(),d(mt.$$.fragment),_o=f(),Ee=n("p"),Cl=a("Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ra=n("em"),Il=a("vocabulary"),Nl=a("."),go=f(),Xt=n("p"),Ol=a("Pass your text to the tokenizer:"),vo=f(),d(ht.$$.fragment),yo=f(),es=n("p"),Ll=a("The tokenizer will return a dictionary containing:"),$o=f(),Ae=n("ul"),ts=n("li"),ss=n("a"),Dl=a("input_ids"),Rl=a(": numerical representions of your tokens."),Hl=f(),as=n("li"),os=n("a"),Ul=a("atttention_mask"),Wl=a(": indicates which tokens should be attended to."),bo=f(),Te=n("p"),Bl=a("Just like the "),rs=n("a"),Yl=a("pipeline()"),Gl=a(", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),wo=f(),d(ct.$$.fragment),ko=f(),xe=n("p"),Ql=a("Read the "),ns=n("a"),Jl=a("preprocessing"),Kl=a(" tutorial for more details about tokenization."),jo=f(),ne=n("h3"),qe=n("a"),na=n("span"),d(dt.$$.fragment),Vl=f(),la=n("span"),Zl=a("AutoModel"),Eo=f(),F=n("p"),Xl=a("\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),ls=n("a"),ei=a("AutoModel"),ti=a(" like you would load an "),is=n("a"),si=a("AutoTokenizer"),ai=a(". The only difference is selecting the correct "),ps=n("a"),oi=a("AutoModel"),ri=a(" for the task. Since you are doing text - or sequence - classification, load "),fs=n("a"),ni=a("AutoModelForSequenceClassification"),li=a(". The TensorFlow equivalent is simply "),us=n("a"),ii=a("TFAutoModelForSequenceClassification"),pi=a(":"),Ao=f(),d(_t.$$.fragment),To=f(),d(ze.$$.fragment),xo=f(),Fe=n("p"),fi=a("Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),ia=n("code"),ui=a("**"),mi=a(". For TensorFlow models, pass the dictionary keys directly to the tensors:"),qo=f(),d(gt.$$.fragment),zo=f(),Z=n("p"),hi=a("The model outputs the final activations in the "),pa=n("code"),ci=a("logits"),di=a(" attribute. Apply the softmax function to the "),fa=n("code"),_i=a("logits"),gi=a(" to retrieve the probabilities:"),Fo=f(),d(vt.$$.fragment),Po=f(),d(Pe.$$.fragment),So=f(),z=n("p"),vi=a("Models are a standard "),yt=n("a"),ua=n("code"),yi=a("torch.nn.Module"),$i=a(" or a "),$t=n("a"),ma=n("code"),bi=a("tf.keras.Model"),wi=a(" so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),ms=n("a"),ki=a("Trainer"),ji=a(" class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),ha=n("code"),Ei=a("fit"),Ai=a(" method from "),bt=n("a"),Ti=a("Keras"),xi=a(". Refer to the "),hs=n("a"),qi=a("training tutorial"),zi=a(" for more details."),Mo=f(),d(Se.$$.fragment),Co=f(),le=n("h3"),Me=n("a"),ca=n("span"),d(wt.$$.fragment),Fi=f(),da=n("span"),Pi=a("Save a model"),Io=f(),Ce=n("p"),Si=a("Once your model is fine-tuned, you can save it with its tokenizer using "),cs=n("a"),Mi=a("PreTrainedModel.save_pretrained()"),Ci=a(":"),No=f(),d(kt.$$.fragment),Oo=f(),Ie=n("p"),Ii=a("When you are ready to use the model again, reload it with "),ds=n("a"),Ni=a("PreTrainedModel.from_pretrained()"),Oi=a(":"),Lo=f(),d(jt.$$.fragment),Do=f(),X=n("p"),Li=a("One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),_a=n("code"),Di=a("from_pt"),Ri=a(" or "),ga=n("code"),Hi=a("from_tf"),Ui=a(" parameter can convert the model from one framework to the other:"),Ro=f(),d(Et.$$.fragment),this.h()},l(e){const r=Jf('[data-svelte="svelte-1phssyn"]',document.head);h=l(r,"META",{name:!0,content:!0}),r.forEach(s),k=u(e),c=l(e,"H1",{class:!0});var At=i(c);w=l(At,"A",{id:!0,class:!0,href:!0});var va=i(w);A=l(va,"SPAN",{});var ya=i(A);_(b.$$.fragment,ya),ya.forEach(s),va.forEach(s),j=u(At),x=l(At,"SPAN",{});var $a=i(x);S=o($a,"Quick tour"),$a.forEach(s),At.forEach(s),E=u(e),_(O.$$.fragment,e),R=u(e),H=l(e,"P",{});var ie=i(H);ur=o(ie,"Get up and running with \u{1F917} Transformers! Start using the "),Tt=l(ie,"A",{href:!0});var Ki=i(Tt);mr=o(Ki,"pipeline()"),Ki.forEach(s),hr=o(ie," for rapid inference, and quickly load a pretrained model and tokenizer with an "),xt=l(ie,"A",{href:!0});var Vi=i(xt);cr=o(Vi,"AutoClass"),Vi.forEach(s),dr=o(ie," to solve your text, vision or audio task."),ie.forEach(s),wa=u(e),_(fe.$$.fragment,e),ka=u(e),te=l(e,"H2",{class:!0});var Uo=i(te);ue=l(Uo,"A",{id:!0,class:!0,href:!0});var Zi=i(ue);Ts=l(Zi,"SPAN",{});var Xi=i(Ts);_(Oe.$$.fragment,Xi),Xi.forEach(s),Zi.forEach(s),_r=u(Uo),xs=l(Uo,"SPAN",{});var ep=i(xs);gr=o(ep,"Pipeline"),ep.forEach(s),Uo.forEach(s),ja=u(e),Le=l(e,"P",{});var Wi=i(Le);qt=l(Wi,"A",{href:!0});var tp=i(qt);vr=o(tp,"pipeline()"),tp.forEach(s),yr=o(Wi," is the easiest way to use a pretrained model for a given task."),Wi.forEach(s),Ea=u(e),_(De.$$.fragment,e),Aa=u(e),me=l(e,"P",{});var Wo=i(me);$r=o(Wo,"The "),zt=l(Wo,"A",{href:!0});var sp=i(zt);br=o(sp,"pipeline()"),sp.forEach(s),wr=o(Wo," supports many common tasks out-of-the-box:"),Wo.forEach(s),Ta=u(e),Re=l(e,"P",{});var Bi=i(Re);qs=l(Bi,"STRONG",{});var ap=i(qs);kr=o(ap,"Text"),ap.forEach(s),jr=o(Bi,":"),Bi.forEach(s),xa=u(e),T=l(e,"UL",{});var P=i(T);zs=l(P,"LI",{});var op=i(zs);Er=o(op,"Sentiment analysis: classify the polarity of a given text."),op.forEach(s),Ar=u(P),Fs=l(P,"LI",{});var rp=i(Fs);Tr=o(rp,"Text generation (in English): generate text from a given input."),rp.forEach(s),xr=u(P),Ps=l(P,"LI",{});var np=i(Ps);qr=o(np,"Name entity recognition (NER): label each word with the entity it represents (person, date, location, etc.)."),np.forEach(s),zr=u(P),Ss=l(P,"LI",{});var lp=i(Ss);Fr=o(lp,"Question answering: extract the answer from the context, given some context and a question."),lp.forEach(s),Pr=u(P),Ms=l(P,"LI",{});var ip=i(Ms);Sr=o(ip,"Fill-mask: fill in the blank given a text with masked words."),ip.forEach(s),Mr=u(P),Cs=l(P,"LI",{});var pp=i(Cs);Cr=o(pp,"Summarization: generate a summary of a long sequence of text or document."),pp.forEach(s),Ir=u(P),Is=l(P,"LI",{});var fp=i(Is);Nr=o(fp,"Translation: translate text into another language."),fp.forEach(s),Or=u(P),Ns=l(P,"LI",{});var up=i(Ns);Lr=o(up,"Feature extraction: create a tensor representation of the text."),up.forEach(s),P.forEach(s),qa=u(e),He=l(e,"P",{});var Yi=i(He);Os=l(Yi,"STRONG",{});var mp=i(Os);Dr=o(mp,"Image"),mp.forEach(s),Rr=o(Yi,":"),Yi.forEach(s),za=u(e),U=l(e,"UL",{});var _s=i(U);Ls=l(_s,"LI",{});var hp=i(Ls);Hr=o(hp,"Image classification: classify an image."),hp.forEach(s),Ur=u(_s),Ds=l(_s,"LI",{});var cp=i(Ds);Wr=o(cp,"Image segmentation: classify every pixel in an image."),cp.forEach(s),Br=u(_s),Rs=l(_s,"LI",{});var dp=i(Rs);Yr=o(dp,"Object detection: detect objects within an image."),dp.forEach(s),_s.forEach(s),Fa=u(e),Ue=l(e,"P",{});var Gi=i(Ue);Hs=l(Gi,"STRONG",{});var _p=i(Hs);Gr=o(_p,"Audio"),_p.forEach(s),Qr=o(Gi,":"),Gi.forEach(s),Pa=u(e),he=l(e,"UL",{});var Bo=i(he);Us=l(Bo,"LI",{});var gp=i(Us);Jr=o(gp,"Audio classification: assign a label to a given segment of audio."),gp.forEach(s),Kr=u(Bo),Ws=l(Bo,"LI",{});var vp=i(Ws);Vr=o(vp,"Automatic speech recognition (ASR): transcribe audio data into text."),vp.forEach(s),Bo.forEach(s),Sa=u(e),_(ce.$$.fragment,e),Ma=u(e),se=l(e,"H3",{class:!0});var Yo=i(se);de=l(Yo,"A",{id:!0,class:!0,href:!0});var yp=i(de);Bs=l(yp,"SPAN",{});var $p=i(Bs);_(We.$$.fragment,$p),$p.forEach(s),yp.forEach(s),Zr=u(Yo),Ys=l(Yo,"SPAN",{});var bp=i(Ys);Xr=o(bp,"Pipeline usage"),bp.forEach(s),Yo.forEach(s),Ca=u(e),_e=l(e,"P",{});var Go=i(_e);en=o(Go,"In the following example, you will use the "),Ft=l(Go,"A",{href:!0});var wp=i(Ft);tn=o(wp,"pipeline()"),wp.forEach(s),sn=o(Go," for sentiment analysis."),Go.forEach(s),Ia=u(e),Pt=l(e,"P",{});var kp=i(Pt);an=o(kp,"Install the following dependencies if you haven\u2019t already:"),kp.forEach(s),Na=u(e),_(Be.$$.fragment,e),Oa=u(e),ge=l(e,"P",{});var Qo=i(ge);on=o(Qo,"Import "),St=l(Qo,"A",{href:!0});var jp=i(St);rn=o(jp,"pipeline()"),jp.forEach(s),nn=o(Qo," and specify the task you want to complete:"),Qo.forEach(s),La=u(e),_(Ye.$$.fragment,e),Da=u(e),W=l(e,"P",{});var gs=i(W);ln=o(gs,"The pipeline downloads and caches a default "),Ge=l(gs,"A",{href:!0,rel:!0});var Ep=i(Ge);pn=o(Ep,"pretrained model"),Ep.forEach(s),fn=o(gs," and tokenizer for sentiment analysis. Now you can use the "),Gs=l(gs,"CODE",{});var Ap=i(Gs);un=o(Ap,"classifier"),Ap.forEach(s),mn=o(gs," on your target text:"),gs.forEach(s),Ra=u(e),_(Qe.$$.fragment,e),Ha=u(e),ve=l(e,"P",{});var Jo=i(ve);hn=o(Jo,"For more than one sentence, pass a list of sentences to the "),Mt=l(Jo,"A",{href:!0});var Tp=i(Mt);cn=o(Tp,"pipeline()"),Tp.forEach(s),dn=o(Jo," which returns a list of dictionaries:"),Jo.forEach(s),Ua=u(e),_(Je.$$.fragment,e),Wa=u(e),B=l(e,"P",{});var vs=i(B);_n=o(vs,"The "),Ct=l(vs,"A",{href:!0});var xp=i(Ct);gn=o(xp,"pipeline()"),xp.forEach(s),vn=o(vs," can also iterate over an entire dataset. Start by installing the "),Ke=l(vs,"A",{href:!0,rel:!0});var qp=i(Ke);yn=o(qp,"\u{1F917} Datasets"),qp.forEach(s),$n=o(vs," library:"),vs.forEach(s),Ba=u(e),_(Ve.$$.fragment,e),Ya=u(e),ye=l(e,"P",{});var Ko=i(ye);bn=o(Ko,"Create a "),It=l(Ko,"A",{href:!0});var zp=i(It);wn=o(zp,"pipeline()"),zp.forEach(s),kn=o(Ko," with the task you want to solve for and the model you want to use."),Ko.forEach(s),Ga=u(e),_(Ze.$$.fragment,e),Qa=u(e),Y=l(e,"P",{});var ys=i(Y);jn=o(ys,"Next, load a dataset (see the \u{1F917} Datasets "),Xe=l(ys,"A",{href:!0,rel:!0});var Fp=i(Xe);En=o(Fp,"Quick Start"),Fp.forEach(s),An=o(ys," for more details) you\u2019d like to iterate over. For example, let\u2019s load the "),et=l(ys,"A",{href:!0,rel:!0});var Pp=i(et);Tn=o(Pp,"SUPERB"),Pp.forEach(s),xn=o(ys," dataset:"),ys.forEach(s),Ja=u(e),_(tt.$$.fragment,e),Ka=u(e),Nt=l(e,"P",{});var Sp=i(Nt);qn=o(Sp,"You can pass a whole dataset pipeline:"),Sp.forEach(s),Va=u(e),_(st.$$.fragment,e),Za=u(e),$e=l(e,"P",{});var Vo=i($e);zn=o(Vo,"For a larger dataset where the inputs are big (like in speech or vision), you will want to pass along a generator instead of a list that loads all the inputs in memory. See the "),Ot=l(Vo,"A",{href:!0});var Mp=i(Ot);Fn=o(Mp,"pipeline documentation"),Mp.forEach(s),Pn=o(Vo," for more information."),Vo.forEach(s),Xa=u(e),ae=l(e,"H3",{class:!0});var Zo=i(ae);be=l(Zo,"A",{id:!0,class:!0,href:!0});var Cp=i(be);Qs=l(Cp,"SPAN",{});var Ip=i(Qs);_(at.$$.fragment,Ip),Ip.forEach(s),Cp.forEach(s),Sn=u(Zo),Js=l(Zo,"SPAN",{});var Np=i(Js);Mn=o(Np,"Use another model and tokenizer in the pipeline"),Np.forEach(s),Zo.forEach(s),eo=u(e),M=l(e,"P",{});var ee=i(M);Cn=o(ee,"The "),Lt=l(ee,"A",{href:!0});var Op=i(Lt);In=o(Op,"pipeline()"),Op.forEach(s),Nn=o(ee," can accommodate any model from the "),ot=l(ee,"A",{href:!0,rel:!0});var Lp=i(ot);On=o(Lp,"Model Hub"),Lp.forEach(s),Ln=o(ee,", making it easy to adapt the "),Dt=l(ee,"A",{href:!0});var Dp=i(Dt);Dn=o(Dp,"pipeline()"),Dp.forEach(s),Rn=o(ee," for other use-cases. For example, if you\u2019d like a model capable of handling French text, use the tags on the Model Hub to filter for an appropriate model. The top filtered result returns a multilingual "),rt=l(ee,"A",{href:!0,rel:!0});var Rp=i(rt);Hn=o(Rp,"BERT model"),Rp.forEach(s),Un=o(ee," fine-tuned for sentiment analysis. Great, let\u2019s use this model!"),ee.forEach(s),to=u(e),_(nt.$$.fragment,e),so=u(e),G=l(e,"P",{});var $s=i(G);Wn=o($s,"Use the "),Rt=l($s,"A",{href:!0});var Hp=i(Rt);Bn=o(Hp,"AutoModelForSequenceClassification"),Hp.forEach(s),Yn=o($s," and [\u2018AutoTokenizer\u2019] to load the pretrained model and it\u2019s associated tokenizer (more on an "),Ks=l($s,"CODE",{});var Up=i(Ks);Gn=o(Up,"AutoClass"),Up.forEach(s),Qn=o($s," below):"),$s.forEach(s),ao=u(e),_(lt.$$.fragment,e),oo=u(e),Q=l(e,"P",{});var bs=i(Q);Jn=o(bs,"Then you can specify the model and tokenizer in the "),Ht=l(bs,"A",{href:!0});var Wp=i(Ht);Kn=o(Wp,"pipeline()"),Wp.forEach(s),Vn=o(bs,", and apply the "),Vs=l(bs,"CODE",{});var Bp=i(Vs);Zn=o(Bp,"classifier"),Bp.forEach(s),Xn=o(bs," on your target text:"),bs.forEach(s),ro=u(e),_(it.$$.fragment,e),no=u(e),J=l(e,"P",{});var ws=i(J);el=o(ws,"If you can\u2019t find a model for your use-case, you will need to fine-tune a pretrained model on your data. Take a look at our "),Ut=l(ws,"A",{href:!0});var Yp=i(Ut);tl=o(Yp,"fine-tuning tutorial"),Yp.forEach(s),sl=o(ws," to learn how. Finally, after you\u2019ve fine-tuned your pretrained model, please consider sharing it (see tutorial "),Wt=l(ws,"A",{href:!0});var Gp=i(Wt);al=o(Gp,"here"),Gp.forEach(s),ol=o(ws,") with the community on the Model Hub to democratize NLP for everyone! \u{1F917}"),ws.forEach(s),lo=u(e),oe=l(e,"H2",{class:!0});var Xo=i(oe);we=l(Xo,"A",{id:!0,class:!0,href:!0});var Qp=i(we);Zs=l(Qp,"SPAN",{});var Jp=i(Zs);_(pt.$$.fragment,Jp),Jp.forEach(s),Qp.forEach(s),rl=u(Xo),Xs=l(Xo,"SPAN",{});var Kp=i(Xs);nl=o(Kp,"AutoClass"),Kp.forEach(s),Xo.forEach(s),io=u(e),_(ft.$$.fragment,e),po=u(e),q=l(e,"P",{});var C=i(q);ll=o(C,"Under the hood, the "),Bt=l(C,"A",{href:!0});var Vp=i(Bt);il=o(Vp,"AutoModelForSequenceClassification"),Vp.forEach(s),pl=o(C," and "),Yt=l(C,"A",{href:!0});var Zp=i(Yt);fl=o(Zp,"AutoTokenizer"),Zp.forEach(s),ul=o(C," classes work together to power the "),Gt=l(C,"A",{href:!0});var Xp=i(Gt);ml=o(Xp,"pipeline()"),Xp.forEach(s),hl=o(C,". An "),Qt=l(C,"A",{href:!0});var ef=i(Qt);cl=o(ef,"AutoClass"),ef.forEach(s),dl=o(C," is a shortcut that automatically retrieves the architecture of a pretrained model from it\u2019s name or path. You only need to select the appropriate "),ea=l(C,"CODE",{});var tf=i(ea);_l=o(tf,"AutoClass"),tf.forEach(s),gl=o(C," for your task and it\u2019s associated tokenizer with "),Jt=l(C,"A",{href:!0});var sf=i(Jt);vl=o(sf,"AutoTokenizer"),sf.forEach(s),yl=o(C,"."),C.forEach(s),fo=u(e),K=l(e,"P",{});var ks=i(K);$l=o(ks,"Let\u2019s return to our example and see how you can use the "),ta=l(ks,"CODE",{});var af=i(ta);bl=o(af,"AutoClass"),af.forEach(s),wl=o(ks," to replicate the results of the "),Kt=l(ks,"A",{href:!0});var of=i(Kt);kl=o(of,"pipeline()"),of.forEach(s),jl=o(ks,"."),ks.forEach(s),uo=u(e),re=l(e,"H3",{class:!0});var er=i(re);ke=l(er,"A",{id:!0,class:!0,href:!0});var rf=i(ke);sa=l(rf,"SPAN",{});var nf=i(sa);_(ut.$$.fragment,nf),nf.forEach(s),rf.forEach(s),El=u(er),aa=l(er,"SPAN",{});var lf=i(aa);Al=o(lf,"AutoTokenizer"),lf.forEach(s),er.forEach(s),mo=u(e),V=l(e,"P",{});var js=i(V);Tl=o(js,"A tokenizer is responsible for preprocessing text into a format that is understandable to the model. First, the tokenizer will split the text into words called "),oa=l(js,"EM",{});var pf=i(oa);xl=o(pf,"tokens"),pf.forEach(s),ql=o(js,". There are multiple rules that govern the tokenization process, including how to split a word and at what level (learn more about tokenization "),Vt=l(js,"A",{href:!0});var ff=i(Vt);zl=o(ff,"here"),ff.forEach(s),Fl=o(js,"). The most important thing to remember though is you need to instantiate the tokenizer with the same model name to ensure you\u2019re using the same tokenization rules a model was pretrained with."),js.forEach(s),ho=u(e),je=l(e,"P",{});var tr=i(je);Pl=o(tr,"Load a tokenizer with "),Zt=l(tr,"A",{href:!0});var uf=i(Zt);Sl=o(uf,"AutoTokenizer"),uf.forEach(s),Ml=o(tr,":"),tr.forEach(s),co=u(e),_(mt.$$.fragment,e),_o=u(e),Ee=l(e,"P",{});var sr=i(Ee);Cl=o(sr,"Next, the tokenizer converts the tokens into numbers in order to construct a tensor as input to the model. This is known as the model\u2019s "),ra=l(sr,"EM",{});var mf=i(ra);Il=o(mf,"vocabulary"),mf.forEach(s),Nl=o(sr,"."),sr.forEach(s),go=u(e),Xt=l(e,"P",{});var hf=i(Xt);Ol=o(hf,"Pass your text to the tokenizer:"),hf.forEach(s),vo=u(e),_(ht.$$.fragment,e),yo=u(e),es=l(e,"P",{});var cf=i(es);Ll=o(cf,"The tokenizer will return a dictionary containing:"),cf.forEach(s),$o=u(e),Ae=l(e,"UL",{});var ar=i(Ae);ts=l(ar,"LI",{});var Qi=i(ts);ss=l(Qi,"A",{href:!0});var df=i(ss);Dl=o(df,"input_ids"),df.forEach(s),Rl=o(Qi,": numerical representions of your tokens."),Qi.forEach(s),Hl=u(ar),as=l(ar,"LI",{});var Ji=i(as);os=l(Ji,"A",{href:!0});var _f=i(os);Ul=o(_f,"atttention_mask"),_f.forEach(s),Wl=o(Ji,": indicates which tokens should be attended to."),Ji.forEach(s),ar.forEach(s),bo=u(e),Te=l(e,"P",{});var or=i(Te);Bl=o(or,"Just like the "),rs=l(or,"A",{href:!0});var gf=i(rs);Yl=o(gf,"pipeline()"),gf.forEach(s),Gl=o(or,", the tokenizer will accept a list of inputs. In addition, the tokenizer can also pad and truncate the text to return a batch with uniform length:"),or.forEach(s),wo=u(e),_(ct.$$.fragment,e),ko=u(e),xe=l(e,"P",{});var rr=i(xe);Ql=o(rr,"Read the "),ns=l(rr,"A",{href:!0});var vf=i(ns);Jl=o(vf,"preprocessing"),vf.forEach(s),Kl=o(rr," tutorial for more details about tokenization."),rr.forEach(s),jo=u(e),ne=l(e,"H3",{class:!0});var nr=i(ne);qe=l(nr,"A",{id:!0,class:!0,href:!0});var yf=i(qe);na=l(yf,"SPAN",{});var $f=i(na);_(dt.$$.fragment,$f),$f.forEach(s),yf.forEach(s),Vl=u(nr),la=l(nr,"SPAN",{});var bf=i(la);Zl=o(bf,"AutoModel"),bf.forEach(s),nr.forEach(s),Eo=u(e),F=l(e,"P",{});var L=i(F);Xl=o(L,"\u{1F917} Transformers provides a simple and unified way to load pretrained instances. This means you can load an "),ls=l(L,"A",{href:!0});var wf=i(ls);ei=o(wf,"AutoModel"),wf.forEach(s),ti=o(L," like you would load an "),is=l(L,"A",{href:!0});var kf=i(is);si=o(kf,"AutoTokenizer"),kf.forEach(s),ai=o(L,". The only difference is selecting the correct "),ps=l(L,"A",{href:!0});var jf=i(ps);oi=o(jf,"AutoModel"),jf.forEach(s),ri=o(L," for the task. Since you are doing text - or sequence - classification, load "),fs=l(L,"A",{href:!0});var Ef=i(fs);ni=o(Ef,"AutoModelForSequenceClassification"),Ef.forEach(s),li=o(L,". The TensorFlow equivalent is simply "),us=l(L,"A",{href:!0});var Af=i(us);ii=o(Af,"TFAutoModelForSequenceClassification"),Af.forEach(s),pi=o(L,":"),L.forEach(s),Ao=u(e),_(_t.$$.fragment,e),To=u(e),_(ze.$$.fragment,e),xo=u(e),Fe=l(e,"P",{});var lr=i(Fe);fi=o(lr,"Now you can pass your preprocessed batch of inputs directly to the model. If you are using a PyTorch model, unpack the dictionary by adding "),ia=l(lr,"CODE",{});var Tf=i(ia);ui=o(Tf,"**"),Tf.forEach(s),mi=o(lr,". For TensorFlow models, pass the dictionary keys directly to the tensors:"),lr.forEach(s),qo=u(e),_(gt.$$.fragment,e),zo=u(e),Z=l(e,"P",{});var Es=i(Z);hi=o(Es,"The model outputs the final activations in the "),pa=l(Es,"CODE",{});var xf=i(pa);ci=o(xf,"logits"),xf.forEach(s),di=o(Es," attribute. Apply the softmax function to the "),fa=l(Es,"CODE",{});var qf=i(fa);_i=o(qf,"logits"),qf.forEach(s),gi=o(Es," to retrieve the probabilities:"),Es.forEach(s),Fo=u(e),_(vt.$$.fragment,e),Po=u(e),_(Pe.$$.fragment,e),So=u(e),z=l(e,"P",{});var I=i(z);vi=o(I,"Models are a standard "),yt=l(I,"A",{href:!0,rel:!0});var zf=i(yt);ua=l(zf,"CODE",{});var Ff=i(ua);yi=o(Ff,"torch.nn.Module"),Ff.forEach(s),zf.forEach(s),$i=o(I," or a "),$t=l(I,"A",{href:!0,rel:!0});var Pf=i($t);ma=l(Pf,"CODE",{});var Sf=i(ma);bi=o(Sf,"tf.keras.Model"),Sf.forEach(s),Pf.forEach(s),wi=o(I," so you can use them in your usual training loop. However, to make things easier, \u{1F917} Transformers provides a "),ms=l(I,"A",{href:!0});var Mf=i(ms);ki=o(Mf,"Trainer"),Mf.forEach(s),ji=o(I," class for PyTorch that adds functionality for distributed training, mixed precision, and more. For TensorFlow, you can use the "),ha=l(I,"CODE",{});var Cf=i(ha);Ei=o(Cf,"fit"),Cf.forEach(s),Ai=o(I," method from "),bt=l(I,"A",{href:!0,rel:!0});var If=i(bt);Ti=o(If,"Keras"),If.forEach(s),xi=o(I,". Refer to the "),hs=l(I,"A",{href:!0});var Nf=i(hs);qi=o(Nf,"training tutorial"),Nf.forEach(s),zi=o(I," for more details."),I.forEach(s),Mo=u(e),_(Se.$$.fragment,e),Co=u(e),le=l(e,"H3",{class:!0});var ir=i(le);Me=l(ir,"A",{id:!0,class:!0,href:!0});var Of=i(Me);ca=l(Of,"SPAN",{});var Lf=i(ca);_(wt.$$.fragment,Lf),Lf.forEach(s),Of.forEach(s),Fi=u(ir),da=l(ir,"SPAN",{});var Df=i(da);Pi=o(Df,"Save a model"),Df.forEach(s),ir.forEach(s),Io=u(e),Ce=l(e,"P",{});var pr=i(Ce);Si=o(pr,"Once your model is fine-tuned, you can save it with its tokenizer using "),cs=l(pr,"A",{href:!0});var Rf=i(cs);Mi=o(Rf,"PreTrainedModel.save_pretrained()"),Rf.forEach(s),Ci=o(pr,":"),pr.forEach(s),No=u(e),_(kt.$$.fragment,e),Oo=u(e),Ie=l(e,"P",{});var fr=i(Ie);Ii=o(fr,"When you are ready to use the model again, reload it with "),ds=l(fr,"A",{href:!0});var Hf=i(ds);Ni=o(Hf,"PreTrainedModel.from_pretrained()"),Hf.forEach(s),Oi=o(fr,":"),fr.forEach(s),Lo=u(e),_(jt.$$.fragment,e),Do=u(e),X=l(e,"P",{});var As=i(X);Li=o(As,"One particularly cool \u{1F917} Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The "),_a=l(As,"CODE",{});var Uf=i(_a);Di=o(Uf,"from_pt"),Uf.forEach(s),Ri=o(As," or "),ga=l(As,"CODE",{});var Wf=i(ga);Hi=o(Wf,"from_tf"),Wf.forEach(s),Ui=o(As," parameter can convert the model from one framework to the other:"),As.forEach(s),Ro=u(e),_(Et.$$.fragment,e),this.h()},h(){m(h,"name","hf:doc:metadata"),m(h,"content",JSON.stringify(au)),m(w,"id","quick-tour"),m(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(w,"href","#quick-tour"),m(c,"class","relative group"),m(Tt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(xt,"href","./model_doc/auto"),m(ue,"id","pipeline"),m(ue,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ue,"href","#pipeline"),m(te,"class","relative group"),m(qt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(zt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(de,"id","pipeline-usage"),m(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(de,"href","#pipeline-usage"),m(se,"class","relative group"),m(Ft,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(St,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(Ge,"href","https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english"),m(Ge,"rel","nofollow"),m(Mt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(Ct,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(Ke,"href","https://huggingface.co/docs/datasets/"),m(Ke,"rel","nofollow"),m(It,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(Xe,"href","https://huggingface.co/docs/datasets/quickstart.html"),m(Xe,"rel","nofollow"),m(et,"href","https://huggingface.co/datasets/superb"),m(et,"rel","nofollow"),m(Ot,"href","./main_classes/pipelines"),m(be,"id","use-another-model-and-tokenizer-in-the-pipeline"),m(be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(be,"href","#use-another-model-and-tokenizer-in-the-pipeline"),m(ae,"class","relative group"),m(Lt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(ot,"href","https://huggingface.co/models"),m(ot,"rel","nofollow"),m(Dt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(rt,"href","https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment"),m(rt,"rel","nofollow"),m(Rt,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Ht,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(Ut,"href","./training"),m(Wt,"href","./model_sharing"),m(we,"id","autoclass"),m(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(we,"href","#autoclass"),m(oe,"class","relative group"),m(Bt,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Yt,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer"),m(Gt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(Qt,"href","./model_doc/auto"),m(Jt,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer"),m(Kt,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(ke,"id","autotokenizer"),m(ke,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ke,"href","#autotokenizer"),m(re,"class","relative group"),m(Vt,"href","./tokenizer_summary"),m(Zt,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer"),m(ss,"href","./glossary#input-ids"),m(os,"href",".glossary#attention-mask"),m(rs,"href","/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline"),m(ns,"href","./preprocessing"),m(qe,"id","automodel"),m(qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(qe,"href","#automodel"),m(ne,"class","relative group"),m(ls,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModel"),m(is,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer"),m(ps,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModel"),m(fs,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(us,"href","/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),m(yt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),m(yt,"rel","nofollow"),m($t,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),m($t,"rel","nofollow"),m(ms,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Trainer"),m(bt,"href","https://keras.io/"),m(bt,"rel","nofollow"),m(hs,"href","./training"),m(Me,"id","save-a-model"),m(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Me,"href","#save-a-model"),m(le,"class","relative group"),m(cs,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained"),m(ds,"href","/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained")},m(e,r){t(document.head,h),p(e,k,r),p(e,c,r),t(c,w),t(w,A),g(b,A,null),t(c,j),t(c,x),t(x,S),p(e,E,r),g(O,e,r),p(e,R,r),p(e,H,r),t(H,ur),t(H,Tt),t(Tt,mr),t(H,hr),t(H,xt),t(xt,cr),t(H,dr),p(e,wa,r),g(fe,e,r),p(e,ka,r),p(e,te,r),t(te,ue),t(ue,Ts),g(Oe,Ts,null),t(te,_r),t(te,xs),t(xs,gr),p(e,ja,r),p(e,Le,r),t(Le,qt),t(qt,vr),t(Le,yr),p(e,Ea,r),g(De,e,r),p(e,Aa,r),p(e,me,r),t(me,$r),t(me,zt),t(zt,br),t(me,wr),p(e,Ta,r),p(e,Re,r),t(Re,qs),t(qs,kr),t(Re,jr),p(e,xa,r),p(e,T,r),t(T,zs),t(zs,Er),t(T,Ar),t(T,Fs),t(Fs,Tr),t(T,xr),t(T,Ps),t(Ps,qr),t(T,zr),t(T,Ss),t(Ss,Fr),t(T,Pr),t(T,Ms),t(Ms,Sr),t(T,Mr),t(T,Cs),t(Cs,Cr),t(T,Ir),t(T,Is),t(Is,Nr),t(T,Or),t(T,Ns),t(Ns,Lr),p(e,qa,r),p(e,He,r),t(He,Os),t(Os,Dr),t(He,Rr),p(e,za,r),p(e,U,r),t(U,Ls),t(Ls,Hr),t(U,Ur),t(U,Ds),t(Ds,Wr),t(U,Br),t(U,Rs),t(Rs,Yr),p(e,Fa,r),p(e,Ue,r),t(Ue,Hs),t(Hs,Gr),t(Ue,Qr),p(e,Pa,r),p(e,he,r),t(he,Us),t(Us,Jr),t(he,Kr),t(he,Ws),t(Ws,Vr),p(e,Sa,r),g(ce,e,r),p(e,Ma,r),p(e,se,r),t(se,de),t(de,Bs),g(We,Bs,null),t(se,Zr),t(se,Ys),t(Ys,Xr),p(e,Ca,r),p(e,_e,r),t(_e,en),t(_e,Ft),t(Ft,tn),t(_e,sn),p(e,Ia,r),p(e,Pt,r),t(Pt,an),p(e,Na,r),g(Be,e,r),p(e,Oa,r),p(e,ge,r),t(ge,on),t(ge,St),t(St,rn),t(ge,nn),p(e,La,r),g(Ye,e,r),p(e,Da,r),p(e,W,r),t(W,ln),t(W,Ge),t(Ge,pn),t(W,fn),t(W,Gs),t(Gs,un),t(W,mn),p(e,Ra,r),g(Qe,e,r),p(e,Ha,r),p(e,ve,r),t(ve,hn),t(ve,Mt),t(Mt,cn),t(ve,dn),p(e,Ua,r),g(Je,e,r),p(e,Wa,r),p(e,B,r),t(B,_n),t(B,Ct),t(Ct,gn),t(B,vn),t(B,Ke),t(Ke,yn),t(B,$n),p(e,Ba,r),g(Ve,e,r),p(e,Ya,r),p(e,ye,r),t(ye,bn),t(ye,It),t(It,wn),t(ye,kn),p(e,Ga,r),g(Ze,e,r),p(e,Qa,r),p(e,Y,r),t(Y,jn),t(Y,Xe),t(Xe,En),t(Y,An),t(Y,et),t(et,Tn),t(Y,xn),p(e,Ja,r),g(tt,e,r),p(e,Ka,r),p(e,Nt,r),t(Nt,qn),p(e,Va,r),g(st,e,r),p(e,Za,r),p(e,$e,r),t($e,zn),t($e,Ot),t(Ot,Fn),t($e,Pn),p(e,Xa,r),p(e,ae,r),t(ae,be),t(be,Qs),g(at,Qs,null),t(ae,Sn),t(ae,Js),t(Js,Mn),p(e,eo,r),p(e,M,r),t(M,Cn),t(M,Lt),t(Lt,In),t(M,Nn),t(M,ot),t(ot,On),t(M,Ln),t(M,Dt),t(Dt,Dn),t(M,Rn),t(M,rt),t(rt,Hn),t(M,Un),p(e,to,r),g(nt,e,r),p(e,so,r),p(e,G,r),t(G,Wn),t(G,Rt),t(Rt,Bn),t(G,Yn),t(G,Ks),t(Ks,Gn),t(G,Qn),p(e,ao,r),g(lt,e,r),p(e,oo,r),p(e,Q,r),t(Q,Jn),t(Q,Ht),t(Ht,Kn),t(Q,Vn),t(Q,Vs),t(Vs,Zn),t(Q,Xn),p(e,ro,r),g(it,e,r),p(e,no,r),p(e,J,r),t(J,el),t(J,Ut),t(Ut,tl),t(J,sl),t(J,Wt),t(Wt,al),t(J,ol),p(e,lo,r),p(e,oe,r),t(oe,we),t(we,Zs),g(pt,Zs,null),t(oe,rl),t(oe,Xs),t(Xs,nl),p(e,io,r),g(ft,e,r),p(e,po,r),p(e,q,r),t(q,ll),t(q,Bt),t(Bt,il),t(q,pl),t(q,Yt),t(Yt,fl),t(q,ul),t(q,Gt),t(Gt,ml),t(q,hl),t(q,Qt),t(Qt,cl),t(q,dl),t(q,ea),t(ea,_l),t(q,gl),t(q,Jt),t(Jt,vl),t(q,yl),p(e,fo,r),p(e,K,r),t(K,$l),t(K,ta),t(ta,bl),t(K,wl),t(K,Kt),t(Kt,kl),t(K,jl),p(e,uo,r),p(e,re,r),t(re,ke),t(ke,sa),g(ut,sa,null),t(re,El),t(re,aa),t(aa,Al),p(e,mo,r),p(e,V,r),t(V,Tl),t(V,oa),t(oa,xl),t(V,ql),t(V,Vt),t(Vt,zl),t(V,Fl),p(e,ho,r),p(e,je,r),t(je,Pl),t(je,Zt),t(Zt,Sl),t(je,Ml),p(e,co,r),g(mt,e,r),p(e,_o,r),p(e,Ee,r),t(Ee,Cl),t(Ee,ra),t(ra,Il),t(Ee,Nl),p(e,go,r),p(e,Xt,r),t(Xt,Ol),p(e,vo,r),g(ht,e,r),p(e,yo,r),p(e,es,r),t(es,Ll),p(e,$o,r),p(e,Ae,r),t(Ae,ts),t(ts,ss),t(ss,Dl),t(ts,Rl),t(Ae,Hl),t(Ae,as),t(as,os),t(os,Ul),t(as,Wl),p(e,bo,r),p(e,Te,r),t(Te,Bl),t(Te,rs),t(rs,Yl),t(Te,Gl),p(e,wo,r),g(ct,e,r),p(e,ko,r),p(e,xe,r),t(xe,Ql),t(xe,ns),t(ns,Jl),t(xe,Kl),p(e,jo,r),p(e,ne,r),t(ne,qe),t(qe,na),g(dt,na,null),t(ne,Vl),t(ne,la),t(la,Zl),p(e,Eo,r),p(e,F,r),t(F,Xl),t(F,ls),t(ls,ei),t(F,ti),t(F,is),t(is,si),t(F,ai),t(F,ps),t(ps,oi),t(F,ri),t(F,fs),t(fs,ni),t(F,li),t(F,us),t(us,ii),t(F,pi),p(e,Ao,r),g(_t,e,r),p(e,To,r),g(ze,e,r),p(e,xo,r),p(e,Fe,r),t(Fe,fi),t(Fe,ia),t(ia,ui),t(Fe,mi),p(e,qo,r),g(gt,e,r),p(e,zo,r),p(e,Z,r),t(Z,hi),t(Z,pa),t(pa,ci),t(Z,di),t(Z,fa),t(fa,_i),t(Z,gi),p(e,Fo,r),g(vt,e,r),p(e,Po,r),g(Pe,e,r),p(e,So,r),p(e,z,r),t(z,vi),t(z,yt),t(yt,ua),t(ua,yi),t(z,$i),t(z,$t),t($t,ma),t(ma,bi),t(z,wi),t(z,ms),t(ms,ki),t(z,ji),t(z,ha),t(ha,Ei),t(z,Ai),t(z,bt),t(bt,Ti),t(z,xi),t(z,hs),t(hs,qi),t(z,zi),p(e,Mo,r),g(Se,e,r),p(e,Co,r),p(e,le,r),t(le,Me),t(Me,ca),g(wt,ca,null),t(le,Fi),t(le,da),t(da,Pi),p(e,Io,r),p(e,Ce,r),t(Ce,Si),t(Ce,cs),t(cs,Mi),t(Ce,Ci),p(e,No,r),g(kt,e,r),p(e,Oo,r),p(e,Ie,r),t(Ie,Ii),t(Ie,ds),t(ds,Ni),t(Ie,Oi),p(e,Lo,r),g(jt,e,r),p(e,Do,r),p(e,X,r),t(X,Li),t(X,_a),t(_a,Di),t(X,Ri),t(X,ga),t(ga,Hi),t(X,Ui),p(e,Ro,r),g(Et,e,r),Ho=!0},p(e,[r]){const At={};r&2&&(At.$$scope={dirty:r,ctx:e}),fe.$set(At);const va={};r&2&&(va.$$scope={dirty:r,ctx:e}),ce.$set(va);const ya={};r&2&&(ya.$$scope={dirty:r,ctx:e}),ze.$set(ya);const $a={};r&2&&($a.$$scope={dirty:r,ctx:e}),Pe.$set($a);const ie={};r&2&&(ie.$$scope={dirty:r,ctx:e}),Se.$set(ie)},i(e){Ho||(v(b.$$.fragment,e),v(O.$$.fragment,e),v(fe.$$.fragment,e),v(Oe.$$.fragment,e),v(De.$$.fragment,e),v(ce.$$.fragment,e),v(We.$$.fragment,e),v(Be.$$.fragment,e),v(Ye.$$.fragment,e),v(Qe.$$.fragment,e),v(Je.$$.fragment,e),v(Ve.$$.fragment,e),v(Ze.$$.fragment,e),v(tt.$$.fragment,e),v(st.$$.fragment,e),v(at.$$.fragment,e),v(nt.$$.fragment,e),v(lt.$$.fragment,e),v(it.$$.fragment,e),v(pt.$$.fragment,e),v(ft.$$.fragment,e),v(ut.$$.fragment,e),v(mt.$$.fragment,e),v(ht.$$.fragment,e),v(ct.$$.fragment,e),v(dt.$$.fragment,e),v(_t.$$.fragment,e),v(ze.$$.fragment,e),v(gt.$$.fragment,e),v(vt.$$.fragment,e),v(Pe.$$.fragment,e),v(Se.$$.fragment,e),v(wt.$$.fragment,e),v(kt.$$.fragment,e),v(jt.$$.fragment,e),v(Et.$$.fragment,e),Ho=!0)},o(e){y(b.$$.fragment,e),y(O.$$.fragment,e),y(fe.$$.fragment,e),y(Oe.$$.fragment,e),y(De.$$.fragment,e),y(ce.$$.fragment,e),y(We.$$.fragment,e),y(Be.$$.fragment,e),y(Ye.$$.fragment,e),y(Qe.$$.fragment,e),y(Je.$$.fragment,e),y(Ve.$$.fragment,e),y(Ze.$$.fragment,e),y(tt.$$.fragment,e),y(st.$$.fragment,e),y(at.$$.fragment,e),y(nt.$$.fragment,e),y(lt.$$.fragment,e),y(it.$$.fragment,e),y(pt.$$.fragment,e),y(ft.$$.fragment,e),y(ut.$$.fragment,e),y(mt.$$.fragment,e),y(ht.$$.fragment,e),y(ct.$$.fragment,e),y(dt.$$.fragment,e),y(_t.$$.fragment,e),y(ze.$$.fragment,e),y(gt.$$.fragment,e),y(vt.$$.fragment,e),y(Pe.$$.fragment,e),y(Se.$$.fragment,e),y(wt.$$.fragment,e),y(kt.$$.fragment,e),y(jt.$$.fragment,e),y(Et.$$.fragment,e),Ho=!1},d(e){s(h),e&&s(k),e&&s(c),$(b),e&&s(E),$(O,e),e&&s(R),e&&s(H),e&&s(wa),$(fe,e),e&&s(ka),e&&s(te),$(Oe),e&&s(ja),e&&s(Le),e&&s(Ea),$(De,e),e&&s(Aa),e&&s(me),e&&s(Ta),e&&s(Re),e&&s(xa),e&&s(T),e&&s(qa),e&&s(He),e&&s(za),e&&s(U),e&&s(Fa),e&&s(Ue),e&&s(Pa),e&&s(he),e&&s(Sa),$(ce,e),e&&s(Ma),e&&s(se),$(We),e&&s(Ca),e&&s(_e),e&&s(Ia),e&&s(Pt),e&&s(Na),$(Be,e),e&&s(Oa),e&&s(ge),e&&s(La),$(Ye,e),e&&s(Da),e&&s(W),e&&s(Ra),$(Qe,e),e&&s(Ha),e&&s(ve),e&&s(Ua),$(Je,e),e&&s(Wa),e&&s(B),e&&s(Ba),$(Ve,e),e&&s(Ya),e&&s(ye),e&&s(Ga),$(Ze,e),e&&s(Qa),e&&s(Y),e&&s(Ja),$(tt,e),e&&s(Ka),e&&s(Nt),e&&s(Va),$(st,e),e&&s(Za),e&&s($e),e&&s(Xa),e&&s(ae),$(at),e&&s(eo),e&&s(M),e&&s(to),$(nt,e),e&&s(so),e&&s(G),e&&s(ao),$(lt,e),e&&s(oo),e&&s(Q),e&&s(ro),$(it,e),e&&s(no),e&&s(J),e&&s(lo),e&&s(oe),$(pt),e&&s(io),$(ft,e),e&&s(po),e&&s(q),e&&s(fo),e&&s(K),e&&s(uo),e&&s(re),$(ut),e&&s(mo),e&&s(V),e&&s(ho),e&&s(je),e&&s(co),$(mt,e),e&&s(_o),e&&s(Ee),e&&s(go),e&&s(Xt),e&&s(vo),$(ht,e),e&&s(yo),e&&s(es),e&&s($o),e&&s(Ae),e&&s(bo),e&&s(Te),e&&s(wo),$(ct,e),e&&s(ko),e&&s(xe),e&&s(jo),e&&s(ne),$(dt),e&&s(Eo),e&&s(F),e&&s(Ao),$(_t,e),e&&s(To),$(ze,e),e&&s(xo),e&&s(Fe),e&&s(qo),$(gt,e),e&&s(zo),e&&s(Z),e&&s(Fo),$(vt,e),e&&s(Po),$(Pe,e),e&&s(So),e&&s(z),e&&s(Mo),$(Se,e),e&&s(Co),e&&s(le),$(wt),e&&s(Io),e&&s(Ce),e&&s(No),$(kt,e),e&&s(Oo),e&&s(Ie),e&&s(Lo),$(jt,e),e&&s(Do),e&&s(X),e&&s(Ro),$(Et,e)}}}const au={local:"quick-tour",sections:[{local:"pipeline",sections:[{local:"pipeline-usage",title:"Pipeline usage"},{local:"use-another-model-and-tokenizer-in-the-pipeline",title:"Use another model and tokenizer in the pipeline"}],title:"Pipeline"},{local:"autoclass",sections:[{local:"autotokenizer",title:"AutoTokenizer"},{local:"automodel",title:"AutoModel"},{local:"save-a-model",title:"Save a model"}],title:"AutoClass"}],title:"Quick tour"};function ou(N,h,k){let{fw:c}=h;return N.$$set=w=>{"fw"in w&&k(0,c=w.fw)},[c]}class hu extends Yf{constructor(h){super();Gf(this,h,ou,su,Qf,{fw:0})}}export{hu as default,au as metadata};
