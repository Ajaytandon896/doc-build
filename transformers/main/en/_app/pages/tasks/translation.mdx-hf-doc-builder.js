import{S as Ca,i as Oa,s as Ia,e as i,k as _,w as v,t as o,M as Ua,c as p,d as t,m as $,a as f,x as S,h as l,b as k,G as a,g as m,y,q as E,o as T,B as x,v as Na,L as Ma}from"../../chunks/vendor-hf-doc-builder.js";import{T as zt}from"../../chunks/Tip-hf-doc-builder.js";import{Y as La}from"../../chunks/Youtube-hf-doc-builder.js";import{I as xt}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{C as ne}from"../../chunks/CodeBlock-hf-doc-builder.js";import{F as Da,M as At}from"../../chunks/Markdown-hf-doc-builder.js";function Ba(P){let s,c,n,u,w;return{c(){s=i("p"),c=o("See the translation "),n=i("a"),u=o("task page"),w=o(" for more information about its associated models, datasets, and metrics."),this.h()},l(g){s=p(g,"P",{});var q=f(s);c=l(q,"See the translation "),n=p(q,"A",{href:!0,rel:!0});var A=f(n);u=l(A,"task page"),A.forEach(t),w=l(q," for more information about its associated models, datasets, and metrics."),q.forEach(t),this.h()},h(){k(n,"href","https://huggingface.co/tasks/translation"),k(n,"rel","nofollow")},m(g,q){m(g,s,q),a(s,c),a(s,n),a(n,u),a(s,w)},d(g){g&&t(s)}}}function Wa(P){let s,c;return s=new ne({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)`}}),{c(){v(s.$$.fragment)},l(n){S(s.$$.fragment,n)},m(n,u){y(s,n,u),c=!0},p:Ma,i(n){c||(E(s.$$.fragment,n),c=!0)},o(n){T(s.$$.fragment,n),c=!1},d(n){x(s,n)}}}function Ya(P){let s,c;return s=new At({props:{$$slots:{default:[Wa]},$$scope:{ctx:P}}}),{c(){v(s.$$.fragment)},l(n){S(s.$$.fragment,n)},m(n,u){y(s,n,u),c=!0},p(n,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:n}),s.$set(w)},i(n){c||(E(s.$$.fragment,n),c=!0)},o(n){T(s.$$.fragment,n),c=!1},d(n){x(s,n)}}}function Ha(P){let s,c;return s=new ne({props:{code:`from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),{c(){v(s.$$.fragment)},l(n){S(s.$$.fragment,n)},m(n,u){y(s,n,u),c=!0},p:Ma,i(n){c||(E(s.$$.fragment,n),c=!0)},o(n){T(s.$$.fragment,n),c=!1},d(n){x(s,n)}}}function Za(P){let s,c;return s=new At({props:{$$slots:{default:[Ha]},$$scope:{ctx:P}}}),{c(){v(s.$$.fragment)},l(n){S(s.$$.fragment,n)},m(n,u){y(s,n,u),c=!0},p(n,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:n}),s.$set(w)},i(n){c||(E(s.$$.fragment,n),c=!0)},o(n){T(s.$$.fragment,n),c=!1},d(n){x(s,n)}}}function Ja(P){let s,c,n,u,w,g,q,A;return{c(){s=i("p"),c=o("If you aren\u2019t familiar with fine-tuning a model with the "),n=i("a"),u=o("Trainer"),w=o(", take a look at the basic tutorial "),g=i("a"),q=o("here"),A=o("!"),this.h()},l(z){s=p(z,"P",{});var j=f(s);c=l(j,"If you aren\u2019t familiar with fine-tuning a model with the "),n=p(j,"A",{href:!0});var L=f(n);u=l(L,"Trainer"),L.forEach(t),w=l(j,", take a look at the basic tutorial "),g=p(j,"A",{href:!0});var N=f(g);q=l(N,"here"),N.forEach(t),A=l(j,"!"),j.forEach(t),this.h()},h(){k(n,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Trainer"),k(g,"href","../training#finetune-with-trainer")},m(z,j){m(z,s,j),a(s,c),a(s,n),a(n,u),a(s,w),a(s,g),a(g,q),a(s,A)},d(z){z&&t(s)}}}function Ka(P){let s,c,n,u,w,g,q,A,z,j,L,N,B,W,M,Y,J,Q,ue,V,C,G,re,ee,X,de,D,O,K,I,ge,H,R,he;return q=new ne({props:{code:`from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

model = AutoModelForSeq2SeqLM.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),z=new zt({props:{$$slots:{default:[Ja]},$$scope:{ctx:P}}}),R=new ne({props:{code:`training_args = Seq2SeqTrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=1,
    fp16=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_books["train"],
    eval_dataset=tokenized_books["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = Seq2SeqTrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Seq2SeqTrainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_books[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_books[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),{c(){s=i("p"),c=o("Load T5 with "),n=i("a"),u=o("AutoModelForSeq2SeqLM"),w=o(":"),g=_(),v(q.$$.fragment),A=_(),v(z.$$.fragment),j=_(),L=i("p"),N=o("At this point, only three steps remain:"),B=_(),W=i("ol"),M=i("li"),Y=o("Define your training hyperparameters in "),J=i("a"),Q=o("Seq2SeqTrainingArguments"),ue=o("."),V=_(),C=i("li"),G=o("Pass the training arguments to "),re=i("a"),ee=o("Seq2SeqTrainer"),X=o(" along with the model, dataset, tokenizer, and data collator."),de=_(),D=i("li"),O=o("Call "),K=i("a"),I=o("train()"),ge=o(" to fine-tune your model."),H=_(),v(R.$$.fragment),this.h()},l(d){s=p(d,"P",{});var F=f(s);c=l(F,"Load T5 with "),n=p(F,"A",{href:!0});var Z=f(n);u=l(Z,"AutoModelForSeq2SeqLM"),Z.forEach(t),w=l(F,":"),F.forEach(t),g=$(d),S(q.$$.fragment,d),A=$(d),S(z.$$.fragment,d),j=$(d),L=p(d,"P",{});var te=f(L);N=l(te,"At this point, only three steps remain:"),te.forEach(t),B=$(d),W=p(d,"OL",{});var U=f(W);M=p(U,"LI",{});var ae=f(M);Y=l(ae,"Define your training hyperparameters in "),J=p(ae,"A",{href:!0});var oe=f(J);Q=l(oe,"Seq2SeqTrainingArguments"),oe.forEach(t),ue=l(ae,"."),ae.forEach(t),V=$(U),C=p(U,"LI",{});var se=f(C);G=l(se,"Pass the training arguments to "),re=p(se,"A",{href:!0});var le=f(re);ee=l(le,"Seq2SeqTrainer"),le.forEach(t),X=l(se," along with the model, dataset, tokenizer, and data collator."),se.forEach(t),de=$(U),D=p(U,"LI",{});var ce=f(D);O=l(ce,"Call "),K=p(ce,"A",{href:!0});var ie=f(K);I=l(ie,"train()"),ie.forEach(t),ge=l(ce," to fine-tune your model."),ce.forEach(t),U.forEach(t),H=$(d),S(R.$$.fragment,d),this.h()},h(){k(n,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM"),k(J,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments"),k(re,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainer"),k(K,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train")},m(d,F){m(d,s,F),a(s,c),a(s,n),a(n,u),a(s,w),m(d,g,F),y(q,d,F),m(d,A,F),y(z,d,F),m(d,j,F),m(d,L,F),a(L,N),m(d,B,F),m(d,W,F),a(W,M),a(M,Y),a(M,J),a(J,Q),a(M,ue),a(W,V),a(W,C),a(C,G),a(C,re),a(re,ee),a(C,X),a(W,de),a(W,D),a(D,O),a(D,K),a(K,I),a(D,ge),m(d,H,F),y(R,d,F),he=!0},p(d,F){const Z={};F&2&&(Z.$$scope={dirty:F,ctx:d}),z.$set(Z)},i(d){he||(E(q.$$.fragment,d),E(z.$$.fragment,d),E(R.$$.fragment,d),he=!0)},o(d){T(q.$$.fragment,d),T(z.$$.fragment,d),T(R.$$.fragment,d),he=!1},d(d){d&&t(s),d&&t(g),x(q,d),d&&t(A),x(z,d),d&&t(j),d&&t(L),d&&t(B),d&&t(W),d&&t(H),x(R,d)}}}function Ra(P){let s,c;return s=new At({props:{$$slots:{default:[Ka]},$$scope:{ctx:P}}}),{c(){v(s.$$.fragment)},l(n){S(s.$$.fragment,n)},m(n,u){y(s,n,u),c=!0},p(n,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:n}),s.$set(w)},i(n){c||(E(s.$$.fragment,n),c=!0)},o(n){T(s.$$.fragment,n),c=!1},d(n){x(s,n)}}}function Ga(P){let s,c,n,u,w;return{c(){s=i("p"),c=o("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),n=i("a"),u=o("here"),w=o("!"),this.h()},l(g){s=p(g,"P",{});var q=f(s);c=l(q,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),n=p(q,"A",{href:!0});var A=f(n);u=l(A,"here"),A.forEach(t),w=l(q,"!"),q.forEach(t),this.h()},h(){k(n,"href","training#finetune-with-keras")},m(g,q){m(g,s,q),a(s,c),a(s,n),a(n,u),a(s,w)},d(g){g&&t(s)}}}function Xa(P){let s,c,n,u,w,g,q,A,z,j,L,N,B,W,M,Y,J,Q,ue,V,C,G,re,ee,X,de,D,O,K,I,ge,H,R,he,d,F,Z,te,U,ae,oe,se,le,ce,ie,ke,je;return B=new ne({props:{code:`tf_train_set = tokenized_books["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_test_set = tokenized_books["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = tokenized_books[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = tokenized_books[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),M=new zt({props:{$$slots:{default:[Ga]},$$scope:{ctx:P}}}),V=new ne({props:{code:`from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`}}),O=new ne({props:{code:`from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),Z=new ne({props:{code:"model.compile(optimizer=optimizer)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)'}}),ke=new ne({props:{code:"model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)'}}),{c(){s=i("p"),c=o("To fine-tune a model in TensorFlow, start by converting your datasets to the "),n=i("code"),u=o("tf.data.Dataset"),w=o(" format with "),g=i("a"),q=o("to_tf_dataset"),A=o(". Specify inputs and labels in "),z=i("code"),j=o("columns"),L=o(", whether to shuffle the dataset order, batch size, and the data collator:"),N=_(),v(B.$$.fragment),W=_(),v(M.$$.fragment),Y=_(),J=i("p"),Q=o("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),ue=_(),v(V.$$.fragment),C=_(),G=i("p"),re=o("Load T5 with "),ee=i("a"),X=o("TFAutoModelForSeq2SeqLM"),de=o(":"),D=_(),v(O.$$.fragment),K=_(),I=i("p"),ge=o("Configure the model for training with "),H=i("a"),R=i("code"),he=o("compile"),d=o(":"),F=_(),v(Z.$$.fragment),te=_(),U=i("p"),ae=o("Call "),oe=i("a"),se=i("code"),le=o("fit"),ce=o(" to fine-tune the model:"),ie=_(),v(ke.$$.fragment),this.h()},l(r){s=p(r,"P",{});var b=f(s);c=l(b,"To fine-tune a model in TensorFlow, start by converting your datasets to the "),n=p(b,"CODE",{});var pe=f(n);u=l(pe,"tf.data.Dataset"),pe.forEach(t),w=l(b," format with "),g=p(b,"A",{href:!0,rel:!0});var We=f(g);q=l(We,"to_tf_dataset"),We.forEach(t),A=l(b,". Specify inputs and labels in "),z=p(b,"CODE",{});var Se=f(z);j=l(Se,"columns"),Se.forEach(t),L=l(b,", whether to shuffle the dataset order, batch size, and the data collator:"),b.forEach(t),N=$(r),S(B.$$.fragment,r),W=$(r),S(M.$$.fragment,r),Y=$(r),J=p(r,"P",{});var Ye=f(J);Q=l(Ye,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Ye.forEach(t),ue=$(r),S(V.$$.fragment,r),C=$(r),G=p(r,"P",{});var ye=f(G);re=l(ye,"Load T5 with "),ee=p(ye,"A",{href:!0});var Le=f(ee);X=l(Le,"TFAutoModelForSeq2SeqLM"),Le.forEach(t),de=l(ye,":"),ye.forEach(t),D=$(r),S(O.$$.fragment,r),K=$(r),I=p(r,"P",{});var fe=f(I);ge=l(fe,"Configure the model for training with "),H=p(fe,"A",{href:!0,rel:!0});var we=f(H);R=p(we,"CODE",{});var Ee=f(R);he=l(Ee,"compile"),Ee.forEach(t),we.forEach(t),d=l(fe,":"),fe.forEach(t),F=$(r),S(Z.$$.fragment,r),te=$(r),U=p(r,"P",{});var _e=f(U);ae=l(_e,"Call "),oe=p(_e,"A",{href:!0,rel:!0});var He=f(oe);se=p(He,"CODE",{});var Te=f(se);le=l(Te,"fit"),Te.forEach(t),He.forEach(t),ce=l(_e," to fine-tune the model:"),_e.forEach(t),ie=$(r),S(ke.$$.fragment,r),this.h()},h(){k(g,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset"),k(g,"rel","nofollow"),k(ee,"href","/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM"),k(H,"href","https://keras.io/api/models/model_training_apis/#compile-method"),k(H,"rel","nofollow"),k(oe,"href","https://keras.io/api/models/model_training_apis/#fit-method"),k(oe,"rel","nofollow")},m(r,b){m(r,s,b),a(s,c),a(s,n),a(n,u),a(s,w),a(s,g),a(g,q),a(s,A),a(s,z),a(z,j),a(s,L),m(r,N,b),y(B,r,b),m(r,W,b),y(M,r,b),m(r,Y,b),m(r,J,b),a(J,Q),m(r,ue,b),y(V,r,b),m(r,C,b),m(r,G,b),a(G,re),a(G,ee),a(ee,X),a(G,de),m(r,D,b),y(O,r,b),m(r,K,b),m(r,I,b),a(I,ge),a(I,H),a(H,R),a(R,he),a(I,d),m(r,F,b),y(Z,r,b),m(r,te,b),m(r,U,b),a(U,ae),a(U,oe),a(oe,se),a(se,le),a(U,ce),m(r,ie,b),y(ke,r,b),je=!0},p(r,b){const pe={};b&2&&(pe.$$scope={dirty:b,ctx:r}),M.$set(pe)},i(r){je||(E(B.$$.fragment,r),E(M.$$.fragment,r),E(V.$$.fragment,r),E(O.$$.fragment,r),E(Z.$$.fragment,r),E(ke.$$.fragment,r),je=!0)},o(r){T(B.$$.fragment,r),T(M.$$.fragment,r),T(V.$$.fragment,r),T(O.$$.fragment,r),T(Z.$$.fragment,r),T(ke.$$.fragment,r),je=!1},d(r){r&&t(s),r&&t(N),x(B,r),r&&t(W),x(M,r),r&&t(Y),r&&t(J),r&&t(ue),x(V,r),r&&t(C),r&&t(G),r&&t(D),x(O,r),r&&t(K),r&&t(I),r&&t(F),x(Z,r),r&&t(te),r&&t(U),r&&t(ie),x(ke,r)}}}function Qa(P){let s,c;return s=new At({props:{$$slots:{default:[Xa]},$$scope:{ctx:P}}}),{c(){v(s.$$.fragment)},l(n){S(s.$$.fragment,n)},m(n,u){y(s,n,u),c=!0},p(n,u){const w={};u&2&&(w.$$scope={dirty:u,ctx:n}),s.$set(w)},i(n){c||(E(s.$$.fragment,n),c=!0)},o(n){T(s.$$.fragment,n),c=!1},d(n){x(s,n)}}}function Va(P){let s,c,n,u,w,g,q,A;return{c(){s=i("p"),c=o(`For a more in-depth example of how to fine-tune a model for translation, take a look at the corresponding
`),n=i("a"),u=o("PyTorch notebook"),w=o(`
or `),g=i("a"),q=o("TensorFlow notebook"),A=o("."),this.h()},l(z){s=p(z,"P",{});var j=f(s);c=l(j,`For a more in-depth example of how to fine-tune a model for translation, take a look at the corresponding
`),n=p(j,"A",{href:!0,rel:!0});var L=f(n);u=l(L,"PyTorch notebook"),L.forEach(t),w=l(j,`
or `),g=p(j,"A",{href:!0,rel:!0});var N=f(g);q=l(N,"TensorFlow notebook"),N.forEach(t),A=l(j,"."),j.forEach(t),this.h()},h(){k(n,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb"),k(n,"rel","nofollow"),k(g,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb"),k(g,"rel","nofollow")},m(z,j){m(z,s,j),a(s,c),a(s,n),a(n,u),a(s,w),a(s,g),a(g,q),a(s,A)},d(z){z&&t(s)}}}function es(P){let s,c,n,u,w,g,q,A,z,j,L,N,B,W,M,Y,J,Q,ue,V,C,G,re,ee,X,de,D,O,K,I,ge,H,R,he,d,F,Z,te,U,ae,oe,se,le,ce,ie,ke,je,r,b,pe,We,Se,Ye,ye,Le,fe,we,Ee,_e,He,Te,Ft,ft,De,mt,Ze,Pt,ht,Me,ct,Je,Lt,ut,be,Xe,Dt,Mt,Qe,Ct,Ot,Ce,It,Ve,Ut,Nt,dt,Oe,_t,$e,Bt,Ie,Wt,Yt,et,Ht,Zt,tt,Jt,Kt,$t,Ue,gt,me,Rt,Ke,Gt,Xt,at,Qt,Vt,st,ea,ta,nt,aa,sa,kt,xe,wt,ve,ze,rt,Ne,na,ot,ra,bt,Ae,qt,Fe,jt;return g=new xt({}),L=new La({props:{id:"1JvfrvZgi6c"}}),X=new zt({props:{$$slots:{default:[Ba]},$$scope:{ctx:P}}}),I=new xt({}),te=new ne({props:{code:`from datasets import load_dataset

books = load_dataset("opus_books", "en-fr")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>books = load_dataset(<span class="hljs-string">&quot;opus_books&quot;</span>, <span class="hljs-string">&quot;en-fr&quot;</span>)`}}),le=new ne({props:{code:'books = books["train"].train_test_split(test_size=0.2)',highlighted:'books = books[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(test_size=<span class="hljs-number">0.2</span>)'}}),r=new ne({props:{code:'books["train"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>books[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;90560&#x27;</span>,
 <span class="hljs-string">&#x27;translation&#x27;</span>: {<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-string">&#x27;But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.&#x27;</span>,
  <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-string">&#x27;Mais ce plateau \xE9lev\xE9 ne mesurait que quelques toises, et bient\xF4t nous f\xFBmes rentr\xE9s dans notre \xE9l\xE9ment.&#x27;</span>}}`}}),_e=new xt({}),De=new La({props:{id:"XAR8jnZZuUs"}}),Me=new ne({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("t5-small")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-small&quot;</span>)`}}),Oe=new ne({props:{code:`source_lang = "en"
target_lang = "fr"
prefix = "translate English to French: "


def preprocess_function(examples):
    inputs = [prefix + example[source_lang] for example in examples["translation"]]
    targets = [example[target_lang] for example in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=128, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>source_lang = <span class="hljs-string">&quot;en&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_lang = <span class="hljs-string">&quot;fr&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prefix = <span class="hljs-string">&quot;translate English to French: &quot;</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    inputs = [prefix + example[source_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    targets = [example[target_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    model_inputs = tokenizer(inputs, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> tokenizer.as_target_tokenizer():
<span class="hljs-meta">... </span>        labels = tokenizer(targets, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    model_inputs[<span class="hljs-string">&quot;labels&quot;</span>] = labels[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> model_inputs`}}),Ue=new ne({props:{code:"tokenized_books = books.map(preprocess_function, batched=True)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_books = books.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),xe=new Da({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Za],pytorch:[Ya]},$$scope:{ctx:P}}}),Ne=new xt({}),Ae=new Da({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Qa],pytorch:[Ra]},$$scope:{ctx:P}}}),Fe=new zt({props:{$$slots:{default:[Va]},$$scope:{ctx:P}}}),{c(){s=i("meta"),c=_(),n=i("h1"),u=i("a"),w=i("span"),v(g.$$.fragment),q=_(),A=i("span"),z=o("Translation"),j=_(),v(L.$$.fragment),N=_(),B=i("p"),W=o("Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework that extends to vision and audio tasks."),M=_(),Y=i("p"),J=o("This guide will show you how to fine-tune "),Q=i("a"),ue=o("T5"),V=o(" on the English-French subset of the "),C=i("a"),G=o("OPUS Books"),re=o(" dataset to translate English text to French."),ee=_(),v(X.$$.fragment),de=_(),D=i("h2"),O=i("a"),K=i("span"),v(I.$$.fragment),ge=_(),H=i("span"),R=o("Load OPUS Books dataset"),he=_(),d=i("p"),F=o("Load the OPUS Books dataset from the \u{1F917} Datasets library:"),Z=_(),v(te.$$.fragment),U=_(),ae=i("p"),oe=o("Split this dataset into a train and test set:"),se=_(),v(le.$$.fragment),ce=_(),ie=i("p"),ke=o("Then take a look at an example:"),je=_(),v(r.$$.fragment),b=_(),pe=i("p"),We=o("The "),Se=i("code"),Ye=o("translation"),ye=o(" field is a dictionary containing the English and French translations of the text."),Le=_(),fe=i("h2"),we=i("a"),Ee=i("span"),v(_e.$$.fragment),He=_(),Te=i("span"),Ft=o("Preprocess"),ft=_(),v(De.$$.fragment),mt=_(),Ze=i("p"),Pt=o("Load the T5 tokenizer to process the language pairs:"),ht=_(),v(Me.$$.fragment),ct=_(),Je=i("p"),Lt=o("The preprocessing function needs to:"),ut=_(),be=i("ol"),Xe=i("li"),Dt=o("Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks."),Mt=_(),Qe=i("li"),Ct=o("Tokenize the input (English) and target (French) separately. You can\u2019t tokenize French text with a tokenizer pretrained on an English vocabulary. A context manager will help set the tokenizer to French first before tokenizing it."),Ot=_(),Ce=i("li"),It=o("Truncate sequences to be no longer than the maximum length set by the "),Ve=i("code"),Ut=o("max_length"),Nt=o(" parameter."),dt=_(),v(Oe.$$.fragment),_t=_(),$e=i("p"),Bt=o("Use \u{1F917} Datasets "),Ie=i("a"),Wt=o("map"),Yt=o(" function to apply the preprocessing function over the entire dataset. You can speed up the "),et=i("code"),Ht=o("map"),Zt=o(" function by setting "),tt=i("code"),Jt=o("batched=True"),Kt=o(" to process multiple elements of the dataset at once:"),$t=_(),v(Ue.$$.fragment),gt=_(),me=i("p"),Rt=o("Use "),Ke=i("a"),Gt=o("DataCollatorForSeq2Seq"),Xt=o(" to create a batch of examples. It will also "),at=i("em"),Qt=o("dynamically pad"),Vt=o(" your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),st=i("code"),ea=o("tokenizer"),ta=o(" function by setting "),nt=i("code"),aa=o("padding=True"),sa=o(", dynamic padding is more efficient."),kt=_(),v(xe.$$.fragment),wt=_(),ve=i("h2"),ze=i("a"),rt=i("span"),v(Ne.$$.fragment),na=_(),ot=i("span"),ra=o("Train"),bt=_(),v(Ae.$$.fragment),qt=_(),v(Fe.$$.fragment),this.h()},l(e){const h=Ua('[data-svelte="svelte-1phssyn"]',document.head);s=p(h,"META",{name:!0,content:!0}),h.forEach(t),c=$(e),n=p(e,"H1",{class:!0});var Be=f(n);u=p(Be,"A",{id:!0,class:!0,href:!0});var lt=f(u);w=p(lt,"SPAN",{});var it=f(w);S(g.$$.fragment,it),it.forEach(t),lt.forEach(t),q=$(Be),A=p(Be,"SPAN",{});var pt=f(A);z=l(pt,"Translation"),pt.forEach(t),Be.forEach(t),j=$(e),S(L.$$.fragment,e),N=$(e),B=p(e,"P",{});var oa=f(B);W=l(oa,"Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework that extends to vision and audio tasks."),oa.forEach(t),M=$(e),Y=p(e,"P",{});var Re=f(Y);J=l(Re,"This guide will show you how to fine-tune "),Q=p(Re,"A",{href:!0,rel:!0});var la=f(Q);ue=l(la,"T5"),la.forEach(t),V=l(Re," on the English-French subset of the "),C=p(Re,"A",{href:!0,rel:!0});var ia=f(C);G=l(ia,"OPUS Books"),ia.forEach(t),re=l(Re," dataset to translate English text to French."),Re.forEach(t),ee=$(e),S(X.$$.fragment,e),de=$(e),D=p(e,"H2",{class:!0});var vt=f(D);O=p(vt,"A",{id:!0,class:!0,href:!0});var pa=f(O);K=p(pa,"SPAN",{});var fa=f(K);S(I.$$.fragment,fa),fa.forEach(t),pa.forEach(t),ge=$(vt),H=p(vt,"SPAN",{});var ma=f(H);R=l(ma,"Load OPUS Books dataset"),ma.forEach(t),vt.forEach(t),he=$(e),d=p(e,"P",{});var ha=f(d);F=l(ha,"Load the OPUS Books dataset from the \u{1F917} Datasets library:"),ha.forEach(t),Z=$(e),S(te.$$.fragment,e),U=$(e),ae=p(e,"P",{});var ca=f(ae);oe=l(ca,"Split this dataset into a train and test set:"),ca.forEach(t),se=$(e),S(le.$$.fragment,e),ce=$(e),ie=p(e,"P",{});var ua=f(ie);ke=l(ua,"Then take a look at an example:"),ua.forEach(t),je=$(e),S(r.$$.fragment,e),b=$(e),pe=p(e,"P",{});var St=f(pe);We=l(St,"The "),Se=p(St,"CODE",{});var da=f(Se);Ye=l(da,"translation"),da.forEach(t),ye=l(St," field is a dictionary containing the English and French translations of the text."),St.forEach(t),Le=$(e),fe=p(e,"H2",{class:!0});var yt=f(fe);we=p(yt,"A",{id:!0,class:!0,href:!0});var _a=f(we);Ee=p(_a,"SPAN",{});var $a=f(Ee);S(_e.$$.fragment,$a),$a.forEach(t),_a.forEach(t),He=$(yt),Te=p(yt,"SPAN",{});var ga=f(Te);Ft=l(ga,"Preprocess"),ga.forEach(t),yt.forEach(t),ft=$(e),S(De.$$.fragment,e),mt=$(e),Ze=p(e,"P",{});var ka=f(Ze);Pt=l(ka,"Load the T5 tokenizer to process the language pairs:"),ka.forEach(t),ht=$(e),S(Me.$$.fragment,e),ct=$(e),Je=p(e,"P",{});var wa=f(Je);Lt=l(wa,"The preprocessing function needs to:"),wa.forEach(t),ut=$(e),be=p(e,"OL",{});var Ge=f(be);Xe=p(Ge,"LI",{});var ba=f(Xe);Dt=l(ba,"Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks."),ba.forEach(t),Mt=$(Ge),Qe=p(Ge,"LI",{});var qa=f(Qe);Ct=l(qa,"Tokenize the input (English) and target (French) separately. You can\u2019t tokenize French text with a tokenizer pretrained on an English vocabulary. A context manager will help set the tokenizer to French first before tokenizing it."),qa.forEach(t),Ot=$(Ge),Ce=p(Ge,"LI",{});var Et=f(Ce);It=l(Et,"Truncate sequences to be no longer than the maximum length set by the "),Ve=p(Et,"CODE",{});var ja=f(Ve);Ut=l(ja,"max_length"),ja.forEach(t),Nt=l(Et," parameter."),Et.forEach(t),Ge.forEach(t),dt=$(e),S(Oe.$$.fragment,e),_t=$(e),$e=p(e,"P",{});var Pe=f($e);Bt=l(Pe,"Use \u{1F917} Datasets "),Ie=p(Pe,"A",{href:!0,rel:!0});var va=f(Ie);Wt=l(va,"map"),va.forEach(t),Yt=l(Pe," function to apply the preprocessing function over the entire dataset. You can speed up the "),et=p(Pe,"CODE",{});var Sa=f(et);Ht=l(Sa,"map"),Sa.forEach(t),Zt=l(Pe," function by setting "),tt=p(Pe,"CODE",{});var ya=f(tt);Jt=l(ya,"batched=True"),ya.forEach(t),Kt=l(Pe," to process multiple elements of the dataset at once:"),Pe.forEach(t),$t=$(e),S(Ue.$$.fragment,e),gt=$(e),me=p(e,"P",{});var qe=f(me);Rt=l(qe,"Use "),Ke=p(qe,"A",{href:!0});var Ea=f(Ke);Gt=l(Ea,"DataCollatorForSeq2Seq"),Ea.forEach(t),Xt=l(qe," to create a batch of examples. It will also "),at=p(qe,"EM",{});var Ta=f(at);Qt=l(Ta,"dynamically pad"),Ta.forEach(t),Vt=l(qe," your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),st=p(qe,"CODE",{});var xa=f(st);ea=l(xa,"tokenizer"),xa.forEach(t),ta=l(qe," function by setting "),nt=p(qe,"CODE",{});var za=f(nt);aa=l(za,"padding=True"),za.forEach(t),sa=l(qe,", dynamic padding is more efficient."),qe.forEach(t),kt=$(e),S(xe.$$.fragment,e),wt=$(e),ve=p(e,"H2",{class:!0});var Tt=f(ve);ze=p(Tt,"A",{id:!0,class:!0,href:!0});var Aa=f(ze);rt=p(Aa,"SPAN",{});var Fa=f(rt);S(Ne.$$.fragment,Fa),Fa.forEach(t),Aa.forEach(t),na=$(Tt),ot=p(Tt,"SPAN",{});var Pa=f(ot);ra=l(Pa,"Train"),Pa.forEach(t),Tt.forEach(t),bt=$(e),S(Ae.$$.fragment,e),qt=$(e),S(Fe.$$.fragment,e),this.h()},h(){k(s,"name","hf:doc:metadata"),k(s,"content",JSON.stringify(ts)),k(u,"id","translation"),k(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(u,"href","#translation"),k(n,"class","relative group"),k(Q,"href","https://huggingface.co/t5-small"),k(Q,"rel","nofollow"),k(C,"href","https://huggingface.co/datasets/opus_books"),k(C,"rel","nofollow"),k(O,"id","load-opus-books-dataset"),k(O,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(O,"href","#load-opus-books-dataset"),k(D,"class","relative group"),k(we,"id","preprocess"),k(we,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(we,"href","#preprocess"),k(fe,"class","relative group"),k(Ie,"href","https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map"),k(Ie,"rel","nofollow"),k(Ke,"href","/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq"),k(ze,"id","train"),k(ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),k(ze,"href","#train"),k(ve,"class","relative group")},m(e,h){a(document.head,s),m(e,c,h),m(e,n,h),a(n,u),a(u,w),y(g,w,null),a(n,q),a(n,A),a(A,z),m(e,j,h),y(L,e,h),m(e,N,h),m(e,B,h),a(B,W),m(e,M,h),m(e,Y,h),a(Y,J),a(Y,Q),a(Q,ue),a(Y,V),a(Y,C),a(C,G),a(Y,re),m(e,ee,h),y(X,e,h),m(e,de,h),m(e,D,h),a(D,O),a(O,K),y(I,K,null),a(D,ge),a(D,H),a(H,R),m(e,he,h),m(e,d,h),a(d,F),m(e,Z,h),y(te,e,h),m(e,U,h),m(e,ae,h),a(ae,oe),m(e,se,h),y(le,e,h),m(e,ce,h),m(e,ie,h),a(ie,ke),m(e,je,h),y(r,e,h),m(e,b,h),m(e,pe,h),a(pe,We),a(pe,Se),a(Se,Ye),a(pe,ye),m(e,Le,h),m(e,fe,h),a(fe,we),a(we,Ee),y(_e,Ee,null),a(fe,He),a(fe,Te),a(Te,Ft),m(e,ft,h),y(De,e,h),m(e,mt,h),m(e,Ze,h),a(Ze,Pt),m(e,ht,h),y(Me,e,h),m(e,ct,h),m(e,Je,h),a(Je,Lt),m(e,ut,h),m(e,be,h),a(be,Xe),a(Xe,Dt),a(be,Mt),a(be,Qe),a(Qe,Ct),a(be,Ot),a(be,Ce),a(Ce,It),a(Ce,Ve),a(Ve,Ut),a(Ce,Nt),m(e,dt,h),y(Oe,e,h),m(e,_t,h),m(e,$e,h),a($e,Bt),a($e,Ie),a(Ie,Wt),a($e,Yt),a($e,et),a(et,Ht),a($e,Zt),a($e,tt),a(tt,Jt),a($e,Kt),m(e,$t,h),y(Ue,e,h),m(e,gt,h),m(e,me,h),a(me,Rt),a(me,Ke),a(Ke,Gt),a(me,Xt),a(me,at),a(at,Qt),a(me,Vt),a(me,st),a(st,ea),a(me,ta),a(me,nt),a(nt,aa),a(me,sa),m(e,kt,h),y(xe,e,h),m(e,wt,h),m(e,ve,h),a(ve,ze),a(ze,rt),y(Ne,rt,null),a(ve,na),a(ve,ot),a(ot,ra),m(e,bt,h),y(Ae,e,h),m(e,qt,h),y(Fe,e,h),jt=!0},p(e,[h]){const Be={};h&2&&(Be.$$scope={dirty:h,ctx:e}),X.$set(Be);const lt={};h&2&&(lt.$$scope={dirty:h,ctx:e}),xe.$set(lt);const it={};h&2&&(it.$$scope={dirty:h,ctx:e}),Ae.$set(it);const pt={};h&2&&(pt.$$scope={dirty:h,ctx:e}),Fe.$set(pt)},i(e){jt||(E(g.$$.fragment,e),E(L.$$.fragment,e),E(X.$$.fragment,e),E(I.$$.fragment,e),E(te.$$.fragment,e),E(le.$$.fragment,e),E(r.$$.fragment,e),E(_e.$$.fragment,e),E(De.$$.fragment,e),E(Me.$$.fragment,e),E(Oe.$$.fragment,e),E(Ue.$$.fragment,e),E(xe.$$.fragment,e),E(Ne.$$.fragment,e),E(Ae.$$.fragment,e),E(Fe.$$.fragment,e),jt=!0)},o(e){T(g.$$.fragment,e),T(L.$$.fragment,e),T(X.$$.fragment,e),T(I.$$.fragment,e),T(te.$$.fragment,e),T(le.$$.fragment,e),T(r.$$.fragment,e),T(_e.$$.fragment,e),T(De.$$.fragment,e),T(Me.$$.fragment,e),T(Oe.$$.fragment,e),T(Ue.$$.fragment,e),T(xe.$$.fragment,e),T(Ne.$$.fragment,e),T(Ae.$$.fragment,e),T(Fe.$$.fragment,e),jt=!1},d(e){t(s),e&&t(c),e&&t(n),x(g),e&&t(j),x(L,e),e&&t(N),e&&t(B),e&&t(M),e&&t(Y),e&&t(ee),x(X,e),e&&t(de),e&&t(D),x(I),e&&t(he),e&&t(d),e&&t(Z),x(te,e),e&&t(U),e&&t(ae),e&&t(se),x(le,e),e&&t(ce),e&&t(ie),e&&t(je),x(r,e),e&&t(b),e&&t(pe),e&&t(Le),e&&t(fe),x(_e),e&&t(ft),x(De,e),e&&t(mt),e&&t(Ze),e&&t(ht),x(Me,e),e&&t(ct),e&&t(Je),e&&t(ut),e&&t(be),e&&t(dt),x(Oe,e),e&&t(_t),e&&t($e),e&&t($t),x(Ue,e),e&&t(gt),e&&t(me),e&&t(kt),x(xe,e),e&&t(wt),e&&t(ve),x(Ne),e&&t(bt),x(Ae,e),e&&t(qt),x(Fe,e)}}}const ts={local:"translation",sections:[{local:"load-opus-books-dataset",title:"Load OPUS Books dataset"},{local:"preprocess",title:"Preprocess"},{local:"train",title:"Train"}],title:"Translation"};function as(P){return Na(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ps extends Ca{constructor(s){super();Oa(this,s,as,es,Ia,{})}}export{ps as default,ts as metadata};
