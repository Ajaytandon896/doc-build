import{S as Xo,i as Zo,s as en,e as l,k as h,w as k,t as o,M as tn,c as r,d as a,m as c,a as i,x as y,h as n,b as m,F as t,g as p,y as j,q as E,o as T,B as x}from"../../chunks/vendor-4833417e.js";import{T as xt}from"../../chunks/Tip-fffd6df1.js";import{Y as an}from"../../chunks/Youtube-27813aed.js";import{I as zt}from"../../chunks/IconCopyLink-4b81c553.js";import{C as F}from"../../chunks/CodeBlock-6a3d1b46.js";import{C as sn}from"../../chunks/CodeBlockFw-27a176a0.js";import"../../chunks/CopyButton-dacfbfaf.js";function on(C){let f,g,d,_,b;return{c(){f=l("p"),g=o("See the text classification "),d=l("a"),_=o("task page"),b=o(" for more information about other forms of text classification and their associated models, datasets, and metrics."),this.h()},l(u){f=r(u,"P",{});var $=i(f);g=n($,"See the text classification "),d=r($,"A",{href:!0,rel:!0});var w=i(d);_=n(w,"task page"),w.forEach(a),b=n($," for more information about other forms of text classification and their associated models, datasets, and metrics."),$.forEach(a),this.h()},h(){m(d,"href","https://huggingface.co/tasks/text-classification"),m(d,"rel","nofollow")},m(u,$){p(u,f,$),t(f,g),t(f,d),t(d,_),t(f,b)},d(u){u&&a(f)}}}function nn(C){let f,g,d,_,b,u,$,w;return{c(){f=l("p"),g=o("If you aren\u2019t familiar with fine-tuning a model with the "),d=l("a"),_=o("Trainer"),b=o(", take a look at the basic tutorial "),u=l("a"),$=o("here"),w=o("!"),this.h()},l(z){f=r(z,"P",{});var v=i(f);g=n(v,"If you aren\u2019t familiar with fine-tuning a model with the "),d=r(v,"A",{href:!0});var q=i(d);_=n(q,"Trainer"),q.forEach(a),b=n(v,", take a look at the basic tutorial "),u=r(v,"A",{href:!0});var S=i(u);$=n(S,"here"),S.forEach(a),w=n(v,"!"),v.forEach(a),this.h()},h(){m(d,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Trainer"),m(u,"href","training#finetune-with-trainer")},m(z,v){p(z,f,v),t(f,g),t(f,d),t(d,_),t(f,b),t(f,u),t(u,$),t(f,w)},d(z){z&&a(f)}}}function ln(C){let f,g,d,_,b,u,$;return{c(){f=l("p"),g=l("a"),d=o("Trainer"),_=o(" will apply dynamic padding by default when you pass "),b=l("code"),u=o("tokenizer"),$=o(" to it. In this case, you don\u2019t need to specify a data collator explicitly."),this.h()},l(w){f=r(w,"P",{});var z=i(f);g=r(z,"A",{href:!0});var v=i(g);d=n(v,"Trainer"),v.forEach(a),_=n(z," will apply dynamic padding by default when you pass "),b=r(z,"CODE",{});var q=i(b);u=n(q,"tokenizer"),q.forEach(a),$=n(z," to it. In this case, you don\u2019t need to specify a data collator explicitly."),z.forEach(a),this.h()},h(){m(g,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Trainer")},m(w,z){p(w,f,z),t(f,g),t(g,d),t(f,_),t(f,b),t(b,u),t(f,$)},d(w){w&&a(f)}}}function rn(C){let f,g,d,_,b;return{c(){f=l("p"),g=o("If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),d=l("a"),_=o("here"),b=o("!"),this.h()},l(u){f=r(u,"P",{});var $=i(f);g=n($,"If you aren\u2019t familiar with fine-tuning a model with Keras, take a look at the basic tutorial "),d=r($,"A",{href:!0});var w=i(d);_=n(w,"here"),w.forEach(a),b=n($,"!"),$.forEach(a),this.h()},h(){m(d,"href","training#finetune-with-keras")},m(u,$){p(u,f,$),t(f,g),t(f,d),t(d,_),t(f,b)},d(u){u&&a(f)}}}function pn(C){let f,g,d,_,b,u,$,w;return{c(){f=l("p"),g=o(`For a more in-depth example of how to fine-tune a model for text classification, take a look at the corresponding
`),d=l("a"),_=o("PyTorch notebook"),b=o(`
or `),u=l("a"),$=o("TensorFlow notebook"),w=o("."),this.h()},l(z){f=r(z,"P",{});var v=i(f);g=n(v,`For a more in-depth example of how to fine-tune a model for text classification, take a look at the corresponding
`),d=r(v,"A",{href:!0,rel:!0});var q=i(d);_=n(q,"PyTorch notebook"),q.forEach(a),b=n(v,`
or `),u=r(v,"A",{href:!0,rel:!0});var S=i(u);$=n(S,"TensorFlow notebook"),S.forEach(a),w=n(v,"."),v.forEach(a),this.h()},h(){m(d,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb"),m(d,"rel","nofollow"),m(u,"href","https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification-tf.ipynb"),m(u,"rel","nofollow")},m(z,v){p(z,f,v),t(f,g),t(f,d),t(d,_),t(f,b),t(f,u),t(u,$),t(f,w)},d(z){z&&a(f)}}}function fn(C){let f,g,d,_,b,u,$,w,z,v,q,S,Ie,Aa,qt,I,Ca,ie,Da,Pa,pe,Fa,Sa,At,U,Ct,N,H,Xe,fe,Ia,Ze,Ma,Dt,Me,Oa,Pt,he,Ft,Oe,La,St,ce,It,Le,Na,Mt,Y,Ne,et,Ba,Wa,Ra,M,tt,Ua,Ha,at,Ya,Ga,st,Ka,Va,Ot,B,G,ot,me,Ja,nt,Qa,Lt,K,Xa,lt,Za,es,Nt,de,Bt,V,ts,rt,as,ss,Wt,ue,Rt,D,os,_e,it,ns,ls,pt,rs,is,ft,ps,fs,Ut,ge,Ht,A,hs,Be,cs,ms,ht,ds,us,ct,_s,gs,mt,$s,bs,Yt,$e,Gt,W,J,dt,be,ws,ut,vs,Kt,Q,ks,We,ys,js,Vt,we,Jt,X,Qt,Re,Es,Xt,O,ve,Ts,Ue,xs,zs,qs,ke,As,He,Cs,Ds,Ps,ye,Fs,Ye,Ss,Is,Zt,je,ea,Z,ta,R,ee,_t,Ee,Ms,gt,Os,aa,Ge,Ls,sa,te,oa,P,Ns,$t,Bs,Ws,Te,bt,Rs,Us,wt,Hs,Ys,na,xe,la,Ke,Gs,ra,ze,ia,ae,Ks,Ve,Vs,Js,pa,qe,fa,se,Qs,Ae,vt,Xs,Zs,ha,Ce,ca,oe,eo,De,kt,to,ao,ma,Pe,da,ne,ua;return u=new zt({}),q=new an({props:{id:"leNG9fN9FQU"}}),U=new xt({props:{$$slots:{default:[on]},$$scope:{ctx:C}}}),fe=new zt({}),he=new F({props:{code:`from datasets import load_dataset

imdb = load_dataset("imdb")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>imdb = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)`}}),ce=new F({props:{code:'imdb["test"][0]',highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>imdb[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-number">0</span>]
{
    <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn&#x27;t match the background, and painfully one-dimensional characters cannot be overcome with a &#x27;sci-fi&#x27; setting. (I&#x27;m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It&#x27;s not. It&#x27;s clich\xE9d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It&#x27;s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it&#x27;s rubbish as they have to always say \\&quot;Gene Roddenberry&#x27;s Earth...\\&quot; otherwise people would not continue watching. Roddenberry&#x27;s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.&quot;</span>,
}`}}),me=new zt({}),de=new F({props:{code:`from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>)`}}),ue=new F({props:{code:`def preprocess_function(examples):
    return tokenizer(examples["text"], truncation=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], truncation=<span class="hljs-literal">True</span>)`}}),ge=new F({props:{code:"tokenized_imdb = imdb.map(preprocess_function, batched=True)",highlighted:'tokenized_imdb = imdb.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)'}}),$e=new sn({props:{group1:{id:"pt",code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`},group2:{id:"tf",code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}}),be=new zt({}),we=new F({props:{code:`from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer

model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),X=new xt({props:{$$slots:{default:[nn]},$$scope:{ctx:C}}}),je=new F({props:{code:`training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_imdb["train"],
    eval_dataset=tokenized_imdb["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

trainer.train()`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">5</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`}}),Z=new xt({props:{$$slots:{default:[ln]},$$scope:{ctx:C}}}),Ee=new zt({}),te=new xt({props:{$$slots:{default:[rn]},$$scope:{ctx:C}}}),xe=new F({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`}}),ze=new F({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
<span class="hljs-meta">&gt;&gt;&gt; </span>total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),qe=new F({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),Ce=new F({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),Pe=new F({props:{code:"model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=<span class="hljs-number">3</span>)'}}),ne=new xt({props:{$$slots:{default:[pn]},$$scope:{ctx:C}}}),{c(){f=l("meta"),g=h(),d=l("h1"),_=l("a"),b=l("span"),k(u.$$.fragment),$=h(),w=l("span"),z=o("Text classification"),v=h(),k(q.$$.fragment),S=h(),Ie=l("p"),Aa=o("Text classification is a common NLP task that assigns a label or class to text. There are many practical applications of text classification widely used in production by some of today\u2019s largest companies. One of the most popular forms of text classification is sentiment analysis, which assigns a label like positive, negative, or neutral to a sequence of text."),qt=h(),I=l("p"),Ca=o("This guide will show you how to fine-tune "),ie=l("a"),Da=o("DistilBERT"),Pa=o(" on the "),pe=l("a"),Fa=o("IMDb"),Sa=o(" dataset to determine whether a movie review is positive or negative."),At=h(),k(U.$$.fragment),Ct=h(),N=l("h2"),H=l("a"),Xe=l("span"),k(fe.$$.fragment),Ia=h(),Ze=l("span"),Ma=o("Load IMDb dataset"),Dt=h(),Me=l("p"),Oa=o("Load the IMDb dataset from the \u{1F917} Datasets library:"),Pt=h(),k(he.$$.fragment),Ft=h(),Oe=l("p"),La=o("Then take a look at an example:"),St=h(),k(ce.$$.fragment),It=h(),Le=l("p"),Na=o("There are two fields in this dataset:"),Mt=h(),Y=l("ul"),Ne=l("li"),et=l("code"),Ba=o("text"),Wa=o(": a string containing the text of the movie review."),Ra=h(),M=l("li"),tt=l("code"),Ua=o("label"),Ha=o(": a value that can either be "),at=l("code"),Ya=o("0"),Ga=o(" for a negative review or "),st=l("code"),Ka=o("1"),Va=o(" for a positive review."),Ot=h(),B=l("h2"),G=l("a"),ot=l("span"),k(me.$$.fragment),Ja=h(),nt=l("span"),Qa=o("Preprocess"),Lt=h(),K=l("p"),Xa=o("Load the DistilBERT tokenizer to process the "),lt=l("code"),Za=o("text"),es=o(" field:"),Nt=h(),k(de.$$.fragment),Bt=h(),V=l("p"),ts=o("Create a preprocessing function to tokenize "),rt=l("code"),as=o("text"),ss=o(" and truncate sequences to be no longer than DistilBERT\u2019s maximum input length:"),Wt=h(),k(ue.$$.fragment),Rt=h(),D=l("p"),os=o("Use \u{1F917} Datasets "),_e=l("a"),it=l("code"),ns=o("map"),ls=o(" function to apply the preprocessing function over the entire dataset. You can speed up the "),pt=l("code"),rs=o("map"),is=o(" function by setting "),ft=l("code"),ps=o("batched=True"),fs=o(" to process multiple elements of the dataset at once:"),Ut=h(),k(ge.$$.fragment),Ht=h(),A=l("p"),hs=o("Use "),Be=l("a"),cs=o("DataCollatorWithPadding"),ms=o(" to create a batch of examples. It will also "),ht=l("em"),ds=o("dynamically pad"),us=o(" your text to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),ct=l("code"),_s=o("tokenizer"),gs=o(" function by setting "),mt=l("code"),$s=o("padding=True"),bs=o(", dynamic padding is more efficient."),Yt=h(),k($e.$$.fragment),Gt=h(),W=l("h2"),J=l("a"),dt=l("span"),k(be.$$.fragment),ws=h(),ut=l("span"),vs=o("Fine-tune with Trainer"),Kt=h(),Q=l("p"),ks=o("Load DistilBERT with "),We=l("a"),ys=o("AutoModelForSequenceClassification"),js=o(" along with the number of expected labels:"),Vt=h(),k(we.$$.fragment),Jt=h(),k(X.$$.fragment),Qt=h(),Re=l("p"),Es=o("At this point, only three steps remain:"),Xt=h(),O=l("ol"),ve=l("li"),Ts=o("Define your training hyperparameters in "),Ue=l("a"),xs=o("TrainingArguments"),zs=o("."),qs=h(),ke=l("li"),As=o("Pass the training arguments to "),He=l("a"),Cs=o("Trainer"),Ds=o(" along with the model, dataset, tokenizer, and data collator."),Ps=h(),ye=l("li"),Fs=o("Call "),Ye=l("a"),Ss=o("train()"),Is=o(" to fine-tune your model."),Zt=h(),k(je.$$.fragment),ea=h(),k(Z.$$.fragment),ta=h(),R=l("h2"),ee=l("a"),_t=l("span"),k(Ee.$$.fragment),Ms=h(),gt=l("span"),Os=o("Fine-tune with TensorFlow"),aa=h(),Ge=l("p"),Ls=o("To fine-tune a model in TensorFlow is just as easy, with only a few differences."),sa=h(),k(te.$$.fragment),oa=h(),P=l("p"),Ns=o("Convert your datasets to the "),$t=l("code"),Bs=o("tf.data.Dataset"),Ws=o(" format with "),Te=l("a"),bt=l("code"),Rs=o("to_tf_dataset"),Us=o(". Specify inputs and labels in "),wt=l("code"),Hs=o("columns"),Ys=o(", whether to shuffle the dataset order, batch size, and the data collator:"),na=h(),k(xe.$$.fragment),la=h(),Ke=l("p"),Gs=o("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),ra=h(),k(ze.$$.fragment),ia=h(),ae=l("p"),Ks=o("Load DistilBERT with "),Ve=l("a"),Vs=o("TFAutoModelForSequenceClassification"),Js=o(" along with the number of expected labels:"),pa=h(),k(qe.$$.fragment),fa=h(),se=l("p"),Qs=o("Configure the model for training with "),Ae=l("a"),vt=l("code"),Xs=o("compile"),Zs=o(":"),ha=h(),k(Ce.$$.fragment),ca=h(),oe=l("p"),eo=o("Call "),De=l("a"),kt=l("code"),to=o("fit"),ao=o(" to fine-tune the model:"),ma=h(),k(Pe.$$.fragment),da=h(),k(ne.$$.fragment),this.h()},l(e){const s=tn('[data-svelte="svelte-1phssyn"]',document.head);f=r(s,"META",{name:!0,content:!0}),s.forEach(a),g=c(e),d=r(e,"H1",{class:!0});var Fe=i(d);_=r(Fe,"A",{id:!0,class:!0,href:!0});var yt=i(_);b=r(yt,"SPAN",{});var jt=i(b);y(u.$$.fragment,jt),jt.forEach(a),yt.forEach(a),$=c(Fe),w=r(Fe,"SPAN",{});var Et=i(w);z=n(Et,"Text classification"),Et.forEach(a),Fe.forEach(a),v=c(e),y(q.$$.fragment,e),S=c(e),Ie=r(e,"P",{});var Tt=i(Ie);Aa=n(Tt,"Text classification is a common NLP task that assigns a label or class to text. There are many practical applications of text classification widely used in production by some of today\u2019s largest companies. One of the most popular forms of text classification is sentiment analysis, which assigns a label like positive, negative, or neutral to a sequence of text."),Tt.forEach(a),qt=c(e),I=r(e,"P",{});var Je=i(I);Ca=n(Je,"This guide will show you how to fine-tune "),ie=r(Je,"A",{href:!0,rel:!0});var oo=i(ie);Da=n(oo,"DistilBERT"),oo.forEach(a),Pa=n(Je," on the "),pe=r(Je,"A",{href:!0,rel:!0});var no=i(pe);Fa=n(no,"IMDb"),no.forEach(a),Sa=n(Je," dataset to determine whether a movie review is positive or negative."),Je.forEach(a),At=c(e),y(U.$$.fragment,e),Ct=c(e),N=r(e,"H2",{class:!0});var _a=i(N);H=r(_a,"A",{id:!0,class:!0,href:!0});var lo=i(H);Xe=r(lo,"SPAN",{});var ro=i(Xe);y(fe.$$.fragment,ro),ro.forEach(a),lo.forEach(a),Ia=c(_a),Ze=r(_a,"SPAN",{});var io=i(Ze);Ma=n(io,"Load IMDb dataset"),io.forEach(a),_a.forEach(a),Dt=c(e),Me=r(e,"P",{});var po=i(Me);Oa=n(po,"Load the IMDb dataset from the \u{1F917} Datasets library:"),po.forEach(a),Pt=c(e),y(he.$$.fragment,e),Ft=c(e),Oe=r(e,"P",{});var fo=i(Oe);La=n(fo,"Then take a look at an example:"),fo.forEach(a),St=c(e),y(ce.$$.fragment,e),It=c(e),Le=r(e,"P",{});var ho=i(Le);Na=n(ho,"There are two fields in this dataset:"),ho.forEach(a),Mt=c(e),Y=r(e,"UL",{});var ga=i(Y);Ne=r(ga,"LI",{});var so=i(Ne);et=r(so,"CODE",{});var co=i(et);Ba=n(co,"text"),co.forEach(a),Wa=n(so,": a string containing the text of the movie review."),so.forEach(a),Ra=c(ga),M=r(ga,"LI",{});var Se=i(M);tt=r(Se,"CODE",{});var mo=i(tt);Ua=n(mo,"label"),mo.forEach(a),Ha=n(Se,": a value that can either be "),at=r(Se,"CODE",{});var uo=i(at);Ya=n(uo,"0"),uo.forEach(a),Ga=n(Se," for a negative review or "),st=r(Se,"CODE",{});var _o=i(st);Ka=n(_o,"1"),_o.forEach(a),Va=n(Se," for a positive review."),Se.forEach(a),ga.forEach(a),Ot=c(e),B=r(e,"H2",{class:!0});var $a=i(B);G=r($a,"A",{id:!0,class:!0,href:!0});var go=i(G);ot=r(go,"SPAN",{});var $o=i(ot);y(me.$$.fragment,$o),$o.forEach(a),go.forEach(a),Ja=c($a),nt=r($a,"SPAN",{});var bo=i(nt);Qa=n(bo,"Preprocess"),bo.forEach(a),$a.forEach(a),Lt=c(e),K=r(e,"P",{});var ba=i(K);Xa=n(ba,"Load the DistilBERT tokenizer to process the "),lt=r(ba,"CODE",{});var wo=i(lt);Za=n(wo,"text"),wo.forEach(a),es=n(ba," field:"),ba.forEach(a),Nt=c(e),y(de.$$.fragment,e),Bt=c(e),V=r(e,"P",{});var wa=i(V);ts=n(wa,"Create a preprocessing function to tokenize "),rt=r(wa,"CODE",{});var vo=i(rt);as=n(vo,"text"),vo.forEach(a),ss=n(wa," and truncate sequences to be no longer than DistilBERT\u2019s maximum input length:"),wa.forEach(a),Wt=c(e),y(ue.$$.fragment,e),Rt=c(e),D=r(e,"P",{});var le=i(D);os=n(le,"Use \u{1F917} Datasets "),_e=r(le,"A",{href:!0,rel:!0});var ko=i(_e);it=r(ko,"CODE",{});var yo=i(it);ns=n(yo,"map"),yo.forEach(a),ko.forEach(a),ls=n(le," function to apply the preprocessing function over the entire dataset. You can speed up the "),pt=r(le,"CODE",{});var jo=i(pt);rs=n(jo,"map"),jo.forEach(a),is=n(le," function by setting "),ft=r(le,"CODE",{});var Eo=i(ft);ps=n(Eo,"batched=True"),Eo.forEach(a),fs=n(le," to process multiple elements of the dataset at once:"),le.forEach(a),Ut=c(e),y(ge.$$.fragment,e),Ht=c(e),A=r(e,"P",{});var L=i(A);hs=n(L,"Use "),Be=r(L,"A",{href:!0});var To=i(Be);cs=n(To,"DataCollatorWithPadding"),To.forEach(a),ms=n(L," to create a batch of examples. It will also "),ht=r(L,"EM",{});var xo=i(ht);ds=n(xo,"dynamically pad"),xo.forEach(a),us=n(L," your text to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the "),ct=r(L,"CODE",{});var zo=i(ct);_s=n(zo,"tokenizer"),zo.forEach(a),gs=n(L," function by setting "),mt=r(L,"CODE",{});var qo=i(mt);$s=n(qo,"padding=True"),qo.forEach(a),bs=n(L,", dynamic padding is more efficient."),L.forEach(a),Yt=c(e),y($e.$$.fragment,e),Gt=c(e),W=r(e,"H2",{class:!0});var va=i(W);J=r(va,"A",{id:!0,class:!0,href:!0});var Ao=i(J);dt=r(Ao,"SPAN",{});var Co=i(dt);y(be.$$.fragment,Co),Co.forEach(a),Ao.forEach(a),ws=c(va),ut=r(va,"SPAN",{});var Do=i(ut);vs=n(Do,"Fine-tune with Trainer"),Do.forEach(a),va.forEach(a),Kt=c(e),Q=r(e,"P",{});var ka=i(Q);ks=n(ka,"Load DistilBERT with "),We=r(ka,"A",{href:!0});var Po=i(We);ys=n(Po,"AutoModelForSequenceClassification"),Po.forEach(a),js=n(ka," along with the number of expected labels:"),ka.forEach(a),Vt=c(e),y(we.$$.fragment,e),Jt=c(e),y(X.$$.fragment,e),Qt=c(e),Re=r(e,"P",{});var Fo=i(Re);Es=n(Fo,"At this point, only three steps remain:"),Fo.forEach(a),Xt=c(e),O=r(e,"OL",{});var Qe=i(O);ve=r(Qe,"LI",{});var ya=i(ve);Ts=n(ya,"Define your training hyperparameters in "),Ue=r(ya,"A",{href:!0});var So=i(Ue);xs=n(So,"TrainingArguments"),So.forEach(a),zs=n(ya,"."),ya.forEach(a),qs=c(Qe),ke=r(Qe,"LI",{});var ja=i(ke);As=n(ja,"Pass the training arguments to "),He=r(ja,"A",{href:!0});var Io=i(He);Cs=n(Io,"Trainer"),Io.forEach(a),Ds=n(ja," along with the model, dataset, tokenizer, and data collator."),ja.forEach(a),Ps=c(Qe),ye=r(Qe,"LI",{});var Ea=i(ye);Fs=n(Ea,"Call "),Ye=r(Ea,"A",{href:!0});var Mo=i(Ye);Ss=n(Mo,"train()"),Mo.forEach(a),Is=n(Ea," to fine-tune your model."),Ea.forEach(a),Qe.forEach(a),Zt=c(e),y(je.$$.fragment,e),ea=c(e),y(Z.$$.fragment,e),ta=c(e),R=r(e,"H2",{class:!0});var Ta=i(R);ee=r(Ta,"A",{id:!0,class:!0,href:!0});var Oo=i(ee);_t=r(Oo,"SPAN",{});var Lo=i(_t);y(Ee.$$.fragment,Lo),Lo.forEach(a),Oo.forEach(a),Ms=c(Ta),gt=r(Ta,"SPAN",{});var No=i(gt);Os=n(No,"Fine-tune with TensorFlow"),No.forEach(a),Ta.forEach(a),aa=c(e),Ge=r(e,"P",{});var Bo=i(Ge);Ls=n(Bo,"To fine-tune a model in TensorFlow is just as easy, with only a few differences."),Bo.forEach(a),sa=c(e),y(te.$$.fragment,e),oa=c(e),P=r(e,"P",{});var re=i(P);Ns=n(re,"Convert your datasets to the "),$t=r(re,"CODE",{});var Wo=i($t);Bs=n(Wo,"tf.data.Dataset"),Wo.forEach(a),Ws=n(re," format with "),Te=r(re,"A",{href:!0,rel:!0});var Ro=i(Te);bt=r(Ro,"CODE",{});var Uo=i(bt);Rs=n(Uo,"to_tf_dataset"),Uo.forEach(a),Ro.forEach(a),Us=n(re,". Specify inputs and labels in "),wt=r(re,"CODE",{});var Ho=i(wt);Hs=n(Ho,"columns"),Ho.forEach(a),Ys=n(re,", whether to shuffle the dataset order, batch size, and the data collator:"),re.forEach(a),na=c(e),y(xe.$$.fragment,e),la=c(e),Ke=r(e,"P",{});var Yo=i(Ke);Gs=n(Yo,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Yo.forEach(a),ra=c(e),y(ze.$$.fragment,e),ia=c(e),ae=r(e,"P",{});var xa=i(ae);Ks=n(xa,"Load DistilBERT with "),Ve=r(xa,"A",{href:!0});var Go=i(Ve);Vs=n(Go,"TFAutoModelForSequenceClassification"),Go.forEach(a),Js=n(xa," along with the number of expected labels:"),xa.forEach(a),pa=c(e),y(qe.$$.fragment,e),fa=c(e),se=r(e,"P",{});var za=i(se);Qs=n(za,"Configure the model for training with "),Ae=r(za,"A",{href:!0,rel:!0});var Ko=i(Ae);vt=r(Ko,"CODE",{});var Vo=i(vt);Xs=n(Vo,"compile"),Vo.forEach(a),Ko.forEach(a),Zs=n(za,":"),za.forEach(a),ha=c(e),y(Ce.$$.fragment,e),ca=c(e),oe=r(e,"P",{});var qa=i(oe);eo=n(qa,"Call "),De=r(qa,"A",{href:!0,rel:!0});var Jo=i(De);kt=r(Jo,"CODE",{});var Qo=i(kt);to=n(Qo,"fit"),Qo.forEach(a),Jo.forEach(a),ao=n(qa," to fine-tune the model:"),qa.forEach(a),ma=c(e),y(Pe.$$.fragment,e),da=c(e),y(ne.$$.fragment,e),this.h()},h(){m(f,"name","hf:doc:metadata"),m(f,"content",JSON.stringify(hn)),m(_,"id","text-classification"),m(_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(_,"href","#text-classification"),m(d,"class","relative group"),m(ie,"href","https://huggingface.co/distilbert-base-uncased"),m(ie,"rel","nofollow"),m(pe,"href","https://huggingface.co/datasets/imdb"),m(pe,"rel","nofollow"),m(H,"id","load-imdb-dataset"),m(H,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(H,"href","#load-imdb-dataset"),m(N,"class","relative group"),m(G,"id","preprocess"),m(G,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(G,"href","#preprocess"),m(B,"class","relative group"),m(_e,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map"),m(_e,"rel","nofollow"),m(Be,"href","/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),m(J,"id","finetune-with-trainer"),m(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(J,"href","#finetune-with-trainer"),m(W,"class","relative group"),m(We,"href","/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification"),m(Ue,"href","/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments"),m(He,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Trainer"),m(Ye,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train"),m(ee,"id","finetune-with-tensorflow"),m(ee,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ee,"href","#finetune-with-tensorflow"),m(R,"class","relative group"),m(Te,"href","https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset"),m(Te,"rel","nofollow"),m(Ve,"href","/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification"),m(Ae,"href","https://keras.io/api/models/model_training_apis/#compile-method"),m(Ae,"rel","nofollow"),m(De,"href","https://keras.io/api/models/model_training_apis/#fit-method"),m(De,"rel","nofollow")},m(e,s){t(document.head,f),p(e,g,s),p(e,d,s),t(d,_),t(_,b),j(u,b,null),t(d,$),t(d,w),t(w,z),p(e,v,s),j(q,e,s),p(e,S,s),p(e,Ie,s),t(Ie,Aa),p(e,qt,s),p(e,I,s),t(I,Ca),t(I,ie),t(ie,Da),t(I,Pa),t(I,pe),t(pe,Fa),t(I,Sa),p(e,At,s),j(U,e,s),p(e,Ct,s),p(e,N,s),t(N,H),t(H,Xe),j(fe,Xe,null),t(N,Ia),t(N,Ze),t(Ze,Ma),p(e,Dt,s),p(e,Me,s),t(Me,Oa),p(e,Pt,s),j(he,e,s),p(e,Ft,s),p(e,Oe,s),t(Oe,La),p(e,St,s),j(ce,e,s),p(e,It,s),p(e,Le,s),t(Le,Na),p(e,Mt,s),p(e,Y,s),t(Y,Ne),t(Ne,et),t(et,Ba),t(Ne,Wa),t(Y,Ra),t(Y,M),t(M,tt),t(tt,Ua),t(M,Ha),t(M,at),t(at,Ya),t(M,Ga),t(M,st),t(st,Ka),t(M,Va),p(e,Ot,s),p(e,B,s),t(B,G),t(G,ot),j(me,ot,null),t(B,Ja),t(B,nt),t(nt,Qa),p(e,Lt,s),p(e,K,s),t(K,Xa),t(K,lt),t(lt,Za),t(K,es),p(e,Nt,s),j(de,e,s),p(e,Bt,s),p(e,V,s),t(V,ts),t(V,rt),t(rt,as),t(V,ss),p(e,Wt,s),j(ue,e,s),p(e,Rt,s),p(e,D,s),t(D,os),t(D,_e),t(_e,it),t(it,ns),t(D,ls),t(D,pt),t(pt,rs),t(D,is),t(D,ft),t(ft,ps),t(D,fs),p(e,Ut,s),j(ge,e,s),p(e,Ht,s),p(e,A,s),t(A,hs),t(A,Be),t(Be,cs),t(A,ms),t(A,ht),t(ht,ds),t(A,us),t(A,ct),t(ct,_s),t(A,gs),t(A,mt),t(mt,$s),t(A,bs),p(e,Yt,s),j($e,e,s),p(e,Gt,s),p(e,W,s),t(W,J),t(J,dt),j(be,dt,null),t(W,ws),t(W,ut),t(ut,vs),p(e,Kt,s),p(e,Q,s),t(Q,ks),t(Q,We),t(We,ys),t(Q,js),p(e,Vt,s),j(we,e,s),p(e,Jt,s),j(X,e,s),p(e,Qt,s),p(e,Re,s),t(Re,Es),p(e,Xt,s),p(e,O,s),t(O,ve),t(ve,Ts),t(ve,Ue),t(Ue,xs),t(ve,zs),t(O,qs),t(O,ke),t(ke,As),t(ke,He),t(He,Cs),t(ke,Ds),t(O,Ps),t(O,ye),t(ye,Fs),t(ye,Ye),t(Ye,Ss),t(ye,Is),p(e,Zt,s),j(je,e,s),p(e,ea,s),j(Z,e,s),p(e,ta,s),p(e,R,s),t(R,ee),t(ee,_t),j(Ee,_t,null),t(R,Ms),t(R,gt),t(gt,Os),p(e,aa,s),p(e,Ge,s),t(Ge,Ls),p(e,sa,s),j(te,e,s),p(e,oa,s),p(e,P,s),t(P,Ns),t(P,$t),t($t,Bs),t(P,Ws),t(P,Te),t(Te,bt),t(bt,Rs),t(P,Us),t(P,wt),t(wt,Hs),t(P,Ys),p(e,na,s),j(xe,e,s),p(e,la,s),p(e,Ke,s),t(Ke,Gs),p(e,ra,s),j(ze,e,s),p(e,ia,s),p(e,ae,s),t(ae,Ks),t(ae,Ve),t(Ve,Vs),t(ae,Js),p(e,pa,s),j(qe,e,s),p(e,fa,s),p(e,se,s),t(se,Qs),t(se,Ae),t(Ae,vt),t(vt,Xs),t(se,Zs),p(e,ha,s),j(Ce,e,s),p(e,ca,s),p(e,oe,s),t(oe,eo),t(oe,De),t(De,kt),t(kt,to),t(oe,ao),p(e,ma,s),j(Pe,e,s),p(e,da,s),j(ne,e,s),ua=!0},p(e,[s]){const Fe={};s&2&&(Fe.$$scope={dirty:s,ctx:e}),U.$set(Fe);const yt={};s&2&&(yt.$$scope={dirty:s,ctx:e}),X.$set(yt);const jt={};s&2&&(jt.$$scope={dirty:s,ctx:e}),Z.$set(jt);const Et={};s&2&&(Et.$$scope={dirty:s,ctx:e}),te.$set(Et);const Tt={};s&2&&(Tt.$$scope={dirty:s,ctx:e}),ne.$set(Tt)},i(e){ua||(E(u.$$.fragment,e),E(q.$$.fragment,e),E(U.$$.fragment,e),E(fe.$$.fragment,e),E(he.$$.fragment,e),E(ce.$$.fragment,e),E(me.$$.fragment,e),E(de.$$.fragment,e),E(ue.$$.fragment,e),E(ge.$$.fragment,e),E($e.$$.fragment,e),E(be.$$.fragment,e),E(we.$$.fragment,e),E(X.$$.fragment,e),E(je.$$.fragment,e),E(Z.$$.fragment,e),E(Ee.$$.fragment,e),E(te.$$.fragment,e),E(xe.$$.fragment,e),E(ze.$$.fragment,e),E(qe.$$.fragment,e),E(Ce.$$.fragment,e),E(Pe.$$.fragment,e),E(ne.$$.fragment,e),ua=!0)},o(e){T(u.$$.fragment,e),T(q.$$.fragment,e),T(U.$$.fragment,e),T(fe.$$.fragment,e),T(he.$$.fragment,e),T(ce.$$.fragment,e),T(me.$$.fragment,e),T(de.$$.fragment,e),T(ue.$$.fragment,e),T(ge.$$.fragment,e),T($e.$$.fragment,e),T(be.$$.fragment,e),T(we.$$.fragment,e),T(X.$$.fragment,e),T(je.$$.fragment,e),T(Z.$$.fragment,e),T(Ee.$$.fragment,e),T(te.$$.fragment,e),T(xe.$$.fragment,e),T(ze.$$.fragment,e),T(qe.$$.fragment,e),T(Ce.$$.fragment,e),T(Pe.$$.fragment,e),T(ne.$$.fragment,e),ua=!1},d(e){a(f),e&&a(g),e&&a(d),x(u),e&&a(v),x(q,e),e&&a(S),e&&a(Ie),e&&a(qt),e&&a(I),e&&a(At),x(U,e),e&&a(Ct),e&&a(N),x(fe),e&&a(Dt),e&&a(Me),e&&a(Pt),x(he,e),e&&a(Ft),e&&a(Oe),e&&a(St),x(ce,e),e&&a(It),e&&a(Le),e&&a(Mt),e&&a(Y),e&&a(Ot),e&&a(B),x(me),e&&a(Lt),e&&a(K),e&&a(Nt),x(de,e),e&&a(Bt),e&&a(V),e&&a(Wt),x(ue,e),e&&a(Rt),e&&a(D),e&&a(Ut),x(ge,e),e&&a(Ht),e&&a(A),e&&a(Yt),x($e,e),e&&a(Gt),e&&a(W),x(be),e&&a(Kt),e&&a(Q),e&&a(Vt),x(we,e),e&&a(Jt),x(X,e),e&&a(Qt),e&&a(Re),e&&a(Xt),e&&a(O),e&&a(Zt),x(je,e),e&&a(ea),x(Z,e),e&&a(ta),e&&a(R),x(Ee),e&&a(aa),e&&a(Ge),e&&a(sa),x(te,e),e&&a(oa),e&&a(P),e&&a(na),x(xe,e),e&&a(la),e&&a(Ke),e&&a(ra),x(ze,e),e&&a(ia),e&&a(ae),e&&a(pa),x(qe,e),e&&a(fa),e&&a(se),e&&a(ha),x(Ce,e),e&&a(ca),e&&a(oe),e&&a(ma),x(Pe,e),e&&a(da),x(ne,e)}}}const hn={local:"text-classification",sections:[{local:"load-imdb-dataset",title:"Load IMDb dataset"},{local:"preprocess",title:"Preprocess"},{local:"finetune-with-trainer",title:"Fine-tune with Trainer"},{local:"finetune-with-tensorflow",title:"Fine-tune with TensorFlow"}],title:"Text classification"};function cn(C,f,g){let{fw:d}=f;return C.$$set=_=>{"fw"in _&&g(0,d=_.fw)},[d]}class wn extends Xo{constructor(f){super();Zo(this,f,cn,fn,en,{fw:0})}}export{wn as default,hn as metadata};
