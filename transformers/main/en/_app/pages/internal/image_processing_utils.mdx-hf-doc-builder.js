import{S as Ra,i as Ja,s as Ba,e as a,k as l,w as b,t as s,M as Ha,c as o,d as r,m as d,a as n,x as y,h as i,b as p,G as e,g as v,y as $,q as E,o as w,B as F,v as Ga,L as Ca}from"../../chunks/vendor-hf-doc-builder.js";import{T as Va}from"../../chunks/Tip-hf-doc-builder.js";import{D as P}from"../../chunks/Docstring-hf-doc-builder.js";import{C as Ua}from"../../chunks/CodeBlock-hf-doc-builder.js";import{I as da}from"../../chunks/IconCopyLink-hf-doc-builder.js";import{E as Wa}from"../../chunks/ExampleCodeBlock-hf-doc-builder.js";function Ya(j){let m,k,f,u,M;return{c(){m=a("p"),k=s("Passing "),f=a("code"),u=s("use_auth_token=True"),M=s(" is required when you want to use a private model.")},l(c){m=o(c,"P",{});var _=n(m);k=i(_,"Passing "),f=o(_,"CODE",{});var U=n(f);u=i(U,"use_auth_token=True"),U.forEach(r),M=i(_," is required when you want to use a private model."),_.forEach(r)},m(c,_){v(c,m,_),e(m,k),e(m,f),e(f,u),e(m,M)},d(c){c&&r(m)}}}function Ka(j){let m,k,f,u,M;return u=new Ua({props:{code:`# We can't instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let's show the examples on a
# derived class: *Wav2Vec2FeatureExtractor*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h"
)  # Download feature_extraction_config from huggingface.co and cache.
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "./test/saved_model/"
)  # E.g. feature_extractor (or model) was saved using *save_pretrained('./test/saved_model/')*
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained("./test/saved_model/preprocessor_config.json")
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False
)
assert feature_extractor.return_attention_mask is False
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    "facebook/wav2vec2-base-960h", return_attention_mask=False, foo=False, return_unused_kwargs=True
)
assert feature_extractor.return_attention_mask is False
assert unused_kwargs == {"foo": False}`,highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`}}),{c(){m=a("p"),k=s("Examples:"),f=l(),b(u.$$.fragment)},l(c){m=o(c,"P",{});var _=n(m);k=i(_,"Examples:"),_.forEach(r),f=d(c),y(u.$$.fragment,c)},m(c,_){v(c,m,_),e(m,k),v(c,f,_),$(u,c,_),M=!0},p:Ca,i(c){M||(E(u.$$.fragment,c),M=!0)},o(c){w(u.$$.fragment,c),M=!1},d(c){c&&r(m),c&&r(f),F(u,c)}}}function Qa(j){let m,k,f,u,M;return u=new Ua({props:{code:`from transformers import AutoFeatureExtractor

feature extractor = AutoFeatureExtractor.from_pretrained("bert-base-cased")

# Push the feature extractor to your namespace with the name "my-finetuned-bert".
feature extractor.push_to_hub("my-finetuned-bert")

# Push the feature extractor to an organization with the name "my-finetuned-bert".
feature extractor.push_to_hub("huggingface/my-finetuned-bert")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

feature extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-comment"># Push the feature extractor to your namespace with the name &quot;my-finetuned-bert&quot;.</span>
feature extractor.push_to_hub(<span class="hljs-string">&quot;my-finetuned-bert&quot;</span>)

<span class="hljs-comment"># Push the feature extractor to an organization with the name &quot;my-finetuned-bert&quot;.</span>
feature extractor.push_to_hub(<span class="hljs-string">&quot;huggingface/my-finetuned-bert&quot;</span>)`}}),{c(){m=a("p"),k=s("Examples:"),f=l(),b(u.$$.fragment)},l(c){m=o(c,"P",{});var _=n(m);k=i(_,"Examples:"),_.forEach(r),f=d(c),y(u.$$.fragment,c)},m(c,_){v(c,m,_),e(m,k),v(c,f,_),$(u,c,_),M=!0},p:Ca,i(c){M||(E(u.$$.fragment,c),M=!0)},o(c){w(u.$$.fragment,c),M=!1},d(c){c&&r(m),c&&r(f),F(u,c)}}}function Xa(j){let m,k;return{c(){m=a("p"),k=s("This API is experimental and may have some slight breaking changes in the next releases.")},l(f){m=o(f,"P",{});var u=n(m);k=i(u,"This API is experimental and may have some slight breaking changes in the next releases."),u.forEach(r)},m(f,u){v(f,m,u),e(m,k)},d(f){f&&r(m)}}}function Za(j){let m,k,f,u,M,c,_,U,Lt,lt,Pe,At,dt,Te,St,mt,L,R,We,se,Ot,Ce,Nt,pt,A,ie,Vt,S,Wt,Ue,Ct,Ut,Re,Rt,Jt,ut,O,ce,Bt,N,Ht,Je,Gt,Yt,Be,Kt,Qt,ft,V,le,Xt,de,Zt,He,er,tr,ht,W,J,Ge,me,rr,Ye,ar,gt,g,pe,or,Ke,nr,sr,B,ue,ir,fe,cr,Ie,lr,dr,mr,H,he,pr,ge,ur,De,fr,hr,gr,T,_e,_r,I,xr,je,vr,br,Qe,yr,$r,qe,Er,wr,Fr,G,kr,Y,Mr,K,xe,Pr,D,Tr,Xe,Ir,Dr,ze,jr,qr,Ze,zr,Lr,Ar,q,ve,Sr,be,Or,et,Nr,Vr,Wr,Q,Cr,z,ye,Ur,$e,Rr,tt,Jr,Br,Hr,X,Gr,Z,Ee,Yr,C,Kr,rt,Qr,Xr,Le,Zr,ea,ta,ee,we,ra,at,aa,oa,te,Fe,na,ot,sa,ia,re,ke,ca,nt,la,_t;return c=new da({}),se=new da({}),ie=new P({props:{name:"transformers.rescale",anchor:"transformers.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": float"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"dtype",val:" = <class 'numpy.float32'>"}],parametersDescription:[{anchor:"transformers.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to rescale.`,name:"image"},{anchor:"transformers.rescale.scale",description:`<strong>scale</strong> (<code>float</code>) &#x2014;
The scale to use for rescaling the image.`,name:"scale"},{anchor:"transformers.rescale.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"},{anchor:"transformers.rescale.dtype",description:`<strong>dtype</strong> (<code>np.dtype</code>, <em>optional</em>, defaults to <code>np.float32</code>) &#x2014;
The dtype of the output image. Defaults to <code>np.float32</code>. Used for backwards compatibility with feature
extractors.`,name:"dtype"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L76",returnDescription:`
<p>The rescaled image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),ce=new P({props:{name:"transformers.resize",anchor:"transformers.resize",parameters:[{name:"image",val:""},{name:"size",val:": typing.Tuple[int, int]"},{name:"resample",val:" = <Resampling.BILINEAR: 2>"},{name:"data_format",val:": typing.Optional[transformers.image_utils.ChannelDimension] = None"},{name:"return_numpy",val:": bool = True"}],parametersDescription:[{anchor:"transformers.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.resize.size",description:`<strong>size</strong> (<code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image.`,name:"size"},{anchor:"transformers.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PIL.Image.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If <code>None</code>, will use the inferred format from the input.`,name:"data_format"},{anchor:"transformers.resize.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return the resized image as a numpy array. If False a <code>PIL.Image.Image</code> object is
returned.`,name:"return_numpy"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L213",returnDescription:`
<p>The resized image.</p>
`,returnType:`
<p><code>np.ndarray</code></p>
`}}),le=new P({props:{name:"transformers.to_pil_image",anchor:"transformers.to_pil_image",parameters:[{name:"image",val:": typing.Union[numpy.ndarray, PIL.Image.Image, ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor'), ForwardRef('jnp.Tensor')]"},{name:"do_rescale",val:": typing.Optional[bool] = None"}],parametersDescription:[{anchor:"transformers.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code> or <code>tf.Tensor</code>) &#x2014;
The image to convert to the <code>PIL.Image</code> format.`,name:"image"},{anchor:"transformers.to_pil_image.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will default
to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"do_rescale"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/image_transforms.py#L106",returnDescription:`
<p>The converted image.</p>
`,returnType:`
<p><code>PIL.Image.Image</code></p>
`}}),me=new da({}),pe=new P({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L198"}}),ue=new P({props:{name:"from_dict",anchor:"transformers.FeatureExtractionMixin.from_dict",parameters:[{name:"feature_extractor_dict",val:": typing.Dict[str, typing.Any]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_dict.feature_extractor_dict",description:`<strong>feature_extractor_dict</strong> (<code>Dict[str, Any]</code>) &#x2014;
Dictionary that will be used to instantiate the feature extractor object. Such a dictionary can be
retrieved from a pretrained checkpoint by leveraging the
<a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.to_dict">to_dict()</a> method.`,name:"feature_extractor_dict"},{anchor:"transformers.FeatureExtractionMixin.from_dict.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>) &#x2014;
Additional parameters from which to initialize the feature extractor object.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L443",returnDescription:`
<p>The feature extractor object instantiated from those
parameters.</p>
`,returnType:`
<p><a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a></p>
`}}),he=new P({props:{name:"from_json_file",anchor:"transformers.FeatureExtractionMixin.from_json_file",parameters:[{name:"json_file",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_json_file.json_file",description:`<strong>json_file</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file containing the parameters.`,name:"json_file"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L492",returnDescription:`
<p>The feature_extractor
object instantiated from that JSON file.</p>
`,returnType:`
<p>A feature extractor of type <a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a></p>
`}}),_e=new P({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L222",returnDescription:`
<p>A feature extractor of type <a
  href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),G=new Va({props:{$$slots:{default:[Ya]},$$scope:{ctx:j}}}),Y=new Wa({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[Ka]},$$scope:{ctx:j}}}),xe=new P({props:{name:"get_feature_extractor_dict",anchor:"transformers.FeatureExtractionMixin.get_feature_extractor_dict",parameters:[{name:"pretrained_model_name_or_path",val:": typing.Union[str, os.PathLike]"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.get_feature_extractor_dict.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.`,name:"pretrained_model_name_or_path"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L350",returnDescription:`
<p>The dictionary(ies) that will be used to instantiate the feature extractor object.</p>
`,returnType:`
<p><code>Tuple[Dict, Dict]</code></p>
`}}),ve=new P({props:{name:"push_to_hub",anchor:"transformers.FeatureExtractionMixin.push_to_hub",parameters:[{name:"repo_id",val:": str"},{name:"use_temp_dir",val:": typing.Optional[bool] = None"},{name:"commit_message",val:": typing.Optional[str] = None"},{name:"private",val:": typing.Optional[bool] = None"},{name:"use_auth_token",val:": typing.Union[bool, str, NoneType] = None"},{name:"max_shard_size",val:": typing.Union[int, str, NoneType] = '10GB'"},{name:"create_pr",val:": bool = False"},{name:"**deprecated_kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.push_to_hub.repo_id",description:`<strong>repo_id</strong> (<code>str</code>) &#x2014;
The name of the repository you want to push your feature extractor to. It should contain your organization name
when pushing to a given organization.`,name:"repo_id"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.use_temp_dir",description:`<strong>use_temp_dir</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.
Will default to <code>True</code> if there is no directory named like <code>repo_id</code>, <code>False</code> otherwise.`,name:"use_temp_dir"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.commit_message",description:`<strong>commit_message</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Message to commit while pushing. Will default to <code>&quot;Upload feature extractor&quot;</code>.`,name:"commit_message"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.private",description:`<strong>private</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not the repository created should be private (requires a paying subscription).`,name:"private"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.use_auth_token",description:`<strong>use_auth_token</strong> (<code>bool</code> or <code>str</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>). Will default to <code>True</code> if <code>repo_url</code>
is not specified.`,name:"use_auth_token"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.max_shard_size",description:`<strong>max_shard_size</strong> (<code>int</code> or <code>str</code>, <em>optional</em>, defaults to <code>&quot;10GB&quot;</code>) &#x2014;
Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard
will then be each of size lower than this size. If expressed as a string, needs to be digits followed
by a unit (like <code>&quot;5MB&quot;</code>).`,name:"max_shard_size"},{anchor:"transformers.FeatureExtractionMixin.push_to_hub.create_pr",description:`<strong>create_pr</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to create a PR with the uploaded files or directly commit.`,name:"create_pr"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/utils/hub.py#L712"}}),Q=new Wa({props:{anchor:"transformers.FeatureExtractionMixin.push_to_hub.example",$$slots:{default:[Qa]},$$scope:{ctx:j}}}),ye=new P({props:{name:"register_for_auto_class",anchor:"transformers.FeatureExtractionMixin.register_for_auto_class",parameters:[{name:"auto_class",val:" = 'AutoFeatureExtractor'"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.register_for_auto_class.auto_class",description:`<strong>auto_class</strong> (<code>str</code> or <code>type</code>, <em>optional</em>, defaults to <code>&quot;AutoFeatureExtractor&quot;</code>) &#x2014;
The auto class to register this new feature extractor with.`,name:"auto_class"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L546"}}),X=new Va({props:{warning:!0,$$slots:{default:[Xa]},$$scope:{ctx:j}}}),Ee=new P({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": typing.Union[str, os.PathLike]"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L306"}}),we=new P({props:{name:"to_dict",anchor:"transformers.FeatureExtractionMixin.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L480",returnDescription:`
<p>Dictionary of all the attributes that make up this feature extractor instance.</p>
`,returnType:`
<p><code>Dict[str, Any]</code></p>
`}}),Fe=new P({props:{name:"to_json_file",anchor:"transformers.FeatureExtractionMixin.to_json_file",parameters:[{name:"json_file_path",val:": typing.Union[str, os.PathLike]"}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.to_json_file.json_file_path",description:`<strong>json_file_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file in which this feature_extractor instance&#x2019;s parameters will be saved.`,name:"json_file_path"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L532"}}),ke=new P({props:{name:"to_json_string",anchor:"transformers.FeatureExtractionMixin.to_json_string",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/feature_extraction_utils.py#L511",returnDescription:`
<p>String containing all the attributes that make up this feature_extractor instance in JSON format.</p>
`,returnType:`
<p><code>str</code></p>
`}}),{c(){m=a("meta"),k=l(),f=a("h1"),u=a("a"),M=a("span"),b(c.$$.fragment),_=l(),U=a("span"),Lt=s("Utilities for Image Processors"),lt=l(),Pe=a("p"),At=s(`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`),dt=l(),Te=a("p"),St=s("Most of those are only useful if you are studying the code of the image processors in the library."),mt=l(),L=a("h2"),R=a("a"),We=a("span"),b(se.$$.fragment),Ot=l(),Ce=a("span"),Nt=s("Image Transformations"),pt=l(),A=a("div"),b(ie.$$.fragment),Vt=l(),S=a("p"),Wt=s("Rescales "),Ue=a("code"),Ct=s("image"),Ut=s(" by "),Re=a("code"),Rt=s("scale"),Jt=s("."),ut=l(),O=a("div"),b(ce.$$.fragment),Bt=l(),N=a("p"),Ht=s("Resizes "),Je=a("code"),Gt=s("image"),Yt=s(" to (h, w) specified by "),Be=a("code"),Kt=s("size"),Qt=s(" using the PIL library."),ft=l(),V=a("div"),b(le.$$.fragment),Xt=l(),de=a("p"),Zt=s("Converts "),He=a("code"),er=s("image"),tr=s(` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),ht=l(),W=a("h2"),J=a("a"),Ge=a("span"),b(me.$$.fragment),rr=l(),Ye=a("span"),ar=s("ImageProcessorMixin"),gt=l(),g=a("div"),b(pe.$$.fragment),or=l(),Ke=a("p"),nr=s(`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),sr=l(),B=a("div"),b(ue.$$.fragment),ir=l(),fe=a("p"),cr=s("Instantiates a type of "),Ie=a("a"),lr=s("FeatureExtractionMixin"),dr=s(` from a Python dictionary of
parameters.`),mr=l(),H=a("div"),b(he.$$.fragment),pr=l(),ge=a("p"),ur=s("Instantiates a feature extractor of type "),De=a("a"),fr=s("FeatureExtractionMixin"),hr=s(` from the path to
a JSON file of parameters.`),gr=l(),T=a("div"),b(_e.$$.fragment),_r=l(),I=a("p"),xr=s("Instantiate a type of "),je=a("a"),vr=s("FeatureExtractionMixin"),br=s(" from a feature extractor, "),Qe=a("em"),yr=s("e.g."),$r=s(` a
derived class of `),qe=a("a"),Er=s("SequenceFeatureExtractor"),wr=s("."),Fr=l(),b(G.$$.fragment),kr=l(),b(Y.$$.fragment),Mr=l(),K=a("div"),b(xe.$$.fragment),Pr=l(),D=a("p"),Tr=s("From a "),Xe=a("code"),Ir=s("pretrained_model_name_or_path"),Dr=s(`, resolve to a dictionary of parameters, to be used for instantiating a
feature extractor of type `),ze=a("a"),jr=s("FeatureExtractionMixin"),qr=s(" using "),Ze=a("code"),zr=s("from_dict"),Lr=s("."),Ar=l(),q=a("div"),b(ve.$$.fragment),Sr=l(),be=a("p"),Or=s(`Upload the feature extractor file to the \u{1F917} Model Hub while synchronizing a local clone of the repo in
`),et=a("code"),Nr=s("repo_path_or_name"),Vr=s("."),Wr=l(),b(Q.$$.fragment),Cr=l(),z=a("div"),b(ye.$$.fragment),Ur=l(),$e=a("p"),Rr=s(`Register this class with a given auto class. This should only be used for custom feature extractors as the ones
in the library are already mapped with `),tt=a("code"),Jr=s("AutoFeatureExtractor"),Br=s("."),Hr=l(),b(X.$$.fragment),Gr=l(),Z=a("div"),b(Ee.$$.fragment),Yr=l(),C=a("p"),Kr=s("Save a feature_extractor object to the directory "),rt=a("code"),Qr=s("save_directory"),Xr=s(`, so that it can be re-loaded using the
`),Le=a("a"),Zr=s("from_pretrained()"),ea=s(" class method."),ta=l(),ee=a("div"),b(we.$$.fragment),ra=l(),at=a("p"),aa=s("Serializes this instance to a Python dictionary."),oa=l(),te=a("div"),b(Fe.$$.fragment),na=l(),ot=a("p"),sa=s("Save this instance to a JSON file."),ia=l(),re=a("div"),b(ke.$$.fragment),ca=l(),nt=a("p"),la=s("Serializes this instance to a JSON string."),this.h()},l(t){const h=Ha('[data-svelte="svelte-1phssyn"]',document.head);m=o(h,"META",{name:!0,content:!0}),h.forEach(r),k=d(t),f=o(t,"H1",{class:!0});var Me=n(f);u=o(Me,"A",{id:!0,class:!0,href:!0});var st=n(u);M=o(st,"SPAN",{});var it=n(M);y(c.$$.fragment,it),it.forEach(r),st.forEach(r),_=d(Me),U=o(Me,"SPAN",{});var ct=n(U);Lt=i(ct,"Utilities for Image Processors"),ct.forEach(r),Me.forEach(r),lt=d(t),Pe=o(t,"P",{});var ma=n(Pe);At=i(ma,`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`),ma.forEach(r),dt=d(t),Te=o(t,"P",{});var pa=n(Te);St=i(pa,"Most of those are only useful if you are studying the code of the image processors in the library."),pa.forEach(r),mt=d(t),L=o(t,"H2",{class:!0});var xt=n(L);R=o(xt,"A",{id:!0,class:!0,href:!0});var ua=n(R);We=o(ua,"SPAN",{});var fa=n(We);y(se.$$.fragment,fa),fa.forEach(r),ua.forEach(r),Ot=d(xt),Ce=o(xt,"SPAN",{});var ha=n(Ce);Nt=i(ha,"Image Transformations"),ha.forEach(r),xt.forEach(r),pt=d(t),A=o(t,"DIV",{class:!0});var vt=n(A);y(ie.$$.fragment,vt),Vt=d(vt),S=o(vt,"P",{});var Ae=n(S);Wt=i(Ae,"Rescales "),Ue=o(Ae,"CODE",{});var ga=n(Ue);Ct=i(ga,"image"),ga.forEach(r),Ut=i(Ae," by "),Re=o(Ae,"CODE",{});var _a=n(Re);Rt=i(_a,"scale"),_a.forEach(r),Jt=i(Ae,"."),Ae.forEach(r),vt.forEach(r),ut=d(t),O=o(t,"DIV",{class:!0});var bt=n(O);y(ce.$$.fragment,bt),Bt=d(bt),N=o(bt,"P",{});var Se=n(N);Ht=i(Se,"Resizes "),Je=o(Se,"CODE",{});var xa=n(Je);Gt=i(xa,"image"),xa.forEach(r),Yt=i(Se," to (h, w) specified by "),Be=o(Se,"CODE",{});var va=n(Be);Kt=i(va,"size"),va.forEach(r),Qt=i(Se," using the PIL library."),Se.forEach(r),bt.forEach(r),ft=d(t),V=o(t,"DIV",{class:!0});var yt=n(V);y(le.$$.fragment,yt),Xt=d(yt),de=o(yt,"P",{});var $t=n(de);Zt=i($t,"Converts "),He=o($t,"CODE",{});var ba=n(He);er=i(ba,"image"),ba.forEach(r),tr=i($t,` to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`),$t.forEach(r),yt.forEach(r),ht=d(t),W=o(t,"H2",{class:!0});var Et=n(W);J=o(Et,"A",{id:!0,class:!0,href:!0});var ya=n(J);Ge=o(ya,"SPAN",{});var $a=n(Ge);y(me.$$.fragment,$a),$a.forEach(r),ya.forEach(r),rr=d(Et),Ye=o(Et,"SPAN",{});var Ea=n(Ye);ar=i(Ea,"ImageProcessorMixin"),Ea.forEach(r),Et.forEach(r),gt=d(t),g=o(t,"DIV",{class:!0});var x=n(g);y(pe.$$.fragment,x),or=d(x),Ke=o(x,"P",{});var wa=n(Ke);nr=i(wa,`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`),wa.forEach(r),sr=d(x),B=o(x,"DIV",{class:!0});var wt=n(B);y(ue.$$.fragment,wt),ir=d(wt),fe=o(wt,"P",{});var Ft=n(fe);cr=i(Ft,"Instantiates a type of "),Ie=o(Ft,"A",{href:!0});var Fa=n(Ie);lr=i(Fa,"FeatureExtractionMixin"),Fa.forEach(r),dr=i(Ft,` from a Python dictionary of
parameters.`),Ft.forEach(r),wt.forEach(r),mr=d(x),H=o(x,"DIV",{class:!0});var kt=n(H);y(he.$$.fragment,kt),pr=d(kt),ge=o(kt,"P",{});var Mt=n(ge);ur=i(Mt,"Instantiates a feature extractor of type "),De=o(Mt,"A",{href:!0});var ka=n(De);fr=i(ka,"FeatureExtractionMixin"),ka.forEach(r),hr=i(Mt,` from the path to
a JSON file of parameters.`),Mt.forEach(r),kt.forEach(r),gr=d(x),T=o(x,"DIV",{class:!0});var ae=n(T);y(_e.$$.fragment,ae),_r=d(ae),I=o(ae,"P",{});var oe=n(I);xr=i(oe,"Instantiate a type of "),je=o(oe,"A",{href:!0});var Ma=n(je);vr=i(Ma,"FeatureExtractionMixin"),Ma.forEach(r),br=i(oe," from a feature extractor, "),Qe=o(oe,"EM",{});var Pa=n(Qe);yr=i(Pa,"e.g."),Pa.forEach(r),$r=i(oe,` a
derived class of `),qe=o(oe,"A",{href:!0});var Ta=n(qe);Er=i(Ta,"SequenceFeatureExtractor"),Ta.forEach(r),wr=i(oe,"."),oe.forEach(r),Fr=d(ae),y(G.$$.fragment,ae),kr=d(ae),y(Y.$$.fragment,ae),ae.forEach(r),Mr=d(x),K=o(x,"DIV",{class:!0});var Pt=n(K);y(xe.$$.fragment,Pt),Pr=d(Pt),D=o(Pt,"P",{});var ne=n(D);Tr=i(ne,"From a "),Xe=o(ne,"CODE",{});var Ia=n(Xe);Ir=i(Ia,"pretrained_model_name_or_path"),Ia.forEach(r),Dr=i(ne,`, resolve to a dictionary of parameters, to be used for instantiating a
feature extractor of type `),ze=o(ne,"A",{href:!0});var Da=n(ze);jr=i(Da,"FeatureExtractionMixin"),Da.forEach(r),qr=i(ne," using "),Ze=o(ne,"CODE",{});var ja=n(Ze);zr=i(ja,"from_dict"),ja.forEach(r),Lr=i(ne,"."),ne.forEach(r),Pt.forEach(r),Ar=d(x),q=o(x,"DIV",{class:!0});var Oe=n(q);y(ve.$$.fragment,Oe),Sr=d(Oe),be=o(Oe,"P",{});var Tt=n(be);Or=i(Tt,`Upload the feature extractor file to the \u{1F917} Model Hub while synchronizing a local clone of the repo in
`),et=o(Tt,"CODE",{});var qa=n(et);Nr=i(qa,"repo_path_or_name"),qa.forEach(r),Vr=i(Tt,"."),Tt.forEach(r),Wr=d(Oe),y(Q.$$.fragment,Oe),Oe.forEach(r),Cr=d(x),z=o(x,"DIV",{class:!0});var Ne=n(z);y(ye.$$.fragment,Ne),Ur=d(Ne),$e=o(Ne,"P",{});var It=n($e);Rr=i(It,`Register this class with a given auto class. This should only be used for custom feature extractors as the ones
in the library are already mapped with `),tt=o(It,"CODE",{});var za=n(tt);Jr=i(za,"AutoFeatureExtractor"),za.forEach(r),Br=i(It,"."),It.forEach(r),Hr=d(Ne),y(X.$$.fragment,Ne),Ne.forEach(r),Gr=d(x),Z=o(x,"DIV",{class:!0});var Dt=n(Z);y(Ee.$$.fragment,Dt),Yr=d(Dt),C=o(Dt,"P",{});var Ve=n(C);Kr=i(Ve,"Save a feature_extractor object to the directory "),rt=o(Ve,"CODE",{});var La=n(rt);Qr=i(La,"save_directory"),La.forEach(r),Xr=i(Ve,`, so that it can be re-loaded using the
`),Le=o(Ve,"A",{href:!0});var Aa=n(Le);Zr=i(Aa,"from_pretrained()"),Aa.forEach(r),ea=i(Ve," class method."),Ve.forEach(r),Dt.forEach(r),ta=d(x),ee=o(x,"DIV",{class:!0});var jt=n(ee);y(we.$$.fragment,jt),ra=d(jt),at=o(jt,"P",{});var Sa=n(at);aa=i(Sa,"Serializes this instance to a Python dictionary."),Sa.forEach(r),jt.forEach(r),oa=d(x),te=o(x,"DIV",{class:!0});var qt=n(te);y(Fe.$$.fragment,qt),na=d(qt),ot=o(qt,"P",{});var Oa=n(ot);sa=i(Oa,"Save this instance to a JSON file."),Oa.forEach(r),qt.forEach(r),ia=d(x),re=o(x,"DIV",{class:!0});var zt=n(re);y(ke.$$.fragment,zt),ca=d(zt),nt=o(zt,"P",{});var Na=n(nt);la=i(Na,"Serializes this instance to a JSON string."),Na.forEach(r),zt.forEach(r),x.forEach(r),this.h()},h(){p(m,"name","hf:doc:metadata"),p(m,"content",JSON.stringify(eo)),p(u,"id","utilities-for-image-processors"),p(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(u,"href","#utilities-for-image-processors"),p(f,"class","relative group"),p(R,"id","transformers.rescale"),p(R,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(R,"href","#transformers.rescale"),p(L,"class","relative group"),p(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(J,"id","transformers.FeatureExtractionMixin"),p(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(J,"href","#transformers.FeatureExtractionMixin"),p(W,"class","relative group"),p(Ie,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),p(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(De,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),p(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(je,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),p(qe,"href","/docs/transformers/main/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor"),p(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ze,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin"),p(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Le,"href","/docs/transformers/main/en/internal/image_processing_utils#transformers.FeatureExtractionMixin.from_pretrained"),p(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(g,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,h){e(document.head,m),v(t,k,h),v(t,f,h),e(f,u),e(u,M),$(c,M,null),e(f,_),e(f,U),e(U,Lt),v(t,lt,h),v(t,Pe,h),e(Pe,At),v(t,dt,h),v(t,Te,h),e(Te,St),v(t,mt,h),v(t,L,h),e(L,R),e(R,We),$(se,We,null),e(L,Ot),e(L,Ce),e(Ce,Nt),v(t,pt,h),v(t,A,h),$(ie,A,null),e(A,Vt),e(A,S),e(S,Wt),e(S,Ue),e(Ue,Ct),e(S,Ut),e(S,Re),e(Re,Rt),e(S,Jt),v(t,ut,h),v(t,O,h),$(ce,O,null),e(O,Bt),e(O,N),e(N,Ht),e(N,Je),e(Je,Gt),e(N,Yt),e(N,Be),e(Be,Kt),e(N,Qt),v(t,ft,h),v(t,V,h),$(le,V,null),e(V,Xt),e(V,de),e(de,Zt),e(de,He),e(He,er),e(de,tr),v(t,ht,h),v(t,W,h),e(W,J),e(J,Ge),$(me,Ge,null),e(W,rr),e(W,Ye),e(Ye,ar),v(t,gt,h),v(t,g,h),$(pe,g,null),e(g,or),e(g,Ke),e(Ke,nr),e(g,sr),e(g,B),$(ue,B,null),e(B,ir),e(B,fe),e(fe,cr),e(fe,Ie),e(Ie,lr),e(fe,dr),e(g,mr),e(g,H),$(he,H,null),e(H,pr),e(H,ge),e(ge,ur),e(ge,De),e(De,fr),e(ge,hr),e(g,gr),e(g,T),$(_e,T,null),e(T,_r),e(T,I),e(I,xr),e(I,je),e(je,vr),e(I,br),e(I,Qe),e(Qe,yr),e(I,$r),e(I,qe),e(qe,Er),e(I,wr),e(T,Fr),$(G,T,null),e(T,kr),$(Y,T,null),e(g,Mr),e(g,K),$(xe,K,null),e(K,Pr),e(K,D),e(D,Tr),e(D,Xe),e(Xe,Ir),e(D,Dr),e(D,ze),e(ze,jr),e(D,qr),e(D,Ze),e(Ze,zr),e(D,Lr),e(g,Ar),e(g,q),$(ve,q,null),e(q,Sr),e(q,be),e(be,Or),e(be,et),e(et,Nr),e(be,Vr),e(q,Wr),$(Q,q,null),e(g,Cr),e(g,z),$(ye,z,null),e(z,Ur),e(z,$e),e($e,Rr),e($e,tt),e(tt,Jr),e($e,Br),e(z,Hr),$(X,z,null),e(g,Gr),e(g,Z),$(Ee,Z,null),e(Z,Yr),e(Z,C),e(C,Kr),e(C,rt),e(rt,Qr),e(C,Xr),e(C,Le),e(Le,Zr),e(C,ea),e(g,ta),e(g,ee),$(we,ee,null),e(ee,ra),e(ee,at),e(at,aa),e(g,oa),e(g,te),$(Fe,te,null),e(te,na),e(te,ot),e(ot,sa),e(g,ia),e(g,re),$(ke,re,null),e(re,ca),e(re,nt),e(nt,la),_t=!0},p(t,[h]){const Me={};h&2&&(Me.$$scope={dirty:h,ctx:t}),G.$set(Me);const st={};h&2&&(st.$$scope={dirty:h,ctx:t}),Y.$set(st);const it={};h&2&&(it.$$scope={dirty:h,ctx:t}),Q.$set(it);const ct={};h&2&&(ct.$$scope={dirty:h,ctx:t}),X.$set(ct)},i(t){_t||(E(c.$$.fragment,t),E(se.$$.fragment,t),E(ie.$$.fragment,t),E(ce.$$.fragment,t),E(le.$$.fragment,t),E(me.$$.fragment,t),E(pe.$$.fragment,t),E(ue.$$.fragment,t),E(he.$$.fragment,t),E(_e.$$.fragment,t),E(G.$$.fragment,t),E(Y.$$.fragment,t),E(xe.$$.fragment,t),E(ve.$$.fragment,t),E(Q.$$.fragment,t),E(ye.$$.fragment,t),E(X.$$.fragment,t),E(Ee.$$.fragment,t),E(we.$$.fragment,t),E(Fe.$$.fragment,t),E(ke.$$.fragment,t),_t=!0)},o(t){w(c.$$.fragment,t),w(se.$$.fragment,t),w(ie.$$.fragment,t),w(ce.$$.fragment,t),w(le.$$.fragment,t),w(me.$$.fragment,t),w(pe.$$.fragment,t),w(ue.$$.fragment,t),w(he.$$.fragment,t),w(_e.$$.fragment,t),w(G.$$.fragment,t),w(Y.$$.fragment,t),w(xe.$$.fragment,t),w(ve.$$.fragment,t),w(Q.$$.fragment,t),w(ye.$$.fragment,t),w(X.$$.fragment,t),w(Ee.$$.fragment,t),w(we.$$.fragment,t),w(Fe.$$.fragment,t),w(ke.$$.fragment,t),_t=!1},d(t){r(m),t&&r(k),t&&r(f),F(c),t&&r(lt),t&&r(Pe),t&&r(dt),t&&r(Te),t&&r(mt),t&&r(L),F(se),t&&r(pt),t&&r(A),F(ie),t&&r(ut),t&&r(O),F(ce),t&&r(ft),t&&r(V),F(le),t&&r(ht),t&&r(W),F(me),t&&r(gt),t&&r(g),F(pe),F(ue),F(he),F(_e),F(G),F(Y),F(xe),F(ve),F(Q),F(ye),F(X),F(Ee),F(we),F(Fe),F(ke)}}}const eo={local:"utilities-for-image-processors",sections:[{local:"transformers.rescale",title:"Image Transformations"},{local:"transformers.FeatureExtractionMixin",title:"ImageProcessorMixin"}],title:"Utilities for Image Processors"};function to(j){return Ga(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class co extends Ra{constructor(m){super();Ja(this,m,to,Za,Ba,{})}}export{co as default,eo as metadata};
