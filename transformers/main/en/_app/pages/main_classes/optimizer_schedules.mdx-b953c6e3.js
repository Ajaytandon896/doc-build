import{S as Ji,i as Ki,s as Qi,e as r,k as l,w as u,t as s,M as Xi,c as n,d as a,m as c,a as o,x as f,h as i,b as m,N as kr,F as t,g as d,y as g,L as Yi,q as _,o as w,B as v,v as Zi}from"../../chunks/vendor-6b77c823.js";import{D as $}from"../../chunks/Docstring-abef54e3.js";import{C as Nr}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Y}from"../../chunks/IconCopyLink-7a11ce68.js";function el(vs){let W,Da,S,Z,yt,_e,Fr,bt,Cr,La,ee,Or,$t,Rr,jr,Pa,P,At,qr,Ur,we,Gr,zt,Vr,Mr,Hr,Et,Br,Wa,I,te,xt,ve,Jr,Tt,Kr,Sa,E,ye,Qr,be,Xr,$e,Yr,Zr,en,ae,Ae,tn,Dt,an,Ia,k,re,Lt,ze,rn,Pt,nn,ka,h,Ee,on,dt,sn,xe,ln,cn,b,mn,Wt,pn,dn,Te,hn,un,St,fn,gn,It,_n,wn,kt,vn,yn,Nt,bn,$n,Ft,An,zn,En,Ct,xn,Tn,De,Dn,Le,Ln,Pn,Wn,x,Pe,Ot,Sn,In,We,Rt,kn,Nn,Se,Fn,Ie,Cn,On,Rn,jt,qt,jn,qn,Ut,Gt,Un,Gn,Vt,Mt,Vn,Mn,Ht,Hn,Bn,ke,Jn,Bt,Kn,Qn,Ne,Xn,T,Yn,Jt,Zn,eo,ht,to,ao,Kt,ro,no,oo,Fe,so,Qt,io,lo,Ce,co,ne,Oe,mo,Xt,po,Na,N,oe,Yt,Re,ho,Zt,uo,Fa,z,je,fo,F,go,ea,_o,wo,qe,vo,yo,bo,ta,$o,Ao,se,Ue,zo,aa,Eo,Ca,C,Ge,xo,ra,To,Oa,O,ie,na,Ve,Do,oa,Lo,Ra,R,le,sa,Me,Po,ia,Wo,ja,j,He,So,la,Io,qa,q,Be,ko,ca,No,Ua,U,Je,Fo,ma,Co,Ga,G,Ke,Oo,pa,Ro,Va,Qe,ys,Ma,V,Xe,jo,da,qo,Ha,Ye,bs,Ba,M,Ze,Uo,ha,Go,Ja,et,$s,Ka,H,tt,Vo,ua,Mo,Qa,at,As,Xa,D,rt,Ho,nt,Bo,fa,Jo,Ko,Qo,ce,Xo,ga,Yo,Zo,ot,es,Ya,B,me,_a,st,ts,wa,as,Za,J,it,rs,va,ns,er,K,pe,ya,lt,os,ba,ss,tr,Q,de,$a,ct,is,Aa,ls,ar,L,mt,cs,X,ms,za,ps,ds,Ea,hs,us,fs,he,pt,gs,xa,_s,rr;return _e=new Y({}),ve=new Y({}),ye=new $({props:{name:"class transformers.AdamW",anchor:"transformers.AdamW",parameters:[{name:"params",val:": typing.Iterable[torch.nn.parameter.Parameter]"},{name:"lr",val:": float = 0.001"},{name:"betas",val:": typing.Tuple[float, float] = (0.9, 0.999)"},{name:"eps",val:": float = 1e-06"},{name:"weight_decay",val:": float = 0.0"},{name:"correct_bias",val:": bool = True"},{name:"no_deprecation_warning",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L273",parametersDescription:[{anchor:"transformers.AdamW.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.AdamW.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The learning rate to use.`,name:"lr"},{anchor:"transformers.AdamW.betas",description:`<strong>betas</strong> (<code>Tuple[float,float]</code>, <em>optional</em>, defaults to (0.9, 0.999)) &#x2014;
Adam&#x2019;s betas parameters (b1, b2).`,name:"betas"},{anchor:"transformers.AdamW.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
Adam&#x2019;s epsilon for numerical stability.`,name:"eps"},{anchor:"transformers.AdamW.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Decoupled weight decay to apply.`,name:"weight_decay"},{anchor:"transformers.AdamW.correct_bias",description:`<strong>correct_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to correct bias in Adam (for instance, in Bert TF repository they use <code>False</code>).`,name:"correct_bias"},{anchor:"transformers.AdamW.no_deprecation_warning",description:`<strong>no_deprecation_warning</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
A flag used to disable the deprecation warning (set to <code>True</code> to disable the warning).`,name:"no_deprecation_warning"}]}}),Ae=new $({props:{name:"step",anchor:"transformers.AdamW.step",parameters:[{name:"closure",val:": typing.Callable = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L323",parametersDescription:[{anchor:"transformers.AdamW.step.closure",description:"<strong>closure</strong> (<code>Callable</code>, <em>optional</em>) &#x2014; A closure that reevaluates the model and returns the loss.",name:"closure"}]}}),ze=new Y({}),Ee=new $({props:{name:"class transformers.Adafactor",anchor:"transformers.Adafactor",parameters:[{name:"params",val:""},{name:"lr",val:" = None"},{name:"eps",val:" = (1e-30, 0.001)"},{name:"clip_threshold",val:" = 1.0"},{name:"decay_rate",val:" = -0.8"},{name:"beta1",val:" = None"},{name:"weight_decay",val:" = 0.0"},{name:"scale_parameter",val:" = True"},{name:"relative_step",val:" = True"},{name:"warmup_init",val:" = False"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L385",parametersDescription:[{anchor:"transformers.Adafactor.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.Adafactor.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The external learning rate.`,name:"lr"},{anchor:"transformers.Adafactor.eps",description:`<strong>eps</strong> (<code>Tuple[float, float]</code>, <em>optional</em>, defaults to (1e-30, 1e-3)) &#x2014;
Regularization constants for square gradient and parameter scale respectively`,name:"eps"},{anchor:"transformers.Adafactor.clip_threshold",description:`<strong>clip_threshold</strong> (<code>float</code>, <em>optional</em>, defaults 1.0) &#x2014;
Threshold of root mean square of final gradient update`,name:"clip_threshold"},{anchor:"transformers.Adafactor.decay_rate",description:`<strong>decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to -0.8) &#x2014;
Coefficient used to compute running averages of square`,name:"decay_rate"},{anchor:"transformers.Adafactor.beta1",description:`<strong>beta1</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Coefficient used for computing running averages of gradient`,name:"beta1"},{anchor:"transformers.Adafactor.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Weight decay (L2 penalty)`,name:"weight_decay"},{anchor:"transformers.Adafactor.scale_parameter",description:`<strong>scale_parameter</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, learning rate is scaled by root mean square`,name:"scale_parameter"},{anchor:"transformers.Adafactor.relative_step",description:`<strong>relative_step</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, time-dependent learning rate is computed instead of external learning rate`,name:"relative_step"},{anchor:"transformers.Adafactor.warmup_init",description:`<strong>warmup_init</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Time-dependent learning rate computation depends on whether warm-up initialization is being used`,name:"warmup_init"}]}}),ke=new Nr({props:{code:"Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3)",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">False</span>, relative_step=<span class="hljs-literal">False</span>, warmup_init=<span class="hljs-literal">False</span>, lr=<span class="hljs-number">1e-3</span>)'}}),Ne=new Nr({props:{code:"Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)'}}),Fe=new Nr({props:{code:`from transformers.optimization import Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`,highlighted:`<span class="hljs-keyword">from</span> transformers.optimization <span class="hljs-keyword">import</span> Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`}}),Ce=new Nr({props:{code:`# replace AdamW with Adafactor
optimizer = Adafactor(
    model.parameters(),
    lr=1e-3,
    eps=(1e-30, 1e-3),
    clip_threshold=1.0,
    decay_rate=-0.8,
    beta1=None,
    weight_decay=0.0,
    relative_step=False,
    scale_parameter=False,
    warmup_init=False,
)`,highlighted:`<span class="hljs-comment"># replace AdamW with Adafactor</span>
optimizer = Adafactor(
    model.parameters(),
    lr=<span class="hljs-number">1e-3</span>,
    eps=(<span class="hljs-number">1e-30</span>, <span class="hljs-number">1e-3</span>),
    clip_threshold=<span class="hljs-number">1.0</span>,
    decay_rate=-<span class="hljs-number">0.8</span>,
    beta1=<span class="hljs-literal">None</span>,
    weight_decay=<span class="hljs-number">0.0</span>,
    relative_step=<span class="hljs-literal">False</span>,
    scale_parameter=<span class="hljs-literal">False</span>,
    warmup_init=<span class="hljs-literal">False</span>,
)`}}),Oe=new $({props:{name:"step",anchor:"transformers.Adafactor.step",parameters:[{name:"closure",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L531",parametersDescription:[{anchor:"transformers.Adafactor.step.closure",description:`<strong>closure</strong> (callable, optional) &#x2014; A closure that reevaluates the model
and returns the loss.`,name:"closure"}]}}),Re=new Y({}),je=new $({props:{name:"class transformers.AdamWeightDecay",anchor:"transformers.AdamWeightDecay",parameters:[{name:"learning_rate",val:": typing.Union[float, keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule] = 0.001"},{name:"beta_1",val:": float = 0.9"},{name:"beta_2",val:": float = 0.999"},{name:"epsilon",val:": float = 1e-07"},{name:"amsgrad",val:": bool = False"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"include_in_weight_decay",val:": typing.Optional[typing.List[str]] = None"},{name:"exclude_from_weight_decay",val:": typing.Optional[typing.List[str]] = None"},{name:"name",val:": str = 'AdamWeightDecay'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization_tf.py#L152",parametersDescription:[{anchor:"transformers.AdamWeightDecay.learning_rate",description:`<strong>learning_rate</strong> (<code>Union[float, tf.keras.optimizers.schedules.LearningRateSchedule]</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The learning rate to use or a schedule.`,name:"learning_rate"},{anchor:"transformers.AdamWeightDecay.beta_1",description:`<strong>beta_1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 parameter in Adam, which is the exponential decay rate for the 1st momentum estimates.`,name:"beta_1"},{anchor:"transformers.AdamWeightDecay.beta_2",description:`<strong>beta_2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 parameter in Adam, which is the exponential decay rate for the 2nd momentum estimates.`,name:"beta_2"},{anchor:"transformers.AdamWeightDecay.epsilon",description:`<strong>epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The epsilon parameter in Adam, which is a small constant for numerical stability.`,name:"epsilon"},{anchor:"transformers.AdamWeightDecay.amsgrad",description:`<strong>amsgrad</strong> (<code>bool</code>, <em>optional</em>, default to <code>False</code>) &#x2014;
Whether to apply AMSGrad variant of this algorithm or not, see <a href="https://arxiv.org/abs/1904.09237" rel="nofollow">On the Convergence of Adam and
Beyond</a>.`,name:"amsgrad"},{anchor:"transformers.AdamWeightDecay.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to apply.`,name:"weight_decay_rate"},{anchor:"transformers.AdamWeightDecay.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters by default (unless they are in <code>exclude_from_weight_decay</code>).`,name:"include_in_weight_decay"},{anchor:"transformers.AdamWeightDecay.exclude_from_weight_decay",description:`<strong>exclude_from_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to exclude from applying weight decay to. If a
<code>include_in_weight_decay</code> is passed, the names in it will supersede this list.`,name:"exclude_from_weight_decay"},{anchor:"transformers.AdamWeightDecay.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>, defaults to &#x2018;AdamWeightDecay&#x2019;) &#x2014;
Optional name for the operations created when applying gradients.
kwargs &#x2014;
Keyword arguments. Allowed to be {<code>clipnorm</code>, <code>clipvalue</code>, <code>lr</code>, <code>decay</code>}. <code>clipnorm</code> is clip gradients by
norm; <code>clipvalue</code> is clip gradients by value, <code>decay</code> is included for backward compatibility to allow time
inverse decay of learning rate. <code>lr</code> is included for backward compatibility, recommended to use
<code>learning_rate</code> instead.`,name:"name"}]}}),Ue=new $({props:{name:"from_config",anchor:"transformers.AdamWeightDecay.from_config",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization_tf.py#L209"}}),Ge=new $({props:{name:"transformers.create_optimizer",anchor:"transformers.create_optimizer",parameters:[{name:"init_lr",val:": float"},{name:"num_train_steps",val:": int"},{name:"num_warmup_steps",val:": int"},{name:"min_lr_ratio",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"power",val:": float = 1.0"},{name:"include_in_weight_decay",val:": typing.Optional[typing.List[str]] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization_tf.py#L82",parametersDescription:[{anchor:"transformers.create_optimizer.init_lr",description:`<strong>init_lr</strong> (<code>float</code>) &#x2014;
The desired learning rate at the end of the warmup phase.`,name:"init_lr"},{anchor:"transformers.create_optimizer.num_train_steps",description:`<strong>num_train_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_train_steps"},{anchor:"transformers.create_optimizer.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of warmup steps.`,name:"num_warmup_steps"},{anchor:"transformers.create_optimizer.min_lr_ratio",description:`<strong>min_lr_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The final learning rate at the end of the linear decay will be <code>init_lr * min_lr_ratio</code>.`,name:"min_lr_ratio"},{anchor:"transformers.create_optimizer.adam_beta1",description:`<strong>adam_beta1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 to use in Adam.`,name:"adam_beta1"},{anchor:"transformers.create_optimizer.adam_beta2",description:`<strong>adam_beta2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 to use in Adam.`,name:"adam_beta2"},{anchor:"transformers.create_optimizer.adam_epsilon",description:`<strong>adam_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-8) &#x2014;
The epsilon to use in Adam.`,name:"adam_epsilon"},{anchor:"transformers.create_optimizer.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to use.`,name:"weight_decay_rate"},{anchor:"transformers.create_optimizer.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The power to use for PolynomialDecay.`,name:"power"},{anchor:"transformers.create_optimizer.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters except bias and layer norm parameters.`,name:"include_in_weight_decay"}]}}),Ve=new Y({}),Me=new Y({}),He=new $({props:{name:"class transformers.SchedulerType",anchor:"transformers.SchedulerType",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/trainer_utils.py#L322"}}),Be=new $({props:{name:"transformers.get_scheduler",anchor:"transformers.get_scheduler",parameters:[{name:"name",val:": typing.Union[str, transformers.trainer_utils.SchedulerType]"},{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": typing.Optional[int] = None"},{name:"num_training_steps",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L233",parametersDescription:[{anchor:"transformers.get_scheduler.name",description:`<strong>name</strong> (<code>str</code> or <code>SchedulerType</code>) &#x2014;
The name of the scheduler to use.`,name:"name"},{anchor:"transformers.get_scheduler.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.Optimizer</code>) &#x2014;
The optimizer that will be used during training.`,name:"optimizer"},{anchor:"transformers.get_scheduler.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of warmup steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_warmup_steps"},{anchor:"transformers.get_scheduler.num_training_steps",description:`<strong>num_training_steps</strong> (\`int&#x201C;, <em>optional</em>) &#x2014;
The number of training steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_training_steps"}]}}),Je=new $({props:{name:"transformers.get_constant_schedule",anchor:"transformers.get_constant_schedule",parameters:[{name:"optimizer",val:": Optimizer"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L34",parametersDescription:[{anchor:"transformers.get_constant_schedule.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Ke=new $({props:{name:"transformers.get_constant_schedule_with_warmup",anchor:"transformers.get_constant_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L50",parametersDescription:[{anchor:"transformers.get_constant_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_constant_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Xe=new $({props:{name:"transformers.get_cosine_schedule_with_warmup",anchor:"transformers.get_cosine_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": float = 0.5"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L104",parametersDescription:[{anchor:"transformers.get_cosine_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0
following a half-cosine).`,name:"num_cycles"},{anchor:"transformers.get_cosine_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Ze=new $({props:{name:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": int = 1"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L138",parametersDescription:[{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of hard restarts to use.`,name:"num_cycles"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),tt=new $({props:{name:"transformers.get_linear_schedule_with_warmup",anchor:"transformers.get_linear_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"last_epoch",val:" = -1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L75",parametersDescription:[{anchor:"transformers.get_linear_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_linear_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),rt=new $({props:{name:"transformers.get_polynomial_decay_schedule_with_warmup",anchor:"transformers.get_polynomial_decay_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"lr_end",val:" = 1e-07"},{name:"power",val:" = 1.0"},{name:"last_epoch",val:" = -1"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization.py#L173",parametersDescription:[{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.lr_end",description:`<strong>lr_end</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The end LR.`,name:"lr_end"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Power factor.`,name:"power"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),st=new Y({}),it=new $({props:{name:"class transformers.WarmUp",anchor:"transformers.WarmUp",parameters:[{name:"initial_learning_rate",val:": float"},{name:"decay_schedule_fn",val:": typing.Callable"},{name:"warmup_steps",val:": int"},{name:"power",val:": float = 1.0"},{name:"name",val:": str = None"}],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization_tf.py#L24",parametersDescription:[{anchor:"transformers.WarmUp.initial_learning_rate",description:`<strong>initial_learning_rate</strong> (<code>float</code>) &#x2014;
The initial learning rate for the schedule after the warmup (so this will be the learning rate at the end
of the warmup).`,name:"initial_learning_rate"},{anchor:"transformers.WarmUp.decay_schedule_fn",description:`<strong>decay_schedule_fn</strong> (<code>Callable</code>) &#x2014;
The schedule function to apply after the warmup for the rest of training.`,name:"decay_schedule_fn"},{anchor:"transformers.WarmUp.warmup_steps",description:`<strong>warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup part of training.`,name:"warmup_steps"},{anchor:"transformers.WarmUp.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
The power to use for the polynomial warmup (defaults is a linear warmup).`,name:"power"},{anchor:"transformers.WarmUp.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Optional name prefix for the returned tensors during the schedule.`,name:"name"}]}}),lt=new Y({}),ct=new Y({}),mt=new $({props:{name:"class transformers.GradientAccumulator",anchor:"transformers.GradientAccumulator",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization_tf.py#L282"}}),pt=new $({props:{name:"reset",anchor:"transformers.GradientAccumulator.reset",parameters:[],source:"https://github.com/huggingface/transformers/blob/main/src/transformers/optimization_tf.py#L344"}}),{c(){W=r("meta"),Da=l(),S=r("h1"),Z=r("a"),yt=r("span"),u(_e.$$.fragment),Fr=l(),bt=r("span"),Cr=s("Optimization"),La=l(),ee=r("p"),Or=s("The "),$t=r("code"),Rr=s(".optimization"),jr=s(" module provides:"),Pa=l(),P=r("ul"),At=r("li"),qr=s("an optimizer with weight decay fixed that can be used to fine-tuned models, and"),Ur=l(),we=r("li"),Gr=s("several schedules in the form of schedule objects that inherit from "),zt=r("code"),Vr=s("_LRSchedule"),Mr=s(":"),Hr=l(),Et=r("li"),Br=s("a gradient accumulation class to accumulate the gradients of multiple batches"),Wa=l(),I=r("h2"),te=r("a"),xt=r("span"),u(ve.$$.fragment),Jr=l(),Tt=r("span"),Kr=s("AdamW (PyTorch)"),Sa=l(),E=r("div"),u(ye.$$.fragment),Qr=l(),be=r("p"),Xr=s("Implements Adam algorithm with weight decay fix as introduced in "),$e=r("a"),Yr=s(`Decoupled Weight Decay
Regularization`),Zr=s("."),en=l(),ae=r("div"),u(Ae.$$.fragment),tn=l(),Dt=r("p"),an=s("Performs a single optimization step."),Ia=l(),k=r("h2"),re=r("a"),Lt=r("span"),u(ze.$$.fragment),rn=l(),Pt=r("span"),nn=s("AdaFactor (PyTorch)"),ka=l(),h=r("div"),u(Ee.$$.fragment),on=l(),dt=r("p"),sn=s(`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
`),xe=r("a"),ln=s("https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),cn=l(),b=r("p"),mn=s("Paper: "),Wt=r("em"),pn=s("Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"),dn=l(),Te=r("a"),hn=s("https://arxiv.org/abs/1804.04235"),un=s(` Note that
this optimizer internally adjusts the learning rate depending on the `),St=r("code"),fn=s("scale_parameter"),gn=s(", "),It=r("code"),_n=s("relative_step"),wn=s(` and
`),kt=r("code"),vn=s("warmup_init"),yn=s(" options. To use a manual (external) learning rate schedule you should set "),Nt=r("code"),bn=s("scale_parameter=False"),$n=s(` and
`),Ft=r("code"),An=s("relative_step=False"),zn=s("."),En=l(),Ct=r("p"),xn=s("This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested."),Tn=l(),De=r("p"),Dn=s("Recommended T5 finetuning settings ("),Le=r("a"),Ln=s("https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),Pn=s("):"),Wn=l(),x=r("ul"),Pe=r("li"),Ot=r("p"),Sn=s("Training without LR warmup or clip_threshold is not recommended."),In=l(),We=r("ul"),Rt=r("li"),kn=s("use scheduled LR warm-up to fixed LR"),Nn=l(),Se=r("li"),Fn=s("use clip_threshold=1.0 ("),Ie=r("a"),Cn=s("https://arxiv.org/abs/1804.04235"),On=s(")"),Rn=l(),jt=r("li"),qt=r("p"),jn=s("Disable relative updates"),qn=l(),Ut=r("li"),Gt=r("p"),Un=s("Use scale_parameter=False"),Gn=l(),Vt=r("li"),Mt=r("p"),Vn=s("Additional optimizer operations like gradient clipping should not be used alongside Adafactor"),Mn=l(),Ht=r("p"),Hn=s("Example:"),Bn=l(),u(ke.$$.fragment),Jn=l(),Bt=r("p"),Kn=s("Others reported the following combination to work well:"),Qn=l(),u(Ne.$$.fragment),Xn=l(),T=r("p"),Yn=s("When using "),Jt=r("code"),Zn=s("lr=None"),eo=s(" with "),ht=r("a"),to=s("Trainer"),ao=s(" you will most likely need to use "),Kt=r("code"),ro=s("AdafactorSchedule"),no=s(`
scheduler as following:`),oo=l(),u(Fe.$$.fragment),so=l(),Qt=r("p"),io=s("Usage:"),lo=l(),u(Ce.$$.fragment),co=l(),ne=r("div"),u(Oe.$$.fragment),mo=l(),Xt=r("p"),po=s("Performs a single optimization step"),Na=l(),N=r("h2"),oe=r("a"),Yt=r("span"),u(Re.$$.fragment),ho=l(),Zt=r("span"),uo=s("AdamWeightDecay (TensorFlow)"),Fa=l(),z=r("div"),u(je.$$.fragment),fo=l(),F=r("p"),go=s(`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is `),ea=r("em"),_o=s("not"),wo=s(` the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in `),qe=r("a"),vo=s(`Decoupled Weight Decay
Regularization`),yo=s("."),bo=l(),ta=r("p"),$o=s(`Instead we want ot decay the weights in a manner that doesn\u2019t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`),Ao=l(),se=r("div"),u(Ue.$$.fragment),zo=l(),aa=r("p"),Eo=s("Creates an optimizer from its config with WarmUp custom object."),Ca=l(),C=r("div"),u(Ge.$$.fragment),xo=l(),ra=r("p"),To=s("Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay."),Oa=l(),O=r("h2"),ie=r("a"),na=r("span"),u(Ve.$$.fragment),Do=l(),oa=r("span"),Lo=s("Schedules"),Ra=l(),R=r("h3"),le=r("a"),sa=r("span"),u(Me.$$.fragment),Po=l(),ia=r("span"),Wo=s("Learning Rate Schedules (Pytorch)"),ja=l(),j=r("div"),u(He.$$.fragment),So=l(),la=r("p"),Io=s("An enumeration."),qa=l(),q=r("div"),u(Be.$$.fragment),ko=l(),ca=r("p"),No=s("Unified API to get any scheduler from its name."),Ua=l(),U=r("div"),u(Je.$$.fragment),Fo=l(),ma=r("p"),Co=s("Create a schedule with a constant learning rate, using the learning rate set in optimizer."),Ga=l(),G=r("div"),u(Ke.$$.fragment),Oo=l(),pa=r("p"),Ro=s(`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`),Va=l(),Qe=r("img"),Ma=l(),V=r("div"),u(Xe.$$.fragment),jo=l(),da=r("p"),qo=s(`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`),Ha=l(),Ye=r("img"),Ba=l(),M=r("div"),u(Ze.$$.fragment),Uo=l(),ha=r("p"),Go=s(`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`),Ja=l(),et=r("img"),Ka=l(),H=r("div"),u(tt.$$.fragment),Vo=l(),ua=r("p"),Mo=s(`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`),Qa=l(),at=r("img"),Xa=l(),D=r("div"),u(rt.$$.fragment),Ho=l(),nt=r("p"),Bo=s(`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by `),fa=r("em"),Jo=s("lr_end"),Ko=s(`, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`),Qo=l(),ce=r("p"),Xo=s("Note: "),ga=r("em"),Yo=s("power"),Zo=s(` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
`),ot=r("a"),es=s("https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),Ya=l(),B=r("h3"),me=r("a"),_a=r("span"),u(st.$$.fragment),ts=l(),wa=r("span"),as=s("Warmup (TensorFlow)"),Za=l(),J=r("div"),u(it.$$.fragment),rs=l(),va=r("p"),ns=s("Applies a warmup schedule on a given learning rate decay schedule."),er=l(),K=r("h2"),pe=r("a"),ya=r("span"),u(lt.$$.fragment),os=l(),ba=r("span"),ss=s("Gradient Strategies"),tr=l(),Q=r("h3"),de=r("a"),$a=r("span"),u(ct.$$.fragment),is=l(),Aa=r("span"),ls=s("GradientAccumulator (TensorFlow)"),ar=l(),L=r("div"),u(mt.$$.fragment),cs=l(),X=r("p"),ms=s(`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call `),za=r("code"),ps=s(".gradients"),ds=s(", scale the gradients if required, and pass the result to "),Ea=r("code"),hs=s("apply_gradients"),us=s("."),fs=l(),he=r("div"),u(pt.$$.fragment),gs=l(),xa=r("p"),_s=s("Resets the accumulated gradients on the current replica."),this.h()},l(e){const p=Xi('[data-svelte="svelte-1phssyn"]',document.head);W=n(p,"META",{name:!0,content:!0}),p.forEach(a),Da=c(e),S=n(e,"H1",{class:!0});var nr=o(S);Z=n(nr,"A",{id:!0,class:!0,href:!0});var zs=o(Z);yt=n(zs,"SPAN",{});var Es=o(yt);f(_e.$$.fragment,Es),Es.forEach(a),zs.forEach(a),Fr=c(nr),bt=n(nr,"SPAN",{});var xs=o(bt);Cr=i(xs,"Optimization"),xs.forEach(a),nr.forEach(a),La=c(e),ee=n(e,"P",{});var or=o(ee);Or=i(or,"The "),$t=n(or,"CODE",{});var Ts=o($t);Rr=i(Ts,".optimization"),Ts.forEach(a),jr=i(or," module provides:"),or.forEach(a),Pa=c(e),P=n(e,"UL",{});var ut=o(P);At=n(ut,"LI",{});var Ds=o(At);qr=i(Ds,"an optimizer with weight decay fixed that can be used to fine-tuned models, and"),Ds.forEach(a),Ur=c(ut),we=n(ut,"LI",{});var sr=o(we);Gr=i(sr,"several schedules in the form of schedule objects that inherit from "),zt=n(sr,"CODE",{});var Ls=o(zt);Vr=i(Ls,"_LRSchedule"),Ls.forEach(a),Mr=i(sr,":"),sr.forEach(a),Hr=c(ut),Et=n(ut,"LI",{});var Ps=o(Et);Br=i(Ps,"a gradient accumulation class to accumulate the gradients of multiple batches"),Ps.forEach(a),ut.forEach(a),Wa=c(e),I=n(e,"H2",{class:!0});var ir=o(I);te=n(ir,"A",{id:!0,class:!0,href:!0});var Ws=o(te);xt=n(Ws,"SPAN",{});var Ss=o(xt);f(ve.$$.fragment,Ss),Ss.forEach(a),Ws.forEach(a),Jr=c(ir),Tt=n(ir,"SPAN",{});var Is=o(Tt);Kr=i(Is,"AdamW (PyTorch)"),Is.forEach(a),ir.forEach(a),Sa=c(e),E=n(e,"DIV",{class:!0});var ft=o(E);f(ye.$$.fragment,ft),Qr=c(ft),be=n(ft,"P",{});var lr=o(be);Xr=i(lr,"Implements Adam algorithm with weight decay fix as introduced in "),$e=n(lr,"A",{href:!0,rel:!0});var ks=o($e);Yr=i(ks,`Decoupled Weight Decay
Regularization`),ks.forEach(a),Zr=i(lr,"."),lr.forEach(a),en=c(ft),ae=n(ft,"DIV",{class:!0});var cr=o(ae);f(Ae.$$.fragment,cr),tn=c(cr),Dt=n(cr,"P",{});var Ns=o(Dt);an=i(Ns,"Performs a single optimization step."),Ns.forEach(a),cr.forEach(a),ft.forEach(a),Ia=c(e),k=n(e,"H2",{class:!0});var mr=o(k);re=n(mr,"A",{id:!0,class:!0,href:!0});var Fs=o(re);Lt=n(Fs,"SPAN",{});var Cs=o(Lt);f(ze.$$.fragment,Cs),Cs.forEach(a),Fs.forEach(a),rn=c(mr),Pt=n(mr,"SPAN",{});var Os=o(Pt);nn=i(Os,"AdaFactor (PyTorch)"),Os.forEach(a),mr.forEach(a),ka=c(e),h=n(e,"DIV",{class:!0});var y=o(h);f(Ee.$$.fragment,y),on=c(y),dt=n(y,"P",{});var ws=o(dt);sn=i(ws,`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
`),xe=n(ws,"A",{href:!0,rel:!0});var Rs=o(xe);ln=i(Rs,"https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),Rs.forEach(a),ws.forEach(a),cn=c(y),b=n(y,"P",{});var A=o(b);mn=i(A,"Paper: "),Wt=n(A,"EM",{});var js=o(Wt);pn=i(js,"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"),js.forEach(a),dn=c(A),Te=n(A,"A",{href:!0,rel:!0});var qs=o(Te);hn=i(qs,"https://arxiv.org/abs/1804.04235"),qs.forEach(a),un=i(A,` Note that
this optimizer internally adjusts the learning rate depending on the `),St=n(A,"CODE",{});var Us=o(St);fn=i(Us,"scale_parameter"),Us.forEach(a),gn=i(A,", "),It=n(A,"CODE",{});var Gs=o(It);_n=i(Gs,"relative_step"),Gs.forEach(a),wn=i(A,` and
`),kt=n(A,"CODE",{});var Vs=o(kt);vn=i(Vs,"warmup_init"),Vs.forEach(a),yn=i(A," options. To use a manual (external) learning rate schedule you should set "),Nt=n(A,"CODE",{});var Ms=o(Nt);bn=i(Ms,"scale_parameter=False"),Ms.forEach(a),$n=i(A,` and
`),Ft=n(A,"CODE",{});var Hs=o(Ft);An=i(Hs,"relative_step=False"),Hs.forEach(a),zn=i(A,"."),A.forEach(a),En=c(y),Ct=n(y,"P",{});var Bs=o(Ct);xn=i(Bs,"This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested."),Bs.forEach(a),Tn=c(y),De=n(y,"P",{});var pr=o(De);Dn=i(pr,"Recommended T5 finetuning settings ("),Le=n(pr,"A",{href:!0,rel:!0});var Js=o(Le);Ln=i(Js,"https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),Js.forEach(a),Pn=i(pr,"):"),pr.forEach(a),Wn=c(y),x=n(y,"UL",{});var ue=o(x);Pe=n(ue,"LI",{});var dr=o(Pe);Ot=n(dr,"P",{});var Ks=o(Ot);Sn=i(Ks,"Training without LR warmup or clip_threshold is not recommended."),Ks.forEach(a),In=c(dr),We=n(dr,"UL",{});var hr=o(We);Rt=n(hr,"LI",{});var Qs=o(Rt);kn=i(Qs,"use scheduled LR warm-up to fixed LR"),Qs.forEach(a),Nn=c(hr),Se=n(hr,"LI",{});var ur=o(Se);Fn=i(ur,"use clip_threshold=1.0 ("),Ie=n(ur,"A",{href:!0,rel:!0});var Xs=o(Ie);Cn=i(Xs,"https://arxiv.org/abs/1804.04235"),Xs.forEach(a),On=i(ur,")"),ur.forEach(a),hr.forEach(a),dr.forEach(a),Rn=c(ue),jt=n(ue,"LI",{});var Ys=o(jt);qt=n(Ys,"P",{});var Zs=o(qt);jn=i(Zs,"Disable relative updates"),Zs.forEach(a),Ys.forEach(a),qn=c(ue),Ut=n(ue,"LI",{});var ei=o(Ut);Gt=n(ei,"P",{});var ti=o(Gt);Un=i(ti,"Use scale_parameter=False"),ti.forEach(a),ei.forEach(a),Gn=c(ue),Vt=n(ue,"LI",{});var ai=o(Vt);Mt=n(ai,"P",{});var ri=o(Mt);Vn=i(ri,"Additional optimizer operations like gradient clipping should not be used alongside Adafactor"),ri.forEach(a),ai.forEach(a),ue.forEach(a),Mn=c(y),Ht=n(y,"P",{});var ni=o(Ht);Hn=i(ni,"Example:"),ni.forEach(a),Bn=c(y),f(ke.$$.fragment,y),Jn=c(y),Bt=n(y,"P",{});var oi=o(Bt);Kn=i(oi,"Others reported the following combination to work well:"),oi.forEach(a),Qn=c(y),f(Ne.$$.fragment,y),Xn=c(y),T=n(y,"P",{});var fe=o(T);Yn=i(fe,"When using "),Jt=n(fe,"CODE",{});var si=o(Jt);Zn=i(si,"lr=None"),si.forEach(a),eo=i(fe," with "),ht=n(fe,"A",{href:!0});var ii=o(ht);to=i(ii,"Trainer"),ii.forEach(a),ao=i(fe," you will most likely need to use "),Kt=n(fe,"CODE",{});var li=o(Kt);ro=i(li,"AdafactorSchedule"),li.forEach(a),no=i(fe,`
scheduler as following:`),fe.forEach(a),oo=c(y),f(Fe.$$.fragment,y),so=c(y),Qt=n(y,"P",{});var ci=o(Qt);io=i(ci,"Usage:"),ci.forEach(a),lo=c(y),f(Ce.$$.fragment,y),co=c(y),ne=n(y,"DIV",{class:!0});var fr=o(ne);f(Oe.$$.fragment,fr),mo=c(fr),Xt=n(fr,"P",{});var mi=o(Xt);po=i(mi,"Performs a single optimization step"),mi.forEach(a),fr.forEach(a),y.forEach(a),Na=c(e),N=n(e,"H2",{class:!0});var gr=o(N);oe=n(gr,"A",{id:!0,class:!0,href:!0});var pi=o(oe);Yt=n(pi,"SPAN",{});var di=o(Yt);f(Re.$$.fragment,di),di.forEach(a),pi.forEach(a),ho=c(gr),Zt=n(gr,"SPAN",{});var hi=o(Zt);uo=i(hi,"AdamWeightDecay (TensorFlow)"),hi.forEach(a),gr.forEach(a),Fa=c(e),z=n(e,"DIV",{class:!0});var ge=o(z);f(je.$$.fragment,ge),fo=c(ge),F=n(ge,"P",{});var gt=o(F);go=i(gt,`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is `),ea=n(gt,"EM",{});var ui=o(ea);_o=i(ui,"not"),ui.forEach(a),wo=i(gt,` the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in `),qe=n(gt,"A",{href:!0,rel:!0});var fi=o(qe);vo=i(fi,`Decoupled Weight Decay
Regularization`),fi.forEach(a),yo=i(gt,"."),gt.forEach(a),bo=c(ge),ta=n(ge,"P",{});var gi=o(ta);$o=i(gi,`Instead we want ot decay the weights in a manner that doesn\u2019t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`),gi.forEach(a),Ao=c(ge),se=n(ge,"DIV",{class:!0});var _r=o(se);f(Ue.$$.fragment,_r),zo=c(_r),aa=n(_r,"P",{});var _i=o(aa);Eo=i(_i,"Creates an optimizer from its config with WarmUp custom object."),_i.forEach(a),_r.forEach(a),ge.forEach(a),Ca=c(e),C=n(e,"DIV",{class:!0});var wr=o(C);f(Ge.$$.fragment,wr),xo=c(wr),ra=n(wr,"P",{});var wi=o(ra);To=i(wi,"Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay."),wi.forEach(a),wr.forEach(a),Oa=c(e),O=n(e,"H2",{class:!0});var vr=o(O);ie=n(vr,"A",{id:!0,class:!0,href:!0});var vi=o(ie);na=n(vi,"SPAN",{});var yi=o(na);f(Ve.$$.fragment,yi),yi.forEach(a),vi.forEach(a),Do=c(vr),oa=n(vr,"SPAN",{});var bi=o(oa);Lo=i(bi,"Schedules"),bi.forEach(a),vr.forEach(a),Ra=c(e),R=n(e,"H3",{class:!0});var yr=o(R);le=n(yr,"A",{id:!0,class:!0,href:!0});var $i=o(le);sa=n($i,"SPAN",{});var Ai=o(sa);f(Me.$$.fragment,Ai),Ai.forEach(a),$i.forEach(a),Po=c(yr),ia=n(yr,"SPAN",{});var zi=o(ia);Wo=i(zi,"Learning Rate Schedules (Pytorch)"),zi.forEach(a),yr.forEach(a),ja=c(e),j=n(e,"DIV",{class:!0});var br=o(j);f(He.$$.fragment,br),So=c(br),la=n(br,"P",{});var Ei=o(la);Io=i(Ei,"An enumeration."),Ei.forEach(a),br.forEach(a),qa=c(e),q=n(e,"DIV",{class:!0});var $r=o(q);f(Be.$$.fragment,$r),ko=c($r),ca=n($r,"P",{});var xi=o(ca);No=i(xi,"Unified API to get any scheduler from its name."),xi.forEach(a),$r.forEach(a),Ua=c(e),U=n(e,"DIV",{class:!0});var Ar=o(U);f(Je.$$.fragment,Ar),Fo=c(Ar),ma=n(Ar,"P",{});var Ti=o(ma);Co=i(Ti,"Create a schedule with a constant learning rate, using the learning rate set in optimizer."),Ti.forEach(a),Ar.forEach(a),Ga=c(e),G=n(e,"DIV",{class:!0});var zr=o(G);f(Ke.$$.fragment,zr),Oo=c(zr),pa=n(zr,"P",{});var Di=o(pa);Ro=i(Di,`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`),Di.forEach(a),zr.forEach(a),Va=c(e),Qe=n(e,"IMG",{alt:!0,src:!0}),Ma=c(e),V=n(e,"DIV",{class:!0});var Er=o(V);f(Xe.$$.fragment,Er),jo=c(Er),da=n(Er,"P",{});var Li=o(da);qo=i(Li,`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`),Li.forEach(a),Er.forEach(a),Ha=c(e),Ye=n(e,"IMG",{alt:!0,src:!0}),Ba=c(e),M=n(e,"DIV",{class:!0});var xr=o(M);f(Ze.$$.fragment,xr),Uo=c(xr),ha=n(xr,"P",{});var Pi=o(ha);Go=i(Pi,`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`),Pi.forEach(a),xr.forEach(a),Ja=c(e),et=n(e,"IMG",{alt:!0,src:!0}),Ka=c(e),H=n(e,"DIV",{class:!0});var Tr=o(H);f(tt.$$.fragment,Tr),Vo=c(Tr),ua=n(Tr,"P",{});var Wi=o(ua);Mo=i(Wi,`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`),Wi.forEach(a),Tr.forEach(a),Qa=c(e),at=n(e,"IMG",{alt:!0,src:!0}),Xa=c(e),D=n(e,"DIV",{class:!0});var _t=o(D);f(rt.$$.fragment,_t),Ho=c(_t),nt=n(_t,"P",{});var Dr=o(nt);Bo=i(Dr,`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by `),fa=n(Dr,"EM",{});var Si=o(fa);Jo=i(Si,"lr_end"),Si.forEach(a),Ko=i(Dr,`, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`),Dr.forEach(a),Qo=c(_t),ce=n(_t,"P",{});var Ta=o(ce);Xo=i(Ta,"Note: "),ga=n(Ta,"EM",{});var Ii=o(ga);Yo=i(Ii,"power"),Ii.forEach(a),Zo=i(Ta,` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
`),ot=n(Ta,"A",{href:!0,rel:!0});var ki=o(ot);es=i(ki,"https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),ki.forEach(a),Ta.forEach(a),_t.forEach(a),Ya=c(e),B=n(e,"H3",{class:!0});var Lr=o(B);me=n(Lr,"A",{id:!0,class:!0,href:!0});var Ni=o(me);_a=n(Ni,"SPAN",{});var Fi=o(_a);f(st.$$.fragment,Fi),Fi.forEach(a),Ni.forEach(a),ts=c(Lr),wa=n(Lr,"SPAN",{});var Ci=o(wa);as=i(Ci,"Warmup (TensorFlow)"),Ci.forEach(a),Lr.forEach(a),Za=c(e),J=n(e,"DIV",{class:!0});var Pr=o(J);f(it.$$.fragment,Pr),rs=c(Pr),va=n(Pr,"P",{});var Oi=o(va);ns=i(Oi,"Applies a warmup schedule on a given learning rate decay schedule."),Oi.forEach(a),Pr.forEach(a),er=c(e),K=n(e,"H2",{class:!0});var Wr=o(K);pe=n(Wr,"A",{id:!0,class:!0,href:!0});var Ri=o(pe);ya=n(Ri,"SPAN",{});var ji=o(ya);f(lt.$$.fragment,ji),ji.forEach(a),Ri.forEach(a),os=c(Wr),ba=n(Wr,"SPAN",{});var qi=o(ba);ss=i(qi,"Gradient Strategies"),qi.forEach(a),Wr.forEach(a),tr=c(e),Q=n(e,"H3",{class:!0});var Sr=o(Q);de=n(Sr,"A",{id:!0,class:!0,href:!0});var Ui=o(de);$a=n(Ui,"SPAN",{});var Gi=o($a);f(ct.$$.fragment,Gi),Gi.forEach(a),Ui.forEach(a),is=c(Sr),Aa=n(Sr,"SPAN",{});var Vi=o(Aa);ls=i(Vi,"GradientAccumulator (TensorFlow)"),Vi.forEach(a),Sr.forEach(a),ar=c(e),L=n(e,"DIV",{class:!0});var wt=o(L);f(mt.$$.fragment,wt),cs=c(wt),X=n(wt,"P",{});var vt=o(X);ms=i(vt,`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call `),za=n(vt,"CODE",{});var Mi=o(za);ps=i(Mi,".gradients"),Mi.forEach(a),ds=i(vt,", scale the gradients if required, and pass the result to "),Ea=n(vt,"CODE",{});var Hi=o(Ea);hs=i(Hi,"apply_gradients"),Hi.forEach(a),us=i(vt,"."),vt.forEach(a),fs=c(wt),he=n(wt,"DIV",{class:!0});var Ir=o(he);f(pt.$$.fragment,Ir),gs=c(Ir),xa=n(Ir,"P",{});var Bi=o(xa);_s=i(Bi,"Resets the accumulated gradients on the current replica."),Bi.forEach(a),Ir.forEach(a),wt.forEach(a),this.h()},h(){m(W,"name","hf:doc:metadata"),m(W,"content",JSON.stringify(tl)),m(Z,"id","optimization"),m(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(Z,"href","#optimization"),m(S,"class","relative group"),m(te,"id","transformers.AdamW"),m(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(te,"href","#transformers.AdamW"),m(I,"class","relative group"),m($e,"href","https://arxiv.org/abs/1711.05101"),m($e,"rel","nofollow"),m(ae,"class","docstring"),m(E,"class","docstring"),m(re,"id","transformers.Adafactor"),m(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(re,"href","#transformers.Adafactor"),m(k,"class","relative group"),m(xe,"href","https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),m(xe,"rel","nofollow"),m(Te,"href","https://arxiv.org/abs/1804.04235"),m(Te,"rel","nofollow"),m(Le,"href","https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),m(Le,"rel","nofollow"),m(Ie,"href","https://arxiv.org/abs/1804.04235"),m(Ie,"rel","nofollow"),m(ht,"href","/docs/transformers/main/en/main_classes/trainer#transformers.Trainer"),m(ne,"class","docstring"),m(h,"class","docstring"),m(oe,"id","transformers.AdamWeightDecay"),m(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(oe,"href","#transformers.AdamWeightDecay"),m(N,"class","relative group"),m(qe,"href","https://arxiv.org/abs/1711.05101"),m(qe,"rel","nofollow"),m(se,"class","docstring"),m(z,"class","docstring"),m(C,"class","docstring"),m(ie,"id","schedules"),m(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ie,"href","#schedules"),m(O,"class","relative group"),m(le,"id","transformers.SchedulerType"),m(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(le,"href","#transformers.SchedulerType"),m(R,"class","relative group"),m(j,"class","docstring"),m(q,"class","docstring"),m(U,"class","docstring"),m(G,"class","docstring"),m(Qe,"alt",""),kr(Qe.src,ys="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png")||m(Qe,"src",ys),m(V,"class","docstring"),m(Ye,"alt",""),kr(Ye.src,bs="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png")||m(Ye,"src",bs),m(M,"class","docstring"),m(et,"alt",""),kr(et.src,$s="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png")||m(et,"src",$s),m(H,"class","docstring"),m(at,"alt",""),kr(at.src,As="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png")||m(at,"src",As),m(ot,"href","https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),m(ot,"rel","nofollow"),m(D,"class","docstring"),m(me,"id","transformers.WarmUp"),m(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(me,"href","#transformers.WarmUp"),m(B,"class","relative group"),m(J,"class","docstring"),m(pe,"id","gradient-strategies"),m(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(pe,"href","#gradient-strategies"),m(K,"class","relative group"),m(de,"id","transformers.GradientAccumulator"),m(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(de,"href","#transformers.GradientAccumulator"),m(Q,"class","relative group"),m(he,"class","docstring"),m(L,"class","docstring")},m(e,p){t(document.head,W),d(e,Da,p),d(e,S,p),t(S,Z),t(Z,yt),g(_e,yt,null),t(S,Fr),t(S,bt),t(bt,Cr),d(e,La,p),d(e,ee,p),t(ee,Or),t(ee,$t),t($t,Rr),t(ee,jr),d(e,Pa,p),d(e,P,p),t(P,At),t(At,qr),t(P,Ur),t(P,we),t(we,Gr),t(we,zt),t(zt,Vr),t(we,Mr),t(P,Hr),t(P,Et),t(Et,Br),d(e,Wa,p),d(e,I,p),t(I,te),t(te,xt),g(ve,xt,null),t(I,Jr),t(I,Tt),t(Tt,Kr),d(e,Sa,p),d(e,E,p),g(ye,E,null),t(E,Qr),t(E,be),t(be,Xr),t(be,$e),t($e,Yr),t(be,Zr),t(E,en),t(E,ae),g(Ae,ae,null),t(ae,tn),t(ae,Dt),t(Dt,an),d(e,Ia,p),d(e,k,p),t(k,re),t(re,Lt),g(ze,Lt,null),t(k,rn),t(k,Pt),t(Pt,nn),d(e,ka,p),d(e,h,p),g(Ee,h,null),t(h,on),t(h,dt),t(dt,sn),t(dt,xe),t(xe,ln),t(h,cn),t(h,b),t(b,mn),t(b,Wt),t(Wt,pn),t(b,dn),t(b,Te),t(Te,hn),t(b,un),t(b,St),t(St,fn),t(b,gn),t(b,It),t(It,_n),t(b,wn),t(b,kt),t(kt,vn),t(b,yn),t(b,Nt),t(Nt,bn),t(b,$n),t(b,Ft),t(Ft,An),t(b,zn),t(h,En),t(h,Ct),t(Ct,xn),t(h,Tn),t(h,De),t(De,Dn),t(De,Le),t(Le,Ln),t(De,Pn),t(h,Wn),t(h,x),t(x,Pe),t(Pe,Ot),t(Ot,Sn),t(Pe,In),t(Pe,We),t(We,Rt),t(Rt,kn),t(We,Nn),t(We,Se),t(Se,Fn),t(Se,Ie),t(Ie,Cn),t(Se,On),t(x,Rn),t(x,jt),t(jt,qt),t(qt,jn),t(x,qn),t(x,Ut),t(Ut,Gt),t(Gt,Un),t(x,Gn),t(x,Vt),t(Vt,Mt),t(Mt,Vn),t(h,Mn),t(h,Ht),t(Ht,Hn),t(h,Bn),g(ke,h,null),t(h,Jn),t(h,Bt),t(Bt,Kn),t(h,Qn),g(Ne,h,null),t(h,Xn),t(h,T),t(T,Yn),t(T,Jt),t(Jt,Zn),t(T,eo),t(T,ht),t(ht,to),t(T,ao),t(T,Kt),t(Kt,ro),t(T,no),t(h,oo),g(Fe,h,null),t(h,so),t(h,Qt),t(Qt,io),t(h,lo),g(Ce,h,null),t(h,co),t(h,ne),g(Oe,ne,null),t(ne,mo),t(ne,Xt),t(Xt,po),d(e,Na,p),d(e,N,p),t(N,oe),t(oe,Yt),g(Re,Yt,null),t(N,ho),t(N,Zt),t(Zt,uo),d(e,Fa,p),d(e,z,p),g(je,z,null),t(z,fo),t(z,F),t(F,go),t(F,ea),t(ea,_o),t(F,wo),t(F,qe),t(qe,vo),t(F,yo),t(z,bo),t(z,ta),t(ta,$o),t(z,Ao),t(z,se),g(Ue,se,null),t(se,zo),t(se,aa),t(aa,Eo),d(e,Ca,p),d(e,C,p),g(Ge,C,null),t(C,xo),t(C,ra),t(ra,To),d(e,Oa,p),d(e,O,p),t(O,ie),t(ie,na),g(Ve,na,null),t(O,Do),t(O,oa),t(oa,Lo),d(e,Ra,p),d(e,R,p),t(R,le),t(le,sa),g(Me,sa,null),t(R,Po),t(R,ia),t(ia,Wo),d(e,ja,p),d(e,j,p),g(He,j,null),t(j,So),t(j,la),t(la,Io),d(e,qa,p),d(e,q,p),g(Be,q,null),t(q,ko),t(q,ca),t(ca,No),d(e,Ua,p),d(e,U,p),g(Je,U,null),t(U,Fo),t(U,ma),t(ma,Co),d(e,Ga,p),d(e,G,p),g(Ke,G,null),t(G,Oo),t(G,pa),t(pa,Ro),d(e,Va,p),d(e,Qe,p),d(e,Ma,p),d(e,V,p),g(Xe,V,null),t(V,jo),t(V,da),t(da,qo),d(e,Ha,p),d(e,Ye,p),d(e,Ba,p),d(e,M,p),g(Ze,M,null),t(M,Uo),t(M,ha),t(ha,Go),d(e,Ja,p),d(e,et,p),d(e,Ka,p),d(e,H,p),g(tt,H,null),t(H,Vo),t(H,ua),t(ua,Mo),d(e,Qa,p),d(e,at,p),d(e,Xa,p),d(e,D,p),g(rt,D,null),t(D,Ho),t(D,nt),t(nt,Bo),t(nt,fa),t(fa,Jo),t(nt,Ko),t(D,Qo),t(D,ce),t(ce,Xo),t(ce,ga),t(ga,Yo),t(ce,Zo),t(ce,ot),t(ot,es),d(e,Ya,p),d(e,B,p),t(B,me),t(me,_a),g(st,_a,null),t(B,ts),t(B,wa),t(wa,as),d(e,Za,p),d(e,J,p),g(it,J,null),t(J,rs),t(J,va),t(va,ns),d(e,er,p),d(e,K,p),t(K,pe),t(pe,ya),g(lt,ya,null),t(K,os),t(K,ba),t(ba,ss),d(e,tr,p),d(e,Q,p),t(Q,de),t(de,$a),g(ct,$a,null),t(Q,is),t(Q,Aa),t(Aa,ls),d(e,ar,p),d(e,L,p),g(mt,L,null),t(L,cs),t(L,X),t(X,ms),t(X,za),t(za,ps),t(X,ds),t(X,Ea),t(Ea,hs),t(X,us),t(L,fs),t(L,he),g(pt,he,null),t(he,gs),t(he,xa),t(xa,_s),rr=!0},p:Yi,i(e){rr||(_(_e.$$.fragment,e),_(ve.$$.fragment,e),_(ye.$$.fragment,e),_(Ae.$$.fragment,e),_(ze.$$.fragment,e),_(Ee.$$.fragment,e),_(ke.$$.fragment,e),_(Ne.$$.fragment,e),_(Fe.$$.fragment,e),_(Ce.$$.fragment,e),_(Oe.$$.fragment,e),_(Re.$$.fragment,e),_(je.$$.fragment,e),_(Ue.$$.fragment,e),_(Ge.$$.fragment,e),_(Ve.$$.fragment,e),_(Me.$$.fragment,e),_(He.$$.fragment,e),_(Be.$$.fragment,e),_(Je.$$.fragment,e),_(Ke.$$.fragment,e),_(Xe.$$.fragment,e),_(Ze.$$.fragment,e),_(tt.$$.fragment,e),_(rt.$$.fragment,e),_(st.$$.fragment,e),_(it.$$.fragment,e),_(lt.$$.fragment,e),_(ct.$$.fragment,e),_(mt.$$.fragment,e),_(pt.$$.fragment,e),rr=!0)},o(e){w(_e.$$.fragment,e),w(ve.$$.fragment,e),w(ye.$$.fragment,e),w(Ae.$$.fragment,e),w(ze.$$.fragment,e),w(Ee.$$.fragment,e),w(ke.$$.fragment,e),w(Ne.$$.fragment,e),w(Fe.$$.fragment,e),w(Ce.$$.fragment,e),w(Oe.$$.fragment,e),w(Re.$$.fragment,e),w(je.$$.fragment,e),w(Ue.$$.fragment,e),w(Ge.$$.fragment,e),w(Ve.$$.fragment,e),w(Me.$$.fragment,e),w(He.$$.fragment,e),w(Be.$$.fragment,e),w(Je.$$.fragment,e),w(Ke.$$.fragment,e),w(Xe.$$.fragment,e),w(Ze.$$.fragment,e),w(tt.$$.fragment,e),w(rt.$$.fragment,e),w(st.$$.fragment,e),w(it.$$.fragment,e),w(lt.$$.fragment,e),w(ct.$$.fragment,e),w(mt.$$.fragment,e),w(pt.$$.fragment,e),rr=!1},d(e){a(W),e&&a(Da),e&&a(S),v(_e),e&&a(La),e&&a(ee),e&&a(Pa),e&&a(P),e&&a(Wa),e&&a(I),v(ve),e&&a(Sa),e&&a(E),v(ye),v(Ae),e&&a(Ia),e&&a(k),v(ze),e&&a(ka),e&&a(h),v(Ee),v(ke),v(Ne),v(Fe),v(Ce),v(Oe),e&&a(Na),e&&a(N),v(Re),e&&a(Fa),e&&a(z),v(je),v(Ue),e&&a(Ca),e&&a(C),v(Ge),e&&a(Oa),e&&a(O),v(Ve),e&&a(Ra),e&&a(R),v(Me),e&&a(ja),e&&a(j),v(He),e&&a(qa),e&&a(q),v(Be),e&&a(Ua),e&&a(U),v(Je),e&&a(Ga),e&&a(G),v(Ke),e&&a(Va),e&&a(Qe),e&&a(Ma),e&&a(V),v(Xe),e&&a(Ha),e&&a(Ye),e&&a(Ba),e&&a(M),v(Ze),e&&a(Ja),e&&a(et),e&&a(Ka),e&&a(H),v(tt),e&&a(Qa),e&&a(at),e&&a(Xa),e&&a(D),v(rt),e&&a(Ya),e&&a(B),v(st),e&&a(Za),e&&a(J),v(it),e&&a(er),e&&a(K),v(lt),e&&a(tr),e&&a(Q),v(ct),e&&a(ar),e&&a(L),v(mt),v(pt)}}}const tl={local:"optimization",sections:[{local:"transformers.AdamW",title:"AdamW (PyTorch)"},{local:"transformers.Adafactor",title:"AdaFactor (PyTorch)"},{local:"transformers.AdamWeightDecay",title:"AdamWeightDecay (TensorFlow)"},{local:"schedules",sections:[{local:"transformers.SchedulerType",title:"Learning Rate Schedules (Pytorch)"},{local:"transformers.WarmUp",title:"Warmup (TensorFlow)"}],title:"Schedules"},{local:"gradient-strategies",sections:[{local:"transformers.GradientAccumulator",title:"GradientAccumulator (TensorFlow)"}],title:"Gradient Strategies"}],title:"Optimization"};function al(vs){return Zi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class il extends Ji{constructor(W){super();Ki(this,W,al,el,Qi,{})}}export{il as default,tl as metadata};
