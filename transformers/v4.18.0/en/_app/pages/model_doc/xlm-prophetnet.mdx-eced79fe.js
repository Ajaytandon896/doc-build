import{S as gn,i as _n,s as kn,e as o,k as d,w as v,t as l,M as vn,c as r,d as s,m as c,a as n,x as $,h as p,b as i,F as t,g as m,y as b,q as P,o as w,B as N,v as $n,L as Bt}from"../../chunks/vendor-6b77c823.js";import{D as C}from"../../chunks/Docstring-1088f2fb.js";import{C as Rt}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as W}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Gt}from"../../chunks/ExampleCodeBlock-5212b321.js";function bn(q){let f,L,_,u,k;return u=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetModel

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetModel.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

last_hidden_states = outputs.last_hidden_state  # main stream hidden states
last_hidden_states_ngram = outputs.last_hidden_state_ngram  # predict hidden states`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetModel.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># main stream hidden states</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states_ngram = outputs.last_hidden_state_ngram  <span class="hljs-comment"># predict hidden states</span>`}}),{c(){f=o("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=r(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Bt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function Pn(q){let f,L,_,u,k;return u=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetEncoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetEncoder.from_pretrained("patrickvonplaten/xprophetnet-large-uncased-standalone")
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetEncoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetEncoder.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/xprophetnet-large-uncased-standalone&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){f=o("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=r(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Bt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function wn(q){let f,L,_,u,k;return u=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetDecoder
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetDecoder.from_pretrained(
    "patrickvonplaten/xprophetnet-large-uncased-standalone", add_cross_attention=False
)
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetDecoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetDecoder.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;patrickvonplaten/xprophetnet-large-uncased-standalone&quot;</span>, add_cross_attention=<span class="hljs-literal">False</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){f=o("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=r(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Bt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function Nn(q){let f,L,_,u,k;return u=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetForConditionalGeneration.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

logits_next_token = outputs.logits  # logits to predict next token as usual
logits_ngram_next_tokens = outputs.logits_ngram  # logits to predict 2nd, 3rd, ... next tokens`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits_next_token = outputs.logits  <span class="hljs-comment"># logits to predict next token as usual</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_ngram_next_tokens = outputs.logits_ngram  <span class="hljs-comment"># logits to predict 2nd, 3rd, ... next tokens</span>`}}),{c(){f=o("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=r(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Bt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function Ln(q){let f,L,_,u,k;return u=new Rt({props:{code:`from transformers import XLMProphetNetTokenizer, XLMProphetNetForCausalLM
import torch

tokenizer = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = XLMProphetNetForCausalLM.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

# Model can also be used with EncoderDecoder framework
from transformers import EncoderDecoderModel, XLMProphetNetTokenizer, XLMRobertaTokenizer
import torch

tokenizer_enc = XLMRobertaTokenizer.from_pretrained("xlm-roberta-large")
tokenizer_dec = XLMProphetNetTokenizer.from_pretrained("microsoft/xprophetnet-large-wiki100-cased")
model = EncoderDecoderModel.from_encoder_decoder_pretrained(
    "xlm-roberta-large", "microsoft/xprophetnet-large-wiki100-cased"
)

ARTICLE = (
    "the us state department said wednesday it had received no "
    "formal word from bolivia that it was expelling the us ambassador there "
    "but said the charges made against him are \`\` baseless ."
)
input_ids = tokenizer_enc(ARTICLE, return_tensors="pt").input_ids
labels = tokenizer_dec("us rejects charges against its ambassador in bolivia", return_tensors="pt").input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-1], labels=labels[:, 1:])

loss = outputs.loss`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLMProphetNetTokenizer, XLMProphetNetForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = XLMProphetNetForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Model can also be used with EncoderDecoder framework</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> EncoderDecoderModel, XLMProphetNetTokenizer, XLMRobertaTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_enc = XLMRobertaTokenizer.from_pretrained(<span class="hljs-string">&quot;xlm-roberta-large&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_dec = XLMProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;xlm-roberta-large&quot;</span>, <span class="hljs-string">&quot;microsoft/xprophetnet-large-wiki100-cased&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ARTICLE = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;the us state department said wednesday it had received no &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;formal word from bolivia that it was expelling the us ambassador there &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;but said the charges made against him are \`\` baseless .&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_enc(ARTICLE, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer_dec(<span class="hljs-string">&quot;us rejects charges against its ambassador in bolivia&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-<span class="hljs-number">1</span>], labels=labels[:, <span class="hljs-number">1</span>:])

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){f=o("p"),L=l("Example:"),_=d(),v(u.$$.fragment)},l(a){f=r(a,"P",{});var g=n(f);L=p(g,"Example:"),g.forEach(s),_=c(a),$(u.$$.fragment,a)},m(a,g){m(a,f,g),t(f,L),m(a,_,g),b(u,a,g),k=!0},p:Bt,i(a){k||(P(u.$$.fragment,a),k=!0)},o(a){w(u.$$.fragment,a),k=!1},d(a){a&&s(f),a&&s(_),N(u,a)}}}function Mn(q){let f,L,_,u,k,a,g,mt,Cs,Ot,A,ut,Ds,As,ue,Ss,Is,Ht,S,U,ft,fe,Fs,gt,Gs,Vt,Y,Bs,ge,Rs,Os,Wt,We,Hs,Ut,Ue,Vs,Yt,Ye,_t,Ws,Jt,J,Us,_e,Ys,Js,Qt,I,Q,kt,ke,Qs,vt,Zs,Zt,F,ve,Ks,$e,eo,Je,to,so,Kt,G,Z,$t,be,oo,bt,ro,es,M,Pe,no,E,ao,Qe,io,lo,Ze,po,co,we,ho,mo,uo,Ne,fo,Ke,go,_o,ko,D,Le,vo,Pt,$o,bo,Me,et,Po,wt,wo,No,tt,Lo,Nt,Mo,xo,K,xe,qo,Lt,Eo,Xo,ee,qe,yo,Mt,jo,zo,te,Ee,To,Xe,Co,xt,Do,Ao,ts,B,se,qt,ye,So,Et,Io,ss,X,je,Fo,ze,Go,st,Bo,Ro,Oo,oe,os,R,re,Xt,Te,Ho,yt,Vo,rs,y,Ce,Wo,De,Uo,ot,Yo,Jo,Qo,ne,ns,O,ae,jt,Ae,Zo,zt,Ko,as,j,Se,er,Ie,tr,rt,sr,or,rr,ie,is,H,le,Tt,Fe,nr,Ct,ar,ls,z,Ge,ir,Be,lr,nt,pr,dr,cr,pe,ps,V,de,Dt,Re,hr,At,mr,ds,T,Oe,ur,He,fr,at,gr,_r,kr,ce,cs;return a=new W({}),fe=new W({}),ke=new W({}),ve=new C({props:{name:"class transformers.XLMProphetNetConfig",anchor:"transformers.XLMProphetNetConfig",parameters:[{name:"activation_dropout",val:" = 0.1"},{name:"activation_function",val:" = 'gelu'"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"num_encoder_layers",val:" = 12"},{name:"num_encoder_attention_heads",val:" = 16"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"num_decoder_layers",val:" = 12"},{name:"num_decoder_attention_heads",val:" = 16"},{name:"attention_dropout",val:" = 0.1"},{name:"dropout",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"init_std",val:" = 0.02"},{name:"is_encoder_decoder",val:" = True"},{name:"add_cross_attention",val:" = True"},{name:"decoder_start_token_id",val:" = 0"},{name:"ngram",val:" = 2"},{name:"num_buckets",val:" = 32"},{name:"relative_max_distance",val:" = 128"},{name:"disable_ngram_loss",val:" = False"},{name:"eps",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/configuration_xlm_prophetnet.py#L29"}}),be=new W({}),Pe=new C({props:{name:"class transformers.XLMProphetNetTokenizer",anchor:"transformers.XLMProphetNetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '[SEP]'"},{name:"eos_token",val:" = '[SEP]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"unk_token",val:" = '[UNK]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"sp_model_kwargs",val:": typing.Union[typing.Dict[str, typing.Any], NoneType] = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.XLMProphetNetTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.XLMProphetNetTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.XLMProphetNetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.XLMProphetNetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.XLMProphetNetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.XLMProphetNetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.XLMProphetNetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.XLMProphetNetTokenizer.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (<code>List[str]</code>, <em>optional</em>, defaults to <code>[&quot;&lt;s&gt;NOTUSED&quot;, &quot;&lt;/s&gt;NOTUSED&quot;]</code>) &#x2014;
Additional special tokens used by the tokenizer.`,name:"additional_special_tokens"},{anchor:"transformers.XLMProphetNetTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.XLMProphetNetTokenizer.sp_model",description:`<strong>sp_model</strong> (<code>SentencePieceProcessor</code>) &#x2014;
The <em>SentencePiece</em> processor that is used for every conversion (string, tokens and IDs).`,name:"sp_model"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L57"}}),Le=new C({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L314",returnDescription:`
<p>list of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),xe=new C({props:{name:"convert_tokens_to_string",anchor:"transformers.XLMProphetNetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L292"}}),qe=new C({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L241",returnDescription:`
<p>List of zeros.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Ee=new C({props:{name:"get_special_tokens_mask",anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.XLMProphetNetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py#L213",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ye=new W({}),je=new C({props:{name:"class transformers.XLMProphetNetModel",anchor:"transformers.XLMProphetNetModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L86"}}),oe=new Gt({props:{anchor:"transformers.XLMProphetNetModel.example",$$slots:{default:[bn]},$$scope:{ctx:q}}}),Te=new W({}),Ce=new C({props:{name:"class transformers.XLMProphetNetEncoder",anchor:"transformers.XLMProphetNetEncoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L38"}}),ne=new Gt({props:{anchor:"transformers.XLMProphetNetEncoder.example",$$slots:{default:[Pn]},$$scope:{ctx:q}}}),Ae=new W({}),Se=new C({props:{name:"class transformers.XLMProphetNetDecoder",anchor:"transformers.XLMProphetNetDecoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L61"}}),ie=new Gt({props:{anchor:"transformers.XLMProphetNetDecoder.example",$$slots:{default:[wn]},$$scope:{ctx:q}}}),Fe=new W({}),Ge=new C({props:{name:"class transformers.XLMProphetNetForConditionalGeneration",anchor:"transformers.XLMProphetNetForConditionalGeneration",parameters:[{name:"config",val:": ProphetNetConfig"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L112"}}),pe=new Gt({props:{anchor:"transformers.XLMProphetNetForConditionalGeneration.example",$$slots:{default:[Nn]},$$scope:{ctx:q}}}),Re=new W({}),Oe=new C({props:{name:"class transformers.XLMProphetNetForCausalLM",anchor:"transformers.XLMProphetNetForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py#L138"}}),ce=new Gt({props:{anchor:"transformers.XLMProphetNetForCausalLM.example",$$slots:{default:[Ln]},$$scope:{ctx:q}}}),{c(){f=o("meta"),L=d(),_=o("h1"),u=o("a"),k=o("span"),v(a.$$.fragment),g=d(),mt=o("span"),Cs=l("XLM-ProphetNet"),Ot=d(),A=o("p"),ut=o("strong"),Ds=l("DISCLAIMER:"),As=l(" If you see something strange, file a "),ue=o("a"),Ss=l("Github Issue"),Is=l(` and assign
@patrickvonplaten`),Ht=d(),S=o("h2"),U=o("a"),ft=o("span"),v(fe.$$.fragment),Fs=d(),gt=o("span"),Gs=l("Overview"),Vt=d(),Y=o("p"),Bs=l("The XLM-ProphetNet model was proposed in "),ge=o("a"),Rs=l("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),Os=l(` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Wt=d(),We=o("p"),Hs=l(`XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of
just the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual
\u201Cwiki100\u201D Wikipedia dump.`),Ut=d(),Ue=o("p"),Vs=l("The abstract from the paper is the following:"),Yt=d(),Ye=o("p"),_t=o("em"),Ws=l(`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),Jt=d(),J=o("p"),Us=l("The Authors\u2019 code can be found "),_e=o("a"),Ys=l("here"),Js=l("."),Qt=d(),I=o("h2"),Q=o("a"),kt=o("span"),v(ke.$$.fragment),Qs=d(),vt=o("span"),Zs=l("XLMProphetNetConfig"),Zt=d(),F=o("div"),v(ve.$$.fragment),Ks=d(),$e=o("p"),eo=l("This class overrides "),Je=o("a"),to=l("ProphetNetConfig"),so=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Kt=d(),G=o("h2"),Z=o("a"),$t=o("span"),v(be.$$.fragment),oo=d(),bt=o("span"),ro=l("XLMProphetNetTokenizer"),es=d(),M=o("div"),v(Pe.$$.fragment),no=d(),E=o("p"),ao=l("Adapted from "),Qe=o("a"),io=l("RobertaTokenizer"),lo=l(" and "),Ze=o("a"),po=l("XLNetTokenizer"),co=l(`. Based on
`),we=o("a"),ho=l("SentencePiece"),mo=l("."),uo=d(),Ne=o("p"),fo=l("This tokenizer inherits from "),Ke=o("a"),go=l("PreTrainedTokenizer"),_o=l(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),ko=d(),D=o("div"),v(Le.$$.fragment),vo=d(),Pt=o("p"),$o=l(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A XLMProphetNet sequence has the following format:`),bo=d(),Me=o("ul"),et=o("li"),Po=l("single sequence: "),wt=o("code"),wo=l("X [SEP]"),No=d(),tt=o("li"),Lo=l("pair of sequences: "),Nt=o("code"),Mo=l("A [SEP] B [SEP]"),xo=d(),K=o("div"),v(xe.$$.fragment),qo=d(),Lt=o("p"),Eo=l("Converts a sequence of tokens (strings for sub-words) in a single string."),Xo=d(),ee=o("div"),v(qe.$$.fragment),yo=d(),Mt=o("p"),jo=l(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. XLMProphetNet
does not make use of token type ids, therefore a list of zeros is returned.`),zo=d(),te=o("div"),v(Ee.$$.fragment),To=d(),Xe=o("p"),Co=l(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),xt=o("code"),Do=l("prepare_for_model"),Ao=l(" method."),ts=d(),B=o("h2"),se=o("a"),qt=o("span"),v(ye.$$.fragment),So=d(),Et=o("span"),Io=l("XLMProphetNetModel"),ss=d(),X=o("div"),v(je.$$.fragment),Fo=d(),ze=o("p"),Go=l("This class overrides "),st=o("a"),Bo=l("ProphetNetModel"),Ro=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Oo=d(),v(oe.$$.fragment),os=d(),R=o("h2"),re=o("a"),Xt=o("span"),v(Te.$$.fragment),Ho=d(),yt=o("span"),Vo=l("XLMProphetNetEncoder"),rs=d(),y=o("div"),v(Ce.$$.fragment),Wo=d(),De=o("p"),Uo=l("This class overrides "),ot=o("a"),Yo=l("ProphetNetEncoder"),Jo=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Qo=d(),v(ne.$$.fragment),ns=d(),O=o("h2"),ae=o("a"),jt=o("span"),v(Ae.$$.fragment),Zo=d(),zt=o("span"),Ko=l("XLMProphetNetDecoder"),as=d(),j=o("div"),v(Se.$$.fragment),er=d(),Ie=o("p"),tr=l("This class overrides "),rt=o("a"),sr=l("ProphetNetDecoder"),or=l(`. Please check the superclass for the appropriate documentation alongside
usage examples.`),rr=d(),v(ie.$$.fragment),is=d(),H=o("h2"),le=o("a"),Tt=o("span"),v(Fe.$$.fragment),nr=d(),Ct=o("span"),ar=l("XLMProphetNetForConditionalGeneration"),ls=d(),z=o("div"),v(Ge.$$.fragment),ir=d(),Be=o("p"),lr=l("This class overrides "),nt=o("a"),pr=l("ProphetNetForConditionalGeneration"),dr=l(`. Please check the superclass for the appropriate
documentation alongside usage examples.`),cr=d(),v(pe.$$.fragment),ps=d(),V=o("h2"),de=o("a"),Dt=o("span"),v(Re.$$.fragment),hr=d(),At=o("span"),mr=l("XLMProphetNetForCausalLM"),ds=d(),T=o("div"),v(Oe.$$.fragment),ur=d(),He=o("p"),fr=l("This class overrides "),at=o("a"),gr=l("ProphetNetForCausalLM"),_r=l(`. Please check the superclass for the appropriate documentation
alongside usage examples.`),kr=d(),v(ce.$$.fragment),this.h()},l(e){const h=vn('[data-svelte="svelte-1phssyn"]',document.head);f=r(h,"META",{name:!0,content:!0}),h.forEach(s),L=c(e),_=r(e,"H1",{class:!0});var Ve=n(_);u=r(Ve,"A",{id:!0,class:!0,href:!0});var St=n(u);k=r(St,"SPAN",{});var It=n(k);$(a.$$.fragment,It),It.forEach(s),St.forEach(s),g=c(Ve),mt=r(Ve,"SPAN",{});var Ft=n(mt);Cs=p(Ft,"XLM-ProphetNet"),Ft.forEach(s),Ve.forEach(s),Ot=c(e),A=r(e,"P",{});var he=n(A);ut=r(he,"STRONG",{});var br=n(ut);Ds=p(br,"DISCLAIMER:"),br.forEach(s),As=p(he," If you see something strange, file a "),ue=r(he,"A",{href:!0,rel:!0});var Pr=n(ue);Ss=p(Pr,"Github Issue"),Pr.forEach(s),Is=p(he,` and assign
@patrickvonplaten`),he.forEach(s),Ht=c(e),S=r(e,"H2",{class:!0});var hs=n(S);U=r(hs,"A",{id:!0,class:!0,href:!0});var wr=n(U);ft=r(wr,"SPAN",{});var Nr=n(ft);$(fe.$$.fragment,Nr),Nr.forEach(s),wr.forEach(s),Fs=c(hs),gt=r(hs,"SPAN",{});var Lr=n(gt);Gs=p(Lr,"Overview"),Lr.forEach(s),hs.forEach(s),Vt=c(e),Y=r(e,"P",{});var ms=n(Y);Bs=p(ms,"The XLM-ProphetNet model was proposed in "),ge=r(ms,"A",{href:!0,rel:!0});var Mr=n(ge);Rs=p(Mr,"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),Mr.forEach(s),Os=p(ms,` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),ms.forEach(s),Wt=c(e),We=r(e,"P",{});var xr=n(We);Hs=p(xr,`XLM-ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of
just the next token. Its architecture is identical to ProhpetNet, but the model was trained on the multi-lingual
\u201Cwiki100\u201D Wikipedia dump.`),xr.forEach(s),Ut=c(e),Ue=r(e,"P",{});var qr=n(Ue);Vs=p(qr,"The abstract from the paper is the following:"),qr.forEach(s),Yt=c(e),Ye=r(e,"P",{});var Er=n(Ye);_t=r(Er,"EM",{});var Xr=n(_t);Ws=p(Xr,`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),Xr.forEach(s),Er.forEach(s),Jt=c(e),J=r(e,"P",{});var us=n(J);Us=p(us,"The Authors\u2019 code can be found "),_e=r(us,"A",{href:!0,rel:!0});var yr=n(_e);Ys=p(yr,"here"),yr.forEach(s),Js=p(us,"."),us.forEach(s),Qt=c(e),I=r(e,"H2",{class:!0});var fs=n(I);Q=r(fs,"A",{id:!0,class:!0,href:!0});var jr=n(Q);kt=r(jr,"SPAN",{});var zr=n(kt);$(ke.$$.fragment,zr),zr.forEach(s),jr.forEach(s),Qs=c(fs),vt=r(fs,"SPAN",{});var Tr=n(vt);Zs=p(Tr,"XLMProphetNetConfig"),Tr.forEach(s),fs.forEach(s),Zt=c(e),F=r(e,"DIV",{class:!0});var gs=n(F);$(ve.$$.fragment,gs),Ks=c(gs),$e=r(gs,"P",{});var _s=n($e);eo=p(_s,"This class overrides "),Je=r(_s,"A",{href:!0});var Cr=n(Je);to=p(Cr,"ProphetNetConfig"),Cr.forEach(s),so=p(_s,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),_s.forEach(s),gs.forEach(s),Kt=c(e),G=r(e,"H2",{class:!0});var ks=n(G);Z=r(ks,"A",{id:!0,class:!0,href:!0});var Dr=n(Z);$t=r(Dr,"SPAN",{});var Ar=n($t);$(be.$$.fragment,Ar),Ar.forEach(s),Dr.forEach(s),oo=c(ks),bt=r(ks,"SPAN",{});var Sr=n(bt);ro=p(Sr,"XLMProphetNetTokenizer"),Sr.forEach(s),ks.forEach(s),es=c(e),M=r(e,"DIV",{class:!0});var x=n(M);$(Pe.$$.fragment,x),no=c(x),E=r(x,"P",{});var me=n(E);ao=p(me,"Adapted from "),Qe=r(me,"A",{href:!0});var Ir=n(Qe);io=p(Ir,"RobertaTokenizer"),Ir.forEach(s),lo=p(me," and "),Ze=r(me,"A",{href:!0});var Fr=n(Ze);po=p(Fr,"XLNetTokenizer"),Fr.forEach(s),co=p(me,`. Based on
`),we=r(me,"A",{href:!0,rel:!0});var Gr=n(we);ho=p(Gr,"SentencePiece"),Gr.forEach(s),mo=p(me,"."),me.forEach(s),uo=c(x),Ne=r(x,"P",{});var vs=n(Ne);fo=p(vs,"This tokenizer inherits from "),Ke=r(vs,"A",{href:!0});var Br=n(Ke);go=p(Br,"PreTrainedTokenizer"),Br.forEach(s),_o=p(vs,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),vs.forEach(s),ko=c(x),D=r(x,"DIV",{class:!0});var it=n(D);$(Le.$$.fragment,it),vo=c(it),Pt=r(it,"P",{});var Rr=n(Pt);$o=p(Rr,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A XLMProphetNet sequence has the following format:`),Rr.forEach(s),bo=c(it),Me=r(it,"UL",{});var $s=n(Me);et=r($s,"LI",{});var vr=n(et);Po=p(vr,"single sequence: "),wt=r(vr,"CODE",{});var Or=n(wt);wo=p(Or,"X [SEP]"),Or.forEach(s),vr.forEach(s),No=c($s),tt=r($s,"LI",{});var $r=n(tt);Lo=p($r,"pair of sequences: "),Nt=r($r,"CODE",{});var Hr=n(Nt);Mo=p(Hr,"A [SEP] B [SEP]"),Hr.forEach(s),$r.forEach(s),$s.forEach(s),it.forEach(s),xo=c(x),K=r(x,"DIV",{class:!0});var bs=n(K);$(xe.$$.fragment,bs),qo=c(bs),Lt=r(bs,"P",{});var Vr=n(Lt);Eo=p(Vr,"Converts a sequence of tokens (strings for sub-words) in a single string."),Vr.forEach(s),bs.forEach(s),Xo=c(x),ee=r(x,"DIV",{class:!0});var Ps=n(ee);$(qe.$$.fragment,Ps),yo=c(Ps),Mt=r(Ps,"P",{});var Wr=n(Mt);jo=p(Wr,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. XLMProphetNet
does not make use of token type ids, therefore a list of zeros is returned.`),Wr.forEach(s),Ps.forEach(s),zo=c(x),te=r(x,"DIV",{class:!0});var ws=n(te);$(Ee.$$.fragment,ws),To=c(ws),Xe=r(ws,"P",{});var Ns=n(Xe);Co=p(Ns,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),xt=r(Ns,"CODE",{});var Ur=n(xt);Do=p(Ur,"prepare_for_model"),Ur.forEach(s),Ao=p(Ns," method."),Ns.forEach(s),ws.forEach(s),x.forEach(s),ts=c(e),B=r(e,"H2",{class:!0});var Ls=n(B);se=r(Ls,"A",{id:!0,class:!0,href:!0});var Yr=n(se);qt=r(Yr,"SPAN",{});var Jr=n(qt);$(ye.$$.fragment,Jr),Jr.forEach(s),Yr.forEach(s),So=c(Ls),Et=r(Ls,"SPAN",{});var Qr=n(Et);Io=p(Qr,"XLMProphetNetModel"),Qr.forEach(s),Ls.forEach(s),ss=c(e),X=r(e,"DIV",{class:!0});var lt=n(X);$(je.$$.fragment,lt),Fo=c(lt),ze=r(lt,"P",{});var Ms=n(ze);Go=p(Ms,"This class overrides "),st=r(Ms,"A",{href:!0});var Zr=n(st);Bo=p(Zr,"ProphetNetModel"),Zr.forEach(s),Ro=p(Ms,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Ms.forEach(s),Oo=c(lt),$(oe.$$.fragment,lt),lt.forEach(s),os=c(e),R=r(e,"H2",{class:!0});var xs=n(R);re=r(xs,"A",{id:!0,class:!0,href:!0});var Kr=n(re);Xt=r(Kr,"SPAN",{});var en=n(Xt);$(Te.$$.fragment,en),en.forEach(s),Kr.forEach(s),Ho=c(xs),yt=r(xs,"SPAN",{});var tn=n(yt);Vo=p(tn,"XLMProphetNetEncoder"),tn.forEach(s),xs.forEach(s),rs=c(e),y=r(e,"DIV",{class:!0});var pt=n(y);$(Ce.$$.fragment,pt),Wo=c(pt),De=r(pt,"P",{});var qs=n(De);Uo=p(qs,"This class overrides "),ot=r(qs,"A",{href:!0});var sn=n(ot);Yo=p(sn,"ProphetNetEncoder"),sn.forEach(s),Jo=p(qs,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),qs.forEach(s),Qo=c(pt),$(ne.$$.fragment,pt),pt.forEach(s),ns=c(e),O=r(e,"H2",{class:!0});var Es=n(O);ae=r(Es,"A",{id:!0,class:!0,href:!0});var on=n(ae);jt=r(on,"SPAN",{});var rn=n(jt);$(Ae.$$.fragment,rn),rn.forEach(s),on.forEach(s),Zo=c(Es),zt=r(Es,"SPAN",{});var nn=n(zt);Ko=p(nn,"XLMProphetNetDecoder"),nn.forEach(s),Es.forEach(s),as=c(e),j=r(e,"DIV",{class:!0});var dt=n(j);$(Se.$$.fragment,dt),er=c(dt),Ie=r(dt,"P",{});var Xs=n(Ie);tr=p(Xs,"This class overrides "),rt=r(Xs,"A",{href:!0});var an=n(rt);sr=p(an,"ProphetNetDecoder"),an.forEach(s),or=p(Xs,`. Please check the superclass for the appropriate documentation alongside
usage examples.`),Xs.forEach(s),rr=c(dt),$(ie.$$.fragment,dt),dt.forEach(s),is=c(e),H=r(e,"H2",{class:!0});var ys=n(H);le=r(ys,"A",{id:!0,class:!0,href:!0});var ln=n(le);Tt=r(ln,"SPAN",{});var pn=n(Tt);$(Fe.$$.fragment,pn),pn.forEach(s),ln.forEach(s),nr=c(ys),Ct=r(ys,"SPAN",{});var dn=n(Ct);ar=p(dn,"XLMProphetNetForConditionalGeneration"),dn.forEach(s),ys.forEach(s),ls=c(e),z=r(e,"DIV",{class:!0});var ct=n(z);$(Ge.$$.fragment,ct),ir=c(ct),Be=r(ct,"P",{});var js=n(Be);lr=p(js,"This class overrides "),nt=r(js,"A",{href:!0});var cn=n(nt);pr=p(cn,"ProphetNetForConditionalGeneration"),cn.forEach(s),dr=p(js,`. Please check the superclass for the appropriate
documentation alongside usage examples.`),js.forEach(s),cr=c(ct),$(pe.$$.fragment,ct),ct.forEach(s),ps=c(e),V=r(e,"H2",{class:!0});var zs=n(V);de=r(zs,"A",{id:!0,class:!0,href:!0});var hn=n(de);Dt=r(hn,"SPAN",{});var mn=n(Dt);$(Re.$$.fragment,mn),mn.forEach(s),hn.forEach(s),hr=c(zs),At=r(zs,"SPAN",{});var un=n(At);mr=p(un,"XLMProphetNetForCausalLM"),un.forEach(s),zs.forEach(s),ds=c(e),T=r(e,"DIV",{class:!0});var ht=n(T);$(Oe.$$.fragment,ht),ur=c(ht),He=r(ht,"P",{});var Ts=n(He);fr=p(Ts,"This class overrides "),at=r(Ts,"A",{href:!0});var fn=n(at);gr=p(fn,"ProphetNetForCausalLM"),fn.forEach(s),_r=p(Ts,`. Please check the superclass for the appropriate documentation
alongside usage examples.`),Ts.forEach(s),kr=c(ht),$(ce.$$.fragment,ht),ht.forEach(s),this.h()},h(){i(f,"name","hf:doc:metadata"),i(f,"content",JSON.stringify(xn)),i(u,"id","xlmprophetnet"),i(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(u,"href","#xlmprophetnet"),i(_,"class","relative group"),i(ue,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),i(ue,"rel","nofollow"),i(U,"id","overview"),i(U,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(U,"href","#overview"),i(S,"class","relative group"),i(ge,"href","https://arxiv.org/abs/2001.04063"),i(ge,"rel","nofollow"),i(_e,"href","https://github.com/microsoft/ProphetNet"),i(_e,"rel","nofollow"),i(Q,"id","transformers.XLMProphetNetConfig"),i(Q,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Q,"href","#transformers.XLMProphetNetConfig"),i(I,"class","relative group"),i(Je,"href","/docs/transformers/v4.18.0/en/model_doc/prophetnet#transformers.ProphetNetConfig"),i(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(Z,"id","transformers.XLMProphetNetTokenizer"),i(Z,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Z,"href","#transformers.XLMProphetNetTokenizer"),i(G,"class","relative group"),i(Qe,"href","/docs/transformers/v4.18.0/en/model_doc/roberta#transformers.RobertaTokenizer"),i(Ze,"href","/docs/transformers/v4.18.0/en/model_doc/xlnet#transformers.XLNetTokenizer"),i(we,"href","https://github.com/google/sentencepiece"),i(we,"rel","nofollow"),i(Ke,"href","/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),i(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(se,"id","transformers.XLMProphetNetModel"),i(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(se,"href","#transformers.XLMProphetNetModel"),i(B,"class","relative group"),i(st,"href","/docs/transformers/v4.18.0/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(re,"id","transformers.XLMProphetNetEncoder"),i(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(re,"href","#transformers.XLMProphetNetEncoder"),i(R,"class","relative group"),i(ot,"href","/docs/transformers/v4.18.0/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(ae,"id","transformers.XLMProphetNetDecoder"),i(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(ae,"href","#transformers.XLMProphetNetDecoder"),i(O,"class","relative group"),i(rt,"href","/docs/transformers/v4.18.0/en/model_doc/prophetnet#transformers.ProphetNetDecoder"),i(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(le,"id","transformers.XLMProphetNetForConditionalGeneration"),i(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(le,"href","#transformers.XLMProphetNetForConditionalGeneration"),i(H,"class","relative group"),i(nt,"href","/docs/transformers/v4.18.0/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),i(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),i(de,"id","transformers.XLMProphetNetForCausalLM"),i(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(de,"href","#transformers.XLMProphetNetForCausalLM"),i(V,"class","relative group"),i(at,"href","/docs/transformers/v4.18.0/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),i(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,h){t(document.head,f),m(e,L,h),m(e,_,h),t(_,u),t(u,k),b(a,k,null),t(_,g),t(_,mt),t(mt,Cs),m(e,Ot,h),m(e,A,h),t(A,ut),t(ut,Ds),t(A,As),t(A,ue),t(ue,Ss),t(A,Is),m(e,Ht,h),m(e,S,h),t(S,U),t(U,ft),b(fe,ft,null),t(S,Fs),t(S,gt),t(gt,Gs),m(e,Vt,h),m(e,Y,h),t(Y,Bs),t(Y,ge),t(ge,Rs),t(Y,Os),m(e,Wt,h),m(e,We,h),t(We,Hs),m(e,Ut,h),m(e,Ue,h),t(Ue,Vs),m(e,Yt,h),m(e,Ye,h),t(Ye,_t),t(_t,Ws),m(e,Jt,h),m(e,J,h),t(J,Us),t(J,_e),t(_e,Ys),t(J,Js),m(e,Qt,h),m(e,I,h),t(I,Q),t(Q,kt),b(ke,kt,null),t(I,Qs),t(I,vt),t(vt,Zs),m(e,Zt,h),m(e,F,h),b(ve,F,null),t(F,Ks),t(F,$e),t($e,eo),t($e,Je),t(Je,to),t($e,so),m(e,Kt,h),m(e,G,h),t(G,Z),t(Z,$t),b(be,$t,null),t(G,oo),t(G,bt),t(bt,ro),m(e,es,h),m(e,M,h),b(Pe,M,null),t(M,no),t(M,E),t(E,ao),t(E,Qe),t(Qe,io),t(E,lo),t(E,Ze),t(Ze,po),t(E,co),t(E,we),t(we,ho),t(E,mo),t(M,uo),t(M,Ne),t(Ne,fo),t(Ne,Ke),t(Ke,go),t(Ne,_o),t(M,ko),t(M,D),b(Le,D,null),t(D,vo),t(D,Pt),t(Pt,$o),t(D,bo),t(D,Me),t(Me,et),t(et,Po),t(et,wt),t(wt,wo),t(Me,No),t(Me,tt),t(tt,Lo),t(tt,Nt),t(Nt,Mo),t(M,xo),t(M,K),b(xe,K,null),t(K,qo),t(K,Lt),t(Lt,Eo),t(M,Xo),t(M,ee),b(qe,ee,null),t(ee,yo),t(ee,Mt),t(Mt,jo),t(M,zo),t(M,te),b(Ee,te,null),t(te,To),t(te,Xe),t(Xe,Co),t(Xe,xt),t(xt,Do),t(Xe,Ao),m(e,ts,h),m(e,B,h),t(B,se),t(se,qt),b(ye,qt,null),t(B,So),t(B,Et),t(Et,Io),m(e,ss,h),m(e,X,h),b(je,X,null),t(X,Fo),t(X,ze),t(ze,Go),t(ze,st),t(st,Bo),t(ze,Ro),t(X,Oo),b(oe,X,null),m(e,os,h),m(e,R,h),t(R,re),t(re,Xt),b(Te,Xt,null),t(R,Ho),t(R,yt),t(yt,Vo),m(e,rs,h),m(e,y,h),b(Ce,y,null),t(y,Wo),t(y,De),t(De,Uo),t(De,ot),t(ot,Yo),t(De,Jo),t(y,Qo),b(ne,y,null),m(e,ns,h),m(e,O,h),t(O,ae),t(ae,jt),b(Ae,jt,null),t(O,Zo),t(O,zt),t(zt,Ko),m(e,as,h),m(e,j,h),b(Se,j,null),t(j,er),t(j,Ie),t(Ie,tr),t(Ie,rt),t(rt,sr),t(Ie,or),t(j,rr),b(ie,j,null),m(e,is,h),m(e,H,h),t(H,le),t(le,Tt),b(Fe,Tt,null),t(H,nr),t(H,Ct),t(Ct,ar),m(e,ls,h),m(e,z,h),b(Ge,z,null),t(z,ir),t(z,Be),t(Be,lr),t(Be,nt),t(nt,pr),t(Be,dr),t(z,cr),b(pe,z,null),m(e,ps,h),m(e,V,h),t(V,de),t(de,Dt),b(Re,Dt,null),t(V,hr),t(V,At),t(At,mr),m(e,ds,h),m(e,T,h),b(Oe,T,null),t(T,ur),t(T,He),t(He,fr),t(He,at),t(at,gr),t(He,_r),t(T,kr),b(ce,T,null),cs=!0},p(e,[h]){const Ve={};h&2&&(Ve.$$scope={dirty:h,ctx:e}),oe.$set(Ve);const St={};h&2&&(St.$$scope={dirty:h,ctx:e}),ne.$set(St);const It={};h&2&&(It.$$scope={dirty:h,ctx:e}),ie.$set(It);const Ft={};h&2&&(Ft.$$scope={dirty:h,ctx:e}),pe.$set(Ft);const he={};h&2&&(he.$$scope={dirty:h,ctx:e}),ce.$set(he)},i(e){cs||(P(a.$$.fragment,e),P(fe.$$.fragment,e),P(ke.$$.fragment,e),P(ve.$$.fragment,e),P(be.$$.fragment,e),P(Pe.$$.fragment,e),P(Le.$$.fragment,e),P(xe.$$.fragment,e),P(qe.$$.fragment,e),P(Ee.$$.fragment,e),P(ye.$$.fragment,e),P(je.$$.fragment,e),P(oe.$$.fragment,e),P(Te.$$.fragment,e),P(Ce.$$.fragment,e),P(ne.$$.fragment,e),P(Ae.$$.fragment,e),P(Se.$$.fragment,e),P(ie.$$.fragment,e),P(Fe.$$.fragment,e),P(Ge.$$.fragment,e),P(pe.$$.fragment,e),P(Re.$$.fragment,e),P(Oe.$$.fragment,e),P(ce.$$.fragment,e),cs=!0)},o(e){w(a.$$.fragment,e),w(fe.$$.fragment,e),w(ke.$$.fragment,e),w(ve.$$.fragment,e),w(be.$$.fragment,e),w(Pe.$$.fragment,e),w(Le.$$.fragment,e),w(xe.$$.fragment,e),w(qe.$$.fragment,e),w(Ee.$$.fragment,e),w(ye.$$.fragment,e),w(je.$$.fragment,e),w(oe.$$.fragment,e),w(Te.$$.fragment,e),w(Ce.$$.fragment,e),w(ne.$$.fragment,e),w(Ae.$$.fragment,e),w(Se.$$.fragment,e),w(ie.$$.fragment,e),w(Fe.$$.fragment,e),w(Ge.$$.fragment,e),w(pe.$$.fragment,e),w(Re.$$.fragment,e),w(Oe.$$.fragment,e),w(ce.$$.fragment,e),cs=!1},d(e){s(f),e&&s(L),e&&s(_),N(a),e&&s(Ot),e&&s(A),e&&s(Ht),e&&s(S),N(fe),e&&s(Vt),e&&s(Y),e&&s(Wt),e&&s(We),e&&s(Ut),e&&s(Ue),e&&s(Yt),e&&s(Ye),e&&s(Jt),e&&s(J),e&&s(Qt),e&&s(I),N(ke),e&&s(Zt),e&&s(F),N(ve),e&&s(Kt),e&&s(G),N(be),e&&s(es),e&&s(M),N(Pe),N(Le),N(xe),N(qe),N(Ee),e&&s(ts),e&&s(B),N(ye),e&&s(ss),e&&s(X),N(je),N(oe),e&&s(os),e&&s(R),N(Te),e&&s(rs),e&&s(y),N(Ce),N(ne),e&&s(ns),e&&s(O),N(Ae),e&&s(as),e&&s(j),N(Se),N(ie),e&&s(is),e&&s(H),N(Fe),e&&s(ls),e&&s(z),N(Ge),N(pe),e&&s(ps),e&&s(V),N(Re),e&&s(ds),e&&s(T),N(Oe),N(ce)}}}const xn={local:"xlmprophetnet",sections:[{local:"overview",title:"Overview"},{local:"transformers.XLMProphetNetConfig",title:"XLMProphetNetConfig"},{local:"transformers.XLMProphetNetTokenizer",title:"XLMProphetNetTokenizer"},{local:"transformers.XLMProphetNetModel",title:"XLMProphetNetModel"},{local:"transformers.XLMProphetNetEncoder",title:"XLMProphetNetEncoder"},{local:"transformers.XLMProphetNetDecoder",title:"XLMProphetNetDecoder"},{local:"transformers.XLMProphetNetForConditionalGeneration",title:"XLMProphetNetForConditionalGeneration"},{local:"transformers.XLMProphetNetForCausalLM",title:"XLMProphetNetForCausalLM"}],title:"XLM-ProphetNet"};function qn(q){return $n(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Tn extends gn{constructor(f){super();_n(this,f,qn,Mn,kn,{})}}export{Tn as default,xn as metadata};
