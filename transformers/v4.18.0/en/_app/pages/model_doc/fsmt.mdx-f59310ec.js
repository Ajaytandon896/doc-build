import{S as Na,i as La,s as ja,e as n,k as l,w as T,t as a,M as Oa,c as s,d as o,m as c,a as r,x as b,h as i,b as h,F as e,g as _,y,q as w,o as M,B as F,v as Ga,L as Ko}from"../../chunks/vendor-6b77c823.js";import{T as Da}from"../../chunks/Tip-39098574.js";import{D as L}from"../../chunks/Docstring-1088f2fb.js";import{C as Yo}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as nt}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Xo}from"../../chunks/ExampleCodeBlock-5212b321.js";function Wa(E){let m,k,g,p,v;return p=new Yo({props:{code:`from transformers import FSMTConfig, FSMTModel

config = FSMTConfig.from_pretrained("facebook/wmt19-en-ru")
model = FSMTModel(config)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FSMTConfig, FSMTModel

<span class="hljs-meta">&gt;&gt;&gt; </span>config = FSMTConfig.from_pretrained(<span class="hljs-string">&quot;facebook/wmt19-en-ru&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FSMTModel(config)`}}),{c(){m=n("p"),k=a("Examples:"),g=l(),T(p.$$.fragment)},l(d){m=s(d,"P",{});var u=r(m);k=i(u,"Examples:"),u.forEach(o),g=c(d),b(p.$$.fragment,d)},m(d,u){_(d,m,u),e(m,k),_(d,g,u),y(p,d,u),v=!0},p:Ko,i(d){v||(w(p.$$.fragment,d),v=!0)},o(d){M(p.$$.fragment,d),v=!1},d(d){d&&o(m),d&&o(g),F(p,d)}}}function Ra(E){let m,k,g,p,v;return p=new Yo({props:{code:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),{c(){m=n("p"),k=a("Transformer sequence pair mask has the following format:"),g=l(),T(p.$$.fragment)},l(d){m=s(d,"P",{});var u=r(m);k=i(u,"Transformer sequence pair mask has the following format:"),u.forEach(o),g=c(d),b(p.$$.fragment,d)},m(d,u){_(d,m,u),e(m,k),_(d,g,u),y(p,d,u),v=!0},p:Ko,i(d){v||(w(p.$$.fragment,d),v=!0)},o(d){M(p.$$.fragment,d),v=!1},d(d){d&&o(m),d&&o(g),F(p,d)}}}function Ba(E){let m,k,g,p,v;return{c(){m=n("p"),k=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),p=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){m=s(d,"P",{});var u=r(m);k=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s(u,"CODE",{});var A=r(g);p=i(A,"Module"),A.forEach(o),v=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(o)},m(d,u){_(d,m,u),e(m,k),e(m,g),e(g,p),e(m,v)},d(d){d&&o(m)}}}function Va(E){let m,k,g,p,v;return p=new Yo({props:{code:`from transformers import FSMTTokenizer, FSMTModel
import torch

tokenizer = FSMTTokenizer.from_pretrained("facebook/wmt19-ru-en")
model = FSMTModel.from_pretrained("facebook/wmt19-ru-en")

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FSMTTokenizer, FSMTModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FSMTTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/wmt19-ru-en&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FSMTModel.from_pretrained(<span class="hljs-string">&quot;facebook/wmt19-ru-en&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),{c(){m=n("p"),k=a("Example:"),g=l(),T(p.$$.fragment)},l(d){m=s(d,"P",{});var u=r(m);k=i(u,"Example:"),u.forEach(o),g=c(d),b(p.$$.fragment,d)},m(d,u){_(d,m,u),e(m,k),_(d,g,u),y(p,d,u),v=!0},p:Ko,i(d){v||(w(p.$$.fragment,d),v=!0)},o(d){M(p.$$.fragment,d),v=!1},d(d){d&&o(m),d&&o(g),F(p,d)}}}function Ua(E){let m,k,g,p,v;return{c(){m=n("p"),k=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),g=n("code"),p=a("Module"),v=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){m=s(d,"P",{});var u=r(m);k=i(u,"Although the recipe for forward pass needs to be defined within this function, one should call the "),g=s(u,"CODE",{});var A=r(g);p=i(A,"Module"),A.forEach(o),v=i(u,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),u.forEach(o)},m(d,u){_(d,m,u),e(m,k),e(m,g),e(g,p),e(m,v)},d(d){d&&o(m)}}}function Ha(E){let m,k,g,p,v;return p=new Yo({props:{code:`from transformers import FSMTTokenizer, FSMTForConditionalGeneration

mname = "facebook/wmt19-ru-en"
model = FSMTForConditionalGeneration.from_pretrained(mname)
tokenizer = FSMTTokenizer.from_pretrained(mname)

src_text = "\u041C\u0430\u0448\u0438\u043D\u043D\u043E\u0435 \u043E\u0431\u0443\u0447\u0435\u043D\u0438\u0435 - \u044D\u0442\u043E \u0437\u0434\u043E\u0440\u043E\u0432\u043E, \u043D\u0435 \u0442\u0430\u043A \u043B\u0438?"
input_ids = tokenizer(src_text, return_tensors="pt").input_ids
outputs = model.generate(input_ids, num_beams=5, num_return_sequences=3)
tokenizer.decode(outputs[0], skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FSMTTokenizer, FSMTForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>mname = <span class="hljs-string">&quot;facebook/wmt19-ru-en&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FSMTForConditionalGeneration.from_pretrained(mname)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = FSMTTokenizer.from_pretrained(mname)

<span class="hljs-meta">&gt;&gt;&gt; </span>src_text = <span class="hljs-string">&quot;\u041C\u0430\u0448\u0438\u043D\u043D\u043E\u0435 \u043E\u0431\u0443\u0447\u0435\u043D\u0438\u0435 - \u044D\u0442\u043E \u0437\u0434\u043E\u0440\u043E\u0432\u043E, \u043D\u0435 \u0442\u0430\u043A \u043B\u0438?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(src_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, num_beams=<span class="hljs-number">5</span>, num_return_sequences=<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&quot;Machine learning is great, isn&#x27;t it?&quot;</span>`}}),{c(){m=n("p"),k=a("Translation example::"),g=l(),T(p.$$.fragment)},l(d){m=s(d,"P",{});var u=r(m);k=i(u,"Translation example::"),u.forEach(o),g=c(d),b(p.$$.fragment,d)},m(d,u){_(d,m,u),e(m,k),_(d,g,u),y(p,d,u),v=!0},p:Ko,i(d){v||(w(p.$$.fragment,d),v=!0)},o(d){M(p.$$.fragment,d),v=!1},d(d){d&&o(m),d&&o(g),F(p,d)}}}function Qa(E){let m,k,g,p,v,d,u,A,Jo,po,V,zt,Zo,en,Me,tn,on,uo,U,se,qt,Fe,nn,xt,sn,_o,re,rn,$e,an,dn,go,st,ln,vo,rt,Et,cn,ko,j,hn,Se,mn,fn,ze,pn,un,To,H,ae,Ct,qe,_n,Pt,gn,bo,at,Q,vn,it,kn,Tn,dt,bn,yn,yo,X,ie,It,xe,wn,At,Mn,wo,z,Ee,Fn,Ce,$n,lt,Sn,zn,qn,K,xn,ct,En,Cn,ht,Pn,In,An,de,Dn,le,Pe,Nn,Y,Ln,Dt,jn,On,Nt,Gn,Wn,Mo,J,ce,Lt,Ie,Rn,jt,Bn,Fo,$,Ae,Vn,Ot,Un,Hn,D,Gt,Qn,Xn,Wt,Kn,Yn,N,Jn,Rt,Zn,es,Bt,ts,os,Vt,ns,ss,rs,De,as,Ut,is,ds,ls,Ne,cs,mt,hs,ms,fs,O,Le,ps,Ht,us,_s,je,ft,gs,Qt,vs,ks,pt,Ts,Xt,bs,ys,he,Oe,ws,Ge,Ms,Kt,Fs,$s,Ss,C,We,zs,Yt,qs,xs,me,Es,Z,Cs,Jt,Ps,Is,Zt,As,Ds,Ns,eo,Ls,js,ut,Re,$o,ee,fe,to,Be,Os,oo,Gs,So,q,Ve,Ws,no,Rs,Bs,Ue,Vs,_t,Us,Hs,Qs,He,Xs,Qe,Ks,Ys,Js,P,Xe,Zs,te,er,gt,tr,or,so,nr,sr,rr,pe,ar,ue,zo,oe,_e,ro,Ke,ir,ao,dr,qo,x,Ye,lr,io,cr,hr,Je,mr,vt,fr,pr,ur,Ze,_r,et,gr,vr,kr,I,tt,Tr,ne,br,kt,yr,wr,lo,Mr,Fr,$r,ge,Sr,ve,xo;return d=new nt({}),Fe=new nt({}),qe=new nt({}),xe=new nt({}),Ee=new L({props:{name:"class transformers.FSMTConfig",anchor:"transformers.FSMTConfig",parameters:[{name:"langs",val:" = ['en', 'de']"},{name:"src_vocab_size",val:" = 42024"},{name:"tgt_vocab_size",val:" = 42024"},{name:"activation_function",val:" = 'relu'"},{name:"d_model",val:" = 1024"},{name:"max_length",val:" = 200"},{name:"max_position_embeddings",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"encoder_layers",val:" = 12"},{name:"encoder_attention_heads",val:" = 16"},{name:"encoder_layerdrop",val:" = 0.0"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"decoder_layers",val:" = 12"},{name:"decoder_attention_heads",val:" = 16"},{name:"decoder_layerdrop",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"dropout",val:" = 0.1"},{name:"activation_dropout",val:" = 0.0"},{name:"init_std",val:" = 0.02"},{name:"decoder_start_token_id",val:" = 2"},{name:"is_encoder_decoder",val:" = True"},{name:"scale_embedding",val:" = True"},{name:"tie_word_embeddings",val:" = False"},{name:"num_beams",val:" = 5"},{name:"length_penalty",val:" = 1.0"},{name:"early_stopping",val:" = False"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"forced_eos_token_id",val:" = 2"},{name:"**common_kwargs",val:""}],parametersDescription:[{anchor:"transformers.FSMTConfig.langs",description:`<strong>langs</strong> (<code>List[str]</code>) &#x2014;
A list with source language and target_language (e.g., [&#x2018;en&#x2019;, &#x2018;ru&#x2019;]).`,name:"langs"},{anchor:"transformers.FSMTConfig.src_vocab_size",description:`<strong>src_vocab_size</strong> (<code>int</code>) &#x2014;
Vocabulary size of the encoder. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed to the forward method in the encoder.`,name:"src_vocab_size"},{anchor:"transformers.FSMTConfig.tgt_vocab_size",description:`<strong>tgt_vocab_size</strong> (<code>int</code>) &#x2014;
Vocabulary size of the decoder. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed to the forward method in the decoder.`,name:"tgt_vocab_size"},{anchor:"transformers.FSMTConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.FSMTConfig.encoder_layers",description:`<strong>encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"encoder_layers"},{anchor:"transformers.FSMTConfig.decoder_layers",description:`<strong>decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"decoder_layers"},{anchor:"transformers.FSMTConfig.encoder_attention_heads",description:`<strong>encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"encoder_attention_heads"},{anchor:"transformers.FSMTConfig.decoder_attention_heads",description:`<strong>decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"decoder_attention_heads"},{anchor:"transformers.FSMTConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.FSMTConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.FSMTConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.FSMTConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.FSMTConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.FSMTConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.FSMTConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.FSMTConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.FSMTConfig.scale_embedding",description:`<strong>scale_embedding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Scale embeddings by diving by sqrt(d_model).`,name:"scale_embedding"},{anchor:"transformers.FSMTConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.FSMTConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.FSMTConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.FSMTConfig.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
This model starts decoding with <code>eos_token_id</code>
encoder_layerdrop &#x2014; (<code>float</code>, <em>optional</em>, defaults to 0.0):
Google &#x201C;layerdrop arxiv&#x201D;, as its not explainable in one line.
decoder_layerdrop &#x2014; (<code>float</code>, <em>optional</em>, defaults to 0.0):
Google &#x201C;layerdrop arxiv&#x201D;, as its not explainable in one line.`,name:"decoder_start_token_id"},{anchor:"transformers.FSMTConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether this is an encoder/decoder model.`,name:"is_encoder_decoder"},{anchor:"transformers.FSMTConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to tie input and output embeddings.`,name:"tie_word_embeddings"},{anchor:"transformers.FSMTConfig.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 5) &#x2014;
Number of beams for beam search that will be used by default in the <code>generate</code> method of the model. 1 means
no beam search.`,name:"num_beams"},{anchor:"transformers.FSMTConfig.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
Exponential penalty to the length that will be used by default in the <code>generate</code> method of the model.`,name:"length_penalty"},{anchor:"transformers.FSMTConfig.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Flag that will be used by default in the <code>generate</code> method of the model. Whether to stop the beam search
when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.FSMTConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.FSMTConfig.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached. Usually set to
<code>eos_token_id</code>.`,name:"forced_eos_token_id"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/configuration_fsmt.py#L41"}}),de=new Xo({props:{anchor:"transformers.FSMTConfig.example",$$slots:{default:[Wa]},$$scope:{ctx:E}}}),Pe=new L({props:{name:"to_dict",anchor:"transformers.FSMTConfig.to_dict",parameters:[],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/configuration_fsmt.py#L209",returnDescription:`
<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`
<p><code>Dict[str, any]</code></p>
`}}),Ie=new nt({}),Ae=new L({props:{name:"class transformers.FSMTTokenizer",anchor:"transformers.FSMTTokenizer",parameters:[{name:"langs",val:" = None"},{name:"src_vocab_file",val:" = None"},{name:"tgt_vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"do_lower_case",val:" = False"},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"sep_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FSMTTokenizer.langs",description:`<strong>langs</strong> (<code>List[str]</code>) &#x2014;
A list of two languages to translate from and to, for instance <code>[&quot;en&quot;, &quot;ru&quot;]</code>.`,name:"langs"},{anchor:"transformers.FSMTTokenizer.src_vocab_file",description:`<strong>src_vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary for the source language.`,name:"src_vocab_file"},{anchor:"transformers.FSMTTokenizer.tgt_vocab_file",description:`<strong>tgt_vocab_file</strong> (<code>st</code>) &#x2014;
File containing the vocabulary for the target language.`,name:"tgt_vocab_file"},{anchor:"transformers.FSMTTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
File containing the merges.`,name:"merges_file"},{anchor:"transformers.FSMTTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.FSMTTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.FSMTTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.FSMTTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.FSMTTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/tokenization_fsmt.py#L137"}}),Le=new L({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.FSMTTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.FSMTTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.FSMTTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/tokenization_fsmt.py#L397",returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),Oe=new L({props:{name:"get_special_tokens_mask",anchor:"transformers.FSMTTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.FSMTTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FSMTTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.FSMTTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/tokenization_fsmt.py#L423",returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),We=new L({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.FSMTTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],parametersDescription:[{anchor:"transformers.FSMTTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.FSMTTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/tokenization_fsmt.py#L451",returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),me=new Xo({props:{anchor:"transformers.FSMTTokenizer.create_token_type_ids_from_sequences.example",$$slots:{default:[Ra]},$$scope:{ctx:E}}}),Re=new L({props:{name:"save_vocabulary",anchor:"transformers.FSMTTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": typing.Optional[str] = None"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/tokenization_fsmt.py#L484"}}),Be=new nt({}),Ve=new L({props:{name:"class transformers.FSMTModel",anchor:"transformers.FSMTModel",parameters:[{name:"config",val:": FSMTConfig"}],parametersDescription:[{anchor:"transformers.FSMTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/modeling_fsmt.py#L989"}}),Xe=new L({props:{name:"forward",anchor:"transformers.FSMTModel.forward",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:": typing.Optional[typing.Tuple] = None"},{name:"past_key_values",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.FSMTModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>IIndices can be obtained using <code>FSTMTokenizer</code>. See <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.18.0/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FSMTModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FSMTModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.18.0/en/model_doc/fsmt#transformers.FSMTTokenizer">FSMTTokenizer</a>. See <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.18.0/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>FSMT uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If <code>past_key_values</code>
is used, optionally only the last <code>decoder_input_ids</code> have to be input (see <code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.FSMTModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FSMTModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FSMTModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.FSMTModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.FSMTModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>Tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code> is a sequence of hidden-states at
the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.FSMTModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.
If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.FSMTModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.FSMTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FSMTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FSMTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/modeling_fsmt.py#L1003",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>transformers.modeling_outputs.Seq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/fsmt#transformers.FSMTConfig"
>FSMTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.modeling_outputs.Seq2SeqModelOutput"
>transformers.modeling_outputs.Seq2SeqModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pe=new Da({props:{$$slots:{default:[Ba]},$$scope:{ctx:E}}}),ue=new Xo({props:{anchor:"transformers.FSMTModel.forward.example",$$slots:{default:[Va]},$$scope:{ctx:E}}}),Ke=new nt({}),Ye=new L({props:{name:"class transformers.FSMTForConditionalGeneration",anchor:"transformers.FSMTForConditionalGeneration",parameters:[{name:"config",val:": FSMTConfig"}],parametersDescription:[{anchor:"transformers.FSMTForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/modeling_fsmt.py#L1113"}}),tt=new L({props:{name:"forward",anchor:"transformers.FSMTForConditionalGeneration.forward",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.FSMTForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>IIndices can be obtained using <code>FSTMTokenizer</code>. See <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.18.0/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FSMTForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/v4.18.0/en/model_doc/fsmt#transformers.FSMTTokenizer">FSMTTokenizer</a>. See <a href="/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/v4.18.0/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>FSMT uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If <code>past_key_values</code>
is used, optionally only the last <code>decoder_input_ids</code> have to be input (see <code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.FSMTForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.FSMTForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>Tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code> is a sequence of hidden-states at
the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.FSMTForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.
If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.FSMTForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.FSMTForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FSMTForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FSMTForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FSMTForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"labels"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/fsmt/modeling_fsmt.py#L1129",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/fsmt#transformers.FSMTConfig"
>FSMTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder\u2019s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.modeling_outputs.Seq2SeqLMOutput"
>transformers.modeling_outputs.Seq2SeqLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ge=new Da({props:{$$slots:{default:[Ua]},$$scope:{ctx:E}}}),ve=new Xo({props:{anchor:"transformers.FSMTForConditionalGeneration.forward.example",$$slots:{default:[Ha]},$$scope:{ctx:E}}}),{c(){m=n("meta"),k=l(),g=n("h1"),p=n("a"),v=n("span"),T(d.$$.fragment),u=l(),A=n("span"),Jo=a("FSMT"),po=l(),V=n("p"),zt=n("strong"),Zo=a("DISCLAIMER:"),en=a(" If you see something strange, file a "),Me=n("a"),tn=a("Github Issue"),on=a(` and assign
@stas00.`),uo=l(),U=n("h2"),se=n("a"),qt=n("span"),T(Fe.$$.fragment),nn=l(),xt=n("span"),sn=a("Overview"),_o=l(),re=n("p"),rn=a("FSMT (FairSeq MachineTranslation) models were introduced in "),$e=n("a"),an=a("Facebook FAIR\u2019s WMT19 News Translation Task Submission"),dn=a(" by Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov."),go=l(),st=n("p"),ln=a("The abstract of the paper is the following:"),vo=l(),rt=n("p"),Et=n("em"),cn=a(`This paper describes Facebook FAIR\u2019s submission to the WMT19 shared news translation task. We participate in two
language pairs and four language directions, English <-> German and English <-> Russian. Following our submission from
last year, our baseline systems are large BPE-based transformer models trained with the Fairseq sequence modeling
toolkit which rely on sampled back-translations. This year we experiment with different bitext data filtering schemes,
as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific
data, then decode using noisy channel model reranking. Our submissions are ranked first in all four directions of the
human evaluation campaign. On En->De, our system significantly outperforms other systems as well as human translations.
This system improves upon our WMT\u201918 submission by 4.5 BLEU points.`),ko=l(),j=n("p"),hn=a("This model was contributed by "),Se=n("a"),mn=a("stas"),fn=a(`. The original code can be found
`),ze=n("a"),pn=a("here"),un=a("."),To=l(),H=n("h2"),ae=n("a"),Ct=n("span"),T(qe.$$.fragment),_n=l(),Pt=n("span"),gn=a("Implementation Notes"),bo=l(),at=n("ul"),Q=n("li"),vn=a(`FSMT uses source and target vocabulary pairs that aren\u2019t combined into one. It doesn\u2019t share embeddings tokens
either. Its tokenizer is very similar to `),it=n("a"),kn=a("XLMTokenizer"),Tn=a(` and the main model is derived from
`),dt=n("a"),bn=a("BartModel"),yn=a("."),yo=l(),X=n("h2"),ie=n("a"),It=n("span"),T(xe.$$.fragment),wn=l(),At=n("span"),Mn=a("FSMTConfig"),wo=l(),z=n("div"),T(Ee.$$.fragment),Fn=l(),Ce=n("p"),$n=a("This is the configuration class to store the configuration of a "),lt=n("a"),Sn=a("FSMTModel"),zn=a(`. It is used to instantiate a FSMT
model according to the specified arguments, defining the model architecture.`),qn=l(),K=n("p"),xn=a("Configuration objects inherit from "),ct=n("a"),En=a("PretrainedConfig"),Cn=a(` and can be used to control the model outputs. Read the
documentation from `),ht=n("a"),Pn=a("PretrainedConfig"),In=a(" for more information."),An=l(),T(de.$$.fragment),Dn=l(),le=n("div"),T(Pe.$$.fragment),Nn=l(),Y=n("p"),Ln=a("Serializes this instance to a Python dictionary. Override the default "),Dt=n("em"),jn=a("to_dict()"),On=a(" from "),Nt=n("em"),Gn=a("PretrainedConfig"),Wn=a("."),Mo=l(),J=n("h2"),ce=n("a"),Lt=n("span"),T(Ie.$$.fragment),Rn=l(),jt=n("span"),Bn=a("FSMTTokenizer"),Fo=l(),$=n("div"),T(Ae.$$.fragment),Vn=l(),Ot=n("p"),Un=a("Construct an FAIRSEQ Transformer tokenizer. Based on Byte-Pair Encoding. The tokenization process is the following:"),Hn=l(),D=n("ul"),Gt=n("li"),Qn=a("Moses preprocessing and tokenization."),Xn=l(),Wt=n("li"),Kn=a("Normalizing all inputs text."),Yn=l(),N=n("li"),Jn=a("The arguments "),Rt=n("code"),Zn=a("special_tokens"),es=a(" and the function "),Bt=n("code"),ts=a("set_special_tokens"),os=a(`, can be used to add additional symbols (like
\u201D`),Vt=n("strong"),ns=a("classify"),ss=a("\u201D) to a vocabulary."),rs=l(),De=n("li"),as=a("The argument "),Ut=n("code"),is=a("langs"),ds=a(" defines a pair of languages."),ls=l(),Ne=n("p"),cs=a("This tokenizer inherits from "),mt=n("a"),hs=a("PreTrainedTokenizer"),ms=a(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),fs=l(),O=n("div"),T(Le.$$.fragment),ps=l(),Ht=n("p"),us=a(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A FAIRSEQ Transformer sequence has the following format:`),_s=l(),je=n("ul"),ft=n("li"),gs=a("single sequence: "),Qt=n("code"),vs=a("<s> X </s>"),ks=l(),pt=n("li"),Ts=a("pair of sequences: "),Xt=n("code"),bs=a("<s> A </s> B </s>"),ys=l(),he=n("div"),T(Oe.$$.fragment),ws=l(),Ge=n("p"),Ms=a(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Kt=n("code"),Fs=a("prepare_for_model"),$s=a(" method."),Ss=l(),C=n("div"),T(We.$$.fragment),zs=l(),Yt=n("p"),qs=a("Create a mask from the two sequences passed to be used in a sequence-pair classification task. A FAIRSEQ"),xs=l(),T(me.$$.fragment),Es=l(),Z=n("p"),Cs=a("If "),Jt=n("code"),Ps=a("token_ids_1"),Is=a(" is "),Zt=n("code"),As=a("None"),Ds=a(", this method only returns the first portion of the mask (0s)."),Ns=l(),eo=n("p"),Ls=a(`Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An
FAIRSEQ_TRANSFORMER sequence pair mask has the following format:`),js=l(),ut=n("div"),T(Re.$$.fragment),$o=l(),ee=n("h2"),fe=n("a"),to=n("span"),T(Be.$$.fragment),Os=l(),oo=n("span"),Gs=a("FSMTModel"),So=l(),q=n("div"),T(Ve.$$.fragment),Ws=l(),no=n("p"),Rs=a("The bare FSMT Model outputting raw hidden-states without any specific head on top."),Bs=l(),Ue=n("p"),Vs=a("This model inherits from "),_t=n("a"),Us=a("PreTrainedModel"),Hs=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Qs=l(),He=n("p"),Xs=a("This model is also a PyTorch "),Qe=n("a"),Ks=a("torch.nn.Module"),Ys=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Js=l(),P=n("div"),T(Xe.$$.fragment),Zs=l(),te=n("p"),er=a("The "),gt=n("a"),tr=a("FSMTModel"),or=a(" forward method, overrides the "),so=n("code"),nr=a("__call__"),sr=a(" special method."),rr=l(),T(pe.$$.fragment),ar=l(),T(ue.$$.fragment),zo=l(),oe=n("h2"),_e=n("a"),ro=n("span"),T(Ke.$$.fragment),ir=l(),ao=n("span"),dr=a("FSMTForConditionalGeneration"),qo=l(),x=n("div"),T(Ye.$$.fragment),lr=l(),io=n("p"),cr=a("The FSMT Model with a language modeling head. Can be used for summarization."),hr=l(),Je=n("p"),mr=a("This model inherits from "),vt=n("a"),fr=a("PreTrainedModel"),pr=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ur=l(),Ze=n("p"),_r=a("This model is also a PyTorch "),et=n("a"),gr=a("torch.nn.Module"),vr=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),kr=l(),I=n("div"),T(tt.$$.fragment),Tr=l(),ne=n("p"),br=a("The "),kt=n("a"),yr=a("FSMTForConditionalGeneration"),wr=a(" forward method, overrides the "),lo=n("code"),Mr=a("__call__"),Fr=a(" special method."),$r=l(),T(ge.$$.fragment),Sr=l(),T(ve.$$.fragment),this.h()},l(t){const f=Oa('[data-svelte="svelte-1phssyn"]',document.head);m=s(f,"META",{name:!0,content:!0}),f.forEach(o),k=c(t),g=s(t,"H1",{class:!0});var ot=r(g);p=s(ot,"A",{id:!0,class:!0,href:!0});var co=r(p);v=s(co,"SPAN",{});var ho=r(v);b(d.$$.fragment,ho),ho.forEach(o),co.forEach(o),u=c(ot),A=s(ot,"SPAN",{});var mo=r(A);Jo=i(mo,"FSMT"),mo.forEach(o),ot.forEach(o),po=c(t),V=s(t,"P",{});var ke=r(V);zt=s(ke,"STRONG",{});var fo=r(zt);Zo=i(fo,"DISCLAIMER:"),fo.forEach(o),en=i(ke," If you see something strange, file a "),Me=s(ke,"A",{href:!0,rel:!0});var xr=r(Me);tn=i(xr,"Github Issue"),xr.forEach(o),on=i(ke,` and assign
@stas00.`),ke.forEach(o),uo=c(t),U=s(t,"H2",{class:!0});var Eo=r(U);se=s(Eo,"A",{id:!0,class:!0,href:!0});var Er=r(se);qt=s(Er,"SPAN",{});var Cr=r(qt);b(Fe.$$.fragment,Cr),Cr.forEach(o),Er.forEach(o),nn=c(Eo),xt=s(Eo,"SPAN",{});var Pr=r(xt);sn=i(Pr,"Overview"),Pr.forEach(o),Eo.forEach(o),_o=c(t),re=s(t,"P",{});var Co=r(re);rn=i(Co,"FSMT (FairSeq MachineTranslation) models were introduced in "),$e=s(Co,"A",{href:!0,rel:!0});var Ir=r($e);an=i(Ir,"Facebook FAIR\u2019s WMT19 News Translation Task Submission"),Ir.forEach(o),dn=i(Co," by Nathan Ng, Kyra Yee, Alexei Baevski, Myle Ott, Michael Auli, Sergey Edunov."),Co.forEach(o),go=c(t),st=s(t,"P",{});var Ar=r(st);ln=i(Ar,"The abstract of the paper is the following:"),Ar.forEach(o),vo=c(t),rt=s(t,"P",{});var Dr=r(rt);Et=s(Dr,"EM",{});var Nr=r(Et);cn=i(Nr,`This paper describes Facebook FAIR\u2019s submission to the WMT19 shared news translation task. We participate in two
language pairs and four language directions, English <-> German and English <-> Russian. Following our submission from
last year, our baseline systems are large BPE-based transformer models trained with the Fairseq sequence modeling
toolkit which rely on sampled back-translations. This year we experiment with different bitext data filtering schemes,
as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific
data, then decode using noisy channel model reranking. Our submissions are ranked first in all four directions of the
human evaluation campaign. On En->De, our system significantly outperforms other systems as well as human translations.
This system improves upon our WMT\u201918 submission by 4.5 BLEU points.`),Nr.forEach(o),Dr.forEach(o),ko=c(t),j=s(t,"P",{});var Tt=r(j);hn=i(Tt,"This model was contributed by "),Se=s(Tt,"A",{href:!0,rel:!0});var Lr=r(Se);mn=i(Lr,"stas"),Lr.forEach(o),fn=i(Tt,`. The original code can be found
`),ze=s(Tt,"A",{href:!0,rel:!0});var jr=r(ze);pn=i(jr,"here"),jr.forEach(o),un=i(Tt,"."),Tt.forEach(o),To=c(t),H=s(t,"H2",{class:!0});var Po=r(H);ae=s(Po,"A",{id:!0,class:!0,href:!0});var Or=r(ae);Ct=s(Or,"SPAN",{});var Gr=r(Ct);b(qe.$$.fragment,Gr),Gr.forEach(o),Or.forEach(o),_n=c(Po),Pt=s(Po,"SPAN",{});var Wr=r(Pt);gn=i(Wr,"Implementation Notes"),Wr.forEach(o),Po.forEach(o),bo=c(t),at=s(t,"UL",{});var Rr=r(at);Q=s(Rr,"LI",{});var bt=r(Q);vn=i(bt,`FSMT uses source and target vocabulary pairs that aren\u2019t combined into one. It doesn\u2019t share embeddings tokens
either. Its tokenizer is very similar to `),it=s(bt,"A",{href:!0});var Br=r(it);kn=i(Br,"XLMTokenizer"),Br.forEach(o),Tn=i(bt,` and the main model is derived from
`),dt=s(bt,"A",{href:!0});var Vr=r(dt);bn=i(Vr,"BartModel"),Vr.forEach(o),yn=i(bt,"."),bt.forEach(o),Rr.forEach(o),yo=c(t),X=s(t,"H2",{class:!0});var Io=r(X);ie=s(Io,"A",{id:!0,class:!0,href:!0});var Ur=r(ie);It=s(Ur,"SPAN",{});var Hr=r(It);b(xe.$$.fragment,Hr),Hr.forEach(o),Ur.forEach(o),wn=c(Io),At=s(Io,"SPAN",{});var Qr=r(At);Mn=i(Qr,"FSMTConfig"),Qr.forEach(o),Io.forEach(o),wo=c(t),z=s(t,"DIV",{class:!0});var G=r(z);b(Ee.$$.fragment,G),Fn=c(G),Ce=s(G,"P",{});var Ao=r(Ce);$n=i(Ao,"This is the configuration class to store the configuration of a "),lt=s(Ao,"A",{href:!0});var Xr=r(lt);Sn=i(Xr,"FSMTModel"),Xr.forEach(o),zn=i(Ao,`. It is used to instantiate a FSMT
model according to the specified arguments, defining the model architecture.`),Ao.forEach(o),qn=c(G),K=s(G,"P",{});var yt=r(K);xn=i(yt,"Configuration objects inherit from "),ct=s(yt,"A",{href:!0});var Kr=r(ct);En=i(Kr,"PretrainedConfig"),Kr.forEach(o),Cn=i(yt,` and can be used to control the model outputs. Read the
documentation from `),ht=s(yt,"A",{href:!0});var Yr=r(ht);Pn=i(Yr,"PretrainedConfig"),Yr.forEach(o),In=i(yt," for more information."),yt.forEach(o),An=c(G),b(de.$$.fragment,G),Dn=c(G),le=s(G,"DIV",{class:!0});var Do=r(le);b(Pe.$$.fragment,Do),Nn=c(Do),Y=s(Do,"P",{});var wt=r(Y);Ln=i(wt,"Serializes this instance to a Python dictionary. Override the default "),Dt=s(wt,"EM",{});var Jr=r(Dt);jn=i(Jr,"to_dict()"),Jr.forEach(o),On=i(wt," from "),Nt=s(wt,"EM",{});var Zr=r(Nt);Gn=i(Zr,"PretrainedConfig"),Zr.forEach(o),Wn=i(wt,"."),wt.forEach(o),Do.forEach(o),G.forEach(o),Mo=c(t),J=s(t,"H2",{class:!0});var No=r(J);ce=s(No,"A",{id:!0,class:!0,href:!0});var ea=r(ce);Lt=s(ea,"SPAN",{});var ta=r(Lt);b(Ie.$$.fragment,ta),ta.forEach(o),ea.forEach(o),Rn=c(No),jt=s(No,"SPAN",{});var oa=r(jt);Bn=i(oa,"FSMTTokenizer"),oa.forEach(o),No.forEach(o),Fo=c(t),$=s(t,"DIV",{class:!0});var S=r($);b(Ae.$$.fragment,S),Vn=c(S),Ot=s(S,"P",{});var na=r(Ot);Un=i(na,"Construct an FAIRSEQ Transformer tokenizer. Based on Byte-Pair Encoding. The tokenization process is the following:"),na.forEach(o),Hn=c(S),D=s(S,"UL",{});var Te=r(D);Gt=s(Te,"LI",{});var sa=r(Gt);Qn=i(sa,"Moses preprocessing and tokenization."),sa.forEach(o),Xn=c(Te),Wt=s(Te,"LI",{});var ra=r(Wt);Kn=i(ra,"Normalizing all inputs text."),ra.forEach(o),Yn=c(Te),N=s(Te,"LI",{});var be=r(N);Jn=i(be,"The arguments "),Rt=s(be,"CODE",{});var aa=r(Rt);Zn=i(aa,"special_tokens"),aa.forEach(o),es=i(be," and the function "),Bt=s(be,"CODE",{});var ia=r(Bt);ts=i(ia,"set_special_tokens"),ia.forEach(o),os=i(be,`, can be used to add additional symbols (like
\u201D`),Vt=s(be,"STRONG",{});var da=r(Vt);ns=i(da,"classify"),da.forEach(o),ss=i(be,"\u201D) to a vocabulary."),be.forEach(o),rs=c(Te),De=s(Te,"LI",{});var Lo=r(De);as=i(Lo,"The argument "),Ut=s(Lo,"CODE",{});var la=r(Ut);is=i(la,"langs"),la.forEach(o),ds=i(Lo," defines a pair of languages."),Lo.forEach(o),Te.forEach(o),ls=c(S),Ne=s(S,"P",{});var jo=r(Ne);cs=i(jo,"This tokenizer inherits from "),mt=s(jo,"A",{href:!0});var ca=r(mt);hs=i(ca,"PreTrainedTokenizer"),ca.forEach(o),ms=i(jo,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),jo.forEach(o),fs=c(S),O=s(S,"DIV",{class:!0});var Mt=r(O);b(Le.$$.fragment,Mt),ps=c(Mt),Ht=s(Mt,"P",{});var ha=r(Ht);us=i(ha,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A FAIRSEQ Transformer sequence has the following format:`),ha.forEach(o),_s=c(Mt),je=s(Mt,"UL",{});var Oo=r(je);ft=s(Oo,"LI",{});var zr=r(ft);gs=i(zr,"single sequence: "),Qt=s(zr,"CODE",{});var ma=r(Qt);vs=i(ma,"<s> X </s>"),ma.forEach(o),zr.forEach(o),ks=c(Oo),pt=s(Oo,"LI",{});var qr=r(pt);Ts=i(qr,"pair of sequences: "),Xt=s(qr,"CODE",{});var fa=r(Xt);bs=i(fa,"<s> A </s> B </s>"),fa.forEach(o),qr.forEach(o),Oo.forEach(o),Mt.forEach(o),ys=c(S),he=s(S,"DIV",{class:!0});var Go=r(he);b(Oe.$$.fragment,Go),ws=c(Go),Ge=s(Go,"P",{});var Wo=r(Ge);Ms=i(Wo,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),Kt=s(Wo,"CODE",{});var pa=r(Kt);Fs=i(pa,"prepare_for_model"),pa.forEach(o),$s=i(Wo," method."),Wo.forEach(o),Go.forEach(o),Ss=c(S),C=s(S,"DIV",{class:!0});var W=r(C);b(We.$$.fragment,W),zs=c(W),Yt=s(W,"P",{});var ua=r(Yt);qs=i(ua,"Create a mask from the two sequences passed to be used in a sequence-pair classification task. A FAIRSEQ"),ua.forEach(o),xs=c(W),b(me.$$.fragment,W),Es=c(W),Z=s(W,"P",{});var Ft=r(Z);Cs=i(Ft,"If "),Jt=s(Ft,"CODE",{});var _a=r(Jt);Ps=i(_a,"token_ids_1"),_a.forEach(o),Is=i(Ft," is "),Zt=s(Ft,"CODE",{});var ga=r(Zt);As=i(ga,"None"),ga.forEach(o),Ds=i(Ft,", this method only returns the first portion of the mask (0s)."),Ft.forEach(o),Ns=c(W),eo=s(W,"P",{});var va=r(eo);Ls=i(va,`Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An
FAIRSEQ_TRANSFORMER sequence pair mask has the following format:`),va.forEach(o),W.forEach(o),js=c(S),ut=s(S,"DIV",{class:!0});var ka=r(ut);b(Re.$$.fragment,ka),ka.forEach(o),S.forEach(o),$o=c(t),ee=s(t,"H2",{class:!0});var Ro=r(ee);fe=s(Ro,"A",{id:!0,class:!0,href:!0});var Ta=r(fe);to=s(Ta,"SPAN",{});var ba=r(to);b(Be.$$.fragment,ba),ba.forEach(o),Ta.forEach(o),Os=c(Ro),oo=s(Ro,"SPAN",{});var ya=r(oo);Gs=i(ya,"FSMTModel"),ya.forEach(o),Ro.forEach(o),So=c(t),q=s(t,"DIV",{class:!0});var R=r(q);b(Ve.$$.fragment,R),Ws=c(R),no=s(R,"P",{});var wa=r(no);Rs=i(wa,"The bare FSMT Model outputting raw hidden-states without any specific head on top."),wa.forEach(o),Bs=c(R),Ue=s(R,"P",{});var Bo=r(Ue);Vs=i(Bo,"This model inherits from "),_t=s(Bo,"A",{href:!0});var Ma=r(_t);Us=i(Ma,"PreTrainedModel"),Ma.forEach(o),Hs=i(Bo,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bo.forEach(o),Qs=c(R),He=s(R,"P",{});var Vo=r(He);Xs=i(Vo,"This model is also a PyTorch "),Qe=s(Vo,"A",{href:!0,rel:!0});var Fa=r(Qe);Ks=i(Fa,"torch.nn.Module"),Fa.forEach(o),Ys=i(Vo,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Vo.forEach(o),Js=c(R),P=s(R,"DIV",{class:!0});var ye=r(P);b(Xe.$$.fragment,ye),Zs=c(ye),te=s(ye,"P",{});var $t=r(te);er=i($t,"The "),gt=s($t,"A",{href:!0});var $a=r(gt);tr=i($a,"FSMTModel"),$a.forEach(o),or=i($t," forward method, overrides the "),so=s($t,"CODE",{});var Sa=r(so);nr=i(Sa,"__call__"),Sa.forEach(o),sr=i($t," special method."),$t.forEach(o),rr=c(ye),b(pe.$$.fragment,ye),ar=c(ye),b(ue.$$.fragment,ye),ye.forEach(o),R.forEach(o),zo=c(t),oe=s(t,"H2",{class:!0});var Uo=r(oe);_e=s(Uo,"A",{id:!0,class:!0,href:!0});var za=r(_e);ro=s(za,"SPAN",{});var qa=r(ro);b(Ke.$$.fragment,qa),qa.forEach(o),za.forEach(o),ir=c(Uo),ao=s(Uo,"SPAN",{});var xa=r(ao);dr=i(xa,"FSMTForConditionalGeneration"),xa.forEach(o),Uo.forEach(o),qo=c(t),x=s(t,"DIV",{class:!0});var B=r(x);b(Ye.$$.fragment,B),lr=c(B),io=s(B,"P",{});var Ea=r(io);cr=i(Ea,"The FSMT Model with a language modeling head. Can be used for summarization."),Ea.forEach(o),hr=c(B),Je=s(B,"P",{});var Ho=r(Je);mr=i(Ho,"This model inherits from "),vt=s(Ho,"A",{href:!0});var Ca=r(vt);fr=i(Ca,"PreTrainedModel"),Ca.forEach(o),pr=i(Ho,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ho.forEach(o),ur=c(B),Ze=s(B,"P",{});var Qo=r(Ze);_r=i(Qo,"This model is also a PyTorch "),et=s(Qo,"A",{href:!0,rel:!0});var Pa=r(et);gr=i(Pa,"torch.nn.Module"),Pa.forEach(o),vr=i(Qo,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Qo.forEach(o),kr=c(B),I=s(B,"DIV",{class:!0});var we=r(I);b(tt.$$.fragment,we),Tr=c(we),ne=s(we,"P",{});var St=r(ne);br=i(St,"The "),kt=s(St,"A",{href:!0});var Ia=r(kt);yr=i(Ia,"FSMTForConditionalGeneration"),Ia.forEach(o),wr=i(St," forward method, overrides the "),lo=s(St,"CODE",{});var Aa=r(lo);Mr=i(Aa,"__call__"),Aa.forEach(o),Fr=i(St," special method."),St.forEach(o),$r=c(we),b(ge.$$.fragment,we),Sr=c(we),b(ve.$$.fragment,we),we.forEach(o),B.forEach(o),this.h()},h(){h(m,"name","hf:doc:metadata"),h(m,"content",JSON.stringify(Xa)),h(p,"id","fsmt"),h(p,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(p,"href","#fsmt"),h(g,"class","relative group"),h(Me,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),h(Me,"rel","nofollow"),h(se,"id","overview"),h(se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(se,"href","#overview"),h(U,"class","relative group"),h($e,"href","https://arxiv.org/abs/1907.06616"),h($e,"rel","nofollow"),h(Se,"href","https://huggingface.co/stas"),h(Se,"rel","nofollow"),h(ze,"href","https://github.com/pytorch/fairseq/tree/master/examples/wmt19"),h(ze,"rel","nofollow"),h(ae,"id","implementation-notes"),h(ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ae,"href","#implementation-notes"),h(H,"class","relative group"),h(it,"href","/docs/transformers/v4.18.0/en/model_doc/xlm#transformers.XLMTokenizer"),h(dt,"href","/docs/transformers/v4.18.0/en/model_doc/bart#transformers.BartModel"),h(ie,"id","transformers.FSMTConfig"),h(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ie,"href","#transformers.FSMTConfig"),h(X,"class","relative group"),h(lt,"href","/docs/transformers/v4.18.0/en/model_doc/fsmt#transformers.FSMTModel"),h(ct,"href","/docs/transformers/v4.18.0/en/main_classes/configuration#transformers.PretrainedConfig"),h(ht,"href","/docs/transformers/v4.18.0/en/main_classes/configuration#transformers.PretrainedConfig"),h(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ce,"id","transformers.FSMTTokenizer"),h(ce,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(ce,"href","#transformers.FSMTTokenizer"),h(J,"class","relative group"),h(mt,"href","/docs/transformers/v4.18.0/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),h(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(fe,"id","transformers.FSMTModel"),h(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(fe,"href","#transformers.FSMTModel"),h(ee,"class","relative group"),h(_t,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel"),h(Qe,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(Qe,"rel","nofollow"),h(gt,"href","/docs/transformers/v4.18.0/en/model_doc/fsmt#transformers.FSMTModel"),h(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(_e,"id","transformers.FSMTForConditionalGeneration"),h(_e,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),h(_e,"href","#transformers.FSMTForConditionalGeneration"),h(oe,"class","relative group"),h(vt,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel"),h(et,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),h(et,"rel","nofollow"),h(kt,"href","/docs/transformers/v4.18.0/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),h(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),h(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,f){e(document.head,m),_(t,k,f),_(t,g,f),e(g,p),e(p,v),y(d,v,null),e(g,u),e(g,A),e(A,Jo),_(t,po,f),_(t,V,f),e(V,zt),e(zt,Zo),e(V,en),e(V,Me),e(Me,tn),e(V,on),_(t,uo,f),_(t,U,f),e(U,se),e(se,qt),y(Fe,qt,null),e(U,nn),e(U,xt),e(xt,sn),_(t,_o,f),_(t,re,f),e(re,rn),e(re,$e),e($e,an),e(re,dn),_(t,go,f),_(t,st,f),e(st,ln),_(t,vo,f),_(t,rt,f),e(rt,Et),e(Et,cn),_(t,ko,f),_(t,j,f),e(j,hn),e(j,Se),e(Se,mn),e(j,fn),e(j,ze),e(ze,pn),e(j,un),_(t,To,f),_(t,H,f),e(H,ae),e(ae,Ct),y(qe,Ct,null),e(H,_n),e(H,Pt),e(Pt,gn),_(t,bo,f),_(t,at,f),e(at,Q),e(Q,vn),e(Q,it),e(it,kn),e(Q,Tn),e(Q,dt),e(dt,bn),e(Q,yn),_(t,yo,f),_(t,X,f),e(X,ie),e(ie,It),y(xe,It,null),e(X,wn),e(X,At),e(At,Mn),_(t,wo,f),_(t,z,f),y(Ee,z,null),e(z,Fn),e(z,Ce),e(Ce,$n),e(Ce,lt),e(lt,Sn),e(Ce,zn),e(z,qn),e(z,K),e(K,xn),e(K,ct),e(ct,En),e(K,Cn),e(K,ht),e(ht,Pn),e(K,In),e(z,An),y(de,z,null),e(z,Dn),e(z,le),y(Pe,le,null),e(le,Nn),e(le,Y),e(Y,Ln),e(Y,Dt),e(Dt,jn),e(Y,On),e(Y,Nt),e(Nt,Gn),e(Y,Wn),_(t,Mo,f),_(t,J,f),e(J,ce),e(ce,Lt),y(Ie,Lt,null),e(J,Rn),e(J,jt),e(jt,Bn),_(t,Fo,f),_(t,$,f),y(Ae,$,null),e($,Vn),e($,Ot),e(Ot,Un),e($,Hn),e($,D),e(D,Gt),e(Gt,Qn),e(D,Xn),e(D,Wt),e(Wt,Kn),e(D,Yn),e(D,N),e(N,Jn),e(N,Rt),e(Rt,Zn),e(N,es),e(N,Bt),e(Bt,ts),e(N,os),e(N,Vt),e(Vt,ns),e(N,ss),e(D,rs),e(D,De),e(De,as),e(De,Ut),e(Ut,is),e(De,ds),e($,ls),e($,Ne),e(Ne,cs),e(Ne,mt),e(mt,hs),e(Ne,ms),e($,fs),e($,O),y(Le,O,null),e(O,ps),e(O,Ht),e(Ht,us),e(O,_s),e(O,je),e(je,ft),e(ft,gs),e(ft,Qt),e(Qt,vs),e(je,ks),e(je,pt),e(pt,Ts),e(pt,Xt),e(Xt,bs),e($,ys),e($,he),y(Oe,he,null),e(he,ws),e(he,Ge),e(Ge,Ms),e(Ge,Kt),e(Kt,Fs),e(Ge,$s),e($,Ss),e($,C),y(We,C,null),e(C,zs),e(C,Yt),e(Yt,qs),e(C,xs),y(me,C,null),e(C,Es),e(C,Z),e(Z,Cs),e(Z,Jt),e(Jt,Ps),e(Z,Is),e(Z,Zt),e(Zt,As),e(Z,Ds),e(C,Ns),e(C,eo),e(eo,Ls),e($,js),e($,ut),y(Re,ut,null),_(t,$o,f),_(t,ee,f),e(ee,fe),e(fe,to),y(Be,to,null),e(ee,Os),e(ee,oo),e(oo,Gs),_(t,So,f),_(t,q,f),y(Ve,q,null),e(q,Ws),e(q,no),e(no,Rs),e(q,Bs),e(q,Ue),e(Ue,Vs),e(Ue,_t),e(_t,Us),e(Ue,Hs),e(q,Qs),e(q,He),e(He,Xs),e(He,Qe),e(Qe,Ks),e(He,Ys),e(q,Js),e(q,P),y(Xe,P,null),e(P,Zs),e(P,te),e(te,er),e(te,gt),e(gt,tr),e(te,or),e(te,so),e(so,nr),e(te,sr),e(P,rr),y(pe,P,null),e(P,ar),y(ue,P,null),_(t,zo,f),_(t,oe,f),e(oe,_e),e(_e,ro),y(Ke,ro,null),e(oe,ir),e(oe,ao),e(ao,dr),_(t,qo,f),_(t,x,f),y(Ye,x,null),e(x,lr),e(x,io),e(io,cr),e(x,hr),e(x,Je),e(Je,mr),e(Je,vt),e(vt,fr),e(Je,pr),e(x,ur),e(x,Ze),e(Ze,_r),e(Ze,et),e(et,gr),e(Ze,vr),e(x,kr),e(x,I),y(tt,I,null),e(I,Tr),e(I,ne),e(ne,br),e(ne,kt),e(kt,yr),e(ne,wr),e(ne,lo),e(lo,Mr),e(ne,Fr),e(I,$r),y(ge,I,null),e(I,Sr),y(ve,I,null),xo=!0},p(t,[f]){const ot={};f&2&&(ot.$$scope={dirty:f,ctx:t}),de.$set(ot);const co={};f&2&&(co.$$scope={dirty:f,ctx:t}),me.$set(co);const ho={};f&2&&(ho.$$scope={dirty:f,ctx:t}),pe.$set(ho);const mo={};f&2&&(mo.$$scope={dirty:f,ctx:t}),ue.$set(mo);const ke={};f&2&&(ke.$$scope={dirty:f,ctx:t}),ge.$set(ke);const fo={};f&2&&(fo.$$scope={dirty:f,ctx:t}),ve.$set(fo)},i(t){xo||(w(d.$$.fragment,t),w(Fe.$$.fragment,t),w(qe.$$.fragment,t),w(xe.$$.fragment,t),w(Ee.$$.fragment,t),w(de.$$.fragment,t),w(Pe.$$.fragment,t),w(Ie.$$.fragment,t),w(Ae.$$.fragment,t),w(Le.$$.fragment,t),w(Oe.$$.fragment,t),w(We.$$.fragment,t),w(me.$$.fragment,t),w(Re.$$.fragment,t),w(Be.$$.fragment,t),w(Ve.$$.fragment,t),w(Xe.$$.fragment,t),w(pe.$$.fragment,t),w(ue.$$.fragment,t),w(Ke.$$.fragment,t),w(Ye.$$.fragment,t),w(tt.$$.fragment,t),w(ge.$$.fragment,t),w(ve.$$.fragment,t),xo=!0)},o(t){M(d.$$.fragment,t),M(Fe.$$.fragment,t),M(qe.$$.fragment,t),M(xe.$$.fragment,t),M(Ee.$$.fragment,t),M(de.$$.fragment,t),M(Pe.$$.fragment,t),M(Ie.$$.fragment,t),M(Ae.$$.fragment,t),M(Le.$$.fragment,t),M(Oe.$$.fragment,t),M(We.$$.fragment,t),M(me.$$.fragment,t),M(Re.$$.fragment,t),M(Be.$$.fragment,t),M(Ve.$$.fragment,t),M(Xe.$$.fragment,t),M(pe.$$.fragment,t),M(ue.$$.fragment,t),M(Ke.$$.fragment,t),M(Ye.$$.fragment,t),M(tt.$$.fragment,t),M(ge.$$.fragment,t),M(ve.$$.fragment,t),xo=!1},d(t){o(m),t&&o(k),t&&o(g),F(d),t&&o(po),t&&o(V),t&&o(uo),t&&o(U),F(Fe),t&&o(_o),t&&o(re),t&&o(go),t&&o(st),t&&o(vo),t&&o(rt),t&&o(ko),t&&o(j),t&&o(To),t&&o(H),F(qe),t&&o(bo),t&&o(at),t&&o(yo),t&&o(X),F(xe),t&&o(wo),t&&o(z),F(Ee),F(de),F(Pe),t&&o(Mo),t&&o(J),F(Ie),t&&o(Fo),t&&o($),F(Ae),F(Le),F(Oe),F(We),F(me),F(Re),t&&o($o),t&&o(ee),F(Be),t&&o(So),t&&o(q),F(Ve),F(Xe),F(pe),F(ue),t&&o(zo),t&&o(oe),F(Ke),t&&o(qo),t&&o(x),F(Ye),F(tt),F(ge),F(ve)}}}const Xa={local:"fsmt",sections:[{local:"overview",title:"Overview"},{local:"implementation-notes",title:"Implementation Notes"},{local:"transformers.FSMTConfig",title:"FSMTConfig"},{local:"transformers.FSMTTokenizer",title:"FSMTTokenizer"},{local:"transformers.FSMTModel",title:"FSMTModel"},{local:"transformers.FSMTForConditionalGeneration",title:"FSMTForConditionalGeneration"}],title:"FSMT"};function Ka(E){return Ga(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ni extends Na{constructor(m){super();La(this,m,Ka,Qa,ja,{})}}export{ni as default,Xa as metadata};
