import{S as iu,i as du,s as lu,e as n,k as l,w as k,t as a,M as cu,c as r,d as t,m as c,a as s,x as $,h as i,b as p,F as e,g as f,y as P,q as w,o as E,B as R,v as pu,L as In}from"../../chunks/vendor-6b77c823.js";import{T as yt}from"../../chunks/Tip-39098574.js";import{D as U}from"../../chunks/Docstring-1088f2fb.js";import{C as Sn}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as ge}from"../../chunks/IconCopyLink-7a11ce68.js";import{E as Ln}from"../../chunks/ExampleCodeBlock-5212b321.js";function hu(F){let h,T,v,u,b;return u=new Sn({props:{code:"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>",highlighted:'[CLS] <span class="hljs-tag">&lt;<span class="hljs-name">question</span> <span class="hljs-attr">token</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">titles</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">texts</span> <span class="hljs-attr">ids</span>&gt;</span>'}}),{c(){h=n("p"),T=a("with the format:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"with the format:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:In,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function fu(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function uu(F){let h,T,v,u,b;return u=new Sn({props:{code:`from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = DPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:In,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function mu(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function gu(F){let h,T,v,u,b;return u=new Sn({props:{code:`from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = DPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="pt")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:In,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function _u(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function vu(F){let h,T,v,u,b;return u=new Sn({props:{code:`from transformers import DPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = DPRReader.from_pretrained("facebook/dpr-reader-single-nq-base")
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="pt",
)
outputs = model(**encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:In,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function bu(F){let h,T,v,u,b,d,g,x,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,A,ne,V,fe,re,B,Te,ue,z,ke,j,$e,Pe,M,we,Ee,Q,K,N,se;return{c(){h=n("p"),T=a("TF 2.0 models accepts two formats as inputs:"),v=l(),u=n("ul"),b=n("li"),d=a("having all inputs as keyword arguments (like PyTorch models), or"),g=l(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=n("p"),J=a("This second option is useful when using "),I=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=n("code"),be=a("model(inputs)"),he=a("."),H=l(),L=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=n("ul"),A=n("li"),ne=a("a single Tensor with "),V=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),B=n("code"),Te=a("model(inputs_ids)"),ue=l(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=n("code"),$e=a("model([input_ids, attention_mask])"),Pe=a(" or "),M=n("code"),we=a("model([input_ids, attention_mask, token_type_ids])"),Ee=l(),Q=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(_){h=r(_,"P",{});var y=s(h);T=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),v=c(_),u=r(_,"UL",{});var G=s(u);b=r(G,"LI",{});var Oe=s(b);d=i(Oe,"having all inputs as keyword arguments (like PyTorch models), or"),Oe.forEach(t),g=c(G),x=r(G,"LI",{});var ze=s(x);_e=i(ze,"having all inputs as a list, tuple or dict in the first positional arguments."),ze.forEach(t),G.forEach(t),Z=c(_),D=r(_,"P",{});var C=s(D);J=i(C,"This second option is useful when using "),I=r(C,"CODE",{});var me=s(I);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(C,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r(C,"CODE",{});var Ne=s(S);be=i(Ne,"model(inputs)"),Ne.forEach(t),he=i(C,"."),C.forEach(t),H=c(_),L=r(_,"P",{});var ae=s(L);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=c(_),q=r(_,"UL",{});var O=s(q);A=r(O,"LI",{});var Y=s(A);ne=i(Y,"a single Tensor with "),V=r(Y,"CODE",{});var je=s(V);fe=i(je,"input_ids"),je.forEach(t),re=i(Y," only and nothing else: "),B=r(Y,"CODE",{});var xe=s(B);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=c(O),z=r(O,"LI",{});var X=s(z);ke=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=r(X,"CODE",{});var Me=s(j);$e=i(Me,"model([input_ids, attention_mask])"),Me.forEach(t),Pe=i(X," or "),M=r(X,"CODE",{});var Qe=s(M);we=i(Qe,"model([input_ids, attention_mask, token_type_ids])"),Qe.forEach(t),X.forEach(t),Ee=c(O),Q=r(O,"LI",{});var W=s(Q);K=i(W,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(W,"CODE",{});var Le=s(N);se=i(Le,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Le.forEach(t),W.forEach(t),O.forEach(t)},m(_,y){f(_,h,y),e(h,T),f(_,v,y),f(_,u,y),e(u,b),e(b,d),e(u,g),e(u,x),e(x,_e),f(_,Z,y),f(_,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),f(_,H,y),f(_,L,y),e(L,te),f(_,oe,y),f(_,q,y),e(q,A),e(A,ne),e(A,V),e(V,fe),e(A,re),e(A,B),e(B,Te),e(q,ue),e(q,z),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(q,Ee),e(q,Q),e(Q,K),e(Q,N),e(N,se)},d(_){_&&t(h),_&&t(v),_&&t(u),_&&t(Z),_&&t(D),_&&t(H),_&&t(L),_&&t(oe),_&&t(q)}}}function Tu(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function ku(F){let h,T,v,u,b;return u=new Sn({props:{code:`from transformers import TFDPRContextEncoder, DPRContextEncoderTokenizer

tokenizer = DPRContextEncoderTokenizer.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base")
model = TFDPRContextEncoder.from_pretrained("facebook/dpr-ctx_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:In,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function $u(F){let h,T,v,u,b,d,g,x,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,A,ne,V,fe,re,B,Te,ue,z,ke,j,$e,Pe,M,we,Ee,Q,K,N,se;return{c(){h=n("p"),T=a("TF 2.0 models accepts two formats as inputs:"),v=l(),u=n("ul"),b=n("li"),d=a("having all inputs as keyword arguments (like PyTorch models), or"),g=l(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=n("p"),J=a("This second option is useful when using "),I=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=n("code"),be=a("model(inputs)"),he=a("."),H=l(),L=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=n("ul"),A=n("li"),ne=a("a single Tensor with "),V=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),B=n("code"),Te=a("model(inputs_ids)"),ue=l(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=n("code"),$e=a("model([input_ids, attention_mask])"),Pe=a(" or "),M=n("code"),we=a("model([input_ids, attention_mask, token_type_ids])"),Ee=l(),Q=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(_){h=r(_,"P",{});var y=s(h);T=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),v=c(_),u=r(_,"UL",{});var G=s(u);b=r(G,"LI",{});var Oe=s(b);d=i(Oe,"having all inputs as keyword arguments (like PyTorch models), or"),Oe.forEach(t),g=c(G),x=r(G,"LI",{});var ze=s(x);_e=i(ze,"having all inputs as a list, tuple or dict in the first positional arguments."),ze.forEach(t),G.forEach(t),Z=c(_),D=r(_,"P",{});var C=s(D);J=i(C,"This second option is useful when using "),I=r(C,"CODE",{});var me=s(I);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(C,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r(C,"CODE",{});var Ne=s(S);be=i(Ne,"model(inputs)"),Ne.forEach(t),he=i(C,"."),C.forEach(t),H=c(_),L=r(_,"P",{});var ae=s(L);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=c(_),q=r(_,"UL",{});var O=s(q);A=r(O,"LI",{});var Y=s(A);ne=i(Y,"a single Tensor with "),V=r(Y,"CODE",{});var je=s(V);fe=i(je,"input_ids"),je.forEach(t),re=i(Y," only and nothing else: "),B=r(Y,"CODE",{});var xe=s(B);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=c(O),z=r(O,"LI",{});var X=s(z);ke=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=r(X,"CODE",{});var Me=s(j);$e=i(Me,"model([input_ids, attention_mask])"),Me.forEach(t),Pe=i(X," or "),M=r(X,"CODE",{});var Qe=s(M);we=i(Qe,"model([input_ids, attention_mask, token_type_ids])"),Qe.forEach(t),X.forEach(t),Ee=c(O),Q=r(O,"LI",{});var W=s(Q);K=i(W,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(W,"CODE",{});var Le=s(N);se=i(Le,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Le.forEach(t),W.forEach(t),O.forEach(t)},m(_,y){f(_,h,y),e(h,T),f(_,v,y),f(_,u,y),e(u,b),e(b,d),e(u,g),e(u,x),e(x,_e),f(_,Z,y),f(_,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),f(_,H,y),f(_,L,y),e(L,te),f(_,oe,y),f(_,q,y),e(q,A),e(A,ne),e(A,V),e(V,fe),e(A,re),e(A,B),e(B,Te),e(q,ue),e(q,z),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(q,Ee),e(q,Q),e(Q,K),e(Q,N),e(N,se)},d(_){_&&t(h),_&&t(v),_&&t(u),_&&t(Z),_&&t(D),_&&t(H),_&&t(L),_&&t(oe),_&&t(q)}}}function Pu(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function wu(F){let h,T,v,u,b;return u=new Sn({props:{code:`from transformers import TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

tokenizer = DPRQuestionEncoderTokenizer.from_pretrained("facebook/dpr-question_encoder-single-nq-base")
model = TFDPRQuestionEncoder.from_pretrained("facebook/dpr-question_encoder-single-nq-base", from_pt=True)
input_ids = tokenizer("Hello, is my dog cute ?", return_tensors="tf")["input_ids"]
embeddings = model(input_ids).pooler_output`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:In,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function Eu(F){let h,T,v,u,b,d,g,x,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,A,ne,V,fe,re,B,Te,ue,z,ke,j,$e,Pe,M,we,Ee,Q,K,N,se;return{c(){h=n("p"),T=a("TF 2.0 models accepts two formats as inputs:"),v=l(),u=n("ul"),b=n("li"),d=a("having all inputs as keyword arguments (like PyTorch models), or"),g=l(),x=n("li"),_e=a("having all inputs as a list, tuple or dict in the first positional arguments."),Z=l(),D=n("p"),J=a("This second option is useful when using "),I=n("code"),ee=a("tf.keras.Model.fit"),ve=a(` method which currently requires having all the
tensors in the first argument of the model call function: `),S=n("code"),be=a("model(inputs)"),he=a("."),H=l(),L=n("p"),te=a(`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),oe=l(),q=n("ul"),A=n("li"),ne=a("a single Tensor with "),V=n("code"),fe=a("input_ids"),re=a(" only and nothing else: "),B=n("code"),Te=a("model(inputs_ids)"),ue=l(),z=n("li"),ke=a(`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=n("code"),$e=a("model([input_ids, attention_mask])"),Pe=a(" or "),M=n("code"),we=a("model([input_ids, attention_mask, token_type_ids])"),Ee=l(),Q=n("li"),K=a(`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=n("code"),se=a('model({"input_ids": input_ids, "token_type_ids": token_type_ids})')},l(_){h=r(_,"P",{});var y=s(h);T=i(y,"TF 2.0 models accepts two formats as inputs:"),y.forEach(t),v=c(_),u=r(_,"UL",{});var G=s(u);b=r(G,"LI",{});var Oe=s(b);d=i(Oe,"having all inputs as keyword arguments (like PyTorch models), or"),Oe.forEach(t),g=c(G),x=r(G,"LI",{});var ze=s(x);_e=i(ze,"having all inputs as a list, tuple or dict in the first positional arguments."),ze.forEach(t),G.forEach(t),Z=c(_),D=r(_,"P",{});var C=s(D);J=i(C,"This second option is useful when using "),I=r(C,"CODE",{});var me=s(I);ee=i(me,"tf.keras.Model.fit"),me.forEach(t),ve=i(C,` method which currently requires having all the
tensors in the first argument of the model call function: `),S=r(C,"CODE",{});var Ne=s(S);be=i(Ne,"model(inputs)"),Ne.forEach(t),he=i(C,"."),C.forEach(t),H=c(_),L=r(_,"P",{});var ae=s(L);te=i(ae,`If you choose this second option, there are three possibilities you can use to gather all the input Tensors in the
first positional argument :`),ae.forEach(t),oe=c(_),q=r(_,"UL",{});var O=s(q);A=r(O,"LI",{});var Y=s(A);ne=i(Y,"a single Tensor with "),V=r(Y,"CODE",{});var je=s(V);fe=i(je,"input_ids"),je.forEach(t),re=i(Y," only and nothing else: "),B=r(Y,"CODE",{});var xe=s(B);Te=i(xe,"model(inputs_ids)"),xe.forEach(t),Y.forEach(t),ue=c(O),z=r(O,"LI",{});var X=s(z);ke=i(X,`a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
`),j=r(X,"CODE",{});var Me=s(j);$e=i(Me,"model([input_ids, attention_mask])"),Me.forEach(t),Pe=i(X," or "),M=r(X,"CODE",{});var Qe=s(M);we=i(Qe,"model([input_ids, attention_mask, token_type_ids])"),Qe.forEach(t),X.forEach(t),Ee=c(O),Q=r(O,"LI",{});var W=s(Q);K=i(W,`a dictionary with one or several input Tensors associated to the input names given in the docstring:
`),N=r(W,"CODE",{});var Le=s(N);se=i(Le,'model({"input_ids": input_ids, "token_type_ids": token_type_ids})'),Le.forEach(t),W.forEach(t),O.forEach(t)},m(_,y){f(_,h,y),e(h,T),f(_,v,y),f(_,u,y),e(u,b),e(b,d),e(u,g),e(u,x),e(x,_e),f(_,Z,y),f(_,D,y),e(D,J),e(D,I),e(I,ee),e(D,ve),e(D,S),e(S,be),e(D,he),f(_,H,y),f(_,L,y),e(L,te),f(_,oe,y),f(_,q,y),e(q,A),e(A,ne),e(A,V),e(V,fe),e(A,re),e(A,B),e(B,Te),e(q,ue),e(q,z),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(q,Ee),e(q,Q),e(Q,K),e(Q,N),e(N,se)},d(_){_&&t(h),_&&t(v),_&&t(u),_&&t(Z),_&&t(D),_&&t(H),_&&t(L),_&&t(oe),_&&t(q)}}}function Ru(F){let h,T,v,u,b;return{c(){h=n("p"),T=a("Although the recipe for forward pass needs to be defined within this function, one should call the "),v=n("code"),u=a("Module"),b=a(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Although the recipe for forward pass needs to be defined within this function, one should call the "),v=r(g,"CODE",{});var x=s(v);u=i(x,"Module"),x.forEach(t),b=i(g,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),g.forEach(t)},m(d,g){f(d,h,g),e(h,T),e(h,v),e(v,u),e(h,b)},d(d){d&&t(h)}}}function Du(F){let h,T,v,u,b;return u=new Sn({props:{code:`from transformers import TFDPRReader, DPRReaderTokenizer

tokenizer = DPRReaderTokenizer.from_pretrained("facebook/dpr-reader-single-nq-base")
model = TFDPRReader.from_pretrained("facebook/dpr-reader-single-nq-base", from_pt=True)
encoded_inputs = tokenizer(
    questions=["What is love ?"],
    titles=["Haddaway"],
    texts=["'What Is Love' is a song recorded by the artist Haddaway"],
    return_tensors="tf",
)
outputs = model(encoded_inputs)
start_logits = outputs.start_logits
end_logits = outputs.end_logits
relevance_logits = outputs.relevance_logits`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`}}),{c(){h=n("p"),T=a("Examples:"),v=l(),k(u.$$.fragment)},l(d){h=r(d,"P",{});var g=s(h);T=i(g,"Examples:"),g.forEach(t),v=c(d),$(u.$$.fragment,d)},m(d,g){f(d,h,g),e(h,T),f(d,v,g),P(u,d,g),b=!0},p:In,i(d){b||(w(u.$$.fragment,d),b=!0)},o(d){E(u.$$.fragment,d),b=!1},d(d){d&&t(h),d&&t(v),R(u,d)}}}function yu(F){let h,T,v,u,b,d,g,x,_e,Z,D,J,I,ee,ve,S,be,he,H,L,te,oe,q,A,ne,V,fe,re,B,Te,ue,z,ke,j,$e,Pe,M,we,Ee,Q,K,N,se,_,y,G,Oe,ze,C,me,Ne,ae,O,Y,je,xe,X,Me,Qe,W,Le,Hn,pi,hi,Bn,fi,ui,Wn,mi,gi,_i,$o,vi,Un,bi,Ti,Ks,st,xt,Ar,Po,ki,Or,$i,Ys,qe,wo,Pi,Nr,wi,Ei,zt,Vn,Ri,Di,Kn,yi,xi,zi,Eo,qi,Yn,Fi,Ci,Xs,at,qt,jr,Ro,Ai,Mr,Oi,Js,Fe,Do,Ni,yo,ji,Qr,Mi,Qi,Li,Ft,Xn,Ii,Si,Jn,Hi,Bi,Wi,xo,Ui,Gn,Vi,Ki,Gs,it,Ct,Lr,zo,Yi,Ir,Xi,Zs,Ce,qo,Ji,Sr,Gi,Zi,At,Zn,ed,td,er,od,nd,rd,Fo,sd,tr,ad,id,ea,dt,Ot,Hr,Co,dd,Br,ld,ta,Ae,Ao,cd,Oo,pd,Wr,hd,fd,ud,Nt,or,md,gd,nr,_d,vd,bd,No,Td,rr,kd,$d,oa,lt,jt,Ur,jo,Pd,Vr,wd,na,ie,Mo,Ed,Kr,Rd,Dd,Ze,sr,yd,xd,ar,zd,qd,ir,Fd,Cd,Ad,Qo,Od,dr,Nd,jd,Md,et,Qd,Yr,Ld,Id,Xr,Sd,Hd,Jr,Bd,Wd,Mt,ra,ct,Qt,Gr,Lo,Ud,Zr,Vd,sa,de,Io,Kd,So,Yd,es,Xd,Jd,Gd,tt,lr,Zd,el,cr,tl,ol,pr,nl,rl,sl,Ho,al,hr,il,dl,ll,Ge,cl,ts,pl,hl,os,fl,ul,ns,ml,gl,_l,rs,vl,aa,pt,Lt,ss,Bo,bl,as,Tl,ia,ht,Wo,kl,Uo,$l,fr,Pl,wl,da,ft,Vo,El,Ko,Rl,ur,Dl,yl,la,ut,Yo,xl,Xo,zl,mr,ql,Fl,ca,mt,It,is,Jo,Cl,ds,Al,pa,Re,Go,Ol,ls,Nl,jl,Zo,Ml,gr,Ql,Ll,Il,en,Sl,tn,Hl,Bl,Wl,Ie,on,Ul,gt,Vl,_r,Kl,Yl,cs,Xl,Jl,Gl,St,Zl,Ht,ha,_t,Bt,ps,nn,ec,hs,tc,fa,De,rn,oc,fs,nc,rc,sn,sc,vr,ac,ic,dc,an,lc,dn,cc,pc,hc,Se,ln,fc,vt,uc,br,mc,gc,us,_c,vc,bc,Wt,Tc,Ut,ua,bt,Vt,ms,cn,kc,gs,$c,ma,ye,pn,Pc,_s,wc,Ec,hn,Rc,Tr,Dc,yc,xc,fn,zc,un,qc,Fc,Cc,He,mn,Ac,Tt,Oc,kr,Nc,jc,vs,Mc,Qc,Lc,Kt,Ic,Yt,ga,kt,Xt,bs,gn,Sc,Ts,Hc,_a,le,_n,Bc,ks,Wc,Uc,vn,Vc,$r,Kc,Yc,Xc,bn,Jc,Tn,Gc,Zc,ep,Jt,tp,Be,kn,op,$t,np,Pr,rp,sp,$s,ap,ip,dp,Gt,lp,Zt,va,Pt,eo,Ps,$n,cp,ws,pp,ba,ce,Pn,hp,Es,fp,up,wn,mp,wr,gp,_p,vp,En,bp,Rn,Tp,kp,$p,to,Pp,We,Dn,wp,wt,Ep,Er,Rp,Dp,Rs,yp,xp,zp,oo,qp,no,Ta,Et,ro,Ds,yn,Fp,ys,Cp,ka,pe,xn,Ap,xs,Op,Np,zn,jp,Rr,Mp,Qp,Lp,qn,Ip,Fn,Sp,Hp,Bp,so,Wp,Ue,Cn,Up,Rt,Vp,Dr,Kp,Yp,zs,Xp,Jp,Gp,ao,Zp,io,$a;return d=new ge({}),ee=new ge({}),_=new ge({}),me=new U({props:{name:"class transformers.DPRConfig",anchor:"transformers.DPRConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"projection_dim",val:": int = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <em>inputs_ids</em>
passed to the forward method of <a href="/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DPRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPRConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPRConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPRConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPRConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPRConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DPRConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.DPRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPRConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPRConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.DPRConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.`,name:"projection_dim"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/configuration_dpr.py#L33"}}),Po=new ge({}),wo=new U({props:{name:"class transformers.DPRContextEncoderTokenizer",anchor:"transformers.DPRContextEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/tokenization_dpr.py#L89"}}),Ro=new ge({}),Do=new U({props:{name:"class transformers.DPRContextEncoderTokenizerFast",anchor:"transformers.DPRContextEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/tokenization_dpr_fast.py#L90"}}),zo=new ge({}),qo=new U({props:{name:"class transformers.DPRQuestionEncoderTokenizer",anchor:"transformers.DPRQuestionEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/tokenization_dpr.py#L105"}}),Co=new ge({}),Ao=new U({props:{name:"class transformers.DPRQuestionEncoderTokenizerFast",anchor:"transformers.DPRQuestionEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/tokenization_dpr_fast.py#L107"}}),jo=new ge({}),Mo=new U({props:{name:"class transformers.DPRReaderTokenizer",anchor:"transformers.DPRReaderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizer.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizer.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizer.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizer.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.18.0/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizer.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.18.0/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizer.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizer.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.18.0/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizer.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/tokenization_dpr.py#L369",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),Mt=new Ln({props:{anchor:"transformers.DPRReaderTokenizer.example",$$slots:{default:[hu]},$$scope:{ctx:F}}}),Lo=new ge({}),Io=new U({props:{name:"class transformers.DPRReaderTokenizerFast",anchor:"transformers.DPRReaderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizerFast.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizerFast.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizerFast.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizerFast.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.18.0/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizerFast.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/v4.18.0/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizerFast.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizerFast.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/v4.18.0/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizerFast.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/tokenization_dpr_fast.py#L370",returnDescription:`
<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`
<p><code>Dict[str, List[List[int]]]</code></p>
`}}),Bo=new ge({}),Wo=new U({props:{name:"class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_dpr.py#L62"}}),Vo=new U({props:{name:"class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_dpr.py#L90"}}),Yo=new U({props:{name:"class transformers.DPRReaderOutput",anchor:"transformers.DPRReaderOutput",parameters:[{name:"start_logits",val:": FloatTensor"},{name:"end_logits",val:": FloatTensor = None"},{name:"relevance_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],parametersDescription:[{anchor:"transformers.DPRReaderOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the start index of the span for each passage.`,name:"start_logits"},{anchor:"transformers.DPRReaderOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the end index of the span for each passage.`,name:"end_logits"},{anchor:"transformers.DPRReaderOutput.relevance_logits",description:`<strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) &#x2014;
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.`,name:"relevance_logits"},{anchor:"transformers.DPRReaderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.DPRReaderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_dpr.py#L118"}}),Jo=new ge({}),Go=new U({props:{name:"class transformers.DPRContextEncoder",anchor:"transformers.DPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_dpr.py#L445"}}),on=new U({props:{name:"forward",anchor:"transformers.DPRContextEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_dpr.py#L453",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),St=new yt({props:{$$slots:{default:[fu]},$$scope:{ctx:F}}}),Ht=new Ln({props:{anchor:"transformers.DPRContextEncoder.forward.example",$$slots:{default:[uu]},$$scope:{ctx:F}}}),nn=new ge({}),rn=new U({props:{name:"class transformers.DPRQuestionEncoder",anchor:"transformers.DPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_dpr.py#L526"}}),ln=new U({props:{name:"forward",anchor:"transformers.DPRQuestionEncoder.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_dpr.py#L534",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Wt=new yt({props:{$$slots:{default:[mu]},$$scope:{ctx:F}}}),Ut=new Ln({props:{anchor:"transformers.DPRQuestionEncoder.forward.example",$$slots:{default:[gu]},$$scope:{ctx:F}}}),cn=new ge({}),pn=new U({props:{name:"class transformers.DPRReader",anchor:"transformers.DPRReader",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_dpr.py#L607"}}),mn=new U({props:{name:"forward",anchor:"transformers.DPRReader.forward",parameters:[{name:"input_ids",val:": typing.Optional[torch.Tensor] = None"},{name:"attention_mask",val:": typing.Optional[torch.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[torch.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"}],parametersDescription:[{anchor:"transformers.DPRReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DPRReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DPRReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPRReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPRReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_dpr.py#L615",returnDescription:`
<p>A <a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Kt=new yt({props:{$$slots:{default:[_u]},$$scope:{ctx:F}}}),Yt=new Ln({props:{anchor:"transformers.DPRReader.forward.example",$$slots:{default:[vu]},$$scope:{ctx:F}}}),gn=new ge({}),_n=new U({props:{name:"class transformers.TFDPRContextEncoder",anchor:"transformers.TFDPRContextEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_tf_dpr.py#L534"}}),Jt=new yt({props:{$$slots:{default:[bu]},$$scope:{ctx:F}}}),kn=new U({props:{name:"call",anchor:"transformers.TFDPRContextEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_tf_dpr.py#L546",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),Gt=new yt({props:{$$slots:{default:[Tu]},$$scope:{ctx:F}}}),Zt=new Ln({props:{anchor:"transformers.TFDPRContextEncoder.call.example",$$slots:{default:[ku]},$$scope:{ctx:F}}}),$n=new ge({}),Pn=new U({props:{name:"class transformers.TFDPRQuestionEncoder",anchor:"transformers.TFDPRQuestionEncoder",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_tf_dpr.py#L621"}}),to=new yt({props:{$$slots:{default:[$u]},$$scope:{ctx:F}}}),Dn=new U({props:{name:"call",anchor:"transformers.TFDPRQuestionEncoder.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"token_type_ids",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_tf_dpr.py#L633",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) \u2014 The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),oo=new yt({props:{$$slots:{default:[Pu]},$$scope:{ctx:F}}}),no=new Ln({props:{anchor:"transformers.TFDPRQuestionEncoder.call.example",$$slots:{default:[wu]},$$scope:{ctx:F}}}),yn=new ge({}),xn=new U({props:{name:"class transformers.TFDPRReader",anchor:"transformers.TFDPRReader",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/v4.18.0/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_tf_dpr.py#L707"}}),so=new yt({props:{$$slots:{default:[Eu]},$$scope:{ctx:F}}}),Cn=new U({props:{name:"call",anchor:"transformers.TFDPRReader.call",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"inputs_embeds",val:": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"},{name:"output_attentions",val:": bool = None"},{name:"output_hidden_states",val:": bool = None"},{name:"return_dict",val:" = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRReader.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDPRReader.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDPRReader.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDPRReader.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/v4.18.0/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDPRReader.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/huggingface/transformers/blob/v4.18.0/src/transformers/models/dpr/modeling_tf_dpr.py#L719",returnDescription:`
<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) \u2014 Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, )</code>) \u2014 Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`
<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),ao=new yt({props:{$$slots:{default:[Ru]},$$scope:{ctx:F}}}),io=new Ln({props:{anchor:"transformers.TFDPRReader.call.example",$$slots:{default:[Du]},$$scope:{ctx:F}}}),{c(){h=n("meta"),T=l(),v=n("h1"),u=n("a"),b=n("span"),k(d.$$.fragment),g=l(),x=n("span"),_e=a("DPR"),Z=l(),D=n("h2"),J=n("a"),I=n("span"),k(ee.$$.fragment),ve=l(),S=n("span"),be=a("Overview"),he=l(),H=n("p"),L=a(`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=n("a"),oe=a("Dense Passage Retrieval for Open-Domain Question Answering"),q=a(` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),A=l(),ne=n("p"),V=a("The abstract from the paper is the following:"),fe=l(),re=n("p"),B=n("em"),Te=a(`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),ue=l(),z=n("p"),ke=a("This model was contributed by "),j=n("a"),$e=a("lhoestq"),Pe=a(". The original code can be found "),M=n("a"),we=a("here"),Ee=a("."),Q=l(),K=n("h2"),N=n("a"),se=n("span"),k(_.$$.fragment),y=l(),G=n("span"),Oe=a("DPRConfig"),ze=l(),C=n("div"),k(me.$$.fragment),Ne=l(),ae=n("p"),O=n("a"),Y=a("DPRConfig"),je=a(" is the configuration class to store the configuration of a "),xe=n("em"),X=a("DPRModel"),Me=a("."),Qe=l(),W=n("p"),Le=a("This is the configuration class to store the configuration of a "),Hn=n("a"),pi=a("DPRContextEncoder"),hi=a(", "),Bn=n("a"),fi=a("DPRQuestionEncoder"),ui=a(`, or a
`),Wn=n("a"),mi=a("DPRReader"),gi=a(". It is used to instantiate the components of the DPR model."),_i=l(),$o=n("p"),vi=a("This class is a subclass of "),Un=n("a"),bi=a("BertConfig"),Ti=a(". Please check the superclass for the documentation of all kwargs."),Ks=l(),st=n("h2"),xt=n("a"),Ar=n("span"),k(Po.$$.fragment),ki=l(),Or=n("span"),$i=a("DPRContextEncoderTokenizer"),Ys=l(),qe=n("div"),k(wo.$$.fragment),Pi=l(),Nr=n("p"),wi=a("Construct a DPRContextEncoder tokenizer."),Ei=l(),zt=n("p"),Vn=n("a"),Ri=a("DPRContextEncoderTokenizer"),Di=a(" is identical to "),Kn=n("a"),yi=a("BertTokenizer"),xi=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),zi=l(),Eo=n("p"),qi=a("Refer to superclass "),Yn=n("a"),Fi=a("BertTokenizer"),Ci=a(" for usage examples and documentation concerning parameters."),Xs=l(),at=n("h2"),qt=n("a"),jr=n("span"),k(Ro.$$.fragment),Ai=l(),Mr=n("span"),Oi=a("DPRContextEncoderTokenizerFast"),Js=l(),Fe=n("div"),k(Do.$$.fragment),Ni=l(),yo=n("p"),ji=a("Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Qr=n("em"),Mi=a("tokenizers"),Qi=a(" library)."),Li=l(),Ft=n("p"),Xn=n("a"),Ii=a("DPRContextEncoderTokenizerFast"),Si=a(" is identical to "),Jn=n("a"),Hi=a("BertTokenizerFast"),Bi=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Wi=l(),xo=n("p"),Ui=a("Refer to superclass "),Gn=n("a"),Vi=a("BertTokenizerFast"),Ki=a(" for usage examples and documentation concerning parameters."),Gs=l(),it=n("h2"),Ct=n("a"),Lr=n("span"),k(zo.$$.fragment),Yi=l(),Ir=n("span"),Xi=a("DPRQuestionEncoderTokenizer"),Zs=l(),Ce=n("div"),k(qo.$$.fragment),Ji=l(),Sr=n("p"),Gi=a("Constructs a DPRQuestionEncoder tokenizer."),Zi=l(),At=n("p"),Zn=n("a"),ed=a("DPRQuestionEncoderTokenizer"),td=a(" is identical to "),er=n("a"),od=a("BertTokenizer"),nd=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),rd=l(),Fo=n("p"),sd=a("Refer to superclass "),tr=n("a"),ad=a("BertTokenizer"),id=a(" for usage examples and documentation concerning parameters."),ea=l(),dt=n("h2"),Ot=n("a"),Hr=n("span"),k(Co.$$.fragment),dd=l(),Br=n("span"),ld=a("DPRQuestionEncoderTokenizerFast"),ta=l(),Ae=n("div"),k(Ao.$$.fragment),cd=l(),Oo=n("p"),pd=a("Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Wr=n("em"),hd=a("tokenizers"),fd=a(" library)."),ud=l(),Nt=n("p"),or=n("a"),md=a("DPRQuestionEncoderTokenizerFast"),gd=a(" is identical to "),nr=n("a"),_d=a("BertTokenizerFast"),vd=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),bd=l(),No=n("p"),Td=a("Refer to superclass "),rr=n("a"),kd=a("BertTokenizerFast"),$d=a(" for usage examples and documentation concerning parameters."),oa=l(),lt=n("h2"),jt=n("a"),Ur=n("span"),k(jo.$$.fragment),Pd=l(),Vr=n("span"),wd=a("DPRReaderTokenizer"),na=l(),ie=n("div"),k(Mo.$$.fragment),Ed=l(),Kr=n("p"),Rd=a("Construct a DPRReader tokenizer."),Dd=l(),Ze=n("p"),sr=n("a"),yd=a("DPRReaderTokenizer"),xd=a(" is almost identical to "),ar=n("a"),zd=a("BertTokenizer"),qd=a(` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),ir=n("a"),Fd=a("DPRReader"),Cd=a(" model."),Ad=l(),Qo=n("p"),Od=a("Refer to superclass "),dr=n("a"),Nd=a("BertTokenizer"),jd=a(" for usage examples and documentation concerning parameters."),Md=l(),et=n("p"),Qd=a("Return a dictionary with the token ids of the input strings and other information to give to "),Yr=n("code"),Ld=a(".decode_best_spans"),Id=a(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Xr=n("code"),Sd=a("input_ids"),Hd=a(" is a matrix of size "),Jr=n("code"),Bd=a("(n_passages, sequence_length)"),Wd=l(),k(Mt.$$.fragment),ra=l(),ct=n("h2"),Qt=n("a"),Gr=n("span"),k(Lo.$$.fragment),Ud=l(),Zr=n("span"),Vd=a("DPRReaderTokenizerFast"),sa=l(),de=n("div"),k(Io.$$.fragment),Kd=l(),So=n("p"),Yd=a("Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),es=n("em"),Xd=a("tokenizers"),Jd=a(" library)."),Gd=l(),tt=n("p"),lr=n("a"),Zd=a("DPRReaderTokenizerFast"),el=a(" is almost identical to "),cr=n("a"),tl=a("BertTokenizerFast"),ol=a(` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),pr=n("a"),nl=a("DPRReader"),rl=a(" model."),sl=l(),Ho=n("p"),al=a("Refer to superclass "),hr=n("a"),il=a("BertTokenizerFast"),dl=a(" for usage examples and documentation concerning parameters."),ll=l(),Ge=n("p"),cl=a("Return a dictionary with the token ids of the input strings and other information to give to "),ts=n("code"),pl=a(".decode_best_spans"),hl=a(`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),os=n("code"),fl=a("input_ids"),ul=a(" is a matrix of size "),ns=n("code"),ml=a("(n_passages, sequence_length)"),gl=a(`
with the format:`),_l=l(),rs=n("p"),vl=a("[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),aa=l(),pt=n("h2"),Lt=n("a"),ss=n("span"),k(Bo.$$.fragment),bl=l(),as=n("span"),Tl=a("DPR specific outputs"),ia=l(),ht=n("div"),k(Wo.$$.fragment),kl=l(),Uo=n("p"),$l=a("Class for outputs of "),fr=n("a"),Pl=a("DPRQuestionEncoder"),wl=a("."),da=l(),ft=n("div"),k(Vo.$$.fragment),El=l(),Ko=n("p"),Rl=a("Class for outputs of "),ur=n("a"),Dl=a("DPRQuestionEncoder"),yl=a("."),la=l(),ut=n("div"),k(Yo.$$.fragment),xl=l(),Xo=n("p"),zl=a("Class for outputs of "),mr=n("a"),ql=a("DPRQuestionEncoder"),Fl=a("."),ca=l(),mt=n("h2"),It=n("a"),is=n("span"),k(Jo.$$.fragment),Cl=l(),ds=n("span"),Al=a("DPRContextEncoder"),pa=l(),Re=n("div"),k(Go.$$.fragment),Ol=l(),ls=n("p"),Nl=a("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),jl=l(),Zo=n("p"),Ml=a("This model inherits from "),gr=n("a"),Ql=a("PreTrainedModel"),Ll=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Il=l(),en=n("p"),Sl=a("This model is also a PyTorch "),tn=n("a"),Hl=a("torch.nn.Module"),Bl=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Wl=l(),Ie=n("div"),k(on.$$.fragment),Ul=l(),gt=n("p"),Vl=a("The "),_r=n("a"),Kl=a("DPRContextEncoder"),Yl=a(" forward method, overrides the "),cs=n("code"),Xl=a("__call__"),Jl=a(" special method."),Gl=l(),k(St.$$.fragment),Zl=l(),k(Ht.$$.fragment),ha=l(),_t=n("h2"),Bt=n("a"),ps=n("span"),k(nn.$$.fragment),ec=l(),hs=n("span"),tc=a("DPRQuestionEncoder"),fa=l(),De=n("div"),k(rn.$$.fragment),oc=l(),fs=n("p"),nc=a("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),rc=l(),sn=n("p"),sc=a("This model inherits from "),vr=n("a"),ac=a("PreTrainedModel"),ic=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),dc=l(),an=n("p"),lc=a("This model is also a PyTorch "),dn=n("a"),cc=a("torch.nn.Module"),pc=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),hc=l(),Se=n("div"),k(ln.$$.fragment),fc=l(),vt=n("p"),uc=a("The "),br=n("a"),mc=a("DPRQuestionEncoder"),gc=a(" forward method, overrides the "),us=n("code"),_c=a("__call__"),vc=a(" special method."),bc=l(),k(Wt.$$.fragment),Tc=l(),k(Ut.$$.fragment),ua=l(),bt=n("h2"),Vt=n("a"),ms=n("span"),k(cn.$$.fragment),kc=l(),gs=n("span"),$c=a("DPRReader"),ma=l(),ye=n("div"),k(pn.$$.fragment),Pc=l(),_s=n("p"),wc=a("The bare DPRReader transformer outputting span predictions."),Ec=l(),hn=n("p"),Rc=a("This model inherits from "),Tr=n("a"),Dc=a("PreTrainedModel"),yc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),xc=l(),fn=n("p"),zc=a("This model is also a PyTorch "),un=n("a"),qc=a("torch.nn.Module"),Fc=a(` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Cc=l(),He=n("div"),k(mn.$$.fragment),Ac=l(),Tt=n("p"),Oc=a("The "),kr=n("a"),Nc=a("DPRReader"),jc=a(" forward method, overrides the "),vs=n("code"),Mc=a("__call__"),Qc=a(" special method."),Lc=l(),k(Kt.$$.fragment),Ic=l(),k(Yt.$$.fragment),ga=l(),kt=n("h2"),Xt=n("a"),bs=n("span"),k(gn.$$.fragment),Sc=l(),Ts=n("span"),Hc=a("TFDPRContextEncoder"),_a=l(),le=n("div"),k(_n.$$.fragment),Bc=l(),ks=n("p"),Wc=a("The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Uc=l(),vn=n("p"),Vc=a("This model inherits from "),$r=n("a"),Kc=a("TFPreTrainedModel"),Yc=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Xc=l(),bn=n("p"),Jc=a("This model is also a Tensorflow "),Tn=n("a"),Gc=a("tf.keras.Model"),Zc=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),ep=l(),k(Jt.$$.fragment),tp=l(),Be=n("div"),k(kn.$$.fragment),op=l(),$t=n("p"),np=a("The "),Pr=n("a"),rp=a("TFDPRContextEncoder"),sp=a(" forward method, overrides the "),$s=n("code"),ap=a("__call__"),ip=a(" special method."),dp=l(),k(Gt.$$.fragment),lp=l(),k(Zt.$$.fragment),va=l(),Pt=n("h2"),eo=n("a"),Ps=n("span"),k($n.$$.fragment),cp=l(),ws=n("span"),pp=a("TFDPRQuestionEncoder"),ba=l(),ce=n("div"),k(Pn.$$.fragment),hp=l(),Es=n("p"),fp=a("The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),up=l(),wn=n("p"),mp=a("This model inherits from "),wr=n("a"),gp=a("TFPreTrainedModel"),_p=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),vp=l(),En=n("p"),bp=a("This model is also a Tensorflow "),Rn=n("a"),Tp=a("tf.keras.Model"),kp=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),$p=l(),k(to.$$.fragment),Pp=l(),We=n("div"),k(Dn.$$.fragment),wp=l(),wt=n("p"),Ep=a("The "),Er=n("a"),Rp=a("TFDPRQuestionEncoder"),Dp=a(" forward method, overrides the "),Rs=n("code"),yp=a("__call__"),xp=a(" special method."),zp=l(),k(oo.$$.fragment),qp=l(),k(no.$$.fragment),Ta=l(),Et=n("h2"),ro=n("a"),Ds=n("span"),k(yn.$$.fragment),Fp=l(),ys=n("span"),Cp=a("TFDPRReader"),ka=l(),pe=n("div"),k(xn.$$.fragment),Ap=l(),xs=n("p"),Op=a("The bare DPRReader transformer outputting span predictions."),Np=l(),zn=n("p"),jp=a("This model inherits from "),Rr=n("a"),Mp=a("TFPreTrainedModel"),Qp=a(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Lp=l(),qn=n("p"),Ip=a("This model is also a Tensorflow "),Fn=n("a"),Sp=a("tf.keras.Model"),Hp=a(`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),Bp=l(),k(so.$$.fragment),Wp=l(),Ue=n("div"),k(Cn.$$.fragment),Up=l(),Rt=n("p"),Vp=a("The "),Dr=n("a"),Kp=a("TFDPRReader"),Yp=a(" forward method, overrides the "),zs=n("code"),Xp=a("__call__"),Jp=a(" special method."),Gp=l(),k(ao.$$.fragment),Zp=l(),k(io.$$.fragment),this.h()},l(o){const m=cu('[data-svelte="svelte-1phssyn"]',document.head);h=r(m,"META",{name:!0,content:!0}),m.forEach(t),T=c(o),v=r(o,"H1",{class:!0});var An=s(v);u=r(An,"A",{id:!0,class:!0,href:!0});var qs=s(u);b=r(qs,"SPAN",{});var Fs=s(b);$(d.$$.fragment,Fs),Fs.forEach(t),qs.forEach(t),g=c(An),x=r(An,"SPAN",{});var Cs=s(x);_e=i(Cs,"DPR"),Cs.forEach(t),An.forEach(t),Z=c(o),D=r(o,"H2",{class:!0});var On=s(D);J=r(On,"A",{id:!0,class:!0,href:!0});var As=s(J);I=r(As,"SPAN",{});var Os=s(I);$(ee.$$.fragment,Os),Os.forEach(t),As.forEach(t),ve=c(On),S=r(On,"SPAN",{});var Ns=s(S);be=i(Ns,"Overview"),Ns.forEach(t),On.forEach(t),he=c(o),H=r(o,"P",{});var Nn=s(H);L=i(Nn,`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&A research. It was
introduced in `),te=r(Nn,"A",{href:!0,rel:!0});var js=s(te);oe=i(js,"Dense Passage Retrieval for Open-Domain Question Answering"),js.forEach(t),q=i(Nn,` by
Vladimir Karpukhin, Barlas O\u011Fuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`),Nn.forEach(t),A=c(o),ne=r(o,"P",{});var Ms=s(ne);V=i(Ms,"The abstract from the paper is the following:"),Ms.forEach(t),fe=c(o),re=r(o,"P",{});var Qs=s(re);B=r(Qs,"EM",{});var Ls=s(B);Te=i(Ls,`Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.`),Ls.forEach(t),Qs.forEach(t),ue=c(o),z=r(o,"P",{});var Dt=s(z);ke=i(Dt,"This model was contributed by "),j=r(Dt,"A",{href:!0,rel:!0});var Is=s(j);$e=i(Is,"lhoestq"),Is.forEach(t),Pe=i(Dt,". The original code can be found "),M=r(Dt,"A",{href:!0,rel:!0});var Ss=s(M);we=i(Ss,"here"),Ss.forEach(t),Ee=i(Dt,"."),Dt.forEach(t),Q=c(o),K=r(o,"H2",{class:!0});var Pa=s(K);N=r(Pa,"A",{id:!0,class:!0,href:!0});var eh=s(N);se=r(eh,"SPAN",{});var th=s(se);$(_.$$.fragment,th),th.forEach(t),eh.forEach(t),y=c(Pa),G=r(Pa,"SPAN",{});var oh=s(G);Oe=i(oh,"DPRConfig"),oh.forEach(t),Pa.forEach(t),ze=c(o),C=r(o,"DIV",{class:!0});var lo=s(C);$(me.$$.fragment,lo),Ne=c(lo),ae=r(lo,"P",{});var Hs=s(ae);O=r(Hs,"A",{href:!0});var nh=s(O);Y=i(nh,"DPRConfig"),nh.forEach(t),je=i(Hs," is the configuration class to store the configuration of a "),xe=r(Hs,"EM",{});var rh=s(xe);X=i(rh,"DPRModel"),rh.forEach(t),Me=i(Hs,"."),Hs.forEach(t),Qe=c(lo),W=r(lo,"P",{});var co=s(W);Le=i(co,"This is the configuration class to store the configuration of a "),Hn=r(co,"A",{href:!0});var sh=s(Hn);pi=i(sh,"DPRContextEncoder"),sh.forEach(t),hi=i(co,", "),Bn=r(co,"A",{href:!0});var ah=s(Bn);fi=i(ah,"DPRQuestionEncoder"),ah.forEach(t),ui=i(co,`, or a
`),Wn=r(co,"A",{href:!0});var ih=s(Wn);mi=i(ih,"DPRReader"),ih.forEach(t),gi=i(co,". It is used to instantiate the components of the DPR model."),co.forEach(t),_i=c(lo),$o=r(lo,"P",{});var wa=s($o);vi=i(wa,"This class is a subclass of "),Un=r(wa,"A",{href:!0});var dh=s(Un);bi=i(dh,"BertConfig"),dh.forEach(t),Ti=i(wa,". Please check the superclass for the documentation of all kwargs."),wa.forEach(t),lo.forEach(t),Ks=c(o),st=r(o,"H2",{class:!0});var Ea=s(st);xt=r(Ea,"A",{id:!0,class:!0,href:!0});var lh=s(xt);Ar=r(lh,"SPAN",{});var ch=s(Ar);$(Po.$$.fragment,ch),ch.forEach(t),lh.forEach(t),ki=c(Ea),Or=r(Ea,"SPAN",{});var ph=s(Or);$i=i(ph,"DPRContextEncoderTokenizer"),ph.forEach(t),Ea.forEach(t),Ys=c(o),qe=r(o,"DIV",{class:!0});var po=s(qe);$(wo.$$.fragment,po),Pi=c(po),Nr=r(po,"P",{});var hh=s(Nr);wi=i(hh,"Construct a DPRContextEncoder tokenizer."),hh.forEach(t),Ei=c(po),zt=r(po,"P",{});var Bs=s(zt);Vn=r(Bs,"A",{href:!0});var fh=s(Vn);Ri=i(fh,"DPRContextEncoderTokenizer"),fh.forEach(t),Di=i(Bs," is identical to "),Kn=r(Bs,"A",{href:!0});var uh=s(Kn);yi=i(uh,"BertTokenizer"),uh.forEach(t),xi=i(Bs,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Bs.forEach(t),zi=c(po),Eo=r(po,"P",{});var Ra=s(Eo);qi=i(Ra,"Refer to superclass "),Yn=r(Ra,"A",{href:!0});var mh=s(Yn);Fi=i(mh,"BertTokenizer"),mh.forEach(t),Ci=i(Ra," for usage examples and documentation concerning parameters."),Ra.forEach(t),po.forEach(t),Xs=c(o),at=r(o,"H2",{class:!0});var Da=s(at);qt=r(Da,"A",{id:!0,class:!0,href:!0});var gh=s(qt);jr=r(gh,"SPAN",{});var _h=s(jr);$(Ro.$$.fragment,_h),_h.forEach(t),gh.forEach(t),Ai=c(Da),Mr=r(Da,"SPAN",{});var vh=s(Mr);Oi=i(vh,"DPRContextEncoderTokenizerFast"),vh.forEach(t),Da.forEach(t),Js=c(o),Fe=r(o,"DIV",{class:!0});var ho=s(Fe);$(Do.$$.fragment,ho),Ni=c(ho),yo=r(ho,"P",{});var ya=s(yo);ji=i(ya,"Construct a \u201Cfast\u201D DPRContextEncoder tokenizer (backed by HuggingFace\u2019s "),Qr=r(ya,"EM",{});var bh=s(Qr);Mi=i(bh,"tokenizers"),bh.forEach(t),Qi=i(ya," library)."),ya.forEach(t),Li=c(ho),Ft=r(ho,"P",{});var Ws=s(Ft);Xn=r(Ws,"A",{href:!0});var Th=s(Xn);Ii=i(Th,"DPRContextEncoderTokenizerFast"),Th.forEach(t),Si=i(Ws," is identical to "),Jn=r(Ws,"A",{href:!0});var kh=s(Jn);Hi=i(kh,"BertTokenizerFast"),kh.forEach(t),Bi=i(Ws,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Ws.forEach(t),Wi=c(ho),xo=r(ho,"P",{});var xa=s(xo);Ui=i(xa,"Refer to superclass "),Gn=r(xa,"A",{href:!0});var $h=s(Gn);Vi=i($h,"BertTokenizerFast"),$h.forEach(t),Ki=i(xa," for usage examples and documentation concerning parameters."),xa.forEach(t),ho.forEach(t),Gs=c(o),it=r(o,"H2",{class:!0});var za=s(it);Ct=r(za,"A",{id:!0,class:!0,href:!0});var Ph=s(Ct);Lr=r(Ph,"SPAN",{});var wh=s(Lr);$(zo.$$.fragment,wh),wh.forEach(t),Ph.forEach(t),Yi=c(za),Ir=r(za,"SPAN",{});var Eh=s(Ir);Xi=i(Eh,"DPRQuestionEncoderTokenizer"),Eh.forEach(t),za.forEach(t),Zs=c(o),Ce=r(o,"DIV",{class:!0});var fo=s(Ce);$(qo.$$.fragment,fo),Ji=c(fo),Sr=r(fo,"P",{});var Rh=s(Sr);Gi=i(Rh,"Constructs a DPRQuestionEncoder tokenizer."),Rh.forEach(t),Zi=c(fo),At=r(fo,"P",{});var Us=s(At);Zn=r(Us,"A",{href:!0});var Dh=s(Zn);ed=i(Dh,"DPRQuestionEncoderTokenizer"),Dh.forEach(t),td=i(Us," is identical to "),er=r(Us,"A",{href:!0});var yh=s(er);od=i(yh,"BertTokenizer"),yh.forEach(t),nd=i(Us,` and runs end-to-end tokenization: punctuation
splitting and wordpiece.`),Us.forEach(t),rd=c(fo),Fo=r(fo,"P",{});var qa=s(Fo);sd=i(qa,"Refer to superclass "),tr=r(qa,"A",{href:!0});var xh=s(tr);ad=i(xh,"BertTokenizer"),xh.forEach(t),id=i(qa," for usage examples and documentation concerning parameters."),qa.forEach(t),fo.forEach(t),ea=c(o),dt=r(o,"H2",{class:!0});var Fa=s(dt);Ot=r(Fa,"A",{id:!0,class:!0,href:!0});var zh=s(Ot);Hr=r(zh,"SPAN",{});var qh=s(Hr);$(Co.$$.fragment,qh),qh.forEach(t),zh.forEach(t),dd=c(Fa),Br=r(Fa,"SPAN",{});var Fh=s(Br);ld=i(Fh,"DPRQuestionEncoderTokenizerFast"),Fh.forEach(t),Fa.forEach(t),ta=c(o),Ae=r(o,"DIV",{class:!0});var uo=s(Ae);$(Ao.$$.fragment,uo),cd=c(uo),Oo=r(uo,"P",{});var Ca=s(Oo);pd=i(Ca,"Constructs a \u201Cfast\u201D DPRQuestionEncoder tokenizer (backed by HuggingFace\u2019s "),Wr=r(Ca,"EM",{});var Ch=s(Wr);hd=i(Ch,"tokenizers"),Ch.forEach(t),fd=i(Ca," library)."),Ca.forEach(t),ud=c(uo),Nt=r(uo,"P",{});var Vs=s(Nt);or=r(Vs,"A",{href:!0});var Ah=s(or);md=i(Ah,"DPRQuestionEncoderTokenizerFast"),Ah.forEach(t),gd=i(Vs," is identical to "),nr=r(Vs,"A",{href:!0});var Oh=s(nr);_d=i(Oh,"BertTokenizerFast"),Oh.forEach(t),vd=i(Vs,` and runs end-to-end tokenization:
punctuation splitting and wordpiece.`),Vs.forEach(t),bd=c(uo),No=r(uo,"P",{});var Aa=s(No);Td=i(Aa,"Refer to superclass "),rr=r(Aa,"A",{href:!0});var Nh=s(rr);kd=i(Nh,"BertTokenizerFast"),Nh.forEach(t),$d=i(Aa," for usage examples and documentation concerning parameters."),Aa.forEach(t),uo.forEach(t),oa=c(o),lt=r(o,"H2",{class:!0});var Oa=s(lt);jt=r(Oa,"A",{id:!0,class:!0,href:!0});var jh=s(jt);Ur=r(jh,"SPAN",{});var Mh=s(Ur);$(jo.$$.fragment,Mh),Mh.forEach(t),jh.forEach(t),Pd=c(Oa),Vr=r(Oa,"SPAN",{});var Qh=s(Vr);wd=i(Qh,"DPRReaderTokenizer"),Qh.forEach(t),Oa.forEach(t),na=c(o),ie=r(o,"DIV",{class:!0});var Ve=s(ie);$(Mo.$$.fragment,Ve),Ed=c(Ve),Kr=r(Ve,"P",{});var Lh=s(Kr);Rd=i(Lh,"Construct a DPRReader tokenizer."),Lh.forEach(t),Dd=c(Ve),Ze=r(Ve,"P",{});var jn=s(Ze);sr=r(jn,"A",{href:!0});var Ih=s(sr);yd=i(Ih,"DPRReaderTokenizer"),Ih.forEach(t),xd=i(jn," is almost identical to "),ar=r(jn,"A",{href:!0});var Sh=s(ar);zd=i(Sh,"BertTokenizer"),Sh.forEach(t),qd=i(jn,` and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the `),ir=r(jn,"A",{href:!0});var Hh=s(ir);Fd=i(Hh,"DPRReader"),Hh.forEach(t),Cd=i(jn," model."),jn.forEach(t),Ad=c(Ve),Qo=r(Ve,"P",{});var Na=s(Qo);Od=i(Na,"Refer to superclass "),dr=r(Na,"A",{href:!0});var Bh=s(dr);Nd=i(Bh,"BertTokenizer"),Bh.forEach(t),jd=i(Na," for usage examples and documentation concerning parameters."),Na.forEach(t),Md=c(Ve),et=r(Ve,"P",{});var Mn=s(et);Qd=i(Mn,"Return a dictionary with the token ids of the input strings and other information to give to "),Yr=r(Mn,"CODE",{});var Wh=s(Yr);Ld=i(Wh,".decode_best_spans"),Wh.forEach(t),Id=i(Mn,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),Xr=r(Mn,"CODE",{});var Uh=s(Xr);Sd=i(Uh,"input_ids"),Uh.forEach(t),Hd=i(Mn," is a matrix of size "),Jr=r(Mn,"CODE",{});var Vh=s(Jr);Bd=i(Vh,"(n_passages, sequence_length)"),Vh.forEach(t),Mn.forEach(t),Wd=c(Ve),$(Mt.$$.fragment,Ve),Ve.forEach(t),ra=c(o),ct=r(o,"H2",{class:!0});var ja=s(ct);Qt=r(ja,"A",{id:!0,class:!0,href:!0});var Kh=s(Qt);Gr=r(Kh,"SPAN",{});var Yh=s(Gr);$(Lo.$$.fragment,Yh),Yh.forEach(t),Kh.forEach(t),Ud=c(ja),Zr=r(ja,"SPAN",{});var Xh=s(Zr);Vd=i(Xh,"DPRReaderTokenizerFast"),Xh.forEach(t),ja.forEach(t),sa=c(o),de=r(o,"DIV",{class:!0});var Ke=s(de);$(Io.$$.fragment,Ke),Kd=c(Ke),So=r(Ke,"P",{});var Ma=s(So);Yd=i(Ma,"Constructs a \u201Cfast\u201D DPRReader tokenizer (backed by HuggingFace\u2019s "),es=r(Ma,"EM",{});var Jh=s(es);Xd=i(Jh,"tokenizers"),Jh.forEach(t),Jd=i(Ma," library)."),Ma.forEach(t),Gd=c(Ke),tt=r(Ke,"P",{});var Qn=s(tt);lr=r(Qn,"A",{href:!0});var Gh=s(lr);Zd=i(Gh,"DPRReaderTokenizerFast"),Gh.forEach(t),el=i(Qn," is almost identical to "),cr=r(Qn,"A",{href:!0});var Zh=s(cr);tl=i(Zh,"BertTokenizerFast"),Zh.forEach(t),ol=i(Qn,` and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the `),pr=r(Qn,"A",{href:!0});var ef=s(pr);nl=i(ef,"DPRReader"),ef.forEach(t),rl=i(Qn," model."),Qn.forEach(t),sl=c(Ke),Ho=r(Ke,"P",{});var Qa=s(Ho);al=i(Qa,"Refer to superclass "),hr=r(Qa,"A",{href:!0});var tf=s(hr);il=i(tf,"BertTokenizerFast"),tf.forEach(t),dl=i(Qa," for usage examples and documentation concerning parameters."),Qa.forEach(t),ll=c(Ke),Ge=r(Ke,"P",{});var mo=s(Ge);cl=i(mo,"Return a dictionary with the token ids of the input strings and other information to give to "),ts=r(mo,"CODE",{});var of=s(ts);pl=i(of,".decode_best_spans"),of.forEach(t),hl=i(mo,`.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting `),os=r(mo,"CODE",{});var nf=s(os);fl=i(nf,"input_ids"),nf.forEach(t),ul=i(mo," is a matrix of size "),ns=r(mo,"CODE",{});var rf=s(ns);ml=i(rf,"(n_passages, sequence_length)"),rf.forEach(t),gl=i(mo,`
with the format:`),mo.forEach(t),_l=c(Ke),rs=r(Ke,"P",{});var sf=s(rs);vl=i(sf,"[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>"),sf.forEach(t),Ke.forEach(t),aa=c(o),pt=r(o,"H2",{class:!0});var La=s(pt);Lt=r(La,"A",{id:!0,class:!0,href:!0});var af=s(Lt);ss=r(af,"SPAN",{});var df=s(ss);$(Bo.$$.fragment,df),df.forEach(t),af.forEach(t),bl=c(La),as=r(La,"SPAN",{});var lf=s(as);Tl=i(lf,"DPR specific outputs"),lf.forEach(t),La.forEach(t),ia=c(o),ht=r(o,"DIV",{class:!0});var Ia=s(ht);$(Wo.$$.fragment,Ia),kl=c(Ia),Uo=r(Ia,"P",{});var Sa=s(Uo);$l=i(Sa,"Class for outputs of "),fr=r(Sa,"A",{href:!0});var cf=s(fr);Pl=i(cf,"DPRQuestionEncoder"),cf.forEach(t),wl=i(Sa,"."),Sa.forEach(t),Ia.forEach(t),da=c(o),ft=r(o,"DIV",{class:!0});var Ha=s(ft);$(Vo.$$.fragment,Ha),El=c(Ha),Ko=r(Ha,"P",{});var Ba=s(Ko);Rl=i(Ba,"Class for outputs of "),ur=r(Ba,"A",{href:!0});var pf=s(ur);Dl=i(pf,"DPRQuestionEncoder"),pf.forEach(t),yl=i(Ba,"."),Ba.forEach(t),Ha.forEach(t),la=c(o),ut=r(o,"DIV",{class:!0});var Wa=s(ut);$(Yo.$$.fragment,Wa),xl=c(Wa),Xo=r(Wa,"P",{});var Ua=s(Xo);zl=i(Ua,"Class for outputs of "),mr=r(Ua,"A",{href:!0});var hf=s(mr);ql=i(hf,"DPRQuestionEncoder"),hf.forEach(t),Fl=i(Ua,"."),Ua.forEach(t),Wa.forEach(t),ca=c(o),mt=r(o,"H2",{class:!0});var Va=s(mt);It=r(Va,"A",{id:!0,class:!0,href:!0});var ff=s(It);is=r(ff,"SPAN",{});var uf=s(is);$(Jo.$$.fragment,uf),uf.forEach(t),ff.forEach(t),Cl=c(Va),ds=r(Va,"SPAN",{});var mf=s(ds);Al=i(mf,"DPRContextEncoder"),mf.forEach(t),Va.forEach(t),pa=c(o),Re=r(o,"DIV",{class:!0});var ot=s(Re);$(Go.$$.fragment,ot),Ol=c(ot),ls=r(ot,"P",{});var gf=s(ls);Nl=i(gf,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),gf.forEach(t),jl=c(ot),Zo=r(ot,"P",{});var Ka=s(Zo);Ml=i(Ka,"This model inherits from "),gr=r(Ka,"A",{href:!0});var _f=s(gr);Ql=i(_f,"PreTrainedModel"),_f.forEach(t),Ll=i(Ka,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ka.forEach(t),Il=c(ot),en=r(ot,"P",{});var Ya=s(en);Sl=i(Ya,"This model is also a PyTorch "),tn=r(Ya,"A",{href:!0,rel:!0});var vf=s(tn);Hl=i(vf,"torch.nn.Module"),vf.forEach(t),Bl=i(Ya,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ya.forEach(t),Wl=c(ot),Ie=r(ot,"DIV",{class:!0});var go=s(Ie);$(on.$$.fragment,go),Ul=c(go),gt=r(go,"P",{});var yr=s(gt);Vl=i(yr,"The "),_r=r(yr,"A",{href:!0});var bf=s(_r);Kl=i(bf,"DPRContextEncoder"),bf.forEach(t),Yl=i(yr," forward method, overrides the "),cs=r(yr,"CODE",{});var Tf=s(cs);Xl=i(Tf,"__call__"),Tf.forEach(t),Jl=i(yr," special method."),yr.forEach(t),Gl=c(go),$(St.$$.fragment,go),Zl=c(go),$(Ht.$$.fragment,go),go.forEach(t),ot.forEach(t),ha=c(o),_t=r(o,"H2",{class:!0});var Xa=s(_t);Bt=r(Xa,"A",{id:!0,class:!0,href:!0});var kf=s(Bt);ps=r(kf,"SPAN",{});var $f=s(ps);$(nn.$$.fragment,$f),$f.forEach(t),kf.forEach(t),ec=c(Xa),hs=r(Xa,"SPAN",{});var Pf=s(hs);tc=i(Pf,"DPRQuestionEncoder"),Pf.forEach(t),Xa.forEach(t),fa=c(o),De=r(o,"DIV",{class:!0});var nt=s(De);$(rn.$$.fragment,nt),oc=c(nt),fs=r(nt,"P",{});var wf=s(fs);nc=i(wf,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),wf.forEach(t),rc=c(nt),sn=r(nt,"P",{});var Ja=s(sn);sc=i(Ja,"This model inherits from "),vr=r(Ja,"A",{href:!0});var Ef=s(vr);ac=i(Ef,"PreTrainedModel"),Ef.forEach(t),ic=i(Ja,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ja.forEach(t),dc=c(nt),an=r(nt,"P",{});var Ga=s(an);lc=i(Ga,"This model is also a PyTorch "),dn=r(Ga,"A",{href:!0,rel:!0});var Rf=s(dn);cc=i(Rf,"torch.nn.Module"),Rf.forEach(t),pc=i(Ga,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),Ga.forEach(t),hc=c(nt),Se=r(nt,"DIV",{class:!0});var _o=s(Se);$(ln.$$.fragment,_o),fc=c(_o),vt=r(_o,"P",{});var xr=s(vt);uc=i(xr,"The "),br=r(xr,"A",{href:!0});var Df=s(br);mc=i(Df,"DPRQuestionEncoder"),Df.forEach(t),gc=i(xr," forward method, overrides the "),us=r(xr,"CODE",{});var yf=s(us);_c=i(yf,"__call__"),yf.forEach(t),vc=i(xr," special method."),xr.forEach(t),bc=c(_o),$(Wt.$$.fragment,_o),Tc=c(_o),$(Ut.$$.fragment,_o),_o.forEach(t),nt.forEach(t),ua=c(o),bt=r(o,"H2",{class:!0});var Za=s(bt);Vt=r(Za,"A",{id:!0,class:!0,href:!0});var xf=s(Vt);ms=r(xf,"SPAN",{});var zf=s(ms);$(cn.$$.fragment,zf),zf.forEach(t),xf.forEach(t),kc=c(Za),gs=r(Za,"SPAN",{});var qf=s(gs);$c=i(qf,"DPRReader"),qf.forEach(t),Za.forEach(t),ma=c(o),ye=r(o,"DIV",{class:!0});var rt=s(ye);$(pn.$$.fragment,rt),Pc=c(rt),_s=r(rt,"P",{});var Ff=s(_s);wc=i(Ff,"The bare DPRReader transformer outputting span predictions."),Ff.forEach(t),Ec=c(rt),hn=r(rt,"P",{});var ei=s(hn);Rc=i(ei,"This model inherits from "),Tr=r(ei,"A",{href:!0});var Cf=s(Tr);Dc=i(Cf,"PreTrainedModel"),Cf.forEach(t),yc=i(ei,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ei.forEach(t),xc=c(rt),fn=r(rt,"P",{});var ti=s(fn);zc=i(ti,"This model is also a PyTorch "),un=r(ti,"A",{href:!0,rel:!0});var Af=s(un);qc=i(Af,"torch.nn.Module"),Af.forEach(t),Fc=i(ti,` subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`),ti.forEach(t),Cc=c(rt),He=r(rt,"DIV",{class:!0});var vo=s(He);$(mn.$$.fragment,vo),Ac=c(vo),Tt=r(vo,"P",{});var zr=s(Tt);Oc=i(zr,"The "),kr=r(zr,"A",{href:!0});var Of=s(kr);Nc=i(Of,"DPRReader"),Of.forEach(t),jc=i(zr," forward method, overrides the "),vs=r(zr,"CODE",{});var Nf=s(vs);Mc=i(Nf,"__call__"),Nf.forEach(t),Qc=i(zr," special method."),zr.forEach(t),Lc=c(vo),$(Kt.$$.fragment,vo),Ic=c(vo),$(Yt.$$.fragment,vo),vo.forEach(t),rt.forEach(t),ga=c(o),kt=r(o,"H2",{class:!0});var oi=s(kt);Xt=r(oi,"A",{id:!0,class:!0,href:!0});var jf=s(Xt);bs=r(jf,"SPAN",{});var Mf=s(bs);$(gn.$$.fragment,Mf),Mf.forEach(t),jf.forEach(t),Sc=c(oi),Ts=r(oi,"SPAN",{});var Qf=s(Ts);Hc=i(Qf,"TFDPRContextEncoder"),Qf.forEach(t),oi.forEach(t),_a=c(o),le=r(o,"DIV",{class:!0});var Ye=s(le);$(_n.$$.fragment,Ye),Bc=c(Ye),ks=r(Ye,"P",{});var Lf=s(ks);Wc=i(Lf,"The bare DPRContextEncoder transformer outputting pooler outputs as context representations."),Lf.forEach(t),Uc=c(Ye),vn=r(Ye,"P",{});var ni=s(vn);Vc=i(ni,"This model inherits from "),$r=r(ni,"A",{href:!0});var If=s($r);Kc=i(If,"TFPreTrainedModel"),If.forEach(t),Yc=i(ni,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ni.forEach(t),Xc=c(Ye),bn=r(Ye,"P",{});var ri=s(bn);Jc=i(ri,"This model is also a Tensorflow "),Tn=r(ri,"A",{href:!0,rel:!0});var Sf=s(Tn);Gc=i(Sf,"tf.keras.Model"),Sf.forEach(t),Zc=i(ri,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),ri.forEach(t),ep=c(Ye),$(Jt.$$.fragment,Ye),tp=c(Ye),Be=r(Ye,"DIV",{class:!0});var bo=s(Be);$(kn.$$.fragment,bo),op=c(bo),$t=r(bo,"P",{});var qr=s($t);np=i(qr,"The "),Pr=r(qr,"A",{href:!0});var Hf=s(Pr);rp=i(Hf,"TFDPRContextEncoder"),Hf.forEach(t),sp=i(qr," forward method, overrides the "),$s=r(qr,"CODE",{});var Bf=s($s);ap=i(Bf,"__call__"),Bf.forEach(t),ip=i(qr," special method."),qr.forEach(t),dp=c(bo),$(Gt.$$.fragment,bo),lp=c(bo),$(Zt.$$.fragment,bo),bo.forEach(t),Ye.forEach(t),va=c(o),Pt=r(o,"H2",{class:!0});var si=s(Pt);eo=r(si,"A",{id:!0,class:!0,href:!0});var Wf=s(eo);Ps=r(Wf,"SPAN",{});var Uf=s(Ps);$($n.$$.fragment,Uf),Uf.forEach(t),Wf.forEach(t),cp=c(si),ws=r(si,"SPAN",{});var Vf=s(ws);pp=i(Vf,"TFDPRQuestionEncoder"),Vf.forEach(t),si.forEach(t),ba=c(o),ce=r(o,"DIV",{class:!0});var Xe=s(ce);$(Pn.$$.fragment,Xe),hp=c(Xe),Es=r(Xe,"P",{});var Kf=s(Es);fp=i(Kf,"The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations."),Kf.forEach(t),up=c(Xe),wn=r(Xe,"P",{});var ai=s(wn);mp=i(ai,"This model inherits from "),wr=r(ai,"A",{href:!0});var Yf=s(wr);gp=i(Yf,"TFPreTrainedModel"),Yf.forEach(t),_p=i(ai,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ai.forEach(t),vp=c(Xe),En=r(Xe,"P",{});var ii=s(En);bp=i(ii,"This model is also a Tensorflow "),Rn=r(ii,"A",{href:!0,rel:!0});var Xf=s(Rn);Tp=i(Xf,"tf.keras.Model"),Xf.forEach(t),kp=i(ii,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),ii.forEach(t),$p=c(Xe),$(to.$$.fragment,Xe),Pp=c(Xe),We=r(Xe,"DIV",{class:!0});var To=s(We);$(Dn.$$.fragment,To),wp=c(To),wt=r(To,"P",{});var Fr=s(wt);Ep=i(Fr,"The "),Er=r(Fr,"A",{href:!0});var Jf=s(Er);Rp=i(Jf,"TFDPRQuestionEncoder"),Jf.forEach(t),Dp=i(Fr," forward method, overrides the "),Rs=r(Fr,"CODE",{});var Gf=s(Rs);yp=i(Gf,"__call__"),Gf.forEach(t),xp=i(Fr," special method."),Fr.forEach(t),zp=c(To),$(oo.$$.fragment,To),qp=c(To),$(no.$$.fragment,To),To.forEach(t),Xe.forEach(t),Ta=c(o),Et=r(o,"H2",{class:!0});var di=s(Et);ro=r(di,"A",{id:!0,class:!0,href:!0});var Zf=s(ro);Ds=r(Zf,"SPAN",{});var eu=s(Ds);$(yn.$$.fragment,eu),eu.forEach(t),Zf.forEach(t),Fp=c(di),ys=r(di,"SPAN",{});var tu=s(ys);Cp=i(tu,"TFDPRReader"),tu.forEach(t),di.forEach(t),ka=c(o),pe=r(o,"DIV",{class:!0});var Je=s(pe);$(xn.$$.fragment,Je),Ap=c(Je),xs=r(Je,"P",{});var ou=s(xs);Op=i(ou,"The bare DPRReader transformer outputting span predictions."),ou.forEach(t),Np=c(Je),zn=r(Je,"P",{});var li=s(zn);jp=i(li,"This model inherits from "),Rr=r(li,"A",{href:!0});var nu=s(Rr);Mp=i(nu,"TFPreTrainedModel"),nu.forEach(t),Qp=i(li,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),li.forEach(t),Lp=c(Je),qn=r(Je,"P",{});var ci=s(qn);Ip=i(ci,"This model is also a Tensorflow "),Fn=r(ci,"A",{href:!0,rel:!0});var ru=s(Fn);Sp=i(ru,"tf.keras.Model"),ru.forEach(t),Hp=i(ci,`
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`),ci.forEach(t),Bp=c(Je),$(so.$$.fragment,Je),Wp=c(Je),Ue=r(Je,"DIV",{class:!0});var ko=s(Ue);$(Cn.$$.fragment,ko),Up=c(ko),Rt=r(ko,"P",{});var Cr=s(Rt);Vp=i(Cr,"The "),Dr=r(Cr,"A",{href:!0});var su=s(Dr);Kp=i(su,"TFDPRReader"),su.forEach(t),Yp=i(Cr," forward method, overrides the "),zs=r(Cr,"CODE",{});var au=s(zs);Xp=i(au,"__call__"),au.forEach(t),Jp=i(Cr," special method."),Cr.forEach(t),Gp=c(ko),$(ao.$$.fragment,ko),Zp=c(ko),$(io.$$.fragment,ko),ko.forEach(t),Je.forEach(t),this.h()},h(){p(h,"name","hf:doc:metadata"),p(h,"content",JSON.stringify(xu)),p(u,"id","dpr"),p(u,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(u,"href","#dpr"),p(v,"class","relative group"),p(J,"id","overview"),p(J,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(J,"href","#overview"),p(D,"class","relative group"),p(te,"href","https://arxiv.org/abs/2004.04906"),p(te,"rel","nofollow"),p(j,"href","https://huggingface.co/lhoestq"),p(j,"rel","nofollow"),p(M,"href","https://github.com/facebookresearch/DPR"),p(M,"rel","nofollow"),p(N,"id","transformers.DPRConfig"),p(N,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(N,"href","#transformers.DPRConfig"),p(K,"class","relative group"),p(O,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRConfig"),p(Hn,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRContextEncoder"),p(Bn,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(Wn,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRReader"),p(Un,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertConfig"),p(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(xt,"id","transformers.DPRContextEncoderTokenizer"),p(xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(xt,"href","#transformers.DPRContextEncoderTokenizer"),p(st,"class","relative group"),p(Vn,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer"),p(Kn,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizer"),p(Yn,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizer"),p(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(qt,"id","transformers.DPRContextEncoderTokenizerFast"),p(qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(qt,"href","#transformers.DPRContextEncoderTokenizerFast"),p(at,"class","relative group"),p(Xn,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast"),p(Jn,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizerFast"),p(Gn,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizerFast"),p(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ct,"id","transformers.DPRQuestionEncoderTokenizer"),p(Ct,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ct,"href","#transformers.DPRQuestionEncoderTokenizer"),p(it,"class","relative group"),p(Zn,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),p(er,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizer"),p(tr,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizer"),p(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Ot,"id","transformers.DPRQuestionEncoderTokenizerFast"),p(Ot,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Ot,"href","#transformers.DPRQuestionEncoderTokenizerFast"),p(dt,"class","relative group"),p(or,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),p(nr,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizerFast"),p(rr,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizerFast"),p(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(jt,"id","transformers.DPRReaderTokenizer"),p(jt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(jt,"href","#transformers.DPRReaderTokenizer"),p(lt,"class","relative group"),p(sr,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRReaderTokenizer"),p(ar,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizer"),p(ir,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRReader"),p(dr,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizer"),p(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Qt,"id","transformers.DPRReaderTokenizerFast"),p(Qt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Qt,"href","#transformers.DPRReaderTokenizerFast"),p(ct,"class","relative group"),p(lr,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRReaderTokenizerFast"),p(cr,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizerFast"),p(pr,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRReader"),p(hr,"href","/docs/transformers/v4.18.0/en/model_doc/bert#transformers.BertTokenizerFast"),p(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Lt,"id","transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),p(Lt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Lt,"href","#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"),p(pt,"class","relative group"),p(fr,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ur,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(mr,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(It,"id","transformers.DPRContextEncoder"),p(It,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(It,"href","#transformers.DPRContextEncoder"),p(mt,"class","relative group"),p(gr,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel"),p(tn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(tn,"rel","nofollow"),p(_r,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRContextEncoder"),p(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Bt,"id","transformers.DPRQuestionEncoder"),p(Bt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Bt,"href","#transformers.DPRQuestionEncoder"),p(_t,"class","relative group"),p(vr,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel"),p(dn,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(dn,"rel","nofollow"),p(br,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRQuestionEncoder"),p(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Vt,"id","transformers.DPRReader"),p(Vt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Vt,"href","#transformers.DPRReader"),p(bt,"class","relative group"),p(Tr,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.PreTrainedModel"),p(un,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),p(un,"rel","nofollow"),p(kr,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.DPRReader"),p(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(Xt,"id","transformers.TFDPRContextEncoder"),p(Xt,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(Xt,"href","#transformers.TFDPRContextEncoder"),p(kt,"class","relative group"),p($r,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.TFPreTrainedModel"),p(Tn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(Tn,"rel","nofollow"),p(Pr,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.TFDPRContextEncoder"),p(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(eo,"id","transformers.TFDPRQuestionEncoder"),p(eo,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(eo,"href","#transformers.TFDPRQuestionEncoder"),p(Pt,"class","relative group"),p(wr,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.TFPreTrainedModel"),p(Rn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(Rn,"rel","nofollow"),p(Er,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),p(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(ro,"id","transformers.TFDPRReader"),p(ro,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),p(ro,"href","#transformers.TFDPRReader"),p(Et,"class","relative group"),p(Rr,"href","/docs/transformers/v4.18.0/en/main_classes/model#transformers.TFPreTrainedModel"),p(Fn,"href","https://www.tensorflow.org/api_docs/python/tf/keras/Model"),p(Fn,"rel","nofollow"),p(Dr,"href","/docs/transformers/v4.18.0/en/model_doc/dpr#transformers.TFDPRReader"),p(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),p(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,m){e(document.head,h),f(o,T,m),f(o,v,m),e(v,u),e(u,b),P(d,b,null),e(v,g),e(v,x),e(x,_e),f(o,Z,m),f(o,D,m),e(D,J),e(J,I),P(ee,I,null),e(D,ve),e(D,S),e(S,be),f(o,he,m),f(o,H,m),e(H,L),e(H,te),e(te,oe),e(H,q),f(o,A,m),f(o,ne,m),e(ne,V),f(o,fe,m),f(o,re,m),e(re,B),e(B,Te),f(o,ue,m),f(o,z,m),e(z,ke),e(z,j),e(j,$e),e(z,Pe),e(z,M),e(M,we),e(z,Ee),f(o,Q,m),f(o,K,m),e(K,N),e(N,se),P(_,se,null),e(K,y),e(K,G),e(G,Oe),f(o,ze,m),f(o,C,m),P(me,C,null),e(C,Ne),e(C,ae),e(ae,O),e(O,Y),e(ae,je),e(ae,xe),e(xe,X),e(ae,Me),e(C,Qe),e(C,W),e(W,Le),e(W,Hn),e(Hn,pi),e(W,hi),e(W,Bn),e(Bn,fi),e(W,ui),e(W,Wn),e(Wn,mi),e(W,gi),e(C,_i),e(C,$o),e($o,vi),e($o,Un),e(Un,bi),e($o,Ti),f(o,Ks,m),f(o,st,m),e(st,xt),e(xt,Ar),P(Po,Ar,null),e(st,ki),e(st,Or),e(Or,$i),f(o,Ys,m),f(o,qe,m),P(wo,qe,null),e(qe,Pi),e(qe,Nr),e(Nr,wi),e(qe,Ei),e(qe,zt),e(zt,Vn),e(Vn,Ri),e(zt,Di),e(zt,Kn),e(Kn,yi),e(zt,xi),e(qe,zi),e(qe,Eo),e(Eo,qi),e(Eo,Yn),e(Yn,Fi),e(Eo,Ci),f(o,Xs,m),f(o,at,m),e(at,qt),e(qt,jr),P(Ro,jr,null),e(at,Ai),e(at,Mr),e(Mr,Oi),f(o,Js,m),f(o,Fe,m),P(Do,Fe,null),e(Fe,Ni),e(Fe,yo),e(yo,ji),e(yo,Qr),e(Qr,Mi),e(yo,Qi),e(Fe,Li),e(Fe,Ft),e(Ft,Xn),e(Xn,Ii),e(Ft,Si),e(Ft,Jn),e(Jn,Hi),e(Ft,Bi),e(Fe,Wi),e(Fe,xo),e(xo,Ui),e(xo,Gn),e(Gn,Vi),e(xo,Ki),f(o,Gs,m),f(o,it,m),e(it,Ct),e(Ct,Lr),P(zo,Lr,null),e(it,Yi),e(it,Ir),e(Ir,Xi),f(o,Zs,m),f(o,Ce,m),P(qo,Ce,null),e(Ce,Ji),e(Ce,Sr),e(Sr,Gi),e(Ce,Zi),e(Ce,At),e(At,Zn),e(Zn,ed),e(At,td),e(At,er),e(er,od),e(At,nd),e(Ce,rd),e(Ce,Fo),e(Fo,sd),e(Fo,tr),e(tr,ad),e(Fo,id),f(o,ea,m),f(o,dt,m),e(dt,Ot),e(Ot,Hr),P(Co,Hr,null),e(dt,dd),e(dt,Br),e(Br,ld),f(o,ta,m),f(o,Ae,m),P(Ao,Ae,null),e(Ae,cd),e(Ae,Oo),e(Oo,pd),e(Oo,Wr),e(Wr,hd),e(Oo,fd),e(Ae,ud),e(Ae,Nt),e(Nt,or),e(or,md),e(Nt,gd),e(Nt,nr),e(nr,_d),e(Nt,vd),e(Ae,bd),e(Ae,No),e(No,Td),e(No,rr),e(rr,kd),e(No,$d),f(o,oa,m),f(o,lt,m),e(lt,jt),e(jt,Ur),P(jo,Ur,null),e(lt,Pd),e(lt,Vr),e(Vr,wd),f(o,na,m),f(o,ie,m),P(Mo,ie,null),e(ie,Ed),e(ie,Kr),e(Kr,Rd),e(ie,Dd),e(ie,Ze),e(Ze,sr),e(sr,yd),e(Ze,xd),e(Ze,ar),e(ar,zd),e(Ze,qd),e(Ze,ir),e(ir,Fd),e(Ze,Cd),e(ie,Ad),e(ie,Qo),e(Qo,Od),e(Qo,dr),e(dr,Nd),e(Qo,jd),e(ie,Md),e(ie,et),e(et,Qd),e(et,Yr),e(Yr,Ld),e(et,Id),e(et,Xr),e(Xr,Sd),e(et,Hd),e(et,Jr),e(Jr,Bd),e(ie,Wd),P(Mt,ie,null),f(o,ra,m),f(o,ct,m),e(ct,Qt),e(Qt,Gr),P(Lo,Gr,null),e(ct,Ud),e(ct,Zr),e(Zr,Vd),f(o,sa,m),f(o,de,m),P(Io,de,null),e(de,Kd),e(de,So),e(So,Yd),e(So,es),e(es,Xd),e(So,Jd),e(de,Gd),e(de,tt),e(tt,lr),e(lr,Zd),e(tt,el),e(tt,cr),e(cr,tl),e(tt,ol),e(tt,pr),e(pr,nl),e(tt,rl),e(de,sl),e(de,Ho),e(Ho,al),e(Ho,hr),e(hr,il),e(Ho,dl),e(de,ll),e(de,Ge),e(Ge,cl),e(Ge,ts),e(ts,pl),e(Ge,hl),e(Ge,os),e(os,fl),e(Ge,ul),e(Ge,ns),e(ns,ml),e(Ge,gl),e(de,_l),e(de,rs),e(rs,vl),f(o,aa,m),f(o,pt,m),e(pt,Lt),e(Lt,ss),P(Bo,ss,null),e(pt,bl),e(pt,as),e(as,Tl),f(o,ia,m),f(o,ht,m),P(Wo,ht,null),e(ht,kl),e(ht,Uo),e(Uo,$l),e(Uo,fr),e(fr,Pl),e(Uo,wl),f(o,da,m),f(o,ft,m),P(Vo,ft,null),e(ft,El),e(ft,Ko),e(Ko,Rl),e(Ko,ur),e(ur,Dl),e(Ko,yl),f(o,la,m),f(o,ut,m),P(Yo,ut,null),e(ut,xl),e(ut,Xo),e(Xo,zl),e(Xo,mr),e(mr,ql),e(Xo,Fl),f(o,ca,m),f(o,mt,m),e(mt,It),e(It,is),P(Jo,is,null),e(mt,Cl),e(mt,ds),e(ds,Al),f(o,pa,m),f(o,Re,m),P(Go,Re,null),e(Re,Ol),e(Re,ls),e(ls,Nl),e(Re,jl),e(Re,Zo),e(Zo,Ml),e(Zo,gr),e(gr,Ql),e(Zo,Ll),e(Re,Il),e(Re,en),e(en,Sl),e(en,tn),e(tn,Hl),e(en,Bl),e(Re,Wl),e(Re,Ie),P(on,Ie,null),e(Ie,Ul),e(Ie,gt),e(gt,Vl),e(gt,_r),e(_r,Kl),e(gt,Yl),e(gt,cs),e(cs,Xl),e(gt,Jl),e(Ie,Gl),P(St,Ie,null),e(Ie,Zl),P(Ht,Ie,null),f(o,ha,m),f(o,_t,m),e(_t,Bt),e(Bt,ps),P(nn,ps,null),e(_t,ec),e(_t,hs),e(hs,tc),f(o,fa,m),f(o,De,m),P(rn,De,null),e(De,oc),e(De,fs),e(fs,nc),e(De,rc),e(De,sn),e(sn,sc),e(sn,vr),e(vr,ac),e(sn,ic),e(De,dc),e(De,an),e(an,lc),e(an,dn),e(dn,cc),e(an,pc),e(De,hc),e(De,Se),P(ln,Se,null),e(Se,fc),e(Se,vt),e(vt,uc),e(vt,br),e(br,mc),e(vt,gc),e(vt,us),e(us,_c),e(vt,vc),e(Se,bc),P(Wt,Se,null),e(Se,Tc),P(Ut,Se,null),f(o,ua,m),f(o,bt,m),e(bt,Vt),e(Vt,ms),P(cn,ms,null),e(bt,kc),e(bt,gs),e(gs,$c),f(o,ma,m),f(o,ye,m),P(pn,ye,null),e(ye,Pc),e(ye,_s),e(_s,wc),e(ye,Ec),e(ye,hn),e(hn,Rc),e(hn,Tr),e(Tr,Dc),e(hn,yc),e(ye,xc),e(ye,fn),e(fn,zc),e(fn,un),e(un,qc),e(fn,Fc),e(ye,Cc),e(ye,He),P(mn,He,null),e(He,Ac),e(He,Tt),e(Tt,Oc),e(Tt,kr),e(kr,Nc),e(Tt,jc),e(Tt,vs),e(vs,Mc),e(Tt,Qc),e(He,Lc),P(Kt,He,null),e(He,Ic),P(Yt,He,null),f(o,ga,m),f(o,kt,m),e(kt,Xt),e(Xt,bs),P(gn,bs,null),e(kt,Sc),e(kt,Ts),e(Ts,Hc),f(o,_a,m),f(o,le,m),P(_n,le,null),e(le,Bc),e(le,ks),e(ks,Wc),e(le,Uc),e(le,vn),e(vn,Vc),e(vn,$r),e($r,Kc),e(vn,Yc),e(le,Xc),e(le,bn),e(bn,Jc),e(bn,Tn),e(Tn,Gc),e(bn,Zc),e(le,ep),P(Jt,le,null),e(le,tp),e(le,Be),P(kn,Be,null),e(Be,op),e(Be,$t),e($t,np),e($t,Pr),e(Pr,rp),e($t,sp),e($t,$s),e($s,ap),e($t,ip),e(Be,dp),P(Gt,Be,null),e(Be,lp),P(Zt,Be,null),f(o,va,m),f(o,Pt,m),e(Pt,eo),e(eo,Ps),P($n,Ps,null),e(Pt,cp),e(Pt,ws),e(ws,pp),f(o,ba,m),f(o,ce,m),P(Pn,ce,null),e(ce,hp),e(ce,Es),e(Es,fp),e(ce,up),e(ce,wn),e(wn,mp),e(wn,wr),e(wr,gp),e(wn,_p),e(ce,vp),e(ce,En),e(En,bp),e(En,Rn),e(Rn,Tp),e(En,kp),e(ce,$p),P(to,ce,null),e(ce,Pp),e(ce,We),P(Dn,We,null),e(We,wp),e(We,wt),e(wt,Ep),e(wt,Er),e(Er,Rp),e(wt,Dp),e(wt,Rs),e(Rs,yp),e(wt,xp),e(We,zp),P(oo,We,null),e(We,qp),P(no,We,null),f(o,Ta,m),f(o,Et,m),e(Et,ro),e(ro,Ds),P(yn,Ds,null),e(Et,Fp),e(Et,ys),e(ys,Cp),f(o,ka,m),f(o,pe,m),P(xn,pe,null),e(pe,Ap),e(pe,xs),e(xs,Op),e(pe,Np),e(pe,zn),e(zn,jp),e(zn,Rr),e(Rr,Mp),e(zn,Qp),e(pe,Lp),e(pe,qn),e(qn,Ip),e(qn,Fn),e(Fn,Sp),e(qn,Hp),e(pe,Bp),P(so,pe,null),e(pe,Wp),e(pe,Ue),P(Cn,Ue,null),e(Ue,Up),e(Ue,Rt),e(Rt,Vp),e(Rt,Dr),e(Dr,Kp),e(Rt,Yp),e(Rt,zs),e(zs,Xp),e(Rt,Jp),e(Ue,Gp),P(ao,Ue,null),e(Ue,Zp),P(io,Ue,null),$a=!0},p(o,[m]){const An={};m&2&&(An.$$scope={dirty:m,ctx:o}),Mt.$set(An);const qs={};m&2&&(qs.$$scope={dirty:m,ctx:o}),St.$set(qs);const Fs={};m&2&&(Fs.$$scope={dirty:m,ctx:o}),Ht.$set(Fs);const Cs={};m&2&&(Cs.$$scope={dirty:m,ctx:o}),Wt.$set(Cs);const On={};m&2&&(On.$$scope={dirty:m,ctx:o}),Ut.$set(On);const As={};m&2&&(As.$$scope={dirty:m,ctx:o}),Kt.$set(As);const Os={};m&2&&(Os.$$scope={dirty:m,ctx:o}),Yt.$set(Os);const Ns={};m&2&&(Ns.$$scope={dirty:m,ctx:o}),Jt.$set(Ns);const Nn={};m&2&&(Nn.$$scope={dirty:m,ctx:o}),Gt.$set(Nn);const js={};m&2&&(js.$$scope={dirty:m,ctx:o}),Zt.$set(js);const Ms={};m&2&&(Ms.$$scope={dirty:m,ctx:o}),to.$set(Ms);const Qs={};m&2&&(Qs.$$scope={dirty:m,ctx:o}),oo.$set(Qs);const Ls={};m&2&&(Ls.$$scope={dirty:m,ctx:o}),no.$set(Ls);const Dt={};m&2&&(Dt.$$scope={dirty:m,ctx:o}),so.$set(Dt);const Is={};m&2&&(Is.$$scope={dirty:m,ctx:o}),ao.$set(Is);const Ss={};m&2&&(Ss.$$scope={dirty:m,ctx:o}),io.$set(Ss)},i(o){$a||(w(d.$$.fragment,o),w(ee.$$.fragment,o),w(_.$$.fragment,o),w(me.$$.fragment,o),w(Po.$$.fragment,o),w(wo.$$.fragment,o),w(Ro.$$.fragment,o),w(Do.$$.fragment,o),w(zo.$$.fragment,o),w(qo.$$.fragment,o),w(Co.$$.fragment,o),w(Ao.$$.fragment,o),w(jo.$$.fragment,o),w(Mo.$$.fragment,o),w(Mt.$$.fragment,o),w(Lo.$$.fragment,o),w(Io.$$.fragment,o),w(Bo.$$.fragment,o),w(Wo.$$.fragment,o),w(Vo.$$.fragment,o),w(Yo.$$.fragment,o),w(Jo.$$.fragment,o),w(Go.$$.fragment,o),w(on.$$.fragment,o),w(St.$$.fragment,o),w(Ht.$$.fragment,o),w(nn.$$.fragment,o),w(rn.$$.fragment,o),w(ln.$$.fragment,o),w(Wt.$$.fragment,o),w(Ut.$$.fragment,o),w(cn.$$.fragment,o),w(pn.$$.fragment,o),w(mn.$$.fragment,o),w(Kt.$$.fragment,o),w(Yt.$$.fragment,o),w(gn.$$.fragment,o),w(_n.$$.fragment,o),w(Jt.$$.fragment,o),w(kn.$$.fragment,o),w(Gt.$$.fragment,o),w(Zt.$$.fragment,o),w($n.$$.fragment,o),w(Pn.$$.fragment,o),w(to.$$.fragment,o),w(Dn.$$.fragment,o),w(oo.$$.fragment,o),w(no.$$.fragment,o),w(yn.$$.fragment,o),w(xn.$$.fragment,o),w(so.$$.fragment,o),w(Cn.$$.fragment,o),w(ao.$$.fragment,o),w(io.$$.fragment,o),$a=!0)},o(o){E(d.$$.fragment,o),E(ee.$$.fragment,o),E(_.$$.fragment,o),E(me.$$.fragment,o),E(Po.$$.fragment,o),E(wo.$$.fragment,o),E(Ro.$$.fragment,o),E(Do.$$.fragment,o),E(zo.$$.fragment,o),E(qo.$$.fragment,o),E(Co.$$.fragment,o),E(Ao.$$.fragment,o),E(jo.$$.fragment,o),E(Mo.$$.fragment,o),E(Mt.$$.fragment,o),E(Lo.$$.fragment,o),E(Io.$$.fragment,o),E(Bo.$$.fragment,o),E(Wo.$$.fragment,o),E(Vo.$$.fragment,o),E(Yo.$$.fragment,o),E(Jo.$$.fragment,o),E(Go.$$.fragment,o),E(on.$$.fragment,o),E(St.$$.fragment,o),E(Ht.$$.fragment,o),E(nn.$$.fragment,o),E(rn.$$.fragment,o),E(ln.$$.fragment,o),E(Wt.$$.fragment,o),E(Ut.$$.fragment,o),E(cn.$$.fragment,o),E(pn.$$.fragment,o),E(mn.$$.fragment,o),E(Kt.$$.fragment,o),E(Yt.$$.fragment,o),E(gn.$$.fragment,o),E(_n.$$.fragment,o),E(Jt.$$.fragment,o),E(kn.$$.fragment,o),E(Gt.$$.fragment,o),E(Zt.$$.fragment,o),E($n.$$.fragment,o),E(Pn.$$.fragment,o),E(to.$$.fragment,o),E(Dn.$$.fragment,o),E(oo.$$.fragment,o),E(no.$$.fragment,o),E(yn.$$.fragment,o),E(xn.$$.fragment,o),E(so.$$.fragment,o),E(Cn.$$.fragment,o),E(ao.$$.fragment,o),E(io.$$.fragment,o),$a=!1},d(o){t(h),o&&t(T),o&&t(v),R(d),o&&t(Z),o&&t(D),R(ee),o&&t(he),o&&t(H),o&&t(A),o&&t(ne),o&&t(fe),o&&t(re),o&&t(ue),o&&t(z),o&&t(Q),o&&t(K),R(_),o&&t(ze),o&&t(C),R(me),o&&t(Ks),o&&t(st),R(Po),o&&t(Ys),o&&t(qe),R(wo),o&&t(Xs),o&&t(at),R(Ro),o&&t(Js),o&&t(Fe),R(Do),o&&t(Gs),o&&t(it),R(zo),o&&t(Zs),o&&t(Ce),R(qo),o&&t(ea),o&&t(dt),R(Co),o&&t(ta),o&&t(Ae),R(Ao),o&&t(oa),o&&t(lt),R(jo),o&&t(na),o&&t(ie),R(Mo),R(Mt),o&&t(ra),o&&t(ct),R(Lo),o&&t(sa),o&&t(de),R(Io),o&&t(aa),o&&t(pt),R(Bo),o&&t(ia),o&&t(ht),R(Wo),o&&t(da),o&&t(ft),R(Vo),o&&t(la),o&&t(ut),R(Yo),o&&t(ca),o&&t(mt),R(Jo),o&&t(pa),o&&t(Re),R(Go),R(on),R(St),R(Ht),o&&t(ha),o&&t(_t),R(nn),o&&t(fa),o&&t(De),R(rn),R(ln),R(Wt),R(Ut),o&&t(ua),o&&t(bt),R(cn),o&&t(ma),o&&t(ye),R(pn),R(mn),R(Kt),R(Yt),o&&t(ga),o&&t(kt),R(gn),o&&t(_a),o&&t(le),R(_n),R(Jt),R(kn),R(Gt),R(Zt),o&&t(va),o&&t(Pt),R($n),o&&t(ba),o&&t(ce),R(Pn),R(to),R(Dn),R(oo),R(no),o&&t(Ta),o&&t(Et),R(yn),o&&t(ka),o&&t(pe),R(xn),R(so),R(Cn),R(ao),R(io)}}}const xu={local:"dpr",sections:[{local:"overview",title:"Overview"},{local:"transformers.DPRConfig",title:"DPRConfig"},{local:"transformers.DPRContextEncoderTokenizer",title:"DPRContextEncoderTokenizer"},{local:"transformers.DPRContextEncoderTokenizerFast",title:"DPRContextEncoderTokenizerFast"},{local:"transformers.DPRQuestionEncoderTokenizer",title:"DPRQuestionEncoderTokenizer"},{local:"transformers.DPRQuestionEncoderTokenizerFast",title:"DPRQuestionEncoderTokenizerFast"},{local:"transformers.DPRReaderTokenizer",title:"DPRReaderTokenizer"},{local:"transformers.DPRReaderTokenizerFast",title:"DPRReaderTokenizerFast"},{local:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",title:"DPR specific outputs"},{local:"transformers.DPRContextEncoder",title:"DPRContextEncoder"},{local:"transformers.DPRQuestionEncoder",title:"DPRQuestionEncoder"},{local:"transformers.DPRReader",title:"DPRReader"},{local:"transformers.TFDPRContextEncoder",title:"TFDPRContextEncoder"},{local:"transformers.TFDPRQuestionEncoder",title:"TFDPRQuestionEncoder"},{local:"transformers.TFDPRReader",title:"TFDPRReader"}],title:"DPR"};function zu(F){return pu(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ju extends iu{constructor(h){super();du(this,h,zu,yu,lu,{})}}export{ju as default,xu as metadata};
