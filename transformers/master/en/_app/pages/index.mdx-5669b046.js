import{S as ee,i as se,s as ae,M as xt,t as n,N as Wt,a as p,h as i,d as e,b as g,g as c,F as s,L as Yt,e as m,w as S,k as _,c as u,x as D,m as h,y as P,O as ge,q as k,o as y,B as T,n as oe,p as le,P as ke,v as ve,Q as me,H as re,I as ne,J as ie,K as pe,R as ye}from"../chunks/vendor-9daddcfa.js";import{I as Ut}from"../chunks/IconCopyLink-a413fd1b.js";import{C as at}from"../chunks/CodeBlock-37b92346.js";import{g as je,I as ze,a as qe}from"../chunks/IconTensorflow-a035d33c.js";import"../chunks/CopyButton-6099fd9d.js";function Ee(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft;return{c(){a=xt("svg"),f=xt("style"),o=n(`.J {
			stroke: #dce0df;
		}
		.K {
			stroke-linejoin: round;
		}
	`),r=xt("g"),$=xt("path"),v=xt("path"),L=xt("path"),q=xt("path"),z=xt("path"),X=xt("path"),F=xt("path"),d=xt("path"),Z=xt("g"),N=xt("path"),U=xt("path"),it=xt("path"),K=xt("g"),pt=xt("path"),Y=xt("path"),O=xt("path"),lt=xt("g"),rt=xt("path"),st=xt("path"),b=xt("g"),B=xt("path"),w=xt("path"),dt=xt("path"),G=xt("path"),_t=xt("path"),jt=xt("path"),tt=xt("path"),qt=xt("path"),yt=xt("g"),et=xt("path"),Et=xt("path"),Ct=xt("path"),ft=xt("path"),W=xt("g"),ct=xt("path"),V=xt("path"),Ft=xt("path"),this.h()},l(nt){a=Wt(nt,"svg",{class:!0,xmlns:!0,"xmlns:xlink":!0,"aria-hidden":!0,focusable:!0,role:!0,width:!0,height:!0,preserveAspectRatio:!0,viewBox:!0});var C=p(a);f=Wt(C,"style",{});var ut=p(f);o=i(ut,`.J {
			stroke: #dce0df;
		}
		.K {
			stroke-linejoin: round;
		}
	`),ut.forEach(e),r=Wt(C,"g",{fill:!0,class:!0});var E=p(r);$=Wt(E,"path",{d:!0}),p($).forEach(e),v=Wt(E,"path",{d:!0}),p(v).forEach(e),L=Wt(E,"path",{d:!0}),p(L).forEach(e),q=Wt(E,"path",{d:!0}),p(q).forEach(e),z=Wt(E,"path",{d:!0}),p(z).forEach(e),X=Wt(E,"path",{d:!0}),p(X).forEach(e),F=Wt(E,"path",{d:!0}),p(F).forEach(e),d=Wt(E,"path",{d:!0}),p(d).forEach(e),E.forEach(e),Z=Wt(C,"g",{fill:!0,class:!0});var St=p(Z);N=Wt(St,"path",{d:!0}),p(N).forEach(e),U=Wt(St,"path",{d:!0}),p(U).forEach(e),it=Wt(St,"path",{d:!0}),p(it).forEach(e),St.forEach(e),K=Wt(C,"g",{fill:!0,class:!0});var R=p(K);pt=Wt(R,"path",{d:!0}),p(pt).forEach(e),Y=Wt(R,"path",{d:!0}),p(Y).forEach(e),R.forEach(e),O=Wt(C,"path",{d:!0,fill:!0,class:!0}),p(O).forEach(e),lt=Wt(C,"g",{fill:!0,class:!0});var Pt=p(lt);rt=Wt(Pt,"path",{d:!0}),p(rt).forEach(e),st=Wt(Pt,"path",{d:!0}),p(st).forEach(e),Pt.forEach(e),b=Wt(C,"g",{fill:!0,class:!0});var zt=p(b);B=Wt(zt,"path",{d:!0}),p(B).forEach(e),w=Wt(zt,"path",{d:!0}),p(w).forEach(e),dt=Wt(zt,"path",{d:!0}),p(dt).forEach(e),G=Wt(zt,"path",{d:!0}),p(G).forEach(e),_t=Wt(zt,"path",{d:!0}),p(_t).forEach(e),jt=Wt(zt,"path",{d:!0}),p(jt).forEach(e),tt=Wt(zt,"path",{d:!0}),p(tt).forEach(e),zt.forEach(e),qt=Wt(C,"path",{d:!0,fill:!0,class:!0}),p(qt).forEach(e),yt=Wt(C,"g",{fill:!0,class:!0});var mt=p(yt);et=Wt(mt,"path",{d:!0}),p(et).forEach(e),Et=Wt(mt,"path",{d:!0}),p(Et).forEach(e),Ct=Wt(mt,"path",{d:!0}),p(Ct).forEach(e),ft=Wt(mt,"path",{d:!0}),p(ft).forEach(e),mt.forEach(e),W=Wt(C,"g",{fill:!0,class:!0});var H=p(W);ct=Wt(H,"path",{d:!0}),p(ct).forEach(e),V=Wt(H,"path",{d:!0}),p(V).forEach(e),Ft=Wt(H,"path",{d:!0}),p(Ft).forEach(e),H.forEach(e),C.forEach(e),this.h()},h(){g($,"d","M50.5 130.4l-25 43.31h50l25-43.31h-50z"),g(v,"d","M.5 217.01l25-43.3h50l-25 43.3H.5z"),g(L,"d","M125.5 173.71h-50l-25 43.3h50l25-43.3z"),g(q,"d","M175.5 173.71h-50l-25 43.3h50l25-43.3z"),g(z,"d","M150.5 130.4l-25 43.31h50l25-43.31h-50z"),g(X,"d","M175.5 87.1l-25 43.3h50l25-43.3h-50z"),g(F,"d","M200.5 43.8l-25 43.3h50l25-43.3h-50z"),g(d,"d","M225.5.5l-25 43.3h50l25-43.3h-50z"),g(r,"fill","#5e97f6"),g(r,"class","J K"),g(N,"d","M.5 217.01l25 43.3h50l-25-43.3H.5z"),g(U,"d","M125.5 260.31h-50l-25-43.3h50l25 43.3z"),g(it,"d","M175.5 260.31h-50l-25-43.3h50l25 43.3z"),g(Z,"fill","#2a56c6"),g(Z,"class","J K"),g(pt,"d","M200.5 217.01l-25-43.3-25 43.3 25 43.3 25-43.3zm50-86.61l-25-43.3-25 43.3h50z"),g(Y,"d","M250.5 43.8l-25 43.3 25 43.3 25-43.3-25-43.3z"),g(K,"fill","#00796b"),g(K,"class","J K"),g(O,"d","M125.5 173.71l-25-43.31-25 43.31h50z"),g(O,"fill","#3367d6"),g(O,"class","J K"),g(rt,"d","M250.5 130.4h-50l-25 43.31h50l25-43.31z"),g(st,"d","M300.5 130.4h-50l-25 43.31h50l25-43.31z"),g(lt,"fill","#26a69a"),g(lt,"class","J K"),g(B,"d","M350.5 43.8L325.5.5l-25 43.3 25 43.3 25-43.3z"),g(w,"d","M375.5 87.1l-25-43.3-25 43.3 25 43.3 25-43.3z"),g(dt,"d","M400.5 130.4l-25-43.3-25 43.3 25 43.31 25-43.31z"),g(G,"d","M425.5 173.71l-25-43.31-25 43.31 25 43.3 25-43.3z"),g(_t,"d","M450.5 217.01l-25-43.3-25 43.3 25 43.3 25-43.3zM425.5.5l-25 43.3 25 43.3 25-43.3-25-43.3z"),g(jt,"d","M375.5 87.1l25-43.3 25 43.3-25 43.3-25-43.3zm-25 43.3l-25 43.31 25 43.3 25-43.3-25-43.31z"),g(tt,"d","M325.5 260.31l-25-43.3 25-43.3 25 43.3-25 43.3z"),g(b,"fill","#9c27b0"),g(b,"class","J K"),g(qt,"d","M275.5 260.31l-25-43.3h50l25 43.3h-50z"),g(qt,"fill","#6a1b9a"),g(qt,"class","J K"),g(et,"d","M225.5 173.71h-50l25 43.3h50l-25-43.3z"),g(Et,"d","M275.5 173.71h-50l25 43.3 25-43.3zm0-86.61l25 43.3h50l-25-43.3h-50z"),g(Ct,"d","M300.5 43.8h-50l25 43.3h50l-25-43.3zm125 216.51l-25-43.3h-50l25 43.3h50z"),g(ft,"d","M375.5 173.71l-25 43.3h50l-25-43.3z"),g(yt,"fill","#00695c"),g(yt,"class","J K"),g(ct,"d","M325.5.5h-50l-25 43.3h50l25-43.3zm0 173.21h-50l-25 43.3h50l25-43.3z"),g(V,"d","M350.5 130.4h-50l-25 43.31h50l25-43.31zM425.5.5h-50l-25 43.3h50l25-43.3z"),g(Ft,"d","M375.5 87.1l-25-43.3h50l-25 43.3z"),g(W,"fill","#ea80fc"),g(W,"class","J K"),g(a,"class",j[0]),g(a,"xmlns","http://www.w3.org/2000/svg"),g(a,"xmlns:xlink","http://www.w3.org/1999/xlink"),g(a,"aria-hidden","true"),g(a,"focusable","false"),g(a,"role","img"),g(a,"width","1.73em"),g(a,"height","1em"),g(a,"preserveAspectRatio","xMidYMid meet"),g(a,"viewBox","0 0 451 260.81")},m(nt,C){c(nt,a,C),s(a,f),s(f,o),s(a,r),s(r,$),s(r,v),s(r,L),s(r,q),s(r,z),s(r,X),s(r,F),s(r,d),s(a,Z),s(Z,N),s(Z,U),s(Z,it),s(a,K),s(K,pt),s(K,Y),s(a,O),s(a,lt),s(lt,rt),s(lt,st),s(a,b),s(b,B),s(b,w),s(b,dt),s(b,G),s(b,_t),s(b,jt),s(b,tt),s(a,qt),s(a,yt),s(yt,et),s(yt,Et),s(yt,Ct),s(yt,ft),s(a,W),s(W,ct),s(W,V),s(W,Ft)},p(nt,[C]){C&1&&g(a,"class",nt[0])},i:Yt,o:Yt,d(nt){nt&&e(a)}}}function Ce(j,a,f){let{classNames:o=""}=a;return j.$$set=r=>{"classNames"in r&&f(0,o=r.classNames)},[o]}class Fe extends ee{constructor(a){super();se(this,a,Ce,Ee,ae,{classNames:0})}}function Se(j){let a,f;return{c(){a=xt("svg"),f=xt("path"),this.h()},l(o){a=Wt(o,"svg",{class:!0,width:!0,height:!0,viewBox:!0,fill:!0,xmlns:!0});var r=p(a);f=Wt(r,"path",{d:!0,fill:!0}),p(f).forEach(e),r.forEach(e),this.h()},h(){g(f,"d","M0 4.50001C0.390979 2.37042 2.25728 0.756592 4.5 0.756592C6.74272 0.756592 8.60861 2.37042 9 4.50001C8.60902 6.62959 6.74272 8.24342 4.5 8.24342C2.25728 8.24342 0.391395 6.62959 0 4.50001ZM4.5 6.57968C5.05156 6.57968 5.58054 6.36057 5.97055 5.97056C6.36057 5.58054 6.57967 5.05157 6.57967 4.50001C6.57967 3.94844 6.36057 3.41947 5.97055 3.02945C5.58054 2.63944 5.05156 2.42033 4.5 2.42033C3.94844 2.42033 3.41946 2.63944 3.02945 3.02945C2.63943 3.41947 2.42033 3.94844 2.42033 4.50001C2.42033 5.05157 2.63943 5.58054 3.02945 5.97056C3.41946 6.36057 3.94844 6.57968 4.5 6.57968ZM4.5 5.74781C4.16906 5.74781 3.85168 5.61635 3.61767 5.38234C3.38366 5.14833 3.2522 4.83094 3.2522 4.50001C3.2522 4.16907 3.38366 3.85168 3.61767 3.61767C3.85168 3.38367 4.16906 3.2522 4.5 3.2522C4.83094 3.2522 5.14832 3.38367 5.38233 3.61767C5.61634 3.85168 5.7478 4.16907 5.7478 4.50001C5.7478 4.83094 5.61634 5.14833 5.38233 5.38234C5.14832 5.61635 4.83094 5.74781 4.5 5.74781Z"),g(f,"fill","currentColor"),g(a,"class",j[0]),g(a,"width","1em"),g(a,"height","1em"),g(a,"viewBox","0 0 9 9"),g(a,"fill","currentColor"),g(a,"xmlns","http://www.w3.org/2000/svg")},m(o,r){c(o,a,r),s(a,f)},p(o,[r]){r&1&&g(a,"class",o[0])},i:Yt,o:Yt,d(o){o&&e(a)}}}function Pe(j,a,f){let{classNames:o=""}=a;return j.$$set=r=>{"classNames"in r&&f(0,o=r.classNames)},[o]}class Te extends ee{constructor(a){super();se(this,a,Pe,Se,ae,{classNames:0})}}function De(j){let a,f;return{c(){a=xt("svg"),f=xt("path"),this.h()},l(o){a=Wt(o,"svg",{class:!0,width:!0,height:!0,viewBox:!0,fill:!0,xmlns:!0});var r=p(a);f=Wt(r,"path",{d:!0,fill:!0}),p(f).forEach(e),r.forEach(e),this.h()},h(){g(f,"d","M1.39125 1.9725L0.0883333 0.669997L0.677917 0.0804138L8.9275 8.33041L8.33792 8.91958L6.95875 7.54041C6.22592 8.00523 5.37572 8.25138 4.50792 8.25C2.26125 8.25 0.392083 6.63333 0 4.5C0.179179 3.52946 0.667345 2.64287 1.39167 1.9725H1.39125ZM5.65667 6.23833L5.04667 5.62833C4.81335 5.73996 4.55116 5.77647 4.29622 5.73282C4.04129 5.68918 3.80617 5.56752 3.62328 5.38463C3.44039 5.20175 3.31874 4.96663 3.27509 4.71169C3.23144 4.45676 3.26795 4.19456 3.37958 3.96125L2.76958 3.35125C2.50447 3.75187 2.38595 4.2318 2.4341 4.70978C2.48225 5.18777 2.6941 5.63442 3.0338 5.97411C3.37349 6.31381 3.82015 6.52567 4.29813 6.57382C4.77611 6.62197 5.25605 6.50345 5.65667 6.23833ZM2.83042 1.06666C3.35 0.862497 3.91625 0.749997 4.50792 0.749997C6.75458 0.749997 8.62375 2.36666 9.01583 4.5C8.88816 5.19404 8.60119 5.84899 8.1775 6.41333L6.56917 4.805C6.61694 4.48317 6.58868 4.15463 6.48664 3.84569C6.3846 3.53675 6.21162 3.256 5.98156 3.02594C5.7515 2.79588 5.47075 2.6229 5.16181 2.52086C4.85287 2.41882 4.52433 2.39056 4.2025 2.43833L2.83042 1.06708V1.06666Z"),g(f,"fill","currentColor"),g(a,"class",j[0]),g(a,"width","1em"),g(a,"height","1em"),g(a,"viewBox","0 0 10 9"),g(a,"fill","currentColor"),g(a,"xmlns","http://www.w3.org/2000/svg")},m(o,r){c(o,a,r),s(a,f)},p(o,[r]){r&1&&g(a,"class",o[0])},i:Yt,o:Yt,d(o){o&&e(a)}}}function Ae(j,a,f){let{classNames:o=""}=a;return j.$$set=r=>{"classNames"in r&&f(0,o=r.classNames)},[o]}class Me extends ee{constructor(a){super();se(this,a,Ae,De,ae,{classNames:0})}}function ue(j){let a,f,o,r,$,v,L,q,z,X;return f=new Me({}),{c(){a=m("div"),S(f.$$.fragment),o=_(),r=m("span"),$=n("Hide "),v=n(j[2]),L=n(" content"),this.h()},l(F){a=u(F,"DIV",{class:!0});var d=p(a);D(f.$$.fragment,d),o=h(d),r=u(d,"SPAN",{});var Z=p(r);$=i(Z,"Hide "),v=i(Z,j[2]),L=i(Z," content"),Z.forEach(e),d.forEach(e),this.h()},h(){g(a,"class","cursor-pointer flex items-center justify-center space-x-1 py-2.5")},m(F,d){c(F,a,d),P(f,a,null),s(a,o),s(a,r),s(r,$),s(r,v),s(r,L),q=!0,z||(X=ge(a,"click",j[4]),z=!0)},p:Yt,i(F){q||(k(f.$$.fragment,F),q=!0)},o(F){y(f.$$.fragment,F),q=!1},d(F){F&&e(a),T(f),z=!1,X()}}}function xe(j){let a;const f=j[7].default,o=re(f,j,j[6],null);return{c(){o&&o.c()},l(r){o&&o.l(r)},m(r,$){o&&o.m(r,$),a=!0},p(r,$){o&&o.p&&(!a||$&64)&&ne(o,f,r,r[6],a?pe(f,r[6],$,null):ie(r[6]),null)},i(r){a||(k(o,r),a=!0)},o(r){y(o,r),a=!1},d(r){o&&o.d(r)}}}function We(j){let a,f,o,r,$,v,L,q,z,X;return f=new Te({}),{c(){a=m("div"),S(f.$$.fragment),o=_(),r=m("span"),$=n("Show "),v=n(j[2]),L=n(" content"),this.h()},l(F){a=u(F,"DIV",{class:!0});var d=p(a);D(f.$$.fragment,d),o=h(d),r=u(d,"SPAN",{});var Z=p(r);$=i(Z,"Show "),v=i(Z,j[2]),L=i(Z," content"),Z.forEach(e),d.forEach(e),this.h()},h(){g(a,"class","cursor-pointer flex items-center justify-center space-x-1 py-2.5")},m(F,d){c(F,a,d),P(f,a,null),s(a,o),s(a,r),s(r,$),s(r,v),s(r,L),q=!0,z||(X=ge(a,"click",j[4]),z=!0)},p:Yt,i(F){q||(k(f.$$.fragment,F),q=!0)},o(F){y(f.$$.fragment,F),q=!1},d(F){F&&e(a),T(f),z=!1,X()}}}function Oe(j){let a,f,o,r,$,v,L,q,z,X,F,d;var Z=j[1];function N(Y){return{}}Z&&(o=new Z(N()));let U=!j[0]&&ue(j);const it=[We,xe],K=[];function pt(Y,O){return Y[0]?0:1}return X=pt(j),F=K[X]=it[X](j),{c(){a=m("div"),f=m("div"),o&&S(o.$$.fragment),r=_(),$=m("span"),v=n(j[2]),L=_(),q=m("div"),U&&U.c(),z=_(),F.c(),this.h()},l(Y){a=u(Y,"DIV",{class:!0});var O=p(a);f=u(O,"DIV",{class:!0});var lt=p(f);o&&D(o.$$.fragment,lt),r=h(lt),$=u(lt,"SPAN",{});var rt=p($);v=i(rt,j[2]),rt.forEach(e),lt.forEach(e),L=h(O),q=u(O,"DIV",{class:!0});var st=p(q);U&&U.l(st),st.forEach(e),z=h(O),F.l(O),O.forEach(e),this.h()},h(){g(f,"class","absolute top-0 left-0 -translate-y-4 translate-x-4 flex items-center space-x-1 px-2 bg-white dark:bg-gray-950"),g(q,"class","absolute top-0 right-0 -translate-y-6 -translate-x-4 flex items-center space-x-1 px-2 bg-white dark:bg-gray-950"),g(a,"class","framework-content border-2 border-gray-200 rounded-xl px-4 relative")},m(Y,O){c(Y,a,O),s(a,f),o&&P(o,f,null),s(f,r),s(f,$),s($,v),s(a,L),s(a,q),U&&U.m(q,null),s(a,z),K[X].m(a,null),d=!0},p(Y,[O]){if(Z!==(Z=Y[1])){if(o){oe();const rt=o;y(rt.$$.fragment,1,0,()=>{T(rt,1)}),le()}Z?(o=new Z(N()),S(o.$$.fragment),k(o.$$.fragment,1),P(o,f,r)):o=null}Y[0]?U&&(oe(),y(U,1,1,()=>{U=null}),le()):U?(U.p(Y,O),O&1&&k(U,1)):(U=ue(Y),U.c(),k(U,1),U.m(q,null));let lt=X;X=pt(Y),X===lt?K[X].p(Y,O):(oe(),y(K[lt],1,1,()=>{K[lt]=null}),le(),F=K[X],F?F.p(Y,O):(F=K[X]=it[X](Y),F.c()),k(F,1),F.m(a,null))},i(Y){d||(o&&k(o.$$.fragment,Y),k(U),k(F),d=!0)},o(Y){o&&y(o.$$.fragment,Y),y(U),y(F),d=!1},d(Y){Y&&e(a),o&&T(o),U&&U.d(),K[X].d()}}}function Ne(j,a,f){let o,{$$slots:r={},$$scope:$}=a,{framework:v}=a;const L={pytorch:{Icon:ze,label:"Pytorch"},tensorflow:{Icon:qe,label:"TensorFlow"},jax:{Icon:Fe,label:"JAX"}},{Icon:q,label:z}=L[v],X=`hf_doc_framework_${v}_is_hidden`,F=je(v);ke(j,F,Z=>f(0,o=Z));function d(){me(F,o=!o,o),localStorage.setItem(X,o?"true":"false")}return ve(()=>{localStorage.getItem(X)==="true"&&me(F,o=!0,o)}),j.$$set=Z=>{"framework"in Z&&f(5,v=Z.framework),"$$scope"in Z&&f(6,$=Z.$$scope)},[o,q,z,F,d,v,$,r]}class ce extends ee{constructor(a){super();se(this,a,Ne,Oe,ae,{framework:5})}}const He=j=>({}),de=j=>({}),Le=j=>({}),_e=j=>({}),Ie=j=>({}),he=j=>({});function $e(j){let a,f;return a=new ce({props:{framework:"pytorch",$$slots:{default:[Je]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&16&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function Je(j){let a;const f=j[3].pytorch,o=re(f,j,j[4],he);return{c(){o&&o.c()},l(r){o&&o.l(r)},m(r,$){o&&o.m(r,$),a=!0},p(r,$){o&&o.p&&(!a||$&16)&&ne(o,f,r,r[4],a?pe(f,r[4],$,Ie):ie(r[4]),he)},i(r){a||(k(o,r),a=!0)},o(r){y(o,r),a=!1},d(r){o&&o.d(r)}}}function be(j){let a,f;return a=new ce({props:{framework:"tensorflow",$$slots:{default:[Ke]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&16&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function Ke(j){let a;const f=j[3].tensorflow,o=re(f,j,j[4],_e);return{c(){o&&o.c()},l(r){o&&o.l(r)},m(r,$){o&&o.m(r,$),a=!0},p(r,$){o&&o.p&&(!a||$&16)&&ne(o,f,r,r[4],a?pe(f,r[4],$,Le):ie(r[4]),_e)},i(r){a||(k(o,r),a=!0)},o(r){y(o,r),a=!1},d(r){o&&o.d(r)}}}function we(j){let a,f;return a=new ce({props:{framework:"jax",$$slots:{default:[Be]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&16&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function Be(j){let a;const f=j[3].jax,o=re(f,j,j[4],de);return{c(){o&&o.c()},l(r){o&&o.l(r)},m(r,$){o&&o.m(r,$),a=!0},p(r,$){o&&o.p&&(!a||$&16)&&ne(o,f,r,r[4],a?pe(f,r[4],$,He):ie(r[4]),de)},i(r){a||(k(o,r),a=!0)},o(r){y(o,r),a=!1},d(r){o&&o.d(r)}}}function Ve(j){let a,f,o,r,$=j[0]&&$e(j),v=j[1]&&be(j),L=j[2]&&we(j);return{c(){a=m("div"),$&&$.c(),f=_(),v&&v.c(),o=_(),L&&L.c(),this.h()},l(q){a=u(q,"DIV",{class:!0});var z=p(a);$&&$.l(z),f=h(z),v&&v.l(z),o=h(z),L&&L.l(z),z.forEach(e),this.h()},h(){g(a,"class","space-y-10 py-6 2xl:py-8 2xl:-mx-4")},m(q,z){c(q,a,z),$&&$.m(a,null),s(a,f),v&&v.m(a,null),s(a,o),L&&L.m(a,null),r=!0},p(q,[z]){q[0]?$?($.p(q,z),z&1&&k($,1)):($=$e(q),$.c(),k($,1),$.m(a,f)):$&&(oe(),y($,1,1,()=>{$=null}),le()),q[1]?v?(v.p(q,z),z&2&&k(v,1)):(v=be(q),v.c(),k(v,1),v.m(a,o)):v&&(oe(),y(v,1,1,()=>{v=null}),le()),q[2]?L?(L.p(q,z),z&4&&k(L,1)):(L=we(q),L.c(),k(L,1),L.m(a,null)):L&&(oe(),y(L,1,1,()=>{L=null}),le())},i(q){r||(k($),k(v),k(L),r=!0)},o(q){y($),y(v),y(L),r=!1},d(q){q&&e(a),$&&$.d(),v&&v.d(),L&&L.d()}}}function Re(j,a,f){let{$$slots:o={},$$scope:r}=a,{pytorch:$=!1}=a,{tensorflow:v=!1}=a,{jax:L=!1}=a;return j.$$set=q=>{"pytorch"in q&&f(0,$=q.pytorch),"tensorflow"in q&&f(1,v=q.tensorflow),"jax"in q&&f(2,L=q.jax),"$$scope"in q&&f(4,r=q.$$scope)},[$,v,L,o,r]}class fe extends ee{constructor(a){super();se(this,a,Re,Ve,ae,{pytorch:0,tensorflow:1,jax:2})}}function Ze(j){let a;const f=j[1].default,o=re(f,j,j[0],null);return{c(){o&&o.c()},l(r){o&&o.l(r)},m(r,$){o&&o.m(r,$),a=!0},p(r,[$]){o&&o.p&&(!a||$&1)&&ne(o,f,r,r[0],a?pe(f,r[0],$,null):ie(r[0]),null)},i(r){a||(k(o,r),a=!0)},o(r){y(o,r),a=!1},d(r){o&&o.d(r)}}}function Ge(j,a,f){let{$$slots:o={},$$scope:r}=a;return j.$$set=$=>{"$$scope"in $&&f(0,r=$.$$scope)},[r,o]}class te extends ee{constructor(a){super();se(this,a,Ge,Ze,ae,{})}}function Qe(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function Xe(j){let a,f;return a=new te({props:{$$slots:{default:[Qe]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function Ye(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function Ue(j){let a,f;return a=new te({props:{$$slots:{default:[Ye]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function ts(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function es(j){let a,f;return a=new te({props:{$$slots:{default:[ts]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function ss(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function as(j){let a,f;return a=new te({props:{$$slots:{default:[ss]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function os(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function ls(j){let a,f;return a=new te({props:{$$slots:{default:[os]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function rs(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function ns(j){let a,f;return a=new te({props:{$$slots:{default:[rs]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function is(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function ps(j){let a,f;return a=new te({props:{$$slots:{default:[is]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function fs(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function cs(j){let a,f;return a=new te({props:{$$slots:{default:[fs]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function ms(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function us(j){let a,f;return a=new te({props:{$$slots:{default:[ms]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function ds(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function _s(j){let a,f;return a=new te({props:{$$slots:{default:[ds]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function hs(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function $s(j){let a,f;return a=new te({props:{$$slots:{default:[hs]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function bs(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st,b,B,w,dt,G,_t,jt,tt,qt,yt,et,Et,Ct,ft,W,ct,V,Ft,nt,C,ut,E,St,R,Pt,zt,mt,H,ht,Q,Tt,$t,I,bt,A,Dt,ot,At,Mt,wt,J,gt;return r=new Ut({}),b=new at({props:{code:`from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors="tf")`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`}}),W=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),C=new at({props:{code:`from transformers import create_optimizer
import tensorflow as tf

batch_size = 16
num_epochs = 5
batches_per_epoch = len(tokenized_imdb["train"]) // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

batch_size = <span class="hljs-number">16</span>
num_epochs = <span class="hljs-number">5</span>
batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`}}),H=new at({props:{code:`from transformers import TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2)`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>)`}}),I=new at({props:{code:`import tensorflow as tf

model.compile(optimizer=optimizer)`,highlighted:`<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`}}),J=new at({props:{code:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`,highlighted:`model.fit(
    tf_train_set,
    validation_data=tf_validation_set,
    epochs=num_train_epochs,
)`}}),{c(){a=m("h3"),f=m("a"),o=m("span"),S(r.$$.fragment),$=_(),v=m("span"),L=n("Some Header"),q=_(),z=m("p"),X=n("Fine-tuning with TensorFlow is just as easy, with only a few differences."),F=_(),d=m("p"),Z=n("Start by batching the processed examples together with dynamic padding using the "),N=m("a"),U=n("DataCollatorWithPadding"),it=n(` function.
Make sure you set `),K=m("code"),pt=n('return_tensors="tf"'),Y=n(" to return "),O=m("code"),lt=n("tf.Tensor"),rt=n(" outputs instead of PyTorch tensors!"),st=_(),S(b.$$.fragment),B=_(),w=m("p"),dt=n("Next, convert your datasets to the "),G=m("code"),_t=n("tf.data.Dataset"),jt=n(" format with "),tt=m("code"),qt=n("to_tf_dataset"),yt=n(`. Specify inputs and labels in the
`),et=m("code"),Et=n("columns"),Ct=n(" argument:"),ft=_(),S(W.$$.fragment),ct=_(),V=m("p"),Ft=n("Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),nt=_(),S(C.$$.fragment),ut=_(),E=m("p"),St=n("Load your model with the "),R=m("a"),Pt=n("TFAutoModelForSequenceClassification"),zt=n(" class along with the number of expected labels:"),mt=_(),S(H.$$.fragment),ht=_(),Q=m("p"),Tt=n("Compile the model:"),$t=_(),S(I.$$.fragment),bt=_(),A=m("p"),Dt=n("Finally, fine-tune the model by calling "),ot=m("code"),At=n("model.fit"),Mt=n(":"),wt=_(),S(J.$$.fragment),this.h()},l(t){a=u(t,"H3",{class:!0});var l=p(a);f=u(l,"A",{id:!0,class:!0,href:!0});var Ot=p(f);o=u(Ot,"SPAN",{});var Nt=p(o);D(r.$$.fragment,Nt),Nt.forEach(e),Ot.forEach(e),$=h(l),v=u(l,"SPAN",{});var Ht=p(v);L=i(Ht,"Some Header"),Ht.forEach(e),l.forEach(e),q=h(t),z=u(t,"P",{});var Lt=p(z);X=i(Lt,"Fine-tuning with TensorFlow is just as easy, with only a few differences."),Lt.forEach(e),F=h(t),d=u(t,"P",{});var M=p(d);Z=i(M,"Start by batching the processed examples together with dynamic padding using the "),N=u(M,"A",{href:!0});var It=p(N);U=i(It,"DataCollatorWithPadding"),It.forEach(e),it=i(M,` function.
Make sure you set `),K=u(M,"CODE",{});var Jt=p(K);pt=i(Jt,'return_tensors="tf"'),Jt.forEach(e),Y=i(M," to return "),O=u(M,"CODE",{});var Kt=p(O);lt=i(Kt,"tf.Tensor"),Kt.forEach(e),rt=i(M," outputs instead of PyTorch tensors!"),M.forEach(e),st=h(t),D(b.$$.fragment,t),B=h(t),w=u(t,"P",{});var x=p(w);dt=i(x,"Next, convert your datasets to the "),G=u(x,"CODE",{});var Bt=p(G);_t=i(Bt,"tf.data.Dataset"),Bt.forEach(e),jt=i(x," format with "),tt=u(x,"CODE",{});var Vt=p(tt);qt=i(Vt,"to_tf_dataset"),Vt.forEach(e),yt=i(x,`. Specify inputs and labels in the
`),et=u(x,"CODE",{});var Rt=p(et);Et=i(Rt,"columns"),Rt.forEach(e),Ct=i(x," argument:"),x.forEach(e),ft=h(t),D(W.$$.fragment,t),ct=h(t),V=u(t,"P",{});var Zt=p(V);Ft=i(Zt,"Set up an optimizer function, learning rate schedule, and some training hyperparameters:"),Zt.forEach(e),nt=h(t),D(C.$$.fragment,t),ut=h(t),E=u(t,"P",{});var kt=p(E);St=i(kt,"Load your model with the "),R=u(kt,"A",{href:!0});var Gt=p(R);Pt=i(Gt,"TFAutoModelForSequenceClassification"),Gt.forEach(e),zt=i(kt," class along with the number of expected labels:"),kt.forEach(e),mt=h(t),D(H.$$.fragment,t),ht=h(t),Q=u(t,"P",{});var Qt=p(Q);Tt=i(Qt,"Compile the model:"),Qt.forEach(e),$t=h(t),D(I.$$.fragment,t),bt=h(t),A=u(t,"P",{});var vt=p(A);Dt=i(vt,"Finally, fine-tune the model by calling "),ot=u(vt,"CODE",{});var Xt=p(ot);At=i(Xt,"model.fit"),Xt.forEach(e),Mt=i(vt,":"),vt.forEach(e),wt=h(t),D(J.$$.fragment,t),this.h()},h(){g(f,"id","some-header"),g(f,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(f,"href","#some-header"),g(a,"class","relative group"),g(N,"href","/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding"),g(R,"href","/docs/transformers/master/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification")},m(t,l){c(t,a,l),s(a,f),s(f,o),P(r,o,null),s(a,$),s(a,v),s(v,L),c(t,q,l),c(t,z,l),s(z,X),c(t,F,l),c(t,d,l),s(d,Z),s(d,N),s(N,U),s(d,it),s(d,K),s(K,pt),s(d,Y),s(d,O),s(O,lt),s(d,rt),c(t,st,l),P(b,t,l),c(t,B,l),c(t,w,l),s(w,dt),s(w,G),s(G,_t),s(w,jt),s(w,tt),s(tt,qt),s(w,yt),s(w,et),s(et,Et),s(w,Ct),c(t,ft,l),P(W,t,l),c(t,ct,l),c(t,V,l),s(V,Ft),c(t,nt,l),P(C,t,l),c(t,ut,l),c(t,E,l),s(E,St),s(E,R),s(R,Pt),s(E,zt),c(t,mt,l),P(H,t,l),c(t,ht,l),c(t,Q,l),s(Q,Tt),c(t,$t,l),P(I,t,l),c(t,bt,l),c(t,A,l),s(A,Dt),s(A,ot),s(ot,At),s(A,Mt),c(t,wt,l),P(J,t,l),gt=!0},p:Yt,i(t){gt||(k(r.$$.fragment,t),k(b.$$.fragment,t),k(W.$$.fragment,t),k(C.$$.fragment,t),k(H.$$.fragment,t),k(I.$$.fragment,t),k(J.$$.fragment,t),gt=!0)},o(t){y(r.$$.fragment,t),y(b.$$.fragment,t),y(W.$$.fragment,t),y(C.$$.fragment,t),y(H.$$.fragment,t),y(I.$$.fragment,t),y(J.$$.fragment,t),gt=!1},d(t){t&&e(a),T(r),t&&e(q),t&&e(z),t&&e(F),t&&e(d),t&&e(st),T(b,t),t&&e(B),t&&e(w),t&&e(ft),T(W,t),t&&e(ct),t&&e(V),t&&e(nt),T(C,t),t&&e(ut),t&&e(E),t&&e(mt),T(H,t),t&&e(ht),t&&e(Q),t&&e($t),T(I,t),t&&e(bt),t&&e(A),t&&e(wt),T(J,t)}}}function ws(j){let a,f;return a=new te({props:{$$slots:{default:[bs]},$$scope:{ctx:j}}}),{c(){S(a.$$.fragment)},l(o){D(a.$$.fragment,o)},m(o,r){P(a,o,r),f=!0},p(o,r){const $={};r&2&&($.$$scope={dirty:r,ctx:o}),a.$set($)},i(o){f||(k(a.$$.fragment,o),f=!0)},o(o){y(a.$$.fragment,o),f=!1},d(o){T(a,o)}}}function gs(j){let a,f,o,r,$,v,L,q,z,X,F,d,Z,N,U,it,K,pt,Y,O,lt,rt,st;return v=new Ut({}),N=new at({props:{code:`tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(tf_train_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=True,
    batch_size=16,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "label"],
    shuffle=False,
    batch_size=16,
    collate_fn=data_collator,
)`,highlighted:`tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(tf_train_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">True</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)

tf_validation_dataset = tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
    shuffle=<span class="hljs-literal">False</span>,
    batch_size=<span class="hljs-number">16</span>,
    collate_fn=data_collator,
)`}}),it=new fe({props:{pytorch:!0,tensorflow:!0,jax:!0,$$slots:{jax:[es],tensorflow:[Ue],pytorch:[Xe]},$$scope:{ctx:j}}}),pt=new fe({props:{pytorch:!0,tensorflow:!0,jax:!0,$$slots:{jax:[ns],tensorflow:[ls],pytorch:[as]},$$scope:{ctx:j}}}),O=new fe({props:{pytorch:!0,tensorflow:!0,jax:!0,$$slots:{jax:[us],tensorflow:[cs],pytorch:[ps]},$$scope:{ctx:j}}}),rt=new fe({props:{pytorch:!0,tensorflow:!0,jax:!0,$$slots:{jax:[ws],tensorflow:[$s],pytorch:[_s]},$$scope:{ctx:j}}}),{c(){a=m("meta"),f=_(),o=m("h1"),r=m("a"),$=m("span"),S(v.$$.fragment),L=_(),q=m("span"),z=n("Common Text"),X=_(),F=m("p"),d=n("Some more paragraphs"),Z=_(),S(N.$$.fragment),U=_(),S(it.$$.fragment),K=_(),S(pt.$$.fragment),Y=_(),S(O.$$.fragment),lt=_(),S(rt.$$.fragment),this.h()},l(b){const B=ye('[data-svelte="svelte-1phssyn"]',document.head);a=u(B,"META",{name:!0,content:!0}),B.forEach(e),f=h(b),o=u(b,"H1",{class:!0});var w=p(o);r=u(w,"A",{id:!0,class:!0,href:!0});var dt=p(r);$=u(dt,"SPAN",{});var G=p($);D(v.$$.fragment,G),G.forEach(e),dt.forEach(e),L=h(w),q=u(w,"SPAN",{});var _t=p(q);z=i(_t,"Common Text"),_t.forEach(e),w.forEach(e),X=h(b),F=u(b,"P",{});var jt=p(F);d=i(jt,"Some more paragraphs"),jt.forEach(e),Z=h(b),D(N.$$.fragment,b),U=h(b),D(it.$$.fragment,b),K=h(b),D(pt.$$.fragment,b),Y=h(b),D(O.$$.fragment,b),lt=h(b),D(rt.$$.fragment,b),this.h()},h(){g(a,"name","hf:doc:metadata"),g(a,"content",JSON.stringify(ks)),g(r,"id","common-text"),g(r,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),g(r,"href","#common-text"),g(o,"class","relative group")},m(b,B){s(document.head,a),c(b,f,B),c(b,o,B),s(o,r),s(r,$),P(v,$,null),s(o,L),s(o,q),s(q,z),c(b,X,B),c(b,F,B),s(F,d),c(b,Z,B),P(N,b,B),c(b,U,B),P(it,b,B),c(b,K,B),P(pt,b,B),c(b,Y,B),P(O,b,B),c(b,lt,B),P(rt,b,B),st=!0},p(b,[B]){const w={};B&2&&(w.$$scope={dirty:B,ctx:b}),it.$set(w);const dt={};B&2&&(dt.$$scope={dirty:B,ctx:b}),pt.$set(dt);const G={};B&2&&(G.$$scope={dirty:B,ctx:b}),O.$set(G);const _t={};B&2&&(_t.$$scope={dirty:B,ctx:b}),rt.$set(_t)},i(b){st||(k(v.$$.fragment,b),k(N.$$.fragment,b),k(it.$$.fragment,b),k(pt.$$.fragment,b),k(O.$$.fragment,b),k(rt.$$.fragment,b),st=!0)},o(b){y(v.$$.fragment,b),y(N.$$.fragment,b),y(it.$$.fragment,b),y(pt.$$.fragment,b),y(O.$$.fragment,b),y(rt.$$.fragment,b),st=!1},d(b){e(a),b&&e(f),b&&e(o),T(v),b&&e(X),b&&e(F),b&&e(Z),T(N,b),b&&e(U),T(it,b),b&&e(K),T(pt,b),b&&e(Y),T(O,b),b&&e(lt),T(rt,b)}}}const ks={local:"common-text",sections:[{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"},{local:"some-header",title:"Some Header"}],title:"Common Text"};function vs(j,a,f){let{fw:o}=a;return j.$$set=r=>{"fw"in r&&f(0,o=r.fw)},[o]}class Cs extends ee{constructor(a){super();se(this,a,vs,gs,ae,{fw:0})}}export{Cs as default,ks as metadata};
