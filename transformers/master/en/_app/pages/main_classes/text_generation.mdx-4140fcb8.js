import{S as Um,i as Vm,s as Km,e as o,k as l,w as m,t,M as Zm,c as a,d as s,m as d,a as r,x as g,h as n,b as c,F as e,g as w,y as u,q as _,o as h,B as f}from"../../chunks/vendor-6b77c823.js";import{T as Rm}from"../../chunks/Tip-39098574.js";import{D as be}from"../../chunks/Docstring-abef54e3.js";import{C as ye}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Co}from"../../chunks/IconCopyLink-7a11ce68.js";function Xm(it){let p,P,j,T,I,E,xe,W,G,B,v;return{c(){p=o("p"),P=t("Apart from "),j=o("code"),T=t("inputs"),I=t(`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),E=o("code"),xe=t("config.json"),W=t(`) which in turn defaults to the
`),G=o("a"),B=t("PretrainedConfig"),v=t(" of the model."),this.h()},l($){p=a($,"P",{});var b=r(p);P=n(b,"Apart from "),j=a(b,"CODE",{});var Me=r(j);T=n(Me,"inputs"),Me.forEach(s),I=n(b,`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),E=a(b,"CODE",{});var Le=r(E);xe=n(Le,"config.json"),Le.forEach(s),W=n(b,`) which in turn defaults to the
`),G=a(b,"A",{href:!0});var H=r(G);B=n(H,"PretrainedConfig"),H.forEach(s),v=n(b," of the model."),b.forEach(s),this.h()},h(){c(G,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig")},m($,b){w($,p,b),e(p,P),e(p,j),e(j,T),e(p,I),e(p,E),e(E,xe),e(p,W),e(p,G),e(G,B),e(p,v)},d($){$&&s(p)}}}function Jm(it){let p,P,j,T,I,E,xe,W,G,B,v;return{c(){p=o("p"),P=t("Apart from "),j=o("code"),T=t("inputs"),I=t(`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),E=o("code"),xe=t("config.json"),W=t(`) which in turn defaults to the
`),G=o("a"),B=t("PretrainedConfig"),v=t(" of the model."),this.h()},l($){p=a($,"P",{});var b=r(p);P=n(b,"Apart from "),j=a(b,"CODE",{});var Me=r(j);T=n(Me,"inputs"),Me.forEach(s),I=n(b,`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),E=a(b,"CODE",{});var Le=r(E);xe=n(Le,"config.json"),Le.forEach(s),W=n(b,`) which in turn defaults to the
`),G=a(b,"A",{href:!0});var H=r(G);B=n(H,"PretrainedConfig"),H.forEach(s),v=n(b," of the model."),b.forEach(s),this.h()},h(){c(G,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig")},m($,b){w($,p,b),e(p,P),e(p,j),e(j,T),e(p,I),e(p,E),e(E,xe),e(p,W),e(p,G),e(G,B),e(p,v)},d($){$&&s(p)}}}function Qm(it){let p,P,j,T,I,E,xe,W,G,B,v,$,b,Me,Le,H,Io,Wo,en,Bo,Ho,tn,Ro,Uo,nn,Vo,Ko,sn,Zo,Xo,uo,S,Jo,On,Qo,Yo,qn,ea,ta,on,na,sa,an,oa,aa,rn,ra,ia,_o,we,Ze,Gn,lt,la,Sn,da,ho,k,dt,ca,ct,pa,ln,ma,ga,ua,pt,_a,dn,ha,fa,ba,F,R,$n,xa,ka,cn,va,ya,Fn,ja,Ma,An,La,wa,Ta,U,zn,Ea,Oa,pn,qa,Ga,Pn,Sa,$a,Nn,Fa,Aa,za,V,Dn,Pa,Na,mn,Da,Ca,Cn,Ia,Wa,In,Ba,Ha,Ra,K,Wn,Ua,Va,gn,Ka,Za,Bn,Xa,Ja,Hn,Qa,Ya,er,Z,Rn,tr,nr,un,sr,or,Un,ar,rr,Vn,ir,lr,dr,X,Kn,cr,pr,_n,mr,gr,Zn,ur,_r,Xn,hr,fr,br,x,mt,xr,Jn,kr,vr,A,J,Qn,yr,jr,hn,Mr,Lr,Yn,wr,Tr,es,Er,Or,qr,Q,ts,Gr,Sr,fn,$r,Fr,ns,Ar,zr,ss,Pr,Nr,Dr,Y,os,Cr,Ir,bn,Wr,Br,as,Hr,Rr,rs,Ur,Vr,Kr,ee,is,Zr,Xr,xn,Jr,Qr,ls,Yr,ei,ds,ti,ni,si,te,cs,oi,ai,kn,ri,ii,ps,li,di,ms,ci,pi,mi,ne,gs,gi,ui,vn,_i,hi,us,fi,bi,_s,xi,ki,vi,Xe,yi,gt,ji,ut,Mi,Li,wi,hs,Ti,Ei,fs,Oi,qi,_t,Gi,bs,Si,$i,ht,Fi,xs,Ai,zi,ft,Pi,se,bt,Ni,xt,Di,ks,Ci,Ii,Wi,vs,Bi,Hi,kt,Ri,oe,vt,Ui,yt,Vi,ys,Ki,Zi,Xi,js,Ji,Qi,jt,Yi,ae,Mt,el,Lt,tl,Ms,nl,sl,ol,Ls,al,rl,wt,il,re,Tt,ll,Et,dl,ws,cl,pl,ml,Ts,gl,ul,Ot,_l,ie,qt,hl,Gt,fl,Es,bl,xl,kl,Os,vl,yl,St,jl,le,$t,Ml,Ft,Ll,qs,wl,Tl,El,Gs,Ol,ql,At,fo,Te,Je,Ss,zt,Gl,$s,Sl,bo,ke,Pt,$l,Nt,Fl,yn,Al,zl,Pl,O,Dt,Nl,Fs,Dl,Cl,Ct,Il,It,Wl,Bl,Hl,ve,Rl,As,Ul,Vl,zs,Kl,Zl,jn,Xl,Jl,Ql,Wt,Yl,Bt,ed,td,nd,Ps,sd,od,Ht,xo,Ee,Qe,Ns,Rt,ad,Ds,rd,ko,z,Ut,id,Vt,ld,Mn,dd,cd,pd,Kt,md,Ln,gd,ud,_d,Oe,de,Cs,hd,fd,Is,bd,xd,Ws,kd,vd,Bs,yd,jd,Md,ce,Hs,Ld,wd,Rs,Td,Ed,Us,Od,qd,Vs,Gd,Sd,$d,pe,Ks,Fd,Ad,Zs,zd,Pd,Xs,Nd,Dd,Js,Cd,Id,Wd,q,Zt,Bd,Qs,Hd,Rd,qe,me,Ys,Ud,Vd,eo,Kd,Zd,to,Xd,Jd,no,Qd,Yd,ec,ge,so,tc,nc,oo,sc,oc,ao,ac,rc,ro,ic,lc,dc,ue,io,cc,pc,lo,mc,gc,co,uc,_c,po,hc,fc,bc,Ye,xc,Xt,kc,Jt,vc,yc,jc,mo,Mc,Lc,Qt,vo;return E=new Co({}),lt=new Co({}),dt=new be({props:{name:"class transformers.generation_utils.GenerationMixin",anchor:"transformers.generation_utils.GenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L379"}}),mt=new be({props:{name:"generate",anchor:"transformers.generation_utils.GenerationMixin.generate",parameters:[{name:"inputs",val:": typing.Optional[torch.Tensor] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"min_length",val:": typing.Optional[int] = None"},{name:"do_sample",val:": typing.Optional[bool] = None"},{name:"early_stopping",val:": typing.Optional[bool] = None"},{name:"num_beams",val:": typing.Optional[int] = None"},{name:"temperature",val:": typing.Optional[float] = None"},{name:"top_k",val:": typing.Optional[int] = None"},{name:"top_p",val:": typing.Optional[float] = None"},{name:"typical_p",val:": typing.Optional[float] = None"},{name:"repetition_penalty",val:": typing.Optional[float] = None"},{name:"bad_words_ids",val:": typing.Optional[typing.Iterable[int]] = None"},{name:"force_words_ids",val:": typing.Union[typing.Iterable[int], typing.Iterable[typing.Iterable[int]], NoneType] = None"},{name:"bos_token_id",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"length_penalty",val:": typing.Optional[float] = None"},{name:"no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"encoder_no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"num_return_sequences",val:": typing.Optional[int] = None"},{name:"max_time",val:": typing.Optional[float] = None"},{name:"max_new_tokens",val:": typing.Optional[int] = None"},{name:"decoder_start_token_id",val:": typing.Optional[int] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"num_beam_groups",val:": typing.Optional[int] = None"},{name:"diversity_penalty",val:": typing.Optional[float] = None"},{name:"prefix_allowed_tokens_fn",val:": typing.Union[typing.Callable[[int, torch.Tensor], typing.List[int]], NoneType] = None"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = []"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = []"},{name:"constraints",val:": typing.Optional[typing.List[transformers.generation_beam_constraints.Constraint]] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"forced_bos_token_id",val:": typing.Optional[int] = None"},{name:"forced_eos_token_id",val:": typing.Optional[int] = None"},{name:"remove_invalid_values",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"exponential_decay_length_penalty",val:": typing.Union[typing.Tuple[typing.Union[int, float]], NoneType] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L832",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.generate.inputs",description:`<strong>inputs</strong> (<code>torch.Tensor</code> of varying shape depending on the modality, <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should of in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.`,name:"inputs"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to <code>model.config.max_length</code>) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_new_tokens",description:`<strong>max_new_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to None) &#x2014;
The maximum numbers of tokens to generate, ignore the current number of tokens. Use either
<code>max_new_tokens</code> or <code>max_length</code> but not both, they serve the same purpose.`,name:"max_new_tokens"},{anchor:"transformers.generation_utils.GenerationMixin.generate.min_length",description:`<strong>min_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The minimum length of the sequence to be generated.`,name:"min_length"},{anchor:"transformers.generation_utils.GenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_utils.GenerationMixin.generate.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop the beam search when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_utils.GenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_utils.GenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_utils.GenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_utils.GenerationMixin.generate.repetition_penalty",description:`<strong>repetition_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" rel="nofollow">this
paper</a> for more details.`,name:"repetition_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Exponential penalty to the length. 1.0 means no penalty. Set to values &lt; 1.0 in order to encourage the
model to generate shorter sequences, to a value &gt; 1.0 in order to encourage the model to produce longer
sequences.`,name:"length_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.no_repeat_ngram_size",description:`<strong>no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size can only occur once.`,name:"no_repeat_ngram_size"},{anchor:"transformers.generation_utils.GenerationMixin.generate.encoder_no_repeat_ngram_size",description:`<strong>encoder_no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size that occur in the <code>encoder_input_ids</code> cannot occur in the
<code>decoder_input_ids</code>.`,name:"encoder_no_repeat_ngram_size"},{anchor:"transformers.generation_utils.GenerationMixin.generate.bad_words_ids(List[List[int]],",description:`<strong>bad_words_ids(<code>List[List[int]]</code>,</strong> <em>optional</em>) &#x2014;
List of token ids that are not allowed to be generated. In order to get the token ids of the words that
should not appear in the generated text, use <code>tokenizer(bad_words, add_prefix_space=True, add_special_tokens=False).input_ids</code>.`,name:"bad_words_ids(List[List[int]],"},{anchor:"transformers.generation_utils.GenerationMixin.generate.force_words_ids(List[List[int]]",description:`<strong>force_words_ids(<code>List[List[int]]</code></strong> or <code>List[List[List[int]]]</code>, <em>optional</em>) &#x2014;
List of token ids that must be generated. If given a <code>List[List[int]]</code>, this is treated as a simple
list of words that must be included, the opposite to <code>bad_words_ids</code>. If given <code>List[List[List[int]]]</code>,
this triggers a <a href="https://github.com/huggingface/transformers/issues/14081" rel="nofollow">disjunctive constraint</a>,
where one can allow different forms of each word.`,name:"force_words_ids(List[List[int]]"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_return_sequences(int,",description:`<strong>num_return_sequences(<code>int</code>,</strong> <em>optional</em>, defaults to 1) &#x2014;
The number of independently computed returned sequences for each element in the batch.`,name:"num_return_sequences(int,"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_time(float,",description:`<strong>max_time(<code>float</code>,</strong> <em>optional</em>, defaults to None) &#x2014;
The maximum amount of time you allow the computation to run for in seconds. generation will still
finish the current pass after allocated time has been passed.`,name:"max_time(float,"},{anchor:"transformers.generation_utils.GenerationMixin.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values are in <code>[0, 1]</code>, 1 for tokens
that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape
as <code>input_ids</code> that masks the pad token. <a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.generation_utils.GenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
use_cache &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_beam_groups",description:`<strong>num_beam_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of groups to divide <code>num_beams</code> into in order to ensure diversity among different groups of
beams. <a href="https://arxiv.org/pdf/1610.02424.pdf" rel="nofollow">this paper</a> for more details.`,name:"num_beam_groups"},{anchor:"transformers.generation_utils.GenerationMixin.generate.diversity_penalty",description:`<strong>diversity_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
This value is subtracted from a beam&#x2019;s score if it generates a token same as any beam from other group
at a particular time. Note that <code>diversity_penalty</code> is only effective if <code>group beam search</code> is
enabled.
prefix_allowed_tokens_fn &#x2014; (<code>Callable[[int, torch.Tensor], List[int]]</code>, <em>optional</em>):
If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <code>batch_id</code> and
<code>input_ids</code>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <code>batch_id</code> and the previously generated tokens <code>inputs_ids</code>. This argument is useful
for constrained generation conditioned on the prefix, as described in <a href="https://arxiv.org/abs/2010.00904" rel="nofollow">Autoregressive Entity
Retrieval</a>.`,name:"diversity_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
Custom logits processors that complement the default logits processors built from arguments and a
model&#x2019;s config. If a logit processor is passed that is already created with the arguments or a model&#x2019;s
config an error is thrown. This feature is intended for advanced users.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.generate.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
Custom stopping criteria that complement the default stopping criteria built from arguments and a
model&#x2019;s config. If a stopping criteria is passed that is already created with the arguments or a
model&#x2019;s config an error is thrown. This feature is intended for advanced users.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.generate.constraints",description:`<strong>constraints</strong> (<code>List[Constraint]</code>, <em>optional</em>) &#x2014;
Custom constraints that can be added to the generation to ensure that the output will contain the use
of certain tokens as defined by <code>Constraint</code> objects, in the most sensible way possible.`,name:"constraints"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.generate.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.generate.forced_bos_token_id",description:`<strong>forced_bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the first generated token after the <code>decoder_start_token_id</code>. Useful
for multilingual models like <a href="../model_doc/mbart">mBART</a> where the first generated token needs to be
the target language token.`,name:"forced_bos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached.`,name:"forced_eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.remove_invalid_values",description:`<strong>remove_invalid_values</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to remove possible <em>nan</em> and <em>inf</em> outputs of the model to prevent the generation method to
crash. Note that using <code>remove_invalid_values</code> can slow down generation.`,name:"remove_invalid_values"},{anchor:"transformers.generation_utils.GenerationMixin.generate.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)`,name:"synced_gpus"},{anchor:"transformers.generation_utils.GenerationMixin.generate.exponential_decay_length_penalty",description:`<strong>exponential_decay_length_penalty</strong> (<code>tuple(int, float)</code>, <em>optional</em>) &#x2014;
This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been
generated. The tuple shall consist of: <code>(start_index, decay_factor)</code> where <code>start_index</code> indicates
where penalty starts and <code>decay_factor</code> represents the factor of exponential decay</p>
<p>model<em>kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with *decoder</em>*.`,name:"exponential_decay_length_penalty"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> (if
<code>return_dict_in_generate=True</code> or when <code>config.return_dict_in_generate=True</code>) or a <code>torch.FloatTensor</code>.</p>
<p>If the model is <em>not</em> an encoder-decoder model (<code>model.config.is_encoder_decoder=False</code>), the possible
<a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a></li>
</ul>
<p>If the model is an encoder-decoder model (<code>model.config.is_encoder_decoder=True</code>), the possible
<a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a></li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> or <code>torch.LongTensor</code></p>
`}}),Xe=new Rm({props:{warning:"&lcub;true}",$$slots:{default:[Xm]},$$scope:{ctx:it}}}),_t=new ye({props:{code:`from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = "Today I believe we can finally"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# generate up to 30 tokens
outputs = model.generate(input_ids, do_sample=False, max_length=30)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today I believe we can finally&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate up to 30 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n&#x27;</span>]`}}),ht=new ye({props:{code:`from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = "Today I believe we can finally"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# sample up to 30 tokens
torch.manual_seed(0)
outputs = model.generate(input_ids, do_sample=True, max_length=30)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today I believe we can finally&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># sample up to 30 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, do_sample=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today I believe we can finally get rid of discrimination,&quot; said Rep. Mark Pocan (D-Wis.).\\n\\n&quot;Just look at the&#x27;</span>]`}}),ft=new ye({props:{code:`from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-de")

sentence = "Paris is one of the densest populated areas in Europe."
input_ids = tokenizer(sentence, return_tensors="pt").input_ids

outputs = model.generate(input_ids)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>sentence = <span class="hljs-string">&quot;Paris is one of the densest populated areas in Europe.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(sentence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Paris ist eines der dichtesten besiedelten Gebiete Europas.&#x27;</span>]`}}),bt=new be({props:{name:"greedy_search",anchor:"transformers.generation_utils.GenerationMixin.greedy_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L1489",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific keyword arguments will be forwarded to the <code>forward</code> function of the model.
If model is an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a>
or <code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),kt=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    StoppingCriteriaList,
    MaxLengthCriteria,
)

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "It might be possible to"
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id),
    ]
)
stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

outputs = model.greedy_search(
    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForCausalLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    StoppingCriteriaList,
<span class="hljs-meta">... </span>    MaxLengthCriteria,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>input_prompt = <span class="hljs-string">&quot;It might be possible to&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">10</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=<span class="hljs-number">20</span>)])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.greedy_search(
<span class="hljs-meta">... </span>    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&quot;It might be possible to get a better understanding of the nature of the problem, but it&#x27;s not&quot;</span>]`}}),vt=new be({props:{name:"sample",anchor:"transformers.generation_utils.GenerationMixin.sample",parameters:[{name:"input_ids",val:": LongTensor"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"logits_warper",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L1721",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.sample.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.sample.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.sample.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.sample.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.sample.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.sample.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.sample.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.sample.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.sample.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),jt=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    TopKLogitsWarper,
    TemperatureLogitsWarper,
    StoppingCriteriaList,
    MaxLengthCriteria,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "Today is a beautiful day, and"
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),
    ]
)
# instantiate logits processors
logits_warper = LogitsProcessorList(
    [
        TopKLogitsWarper(50),
        TemperatureLogitsWarper(0.7),
    ]
)

stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

torch.manual_seed(0)
outputs = model.sample(
    input_ids,
    logits_processor=logits_processor,
    logits_warper=logits_warper,
    stopping_criteria=stopping_criteria,
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForCausalLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    TopKLogitsWarper,
<span class="hljs-meta">... </span>    TemperatureLogitsWarper,
<span class="hljs-meta">... </span>    StoppingCriteriaList,
<span class="hljs-meta">... </span>    MaxLengthCriteria,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>input_prompt = <span class="hljs-string">&quot;Today is a beautiful day, and&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">15</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_warper = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        TopKLogitsWarper(<span class="hljs-number">50</span>),
<span class="hljs-meta">... </span>        TemperatureLogitsWarper(<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=<span class="hljs-number">20</span>)])

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.sample(
<span class="hljs-meta">... </span>    input_ids,
<span class="hljs-meta">... </span>    logits_processor=logits_processor,
<span class="hljs-meta">... </span>    logits_warper=logits_warper,
<span class="hljs-meta">... </span>    stopping_criteria=stopping_criteria,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today is a beautiful day, and a wonderful day.\\n\\nI was lucky enough to meet the&#x27;</span>]`}}),Mt=new be({props:{name:"beam_search",anchor:"transformers.generation_utils.GenerationMixin.beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L1977",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
An derived instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><code>generation_utilsBeamSearchDecoderOnlyOutput</code>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),wt=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    num_beams=num_beams,
    device=model.device,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),Tt=new be({props:{name:"beam_sample",anchor:"transformers.generation_utils.GenerationMixin.beam_sample",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"logits_warper",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L2289",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
A derived instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),Ot=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    TopKLogitsWarper,
    TemperatureLogitsWarper,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    max_length=model.config.max_length,
    num_beams=num_beams,
    device=model.device,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]
)
# instantiate logits processors
logits_warper = LogitsProcessorList(
    [
        TopKLogitsWarper(50),
        TemperatureLogitsWarper(0.7),
    ]
)

outputs = model.beam_sample(
    input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    TopKLogitsWarper,
<span class="hljs-meta">... </span>    TemperatureLogitsWarper,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_length=model.config.max_length,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id)]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_warper = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        TopKLogitsWarper(<span class="hljs-number">50</span>),
<span class="hljs-meta">... </span>        TemperatureLogitsWarper(<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.beam_sample(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),qt=new be({props:{name:"group_beam_search",anchor:"transformers.generation_utils.GenerationMixin.group_beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L2611",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
An derived instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
<p>model_kwargs &#x2014;
Additional model specific kwargs that will be forwarded to the <code>forward</code> function of the model. If
model is an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if
<code>model.config.is_encoder_decoder=False</code> and <code>return_dict_in_generate=True</code> or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if <code>model.config.is_encoder_decoder=True</code>.</p>
`}}),St=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    HammingDiversityLogitsProcessor,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run diverse beam search using 6 beams
num_beams = 6
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    max_length=model.config.max_length,
    num_beams=num_beams,
    device=model.device,
    num_beam_groups=3,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.group_beam_search(
    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    HammingDiversityLogitsProcessor,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run diverse beam search using 6 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">6</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_length=model.config.max_length,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>    num_beam_groups=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        HammingDiversityLogitsProcessor(<span class="hljs-number">5.5</span>, num_beams=<span class="hljs-number">6</span>, num_beam_groups=<span class="hljs-number">3</span>),
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.group_beam_search(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),$t=new be({props:{name:"constrained_beam_search",anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"constrained_beam_scorer",val:": ConstrainedBeamSearchScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L2976",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.constrained_beam_scorer",description:`<strong>constrained_beam_scorer</strong> (<code>ConstrainedBeamSearchScorer</code>) &#x2014;
A derived instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation, while satisfying a list of positive constraints. For more information, the
documentation of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.ConstrainedBeamSearchScorer">ConstrainedBeamSearchScorer</a> should be read.`,name:"constrained_beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><code>generation_utilsBeamSearchDecoderOnlyOutput</code>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),At=new ye({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    ConstrainedBeamSearchScorer,
    PhrasalConstraint,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

constraint_str = "sind"
constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token
constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


# instantiate beam scorer
beam_scorer = ConstrainedBeamSearchScorer(
    batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.constrained_beam_search(
    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    ConstrainedBeamSearchScorer,
<span class="hljs-meta">... </span>    PhrasalConstraint,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span>constraint_str = <span class="hljs-string">&quot;sind&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>constraint_token_ids = tokenizer.encode(constraint_str)[:-<span class="hljs-number">1</span>]  <span class="hljs-comment"># slice to remove eos token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = ConstrainedBeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>, num_beams=num_beams, device=model.device, constraints=constraints
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.constrained_beam_search(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt sind Sie?&#x27;</span>]`}}),zt=new Co({}),Pt=new be({props:{name:"class transformers.generation_tf_utils.TFGenerationMixin",anchor:"transformers.generation_tf_utils.TFGenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_tf_utils.py#L342"}}),Dt=new be({props:{name:"generate",anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate",parameters:[{name:"input_ids",val:" = None"},{name:"max_length",val:" = None"},{name:"min_length",val:" = None"},{name:"do_sample",val:" = None"},{name:"early_stopping",val:" = None"},{name:"num_beams",val:" = None"},{name:"temperature",val:" = None"},{name:"top_k",val:" = None"},{name:"top_p",val:" = None"},{name:"repetition_penalty",val:" = None"},{name:"bad_words_ids",val:" = None"},{name:"bos_token_id",val:" = None"},{name:"pad_token_id",val:" = None"},{name:"eos_token_id",val:" = None"},{name:"length_penalty",val:" = None"},{name:"no_repeat_ngram_size",val:" = None"},{name:"num_return_sequences",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_start_token_id",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_scores",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict_in_generate",val:" = None"},{name:"forced_bos_token_id",val:" = None"},{name:"forced_eos_token_id",val:" = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_tf_utils.py#L362",parametersDescription:[{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.input_ids",description:"<strong>input_ids</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, `(batch_size, sequence_length, &#x2014;",name:"input_ids"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.feature_dim)`",description:`<strong>feature_dim)\`</strong> or <code>(batch_size, num_channels, height, width)</code>, <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should of in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.`,name:"feature_dim)`"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.min_length",description:`<strong>min_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The minimum length of the sequence to be generated.`,name:"min_length"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop the beam search when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.repetition_penalty",description:`<strong>repetition_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" rel="nofollow">this
paper</a> for more details.`,name:"repetition_penalty"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Exponential penalty to the length. 1.0 means no penalty.</p>
<p>Set to values &lt; 1.0 in order to encourage the model to generate shorter sequences, to a value &gt; 1.0 in
order to encourage the model to produce longer sequences.`,name:"length_penalty"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.no_repeat_ngram_size",description:`<strong>no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size can only occur once.`,name:"no_repeat_ngram_size"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.bad_words_ids(List[int],",description:`<strong>bad_words_ids(<code>List[int]</code>,</strong> <em>optional</em>) &#x2014;
List of token ids that are not allowed to be generated. In order to get the tokens of the words that
should not appear in the generated text, use <code>tokenizer.encode(bad_word, add_prefix_space=True)</code>.`,name:"bad_words_ids(List[int],"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.num_return_sequences(int,",description:`<strong>num_return_sequences(<code>int</code>,</strong> <em>optional</em>, defaults to 1) &#x2014;
The number of independently computed returned sequences for each element in the batch.`,name:"num_return_sequences(int,"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>tf.Tensor</code> of <code>dtype=tf.int32</code> and shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values are in <code>[0, 1]</code>, 1 for tokens
that are not masked, and 0 for masked tokens.</p>
<p>If not provided, will default to a tensor the same shape as <code>input_ids</code> that masks the pad token.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
use_cache &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.forced_bos_token_id",description:`<strong>forced_bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the first generated token after the <code>decoder_start_token_id</code>. Useful
for multilingual models like <a href="../model_doc/mbart">mBART</a> where the first generated token needs to be
the target language token.`,name:"forced_bos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached.
model_specific_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model.`,name:"forced_eos_token_id"}],returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> or <code>tf.Tensor</code></p>
`}}),Ht=new ye({props:{code:`tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "distilgpt2"
)  # Download model and configuration from huggingface.co and cache.
outputs = model.generate(max_length=40)  # do greedy decoding
print(f"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("openai-gpt")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "openai-gpt"
)  # Download model and configuration from huggingface.co and cache.
input_context = "The dog"
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5
)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'
for i in range(3):  #  3 output sequences were generated
    print(f"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "distilgpt2"
)  # Download model and configuration from huggingface.co and cache.
input_context = "The dog"
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3, do_sample=True
)  # generate 3 candidates using sampling
for i in range(3):  #  3 output sequences were generated
    print(f"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("ctrl")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "ctrl"
)  # Download model and configuration from huggingface.co and cache.
input_context = "Legal My neighbor is"  # "Legal" is one of the control codes for ctrl
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2
)  # generate sequences
print(f"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("gpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "gpt2"
)  # Download model and configuration from huggingface.co and cache.
input_context = "My cute dog"
bad_words_ids = [
    tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ["idiot", "stupid", "shut up"]
]
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids
)  # generate sequences without allowing bad_words to be generated`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;distilgpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
outputs = model.generate(max_length=<span class="hljs-number">40</span>)  <span class="hljs-comment"># do greedy decoding</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated: <span class="hljs-subst">{tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-gpt&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;openai-gpt&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;The dog&quot;</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, num_beams=<span class="hljs-number">5</span>, num_return_sequences=<span class="hljs-number">3</span>, temperature=<span class="hljs-number">1.5</span>
)  <span class="hljs-comment"># generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context &#x27;The dog&#x27;</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):  <span class="hljs-comment">#  3 output sequences were generated</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{i}</span>: <span class="hljs-subst">{tokenizer.decode(outputs[i], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;distilgpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;The dog&quot;</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">40</span>, temperature=<span class="hljs-number">0.7</span>, num_return_sequences=<span class="hljs-number">3</span>, do_sample=<span class="hljs-literal">True</span>
)  <span class="hljs-comment"># generate 3 candidates using sampling</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):  <span class="hljs-comment">#  3 output sequences were generated</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{i}</span>: <span class="hljs-subst">{tokenizer.decode(outputs[i], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;ctrl&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;ctrl&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;Legal My neighbor is&quot;</span>  <span class="hljs-comment"># &quot;Legal&quot; is one of the control codes for ctrl</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">50</span>, temperature=<span class="hljs-number">0.7</span>, repetition_penalty=<span class="hljs-number">1.2</span>
)  <span class="hljs-comment"># generate sequences</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated: <span class="hljs-subst">{tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;My cute dog&quot;</span>
bad_words_ids = [
    tokenizer.encode(bad_word, add_prefix_space=<span class="hljs-literal">True</span>) <span class="hljs-keyword">for</span> bad_word <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idiot&quot;</span>, <span class="hljs-string">&quot;stupid&quot;</span>, <span class="hljs-string">&quot;shut up&quot;</span>]
]
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>, bad_words_ids=bad_words_ids
)  <span class="hljs-comment"># generate sequences without allowing bad_words to be generated</span>`}}),Rt=new Co({}),Ut=new be({props:{name:"class transformers.generation_flax_utils.FlaxGenerationMixin",anchor:"transformers.generation_flax_utils.FlaxGenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_flax_utils.py#L119"}}),Zt=new be({props:{name:"generate",anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate",parameters:[{name:"input_ids",val:": ndarray"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"bos_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"decoder_start_token_id",val:": typing.Optional[int] = None"},{name:"do_sample",val:": typing.Optional[bool] = None"},{name:"prng_key",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"top_k",val:": typing.Optional[int] = None"},{name:"top_p",val:": typing.Optional[float] = None"},{name:"temperature",val:": typing.Optional[float] = None"},{name:"num_beams",val:": typing.Optional[int] = None"},{name:"no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"min_length",val:": typing.Optional[int] = None"},{name:"forced_bos_token_id",val:": typing.Optional[int] = None"},{name:"forced_eos_token_id",val:": typing.Optional[int] = None"},{name:"length_penalty",val:": typing.Optional[float] = None"},{name:"early_stopping",val:": typing.Optional[bool] = None"},{name:"trace",val:": bool = True"},{name:"params",val:": typing.Union[typing.Dict[str, jax._src.numpy.ndarray.ndarray], NoneType] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_flax_utils.py#L163",parametersDescription:[{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.input_ids",description:`<strong>input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.trace",description:`<strong>trace</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to trace generation. Setting <code>trace=False</code> should only be used for debugging and will lead to a
considerably slower runtime.`,name:"trace"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.params",description:`<strong>params</strong> (<code>Dict[str, jnp.ndarray]</code>, <em>optional</em>) &#x2014;
Optionally the model parameters can be passed. Can be useful for parallelized generation.
model<em>kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with *decoder</em>*. Also accepts <code>encoder_outputs</code> to skip encoder part.`,name:"params"}],returnDescription:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a>.</p>
`}}),Ye=new Rm({props:{warning:"&lcub;true}",$$slots:{default:[Jm]},$$scope:{ctx:it}}}),Qt=new ye({props:{code:`from transformers import AutoTokenizer, FlaxAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
model = FlaxAutoModelForCausalLM.from_pretrained("distilgpt2")
input_context = "The dog"
# encode input context
input_ids = tokenizer(input_context, return_tensors="np").input_ids
# generate candidates using sampling
outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_context = <span class="hljs-string">&quot;The dog&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># encode input context</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_context, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate candidates using sampling</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids=input_ids, max_length=<span class="hljs-number">20</span>, top_k=<span class="hljs-number">30</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)`}}),{c(){p=o("meta"),P=l(),j=o("h1"),T=o("a"),I=o("span"),m(E.$$.fragment),xe=l(),W=o("span"),G=t("Generation"),B=l(),v=o("p"),$=t("The methods for auto-regressive text generation, namely "),b=o("a"),Me=t("generate()"),Le=t(" (for the PyTorch models), "),H=o("a"),Io=t("generate()"),Wo=t(" (for the TensorFlow models) and "),en=o("a"),Bo=t("generate()"),Ho=t(" (for the Flax/JAX models), are implemented in "),tn=o("a"),Ro=t("GenerationMixin"),Uo=t(", "),nn=o("a"),Vo=t("TFGenerationMixin"),Ko=t(" and "),sn=o("a"),Zo=t("FlaxGenerationMixin"),Xo=t(" respectively."),uo=l(),S=o("p"),Jo=t("The "),On=o("code"),Qo=t("GenerationMixin"),Yo=t(" classes are inherited by the corresponding base model classes, "),qn=o("em"),ea=t("e.g."),ta=l(),on=o("a"),na=t("PreTrainedModel"),sa=t(", "),an=o("a"),oa=t("TFPreTrainedModel"),aa=t(", and "),rn=o("a"),ra=t("FlaxPreTrainedModel"),ia=t(` respectively, therefore exposing all
methods for auto-regressive text generation to every model class.`),_o=l(),we=o("h2"),Ze=o("a"),Gn=o("span"),m(lt.$$.fragment),la=l(),Sn=o("span"),da=t("GenerationMixn"),ho=l(),k=o("div"),m(dt.$$.fragment),ca=l(),ct=o("p"),pa=t("A class containing all functions for auto-regressive text generation, to be used as a mixin in "),ln=o("a"),ma=t("PreTrainedModel"),ga=t("."),ua=l(),pt=o("p"),_a=t("The class exposes "),dn=o("a"),ha=t("generate()"),fa=t(", which can be used for:"),ba=l(),F=o("ul"),R=o("li"),$n=o("em"),xa=t("greedy decoding"),ka=t(" by calling "),cn=o("a"),va=t("greedy_search()"),ya=t(" if "),Fn=o("code"),ja=t("num_beams=1"),Ma=t(` and
`),An=o("code"),La=t("do_sample=False"),wa=t("."),Ta=l(),U=o("li"),zn=o("em"),Ea=t("multinomial sampling"),Oa=t(" by calling "),pn=o("a"),qa=t("sample()"),Ga=t(" if "),Pn=o("code"),Sa=t("num_beams=1"),$a=t(` and
`),Nn=o("code"),Fa=t("do_sample=True"),Aa=t("."),za=l(),V=o("li"),Dn=o("em"),Pa=t("beam-search decoding"),Na=t(" by calling "),mn=o("a"),Da=t("beam_search()"),Ca=t(" if "),Cn=o("code"),Ia=t("num_beams>1"),Wa=t(` and
`),In=o("code"),Ba=t("do_sample=False"),Ha=t("."),Ra=l(),K=o("li"),Wn=o("em"),Ua=t("beam-search multinomial sampling"),Va=t(" by calling "),gn=o("a"),Ka=t("beam_sample()"),Za=t(` if
`),Bn=o("code"),Xa=t("num_beams>1"),Ja=t(" and "),Hn=o("code"),Qa=t("do_sample=True"),Ya=t("."),er=l(),Z=o("li"),Rn=o("em"),tr=t("diverse beam-search decoding"),nr=t(" by calling "),un=o("a"),sr=t("group_beam_search()"),or=t(`, if
`),Un=o("code"),ar=t("num_beams>1"),rr=t(" and "),Vn=o("code"),ir=t("num_beam_groups>1"),lr=t("."),dr=l(),X=o("li"),Kn=o("em"),cr=t("constrained beam-search decoding"),pr=t(" by calling "),_n=o("a"),mr=t("constrained_beam_search()"),gr=t(`,
if `),Zn=o("code"),ur=t("constraints!=None"),_r=t(" or "),Xn=o("code"),hr=t("force_words_ids!=None"),fr=t("."),br=l(),x=o("div"),m(mt.$$.fragment),xr=l(),Jn=o("p"),kr=t(`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),vr=l(),A=o("ul"),J=o("li"),Qn=o("em"),yr=t("greedy decoding"),jr=t(" by calling "),hn=o("a"),Mr=t("greedy_search()"),Lr=t(" if "),Yn=o("code"),wr=t("num_beams=1"),Tr=t(` and
`),es=o("code"),Er=t("do_sample=False"),Or=t("."),qr=l(),Q=o("li"),ts=o("em"),Gr=t("multinomial sampling"),Sr=t(" by calling "),fn=o("a"),$r=t("sample()"),Fr=t(" if "),ns=o("code"),Ar=t("num_beams=1"),zr=t(` and
`),ss=o("code"),Pr=t("do_sample=True"),Nr=t("."),Dr=l(),Y=o("li"),os=o("em"),Cr=t("beam-search decoding"),Ir=t(" by calling "),bn=o("a"),Wr=t("beam_search()"),Br=t(" if "),as=o("code"),Hr=t("num_beams>1"),Rr=t(` and
`),rs=o("code"),Ur=t("do_sample=False"),Vr=t("."),Kr=l(),ee=o("li"),is=o("em"),Zr=t("beam-search multinomial sampling"),Xr=t(" by calling "),xn=o("a"),Jr=t("beam_sample()"),Qr=t(` if
`),ls=o("code"),Yr=t("num_beams>1"),ei=t(" and "),ds=o("code"),ti=t("do_sample=True"),ni=t("."),si=l(),te=o("li"),cs=o("em"),oi=t("diverse beam-search decoding"),ai=t(" by calling "),kn=o("a"),ri=t("group_beam_search()"),ii=t(`, if
`),ps=o("code"),li=t("num_beams>1"),di=t(" and "),ms=o("code"),ci=t("num_beam_groups>1"),pi=t("."),mi=l(),ne=o("li"),gs=o("em"),gi=t("constrained beam-search decoding"),ui=t(` by calling
`),vn=o("a"),_i=t("constrained_beam_search()"),hi=t(", if "),us=o("code"),fi=t("constraints!=None"),bi=t(` or
`),_s=o("code"),xi=t("force_words_ids!=None"),ki=t("."),vi=l(),m(Xe.$$.fragment),yi=l(),gt=o("p"),ji=t("Most of these parameters are explained in more detail in "),ut=o("a"),Mi=t(`this blog
post`),Li=t("."),wi=l(),hs=o("p"),Ti=t("Examples:"),Ei=l(),fs=o("p"),Oi=t("Greedy Decoding:"),qi=l(),m(_t.$$.fragment),Gi=l(),bs=o("p"),Si=t("Multinomial Sampling:"),$i=l(),m(ht.$$.fragment),Fi=l(),xs=o("p"),Ai=t("Beam-search decoding:"),zi=l(),m(ft.$$.fragment),Pi=l(),se=o("div"),m(bt.$$.fragment),Ni=l(),xt=o("p"),Di=t("Generates sequences of token ids for models with a language modeling head using "),ks=o("strong"),Ci=t("greedy decoding"),Ii=t(` and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Wi=l(),vs=o("p"),Bi=t("Examples:"),Hi=l(),m(kt.$$.fragment),Ri=l(),oe=o("div"),m(vt.$$.fragment),Ui=l(),yt=o("p"),Vi=t("Generates sequences of token ids for models with a language modeling head using "),ys=o("strong"),Ki=t("multinomial sampling"),Zi=t(` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Xi=l(),js=o("p"),Ji=t("Examples:"),Qi=l(),m(jt.$$.fragment),Yi=l(),ae=o("div"),m(Mt.$$.fragment),el=l(),Lt=o("p"),tl=t("Generates sequences of token ids for models with a language modeling head using "),Ms=o("strong"),nl=t("beam search decoding"),sl=t(` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),ol=l(),Ls=o("p"),al=t("Examples:"),rl=l(),m(wt.$$.fragment),il=l(),re=o("div"),m(Tt.$$.fragment),ll=l(),Et=o("p"),dl=t("Generates sequences of token ids for models with a language modeling head using "),ws=o("strong"),cl=t(`beam search multinomial
sampling`),pl=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),ml=l(),Ts=o("p"),gl=t("Examples:"),ul=l(),m(Ot.$$.fragment),_l=l(),ie=o("div"),m(qt.$$.fragment),hl=l(),Gt=o("p"),fl=t("Generates sequences of token ids for models with a language modeling head using "),Es=o("strong"),bl=t(`diverse beam search
decoding`),xl=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),kl=l(),Os=o("p"),vl=t("Examples:"),yl=l(),m(St.$$.fragment),jl=l(),le=o("div"),m($t.$$.fragment),Ml=l(),Ft=o("p"),Ll=t("Generates sequences of token ids for models with a language modeling head using "),qs=o("strong"),wl=t(`constrained beam search
decoding`),Tl=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),El=l(),Gs=o("p"),Ol=t("Examples:"),ql=l(),m(At.$$.fragment),fo=l(),Te=o("h2"),Je=o("a"),Ss=o("span"),m(zt.$$.fragment),Gl=l(),$s=o("span"),Sl=t("TFGenerationMixn"),bo=l(),ke=o("div"),m(Pt.$$.fragment),$l=l(),Nt=o("p"),Fl=t("A class containing all of the functions supporting generation, to be used as a mixin in "),yn=o("a"),Al=t("TFPreTrainedModel"),zl=t("."),Pl=l(),O=o("div"),m(Dt.$$.fragment),Nl=l(),Fs=o("p"),Dl=t(`Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.`),Cl=l(),Ct=o("p"),Il=t("Adapted in part from "),It=o("a"),Wl=t(`Facebook\u2019s XLM beam search
code`),Bl=t("."),Hl=l(),ve=o("p"),Rl=t("Apart from "),As=o("code"),Ul=t("input_ids"),Vl=t(" and "),zs=o("code"),Kl=t("attention_mask"),Zl=t(`, all the arguments below will default to the value of the attribute
of the same name inside the `),jn=o("a"),Xl=t("PretrainedConfig"),Jl=t(` of the model. The default values indicated are the default
values of those config.`),Ql=l(),Wt=o("p"),Yl=t("Most of these parameters are explained in more detail in "),Bt=o("a"),ed=t(`this blog
post`),td=t("."),nd=l(),Ps=o("p"),sd=t("Examples:"),od=l(),m(Ht.$$.fragment),xo=l(),Ee=o("h2"),Qe=o("a"),Ns=o("span"),m(Rt.$$.fragment),ad=l(),Ds=o("span"),rd=t("FlaxGenerationMixn"),ko=l(),z=o("div"),m(Ut.$$.fragment),id=l(),Vt=o("p"),ld=t(`A class containing all functions for auto-regressive text generation, to be used as a mixin in
`),Mn=o("a"),dd=t("FlaxPreTrainedModel"),cd=t("."),pd=l(),Kt=o("p"),md=t("The class exposes "),Ln=o("a"),gd=t("generate()"),ud=t(", which can be used for:"),_d=l(),Oe=o("ul"),de=o("li"),Cs=o("em"),hd=t("greedy decoding"),fd=t(" by calling "),Is=o("code"),bd=t("_greedy_search()"),xd=t(`if
`),Ws=o("code"),kd=t("num_beams=1"),vd=t(" and "),Bs=o("code"),yd=t("do_sample=False"),jd=t("."),Md=l(),ce=o("li"),Hs=o("em"),Ld=t("multinomial sampling"),wd=t(" by calling "),Rs=o("code"),Td=t("_sample()"),Ed=t("if "),Us=o("code"),Od=t("num_beams=1"),qd=t(`
and `),Vs=o("code"),Gd=t("do_sample=True"),Sd=t("."),$d=l(),pe=o("li"),Ks=o("em"),Fd=t("beam-search decoding"),Ad=t(" by calling "),Zs=o("code"),zd=t("_beam_search"),Pd=t(" if "),Xs=o("code"),Nd=t("num_beams>1"),Dd=t(`
and `),Js=o("code"),Cd=t("do_sample=False"),Id=t("."),Wd=l(),q=o("div"),m(Zt.$$.fragment),Bd=l(),Qs=o("p"),Hd=t(`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),Rd=l(),qe=o("ul"),me=o("li"),Ys=o("em"),Ud=t("greedy decoding"),Vd=t(" by calling "),eo=o("code"),Kd=t("_greedy_search()"),Zd=t(`if
`),to=o("code"),Xd=t("num_beams=1"),Jd=t(" and "),no=o("code"),Qd=t("do_sample=False"),Yd=t("."),ec=l(),ge=o("li"),so=o("em"),tc=t("multinomial sampling"),nc=t(" by calling "),oo=o("code"),sc=t("_sample()"),oc=t("if "),ao=o("code"),ac=t("num_beams=1"),rc=t(`
and `),ro=o("code"),ic=t("do_sample=True"),lc=t("."),dc=l(),ue=o("li"),io=o("em"),cc=t("beam-search decoding"),pc=t(" by calling "),lo=o("code"),mc=t("_beam_search"),gc=t(" if "),co=o("code"),uc=t("num_beams>1"),_c=t(`
and `),po=o("code"),hc=t("do_sample=False"),fc=t("."),bc=l(),m(Ye.$$.fragment),xc=l(),Xt=o("p"),kc=t("Most of these parameters are explained in more detail in "),Jt=o("a"),vc=t(`this blog
post`),yc=t("."),jc=l(),mo=o("p"),Mc=t("Examples:"),Lc=l(),m(Qt.$$.fragment),this.h()},l(i){const y=Zm('[data-svelte="svelte-1phssyn"]',document.head);p=a(y,"META",{name:!0,content:!0}),y.forEach(s),P=d(i),j=a(i,"H1",{class:!0});var Yt=r(j);T=a(Yt,"A",{id:!0,class:!0,href:!0});var go=r(T);I=a(go,"SPAN",{});var wc=r(I);g(E.$$.fragment,wc),wc.forEach(s),go.forEach(s),xe=d(Yt),W=a(Yt,"SPAN",{});var Tc=r(W);G=n(Tc,"Generation"),Tc.forEach(s),Yt.forEach(s),B=d(i),v=a(i,"P",{});var N=r(v);$=n(N,"The methods for auto-regressive text generation, namely "),b=a(N,"A",{href:!0});var Ec=r(b);Me=n(Ec,"generate()"),Ec.forEach(s),Le=n(N," (for the PyTorch models), "),H=a(N,"A",{href:!0});var Oc=r(H);Io=n(Oc,"generate()"),Oc.forEach(s),Wo=n(N," (for the TensorFlow models) and "),en=a(N,"A",{href:!0});var qc=r(en);Bo=n(qc,"generate()"),qc.forEach(s),Ho=n(N," (for the Flax/JAX models), are implemented in "),tn=a(N,"A",{href:!0});var Gc=r(tn);Ro=n(Gc,"GenerationMixin"),Gc.forEach(s),Uo=n(N,", "),nn=a(N,"A",{href:!0});var Sc=r(nn);Vo=n(Sc,"TFGenerationMixin"),Sc.forEach(s),Ko=n(N," and "),sn=a(N,"A",{href:!0});var $c=r(sn);Zo=n($c,"FlaxGenerationMixin"),$c.forEach(s),Xo=n(N," respectively."),N.forEach(s),uo=d(i),S=a(i,"P",{});var _e=r(S);Jo=n(_e,"The "),On=a(_e,"CODE",{});var Fc=r(On);Qo=n(Fc,"GenerationMixin"),Fc.forEach(s),Yo=n(_e," classes are inherited by the corresponding base model classes, "),qn=a(_e,"EM",{});var Ac=r(qn);ea=n(Ac,"e.g."),Ac.forEach(s),ta=d(_e),on=a(_e,"A",{href:!0});var zc=r(on);na=n(zc,"PreTrainedModel"),zc.forEach(s),sa=n(_e,", "),an=a(_e,"A",{href:!0});var Pc=r(an);oa=n(Pc,"TFPreTrainedModel"),Pc.forEach(s),aa=n(_e,", and "),rn=a(_e,"A",{href:!0});var Nc=r(rn);ra=n(Nc,"FlaxPreTrainedModel"),Nc.forEach(s),ia=n(_e,` respectively, therefore exposing all
methods for auto-regressive text generation to every model class.`),_e.forEach(s),_o=d(i),we=a(i,"H2",{class:!0});var yo=r(we);Ze=a(yo,"A",{id:!0,class:!0,href:!0});var Dc=r(Ze);Gn=a(Dc,"SPAN",{});var Cc=r(Gn);g(lt.$$.fragment,Cc),Cc.forEach(s),Dc.forEach(s),la=d(yo),Sn=a(yo,"SPAN",{});var Ic=r(Sn);da=n(Ic,"GenerationMixn"),Ic.forEach(s),yo.forEach(s),ho=d(i),k=a(i,"DIV",{class:!0});var L=r(k);g(dt.$$.fragment,L),ca=d(L),ct=a(L,"P",{});var jo=r(ct);pa=n(jo,"A class containing all functions for auto-regressive text generation, to be used as a mixin in "),ln=a(jo,"A",{href:!0});var Wc=r(ln);ma=n(Wc,"PreTrainedModel"),Wc.forEach(s),ga=n(jo,"."),jo.forEach(s),ua=d(L),pt=a(L,"P",{});var Mo=r(pt);_a=n(Mo,"The class exposes "),dn=a(Mo,"A",{href:!0});var Bc=r(dn);ha=n(Bc,"generate()"),Bc.forEach(s),fa=n(Mo,", which can be used for:"),Mo.forEach(s),ba=d(L),F=a(L,"UL",{});var he=r(F);R=a(he,"LI",{});var Ge=r(R);$n=a(Ge,"EM",{});var Hc=r($n);xa=n(Hc,"greedy decoding"),Hc.forEach(s),ka=n(Ge," by calling "),cn=a(Ge,"A",{href:!0});var Rc=r(cn);va=n(Rc,"greedy_search()"),Rc.forEach(s),ya=n(Ge," if "),Fn=a(Ge,"CODE",{});var Uc=r(Fn);ja=n(Uc,"num_beams=1"),Uc.forEach(s),Ma=n(Ge,` and
`),An=a(Ge,"CODE",{});var Vc=r(An);La=n(Vc,"do_sample=False"),Vc.forEach(s),wa=n(Ge,"."),Ge.forEach(s),Ta=d(he),U=a(he,"LI",{});var Se=r(U);zn=a(Se,"EM",{});var Kc=r(zn);Ea=n(Kc,"multinomial sampling"),Kc.forEach(s),Oa=n(Se," by calling "),pn=a(Se,"A",{href:!0});var Zc=r(pn);qa=n(Zc,"sample()"),Zc.forEach(s),Ga=n(Se," if "),Pn=a(Se,"CODE",{});var Xc=r(Pn);Sa=n(Xc,"num_beams=1"),Xc.forEach(s),$a=n(Se,` and
`),Nn=a(Se,"CODE",{});var Jc=r(Nn);Fa=n(Jc,"do_sample=True"),Jc.forEach(s),Aa=n(Se,"."),Se.forEach(s),za=d(he),V=a(he,"LI",{});var $e=r(V);Dn=a($e,"EM",{});var Qc=r(Dn);Pa=n(Qc,"beam-search decoding"),Qc.forEach(s),Na=n($e," by calling "),mn=a($e,"A",{href:!0});var Yc=r(mn);Da=n(Yc,"beam_search()"),Yc.forEach(s),Ca=n($e," if "),Cn=a($e,"CODE",{});var ep=r(Cn);Ia=n(ep,"num_beams>1"),ep.forEach(s),Wa=n($e,` and
`),In=a($e,"CODE",{});var tp=r(In);Ba=n(tp,"do_sample=False"),tp.forEach(s),Ha=n($e,"."),$e.forEach(s),Ra=d(he),K=a(he,"LI",{});var Fe=r(K);Wn=a(Fe,"EM",{});var np=r(Wn);Ua=n(np,"beam-search multinomial sampling"),np.forEach(s),Va=n(Fe," by calling "),gn=a(Fe,"A",{href:!0});var sp=r(gn);Ka=n(sp,"beam_sample()"),sp.forEach(s),Za=n(Fe,` if
`),Bn=a(Fe,"CODE",{});var op=r(Bn);Xa=n(op,"num_beams>1"),op.forEach(s),Ja=n(Fe," and "),Hn=a(Fe,"CODE",{});var ap=r(Hn);Qa=n(ap,"do_sample=True"),ap.forEach(s),Ya=n(Fe,"."),Fe.forEach(s),er=d(he),Z=a(he,"LI",{});var Ae=r(Z);Rn=a(Ae,"EM",{});var rp=r(Rn);tr=n(rp,"diverse beam-search decoding"),rp.forEach(s),nr=n(Ae," by calling "),un=a(Ae,"A",{href:!0});var ip=r(un);sr=n(ip,"group_beam_search()"),ip.forEach(s),or=n(Ae,`, if
`),Un=a(Ae,"CODE",{});var lp=r(Un);ar=n(lp,"num_beams>1"),lp.forEach(s),rr=n(Ae," and "),Vn=a(Ae,"CODE",{});var dp=r(Vn);ir=n(dp,"num_beam_groups>1"),dp.forEach(s),lr=n(Ae,"."),Ae.forEach(s),dr=d(he),X=a(he,"LI",{});var ze=r(X);Kn=a(ze,"EM",{});var cp=r(Kn);cr=n(cp,"constrained beam-search decoding"),cp.forEach(s),pr=n(ze," by calling "),_n=a(ze,"A",{href:!0});var pp=r(_n);mr=n(pp,"constrained_beam_search()"),pp.forEach(s),gr=n(ze,`,
if `),Zn=a(ze,"CODE",{});var mp=r(Zn);ur=n(mp,"constraints!=None"),mp.forEach(s),_r=n(ze," or "),Xn=a(ze,"CODE",{});var gp=r(Xn);hr=n(gp,"force_words_ids!=None"),gp.forEach(s),fr=n(ze,"."),ze.forEach(s),he.forEach(s),br=d(L),x=a(L,"DIV",{class:!0});var M=r(x);g(mt.$$.fragment,M),xr=d(M),Jn=a(M,"P",{});var up=r(Jn);kr=n(up,`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),up.forEach(s),vr=d(M),A=a(M,"UL",{});var fe=r(A);J=a(fe,"LI",{});var Pe=r(J);Qn=a(Pe,"EM",{});var _p=r(Qn);yr=n(_p,"greedy decoding"),_p.forEach(s),jr=n(Pe," by calling "),hn=a(Pe,"A",{href:!0});var hp=r(hn);Mr=n(hp,"greedy_search()"),hp.forEach(s),Lr=n(Pe," if "),Yn=a(Pe,"CODE",{});var fp=r(Yn);wr=n(fp,"num_beams=1"),fp.forEach(s),Tr=n(Pe,` and
`),es=a(Pe,"CODE",{});var bp=r(es);Er=n(bp,"do_sample=False"),bp.forEach(s),Or=n(Pe,"."),Pe.forEach(s),qr=d(fe),Q=a(fe,"LI",{});var Ne=r(Q);ts=a(Ne,"EM",{});var xp=r(ts);Gr=n(xp,"multinomial sampling"),xp.forEach(s),Sr=n(Ne," by calling "),fn=a(Ne,"A",{href:!0});var kp=r(fn);$r=n(kp,"sample()"),kp.forEach(s),Fr=n(Ne," if "),ns=a(Ne,"CODE",{});var vp=r(ns);Ar=n(vp,"num_beams=1"),vp.forEach(s),zr=n(Ne,` and
`),ss=a(Ne,"CODE",{});var yp=r(ss);Pr=n(yp,"do_sample=True"),yp.forEach(s),Nr=n(Ne,"."),Ne.forEach(s),Dr=d(fe),Y=a(fe,"LI",{});var De=r(Y);os=a(De,"EM",{});var jp=r(os);Cr=n(jp,"beam-search decoding"),jp.forEach(s),Ir=n(De," by calling "),bn=a(De,"A",{href:!0});var Mp=r(bn);Wr=n(Mp,"beam_search()"),Mp.forEach(s),Br=n(De," if "),as=a(De,"CODE",{});var Lp=r(as);Hr=n(Lp,"num_beams>1"),Lp.forEach(s),Rr=n(De,` and
`),rs=a(De,"CODE",{});var wp=r(rs);Ur=n(wp,"do_sample=False"),wp.forEach(s),Vr=n(De,"."),De.forEach(s),Kr=d(fe),ee=a(fe,"LI",{});var Ce=r(ee);is=a(Ce,"EM",{});var Tp=r(is);Zr=n(Tp,"beam-search multinomial sampling"),Tp.forEach(s),Xr=n(Ce," by calling "),xn=a(Ce,"A",{href:!0});var Ep=r(xn);Jr=n(Ep,"beam_sample()"),Ep.forEach(s),Qr=n(Ce,` if
`),ls=a(Ce,"CODE",{});var Op=r(ls);Yr=n(Op,"num_beams>1"),Op.forEach(s),ei=n(Ce," and "),ds=a(Ce,"CODE",{});var qp=r(ds);ti=n(qp,"do_sample=True"),qp.forEach(s),ni=n(Ce,"."),Ce.forEach(s),si=d(fe),te=a(fe,"LI",{});var Ie=r(te);cs=a(Ie,"EM",{});var Gp=r(cs);oi=n(Gp,"diverse beam-search decoding"),Gp.forEach(s),ai=n(Ie," by calling "),kn=a(Ie,"A",{href:!0});var Sp=r(kn);ri=n(Sp,"group_beam_search()"),Sp.forEach(s),ii=n(Ie,`, if
`),ps=a(Ie,"CODE",{});var $p=r(ps);li=n($p,"num_beams>1"),$p.forEach(s),di=n(Ie," and "),ms=a(Ie,"CODE",{});var Fp=r(ms);ci=n(Fp,"num_beam_groups>1"),Fp.forEach(s),pi=n(Ie,"."),Ie.forEach(s),mi=d(fe),ne=a(fe,"LI",{});var We=r(ne);gs=a(We,"EM",{});var Ap=r(gs);gi=n(Ap,"constrained beam-search decoding"),Ap.forEach(s),ui=n(We,` by calling
`),vn=a(We,"A",{href:!0});var zp=r(vn);_i=n(zp,"constrained_beam_search()"),zp.forEach(s),hi=n(We,", if "),us=a(We,"CODE",{});var Pp=r(us);fi=n(Pp,"constraints!=None"),Pp.forEach(s),bi=n(We,` or
`),_s=a(We,"CODE",{});var Np=r(_s);xi=n(Np,"force_words_ids!=None"),Np.forEach(s),ki=n(We,"."),We.forEach(s),fe.forEach(s),vi=d(M),g(Xe.$$.fragment,M),yi=d(M),gt=a(M,"P",{});var Lo=r(gt);ji=n(Lo,"Most of these parameters are explained in more detail in "),ut=a(Lo,"A",{href:!0,rel:!0});var Dp=r(ut);Mi=n(Dp,`this blog
post`),Dp.forEach(s),Li=n(Lo,"."),Lo.forEach(s),wi=d(M),hs=a(M,"P",{});var Cp=r(hs);Ti=n(Cp,"Examples:"),Cp.forEach(s),Ei=d(M),fs=a(M,"P",{});var Ip=r(fs);Oi=n(Ip,"Greedy Decoding:"),Ip.forEach(s),qi=d(M),g(_t.$$.fragment,M),Gi=d(M),bs=a(M,"P",{});var Wp=r(bs);Si=n(Wp,"Multinomial Sampling:"),Wp.forEach(s),$i=d(M),g(ht.$$.fragment,M),Fi=d(M),xs=a(M,"P",{});var Bp=r(xs);Ai=n(Bp,"Beam-search decoding:"),Bp.forEach(s),zi=d(M),g(ft.$$.fragment,M),M.forEach(s),Pi=d(L),se=a(L,"DIV",{class:!0});var et=r(se);g(bt.$$.fragment,et),Ni=d(et),xt=a(et,"P",{});var wo=r(xt);Di=n(wo,"Generates sequences of token ids for models with a language modeling head using "),ks=a(wo,"STRONG",{});var Hp=r(ks);Ci=n(Hp,"greedy decoding"),Hp.forEach(s),Ii=n(wo,` and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),wo.forEach(s),Wi=d(et),vs=a(et,"P",{});var Rp=r(vs);Bi=n(Rp,"Examples:"),Rp.forEach(s),Hi=d(et),g(kt.$$.fragment,et),et.forEach(s),Ri=d(L),oe=a(L,"DIV",{class:!0});var tt=r(oe);g(vt.$$.fragment,tt),Ui=d(tt),yt=a(tt,"P",{});var To=r(yt);Vi=n(To,"Generates sequences of token ids for models with a language modeling head using "),ys=a(To,"STRONG",{});var Up=r(ys);Ki=n(Up,"multinomial sampling"),Up.forEach(s),Zi=n(To,` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),To.forEach(s),Xi=d(tt),js=a(tt,"P",{});var Vp=r(js);Ji=n(Vp,"Examples:"),Vp.forEach(s),Qi=d(tt),g(jt.$$.fragment,tt),tt.forEach(s),Yi=d(L),ae=a(L,"DIV",{class:!0});var nt=r(ae);g(Mt.$$.fragment,nt),el=d(nt),Lt=a(nt,"P",{});var Eo=r(Lt);tl=n(Eo,"Generates sequences of token ids for models with a language modeling head using "),Ms=a(Eo,"STRONG",{});var Kp=r(Ms);nl=n(Kp,"beam search decoding"),Kp.forEach(s),sl=n(Eo,` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Eo.forEach(s),ol=d(nt),Ls=a(nt,"P",{});var Zp=r(Ls);al=n(Zp,"Examples:"),Zp.forEach(s),rl=d(nt),g(wt.$$.fragment,nt),nt.forEach(s),il=d(L),re=a(L,"DIV",{class:!0});var st=r(re);g(Tt.$$.fragment,st),ll=d(st),Et=a(st,"P",{});var Oo=r(Et);dl=n(Oo,"Generates sequences of token ids for models with a language modeling head using "),ws=a(Oo,"STRONG",{});var Xp=r(ws);cl=n(Xp,`beam search multinomial
sampling`),Xp.forEach(s),pl=n(Oo," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),Oo.forEach(s),ml=d(st),Ts=a(st,"P",{});var Jp=r(Ts);gl=n(Jp,"Examples:"),Jp.forEach(s),ul=d(st),g(Ot.$$.fragment,st),st.forEach(s),_l=d(L),ie=a(L,"DIV",{class:!0});var ot=r(ie);g(qt.$$.fragment,ot),hl=d(ot),Gt=a(ot,"P",{});var qo=r(Gt);fl=n(qo,"Generates sequences of token ids for models with a language modeling head using "),Es=a(qo,"STRONG",{});var Qp=r(Es);bl=n(Qp,`diverse beam search
decoding`),Qp.forEach(s),xl=n(qo," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),qo.forEach(s),kl=d(ot),Os=a(ot,"P",{});var Yp=r(Os);vl=n(Yp,"Examples:"),Yp.forEach(s),yl=d(ot),g(St.$$.fragment,ot),ot.forEach(s),jl=d(L),le=a(L,"DIV",{class:!0});var at=r(le);g($t.$$.fragment,at),Ml=d(at),Ft=a(at,"P",{});var Go=r(Ft);Ll=n(Go,"Generates sequences of token ids for models with a language modeling head using "),qs=a(Go,"STRONG",{});var em=r(qs);wl=n(em,`constrained beam search
decoding`),em.forEach(s),Tl=n(Go," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),Go.forEach(s),El=d(at),Gs=a(at,"P",{});var tm=r(Gs);Ol=n(tm,"Examples:"),tm.forEach(s),ql=d(at),g(At.$$.fragment,at),at.forEach(s),L.forEach(s),fo=d(i),Te=a(i,"H2",{class:!0});var So=r(Te);Je=a(So,"A",{id:!0,class:!0,href:!0});var nm=r(Je);Ss=a(nm,"SPAN",{});var sm=r(Ss);g(zt.$$.fragment,sm),sm.forEach(s),nm.forEach(s),Gl=d(So),$s=a(So,"SPAN",{});var om=r($s);Sl=n(om,"TFGenerationMixn"),om.forEach(s),So.forEach(s),bo=d(i),ke=a(i,"DIV",{class:!0});var wn=r(ke);g(Pt.$$.fragment,wn),$l=d(wn),Nt=a(wn,"P",{});var $o=r(Nt);Fl=n($o,"A class containing all of the functions supporting generation, to be used as a mixin in "),yn=a($o,"A",{href:!0});var am=r(yn);Al=n(am,"TFPreTrainedModel"),am.forEach(s),zl=n($o,"."),$o.forEach(s),Pl=d(wn),O=a(wn,"DIV",{class:!0});var D=r(O);g(Dt.$$.fragment,D),Nl=d(D),Fs=a(D,"P",{});var rm=r(Fs);Dl=n(rm,`Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.`),rm.forEach(s),Cl=d(D),Ct=a(D,"P",{});var Fo=r(Ct);Il=n(Fo,"Adapted in part from "),It=a(Fo,"A",{href:!0,rel:!0});var im=r(It);Wl=n(im,`Facebook\u2019s XLM beam search
code`),im.forEach(s),Bl=n(Fo,"."),Fo.forEach(s),Hl=d(D),ve=a(D,"P",{});var rt=r(ve);Rl=n(rt,"Apart from "),As=a(rt,"CODE",{});var lm=r(As);Ul=n(lm,"input_ids"),lm.forEach(s),Vl=n(rt," and "),zs=a(rt,"CODE",{});var dm=r(zs);Kl=n(dm,"attention_mask"),dm.forEach(s),Zl=n(rt,`, all the arguments below will default to the value of the attribute
of the same name inside the `),jn=a(rt,"A",{href:!0});var cm=r(jn);Xl=n(cm,"PretrainedConfig"),cm.forEach(s),Jl=n(rt,` of the model. The default values indicated are the default
values of those config.`),rt.forEach(s),Ql=d(D),Wt=a(D,"P",{});var Ao=r(Wt);Yl=n(Ao,"Most of these parameters are explained in more detail in "),Bt=a(Ao,"A",{href:!0,rel:!0});var pm=r(Bt);ed=n(pm,`this blog
post`),pm.forEach(s),td=n(Ao,"."),Ao.forEach(s),nd=d(D),Ps=a(D,"P",{});var mm=r(Ps);sd=n(mm,"Examples:"),mm.forEach(s),od=d(D),g(Ht.$$.fragment,D),D.forEach(s),wn.forEach(s),xo=d(i),Ee=a(i,"H2",{class:!0});var zo=r(Ee);Qe=a(zo,"A",{id:!0,class:!0,href:!0});var gm=r(Qe);Ns=a(gm,"SPAN",{});var um=r(Ns);g(Rt.$$.fragment,um),um.forEach(s),gm.forEach(s),ad=d(zo),Ds=a(zo,"SPAN",{});var _m=r(Ds);rd=n(_m,"FlaxGenerationMixn"),_m.forEach(s),zo.forEach(s),ko=d(i),z=a(i,"DIV",{class:!0});var je=r(z);g(Ut.$$.fragment,je),id=d(je),Vt=a(je,"P",{});var Po=r(Vt);ld=n(Po,`A class containing all functions for auto-regressive text generation, to be used as a mixin in
`),Mn=a(Po,"A",{href:!0});var hm=r(Mn);dd=n(hm,"FlaxPreTrainedModel"),hm.forEach(s),cd=n(Po,"."),Po.forEach(s),pd=d(je),Kt=a(je,"P",{});var No=r(Kt);md=n(No,"The class exposes "),Ln=a(No,"A",{href:!0});var fm=r(Ln);gd=n(fm,"generate()"),fm.forEach(s),ud=n(No,", which can be used for:"),No.forEach(s),_d=d(je),Oe=a(je,"UL",{});var Tn=r(Oe);de=a(Tn,"LI",{});var Be=r(de);Cs=a(Be,"EM",{});var bm=r(Cs);hd=n(bm,"greedy decoding"),bm.forEach(s),fd=n(Be," by calling "),Is=a(Be,"CODE",{});var xm=r(Is);bd=n(xm,"_greedy_search()"),xm.forEach(s),xd=n(Be,`if
`),Ws=a(Be,"CODE",{});var km=r(Ws);kd=n(km,"num_beams=1"),km.forEach(s),vd=n(Be," and "),Bs=a(Be,"CODE",{});var vm=r(Bs);yd=n(vm,"do_sample=False"),vm.forEach(s),jd=n(Be,"."),Be.forEach(s),Md=d(Tn),ce=a(Tn,"LI",{});var He=r(ce);Hs=a(He,"EM",{});var ym=r(Hs);Ld=n(ym,"multinomial sampling"),ym.forEach(s),wd=n(He," by calling "),Rs=a(He,"CODE",{});var jm=r(Rs);Td=n(jm,"_sample()"),jm.forEach(s),Ed=n(He,"if "),Us=a(He,"CODE",{});var Mm=r(Us);Od=n(Mm,"num_beams=1"),Mm.forEach(s),qd=n(He,`
and `),Vs=a(He,"CODE",{});var Lm=r(Vs);Gd=n(Lm,"do_sample=True"),Lm.forEach(s),Sd=n(He,"."),He.forEach(s),$d=d(Tn),pe=a(Tn,"LI",{});var Re=r(pe);Ks=a(Re,"EM",{});var wm=r(Ks);Fd=n(wm,"beam-search decoding"),wm.forEach(s),Ad=n(Re," by calling "),Zs=a(Re,"CODE",{});var Tm=r(Zs);zd=n(Tm,"_beam_search"),Tm.forEach(s),Pd=n(Re," if "),Xs=a(Re,"CODE",{});var Em=r(Xs);Nd=n(Em,"num_beams>1"),Em.forEach(s),Dd=n(Re,`
and `),Js=a(Re,"CODE",{});var Om=r(Js);Cd=n(Om,"do_sample=False"),Om.forEach(s),Id=n(Re,"."),Re.forEach(s),Tn.forEach(s),Wd=d(je),q=a(je,"DIV",{class:!0});var C=r(q);g(Zt.$$.fragment,C),Bd=d(C),Qs=a(C,"P",{});var qm=r(Qs);Hd=n(qm,`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),qm.forEach(s),Rd=d(C),qe=a(C,"UL",{});var En=r(qe);me=a(En,"LI",{});var Ue=r(me);Ys=a(Ue,"EM",{});var Gm=r(Ys);Ud=n(Gm,"greedy decoding"),Gm.forEach(s),Vd=n(Ue," by calling "),eo=a(Ue,"CODE",{});var Sm=r(eo);Kd=n(Sm,"_greedy_search()"),Sm.forEach(s),Zd=n(Ue,`if
`),to=a(Ue,"CODE",{});var $m=r(to);Xd=n($m,"num_beams=1"),$m.forEach(s),Jd=n(Ue," and "),no=a(Ue,"CODE",{});var Fm=r(no);Qd=n(Fm,"do_sample=False"),Fm.forEach(s),Yd=n(Ue,"."),Ue.forEach(s),ec=d(En),ge=a(En,"LI",{});var Ve=r(ge);so=a(Ve,"EM",{});var Am=r(so);tc=n(Am,"multinomial sampling"),Am.forEach(s),nc=n(Ve," by calling "),oo=a(Ve,"CODE",{});var zm=r(oo);sc=n(zm,"_sample()"),zm.forEach(s),oc=n(Ve,"if "),ao=a(Ve,"CODE",{});var Pm=r(ao);ac=n(Pm,"num_beams=1"),Pm.forEach(s),rc=n(Ve,`
and `),ro=a(Ve,"CODE",{});var Nm=r(ro);ic=n(Nm,"do_sample=True"),Nm.forEach(s),lc=n(Ve,"."),Ve.forEach(s),dc=d(En),ue=a(En,"LI",{});var Ke=r(ue);io=a(Ke,"EM",{});var Dm=r(io);cc=n(Dm,"beam-search decoding"),Dm.forEach(s),pc=n(Ke," by calling "),lo=a(Ke,"CODE",{});var Cm=r(lo);mc=n(Cm,"_beam_search"),Cm.forEach(s),gc=n(Ke," if "),co=a(Ke,"CODE",{});var Im=r(co);uc=n(Im,"num_beams>1"),Im.forEach(s),_c=n(Ke,`
and `),po=a(Ke,"CODE",{});var Wm=r(po);hc=n(Wm,"do_sample=False"),Wm.forEach(s),fc=n(Ke,"."),Ke.forEach(s),En.forEach(s),bc=d(C),g(Ye.$$.fragment,C),xc=d(C),Xt=a(C,"P",{});var Do=r(Xt);kc=n(Do,"Most of these parameters are explained in more detail in "),Jt=a(Do,"A",{href:!0,rel:!0});var Bm=r(Jt);vc=n(Bm,`this blog
post`),Bm.forEach(s),yc=n(Do,"."),Do.forEach(s),jc=d(C),mo=a(C,"P",{});var Hm=r(mo);Mc=n(Hm,"Examples:"),Hm.forEach(s),Lc=d(C),g(Qt.$$.fragment,C),C.forEach(s),je.forEach(s),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Ym)),c(T,"id","generation"),c(T,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(T,"href","#generation"),c(j,"class","relative group"),c(b,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"),c(H,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_tf_utils.TFGenerationMixin.generate"),c(en,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin.generate"),c(tn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin"),c(nn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_tf_utils.TFGenerationMixin"),c(sn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin"),c(on,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(an,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(rn,"href","/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ze,"id","transformers.generation_utils.GenerationMixin"),c(Ze,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ze,"href","#transformers.generation_utils.GenerationMixin"),c(we,"class","relative group"),c(ln,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(dn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"),c(cn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.greedy_search"),c(pn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.sample"),c(mn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_search"),c(gn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_sample"),c(un,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.group_beam_search"),c(_n,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.constrained_beam_search"),c(hn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.greedy_search"),c(fn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.sample"),c(bn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_search"),c(xn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_sample"),c(kn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.group_beam_search"),c(vn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.constrained_beam_search"),c(ut,"href","https://huggingface.co/blog/how-to-generate"),c(ut,"rel","nofollow"),c(x,"class","docstring"),c(se,"class","docstring"),c(oe,"class","docstring"),c(ae,"class","docstring"),c(re,"class","docstring"),c(ie,"class","docstring"),c(le,"class","docstring"),c(k,"class","docstring"),c(Je,"id","transformers.generation_tf_utils.TFGenerationMixin"),c(Je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Je,"href","#transformers.generation_tf_utils.TFGenerationMixin"),c(Te,"class","relative group"),c(yn,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(It,"href","https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529"),c(It,"rel","nofollow"),c(jn,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(Bt,"href","https://huggingface.co/blog/how-to-generate"),c(Bt,"rel","nofollow"),c(O,"class","docstring"),c(ke,"class","docstring"),c(Qe,"id","transformers.generation_flax_utils.FlaxGenerationMixin"),c(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qe,"href","#transformers.generation_flax_utils.FlaxGenerationMixin"),c(Ee,"class","relative group"),c(Mn,"href","/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Ln,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin.generate"),c(Jt,"href","https://huggingface.co/blog/how-to-generate"),c(Jt,"rel","nofollow"),c(q,"class","docstring"),c(z,"class","docstring")},m(i,y){e(document.head,p),w(i,P,y),w(i,j,y),e(j,T),e(T,I),u(E,I,null),e(j,xe),e(j,W),e(W,G),w(i,B,y),w(i,v,y),e(v,$),e(v,b),e(b,Me),e(v,Le),e(v,H),e(H,Io),e(v,Wo),e(v,en),e(en,Bo),e(v,Ho),e(v,tn),e(tn,Ro),e(v,Uo),e(v,nn),e(nn,Vo),e(v,Ko),e(v,sn),e(sn,Zo),e(v,Xo),w(i,uo,y),w(i,S,y),e(S,Jo),e(S,On),e(On,Qo),e(S,Yo),e(S,qn),e(qn,ea),e(S,ta),e(S,on),e(on,na),e(S,sa),e(S,an),e(an,oa),e(S,aa),e(S,rn),e(rn,ra),e(S,ia),w(i,_o,y),w(i,we,y),e(we,Ze),e(Ze,Gn),u(lt,Gn,null),e(we,la),e(we,Sn),e(Sn,da),w(i,ho,y),w(i,k,y),u(dt,k,null),e(k,ca),e(k,ct),e(ct,pa),e(ct,ln),e(ln,ma),e(ct,ga),e(k,ua),e(k,pt),e(pt,_a),e(pt,dn),e(dn,ha),e(pt,fa),e(k,ba),e(k,F),e(F,R),e(R,$n),e($n,xa),e(R,ka),e(R,cn),e(cn,va),e(R,ya),e(R,Fn),e(Fn,ja),e(R,Ma),e(R,An),e(An,La),e(R,wa),e(F,Ta),e(F,U),e(U,zn),e(zn,Ea),e(U,Oa),e(U,pn),e(pn,qa),e(U,Ga),e(U,Pn),e(Pn,Sa),e(U,$a),e(U,Nn),e(Nn,Fa),e(U,Aa),e(F,za),e(F,V),e(V,Dn),e(Dn,Pa),e(V,Na),e(V,mn),e(mn,Da),e(V,Ca),e(V,Cn),e(Cn,Ia),e(V,Wa),e(V,In),e(In,Ba),e(V,Ha),e(F,Ra),e(F,K),e(K,Wn),e(Wn,Ua),e(K,Va),e(K,gn),e(gn,Ka),e(K,Za),e(K,Bn),e(Bn,Xa),e(K,Ja),e(K,Hn),e(Hn,Qa),e(K,Ya),e(F,er),e(F,Z),e(Z,Rn),e(Rn,tr),e(Z,nr),e(Z,un),e(un,sr),e(Z,or),e(Z,Un),e(Un,ar),e(Z,rr),e(Z,Vn),e(Vn,ir),e(Z,lr),e(F,dr),e(F,X),e(X,Kn),e(Kn,cr),e(X,pr),e(X,_n),e(_n,mr),e(X,gr),e(X,Zn),e(Zn,ur),e(X,_r),e(X,Xn),e(Xn,hr),e(X,fr),e(k,br),e(k,x),u(mt,x,null),e(x,xr),e(x,Jn),e(Jn,kr),e(x,vr),e(x,A),e(A,J),e(J,Qn),e(Qn,yr),e(J,jr),e(J,hn),e(hn,Mr),e(J,Lr),e(J,Yn),e(Yn,wr),e(J,Tr),e(J,es),e(es,Er),e(J,Or),e(A,qr),e(A,Q),e(Q,ts),e(ts,Gr),e(Q,Sr),e(Q,fn),e(fn,$r),e(Q,Fr),e(Q,ns),e(ns,Ar),e(Q,zr),e(Q,ss),e(ss,Pr),e(Q,Nr),e(A,Dr),e(A,Y),e(Y,os),e(os,Cr),e(Y,Ir),e(Y,bn),e(bn,Wr),e(Y,Br),e(Y,as),e(as,Hr),e(Y,Rr),e(Y,rs),e(rs,Ur),e(Y,Vr),e(A,Kr),e(A,ee),e(ee,is),e(is,Zr),e(ee,Xr),e(ee,xn),e(xn,Jr),e(ee,Qr),e(ee,ls),e(ls,Yr),e(ee,ei),e(ee,ds),e(ds,ti),e(ee,ni),e(A,si),e(A,te),e(te,cs),e(cs,oi),e(te,ai),e(te,kn),e(kn,ri),e(te,ii),e(te,ps),e(ps,li),e(te,di),e(te,ms),e(ms,ci),e(te,pi),e(A,mi),e(A,ne),e(ne,gs),e(gs,gi),e(ne,ui),e(ne,vn),e(vn,_i),e(ne,hi),e(ne,us),e(us,fi),e(ne,bi),e(ne,_s),e(_s,xi),e(ne,ki),e(x,vi),u(Xe,x,null),e(x,yi),e(x,gt),e(gt,ji),e(gt,ut),e(ut,Mi),e(gt,Li),e(x,wi),e(x,hs),e(hs,Ti),e(x,Ei),e(x,fs),e(fs,Oi),e(x,qi),u(_t,x,null),e(x,Gi),e(x,bs),e(bs,Si),e(x,$i),u(ht,x,null),e(x,Fi),e(x,xs),e(xs,Ai),e(x,zi),u(ft,x,null),e(k,Pi),e(k,se),u(bt,se,null),e(se,Ni),e(se,xt),e(xt,Di),e(xt,ks),e(ks,Ci),e(xt,Ii),e(se,Wi),e(se,vs),e(vs,Bi),e(se,Hi),u(kt,se,null),e(k,Ri),e(k,oe),u(vt,oe,null),e(oe,Ui),e(oe,yt),e(yt,Vi),e(yt,ys),e(ys,Ki),e(yt,Zi),e(oe,Xi),e(oe,js),e(js,Ji),e(oe,Qi),u(jt,oe,null),e(k,Yi),e(k,ae),u(Mt,ae,null),e(ae,el),e(ae,Lt),e(Lt,tl),e(Lt,Ms),e(Ms,nl),e(Lt,sl),e(ae,ol),e(ae,Ls),e(Ls,al),e(ae,rl),u(wt,ae,null),e(k,il),e(k,re),u(Tt,re,null),e(re,ll),e(re,Et),e(Et,dl),e(Et,ws),e(ws,cl),e(Et,pl),e(re,ml),e(re,Ts),e(Ts,gl),e(re,ul),u(Ot,re,null),e(k,_l),e(k,ie),u(qt,ie,null),e(ie,hl),e(ie,Gt),e(Gt,fl),e(Gt,Es),e(Es,bl),e(Gt,xl),e(ie,kl),e(ie,Os),e(Os,vl),e(ie,yl),u(St,ie,null),e(k,jl),e(k,le),u($t,le,null),e(le,Ml),e(le,Ft),e(Ft,Ll),e(Ft,qs),e(qs,wl),e(Ft,Tl),e(le,El),e(le,Gs),e(Gs,Ol),e(le,ql),u(At,le,null),w(i,fo,y),w(i,Te,y),e(Te,Je),e(Je,Ss),u(zt,Ss,null),e(Te,Gl),e(Te,$s),e($s,Sl),w(i,bo,y),w(i,ke,y),u(Pt,ke,null),e(ke,$l),e(ke,Nt),e(Nt,Fl),e(Nt,yn),e(yn,Al),e(Nt,zl),e(ke,Pl),e(ke,O),u(Dt,O,null),e(O,Nl),e(O,Fs),e(Fs,Dl),e(O,Cl),e(O,Ct),e(Ct,Il),e(Ct,It),e(It,Wl),e(Ct,Bl),e(O,Hl),e(O,ve),e(ve,Rl),e(ve,As),e(As,Ul),e(ve,Vl),e(ve,zs),e(zs,Kl),e(ve,Zl),e(ve,jn),e(jn,Xl),e(ve,Jl),e(O,Ql),e(O,Wt),e(Wt,Yl),e(Wt,Bt),e(Bt,ed),e(Wt,td),e(O,nd),e(O,Ps),e(Ps,sd),e(O,od),u(Ht,O,null),w(i,xo,y),w(i,Ee,y),e(Ee,Qe),e(Qe,Ns),u(Rt,Ns,null),e(Ee,ad),e(Ee,Ds),e(Ds,rd),w(i,ko,y),w(i,z,y),u(Ut,z,null),e(z,id),e(z,Vt),e(Vt,ld),e(Vt,Mn),e(Mn,dd),e(Vt,cd),e(z,pd),e(z,Kt),e(Kt,md),e(Kt,Ln),e(Ln,gd),e(Kt,ud),e(z,_d),e(z,Oe),e(Oe,de),e(de,Cs),e(Cs,hd),e(de,fd),e(de,Is),e(Is,bd),e(de,xd),e(de,Ws),e(Ws,kd),e(de,vd),e(de,Bs),e(Bs,yd),e(de,jd),e(Oe,Md),e(Oe,ce),e(ce,Hs),e(Hs,Ld),e(ce,wd),e(ce,Rs),e(Rs,Td),e(ce,Ed),e(ce,Us),e(Us,Od),e(ce,qd),e(ce,Vs),e(Vs,Gd),e(ce,Sd),e(Oe,$d),e(Oe,pe),e(pe,Ks),e(Ks,Fd),e(pe,Ad),e(pe,Zs),e(Zs,zd),e(pe,Pd),e(pe,Xs),e(Xs,Nd),e(pe,Dd),e(pe,Js),e(Js,Cd),e(pe,Id),e(z,Wd),e(z,q),u(Zt,q,null),e(q,Bd),e(q,Qs),e(Qs,Hd),e(q,Rd),e(q,qe),e(qe,me),e(me,Ys),e(Ys,Ud),e(me,Vd),e(me,eo),e(eo,Kd),e(me,Zd),e(me,to),e(to,Xd),e(me,Jd),e(me,no),e(no,Qd),e(me,Yd),e(qe,ec),e(qe,ge),e(ge,so),e(so,tc),e(ge,nc),e(ge,oo),e(oo,sc),e(ge,oc),e(ge,ao),e(ao,ac),e(ge,rc),e(ge,ro),e(ro,ic),e(ge,lc),e(qe,dc),e(qe,ue),e(ue,io),e(io,cc),e(ue,pc),e(ue,lo),e(lo,mc),e(ue,gc),e(ue,co),e(co,uc),e(ue,_c),e(ue,po),e(po,hc),e(ue,fc),e(q,bc),u(Ye,q,null),e(q,xc),e(q,Xt),e(Xt,kc),e(Xt,Jt),e(Jt,vc),e(Xt,yc),e(q,jc),e(q,mo),e(mo,Mc),e(q,Lc),u(Qt,q,null),vo=!0},p(i,[y]){const Yt={};y&2&&(Yt.$$scope={dirty:y,ctx:i}),Xe.$set(Yt);const go={};y&2&&(go.$$scope={dirty:y,ctx:i}),Ye.$set(go)},i(i){vo||(_(E.$$.fragment,i),_(lt.$$.fragment,i),_(dt.$$.fragment,i),_(mt.$$.fragment,i),_(Xe.$$.fragment,i),_(_t.$$.fragment,i),_(ht.$$.fragment,i),_(ft.$$.fragment,i),_(bt.$$.fragment,i),_(kt.$$.fragment,i),_(vt.$$.fragment,i),_(jt.$$.fragment,i),_(Mt.$$.fragment,i),_(wt.$$.fragment,i),_(Tt.$$.fragment,i),_(Ot.$$.fragment,i),_(qt.$$.fragment,i),_(St.$$.fragment,i),_($t.$$.fragment,i),_(At.$$.fragment,i),_(zt.$$.fragment,i),_(Pt.$$.fragment,i),_(Dt.$$.fragment,i),_(Ht.$$.fragment,i),_(Rt.$$.fragment,i),_(Ut.$$.fragment,i),_(Zt.$$.fragment,i),_(Ye.$$.fragment,i),_(Qt.$$.fragment,i),vo=!0)},o(i){h(E.$$.fragment,i),h(lt.$$.fragment,i),h(dt.$$.fragment,i),h(mt.$$.fragment,i),h(Xe.$$.fragment,i),h(_t.$$.fragment,i),h(ht.$$.fragment,i),h(ft.$$.fragment,i),h(bt.$$.fragment,i),h(kt.$$.fragment,i),h(vt.$$.fragment,i),h(jt.$$.fragment,i),h(Mt.$$.fragment,i),h(wt.$$.fragment,i),h(Tt.$$.fragment,i),h(Ot.$$.fragment,i),h(qt.$$.fragment,i),h(St.$$.fragment,i),h($t.$$.fragment,i),h(At.$$.fragment,i),h(zt.$$.fragment,i),h(Pt.$$.fragment,i),h(Dt.$$.fragment,i),h(Ht.$$.fragment,i),h(Rt.$$.fragment,i),h(Ut.$$.fragment,i),h(Zt.$$.fragment,i),h(Ye.$$.fragment,i),h(Qt.$$.fragment,i),vo=!1},d(i){s(p),i&&s(P),i&&s(j),f(E),i&&s(B),i&&s(v),i&&s(uo),i&&s(S),i&&s(_o),i&&s(we),f(lt),i&&s(ho),i&&s(k),f(dt),f(mt),f(Xe),f(_t),f(ht),f(ft),f(bt),f(kt),f(vt),f(jt),f(Mt),f(wt),f(Tt),f(Ot),f(qt),f(St),f($t),f(At),i&&s(fo),i&&s(Te),f(zt),i&&s(bo),i&&s(ke),f(Pt),f(Dt),f(Ht),i&&s(xo),i&&s(Ee),f(Rt),i&&s(ko),i&&s(z),f(Ut),f(Zt),f(Ye),f(Qt)}}}const Ym={local:"generation",sections:[{local:"transformers.generation_utils.GenerationMixin",title:"GenerationMixn"},{local:"transformers.generation_tf_utils.TFGenerationMixin",title:"TFGenerationMixn"},{local:"transformers.generation_flax_utils.FlaxGenerationMixin",title:"FlaxGenerationMixn"}],title:"Generation"};function eg(it,p,P){let{fw:j}=p;return it.$$set=T=>{"fw"in T&&P(0,j=T.fw)},[j]}class rg extends Um{constructor(p){super();Vm(this,p,eg,Qm,Km,{fw:0})}}export{rg as default,Ym as metadata};
