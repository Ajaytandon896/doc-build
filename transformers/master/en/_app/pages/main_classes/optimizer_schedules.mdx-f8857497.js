import{S as Ji,i as Ki,s as Qi,e as r,k as l,w as u,t as s,M as Xi,c as n,d as a,m as c,a as o,x as f,h as i,b as m,N as kr,F as t,g as d,y as g,L as Yi,q as _,o as w,B as v}from"../../chunks/vendor-6b77c823.js";import{D as $}from"../../chunks/Docstring-abef54e3.js";import{C as Nr}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Z}from"../../chunks/IconCopyLink-7a11ce68.js";function Zi(Fr){let S,dt,E,x,bt,_e,Cr,$t,Or,La,ee,Rr,At,jr,qr,Pa,I,zt,Ur,Gr,we,Vr,Et,Mr,Hr,Br,xt,Jr,Wa,k,te,Tt,ve,Kr,Dt,Qr,Sa,T,ye,Xr,be,Yr,$e,Zr,en,tn,ae,Ae,an,Lt,rn,Ia,N,re,Pt,ze,nn,Wt,on,ka,h,Ee,sn,ht,ln,xe,cn,mn,b,pn,St,dn,hn,Te,un,fn,It,gn,_n,kt,wn,vn,Nt,yn,bn,Ft,$n,An,Ct,zn,En,xn,Ot,Tn,Dn,De,Ln,Le,Pn,Wn,Sn,D,Pe,Rt,In,kn,We,jt,Nn,Fn,Se,Cn,Ie,On,Rn,jn,qt,Ut,qn,Un,Gt,Vt,Gn,Vn,Mt,Ht,Mn,Hn,Bt,Bn,Jn,ke,Kn,Jt,Qn,Xn,Ne,Yn,L,Zn,Kt,eo,to,ut,ao,ro,Qt,no,oo,so,Fe,io,Xt,lo,co,Ce,mo,ne,Oe,po,Yt,ho,Na,F,oe,Zt,Re,uo,ea,fo,Fa,z,je,go,C,_o,ta,wo,vo,qe,yo,bo,$o,aa,Ao,zo,se,Ue,Eo,ra,xo,Ca,O,Ge,To,na,Do,Oa,R,ie,oa,Ve,Lo,sa,Po,Ra,j,le,ia,Me,Wo,la,So,ja,q,He,Io,ca,ko,qa,U,Be,No,ma,Fo,Ua,G,Je,Co,pa,Oo,Ga,V,Ke,Ro,da,jo,Va,Qe,ys,Ma,M,Xe,qo,ha,Uo,Ha,Ye,bs,Ba,H,Ze,Go,ua,Vo,Ja,et,$s,Ka,B,tt,Mo,fa,Ho,Qa,at,As,Xa,P,rt,Bo,nt,Jo,ga,Ko,Qo,Xo,ce,Yo,_a,Zo,es,ot,ts,Ya,J,me,wa,st,as,va,rs,Za,K,it,ns,ya,os,er,Q,pe,ba,lt,ss,$a,is,tr,X,de,Aa,ct,ls,za,cs,ar,W,mt,ms,Y,ps,Ea,ds,hs,xa,us,fs,gs,he,pt,_s,Ta,ws,rr;return _e=new Z({}),ve=new Z({}),ye=new $({props:{name:"class transformers.AdamW",anchor:"transformers.AdamW",parameters:[{name:"params",val:": typing.Iterable[torch.nn.parameter.Parameter]"},{name:"lr",val:": float = 0.001"},{name:"betas",val:": typing.Tuple[float, float] = (0.9, 0.999)"},{name:"eps",val:": float = 1e-06"},{name:"weight_decay",val:": float = 0.0"},{name:"correct_bias",val:": bool = True"},{name:"no_deprecation_warning",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L273",parametersDescription:[{anchor:"transformers.AdamW.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.AdamW.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The learning rate to use.`,name:"lr"},{anchor:"transformers.AdamW.betas",description:`<strong>betas</strong> (<code>Tuple[float,float]</code>, <em>optional</em>, defaults to (0.9, 0.999)) &#x2014;
Adam&#x2019;s betas parameters (b1, b2).`,name:"betas"},{anchor:"transformers.AdamW.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
Adam&#x2019;s epsilon for numerical stability.`,name:"eps"},{anchor:"transformers.AdamW.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Decoupled weight decay to apply.`,name:"weight_decay"},{anchor:"transformers.AdamW.correct_bias",description:`<strong>correct_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to correct bias in Adam (for instance, in Bert TF repository they use <code>False</code>).`,name:"correct_bias"},{anchor:"transformers.AdamW.no_deprecation_warning",description:`<strong>no_deprecation_warning</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
A flag used to disable the deprecation warning (set to <code>True</code> to disable the warning).`,name:"no_deprecation_warning"}]}}),Ae=new $({props:{name:"step",anchor:"transformers.AdamW.step",parameters:[{name:"closure",val:": typing.Callable = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L323",parametersDescription:[{anchor:"transformers.AdamW.step.closure",description:"<strong>closure</strong> (<code>Callable</code>, <em>optional</em>) &#x2014; A closure that reevaluates the model and returns the loss.",name:"closure"}]}}),ze=new Z({}),Ee=new $({props:{name:"class transformers.Adafactor",anchor:"transformers.Adafactor",parameters:[{name:"params",val:""},{name:"lr",val:" = None"},{name:"eps",val:" = (1e-30, 0.001)"},{name:"clip_threshold",val:" = 1.0"},{name:"decay_rate",val:" = -0.8"},{name:"beta1",val:" = None"},{name:"weight_decay",val:" = 0.0"},{name:"scale_parameter",val:" = True"},{name:"relative_step",val:" = True"},{name:"warmup_init",val:" = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L385",parametersDescription:[{anchor:"transformers.Adafactor.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.Adafactor.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The external learning rate.`,name:"lr"},{anchor:"transformers.Adafactor.eps",description:`<strong>eps</strong> (<code>Tuple[float, float]</code>, <em>optional</em>, defaults to (1e-30, 1e-3)) &#x2014;
Regularization constants for square gradient and parameter scale respectively`,name:"eps"},{anchor:"transformers.Adafactor.clip_threshold",description:`<strong>clip_threshold</strong> (<code>float</code>, <em>optional</em>, defaults 1.0) &#x2014;
Threshold of root mean square of final gradient update`,name:"clip_threshold"},{anchor:"transformers.Adafactor.decay_rate",description:`<strong>decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to -0.8) &#x2014;
Coefficient used to compute running averages of square`,name:"decay_rate"},{anchor:"transformers.Adafactor.beta1",description:`<strong>beta1</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Coefficient used for computing running averages of gradient`,name:"beta1"},{anchor:"transformers.Adafactor.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
Weight decay (L2 penalty)`,name:"weight_decay"},{anchor:"transformers.Adafactor.scale_parameter",description:`<strong>scale_parameter</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, learning rate is scaled by root mean square`,name:"scale_parameter"},{anchor:"transformers.Adafactor.relative_step",description:`<strong>relative_step</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, time-dependent learning rate is computed instead of external learning rate`,name:"relative_step"},{anchor:"transformers.Adafactor.warmup_init",description:`<strong>warmup_init</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Time-dependent learning rate computation depends on whether warm-up initialization is being used`,name:"warmup_init"}]}}),ke=new Nr({props:{code:"Adafactor(model.parameters(), scale_parameter=False, relative_step=False, warmup_init=False, lr=1e-3)",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">False</span>, relative_step=<span class="hljs-literal">False</span>, warmup_init=<span class="hljs-literal">False</span>, lr=<span class="hljs-number">1e-3</span>)'}}),Ne=new Nr({props:{code:"Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)'}}),Fe=new Nr({props:{code:`from transformers.optimization import Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`,highlighted:`<span class="hljs-keyword">from</span> transformers.optimization <span class="hljs-keyword">import</span> Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`}}),Ce=new Nr({props:{code:`# replace AdamW with Adafactor
optimizer = Adafactor(
    model.parameters(),
    lr=1e-3,
    eps=(1e-30, 1e-3),
    clip_threshold=1.0,
    decay_rate=-0.8,
    beta1=None,
    weight_decay=0.0,
    relative_step=False,
    scale_parameter=False,
    warmup_init=False,
)`,highlighted:`<span class="hljs-comment"># replace AdamW with Adafactor</span>
optimizer = Adafactor(
    model.parameters(),
    lr=<span class="hljs-number">1e-3</span>,
    eps=(<span class="hljs-number">1e-30</span>, <span class="hljs-number">1e-3</span>),
    clip_threshold=<span class="hljs-number">1.0</span>,
    decay_rate=-<span class="hljs-number">0.8</span>,
    beta1=<span class="hljs-literal">None</span>,
    weight_decay=<span class="hljs-number">0.0</span>,
    relative_step=<span class="hljs-literal">False</span>,
    scale_parameter=<span class="hljs-literal">False</span>,
    warmup_init=<span class="hljs-literal">False</span>,
)`}}),Oe=new $({props:{name:"step",anchor:"transformers.Adafactor.step",parameters:[{name:"closure",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L531",parametersDescription:[{anchor:"transformers.Adafactor.step.closure",description:`<strong>closure</strong> (callable, optional) &#x2014; A closure that reevaluates the model
and returns the loss.`,name:"closure"}]}}),Re=new Z({}),je=new $({props:{name:"class transformers.AdamWeightDecay",anchor:"transformers.AdamWeightDecay",parameters:[{name:"learning_rate",val:": typing.Union[float, keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule] = 0.001"},{name:"beta_1",val:": float = 0.9"},{name:"beta_2",val:": float = 0.999"},{name:"epsilon",val:": float = 1e-07"},{name:"amsgrad",val:": bool = False"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"include_in_weight_decay",val:": typing.Optional[typing.List[str]] = None"},{name:"exclude_from_weight_decay",val:": typing.Optional[typing.List[str]] = None"},{name:"name",val:": str = 'AdamWeightDecay'"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L152",parametersDescription:[{anchor:"transformers.AdamWeightDecay.learning_rate",description:`<strong>learning_rate</strong> (<code>Union[float, tf.keras.optimizers.schedules.LearningRateSchedule]</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The learning rate to use or a schedule.`,name:"learning_rate"},{anchor:"transformers.AdamWeightDecay.beta_1",description:`<strong>beta_1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 parameter in Adam, which is the exponential decay rate for the 1st momentum estimates.`,name:"beta_1"},{anchor:"transformers.AdamWeightDecay.beta_2",description:`<strong>beta_2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 parameter in Adam, which is the exponential decay rate for the 2nd momentum estimates.`,name:"beta_2"},{anchor:"transformers.AdamWeightDecay.epsilon",description:`<strong>epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The epsilon parameter in Adam, which is a small constant for numerical stability.`,name:"epsilon"},{anchor:"transformers.AdamWeightDecay.amsgrad",description:`<strong>amsgrad</strong> (<code>bool</code>, <em>optional</em>, default to <code>False</code>) &#x2014;
Whether to apply AMSGrad variant of this algorithm or not, see <a href="https://arxiv.org/abs/1904.09237" rel="nofollow">On the Convergence of Adam and
Beyond</a>.`,name:"amsgrad"},{anchor:"transformers.AdamWeightDecay.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to apply.`,name:"weight_decay_rate"},{anchor:"transformers.AdamWeightDecay.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters by default (unless they are in <code>exclude_from_weight_decay</code>).`,name:"include_in_weight_decay"},{anchor:"transformers.AdamWeightDecay.exclude_from_weight_decay",description:`<strong>exclude_from_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to exclude from applying weight decay to. If a
<code>include_in_weight_decay</code> is passed, the names in it will supersede this list.`,name:"exclude_from_weight_decay"},{anchor:"transformers.AdamWeightDecay.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>, defaults to &#x2018;AdamWeightDecay&#x2019;) &#x2014;
Optional name for the operations created when applying gradients.
kwargs &#x2014;
Keyword arguments. Allowed to be {<code>clipnorm</code>, <code>clipvalue</code>, <code>lr</code>, <code>decay</code>}. <code>clipnorm</code> is clip gradients by
norm; <code>clipvalue</code> is clip gradients by value, <code>decay</code> is included for backward compatibility to allow time
inverse decay of learning rate. <code>lr</code> is included for backward compatibility, recommended to use
<code>learning_rate</code> instead.`,name:"name"}]}}),Ue=new $({props:{name:"from_config",anchor:"transformers.AdamWeightDecay.from_config",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L209"}}),Ge=new $({props:{name:"transformers.create_optimizer",anchor:"transformers.create_optimizer",parameters:[{name:"init_lr",val:": float"},{name:"num_train_steps",val:": int"},{name:"num_warmup_steps",val:": int"},{name:"min_lr_ratio",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"power",val:": float = 1.0"},{name:"include_in_weight_decay",val:": typing.Optional[typing.List[str]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L82",parametersDescription:[{anchor:"transformers.create_optimizer.init_lr",description:`<strong>init_lr</strong> (<code>float</code>) &#x2014;
The desired learning rate at the end of the warmup phase.`,name:"init_lr"},{anchor:"transformers.create_optimizer.num_train_steps",description:`<strong>num_train_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_train_steps"},{anchor:"transformers.create_optimizer.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of warmup steps.`,name:"num_warmup_steps"},{anchor:"transformers.create_optimizer.min_lr_ratio",description:`<strong>min_lr_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The final learning rate at the end of the linear decay will be <code>init_lr * min_lr_ratio</code>.`,name:"min_lr_ratio"},{anchor:"transformers.create_optimizer.adam_beta1",description:`<strong>adam_beta1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 to use in Adam.`,name:"adam_beta1"},{anchor:"transformers.create_optimizer.adam_beta2",description:`<strong>adam_beta2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 to use in Adam.`,name:"adam_beta2"},{anchor:"transformers.create_optimizer.adam_epsilon",description:`<strong>adam_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-8) &#x2014;
The epsilon to use in Adam.`,name:"adam_epsilon"},{anchor:"transformers.create_optimizer.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to use.`,name:"weight_decay_rate"},{anchor:"transformers.create_optimizer.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The power to use for PolynomialDecay.`,name:"power"},{anchor:"transformers.create_optimizer.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters except bias and layer norm parameters.`,name:"include_in_weight_decay"}]}}),Ve=new Z({}),Me=new Z({}),He=new $({props:{name:"class transformers.SchedulerType",anchor:"transformers.SchedulerType",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_utils.py#L301"}}),Be=new $({props:{name:"transformers.get_scheduler",anchor:"transformers.get_scheduler",parameters:[{name:"name",val:": typing.Union[str, transformers.trainer_utils.SchedulerType]"},{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": typing.Optional[int] = None"},{name:"num_training_steps",val:": typing.Optional[int] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L233",parametersDescription:[{anchor:"transformers.get_scheduler.name",description:`<strong>name</strong> (<code>str</code> or <code>SchedulerType</code>) &#x2014;
The name of the scheduler to use.`,name:"name"},{anchor:"transformers.get_scheduler.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.Optimizer</code>) &#x2014;
The optimizer that will be used during training.`,name:"optimizer"},{anchor:"transformers.get_scheduler.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of warmup steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_warmup_steps"},{anchor:"transformers.get_scheduler.num_training_steps",description:`<strong>num_training_steps</strong> (\`int&#x201C;, <em>optional</em>) &#x2014;
The number of training steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_training_steps"}]}}),Je=new $({props:{name:"transformers.get_constant_schedule",anchor:"transformers.get_constant_schedule",parameters:[{name:"optimizer",val:": Optimizer"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L34",parametersDescription:[{anchor:"transformers.get_constant_schedule.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Ke=new $({props:{name:"transformers.get_constant_schedule_with_warmup",anchor:"transformers.get_constant_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L50",parametersDescription:[{anchor:"transformers.get_constant_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_constant_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Xe=new $({props:{name:"transformers.get_cosine_schedule_with_warmup",anchor:"transformers.get_cosine_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": float = 0.5"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L104",parametersDescription:[{anchor:"transformers.get_cosine_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0
following a half-cosine).`,name:"num_cycles"},{anchor:"transformers.get_cosine_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Ze=new $({props:{name:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": int = 1"},{name:"last_epoch",val:": int = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L138",parametersDescription:[{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of hard restarts to use.`,name:"num_cycles"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),tt=new $({props:{name:"transformers.get_linear_schedule_with_warmup",anchor:"transformers.get_linear_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"last_epoch",val:" = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L75",parametersDescription:[{anchor:"transformers.get_linear_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_linear_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),rt=new $({props:{name:"transformers.get_polynomial_decay_schedule_with_warmup",anchor:"transformers.get_polynomial_decay_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"lr_end",val:" = 1e-07"},{name:"power",val:" = 1.0"},{name:"last_epoch",val:" = -1"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization.py#L173",parametersDescription:[{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.lr_end",description:`<strong>lr_end</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The end LR.`,name:"lr_end"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Power factor.`,name:"power"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],returnDescription:`
<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),st=new Z({}),it=new $({props:{name:"class transformers.WarmUp",anchor:"transformers.WarmUp",parameters:[{name:"initial_learning_rate",val:": float"},{name:"decay_schedule_fn",val:": typing.Callable"},{name:"warmup_steps",val:": int"},{name:"power",val:": float = 1.0"},{name:"name",val:": str = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L24",parametersDescription:[{anchor:"transformers.WarmUp.initial_learning_rate",description:`<strong>initial_learning_rate</strong> (<code>float</code>) &#x2014;
The initial learning rate for the schedule after the warmup (so this will be the learning rate at the end
of the warmup).`,name:"initial_learning_rate"},{anchor:"transformers.WarmUp.decay_schedule_fn",description:`<strong>decay_schedule_fn</strong> (<code>Callable</code>) &#x2014;
The schedule function to apply after the warmup for the rest of training.`,name:"decay_schedule_fn"},{anchor:"transformers.WarmUp.warmup_steps",description:`<strong>warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup part of training.`,name:"warmup_steps"},{anchor:"transformers.WarmUp.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
The power to use for the polynomial warmup (defaults is a linear warmup).`,name:"power"},{anchor:"transformers.WarmUp.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Optional name prefix for the returned tensors during the schedule.`,name:"name"}]}}),lt=new Z({}),ct=new Z({}),mt=new $({props:{name:"class transformers.GradientAccumulator",anchor:"transformers.GradientAccumulator",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L282"}}),pt=new $({props:{name:"reset",anchor:"transformers.GradientAccumulator.reset",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/optimization_tf.py#L344"}}),{c(){S=r("meta"),dt=l(),E=r("h1"),x=r("a"),bt=r("span"),u(_e.$$.fragment),Cr=l(),$t=r("span"),Or=s("Optimization"),La=l(),ee=r("p"),Rr=s("The "),At=r("code"),jr=s(".optimization"),qr=s(" module provides:"),Pa=l(),I=r("ul"),zt=r("li"),Ur=s("an optimizer with weight decay fixed that can be used to fine-tuned models, and"),Gr=l(),we=r("li"),Vr=s("several schedules in the form of schedule objects that inherit from "),Et=r("code"),Mr=s("_LRSchedule"),Hr=s(":"),Br=l(),xt=r("li"),Jr=s("a gradient accumulation class to accumulate the gradients of multiple batches"),Wa=l(),k=r("h2"),te=r("a"),Tt=r("span"),u(ve.$$.fragment),Kr=l(),Dt=r("span"),Qr=s("AdamW (PyTorch)"),Sa=l(),T=r("div"),u(ye.$$.fragment),Xr=l(),be=r("p"),Yr=s("Implements Adam algorithm with weight decay fix as introduced in "),$e=r("a"),Zr=s(`Decoupled Weight Decay
Regularization`),en=s("."),tn=l(),ae=r("div"),u(Ae.$$.fragment),an=l(),Lt=r("p"),rn=s("Performs a single optimization step."),Ia=l(),N=r("h2"),re=r("a"),Pt=r("span"),u(ze.$$.fragment),nn=l(),Wt=r("span"),on=s("AdaFactor (PyTorch)"),ka=l(),h=r("div"),u(Ee.$$.fragment),sn=l(),ht=r("p"),ln=s(`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
`),xe=r("a"),cn=s("https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),mn=l(),b=r("p"),pn=s("Paper: "),St=r("em"),dn=s("Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"),hn=l(),Te=r("a"),un=s("https://arxiv.org/abs/1804.04235"),fn=s(` Note that
this optimizer internally adjusts the learning rate depending on the `),It=r("code"),gn=s("scale_parameter"),_n=s(", "),kt=r("code"),wn=s("relative_step"),vn=s(` and
`),Nt=r("code"),yn=s("warmup_init"),bn=s(" options. To use a manual (external) learning rate schedule you should set "),Ft=r("code"),$n=s("scale_parameter=False"),An=s(` and
`),Ct=r("code"),zn=s("relative_step=False"),En=s("."),xn=l(),Ot=r("p"),Tn=s("This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested."),Dn=l(),De=r("p"),Ln=s("Recommended T5 finetuning settings ("),Le=r("a"),Pn=s("https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),Wn=s("):"),Sn=l(),D=r("ul"),Pe=r("li"),Rt=r("p"),In=s("Training without LR warmup or clip_threshold is not recommended."),kn=l(),We=r("ul"),jt=r("li"),Nn=s("use scheduled LR warm-up to fixed LR"),Fn=l(),Se=r("li"),Cn=s("use clip_threshold=1.0 ("),Ie=r("a"),On=s("https://arxiv.org/abs/1804.04235"),Rn=s(")"),jn=l(),qt=r("li"),Ut=r("p"),qn=s("Disable relative updates"),Un=l(),Gt=r("li"),Vt=r("p"),Gn=s("Use scale_parameter=False"),Vn=l(),Mt=r("li"),Ht=r("p"),Mn=s("Additional optimizer operations like gradient clipping should not be used alongside Adafactor"),Hn=l(),Bt=r("p"),Bn=s("Example:"),Jn=l(),u(ke.$$.fragment),Kn=l(),Jt=r("p"),Qn=s("Others reported the following combination to work well:"),Xn=l(),u(Ne.$$.fragment),Yn=l(),L=r("p"),Zn=s("When using "),Kt=r("code"),eo=s("lr=None"),to=s(" with "),ut=r("a"),ao=s("Trainer"),ro=s(" you will most likely need to use "),Qt=r("code"),no=s("AdafactorSchedule"),oo=s("scheduler as following:"),so=l(),u(Fe.$$.fragment),io=l(),Xt=r("p"),lo=s("Usage:"),co=l(),u(Ce.$$.fragment),mo=l(),ne=r("div"),u(Oe.$$.fragment),po=l(),Yt=r("p"),ho=s("Performs a single optimization step"),Na=l(),F=r("h2"),oe=r("a"),Zt=r("span"),u(Re.$$.fragment),uo=l(),ea=r("span"),fo=s("AdamWeightDecay (TensorFlow)"),Fa=l(),z=r("div"),u(je.$$.fragment),go=l(),C=r("p"),_o=s(`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is `),ta=r("em"),wo=s("not"),vo=s(` the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in `),qe=r("a"),yo=s(`Decoupled Weight Decay
Regularization`),bo=s("."),$o=l(),aa=r("p"),Ao=s(`Instead we want ot decay the weights in a manner that doesn\u2019t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`),zo=l(),se=r("div"),u(Ue.$$.fragment),Eo=l(),ra=r("p"),xo=s("Creates an optimizer from its config with WarmUp custom object."),Ca=l(),O=r("div"),u(Ge.$$.fragment),To=l(),na=r("p"),Do=s("Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay."),Oa=l(),R=r("h2"),ie=r("a"),oa=r("span"),u(Ve.$$.fragment),Lo=l(),sa=r("span"),Po=s("Schedules"),Ra=l(),j=r("h3"),le=r("a"),ia=r("span"),u(Me.$$.fragment),Wo=l(),la=r("span"),So=s("Learning Rate Schedules (Pytorch)"),ja=l(),q=r("div"),u(He.$$.fragment),Io=l(),ca=r("p"),ko=s("An enumeration."),qa=l(),U=r("div"),u(Be.$$.fragment),No=l(),ma=r("p"),Fo=s("Unified API to get any scheduler from its name."),Ua=l(),G=r("div"),u(Je.$$.fragment),Co=l(),pa=r("p"),Oo=s("Create a schedule with a constant learning rate, using the learning rate set in optimizer."),Ga=l(),V=r("div"),u(Ke.$$.fragment),Ro=l(),da=r("p"),jo=s(`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`),Va=l(),Qe=r("img"),Ma=l(),M=r("div"),u(Xe.$$.fragment),qo=l(),ha=r("p"),Uo=s(`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`),Ha=l(),Ye=r("img"),Ba=l(),H=r("div"),u(Ze.$$.fragment),Go=l(),ua=r("p"),Vo=s(`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`),Ja=l(),et=r("img"),Ka=l(),B=r("div"),u(tt.$$.fragment),Mo=l(),fa=r("p"),Ho=s(`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`),Qa=l(),at=r("img"),Xa=l(),P=r("div"),u(rt.$$.fragment),Bo=l(),nt=r("p"),Jo=s(`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by `),ga=r("em"),Ko=s("lr_end"),Qo=s(`, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`),Xo=l(),ce=r("p"),Yo=s("Note: "),_a=r("em"),Zo=s("power"),es=s(` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
`),ot=r("a"),ts=s("https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),Ya=l(),J=r("h3"),me=r("a"),wa=r("span"),u(st.$$.fragment),as=l(),va=r("span"),rs=s("Warmup (TensorFlow)"),Za=l(),K=r("div"),u(it.$$.fragment),ns=l(),ya=r("p"),os=s("Applies a warmup schedule on a given learning rate decay schedule."),er=l(),Q=r("h2"),pe=r("a"),ba=r("span"),u(lt.$$.fragment),ss=l(),$a=r("span"),is=s("Gradient Strategies"),tr=l(),X=r("h3"),de=r("a"),Aa=r("span"),u(ct.$$.fragment),ls=l(),za=r("span"),cs=s("GradientAccumulator (TensorFlow)"),ar=l(),W=r("div"),u(mt.$$.fragment),ms=l(),Y=r("p"),ps=s(`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call `),Ea=r("code"),ds=s(".gradients"),hs=s(", scale the gradients if required, and pass the result to "),xa=r("code"),us=s("apply_gradients"),fs=s("."),gs=l(),he=r("div"),u(pt.$$.fragment),_s=l(),Ta=r("p"),ws=s("Resets the accumulated gradients on the current replica."),this.h()},l(e){const p=Xi('[data-svelte="svelte-1phssyn"]',document.head);S=n(p,"META",{name:!0,content:!0}),p.forEach(a),dt=c(e),E=n(e,"H1",{class:!0});var nr=o(E);x=n(nr,"A",{id:!0,class:!0,href:!0});var zs=o(x);bt=n(zs,"SPAN",{});var Es=o(bt);f(_e.$$.fragment,Es),Es.forEach(a),zs.forEach(a),Cr=c(nr),$t=n(nr,"SPAN",{});var xs=o($t);Or=i(xs,"Optimization"),xs.forEach(a),nr.forEach(a),La=c(e),ee=n(e,"P",{});var or=o(ee);Rr=i(or,"The "),At=n(or,"CODE",{});var Ts=o(At);jr=i(Ts,".optimization"),Ts.forEach(a),qr=i(or," module provides:"),or.forEach(a),Pa=c(e),I=n(e,"UL",{});var ft=o(I);zt=n(ft,"LI",{});var Ds=o(zt);Ur=i(Ds,"an optimizer with weight decay fixed that can be used to fine-tuned models, and"),Ds.forEach(a),Gr=c(ft),we=n(ft,"LI",{});var sr=o(we);Vr=i(sr,"several schedules in the form of schedule objects that inherit from "),Et=n(sr,"CODE",{});var Ls=o(Et);Mr=i(Ls,"_LRSchedule"),Ls.forEach(a),Hr=i(sr,":"),sr.forEach(a),Br=c(ft),xt=n(ft,"LI",{});var Ps=o(xt);Jr=i(Ps,"a gradient accumulation class to accumulate the gradients of multiple batches"),Ps.forEach(a),ft.forEach(a),Wa=c(e),k=n(e,"H2",{class:!0});var ir=o(k);te=n(ir,"A",{id:!0,class:!0,href:!0});var Ws=o(te);Tt=n(Ws,"SPAN",{});var Ss=o(Tt);f(ve.$$.fragment,Ss),Ss.forEach(a),Ws.forEach(a),Kr=c(ir),Dt=n(ir,"SPAN",{});var Is=o(Dt);Qr=i(Is,"AdamW (PyTorch)"),Is.forEach(a),ir.forEach(a),Sa=c(e),T=n(e,"DIV",{class:!0});var gt=o(T);f(ye.$$.fragment,gt),Xr=c(gt),be=n(gt,"P",{});var lr=o(be);Yr=i(lr,"Implements Adam algorithm with weight decay fix as introduced in "),$e=n(lr,"A",{href:!0,rel:!0});var ks=o($e);Zr=i(ks,`Decoupled Weight Decay
Regularization`),ks.forEach(a),en=i(lr,"."),lr.forEach(a),tn=c(gt),ae=n(gt,"DIV",{class:!0});var cr=o(ae);f(Ae.$$.fragment,cr),an=c(cr),Lt=n(cr,"P",{});var Ns=o(Lt);rn=i(Ns,"Performs a single optimization step."),Ns.forEach(a),cr.forEach(a),gt.forEach(a),Ia=c(e),N=n(e,"H2",{class:!0});var mr=o(N);re=n(mr,"A",{id:!0,class:!0,href:!0});var Fs=o(re);Pt=n(Fs,"SPAN",{});var Cs=o(Pt);f(ze.$$.fragment,Cs),Cs.forEach(a),Fs.forEach(a),nn=c(mr),Wt=n(mr,"SPAN",{});var Os=o(Wt);on=i(Os,"AdaFactor (PyTorch)"),Os.forEach(a),mr.forEach(a),ka=c(e),h=n(e,"DIV",{class:!0});var y=o(h);f(Ee.$$.fragment,y),sn=c(y),ht=n(y,"P",{});var vs=o(ht);ln=i(vs,`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
`),xe=n(vs,"A",{href:!0,rel:!0});var Rs=o(xe);cn=i(Rs,"https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),Rs.forEach(a),vs.forEach(a),mn=c(y),b=n(y,"P",{});var A=o(b);pn=i(A,"Paper: "),St=n(A,"EM",{});var js=o(St);dn=i(js,"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"),js.forEach(a),hn=c(A),Te=n(A,"A",{href:!0,rel:!0});var qs=o(Te);un=i(qs,"https://arxiv.org/abs/1804.04235"),qs.forEach(a),fn=i(A,` Note that
this optimizer internally adjusts the learning rate depending on the `),It=n(A,"CODE",{});var Us=o(It);gn=i(Us,"scale_parameter"),Us.forEach(a),_n=i(A,", "),kt=n(A,"CODE",{});var Gs=o(kt);wn=i(Gs,"relative_step"),Gs.forEach(a),vn=i(A,` and
`),Nt=n(A,"CODE",{});var Vs=o(Nt);yn=i(Vs,"warmup_init"),Vs.forEach(a),bn=i(A," options. To use a manual (external) learning rate schedule you should set "),Ft=n(A,"CODE",{});var Ms=o(Ft);$n=i(Ms,"scale_parameter=False"),Ms.forEach(a),An=i(A,` and
`),Ct=n(A,"CODE",{});var Hs=o(Ct);zn=i(Hs,"relative_step=False"),Hs.forEach(a),En=i(A,"."),A.forEach(a),xn=c(y),Ot=n(y,"P",{});var Bs=o(Ot);Tn=i(Bs,"This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested."),Bs.forEach(a),Dn=c(y),De=n(y,"P",{});var pr=o(De);Ln=i(pr,"Recommended T5 finetuning settings ("),Le=n(pr,"A",{href:!0,rel:!0});var Js=o(Le);Pn=i(Js,"https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),Js.forEach(a),Wn=i(pr,"):"),pr.forEach(a),Sn=c(y),D=n(y,"UL",{});var ue=o(D);Pe=n(ue,"LI",{});var dr=o(Pe);Rt=n(dr,"P",{});var Ks=o(Rt);In=i(Ks,"Training without LR warmup or clip_threshold is not recommended."),Ks.forEach(a),kn=c(dr),We=n(dr,"UL",{});var hr=o(We);jt=n(hr,"LI",{});var Qs=o(jt);Nn=i(Qs,"use scheduled LR warm-up to fixed LR"),Qs.forEach(a),Fn=c(hr),Se=n(hr,"LI",{});var ur=o(Se);Cn=i(ur,"use clip_threshold=1.0 ("),Ie=n(ur,"A",{href:!0,rel:!0});var Xs=o(Ie);On=i(Xs,"https://arxiv.org/abs/1804.04235"),Xs.forEach(a),Rn=i(ur,")"),ur.forEach(a),hr.forEach(a),dr.forEach(a),jn=c(ue),qt=n(ue,"LI",{});var Ys=o(qt);Ut=n(Ys,"P",{});var Zs=o(Ut);qn=i(Zs,"Disable relative updates"),Zs.forEach(a),Ys.forEach(a),Un=c(ue),Gt=n(ue,"LI",{});var ei=o(Gt);Vt=n(ei,"P",{});var ti=o(Vt);Gn=i(ti,"Use scale_parameter=False"),ti.forEach(a),ei.forEach(a),Vn=c(ue),Mt=n(ue,"LI",{});var ai=o(Mt);Ht=n(ai,"P",{});var ri=o(Ht);Mn=i(ri,"Additional optimizer operations like gradient clipping should not be used alongside Adafactor"),ri.forEach(a),ai.forEach(a),ue.forEach(a),Hn=c(y),Bt=n(y,"P",{});var ni=o(Bt);Bn=i(ni,"Example:"),ni.forEach(a),Jn=c(y),f(ke.$$.fragment,y),Kn=c(y),Jt=n(y,"P",{});var oi=o(Jt);Qn=i(oi,"Others reported the following combination to work well:"),oi.forEach(a),Xn=c(y),f(Ne.$$.fragment,y),Yn=c(y),L=n(y,"P",{});var fe=o(L);Zn=i(fe,"When using "),Kt=n(fe,"CODE",{});var si=o(Kt);eo=i(si,"lr=None"),si.forEach(a),to=i(fe," with "),ut=n(fe,"A",{href:!0});var ii=o(ut);ao=i(ii,"Trainer"),ii.forEach(a),ro=i(fe," you will most likely need to use "),Qt=n(fe,"CODE",{});var li=o(Qt);no=i(li,"AdafactorSchedule"),li.forEach(a),oo=i(fe,"scheduler as following:"),fe.forEach(a),so=c(y),f(Fe.$$.fragment,y),io=c(y),Xt=n(y,"P",{});var ci=o(Xt);lo=i(ci,"Usage:"),ci.forEach(a),co=c(y),f(Ce.$$.fragment,y),mo=c(y),ne=n(y,"DIV",{class:!0});var fr=o(ne);f(Oe.$$.fragment,fr),po=c(fr),Yt=n(fr,"P",{});var mi=o(Yt);ho=i(mi,"Performs a single optimization step"),mi.forEach(a),fr.forEach(a),y.forEach(a),Na=c(e),F=n(e,"H2",{class:!0});var gr=o(F);oe=n(gr,"A",{id:!0,class:!0,href:!0});var pi=o(oe);Zt=n(pi,"SPAN",{});var di=o(Zt);f(Re.$$.fragment,di),di.forEach(a),pi.forEach(a),uo=c(gr),ea=n(gr,"SPAN",{});var hi=o(ea);fo=i(hi,"AdamWeightDecay (TensorFlow)"),hi.forEach(a),gr.forEach(a),Fa=c(e),z=n(e,"DIV",{class:!0});var ge=o(z);f(je.$$.fragment,ge),go=c(ge),C=n(ge,"P",{});var _t=o(C);_o=i(_t,`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is `),ta=n(_t,"EM",{});var ui=o(ta);wo=i(ui,"not"),ui.forEach(a),vo=i(_t,` the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in `),qe=n(_t,"A",{href:!0,rel:!0});var fi=o(qe);yo=i(fi,`Decoupled Weight Decay
Regularization`),fi.forEach(a),bo=i(_t,"."),_t.forEach(a),$o=c(ge),aa=n(ge,"P",{});var gi=o(aa);Ao=i(gi,`Instead we want ot decay the weights in a manner that doesn\u2019t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`),gi.forEach(a),zo=c(ge),se=n(ge,"DIV",{class:!0});var _r=o(se);f(Ue.$$.fragment,_r),Eo=c(_r),ra=n(_r,"P",{});var _i=o(ra);xo=i(_i,"Creates an optimizer from its config with WarmUp custom object."),_i.forEach(a),_r.forEach(a),ge.forEach(a),Ca=c(e),O=n(e,"DIV",{class:!0});var wr=o(O);f(Ge.$$.fragment,wr),To=c(wr),na=n(wr,"P",{});var wi=o(na);Do=i(wi,"Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay."),wi.forEach(a),wr.forEach(a),Oa=c(e),R=n(e,"H2",{class:!0});var vr=o(R);ie=n(vr,"A",{id:!0,class:!0,href:!0});var vi=o(ie);oa=n(vi,"SPAN",{});var yi=o(oa);f(Ve.$$.fragment,yi),yi.forEach(a),vi.forEach(a),Lo=c(vr),sa=n(vr,"SPAN",{});var bi=o(sa);Po=i(bi,"Schedules"),bi.forEach(a),vr.forEach(a),Ra=c(e),j=n(e,"H3",{class:!0});var yr=o(j);le=n(yr,"A",{id:!0,class:!0,href:!0});var $i=o(le);ia=n($i,"SPAN",{});var Ai=o(ia);f(Me.$$.fragment,Ai),Ai.forEach(a),$i.forEach(a),Wo=c(yr),la=n(yr,"SPAN",{});var zi=o(la);So=i(zi,"Learning Rate Schedules (Pytorch)"),zi.forEach(a),yr.forEach(a),ja=c(e),q=n(e,"DIV",{class:!0});var br=o(q);f(He.$$.fragment,br),Io=c(br),ca=n(br,"P",{});var Ei=o(ca);ko=i(Ei,"An enumeration."),Ei.forEach(a),br.forEach(a),qa=c(e),U=n(e,"DIV",{class:!0});var $r=o(U);f(Be.$$.fragment,$r),No=c($r),ma=n($r,"P",{});var xi=o(ma);Fo=i(xi,"Unified API to get any scheduler from its name."),xi.forEach(a),$r.forEach(a),Ua=c(e),G=n(e,"DIV",{class:!0});var Ar=o(G);f(Je.$$.fragment,Ar),Co=c(Ar),pa=n(Ar,"P",{});var Ti=o(pa);Oo=i(Ti,"Create a schedule with a constant learning rate, using the learning rate set in optimizer."),Ti.forEach(a),Ar.forEach(a),Ga=c(e),V=n(e,"DIV",{class:!0});var zr=o(V);f(Ke.$$.fragment,zr),Ro=c(zr),da=n(zr,"P",{});var Di=o(da);jo=i(Di,`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`),Di.forEach(a),zr.forEach(a),Va=c(e),Qe=n(e,"IMG",{alt:!0,src:!0}),Ma=c(e),M=n(e,"DIV",{class:!0});var Er=o(M);f(Xe.$$.fragment,Er),qo=c(Er),ha=n(Er,"P",{});var Li=o(ha);Uo=i(Li,`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`),Li.forEach(a),Er.forEach(a),Ha=c(e),Ye=n(e,"IMG",{alt:!0,src:!0}),Ba=c(e),H=n(e,"DIV",{class:!0});var xr=o(H);f(Ze.$$.fragment,xr),Go=c(xr),ua=n(xr,"P",{});var Pi=o(ua);Vo=i(Pi,`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`),Pi.forEach(a),xr.forEach(a),Ja=c(e),et=n(e,"IMG",{alt:!0,src:!0}),Ka=c(e),B=n(e,"DIV",{class:!0});var Tr=o(B);f(tt.$$.fragment,Tr),Mo=c(Tr),fa=n(Tr,"P",{});var Wi=o(fa);Ho=i(Wi,`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`),Wi.forEach(a),Tr.forEach(a),Qa=c(e),at=n(e,"IMG",{alt:!0,src:!0}),Xa=c(e),P=n(e,"DIV",{class:!0});var wt=o(P);f(rt.$$.fragment,wt),Bo=c(wt),nt=n(wt,"P",{});var Dr=o(nt);Jo=i(Dr,`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by `),ga=n(Dr,"EM",{});var Si=o(ga);Ko=i(Si,"lr_end"),Si.forEach(a),Qo=i(Dr,`, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`),Dr.forEach(a),Xo=c(wt),ce=n(wt,"P",{});var Da=o(ce);Yo=i(Da,"Note: "),_a=n(Da,"EM",{});var Ii=o(_a);Zo=i(Ii,"power"),Ii.forEach(a),es=i(Da,` defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
`),ot=n(Da,"A",{href:!0,rel:!0});var ki=o(ot);ts=i(ki,"https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),ki.forEach(a),Da.forEach(a),wt.forEach(a),Ya=c(e),J=n(e,"H3",{class:!0});var Lr=o(J);me=n(Lr,"A",{id:!0,class:!0,href:!0});var Ni=o(me);wa=n(Ni,"SPAN",{});var Fi=o(wa);f(st.$$.fragment,Fi),Fi.forEach(a),Ni.forEach(a),as=c(Lr),va=n(Lr,"SPAN",{});var Ci=o(va);rs=i(Ci,"Warmup (TensorFlow)"),Ci.forEach(a),Lr.forEach(a),Za=c(e),K=n(e,"DIV",{class:!0});var Pr=o(K);f(it.$$.fragment,Pr),ns=c(Pr),ya=n(Pr,"P",{});var Oi=o(ya);os=i(Oi,"Applies a warmup schedule on a given learning rate decay schedule."),Oi.forEach(a),Pr.forEach(a),er=c(e),Q=n(e,"H2",{class:!0});var Wr=o(Q);pe=n(Wr,"A",{id:!0,class:!0,href:!0});var Ri=o(pe);ba=n(Ri,"SPAN",{});var ji=o(ba);f(lt.$$.fragment,ji),ji.forEach(a),Ri.forEach(a),ss=c(Wr),$a=n(Wr,"SPAN",{});var qi=o($a);is=i(qi,"Gradient Strategies"),qi.forEach(a),Wr.forEach(a),tr=c(e),X=n(e,"H3",{class:!0});var Sr=o(X);de=n(Sr,"A",{id:!0,class:!0,href:!0});var Ui=o(de);Aa=n(Ui,"SPAN",{});var Gi=o(Aa);f(ct.$$.fragment,Gi),Gi.forEach(a),Ui.forEach(a),ls=c(Sr),za=n(Sr,"SPAN",{});var Vi=o(za);cs=i(Vi,"GradientAccumulator (TensorFlow)"),Vi.forEach(a),Sr.forEach(a),ar=c(e),W=n(e,"DIV",{class:!0});var vt=o(W);f(mt.$$.fragment,vt),ms=c(vt),Y=n(vt,"P",{});var yt=o(Y);ps=i(yt,`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call `),Ea=n(yt,"CODE",{});var Mi=o(Ea);ds=i(Mi,".gradients"),Mi.forEach(a),hs=i(yt,", scale the gradients if required, and pass the result to "),xa=n(yt,"CODE",{});var Hi=o(xa);us=i(Hi,"apply_gradients"),Hi.forEach(a),fs=i(yt,"."),yt.forEach(a),gs=c(vt),he=n(vt,"DIV",{class:!0});var Ir=o(he);f(pt.$$.fragment,Ir),_s=c(Ir),Ta=n(Ir,"P",{});var Bi=o(Ta);ws=i(Bi,"Resets the accumulated gradients on the current replica."),Bi.forEach(a),Ir.forEach(a),vt.forEach(a),this.h()},h(){m(S,"name","hf:doc:metadata"),m(S,"content",JSON.stringify(el)),m(x,"id","optimization"),m(x,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(x,"href","#optimization"),m(E,"class","relative group"),m(te,"id","transformers.AdamW"),m(te,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(te,"href","#transformers.AdamW"),m(k,"class","relative group"),m($e,"href","https://arxiv.org/abs/1711.05101"),m($e,"rel","nofollow"),m(ae,"class","docstring"),m(T,"class","docstring"),m(re,"id","transformers.Adafactor"),m(re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(re,"href","#transformers.Adafactor"),m(N,"class","relative group"),m(xe,"href","https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py"),m(xe,"rel","nofollow"),m(Te,"href","https://arxiv.org/abs/1804.04235"),m(Te,"rel","nofollow"),m(Le,"href","https://discuss.huggingface.co/t/t5-finetuning-tips/684/3"),m(Le,"rel","nofollow"),m(Ie,"href","https://arxiv.org/abs/1804.04235"),m(Ie,"rel","nofollow"),m(ut,"href","/docs/transformers/master/en/main_classes/trainer#transformers.Trainer"),m(ne,"class","docstring"),m(h,"class","docstring"),m(oe,"id","transformers.AdamWeightDecay"),m(oe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(oe,"href","#transformers.AdamWeightDecay"),m(F,"class","relative group"),m(qe,"href","https://arxiv.org/abs/1711.05101"),m(qe,"rel","nofollow"),m(se,"class","docstring"),m(z,"class","docstring"),m(O,"class","docstring"),m(ie,"id","schedules"),m(ie,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(ie,"href","#schedules"),m(R,"class","relative group"),m(le,"id","transformers.SchedulerType"),m(le,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(le,"href","#transformers.SchedulerType"),m(j,"class","relative group"),m(q,"class","docstring"),m(U,"class","docstring"),m(G,"class","docstring"),m(V,"class","docstring"),m(Qe,"alt",""),kr(Qe.src,ys="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png")||m(Qe,"src",ys),m(M,"class","docstring"),m(Ye,"alt",""),kr(Ye.src,bs="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png")||m(Ye,"src",bs),m(H,"class","docstring"),m(et,"alt",""),kr(et.src,$s="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png")||m(et,"src",$s),m(B,"class","docstring"),m(at,"alt",""),kr(at.src,As="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png")||m(at,"src",As),m(ot,"href","https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37"),m(ot,"rel","nofollow"),m(P,"class","docstring"),m(me,"id","transformers.WarmUp"),m(me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(me,"href","#transformers.WarmUp"),m(J,"class","relative group"),m(K,"class","docstring"),m(pe,"id","gradient-strategies"),m(pe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(pe,"href","#gradient-strategies"),m(Q,"class","relative group"),m(de,"id","transformers.GradientAccumulator"),m(de,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),m(de,"href","#transformers.GradientAccumulator"),m(X,"class","relative group"),m(he,"class","docstring"),m(W,"class","docstring")},m(e,p){t(document.head,S),d(e,dt,p),d(e,E,p),t(E,x),t(x,bt),g(_e,bt,null),t(E,Cr),t(E,$t),t($t,Or),d(e,La,p),d(e,ee,p),t(ee,Rr),t(ee,At),t(At,jr),t(ee,qr),d(e,Pa,p),d(e,I,p),t(I,zt),t(zt,Ur),t(I,Gr),t(I,we),t(we,Vr),t(we,Et),t(Et,Mr),t(we,Hr),t(I,Br),t(I,xt),t(xt,Jr),d(e,Wa,p),d(e,k,p),t(k,te),t(te,Tt),g(ve,Tt,null),t(k,Kr),t(k,Dt),t(Dt,Qr),d(e,Sa,p),d(e,T,p),g(ye,T,null),t(T,Xr),t(T,be),t(be,Yr),t(be,$e),t($e,Zr),t(be,en),t(T,tn),t(T,ae),g(Ae,ae,null),t(ae,an),t(ae,Lt),t(Lt,rn),d(e,Ia,p),d(e,N,p),t(N,re),t(re,Pt),g(ze,Pt,null),t(N,nn),t(N,Wt),t(Wt,on),d(e,ka,p),d(e,h,p),g(Ee,h,null),t(h,sn),t(h,ht),t(ht,ln),t(ht,xe),t(xe,cn),t(h,mn),t(h,b),t(b,pn),t(b,St),t(St,dn),t(b,hn),t(b,Te),t(Te,un),t(b,fn),t(b,It),t(It,gn),t(b,_n),t(b,kt),t(kt,wn),t(b,vn),t(b,Nt),t(Nt,yn),t(b,bn),t(b,Ft),t(Ft,$n),t(b,An),t(b,Ct),t(Ct,zn),t(b,En),t(h,xn),t(h,Ot),t(Ot,Tn),t(h,Dn),t(h,De),t(De,Ln),t(De,Le),t(Le,Pn),t(De,Wn),t(h,Sn),t(h,D),t(D,Pe),t(Pe,Rt),t(Rt,In),t(Pe,kn),t(Pe,We),t(We,jt),t(jt,Nn),t(We,Fn),t(We,Se),t(Se,Cn),t(Se,Ie),t(Ie,On),t(Se,Rn),t(D,jn),t(D,qt),t(qt,Ut),t(Ut,qn),t(D,Un),t(D,Gt),t(Gt,Vt),t(Vt,Gn),t(D,Vn),t(D,Mt),t(Mt,Ht),t(Ht,Mn),t(h,Hn),t(h,Bt),t(Bt,Bn),t(h,Jn),g(ke,h,null),t(h,Kn),t(h,Jt),t(Jt,Qn),t(h,Xn),g(Ne,h,null),t(h,Yn),t(h,L),t(L,Zn),t(L,Kt),t(Kt,eo),t(L,to),t(L,ut),t(ut,ao),t(L,ro),t(L,Qt),t(Qt,no),t(L,oo),t(h,so),g(Fe,h,null),t(h,io),t(h,Xt),t(Xt,lo),t(h,co),g(Ce,h,null),t(h,mo),t(h,ne),g(Oe,ne,null),t(ne,po),t(ne,Yt),t(Yt,ho),d(e,Na,p),d(e,F,p),t(F,oe),t(oe,Zt),g(Re,Zt,null),t(F,uo),t(F,ea),t(ea,fo),d(e,Fa,p),d(e,z,p),g(je,z,null),t(z,go),t(z,C),t(C,_o),t(C,ta),t(ta,wo),t(C,vo),t(C,qe),t(qe,yo),t(C,bo),t(z,$o),t(z,aa),t(aa,Ao),t(z,zo),t(z,se),g(Ue,se,null),t(se,Eo),t(se,ra),t(ra,xo),d(e,Ca,p),d(e,O,p),g(Ge,O,null),t(O,To),t(O,na),t(na,Do),d(e,Oa,p),d(e,R,p),t(R,ie),t(ie,oa),g(Ve,oa,null),t(R,Lo),t(R,sa),t(sa,Po),d(e,Ra,p),d(e,j,p),t(j,le),t(le,ia),g(Me,ia,null),t(j,Wo),t(j,la),t(la,So),d(e,ja,p),d(e,q,p),g(He,q,null),t(q,Io),t(q,ca),t(ca,ko),d(e,qa,p),d(e,U,p),g(Be,U,null),t(U,No),t(U,ma),t(ma,Fo),d(e,Ua,p),d(e,G,p),g(Je,G,null),t(G,Co),t(G,pa),t(pa,Oo),d(e,Ga,p),d(e,V,p),g(Ke,V,null),t(V,Ro),t(V,da),t(da,jo),d(e,Va,p),d(e,Qe,p),d(e,Ma,p),d(e,M,p),g(Xe,M,null),t(M,qo),t(M,ha),t(ha,Uo),d(e,Ha,p),d(e,Ye,p),d(e,Ba,p),d(e,H,p),g(Ze,H,null),t(H,Go),t(H,ua),t(ua,Vo),d(e,Ja,p),d(e,et,p),d(e,Ka,p),d(e,B,p),g(tt,B,null),t(B,Mo),t(B,fa),t(fa,Ho),d(e,Qa,p),d(e,at,p),d(e,Xa,p),d(e,P,p),g(rt,P,null),t(P,Bo),t(P,nt),t(nt,Jo),t(nt,ga),t(ga,Ko),t(nt,Qo),t(P,Xo),t(P,ce),t(ce,Yo),t(ce,_a),t(_a,Zo),t(ce,es),t(ce,ot),t(ot,ts),d(e,Ya,p),d(e,J,p),t(J,me),t(me,wa),g(st,wa,null),t(J,as),t(J,va),t(va,rs),d(e,Za,p),d(e,K,p),g(it,K,null),t(K,ns),t(K,ya),t(ya,os),d(e,er,p),d(e,Q,p),t(Q,pe),t(pe,ba),g(lt,ba,null),t(Q,ss),t(Q,$a),t($a,is),d(e,tr,p),d(e,X,p),t(X,de),t(de,Aa),g(ct,Aa,null),t(X,ls),t(X,za),t(za,cs),d(e,ar,p),d(e,W,p),g(mt,W,null),t(W,ms),t(W,Y),t(Y,ps),t(Y,Ea),t(Ea,ds),t(Y,hs),t(Y,xa),t(xa,us),t(Y,fs),t(W,gs),t(W,he),g(pt,he,null),t(he,_s),t(he,Ta),t(Ta,ws),rr=!0},p:Yi,i(e){rr||(_(_e.$$.fragment,e),_(ve.$$.fragment,e),_(ye.$$.fragment,e),_(Ae.$$.fragment,e),_(ze.$$.fragment,e),_(Ee.$$.fragment,e),_(ke.$$.fragment,e),_(Ne.$$.fragment,e),_(Fe.$$.fragment,e),_(Ce.$$.fragment,e),_(Oe.$$.fragment,e),_(Re.$$.fragment,e),_(je.$$.fragment,e),_(Ue.$$.fragment,e),_(Ge.$$.fragment,e),_(Ve.$$.fragment,e),_(Me.$$.fragment,e),_(He.$$.fragment,e),_(Be.$$.fragment,e),_(Je.$$.fragment,e),_(Ke.$$.fragment,e),_(Xe.$$.fragment,e),_(Ze.$$.fragment,e),_(tt.$$.fragment,e),_(rt.$$.fragment,e),_(st.$$.fragment,e),_(it.$$.fragment,e),_(lt.$$.fragment,e),_(ct.$$.fragment,e),_(mt.$$.fragment,e),_(pt.$$.fragment,e),rr=!0)},o(e){w(_e.$$.fragment,e),w(ve.$$.fragment,e),w(ye.$$.fragment,e),w(Ae.$$.fragment,e),w(ze.$$.fragment,e),w(Ee.$$.fragment,e),w(ke.$$.fragment,e),w(Ne.$$.fragment,e),w(Fe.$$.fragment,e),w(Ce.$$.fragment,e),w(Oe.$$.fragment,e),w(Re.$$.fragment,e),w(je.$$.fragment,e),w(Ue.$$.fragment,e),w(Ge.$$.fragment,e),w(Ve.$$.fragment,e),w(Me.$$.fragment,e),w(He.$$.fragment,e),w(Be.$$.fragment,e),w(Je.$$.fragment,e),w(Ke.$$.fragment,e),w(Xe.$$.fragment,e),w(Ze.$$.fragment,e),w(tt.$$.fragment,e),w(rt.$$.fragment,e),w(st.$$.fragment,e),w(it.$$.fragment,e),w(lt.$$.fragment,e),w(ct.$$.fragment,e),w(mt.$$.fragment,e),w(pt.$$.fragment,e),rr=!1},d(e){a(S),e&&a(dt),e&&a(E),v(_e),e&&a(La),e&&a(ee),e&&a(Pa),e&&a(I),e&&a(Wa),e&&a(k),v(ve),e&&a(Sa),e&&a(T),v(ye),v(Ae),e&&a(Ia),e&&a(N),v(ze),e&&a(ka),e&&a(h),v(Ee),v(ke),v(Ne),v(Fe),v(Ce),v(Oe),e&&a(Na),e&&a(F),v(Re),e&&a(Fa),e&&a(z),v(je),v(Ue),e&&a(Ca),e&&a(O),v(Ge),e&&a(Oa),e&&a(R),v(Ve),e&&a(Ra),e&&a(j),v(Me),e&&a(ja),e&&a(q),v(He),e&&a(qa),e&&a(U),v(Be),e&&a(Ua),e&&a(G),v(Je),e&&a(Ga),e&&a(V),v(Ke),e&&a(Va),e&&a(Qe),e&&a(Ma),e&&a(M),v(Xe),e&&a(Ha),e&&a(Ye),e&&a(Ba),e&&a(H),v(Ze),e&&a(Ja),e&&a(et),e&&a(Ka),e&&a(B),v(tt),e&&a(Qa),e&&a(at),e&&a(Xa),e&&a(P),v(rt),e&&a(Ya),e&&a(J),v(st),e&&a(Za),e&&a(K),v(it),e&&a(er),e&&a(Q),v(lt),e&&a(tr),e&&a(X),v(ct),e&&a(ar),e&&a(W),v(mt),v(pt)}}}const el={local:"optimization",sections:[{local:"transformers.AdamW",title:"AdamW (PyTorch)"},{local:"transformers.Adafactor",title:"AdaFactor (PyTorch)"},{local:"transformers.AdamWeightDecay",title:"AdamWeightDecay (TensorFlow)"},{local:"schedules",sections:[{local:"transformers.SchedulerType",title:"Learning Rate Schedules (Pytorch)"},{local:"transformers.WarmUp",title:"Warmup (TensorFlow)"}],title:"Schedules"},{local:"gradient-strategies",sections:[{local:"transformers.GradientAccumulator",title:"GradientAccumulator (TensorFlow)"}],title:"Gradient Strategies"}],title:"Optimization"};function tl(Fr,S,dt){let{fw:E}=S;return Fr.$$set=x=>{"fw"in x&&dt(0,E=x.fw)},[E]}class sl extends Ji{constructor(S){super();Ki(this,S,tl,Zi,Qi,{fw:0})}}export{sl as default,el as metadata};
