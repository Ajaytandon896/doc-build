import{S as Cm,i as Im,s as Wm,e as o,k as l,w as m,t,M as Bm,c as a,d as s,m as d,a as r,x as g,h as n,b as c,F as e,g as M,y as u,q as _,o as h,B as f}from"../../chunks/vendor-6b77c823.js";import{T as Dm}from"../../chunks/Tip-39098574.js";import{D as ue}from"../../chunks/Docstring-abef54e3.js";import{C as be}from"../../chunks/CodeBlock-3a8b25a8.js";import{I as Wo}from"../../chunks/IconCopyLink-7a11ce68.js";function Hm(lt){let p,z,y,w,D,T,_e,C,q,I,G;return{c(){p=o("p"),z=t("Apart from "),y=o("code"),w=t("inputs"),D=t(`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),T=o("code"),_e=t("config.json"),C=t(`) which in turn defaults to the
`),q=o("a"),I=t("PretrainedConfig"),G=t(" of the model."),this.h()},l(S){p=a(S,"P",{});var x=r(p);z=n(x,"Apart from "),y=a(x,"CODE",{});var ye=r(y);w=n(ye,"inputs"),ye.forEach(s),D=n(x,`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),T=a(x,"CODE",{});var je=r(T);_e=n(je,"config.json"),je.forEach(s),C=n(x,`) which in turn defaults to the
`),q=a(x,"A",{href:!0});var xe=r(q);I=n(xe,"PretrainedConfig"),xe.forEach(s),G=n(x," of the model."),x.forEach(s),this.h()},h(){c(q,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig")},m(S,x){M(S,p,x),e(p,z),e(p,y),e(y,w),e(p,D),e(p,T),e(T,_e),e(p,C),e(p,q),e(q,I),e(p,G)},d(S){S&&s(p)}}}function Rm(lt){let p,z,y,w,D,T,_e,C,q,I,G;return{c(){p=o("p"),z=t("Apart from "),y=o("code"),w=t("inputs"),D=t(`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),T=o("code"),_e=t("config.json"),C=t(`) which in turn defaults to the
`),q=o("a"),I=t("PretrainedConfig"),G=t(" of the model."),this.h()},l(S){p=a(S,"P",{});var x=r(p);z=n(x,"Apart from "),y=a(x,"CODE",{});var ye=r(y);w=n(ye,"inputs"),ye.forEach(s),D=n(x,`, all the arguments below will default to the value of the attribute of the same name as
defined in the model\u2019s config (`),T=a(x,"CODE",{});var je=r(T);_e=n(je,"config.json"),je.forEach(s),C=n(x,`) which in turn defaults to the
`),q=a(x,"A",{href:!0});var xe=r(q);I=n(xe,"PretrainedConfig"),xe.forEach(s),G=n(x," of the model."),x.forEach(s),this.h()},h(){c(q,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig")},m(S,x){M(S,p,x),e(p,z),e(p,y),e(y,w),e(p,D),e(p,T),e(T,_e),e(p,C),e(p,q),e(q,I),e(p,G)},d(S){S&&s(p)}}}function Um(lt){let p,z,y,w,D,T,_e,C,q,I,G,S,x,ye,je,xe,ke,Le,Bo,tn,Ho,Ro,nn,Uo,Vo,Ko,Me,Zo,sn,Xo,Jo,on,Qo,Yo,ea,we,ta,an,na,sa,rn,oa,aa,ho,Te,Xe,$n,dt,ra,Fn,ia,fo,k,ct,la,pt,da,ln,ca,pa,ma,mt,ga,dn,ua,_a,ha,$,W,An,fa,ba,cn,xa,ka,zn,va,ya,Pn,ja,La,Ma,B,Nn,wa,Ta,pn,Ea,Oa,Dn,qa,Ga,Cn,Sa,$a,Fa,H,In,Aa,za,mn,Pa,Na,Wn,Da,Ca,Bn,Ia,Wa,Ba,R,Hn,Ha,Ra,gn,Ua,Va,Rn,Ka,Za,Un,Xa,Ja,Qa,U,Vn,Ya,er,un,tr,nr,Kn,sr,or,Zn,ar,rr,ir,V,Xn,lr,dr,_n,cr,pr,Jn,mr,gr,Qn,ur,_r,hr,b,gt,fr,Yn,br,xr,F,K,es,kr,vr,hn,yr,jr,ts,Lr,Mr,ns,wr,Tr,Er,Z,ss,Or,qr,fn,Gr,Sr,os,$r,Fr,as,Ar,zr,Pr,X,rs,Nr,Dr,bn,Cr,Ir,is,Wr,Br,ls,Hr,Rr,Ur,J,ds,Vr,Kr,xn,Zr,Xr,cs,Jr,Qr,ps,Yr,ei,ti,Q,ms,ni,si,kn,oi,ai,gs,ri,ii,us,li,di,ci,Y,_s,pi,mi,vn,gi,ui,hs,_i,hi,fs,fi,bi,xi,Je,ki,ut,vi,_t,yi,ji,Li,bs,Mi,wi,xs,Ti,Ei,ht,Oi,ks,qi,Gi,ft,Si,vs,$i,Fi,bt,Ai,ee,xt,zi,kt,Pi,ys,Ni,Di,Ci,js,Ii,Wi,vt,Bi,te,yt,Hi,jt,Ri,Ls,Ui,Vi,Ki,Ms,Zi,Xi,Lt,Ji,ne,Mt,Qi,wt,Yi,ws,el,tl,nl,Ts,sl,ol,Tt,al,se,Et,rl,Ot,il,Es,ll,dl,cl,Os,pl,ml,qt,gl,oe,Gt,ul,St,_l,qs,hl,fl,bl,Gs,xl,kl,$t,vl,ae,Ft,yl,At,jl,Ss,Ll,Ml,wl,$s,Tl,El,zt,bo,Ee,Qe,Fs,Pt,Ol,As,ql,xo,he,Nt,Gl,Dt,Sl,yn,$l,Fl,Al,E,Ct,zl,zs,Pl,Nl,It,Dl,Wt,Cl,Il,Wl,fe,Bl,Ps,Hl,Rl,Ns,Ul,Vl,jn,Kl,Zl,Xl,Bt,Jl,Ht,Ql,Yl,ed,Ds,td,nd,Rt,ko,Oe,Ye,Cs,Ut,sd,Is,od,vo,A,Vt,ad,Kt,rd,Ln,id,ld,dd,Zt,cd,Mn,pd,md,gd,qe,re,Ws,ud,_d,Bs,hd,fd,Hs,bd,xd,Rs,kd,vd,yd,ie,Us,jd,Ld,Vs,Md,wd,Ks,Td,Ed,Zs,Od,qd,Gd,le,Xs,Sd,$d,Js,Fd,Ad,Qs,zd,Pd,Ys,Nd,Dd,Cd,O,Xt,Id,eo,Wd,Bd,Ge,de,to,Hd,Rd,no,Ud,Vd,so,Kd,Zd,oo,Xd,Jd,Qd,ce,ao,Yd,ec,ro,tc,nc,io,sc,oc,lo,ac,rc,ic,pe,co,lc,dc,po,cc,pc,mo,mc,gc,go,uc,_c,hc,et,fc,Jt,bc,Qt,xc,kc,vc,uo,yc,jc,Yt,yo;return T=new Wo({}),dt=new Wo({}),ct=new ue({props:{name:"class transformers.generation_utils.GenerationMixin",anchor:"transformers.generation_utils.GenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L379"}}),gt=new ue({props:{name:"generate",anchor:"transformers.generation_utils.GenerationMixin.generate",parameters:[{name:"inputs",val:": typing.Optional[torch.Tensor] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"min_length",val:": typing.Optional[int] = None"},{name:"do_sample",val:": typing.Optional[bool] = None"},{name:"early_stopping",val:": typing.Optional[bool] = None"},{name:"num_beams",val:": typing.Optional[int] = None"},{name:"temperature",val:": typing.Optional[float] = None"},{name:"top_k",val:": typing.Optional[int] = None"},{name:"top_p",val:": typing.Optional[float] = None"},{name:"typical_p",val:": typing.Optional[float] = None"},{name:"repetition_penalty",val:": typing.Optional[float] = None"},{name:"bad_words_ids",val:": typing.Optional[typing.Iterable[int]] = None"},{name:"force_words_ids",val:": typing.Union[typing.Iterable[int], typing.Iterable[typing.Iterable[int]], NoneType] = None"},{name:"bos_token_id",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"length_penalty",val:": typing.Optional[float] = None"},{name:"no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"encoder_no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"num_return_sequences",val:": typing.Optional[int] = None"},{name:"max_time",val:": typing.Optional[float] = None"},{name:"max_new_tokens",val:": typing.Optional[int] = None"},{name:"decoder_start_token_id",val:": typing.Optional[int] = None"},{name:"use_cache",val:": typing.Optional[bool] = None"},{name:"num_beam_groups",val:": typing.Optional[int] = None"},{name:"diversity_penalty",val:": typing.Optional[float] = None"},{name:"prefix_allowed_tokens_fn",val:": typing.Union[typing.Callable[[int, torch.Tensor], typing.List[int]], NoneType] = None"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = []"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = []"},{name:"constraints",val:": typing.Optional[typing.List[transformers.generation_beam_constraints.Constraint]] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"forced_bos_token_id",val:": typing.Optional[int] = None"},{name:"forced_eos_token_id",val:": typing.Optional[int] = None"},{name:"remove_invalid_values",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"exponential_decay_length_penalty",val:": typing.Union[typing.Tuple[typing.Union[int, float]], NoneType] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L832",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.generate.inputs",description:`<strong>inputs</strong> (<code>torch.Tensor</code> of varying shape depending on the modality, <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should of in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.`,name:"inputs"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to <code>model.config.max_length</code>) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_new_tokens",description:`<strong>max_new_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to None) &#x2014;
The maximum numbers of tokens to generate, ignore the current number of tokens. Use either
<code>max_new_tokens</code> or <code>max_length</code> but not both, they serve the same purpose.`,name:"max_new_tokens"},{anchor:"transformers.generation_utils.GenerationMixin.generate.min_length",description:`<strong>min_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The minimum length of the sequence to be generated.`,name:"min_length"},{anchor:"transformers.generation_utils.GenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_utils.GenerationMixin.generate.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop the beam search when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_utils.GenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_utils.GenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_utils.GenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_utils.GenerationMixin.generate.repetition_penalty",description:`<strong>repetition_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" rel="nofollow">this
paper</a> for more details.`,name:"repetition_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Exponential penalty to the length. 1.0 means no penalty. Set to values &lt; 1.0 in order to encourage the
model to generate shorter sequences, to a value &gt; 1.0 in order to encourage the model to produce longer
sequences.`,name:"length_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.no_repeat_ngram_size",description:`<strong>no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size can only occur once.`,name:"no_repeat_ngram_size"},{anchor:"transformers.generation_utils.GenerationMixin.generate.encoder_no_repeat_ngram_size",description:`<strong>encoder_no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size that occur in the <code>encoder_input_ids</code> cannot occur in the
<code>decoder_input_ids</code>.`,name:"encoder_no_repeat_ngram_size"},{anchor:"transformers.generation_utils.GenerationMixin.generate.bad_words_ids(List[List[int]],",description:`<strong>bad_words_ids(<code>List[List[int]]</code>,</strong> <em>optional</em>) &#x2014;
List of token ids that are not allowed to be generated. In order to get the token ids of the words that
should not appear in the generated text, use <code>tokenizer(bad_words, add_prefix_space=True, add_special_tokens=False).input_ids</code>.`,name:"bad_words_ids(List[List[int]],"},{anchor:"transformers.generation_utils.GenerationMixin.generate.force_words_ids(List[List[int]]",description:`<strong>force_words_ids(<code>List[List[int]]</code></strong> or <code>List[List[List[int]]]</code>, <em>optional</em>) &#x2014;
List of token ids that must be generated. If given a <code>List[List[int]]</code>, this is treated as a simple
list of words that must be included, the opposite to <code>bad_words_ids</code>. If given <code>List[List[List[int]]]</code>,
this triggers a <a href="https://github.com/huggingface/transformers/issues/14081" rel="nofollow">disjunctive constraint</a>,
where one can allow different forms of each word.`,name:"force_words_ids(List[List[int]]"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_return_sequences(int,",description:`<strong>num_return_sequences(<code>int</code>,</strong> <em>optional</em>, defaults to 1) &#x2014;
The number of independently computed returned sequences for each element in the batch.`,name:"num_return_sequences(int,"},{anchor:"transformers.generation_utils.GenerationMixin.generate.max_time(float,",description:`<strong>max_time(<code>float</code>,</strong> <em>optional</em>, defaults to None) &#x2014;
The maximum amount of time you allow the computation to run for in seconds. generation will still
finish the current pass after allocated time has been passed.`,name:"max_time(float,"},{anchor:"transformers.generation_utils.GenerationMixin.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values are in <code>[0, 1]</code>, 1 for tokens
that are not masked, and 0 for masked tokens. If not provided, will default to a tensor the same shape
as <code>input_ids</code> that masks the pad token. <a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.generation_utils.GenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
use_cache &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.num_beam_groups",description:`<strong>num_beam_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of groups to divide <code>num_beams</code> into in order to ensure diversity among different groups of
beams. <a href="https://arxiv.org/pdf/1610.02424.pdf" rel="nofollow">this paper</a> for more details.`,name:"num_beam_groups"},{anchor:"transformers.generation_utils.GenerationMixin.generate.diversity_penalty",description:`<strong>diversity_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
This value is subtracted from a beam&#x2019;s score if it generates a token same as any beam from other group
at a particular time. Note that <code>diversity_penalty</code> is only effective if <code>group beam search</code> is
enabled.
prefix_allowed_tokens_fn &#x2014; (<code>Callable[[int, torch.Tensor], List[int]]</code>, <em>optional</em>):
If provided, this function constraints the beam search to allowed tokens only at each step. If not
provided no constraint is applied. This function takes 2 arguments: the batch ID <code>batch_id</code> and
<code>input_ids</code>. It has to return a list with the allowed tokens for the next generation step conditioned
on the batch ID <code>batch_id</code> and the previously generated tokens <code>inputs_ids</code>. This argument is useful
for constrained generation conditioned on the prefix, as described in <a href="https://arxiv.org/abs/2010.00904" rel="nofollow">Autoregressive Entity
Retrieval</a>.`,name:"diversity_penalty"},{anchor:"transformers.generation_utils.GenerationMixin.generate.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
Custom logits processors that complement the default logits processors built from arguments and a
model&#x2019;s config. If a logit processor is passed that is already created with the arguments or a model&#x2019;s
config an error is thrown. This feature is intended for advanced users.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.generate.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
Custom stopping criteria that complement the default stopping criteria built from arguments and a
model&#x2019;s config. If a stopping criteria is passed that is already created with the arguments or a
model&#x2019;s config an error is thrown. This feature is intended for advanced users.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.generate.constraints",description:`<strong>constraints</strong> (<code>List[Constraint]</code>, <em>optional</em>) &#x2014;
Custom constraints that can be added to the generation to ensure that the output will contain the use
of certain tokens as defined by <code>Constraint</code> objects, in the most sensible way possible.`,name:"constraints"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.generate.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.generate.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.generate.forced_bos_token_id",description:`<strong>forced_bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the first generated token after the <code>decoder_start_token_id</code>. Useful
for multilingual models like <a href="../model_doc/mbart">mBART</a> where the first generated token needs to be
the target language token.`,name:"forced_bos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached.`,name:"forced_eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.generate.remove_invalid_values",description:`<strong>remove_invalid_values</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to remove possible <em>nan</em> and <em>inf</em> outputs of the model to prevent the generation method to
crash. Note that using <code>remove_invalid_values</code> can slow down generation.`,name:"remove_invalid_values"},{anchor:"transformers.generation_utils.GenerationMixin.generate.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)`,name:"synced_gpus"},{anchor:"transformers.generation_utils.GenerationMixin.generate.exponential_decay_length_penalty",description:`<strong>exponential_decay_length_penalty</strong> (<code>tuple(int, float)</code>, <em>optional</em>) &#x2014;
This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been
generated. The tuple shall consist of: <code>(start_index, decay_factor)</code> where <code>start_index</code> indicates
where penalty starts and <code>decay_factor</code> represents the factor of exponential decay</p>
<p>model<em>kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with *decoder</em>*.`,name:"exponential_decay_length_penalty"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> (if
<code>return_dict_in_generate=True</code> or when <code>config.return_dict_in_generate=True</code>) or a <code>torch.FloatTensor</code>.</p>
<p>If the model is <em>not</em> an encoder-decoder model (<code>model.config.is_encoder_decoder=False</code>), the possible
<a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a></li>
</ul>
<p>If the model is an encoder-decoder model (<code>model.config.is_encoder_decoder=True</code>), the possible
<a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> types are:</p>
<ul>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a>,</li>
<li><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a></li>
</ul>
`,returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> or <code>torch.LongTensor</code></p>
`}}),Je=new Dm({props:{warning:"&lcub;true}",$$slots:{default:[Hm]},$$scope:{ctx:lt}}}),ht=new be({props:{code:`from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = "Today I believe we can finally"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# generate up to 30 tokens
outputs = model.generate(input_ids, do_sample=False, max_length=30)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today I believe we can finally&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate up to 30 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, do_sample=<span class="hljs-literal">False</span>, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today I believe we can finally get to the point where we can make a difference in the lives of the people of the United States of America.\\n&#x27;</span>]`}}),ft=new be({props:{code:`from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

prompt = "Today I believe we can finally"
input_ids = tokenizer(prompt, return_tensors="pt").input_ids

# sample up to 30 tokens
torch.manual_seed(0)
outputs = model.generate(input_ids, do_sample=True, max_length=30)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Today I believe we can finally&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># sample up to 30 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids, do_sample=<span class="hljs-literal">True</span>, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today I believe we can finally get rid of discrimination,&quot; said Rep. Mark Pocan (D-Wis.).\\n\\n&quot;Just look at the&#x27;</span>]`}}),bt=new be({props:{code:`from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("Helsinki-NLP/opus-mt-en-de")
model = AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-de")

sentence = "Paris is one of the densest populated areas in Europe."
input_ids = tokenizer(sentence, return_tensors="pt").input_ids

outputs = model.generate(input_ids)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;Helsinki-NLP/opus-mt-en-de&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>sentence = <span class="hljs-string">&quot;Paris is one of the densest populated areas in Europe.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(sentence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Paris ist eines der dichtesten besiedelten Gebiete Europas.&#x27;</span>]`}}),xt=new ue({props:{name:"greedy_search",anchor:"transformers.generation_utils.GenerationMixin.greedy_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L1489",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.greedy_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific keyword arguments will be forwarded to the <code>forward</code> function of the model.
If model is an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a>
or <code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchDecoderOnlyOutput"
>GreedySearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.GreedySearchEncoderDecoderOutput"
>GreedySearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),vt=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    StoppingCriteriaList,
    MaxLengthCriteria,
)

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "It might be possible to"
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(10, eos_token_id=model.config.eos_token_id),
    ]
)
stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

outputs = model.greedy_search(
    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForCausalLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    StoppingCriteriaList,
<span class="hljs-meta">... </span>    MaxLengthCriteria,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>input_prompt = <span class="hljs-string">&quot;It might be possible to&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">10</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=<span class="hljs-number">20</span>)])

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.greedy_search(
<span class="hljs-meta">... </span>    input_ids, logits_processor=logits_processor, stopping_criteria=stopping_criteria
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&quot;It might be possible to get a better understanding of the nature of the problem, but it&#x27;s not&quot;</span>]`}}),yt=new ue({props:{name:"sample",anchor:"transformers.generation_utils.GenerationMixin.sample",parameters:[{name:"input_ids",val:": LongTensor"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"logits_warper",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L1721",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.sample.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.sample.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.sample.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.sample.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.sample.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.sample.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.sample.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.sample.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.sample.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.sample.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleDecoderOnlyOutput"
>SampleDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.SampleEncoderDecoderOutput"
>SampleEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),Lt=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    TopKLogitsWarper,
    TemperatureLogitsWarper,
    StoppingCriteriaList,
    MaxLengthCriteria,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")

# set pad_token_id to eos_token_id because GPT2 does not have a EOS token
model.config.pad_token_id = model.config.eos_token_id

input_prompt = "Today is a beautiful day, and"
input_ids = tokenizer(input_prompt, return_tensors="pt").input_ids

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(15, eos_token_id=model.config.eos_token_id),
    ]
)
# instantiate logits processors
logits_warper = LogitsProcessorList(
    [
        TopKLogitsWarper(50),
        TemperatureLogitsWarper(0.7),
    ]
)

stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=20)])

torch.manual_seed(0)
outputs = model.sample(
    input_ids,
    logits_processor=logits_processor,
    logits_warper=logits_warper,
    stopping_criteria=stopping_criteria,
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForCausalLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    TopKLogitsWarper,
<span class="hljs-meta">... </span>    TemperatureLogitsWarper,
<span class="hljs-meta">... </span>    StoppingCriteriaList,
<span class="hljs-meta">... </span>    MaxLengthCriteria,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set pad_token_id to eos_token_id because GPT2 does not have a EOS token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id = model.config.eos_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span>input_prompt = <span class="hljs-string">&quot;Today is a beautiful day, and&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">15</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_warper = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        TopKLogitsWarper(<span class="hljs-number">50</span>),
<span class="hljs-meta">... </span>        TemperatureLogitsWarper(<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>stopping_criteria = StoppingCriteriaList([MaxLengthCriteria(max_length=<span class="hljs-number">20</span>)])

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.sample(
<span class="hljs-meta">... </span>    input_ids,
<span class="hljs-meta">... </span>    logits_processor=logits_processor,
<span class="hljs-meta">... </span>    logits_warper=logits_warper,
<span class="hljs-meta">... </span>    stopping_criteria=stopping_criteria,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Today is a beautiful day, and a wonderful day.\\n\\nI was lucky enough to meet the&#x27;</span>]`}}),Mt=new ue({props:{name:"beam_search",anchor:"transformers.generation_utils.GenerationMixin.beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L1977",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
An derived instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><code>generation_utilsBeamSearchDecoderOnlyOutput</code>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),Tt=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    num_beams=num_beams,
    device=model.device,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.beam_search(input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),Et=new ue({props:{name:"beam_sample",anchor:"transformers.generation_utils.GenerationMixin.beam_sample",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"logits_warper",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L2289",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
A derived instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.beam_sample.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleDecoderOnlyOutput"
>BeamSampleDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSampleEncoderDecoderOutput"
>BeamSampleEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),qt=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    TopKLogitsWarper,
    TemperatureLogitsWarper,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids

# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    max_length=model.config.max_length,
    num_beams=num_beams,
    device=model.device,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id)]
)
# instantiate logits processors
logits_warper = LogitsProcessorList(
    [
        TopKLogitsWarper(50),
        TemperatureLogitsWarper(0.7),
    ]
)

outputs = model.beam_sample(
    input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    TopKLogitsWarper,
<span class="hljs-meta">... </span>    TemperatureLogitsWarper,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_length=model.config.max_length,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id)]
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_warper = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        TopKLogitsWarper(<span class="hljs-number">50</span>),
<span class="hljs-meta">... </span>        TemperatureLogitsWarper(<span class="hljs-number">0.7</span>),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.beam_sample(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, logits_processor=logits_processor, logits_warper=logits_warper, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),Gt=new ue({props:{name:"group_beam_search",anchor:"transformers.generation_utils.GenerationMixin.group_beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"beam_scorer",val:": BeamScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = False"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L2611",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.beam_scorer",description:`<strong>beam_scorer</strong> (<code>BeamScorer</code>) &#x2014;
An derived instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation. For more information, the documentation of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> should be read.`,name:"beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.group_beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)</p>
<p>model_kwargs &#x2014;
Additional model specific kwargs that will be forwarded to the <code>forward</code> function of the model. If
model is an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if
<code>model.config.is_encoder_decoder=False</code> and <code>return_dict_in_generate=True</code> or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if <code>model.config.is_encoder_decoder=True</code>.</p>
`}}),$t=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    HammingDiversityLogitsProcessor,
    BeamSearchScorer,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run diverse beam search using 6 beams
num_beams = 6
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

# instantiate beam scorer
beam_scorer = BeamSearchScorer(
    batch_size=1,
    max_length=model.config.max_length,
    num_beams=num_beams,
    device=model.device,
    num_beam_groups=3,
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        HammingDiversityLogitsProcessor(5.5, num_beams=6, num_beam_groups=3),
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.group_beam_search(
    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    HammingDiversityLogitsProcessor,
<span class="hljs-meta">... </span>    BeamSearchScorer,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run diverse beam search using 6 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">6</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = BeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_length=model.config.max_length,
<span class="hljs-meta">... </span>    num_beams=num_beams,
<span class="hljs-meta">... </span>    device=model.device,
<span class="hljs-meta">... </span>    num_beam_groups=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        HammingDiversityLogitsProcessor(<span class="hljs-number">5.5</span>, num_beams=<span class="hljs-number">6</span>, num_beam_groups=<span class="hljs-number">3</span>),
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.group_beam_search(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, logits_processor=logits_processor, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt bist du?&#x27;</span>]`}}),Ft=new ue({props:{name:"constrained_beam_search",anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search",parameters:[{name:"input_ids",val:": LongTensor"},{name:"constrained_beam_scorer",val:": ConstrainedBeamSearchScorer"},{name:"logits_processor",val:": typing.Optional[transformers.generation_logits_process.LogitsProcessorList] = None"},{name:"stopping_criteria",val:": typing.Optional[transformers.generation_stopping_criteria.StoppingCriteriaList] = None"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"output_attentions",val:": typing.Optional[bool] = None"},{name:"output_hidden_states",val:": typing.Optional[bool] = None"},{name:"output_scores",val:": typing.Optional[bool] = None"},{name:"return_dict_in_generate",val:": typing.Optional[bool] = None"},{name:"synced_gpus",val:": typing.Optional[bool] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_utils.py#L2976",parametersDescription:[{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.constrained_beam_scorer",description:`<strong>constrained_beam_scorer</strong> (<code>ConstrainedBeamSearchScorer</code>) &#x2014;
A derived instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.BeamScorer">BeamScorer</a> that defines how beam hypotheses are constructed, stored and
sorted during generation, while satisfying a list of positive constraints. For more information, the
documentation of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.ConstrainedBeamSearchScorer">ConstrainedBeamSearchScorer</a> should be read.`,name:"constrained_beam_scorer"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.logits_processor",description:`<strong>logits_processor</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessor">LogitsProcessor</a>
used to modify the prediction scores of the language modeling head applied at each generation step.`,name:"logits_processor"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.stopping_criteria",description:`<strong>stopping_criteria</strong> (<code>StoppingCriteriaList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteriaList">StoppingCriteriaList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.StoppingCriteria">StoppingCriteria</a>
used to tell if the generation loop should stop.`,name:"stopping_criteria"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.logits_warper",description:`<strong>logits_warper</strong> (<code>LogitsProcessorList</code>, <em>optional</em>) &#x2014;
An instance of <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsProcessorList">LogitsProcessorList</a>. List of instances of class derived from <a href="/docs/transformers/master/en/internal/generation_utils#transformers.LogitsWarper">LogitsWarper</a> used
to warp the prediction score distribution of the language modeling head applied before multinomial
sampling at each generation step.`,name:"logits_warper"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
<strong>DEPRECATED</strong>. Use <code>logits_processor</code> or <code>stopping_criteria</code> directly to cap the number of generated
tokens. The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_utils.GenerationMixin.constrained_beam_search.synced_gpus",description:`<strong>synced_gpus</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to continue running the while loop until max_length (needed for ZeRO stage 3)
model_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If model is
an encoder-decoder model the kwargs should include <code>encoder_outputs</code>.`,name:"synced_gpus"}],returnDescription:`
<p><code>generation_utilsBeamSearchDecoderOnlyOutput</code>, <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> or
<code>torch.LongTensor</code>: A <code>torch.LongTensor</code> containing the generated tokens (default behaviour) or a
<a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchDecoderOnlyOutput"
>BeamSearchDecoderOnlyOutput</a> if <code>model.config.is_encoder_decoder=False</code> and
<code>return_dict_in_generate=True</code> or a <a
  href="/docs/transformers/master/en/internal/generation_utils#transformers.generation_utils.BeamSearchEncoderDecoderOutput"
>BeamSearchEncoderDecoderOutput</a> if
<code>model.config.is_encoder_decoder=True</code>.</p>
`}}),zt=new be({props:{code:`from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    LogitsProcessorList,
    MinLengthLogitsProcessor,
    ConstrainedBeamSearchScorer,
    PhrasalConstraint,
)
import torch

tokenizer = AutoTokenizer.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

encoder_input_str = "translate English to German: How old are you?"
encoder_input_ids = tokenizer(encoder_input_str, return_tensors="pt").input_ids


# lets run beam search using 3 beams
num_beams = 3
# define decoder start token ids
input_ids = torch.ones((num_beams, 1), device=model.device, dtype=torch.long)
input_ids = input_ids * model.config.decoder_start_token_id

# add encoder_outputs to model keyword arguments
model_kwargs = {
    "encoder_outputs": model.get_encoder()(
        encoder_input_ids.repeat_interleave(num_beams, dim=0), return_dict=True
    )
}

constraint_str = "Sie"
constraint_token_ids = tokenizer.encode(constraint_str)[:-1]  # slice to remove eos token
constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


# instantiate beam scorer
beam_scorer = ConstrainedBeamSearchScorer(
    batch_size=1, num_beams=num_beams, device=model.device, constraints=constraints
)

# instantiate logits processors
logits_processor = LogitsProcessorList(
    [
        MinLengthLogitsProcessor(5, eos_token_id=model.config.eos_token_id),
    ]
)

outputs = model.constrained_beam_search(
    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
)

tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    AutoTokenizer,
<span class="hljs-meta">... </span>    AutoModelForSeq2SeqLM,
<span class="hljs-meta">... </span>    LogitsProcessorList,
<span class="hljs-meta">... </span>    MinLengthLogitsProcessor,
<span class="hljs-meta">... </span>    ConstrainedBeamSearchScorer,
<span class="hljs-meta">... </span>    PhrasalConstraint,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_str = <span class="hljs-string">&quot;translate English to German: How old are you?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoder_input_ids = tokenizer(encoder_input_str, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># lets run beam search using 3 beams</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_beams = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># define decoder start token ids</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.ones((num_beams, <span class="hljs-number">1</span>), device=model.device, dtype=torch.long)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = input_ids * model.config.decoder_start_token_id

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># add encoder_outputs to model keyword arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_kwargs = {
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;encoder_outputs&quot;</span>: model.get_encoder()(
<span class="hljs-meta">... </span>        encoder_input_ids.repeat_interleave(num_beams, dim=<span class="hljs-number">0</span>), return_dict=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>}

<span class="hljs-meta">&gt;&gt;&gt; </span>constraint_str = <span class="hljs-string">&quot;Sie&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>constraint_token_ids = tokenizer.encode(constraint_str)[:-<span class="hljs-number">1</span>]  <span class="hljs-comment"># slice to remove eos token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>constraints = [PhrasalConstraint(token_ids=constraint_token_ids)]


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate beam scorer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>beam_scorer = ConstrainedBeamSearchScorer(
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">1</span>, num_beams=num_beams, device=model.device, constraints=constraints
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># instantiate logits processors</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_processor = LogitsProcessorList(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        MinLengthLogitsProcessor(<span class="hljs-number">5</span>, eos_token_id=model.config.eos_token_id),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.constrained_beam_search(
<span class="hljs-meta">... </span>    input_ids, beam_scorer, constraints=constraints, logits_processor=logits_processor, **model_kwargs
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)
[<span class="hljs-string">&#x27;Wie alt sind Sie?&#x27;</span>]`}}),Pt=new Wo({}),Nt=new ue({props:{name:"class transformers.generation_tf_utils.TFGenerationMixin",anchor:"transformers.generation_tf_utils.TFGenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_tf_utils.py#L342"}}),Ct=new ue({props:{name:"generate",anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate",parameters:[{name:"input_ids",val:" = None"},{name:"max_length",val:" = None"},{name:"min_length",val:" = None"},{name:"do_sample",val:" = None"},{name:"early_stopping",val:" = None"},{name:"num_beams",val:" = None"},{name:"temperature",val:" = None"},{name:"top_k",val:" = None"},{name:"top_p",val:" = None"},{name:"repetition_penalty",val:" = None"},{name:"bad_words_ids",val:" = None"},{name:"bos_token_id",val:" = None"},{name:"pad_token_id",val:" = None"},{name:"eos_token_id",val:" = None"},{name:"length_penalty",val:" = None"},{name:"no_repeat_ngram_size",val:" = None"},{name:"num_return_sequences",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_start_token_id",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_scores",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict_in_generate",val:" = None"},{name:"forced_bos_token_id",val:" = None"},{name:"forced_eos_token_id",val:" = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_tf_utils.py#L362",parametersDescription:[{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.input_ids",description:"<strong>input_ids</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, `(batch_size, sequence_length, &#x2014;",name:"input_ids"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.feature_dim)`",description:`<strong>feature_dim)\`</strong> or <code>(batch_size, num_channels, height, width)</code>, <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation or as model inputs to the encoder. If <code>None</code> the
method initializes it with <code>bos_token_id</code> and a batch size of 1. For decoder-only models <code>inputs</code>
should of in the format of <code>input_ids</code>. For encoder-decoder models <em>inputs</em> can represent any of
<code>input_ids</code>, <code>input_values</code>, <code>input_features</code>, or <code>pixel_values</code>.`,name:"feature_dim)`"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.min_length",description:`<strong>min_length</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
The minimum length of the sequence to be generated.`,name:"min_length"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.early_stopping",description:`<strong>early_stopping</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop the beam search when at least <code>num_beams</code> sentences are finished per batch or not.`,name:"early_stopping"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.repetition_penalty",description:`<strong>repetition_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The parameter for repetition penalty. 1.0 means no penalty. See <a href="https://arxiv.org/pdf/1909.05858.pdf" rel="nofollow">this
paper</a> for more details.`,name:"repetition_penalty"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.length_penalty",description:`<strong>length_penalty</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Exponential penalty to the length. 1.0 means no penalty.</p>
<p>Set to values &lt; 1.0 in order to encourage the model to generate shorter sequences, to a value &gt; 1.0 in
order to encourage the model to produce longer sequences.`,name:"length_penalty"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.no_repeat_ngram_size",description:`<strong>no_repeat_ngram_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
If set to int &gt; 0, all ngrams of that size can only occur once.`,name:"no_repeat_ngram_size"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.bad_words_ids(List[int],",description:`<strong>bad_words_ids(<code>List[int]</code>,</strong> <em>optional</em>) &#x2014;
List of token ids that are not allowed to be generated. In order to get the tokens of the words that
should not appear in the generated text, use <code>tokenizer.encode(bad_word, add_prefix_space=True)</code>.`,name:"bad_words_ids(List[int],"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.num_return_sequences(int,",description:`<strong>num_return_sequences(<code>int</code>,</strong> <em>optional</em>, defaults to 1) &#x2014;
The number of independently computed returned sequences for each element in the batch.`,name:"num_return_sequences(int,"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>tf.Tensor</code> of <code>dtype=tf.int32</code> and shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values are in <code>[0, 1]</code>, 1 for tokens
that are not masked, and 0 for masked tokens.</p>
<p>If not provided, will default to a tensor the same shape as <code>input_ids</code> that masks the pad token.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.
use_cache &#x2014; (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>):
Whether or not the model should use the past last key/values attentions (if applicable to the model) to
speed up decoding.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under
returned tensors for more details.`,name:"output_attentions"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors
for more details.`,name:"output_hidden_states"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.output_scores",description:`<strong>output_scores</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the prediction scores. See <code>scores</code> under returned tensors for more details.`,name:"output_scores"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.return_dict_in_generate",description:`<strong>return_dict_in_generate</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict_in_generate"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.forced_bos_token_id",description:`<strong>forced_bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the first generated token after the <code>decoder_start_token_id</code>. Useful
for multilingual models like <a href="../model_doc/mbart">mBART</a> where the first generated token needs to be
the target language token.`,name:"forced_bos_token_id"},{anchor:"transformers.generation_tf_utils.TFGenerationMixin.generate.forced_eos_token_id",description:`<strong>forced_eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the token to force as the last generated token when <code>max_length</code> is reached.
model_specific_kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model.`,name:"forced_eos_token_id"}],returnType:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a> or <code>tf.Tensor</code></p>
`}}),Rt=new be({props:{code:`tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "distilgpt2"
)  # Download model and configuration from huggingface.co and cache.
outputs = model.generate(max_length=40)  # do greedy decoding
print(f"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("openai-gpt")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "openai-gpt"
)  # Download model and configuration from huggingface.co and cache.
input_context = "The dog"
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5
)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'
for i in range(3):  #  3 output sequences were generated
    print(f"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "distilgpt2"
)  # Download model and configuration from huggingface.co and cache.
input_context = "The dog"
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=40, temperature=0.7, num_return_sequences=3, do_sample=True
)  # generate 3 candidates using sampling
for i in range(3):  #  3 output sequences were generated
    print(f"Generated {i}: {tokenizer.decode(outputs[i], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("ctrl")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "ctrl"
)  # Download model and configuration from huggingface.co and cache.
input_context = "Legal My neighbor is"  # "Legal" is one of the control codes for ctrl
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2
)  # generate sequences
print(f"Generated: {tokenizer.decode(outputs[0], skip_special_tokens=True)}")

tokenizer = AutoTokenizer.from_pretrained("gpt2")  # Initialize tokenizer
model = TFAutoModelWithLMHead.from_pretrained(
    "gpt2"
)  # Download model and configuration from huggingface.co and cache.
input_context = "My cute dog"
bad_words_ids = [
    tokenizer.encode(bad_word, add_prefix_space=True) for bad_word in ["idiot", "stupid", "shut up"]
]
input_ids = tokenizer.encode(input_context, return_tensors="tf")  # encode input context
outputs = model.generate(
    input_ids=input_ids, max_length=100, do_sample=True, bad_words_ids=bad_words_ids
)  # generate sequences without allowing bad_words to be generated`,highlighted:`tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;distilgpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
outputs = model.generate(max_length=<span class="hljs-number">40</span>)  <span class="hljs-comment"># do greedy decoding</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated: <span class="hljs-subst">{tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai-gpt&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;openai-gpt&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;The dog&quot;</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, num_beams=<span class="hljs-number">5</span>, num_return_sequences=<span class="hljs-number">3</span>, temperature=<span class="hljs-number">1.5</span>
)  <span class="hljs-comment"># generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context &#x27;The dog&#x27;</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):  <span class="hljs-comment">#  3 output sequences were generated</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{i}</span>: <span class="hljs-subst">{tokenizer.decode(outputs[i], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;distilgpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;The dog&quot;</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">40</span>, temperature=<span class="hljs-number">0.7</span>, num_return_sequences=<span class="hljs-number">3</span>, do_sample=<span class="hljs-literal">True</span>
)  <span class="hljs-comment"># generate 3 candidates using sampling</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">3</span>):  <span class="hljs-comment">#  3 output sequences were generated</span>
    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated <span class="hljs-subst">{i}</span>: <span class="hljs-subst">{tokenizer.decode(outputs[i], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;ctrl&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;ctrl&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;Legal My neighbor is&quot;</span>  <span class="hljs-comment"># &quot;Legal&quot; is one of the control codes for ctrl</span>
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">50</span>, temperature=<span class="hljs-number">0.7</span>, repetition_penalty=<span class="hljs-number">1.2</span>
)  <span class="hljs-comment"># generate sequences</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Generated: <span class="hljs-subst">{tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)}</span>&quot;</span>)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;gpt2&quot;</span>)  <span class="hljs-comment"># Initialize tokenizer</span>
model = TFAutoModelWithLMHead.from_pretrained(
    <span class="hljs-string">&quot;gpt2&quot;</span>
)  <span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
input_context = <span class="hljs-string">&quot;My cute dog&quot;</span>
bad_words_ids = [
    tokenizer.encode(bad_word, add_prefix_space=<span class="hljs-literal">True</span>) <span class="hljs-keyword">for</span> bad_word <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;idiot&quot;</span>, <span class="hljs-string">&quot;stupid&quot;</span>, <span class="hljs-string">&quot;shut up&quot;</span>]
]
input_ids = tokenizer.encode(input_context, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)  <span class="hljs-comment"># encode input context</span>
outputs = model.generate(
    input_ids=input_ids, max_length=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>, bad_words_ids=bad_words_ids
)  <span class="hljs-comment"># generate sequences without allowing bad_words to be generated</span>`}}),Ut=new Wo({}),Vt=new ue({props:{name:"class transformers.generation_flax_utils.FlaxGenerationMixin",anchor:"transformers.generation_flax_utils.FlaxGenerationMixin",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_flax_utils.py#L119"}}),Xt=new ue({props:{name:"generate",anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate",parameters:[{name:"input_ids",val:": ndarray"},{name:"max_length",val:": typing.Optional[int] = None"},{name:"pad_token_id",val:": typing.Optional[int] = None"},{name:"bos_token_id",val:": typing.Optional[int] = None"},{name:"eos_token_id",val:": typing.Optional[int] = None"},{name:"decoder_start_token_id",val:": typing.Optional[int] = None"},{name:"do_sample",val:": typing.Optional[bool] = None"},{name:"prng_key",val:": typing.Optional[jax._src.numpy.ndarray.ndarray] = None"},{name:"top_k",val:": typing.Optional[int] = None"},{name:"top_p",val:": typing.Optional[float] = None"},{name:"temperature",val:": typing.Optional[float] = None"},{name:"num_beams",val:": typing.Optional[int] = None"},{name:"no_repeat_ngram_size",val:": typing.Optional[int] = None"},{name:"min_length",val:": typing.Optional[int] = None"},{name:"forced_bos_token_id",val:": typing.Optional[int] = None"},{name:"forced_eos_token_id",val:": typing.Optional[int] = None"},{name:"length_penalty",val:": typing.Optional[float] = None"},{name:"early_stopping",val:": typing.Optional[bool] = None"},{name:"trace",val:": bool = True"},{name:"params",val:": typing.Union[typing.Dict[str, jax._src.numpy.ndarray.ndarray], NoneType] = None"},{name:"**model_kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/generation_flax_utils.py#L163",parametersDescription:[{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.input_ids",description:`<strong>input_ids</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 20) &#x2014;
The maximum length of the sequence to be generated.`,name:"max_length"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.do_sample",description:`<strong>do_sample</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use sampling ; use greedy decoding otherwise.`,name:"do_sample"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.temperature",description:`<strong>temperature</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The value used to module the next token probabilities.`,name:"temperature"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.top_k",description:`<strong>top_k</strong> (<code>int</code>, <em>optional</em>, defaults to 50) &#x2014;
The number of highest probability vocabulary tokens to keep for top-k-filtering.`,name:"top_k"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.top_p",description:`<strong>top_p</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
If set to float &lt; 1, only the most probable tokens with probabilities that add up to <code>top_p</code> or higher
are kept for generation.`,name:"top_p"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>padding</em> token.`,name:"pad_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>beginning-of-sequence</em> token.`,name:"bos_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the <em>end-of-sequence</em> token.`,name:"eos_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.num_beams",description:`<strong>num_beams</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of beams for beam search. 1 means no beam search.`,name:"num_beams"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.decoder_start_token_id",description:`<strong>decoder_start_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If an encoder-decoder model starts decoding with a different token than <em>bos</em>, the id of that token.`,name:"decoder_start_token_id"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.trace",description:`<strong>trace</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to trace generation. Setting <code>trace=False</code> should only be used for debugging and will lead to a
considerably slower runtime.`,name:"trace"},{anchor:"transformers.generation_flax_utils.FlaxGenerationMixin.generate.params",description:`<strong>params</strong> (<code>Dict[str, jnp.ndarray]</code>, <em>optional</em>) &#x2014;
Optionally the model parameters can be passed. Can be useful for parallelized generation.
model<em>kwargs &#x2014;
Additional model specific kwargs will be forwarded to the <code>forward</code> function of the model. If the model
is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
should be prefixed with *decoder</em>*. Also accepts <code>encoder_outputs</code> to skip encoder part.`,name:"params"}],returnDescription:`
<p><a
  href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput"
>ModelOutput</a>.</p>
`}}),et=new Dm({props:{warning:"&lcub;true}",$$slots:{default:[Rm]},$$scope:{ctx:lt}}}),Yt=new be({props:{code:`from transformers import AutoTokenizer, FlaxAutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
model = FlaxAutoModelForCausalLM.from_pretrained("distilgpt2")
input_context = "The dog"
# encode input context
input_ids = tokenizer(input_context, return_tensors="np").input_ids
# generate candidates using sampling
outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)
tokenizer.batch_decode(outputs, skip_special_tokens=True)`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilgpt2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_context = <span class="hljs-string">&quot;The dog&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># encode input context</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(input_context, return_tensors=<span class="hljs-string">&quot;np&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate candidates using sampling</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(input_ids=input_ids, max_length=<span class="hljs-number">20</span>, top_k=<span class="hljs-number">30</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)`}}),{c(){p=o("meta"),z=l(),y=o("h1"),w=o("a"),D=o("span"),m(T.$$.fragment),_e=l(),C=o("span"),q=t("Generation"),I=l(),G=o("p"),S=t("Each framework has a generate method for auto-regressive text generation implemented in their respective "),x=o("code"),ye=t("GenerationMixin"),je=t(" class:"),xe=l(),ke=o("ul"),Le=o("li"),Bo=t("PyTorch "),tn=o("a"),Ho=t("generate()"),Ro=t(" is implemented in "),nn=o("a"),Uo=t("GenerationMixin"),Vo=t("."),Ko=l(),Me=o("li"),Zo=t("TensorFlow "),sn=o("a"),Xo=t("generate()"),Jo=t(" is implemented in "),on=o("a"),Qo=t("TFGenerationMixin"),Yo=t("."),ea=l(),we=o("li"),ta=t("Flax/JAX "),an=o("a"),na=t("generate()"),sa=t(" is implemented in "),rn=o("a"),oa=t("FlaxGenerationMixin"),aa=t("."),ho=l(),Te=o("h2"),Xe=o("a"),$n=o("span"),m(dt.$$.fragment),ra=l(),Fn=o("span"),ia=t("GenerationMixin"),fo=l(),k=o("div"),m(ct.$$.fragment),la=l(),pt=o("p"),da=t("A class containing all functions for auto-regressive text generation, to be used as a mixin in "),ln=o("a"),ca=t("PreTrainedModel"),pa=t("."),ma=l(),mt=o("p"),ga=t("The class exposes "),dn=o("a"),ua=t("generate()"),_a=t(", which can be used for:"),ha=l(),$=o("ul"),W=o("li"),An=o("em"),fa=t("greedy decoding"),ba=t(" by calling "),cn=o("a"),xa=t("greedy_search()"),ka=t(" if "),zn=o("code"),va=t("num_beams=1"),ya=t(` and
`),Pn=o("code"),ja=t("do_sample=False"),La=t("."),Ma=l(),B=o("li"),Nn=o("em"),wa=t("multinomial sampling"),Ta=t(" by calling "),pn=o("a"),Ea=t("sample()"),Oa=t(" if "),Dn=o("code"),qa=t("num_beams=1"),Ga=t(` and
`),Cn=o("code"),Sa=t("do_sample=True"),$a=t("."),Fa=l(),H=o("li"),In=o("em"),Aa=t("beam-search decoding"),za=t(" by calling "),mn=o("a"),Pa=t("beam_search()"),Na=t(" if "),Wn=o("code"),Da=t("num_beams>1"),Ca=t(` and
`),Bn=o("code"),Ia=t("do_sample=False"),Wa=t("."),Ba=l(),R=o("li"),Hn=o("em"),Ha=t("beam-search multinomial sampling"),Ra=t(" by calling "),gn=o("a"),Ua=t("beam_sample()"),Va=t(` if
`),Rn=o("code"),Ka=t("num_beams>1"),Za=t(" and "),Un=o("code"),Xa=t("do_sample=True"),Ja=t("."),Qa=l(),U=o("li"),Vn=o("em"),Ya=t("diverse beam-search decoding"),er=t(" by calling "),un=o("a"),tr=t("group_beam_search()"),nr=t(`, if
`),Kn=o("code"),sr=t("num_beams>1"),or=t(" and "),Zn=o("code"),ar=t("num_beam_groups>1"),rr=t("."),ir=l(),V=o("li"),Xn=o("em"),lr=t("constrained beam-search decoding"),dr=t(" by calling "),_n=o("a"),cr=t("constrained_beam_search()"),pr=t(`,
if `),Jn=o("code"),mr=t("constraints!=None"),gr=t(" or "),Qn=o("code"),ur=t("force_words_ids!=None"),_r=t("."),hr=l(),b=o("div"),m(gt.$$.fragment),fr=l(),Yn=o("p"),br=t(`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),xr=l(),F=o("ul"),K=o("li"),es=o("em"),kr=t("greedy decoding"),vr=t(" by calling "),hn=o("a"),yr=t("greedy_search()"),jr=t(" if "),ts=o("code"),Lr=t("num_beams=1"),Mr=t(` and
`),ns=o("code"),wr=t("do_sample=False"),Tr=t("."),Er=l(),Z=o("li"),ss=o("em"),Or=t("multinomial sampling"),qr=t(" by calling "),fn=o("a"),Gr=t("sample()"),Sr=t(" if "),os=o("code"),$r=t("num_beams=1"),Fr=t(` and
`),as=o("code"),Ar=t("do_sample=True"),zr=t("."),Pr=l(),X=o("li"),rs=o("em"),Nr=t("beam-search decoding"),Dr=t(" by calling "),bn=o("a"),Cr=t("beam_search()"),Ir=t(" if "),is=o("code"),Wr=t("num_beams>1"),Br=t(` and
`),ls=o("code"),Hr=t("do_sample=False"),Rr=t("."),Ur=l(),J=o("li"),ds=o("em"),Vr=t("beam-search multinomial sampling"),Kr=t(" by calling "),xn=o("a"),Zr=t("beam_sample()"),Xr=t(` if
`),cs=o("code"),Jr=t("num_beams>1"),Qr=t(" and "),ps=o("code"),Yr=t("do_sample=True"),ei=t("."),ti=l(),Q=o("li"),ms=o("em"),ni=t("diverse beam-search decoding"),si=t(" by calling "),kn=o("a"),oi=t("group_beam_search()"),ai=t(`, if
`),gs=o("code"),ri=t("num_beams>1"),ii=t(" and "),us=o("code"),li=t("num_beam_groups>1"),di=t("."),ci=l(),Y=o("li"),_s=o("em"),pi=t("constrained beam-search decoding"),mi=t(` by calling
`),vn=o("a"),gi=t("constrained_beam_search()"),ui=t(", if "),hs=o("code"),_i=t("constraints!=None"),hi=t(` or
`),fs=o("code"),fi=t("force_words_ids!=None"),bi=t("."),xi=l(),m(Je.$$.fragment),ki=l(),ut=o("p"),vi=t("Most of these parameters are explained in more detail in "),_t=o("a"),yi=t(`this blog
post`),ji=t("."),Li=l(),bs=o("p"),Mi=t("Examples:"),wi=l(),xs=o("p"),Ti=t("Greedy Decoding:"),Ei=l(),m(ht.$$.fragment),Oi=l(),ks=o("p"),qi=t("Multinomial Sampling:"),Gi=l(),m(ft.$$.fragment),Si=l(),vs=o("p"),$i=t("Beam-search decoding:"),Fi=l(),m(bt.$$.fragment),Ai=l(),ee=o("div"),m(xt.$$.fragment),zi=l(),kt=o("p"),Pi=t("Generates sequences of token ids for models with a language modeling head using "),ys=o("strong"),Ni=t("greedy decoding"),Di=t(` and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Ci=l(),js=o("p"),Ii=t("Examples:"),Wi=l(),m(vt.$$.fragment),Bi=l(),te=o("div"),m(yt.$$.fragment),Hi=l(),jt=o("p"),Ri=t("Generates sequences of token ids for models with a language modeling head using "),Ls=o("strong"),Ui=t("multinomial sampling"),Vi=t(` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Ki=l(),Ms=o("p"),Zi=t("Examples:"),Xi=l(),m(Lt.$$.fragment),Ji=l(),ne=o("div"),m(Mt.$$.fragment),Qi=l(),wt=o("p"),Yi=t("Generates sequences of token ids for models with a language modeling head using "),ws=o("strong"),el=t("beam search decoding"),tl=t(` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),nl=l(),Ts=o("p"),sl=t("Examples:"),ol=l(),m(Tt.$$.fragment),al=l(),se=o("div"),m(Et.$$.fragment),rl=l(),Ot=o("p"),il=t("Generates sequences of token ids for models with a language modeling head using "),Es=o("strong"),ll=t(`beam search multinomial
sampling`),dl=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),cl=l(),Os=o("p"),pl=t("Examples:"),ml=l(),m(qt.$$.fragment),gl=l(),oe=o("div"),m(Gt.$$.fragment),ul=l(),St=o("p"),_l=t("Generates sequences of token ids for models with a language modeling head using "),qs=o("strong"),hl=t(`diverse beam search
decoding`),fl=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),bl=l(),Gs=o("p"),xl=t("Examples:"),kl=l(),m($t.$$.fragment),vl=l(),ae=o("div"),m(Ft.$$.fragment),yl=l(),At=o("p"),jl=t("Generates sequences of token ids for models with a language modeling head using "),Ss=o("strong"),Ll=t(`constrained beam search
decoding`),Ml=t(" and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),wl=l(),$s=o("p"),Tl=t("Examples:"),El=l(),m(zt.$$.fragment),bo=l(),Ee=o("h2"),Qe=o("a"),Fs=o("span"),m(Pt.$$.fragment),Ol=l(),As=o("span"),ql=t("TFGenerationMixin"),xo=l(),he=o("div"),m(Nt.$$.fragment),Gl=l(),Dt=o("p"),Sl=t("A class containing all of the functions supporting generation, to be used as a mixin in "),yn=o("a"),$l=t("TFPreTrainedModel"),Fl=t("."),Al=l(),E=o("div"),m(Ct.$$.fragment),zl=l(),zs=o("p"),Pl=t(`Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.`),Nl=l(),It=o("p"),Dl=t("Adapted in part from "),Wt=o("a"),Cl=t(`Facebook\u2019s XLM beam search
code`),Il=t("."),Wl=l(),fe=o("p"),Bl=t("Apart from "),Ps=o("code"),Hl=t("input_ids"),Rl=t(" and "),Ns=o("code"),Ul=t("attention_mask"),Vl=t(`, all the arguments below will default to the value of the attribute
of the same name inside the `),jn=o("a"),Kl=t("PretrainedConfig"),Zl=t(` of the model. The default values indicated are the default
values of those config.`),Xl=l(),Bt=o("p"),Jl=t("Most of these parameters are explained in more detail in "),Ht=o("a"),Ql=t(`this blog
post`),Yl=t("."),ed=l(),Ds=o("p"),td=t("Examples:"),nd=l(),m(Rt.$$.fragment),ko=l(),Oe=o("h2"),Ye=o("a"),Cs=o("span"),m(Ut.$$.fragment),sd=l(),Is=o("span"),od=t("FlaxGenerationMixin"),vo=l(),A=o("div"),m(Vt.$$.fragment),ad=l(),Kt=o("p"),rd=t(`A class containing all functions for auto-regressive text generation, to be used as a mixin in
`),Ln=o("a"),id=t("FlaxPreTrainedModel"),ld=t("."),dd=l(),Zt=o("p"),cd=t("The class exposes "),Mn=o("a"),pd=t("generate()"),md=t(", which can be used for:"),gd=l(),qe=o("ul"),re=o("li"),Ws=o("em"),ud=t("greedy decoding"),_d=t(" by calling "),Bs=o("code"),hd=t("_greedy_search()"),fd=t(`if
`),Hs=o("code"),bd=t("num_beams=1"),xd=t(" and "),Rs=o("code"),kd=t("do_sample=False"),vd=t("."),yd=l(),ie=o("li"),Us=o("em"),jd=t("multinomial sampling"),Ld=t(" by calling "),Vs=o("code"),Md=t("_sample()"),wd=t("if "),Ks=o("code"),Td=t("num_beams=1"),Ed=t(`
and `),Zs=o("code"),Od=t("do_sample=True"),qd=t("."),Gd=l(),le=o("li"),Xs=o("em"),Sd=t("beam-search decoding"),$d=t(" by calling "),Js=o("code"),Fd=t("_beam_search"),Ad=t(" if "),Qs=o("code"),zd=t("num_beams>1"),Pd=t(`
and `),Ys=o("code"),Nd=t("do_sample=False"),Dd=t("."),Cd=l(),O=o("div"),m(Xt.$$.fragment),Id=l(),eo=o("p"),Wd=t(`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),Bd=l(),Ge=o("ul"),de=o("li"),to=o("em"),Hd=t("greedy decoding"),Rd=t(" by calling "),no=o("code"),Ud=t("_greedy_search()"),Vd=t(`if
`),so=o("code"),Kd=t("num_beams=1"),Zd=t(" and "),oo=o("code"),Xd=t("do_sample=False"),Jd=t("."),Qd=l(),ce=o("li"),ao=o("em"),Yd=t("multinomial sampling"),ec=t(" by calling "),ro=o("code"),tc=t("_sample()"),nc=t("if "),io=o("code"),sc=t("num_beams=1"),oc=t(`
and `),lo=o("code"),ac=t("do_sample=True"),rc=t("."),ic=l(),pe=o("li"),co=o("em"),lc=t("beam-search decoding"),dc=t(" by calling "),po=o("code"),cc=t("_beam_search"),pc=t(" if "),mo=o("code"),mc=t("num_beams>1"),gc=t(`
and `),go=o("code"),uc=t("do_sample=False"),_c=t("."),hc=l(),m(et.$$.fragment),fc=l(),Jt=o("p"),bc=t("Most of these parameters are explained in more detail in "),Qt=o("a"),xc=t(`this blog
post`),kc=t("."),vc=l(),uo=o("p"),yc=t("Examples:"),jc=l(),m(Yt.$$.fragment),this.h()},l(i){const v=Bm('[data-svelte="svelte-1phssyn"]',document.head);p=a(v,"META",{name:!0,content:!0}),v.forEach(s),z=d(i),y=a(i,"H1",{class:!0});var en=r(y);w=a(en,"A",{id:!0,class:!0,href:!0});var _o=r(w);D=a(_o,"SPAN",{});var Lc=r(D);g(T.$$.fragment,Lc),Lc.forEach(s),_o.forEach(s),_e=d(en),C=a(en,"SPAN",{});var Mc=r(C);q=n(Mc,"Generation"),Mc.forEach(s),en.forEach(s),I=d(i),G=a(i,"P",{});var jo=r(G);S=n(jo,"Each framework has a generate method for auto-regressive text generation implemented in their respective "),x=a(jo,"CODE",{});var wc=r(x);ye=n(wc,"GenerationMixin"),wc.forEach(s),je=n(jo," class:"),jo.forEach(s),xe=d(i),ke=a(i,"UL",{});var wn=r(ke);Le=a(wn,"LI",{});var Tn=r(Le);Bo=n(Tn,"PyTorch "),tn=a(Tn,"A",{href:!0});var Tc=r(tn);Ho=n(Tc,"generate()"),Tc.forEach(s),Ro=n(Tn," is implemented in "),nn=a(Tn,"A",{href:!0});var Ec=r(nn);Uo=n(Ec,"GenerationMixin"),Ec.forEach(s),Vo=n(Tn,"."),Tn.forEach(s),Ko=d(wn),Me=a(wn,"LI",{});var En=r(Me);Zo=n(En,"TensorFlow "),sn=a(En,"A",{href:!0});var Oc=r(sn);Xo=n(Oc,"generate()"),Oc.forEach(s),Jo=n(En," is implemented in "),on=a(En,"A",{href:!0});var qc=r(on);Qo=n(qc,"TFGenerationMixin"),qc.forEach(s),Yo=n(En,"."),En.forEach(s),ea=d(wn),we=a(wn,"LI",{});var On=r(we);ta=n(On,"Flax/JAX "),an=a(On,"A",{href:!0});var Gc=r(an);na=n(Gc,"generate()"),Gc.forEach(s),sa=n(On," is implemented in "),rn=a(On,"A",{href:!0});var Sc=r(rn);oa=n(Sc,"FlaxGenerationMixin"),Sc.forEach(s),aa=n(On,"."),On.forEach(s),wn.forEach(s),ho=d(i),Te=a(i,"H2",{class:!0});var Lo=r(Te);Xe=a(Lo,"A",{id:!0,class:!0,href:!0});var $c=r(Xe);$n=a($c,"SPAN",{});var Fc=r($n);g(dt.$$.fragment,Fc),Fc.forEach(s),$c.forEach(s),ra=d(Lo),Fn=a(Lo,"SPAN",{});var Ac=r(Fn);ia=n(Ac,"GenerationMixin"),Ac.forEach(s),Lo.forEach(s),fo=d(i),k=a(i,"DIV",{class:!0});var L=r(k);g(ct.$$.fragment,L),la=d(L),pt=a(L,"P",{});var Mo=r(pt);da=n(Mo,"A class containing all functions for auto-regressive text generation, to be used as a mixin in "),ln=a(Mo,"A",{href:!0});var zc=r(ln);ca=n(zc,"PreTrainedModel"),zc.forEach(s),pa=n(Mo,"."),Mo.forEach(s),ma=d(L),mt=a(L,"P",{});var wo=r(mt);ga=n(wo,"The class exposes "),dn=a(wo,"A",{href:!0});var Pc=r(dn);ua=n(Pc,"generate()"),Pc.forEach(s),_a=n(wo,", which can be used for:"),wo.forEach(s),ha=d(L),$=a(L,"UL",{});var me=r($);W=a(me,"LI",{});var Se=r(W);An=a(Se,"EM",{});var Nc=r(An);fa=n(Nc,"greedy decoding"),Nc.forEach(s),ba=n(Se," by calling "),cn=a(Se,"A",{href:!0});var Dc=r(cn);xa=n(Dc,"greedy_search()"),Dc.forEach(s),ka=n(Se," if "),zn=a(Se,"CODE",{});var Cc=r(zn);va=n(Cc,"num_beams=1"),Cc.forEach(s),ya=n(Se,` and
`),Pn=a(Se,"CODE",{});var Ic=r(Pn);ja=n(Ic,"do_sample=False"),Ic.forEach(s),La=n(Se,"."),Se.forEach(s),Ma=d(me),B=a(me,"LI",{});var $e=r(B);Nn=a($e,"EM",{});var Wc=r(Nn);wa=n(Wc,"multinomial sampling"),Wc.forEach(s),Ta=n($e," by calling "),pn=a($e,"A",{href:!0});var Bc=r(pn);Ea=n(Bc,"sample()"),Bc.forEach(s),Oa=n($e," if "),Dn=a($e,"CODE",{});var Hc=r(Dn);qa=n(Hc,"num_beams=1"),Hc.forEach(s),Ga=n($e,` and
`),Cn=a($e,"CODE",{});var Rc=r(Cn);Sa=n(Rc,"do_sample=True"),Rc.forEach(s),$a=n($e,"."),$e.forEach(s),Fa=d(me),H=a(me,"LI",{});var Fe=r(H);In=a(Fe,"EM",{});var Uc=r(In);Aa=n(Uc,"beam-search decoding"),Uc.forEach(s),za=n(Fe," by calling "),mn=a(Fe,"A",{href:!0});var Vc=r(mn);Pa=n(Vc,"beam_search()"),Vc.forEach(s),Na=n(Fe," if "),Wn=a(Fe,"CODE",{});var Kc=r(Wn);Da=n(Kc,"num_beams>1"),Kc.forEach(s),Ca=n(Fe,` and
`),Bn=a(Fe,"CODE",{});var Zc=r(Bn);Ia=n(Zc,"do_sample=False"),Zc.forEach(s),Wa=n(Fe,"."),Fe.forEach(s),Ba=d(me),R=a(me,"LI",{});var Ae=r(R);Hn=a(Ae,"EM",{});var Xc=r(Hn);Ha=n(Xc,"beam-search multinomial sampling"),Xc.forEach(s),Ra=n(Ae," by calling "),gn=a(Ae,"A",{href:!0});var Jc=r(gn);Ua=n(Jc,"beam_sample()"),Jc.forEach(s),Va=n(Ae,` if
`),Rn=a(Ae,"CODE",{});var Qc=r(Rn);Ka=n(Qc,"num_beams>1"),Qc.forEach(s),Za=n(Ae," and "),Un=a(Ae,"CODE",{});var Yc=r(Un);Xa=n(Yc,"do_sample=True"),Yc.forEach(s),Ja=n(Ae,"."),Ae.forEach(s),Qa=d(me),U=a(me,"LI",{});var ze=r(U);Vn=a(ze,"EM",{});var ep=r(Vn);Ya=n(ep,"diverse beam-search decoding"),ep.forEach(s),er=n(ze," by calling "),un=a(ze,"A",{href:!0});var tp=r(un);tr=n(tp,"group_beam_search()"),tp.forEach(s),nr=n(ze,`, if
`),Kn=a(ze,"CODE",{});var np=r(Kn);sr=n(np,"num_beams>1"),np.forEach(s),or=n(ze," and "),Zn=a(ze,"CODE",{});var sp=r(Zn);ar=n(sp,"num_beam_groups>1"),sp.forEach(s),rr=n(ze,"."),ze.forEach(s),ir=d(me),V=a(me,"LI",{});var Pe=r(V);Xn=a(Pe,"EM",{});var op=r(Xn);lr=n(op,"constrained beam-search decoding"),op.forEach(s),dr=n(Pe," by calling "),_n=a(Pe,"A",{href:!0});var ap=r(_n);cr=n(ap,"constrained_beam_search()"),ap.forEach(s),pr=n(Pe,`,
if `),Jn=a(Pe,"CODE",{});var rp=r(Jn);mr=n(rp,"constraints!=None"),rp.forEach(s),gr=n(Pe," or "),Qn=a(Pe,"CODE",{});var ip=r(Qn);ur=n(ip,"force_words_ids!=None"),ip.forEach(s),_r=n(Pe,"."),Pe.forEach(s),me.forEach(s),hr=d(L),b=a(L,"DIV",{class:!0});var j=r(b);g(gt.$$.fragment,j),fr=d(j),Yn=a(j,"P",{});var lp=r(Yn);br=n(lp,`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),lp.forEach(s),xr=d(j),F=a(j,"UL",{});var ge=r(F);K=a(ge,"LI",{});var Ne=r(K);es=a(Ne,"EM",{});var dp=r(es);kr=n(dp,"greedy decoding"),dp.forEach(s),vr=n(Ne," by calling "),hn=a(Ne,"A",{href:!0});var cp=r(hn);yr=n(cp,"greedy_search()"),cp.forEach(s),jr=n(Ne," if "),ts=a(Ne,"CODE",{});var pp=r(ts);Lr=n(pp,"num_beams=1"),pp.forEach(s),Mr=n(Ne,` and
`),ns=a(Ne,"CODE",{});var mp=r(ns);wr=n(mp,"do_sample=False"),mp.forEach(s),Tr=n(Ne,"."),Ne.forEach(s),Er=d(ge),Z=a(ge,"LI",{});var De=r(Z);ss=a(De,"EM",{});var gp=r(ss);Or=n(gp,"multinomial sampling"),gp.forEach(s),qr=n(De," by calling "),fn=a(De,"A",{href:!0});var up=r(fn);Gr=n(up,"sample()"),up.forEach(s),Sr=n(De," if "),os=a(De,"CODE",{});var _p=r(os);$r=n(_p,"num_beams=1"),_p.forEach(s),Fr=n(De,` and
`),as=a(De,"CODE",{});var hp=r(as);Ar=n(hp,"do_sample=True"),hp.forEach(s),zr=n(De,"."),De.forEach(s),Pr=d(ge),X=a(ge,"LI",{});var Ce=r(X);rs=a(Ce,"EM",{});var fp=r(rs);Nr=n(fp,"beam-search decoding"),fp.forEach(s),Dr=n(Ce," by calling "),bn=a(Ce,"A",{href:!0});var bp=r(bn);Cr=n(bp,"beam_search()"),bp.forEach(s),Ir=n(Ce," if "),is=a(Ce,"CODE",{});var xp=r(is);Wr=n(xp,"num_beams>1"),xp.forEach(s),Br=n(Ce,` and
`),ls=a(Ce,"CODE",{});var kp=r(ls);Hr=n(kp,"do_sample=False"),kp.forEach(s),Rr=n(Ce,"."),Ce.forEach(s),Ur=d(ge),J=a(ge,"LI",{});var Ie=r(J);ds=a(Ie,"EM",{});var vp=r(ds);Vr=n(vp,"beam-search multinomial sampling"),vp.forEach(s),Kr=n(Ie," by calling "),xn=a(Ie,"A",{href:!0});var yp=r(xn);Zr=n(yp,"beam_sample()"),yp.forEach(s),Xr=n(Ie,` if
`),cs=a(Ie,"CODE",{});var jp=r(cs);Jr=n(jp,"num_beams>1"),jp.forEach(s),Qr=n(Ie," and "),ps=a(Ie,"CODE",{});var Lp=r(ps);Yr=n(Lp,"do_sample=True"),Lp.forEach(s),ei=n(Ie,"."),Ie.forEach(s),ti=d(ge),Q=a(ge,"LI",{});var We=r(Q);ms=a(We,"EM",{});var Mp=r(ms);ni=n(Mp,"diverse beam-search decoding"),Mp.forEach(s),si=n(We," by calling "),kn=a(We,"A",{href:!0});var wp=r(kn);oi=n(wp,"group_beam_search()"),wp.forEach(s),ai=n(We,`, if
`),gs=a(We,"CODE",{});var Tp=r(gs);ri=n(Tp,"num_beams>1"),Tp.forEach(s),ii=n(We," and "),us=a(We,"CODE",{});var Ep=r(us);li=n(Ep,"num_beam_groups>1"),Ep.forEach(s),di=n(We,"."),We.forEach(s),ci=d(ge),Y=a(ge,"LI",{});var Be=r(Y);_s=a(Be,"EM",{});var Op=r(_s);pi=n(Op,"constrained beam-search decoding"),Op.forEach(s),mi=n(Be,` by calling
`),vn=a(Be,"A",{href:!0});var qp=r(vn);gi=n(qp,"constrained_beam_search()"),qp.forEach(s),ui=n(Be,", if "),hs=a(Be,"CODE",{});var Gp=r(hs);_i=n(Gp,"constraints!=None"),Gp.forEach(s),hi=n(Be,` or
`),fs=a(Be,"CODE",{});var Sp=r(fs);fi=n(Sp,"force_words_ids!=None"),Sp.forEach(s),bi=n(Be,"."),Be.forEach(s),ge.forEach(s),xi=d(j),g(Je.$$.fragment,j),ki=d(j),ut=a(j,"P",{});var To=r(ut);vi=n(To,"Most of these parameters are explained in more detail in "),_t=a(To,"A",{href:!0,rel:!0});var $p=r(_t);yi=n($p,`this blog
post`),$p.forEach(s),ji=n(To,"."),To.forEach(s),Li=d(j),bs=a(j,"P",{});var Fp=r(bs);Mi=n(Fp,"Examples:"),Fp.forEach(s),wi=d(j),xs=a(j,"P",{});var Ap=r(xs);Ti=n(Ap,"Greedy Decoding:"),Ap.forEach(s),Ei=d(j),g(ht.$$.fragment,j),Oi=d(j),ks=a(j,"P",{});var zp=r(ks);qi=n(zp,"Multinomial Sampling:"),zp.forEach(s),Gi=d(j),g(ft.$$.fragment,j),Si=d(j),vs=a(j,"P",{});var Pp=r(vs);$i=n(Pp,"Beam-search decoding:"),Pp.forEach(s),Fi=d(j),g(bt.$$.fragment,j),j.forEach(s),Ai=d(L),ee=a(L,"DIV",{class:!0});var tt=r(ee);g(xt.$$.fragment,tt),zi=d(tt),kt=a(tt,"P",{});var Eo=r(kt);Pi=n(Eo,"Generates sequences of token ids for models with a language modeling head using "),ys=a(Eo,"STRONG",{});var Np=r(ys);Ni=n(Np,"greedy decoding"),Np.forEach(s),Di=n(Eo,` and can be
used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Eo.forEach(s),Ci=d(tt),js=a(tt,"P",{});var Dp=r(js);Ii=n(Dp,"Examples:"),Dp.forEach(s),Wi=d(tt),g(vt.$$.fragment,tt),tt.forEach(s),Bi=d(L),te=a(L,"DIV",{class:!0});var nt=r(te);g(yt.$$.fragment,nt),Hi=d(nt),jt=a(nt,"P",{});var Oo=r(jt);Ri=n(Oo,"Generates sequences of token ids for models with a language modeling head using "),Ls=a(Oo,"STRONG",{});var Cp=r(Ls);Ui=n(Cp,"multinomial sampling"),Cp.forEach(s),Vi=n(Oo,` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),Oo.forEach(s),Ki=d(nt),Ms=a(nt,"P",{});var Ip=r(Ms);Zi=n(Ip,"Examples:"),Ip.forEach(s),Xi=d(nt),g(Lt.$$.fragment,nt),nt.forEach(s),Ji=d(L),ne=a(L,"DIV",{class:!0});var st=r(ne);g(Mt.$$.fragment,st),Qi=d(st),wt=a(st,"P",{});var qo=r(wt);Yi=n(qo,"Generates sequences of token ids for models with a language modeling head using "),ws=a(qo,"STRONG",{});var Wp=r(ws);el=n(Wp,"beam search decoding"),Wp.forEach(s),tl=n(qo,` and
can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.`),qo.forEach(s),nl=d(st),Ts=a(st,"P",{});var Bp=r(Ts);sl=n(Bp,"Examples:"),Bp.forEach(s),ol=d(st),g(Tt.$$.fragment,st),st.forEach(s),al=d(L),se=a(L,"DIV",{class:!0});var ot=r(se);g(Et.$$.fragment,ot),rl=d(ot),Ot=a(ot,"P",{});var Go=r(Ot);il=n(Go,"Generates sequences of token ids for models with a language modeling head using "),Es=a(Go,"STRONG",{});var Hp=r(Es);ll=n(Hp,`beam search multinomial
sampling`),Hp.forEach(s),dl=n(Go," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),Go.forEach(s),cl=d(ot),Os=a(ot,"P",{});var Rp=r(Os);pl=n(Rp,"Examples:"),Rp.forEach(s),ml=d(ot),g(qt.$$.fragment,ot),ot.forEach(s),gl=d(L),oe=a(L,"DIV",{class:!0});var at=r(oe);g(Gt.$$.fragment,at),ul=d(at),St=a(at,"P",{});var So=r(St);_l=n(So,"Generates sequences of token ids for models with a language modeling head using "),qs=a(So,"STRONG",{});var Up=r(qs);hl=n(Up,`diverse beam search
decoding`),Up.forEach(s),fl=n(So," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),So.forEach(s),bl=d(at),Gs=a(at,"P",{});var Vp=r(Gs);xl=n(Vp,"Examples:"),Vp.forEach(s),kl=d(at),g($t.$$.fragment,at),at.forEach(s),vl=d(L),ae=a(L,"DIV",{class:!0});var rt=r(ae);g(Ft.$$.fragment,rt),yl=d(rt),At=a(rt,"P",{});var $o=r(At);jl=n($o,"Generates sequences of token ids for models with a language modeling head using "),Ss=a($o,"STRONG",{});var Kp=r(Ss);Ll=n(Kp,`constrained beam search
decoding`),Kp.forEach(s),Ml=n($o," and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models."),$o.forEach(s),wl=d(rt),$s=a(rt,"P",{});var Zp=r($s);Tl=n(Zp,"Examples:"),Zp.forEach(s),El=d(rt),g(zt.$$.fragment,rt),rt.forEach(s),L.forEach(s),bo=d(i),Ee=a(i,"H2",{class:!0});var Fo=r(Ee);Qe=a(Fo,"A",{id:!0,class:!0,href:!0});var Xp=r(Qe);Fs=a(Xp,"SPAN",{});var Jp=r(Fs);g(Pt.$$.fragment,Jp),Jp.forEach(s),Xp.forEach(s),Ol=d(Fo),As=a(Fo,"SPAN",{});var Qp=r(As);ql=n(Qp,"TFGenerationMixin"),Qp.forEach(s),Fo.forEach(s),xo=d(i),he=a(i,"DIV",{class:!0});var qn=r(he);g(Nt.$$.fragment,qn),Gl=d(qn),Dt=a(qn,"P",{});var Ao=r(Dt);Sl=n(Ao,"A class containing all of the functions supporting generation, to be used as a mixin in "),yn=a(Ao,"A",{href:!0});var Yp=r(yn);$l=n(Yp,"TFPreTrainedModel"),Yp.forEach(s),Fl=n(Ao,"."),Ao.forEach(s),Al=d(qn),E=a(qn,"DIV",{class:!0});var P=r(E);g(Ct.$$.fragment,P),zl=d(P),zs=a(P,"P",{});var em=r(zs);Pl=n(em,`Generates sequences for models with a language modeling head. The method currently supports greedy decoding,
beam-search decoding, sampling with temperature, sampling with top-k or nucleus sampling.`),em.forEach(s),Nl=d(P),It=a(P,"P",{});var zo=r(It);Dl=n(zo,"Adapted in part from "),Wt=a(zo,"A",{href:!0,rel:!0});var tm=r(Wt);Cl=n(tm,`Facebook\u2019s XLM beam search
code`),tm.forEach(s),Il=n(zo,"."),zo.forEach(s),Wl=d(P),fe=a(P,"P",{});var it=r(fe);Bl=n(it,"Apart from "),Ps=a(it,"CODE",{});var nm=r(Ps);Hl=n(nm,"input_ids"),nm.forEach(s),Rl=n(it," and "),Ns=a(it,"CODE",{});var sm=r(Ns);Ul=n(sm,"attention_mask"),sm.forEach(s),Vl=n(it,`, all the arguments below will default to the value of the attribute
of the same name inside the `),jn=a(it,"A",{href:!0});var om=r(jn);Kl=n(om,"PretrainedConfig"),om.forEach(s),Zl=n(it,` of the model. The default values indicated are the default
values of those config.`),it.forEach(s),Xl=d(P),Bt=a(P,"P",{});var Po=r(Bt);Jl=n(Po,"Most of these parameters are explained in more detail in "),Ht=a(Po,"A",{href:!0,rel:!0});var am=r(Ht);Ql=n(am,`this blog
post`),am.forEach(s),Yl=n(Po,"."),Po.forEach(s),ed=d(P),Ds=a(P,"P",{});var rm=r(Ds);td=n(rm,"Examples:"),rm.forEach(s),nd=d(P),g(Rt.$$.fragment,P),P.forEach(s),qn.forEach(s),ko=d(i),Oe=a(i,"H2",{class:!0});var No=r(Oe);Ye=a(No,"A",{id:!0,class:!0,href:!0});var im=r(Ye);Cs=a(im,"SPAN",{});var lm=r(Cs);g(Ut.$$.fragment,lm),lm.forEach(s),im.forEach(s),sd=d(No),Is=a(No,"SPAN",{});var dm=r(Is);od=n(dm,"FlaxGenerationMixin"),dm.forEach(s),No.forEach(s),vo=d(i),A=a(i,"DIV",{class:!0});var ve=r(A);g(Vt.$$.fragment,ve),ad=d(ve),Kt=a(ve,"P",{});var Do=r(Kt);rd=n(Do,`A class containing all functions for auto-regressive text generation, to be used as a mixin in
`),Ln=a(Do,"A",{href:!0});var cm=r(Ln);id=n(cm,"FlaxPreTrainedModel"),cm.forEach(s),ld=n(Do,"."),Do.forEach(s),dd=d(ve),Zt=a(ve,"P",{});var Co=r(Zt);cd=n(Co,"The class exposes "),Mn=a(Co,"A",{href:!0});var pm=r(Mn);pd=n(pm,"generate()"),pm.forEach(s),md=n(Co,", which can be used for:"),Co.forEach(s),gd=d(ve),qe=a(ve,"UL",{});var Gn=r(qe);re=a(Gn,"LI",{});var He=r(re);Ws=a(He,"EM",{});var mm=r(Ws);ud=n(mm,"greedy decoding"),mm.forEach(s),_d=n(He," by calling "),Bs=a(He,"CODE",{});var gm=r(Bs);hd=n(gm,"_greedy_search()"),gm.forEach(s),fd=n(He,`if
`),Hs=a(He,"CODE",{});var um=r(Hs);bd=n(um,"num_beams=1"),um.forEach(s),xd=n(He," and "),Rs=a(He,"CODE",{});var _m=r(Rs);kd=n(_m,"do_sample=False"),_m.forEach(s),vd=n(He,"."),He.forEach(s),yd=d(Gn),ie=a(Gn,"LI",{});var Re=r(ie);Us=a(Re,"EM",{});var hm=r(Us);jd=n(hm,"multinomial sampling"),hm.forEach(s),Ld=n(Re," by calling "),Vs=a(Re,"CODE",{});var fm=r(Vs);Md=n(fm,"_sample()"),fm.forEach(s),wd=n(Re,"if "),Ks=a(Re,"CODE",{});var bm=r(Ks);Td=n(bm,"num_beams=1"),bm.forEach(s),Ed=n(Re,`
and `),Zs=a(Re,"CODE",{});var xm=r(Zs);Od=n(xm,"do_sample=True"),xm.forEach(s),qd=n(Re,"."),Re.forEach(s),Gd=d(Gn),le=a(Gn,"LI",{});var Ue=r(le);Xs=a(Ue,"EM",{});var km=r(Xs);Sd=n(km,"beam-search decoding"),km.forEach(s),$d=n(Ue," by calling "),Js=a(Ue,"CODE",{});var vm=r(Js);Fd=n(vm,"_beam_search"),vm.forEach(s),Ad=n(Ue," if "),Qs=a(Ue,"CODE",{});var ym=r(Qs);zd=n(ym,"num_beams>1"),ym.forEach(s),Pd=n(Ue,`
and `),Ys=a(Ue,"CODE",{});var jm=r(Ys);Nd=n(jm,"do_sample=False"),jm.forEach(s),Dd=n(Ue,"."),Ue.forEach(s),Gn.forEach(s),Cd=d(ve),O=a(ve,"DIV",{class:!0});var N=r(O);g(Xt.$$.fragment,N),Id=d(N),eo=a(N,"P",{});var Lm=r(eo);Wd=n(Lm,`Generates sequences of token ids for models with a language modeling head. The method supports the following
generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:`),Lm.forEach(s),Bd=d(N),Ge=a(N,"UL",{});var Sn=r(Ge);de=a(Sn,"LI",{});var Ve=r(de);to=a(Ve,"EM",{});var Mm=r(to);Hd=n(Mm,"greedy decoding"),Mm.forEach(s),Rd=n(Ve," by calling "),no=a(Ve,"CODE",{});var wm=r(no);Ud=n(wm,"_greedy_search()"),wm.forEach(s),Vd=n(Ve,`if
`),so=a(Ve,"CODE",{});var Tm=r(so);Kd=n(Tm,"num_beams=1"),Tm.forEach(s),Zd=n(Ve," and "),oo=a(Ve,"CODE",{});var Em=r(oo);Xd=n(Em,"do_sample=False"),Em.forEach(s),Jd=n(Ve,"."),Ve.forEach(s),Qd=d(Sn),ce=a(Sn,"LI",{});var Ke=r(ce);ao=a(Ke,"EM",{});var Om=r(ao);Yd=n(Om,"multinomial sampling"),Om.forEach(s),ec=n(Ke," by calling "),ro=a(Ke,"CODE",{});var qm=r(ro);tc=n(qm,"_sample()"),qm.forEach(s),nc=n(Ke,"if "),io=a(Ke,"CODE",{});var Gm=r(io);sc=n(Gm,"num_beams=1"),Gm.forEach(s),oc=n(Ke,`
and `),lo=a(Ke,"CODE",{});var Sm=r(lo);ac=n(Sm,"do_sample=True"),Sm.forEach(s),rc=n(Ke,"."),Ke.forEach(s),ic=d(Sn),pe=a(Sn,"LI",{});var Ze=r(pe);co=a(Ze,"EM",{});var $m=r(co);lc=n($m,"beam-search decoding"),$m.forEach(s),dc=n(Ze," by calling "),po=a(Ze,"CODE",{});var Fm=r(po);cc=n(Fm,"_beam_search"),Fm.forEach(s),pc=n(Ze," if "),mo=a(Ze,"CODE",{});var Am=r(mo);mc=n(Am,"num_beams>1"),Am.forEach(s),gc=n(Ze,`
and `),go=a(Ze,"CODE",{});var zm=r(go);uc=n(zm,"do_sample=False"),zm.forEach(s),_c=n(Ze,"."),Ze.forEach(s),Sn.forEach(s),hc=d(N),g(et.$$.fragment,N),fc=d(N),Jt=a(N,"P",{});var Io=r(Jt);bc=n(Io,"Most of these parameters are explained in more detail in "),Qt=a(Io,"A",{href:!0,rel:!0});var Pm=r(Qt);xc=n(Pm,`this blog
post`),Pm.forEach(s),kc=n(Io,"."),Io.forEach(s),vc=d(N),uo=a(N,"P",{});var Nm=r(uo);yc=n(Nm,"Examples:"),Nm.forEach(s),jc=d(N),g(Yt.$$.fragment,N),N.forEach(s),ve.forEach(s),this.h()},h(){c(p,"name","hf:doc:metadata"),c(p,"content",JSON.stringify(Vm)),c(w,"id","generation"),c(w,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(w,"href","#generation"),c(y,"class","relative group"),c(tn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"),c(nn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin"),c(sn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_tf_utils.TFGenerationMixin.generate"),c(on,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_tf_utils.TFGenerationMixin"),c(an,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin.generate"),c(rn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin"),c(Xe,"id","transformers.generation_utils.GenerationMixin"),c(Xe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Xe,"href","#transformers.generation_utils.GenerationMixin"),c(Te,"class","relative group"),c(ln,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),c(dn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.generate"),c(cn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.greedy_search"),c(pn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.sample"),c(mn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_search"),c(gn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_sample"),c(un,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.group_beam_search"),c(_n,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.constrained_beam_search"),c(hn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.greedy_search"),c(fn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.sample"),c(bn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_search"),c(xn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.beam_sample"),c(kn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.group_beam_search"),c(vn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin.constrained_beam_search"),c(_t,"href","https://huggingface.co/blog/how-to-generate"),c(_t,"rel","nofollow"),c(b,"class","docstring"),c(ee,"class","docstring"),c(te,"class","docstring"),c(ne,"class","docstring"),c(se,"class","docstring"),c(oe,"class","docstring"),c(ae,"class","docstring"),c(k,"class","docstring"),c(Qe,"id","transformers.generation_tf_utils.TFGenerationMixin"),c(Qe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Qe,"href","#transformers.generation_tf_utils.TFGenerationMixin"),c(Ee,"class","relative group"),c(yn,"href","/docs/transformers/master/en/main_classes/model#transformers.TFPreTrainedModel"),c(Wt,"href","https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529"),c(Wt,"rel","nofollow"),c(jn,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),c(Ht,"href","https://huggingface.co/blog/how-to-generate"),c(Ht,"rel","nofollow"),c(E,"class","docstring"),c(he,"class","docstring"),c(Ye,"id","transformers.generation_flax_utils.FlaxGenerationMixin"),c(Ye,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),c(Ye,"href","#transformers.generation_flax_utils.FlaxGenerationMixin"),c(Oe,"class","relative group"),c(Ln,"href","/docs/transformers/master/en/main_classes/model#transformers.FlaxPreTrainedModel"),c(Mn,"href","/docs/transformers/master/en/main_classes/text_generation#transformers.generation_flax_utils.FlaxGenerationMixin.generate"),c(Qt,"href","https://huggingface.co/blog/how-to-generate"),c(Qt,"rel","nofollow"),c(O,"class","docstring"),c(A,"class","docstring")},m(i,v){e(document.head,p),M(i,z,v),M(i,y,v),e(y,w),e(w,D),u(T,D,null),e(y,_e),e(y,C),e(C,q),M(i,I,v),M(i,G,v),e(G,S),e(G,x),e(x,ye),e(G,je),M(i,xe,v),M(i,ke,v),e(ke,Le),e(Le,Bo),e(Le,tn),e(tn,Ho),e(Le,Ro),e(Le,nn),e(nn,Uo),e(Le,Vo),e(ke,Ko),e(ke,Me),e(Me,Zo),e(Me,sn),e(sn,Xo),e(Me,Jo),e(Me,on),e(on,Qo),e(Me,Yo),e(ke,ea),e(ke,we),e(we,ta),e(we,an),e(an,na),e(we,sa),e(we,rn),e(rn,oa),e(we,aa),M(i,ho,v),M(i,Te,v),e(Te,Xe),e(Xe,$n),u(dt,$n,null),e(Te,ra),e(Te,Fn),e(Fn,ia),M(i,fo,v),M(i,k,v),u(ct,k,null),e(k,la),e(k,pt),e(pt,da),e(pt,ln),e(ln,ca),e(pt,pa),e(k,ma),e(k,mt),e(mt,ga),e(mt,dn),e(dn,ua),e(mt,_a),e(k,ha),e(k,$),e($,W),e(W,An),e(An,fa),e(W,ba),e(W,cn),e(cn,xa),e(W,ka),e(W,zn),e(zn,va),e(W,ya),e(W,Pn),e(Pn,ja),e(W,La),e($,Ma),e($,B),e(B,Nn),e(Nn,wa),e(B,Ta),e(B,pn),e(pn,Ea),e(B,Oa),e(B,Dn),e(Dn,qa),e(B,Ga),e(B,Cn),e(Cn,Sa),e(B,$a),e($,Fa),e($,H),e(H,In),e(In,Aa),e(H,za),e(H,mn),e(mn,Pa),e(H,Na),e(H,Wn),e(Wn,Da),e(H,Ca),e(H,Bn),e(Bn,Ia),e(H,Wa),e($,Ba),e($,R),e(R,Hn),e(Hn,Ha),e(R,Ra),e(R,gn),e(gn,Ua),e(R,Va),e(R,Rn),e(Rn,Ka),e(R,Za),e(R,Un),e(Un,Xa),e(R,Ja),e($,Qa),e($,U),e(U,Vn),e(Vn,Ya),e(U,er),e(U,un),e(un,tr),e(U,nr),e(U,Kn),e(Kn,sr),e(U,or),e(U,Zn),e(Zn,ar),e(U,rr),e($,ir),e($,V),e(V,Xn),e(Xn,lr),e(V,dr),e(V,_n),e(_n,cr),e(V,pr),e(V,Jn),e(Jn,mr),e(V,gr),e(V,Qn),e(Qn,ur),e(V,_r),e(k,hr),e(k,b),u(gt,b,null),e(b,fr),e(b,Yn),e(Yn,br),e(b,xr),e(b,F),e(F,K),e(K,es),e(es,kr),e(K,vr),e(K,hn),e(hn,yr),e(K,jr),e(K,ts),e(ts,Lr),e(K,Mr),e(K,ns),e(ns,wr),e(K,Tr),e(F,Er),e(F,Z),e(Z,ss),e(ss,Or),e(Z,qr),e(Z,fn),e(fn,Gr),e(Z,Sr),e(Z,os),e(os,$r),e(Z,Fr),e(Z,as),e(as,Ar),e(Z,zr),e(F,Pr),e(F,X),e(X,rs),e(rs,Nr),e(X,Dr),e(X,bn),e(bn,Cr),e(X,Ir),e(X,is),e(is,Wr),e(X,Br),e(X,ls),e(ls,Hr),e(X,Rr),e(F,Ur),e(F,J),e(J,ds),e(ds,Vr),e(J,Kr),e(J,xn),e(xn,Zr),e(J,Xr),e(J,cs),e(cs,Jr),e(J,Qr),e(J,ps),e(ps,Yr),e(J,ei),e(F,ti),e(F,Q),e(Q,ms),e(ms,ni),e(Q,si),e(Q,kn),e(kn,oi),e(Q,ai),e(Q,gs),e(gs,ri),e(Q,ii),e(Q,us),e(us,li),e(Q,di),e(F,ci),e(F,Y),e(Y,_s),e(_s,pi),e(Y,mi),e(Y,vn),e(vn,gi),e(Y,ui),e(Y,hs),e(hs,_i),e(Y,hi),e(Y,fs),e(fs,fi),e(Y,bi),e(b,xi),u(Je,b,null),e(b,ki),e(b,ut),e(ut,vi),e(ut,_t),e(_t,yi),e(ut,ji),e(b,Li),e(b,bs),e(bs,Mi),e(b,wi),e(b,xs),e(xs,Ti),e(b,Ei),u(ht,b,null),e(b,Oi),e(b,ks),e(ks,qi),e(b,Gi),u(ft,b,null),e(b,Si),e(b,vs),e(vs,$i),e(b,Fi),u(bt,b,null),e(k,Ai),e(k,ee),u(xt,ee,null),e(ee,zi),e(ee,kt),e(kt,Pi),e(kt,ys),e(ys,Ni),e(kt,Di),e(ee,Ci),e(ee,js),e(js,Ii),e(ee,Wi),u(vt,ee,null),e(k,Bi),e(k,te),u(yt,te,null),e(te,Hi),e(te,jt),e(jt,Ri),e(jt,Ls),e(Ls,Ui),e(jt,Vi),e(te,Ki),e(te,Ms),e(Ms,Zi),e(te,Xi),u(Lt,te,null),e(k,Ji),e(k,ne),u(Mt,ne,null),e(ne,Qi),e(ne,wt),e(wt,Yi),e(wt,ws),e(ws,el),e(wt,tl),e(ne,nl),e(ne,Ts),e(Ts,sl),e(ne,ol),u(Tt,ne,null),e(k,al),e(k,se),u(Et,se,null),e(se,rl),e(se,Ot),e(Ot,il),e(Ot,Es),e(Es,ll),e(Ot,dl),e(se,cl),e(se,Os),e(Os,pl),e(se,ml),u(qt,se,null),e(k,gl),e(k,oe),u(Gt,oe,null),e(oe,ul),e(oe,St),e(St,_l),e(St,qs),e(qs,hl),e(St,fl),e(oe,bl),e(oe,Gs),e(Gs,xl),e(oe,kl),u($t,oe,null),e(k,vl),e(k,ae),u(Ft,ae,null),e(ae,yl),e(ae,At),e(At,jl),e(At,Ss),e(Ss,Ll),e(At,Ml),e(ae,wl),e(ae,$s),e($s,Tl),e(ae,El),u(zt,ae,null),M(i,bo,v),M(i,Ee,v),e(Ee,Qe),e(Qe,Fs),u(Pt,Fs,null),e(Ee,Ol),e(Ee,As),e(As,ql),M(i,xo,v),M(i,he,v),u(Nt,he,null),e(he,Gl),e(he,Dt),e(Dt,Sl),e(Dt,yn),e(yn,$l),e(Dt,Fl),e(he,Al),e(he,E),u(Ct,E,null),e(E,zl),e(E,zs),e(zs,Pl),e(E,Nl),e(E,It),e(It,Dl),e(It,Wt),e(Wt,Cl),e(It,Il),e(E,Wl),e(E,fe),e(fe,Bl),e(fe,Ps),e(Ps,Hl),e(fe,Rl),e(fe,Ns),e(Ns,Ul),e(fe,Vl),e(fe,jn),e(jn,Kl),e(fe,Zl),e(E,Xl),e(E,Bt),e(Bt,Jl),e(Bt,Ht),e(Ht,Ql),e(Bt,Yl),e(E,ed),e(E,Ds),e(Ds,td),e(E,nd),u(Rt,E,null),M(i,ko,v),M(i,Oe,v),e(Oe,Ye),e(Ye,Cs),u(Ut,Cs,null),e(Oe,sd),e(Oe,Is),e(Is,od),M(i,vo,v),M(i,A,v),u(Vt,A,null),e(A,ad),e(A,Kt),e(Kt,rd),e(Kt,Ln),e(Ln,id),e(Kt,ld),e(A,dd),e(A,Zt),e(Zt,cd),e(Zt,Mn),e(Mn,pd),e(Zt,md),e(A,gd),e(A,qe),e(qe,re),e(re,Ws),e(Ws,ud),e(re,_d),e(re,Bs),e(Bs,hd),e(re,fd),e(re,Hs),e(Hs,bd),e(re,xd),e(re,Rs),e(Rs,kd),e(re,vd),e(qe,yd),e(qe,ie),e(ie,Us),e(Us,jd),e(ie,Ld),e(ie,Vs),e(Vs,Md),e(ie,wd),e(ie,Ks),e(Ks,Td),e(ie,Ed),e(ie,Zs),e(Zs,Od),e(ie,qd),e(qe,Gd),e(qe,le),e(le,Xs),e(Xs,Sd),e(le,$d),e(le,Js),e(Js,Fd),e(le,Ad),e(le,Qs),e(Qs,zd),e(le,Pd),e(le,Ys),e(Ys,Nd),e(le,Dd),e(A,Cd),e(A,O),u(Xt,O,null),e(O,Id),e(O,eo),e(eo,Wd),e(O,Bd),e(O,Ge),e(Ge,de),e(de,to),e(to,Hd),e(de,Rd),e(de,no),e(no,Ud),e(de,Vd),e(de,so),e(so,Kd),e(de,Zd),e(de,oo),e(oo,Xd),e(de,Jd),e(Ge,Qd),e(Ge,ce),e(ce,ao),e(ao,Yd),e(ce,ec),e(ce,ro),e(ro,tc),e(ce,nc),e(ce,io),e(io,sc),e(ce,oc),e(ce,lo),e(lo,ac),e(ce,rc),e(Ge,ic),e(Ge,pe),e(pe,co),e(co,lc),e(pe,dc),e(pe,po),e(po,cc),e(pe,pc),e(pe,mo),e(mo,mc),e(pe,gc),e(pe,go),e(go,uc),e(pe,_c),e(O,hc),u(et,O,null),e(O,fc),e(O,Jt),e(Jt,bc),e(Jt,Qt),e(Qt,xc),e(Jt,kc),e(O,vc),e(O,uo),e(uo,yc),e(O,jc),u(Yt,O,null),yo=!0},p(i,[v]){const en={};v&2&&(en.$$scope={dirty:v,ctx:i}),Je.$set(en);const _o={};v&2&&(_o.$$scope={dirty:v,ctx:i}),et.$set(_o)},i(i){yo||(_(T.$$.fragment,i),_(dt.$$.fragment,i),_(ct.$$.fragment,i),_(gt.$$.fragment,i),_(Je.$$.fragment,i),_(ht.$$.fragment,i),_(ft.$$.fragment,i),_(bt.$$.fragment,i),_(xt.$$.fragment,i),_(vt.$$.fragment,i),_(yt.$$.fragment,i),_(Lt.$$.fragment,i),_(Mt.$$.fragment,i),_(Tt.$$.fragment,i),_(Et.$$.fragment,i),_(qt.$$.fragment,i),_(Gt.$$.fragment,i),_($t.$$.fragment,i),_(Ft.$$.fragment,i),_(zt.$$.fragment,i),_(Pt.$$.fragment,i),_(Nt.$$.fragment,i),_(Ct.$$.fragment,i),_(Rt.$$.fragment,i),_(Ut.$$.fragment,i),_(Vt.$$.fragment,i),_(Xt.$$.fragment,i),_(et.$$.fragment,i),_(Yt.$$.fragment,i),yo=!0)},o(i){h(T.$$.fragment,i),h(dt.$$.fragment,i),h(ct.$$.fragment,i),h(gt.$$.fragment,i),h(Je.$$.fragment,i),h(ht.$$.fragment,i),h(ft.$$.fragment,i),h(bt.$$.fragment,i),h(xt.$$.fragment,i),h(vt.$$.fragment,i),h(yt.$$.fragment,i),h(Lt.$$.fragment,i),h(Mt.$$.fragment,i),h(Tt.$$.fragment,i),h(Et.$$.fragment,i),h(qt.$$.fragment,i),h(Gt.$$.fragment,i),h($t.$$.fragment,i),h(Ft.$$.fragment,i),h(zt.$$.fragment,i),h(Pt.$$.fragment,i),h(Nt.$$.fragment,i),h(Ct.$$.fragment,i),h(Rt.$$.fragment,i),h(Ut.$$.fragment,i),h(Vt.$$.fragment,i),h(Xt.$$.fragment,i),h(et.$$.fragment,i),h(Yt.$$.fragment,i),yo=!1},d(i){s(p),i&&s(z),i&&s(y),f(T),i&&s(I),i&&s(G),i&&s(xe),i&&s(ke),i&&s(ho),i&&s(Te),f(dt),i&&s(fo),i&&s(k),f(ct),f(gt),f(Je),f(ht),f(ft),f(bt),f(xt),f(vt),f(yt),f(Lt),f(Mt),f(Tt),f(Et),f(qt),f(Gt),f($t),f(Ft),f(zt),i&&s(bo),i&&s(Ee),f(Pt),i&&s(xo),i&&s(he),f(Nt),f(Ct),f(Rt),i&&s(ko),i&&s(Oe),f(Ut),i&&s(vo),i&&s(A),f(Vt),f(Xt),f(et),f(Yt)}}}const Vm={local:"generation",sections:[{local:"transformers.generation_utils.GenerationMixin",title:"GenerationMixin"},{local:"transformers.generation_tf_utils.TFGenerationMixin",title:"TFGenerationMixin"},{local:"transformers.generation_flax_utils.FlaxGenerationMixin",title:"FlaxGenerationMixin"}],title:"Generation"};function Km(lt,p,z){let{fw:y}=p;return lt.$$set=w=>{"fw"in w&&z(0,y=w.fw)},[y]}class eg extends Cm{constructor(p){super();Im(this,p,Km,Um,Wm,{fw:0})}}export{eg as default,Vm as metadata};
