import{S as O2t,i as X2t,s as V2t,e as a,k as l,w as m,t as o,M as z2t,c as s,d as t,m as i,a as n,x as f,h as r,b as d,F as e,g as b,y as g,q as h,o as u,B as p}from"../../chunks/vendor-4833417e.js";import{T as iLr}from"../../chunks/Tip-fffd6df1.js";import{D as E}from"../../chunks/Docstring-44c5af16.js";import{C as w}from"../../chunks/CodeBlock-90ffda97.js";import{I as V}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-04a16537.js";function W2t(Ai){let J,Be,ie,fe,so,ce,_e,Go,Li,Mm,na,Bi,xi,T5,Em,ye,io,ki,$s,F5,Is,Ds,C5,Ri,js,M5,Si,ym,$a;return{c(){J=a("p"),Be=o("If your "),ie=a("code"),fe=o("NewModelConfig"),so=o(" is a subclass of "),ce=a("code"),_e=o("PretrainedConfig"),Go=o(`, make sure its
`),Li=a("code"),Mm=o("model_type"),na=o(" attribute is set to the same key you use when registering the config (here "),Bi=a("code"),xi=o('"new-model"'),T5=o(")."),Em=l(),ye=a("p"),io=o("Likewise, if your "),ki=a("code"),$s=o("NewModel"),F5=o(" is a subclass of "),Is=a("a"),Ds=o("PreTrainedModel"),C5=o(`, make sure its
`),Ri=a("code"),js=o("config_class"),M5=o(` attribute is set to the same class you use when registering the model (here
`),Si=a("code"),ym=o("NewModelConfig"),$a=o(")."),this.h()},l(co){J=s(co,"P",{});var ge=n(J);Be=r(ge,"If your "),ie=s(ge,"CODE",{});var s7=n(ie);fe=r(s7,"NewModelConfig"),s7.forEach(t),so=r(ge," is a subclass of "),ce=s(ge,"CODE",{});var Pi=n(ce);_e=r(Pi,"PretrainedConfig"),Pi.forEach(t),Go=r(ge,`, make sure its
`),Li=s(ge,"CODE",{});var n7=n(Li);Mm=r(n7,"model_type"),n7.forEach(t),na=r(ge," attribute is set to the same key you use when registering the config (here "),Bi=s(ge,"CODE",{});var l7=n(Bi);xi=r(l7,'"new-model"'),l7.forEach(t),T5=r(ge,")."),ge.forEach(t),Em=i(co),ye=s(co,"P",{});var Oo=n(ye);io=r(Oo,"Likewise, if your "),ki=s(Oo,"CODE",{});var Ia=n(ki);$s=r(Ia,"NewModel"),Ia.forEach(t),F5=r(Oo," is a subclass of "),Is=s(Oo,"A",{href:!0});var i7=n(Is);Ds=r(i7,"PreTrainedModel"),i7.forEach(t),C5=r(Oo,`, make sure its
`),Ri=s(Oo,"CODE",{});var wm=n(Ri);js=r(wm,"config_class"),wm.forEach(t),M5=r(Oo,` attribute is set to the same class you use when registering the model (here
`),Si=s(Oo,"CODE",{});var d7=n(Si);ym=r(d7,"NewModelConfig"),d7.forEach(t),$a=r(Oo,")."),Oo.forEach(t),this.h()},h(){d(Is,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel")},m(co,ge){b(co,J,ge),e(J,Be),e(J,ie),e(ie,fe),e(J,so),e(J,ce),e(ce,_e),e(J,Go),e(J,Li),e(Li,Mm),e(J,na),e(J,Bi),e(Bi,xi),e(J,T5),b(co,Em,ge),b(co,ye,ge),e(ye,io),e(ye,ki),e(ki,$s),e(ye,F5),e(ye,Is),e(Is,Ds),e(ye,C5),e(ye,Ri),e(Ri,js),e(ye,M5),e(ye,Si),e(Si,ym),e(ye,$a)},d(co){co&&t(J),co&&t(Em),co&&t(ye)}}}function Q2t(Ai){let J,Be,ie,fe,so;return{c(){J=a("p"),Be=o("Passing "),ie=a("code"),fe=o("use_auth_token=True"),so=o(" is required when you want to use a private model.")},l(ce){J=s(ce,"P",{});var _e=n(J);Be=r(_e,"Passing "),ie=s(_e,"CODE",{});var Go=n(ie);fe=r(Go,"use_auth_token=True"),Go.forEach(t),so=r(_e," is required when you want to use a private model."),_e.forEach(t)},m(ce,_e){b(ce,J,_e),e(J,Be),e(J,ie),e(ie,fe),e(J,so)},d(ce){ce&&t(J)}}}function H2t(Ai){let J,Be,ie,fe,so;return{c(){J=a("p"),Be=o("Passing "),ie=a("code"),fe=o("use_auth_token=True"),so=o(" is required when you want to use a private model.")},l(ce){J=s(ce,"P",{});var _e=n(J);Be=r(_e,"Passing "),ie=s(_e,"CODE",{});var Go=n(ie);fe=r(Go,"use_auth_token=True"),Go.forEach(t),so=r(_e," is required when you want to use a private model."),_e.forEach(t)},m(ce,_e){b(ce,J,_e),e(J,Be),e(J,ie),e(ie,fe),e(J,so)},d(ce){ce&&t(J)}}}function U2t(Ai){let J,Be,ie,fe,so,ce,_e,Go,Li,Mm,na,Bi,xi,T5,Em,ye,io,ki,$s,F5,Is,Ds,C5,Ri,js,M5,Si,ym,$a,co,ge,s7,Pi,n7,l7,Oo,Ia,i7,wm,d7,ARe,_9e,$i,Am,pW,E5,LRe,_W,BRe,b9e,Ns,xRe,bW,kRe,RRe,vW,SRe,PRe,v9e,y5,T9e,c7,$Re,F9e,Lm,C9e,Ii,Bm,TW,w5,IRe,FW,DRe,M9e,Xo,A5,jRe,L5,NRe,m7,qRe,GRe,ORe,B5,XRe,CW,VRe,zRe,WRe,mo,x5,QRe,MW,HRe,URe,Di,JRe,EW,YRe,KRe,yW,ZRe,eSe,oSe,v,xm,wW,rSe,tSe,f7,aSe,sSe,nSe,km,AW,lSe,iSe,g7,dSe,cSe,mSe,Rm,LW,fSe,gSe,h7,hSe,uSe,pSe,Sm,BW,_Se,bSe,u7,vSe,TSe,FSe,Pm,xW,CSe,MSe,p7,ESe,ySe,wSe,$m,kW,ASe,LSe,_7,BSe,xSe,kSe,Im,RW,RSe,SSe,b7,PSe,$Se,ISe,Dm,SW,DSe,jSe,v7,NSe,qSe,GSe,jm,PW,OSe,XSe,T7,VSe,zSe,WSe,Nm,$W,QSe,HSe,F7,USe,JSe,YSe,qm,IW,KSe,ZSe,C7,ePe,oPe,rPe,Gm,DW,tPe,aPe,M7,sPe,nPe,lPe,Om,jW,iPe,dPe,E7,cPe,mPe,fPe,Xm,NW,gPe,hPe,y7,uPe,pPe,_Pe,Vm,qW,bPe,vPe,w7,TPe,FPe,CPe,zm,GW,MPe,EPe,A7,yPe,wPe,APe,Wm,OW,LPe,BPe,L7,xPe,kPe,RPe,Qm,XW,SPe,PPe,B7,$Pe,IPe,DPe,Hm,VW,jPe,NPe,x7,qPe,GPe,OPe,Um,zW,XPe,VPe,k7,zPe,WPe,QPe,Jm,WW,HPe,UPe,R7,JPe,YPe,KPe,Ym,QW,ZPe,e$e,S7,o$e,r$e,t$e,Km,HW,a$e,s$e,P7,n$e,l$e,i$e,Zm,UW,d$e,c$e,$7,m$e,f$e,g$e,ef,JW,h$e,u$e,I7,p$e,_$e,b$e,of,YW,v$e,T$e,D7,F$e,C$e,M$e,rf,KW,E$e,y$e,j7,w$e,A$e,L$e,tf,ZW,B$e,x$e,N7,k$e,R$e,S$e,af,eQ,P$e,$$e,q7,I$e,D$e,j$e,sf,oQ,N$e,q$e,G7,G$e,O$e,X$e,nf,rQ,V$e,z$e,O7,W$e,Q$e,H$e,lf,tQ,U$e,J$e,X7,Y$e,K$e,Z$e,df,aQ,eIe,oIe,V7,rIe,tIe,aIe,cf,sQ,sIe,nIe,z7,lIe,iIe,dIe,mf,nQ,cIe,mIe,W7,fIe,gIe,hIe,ff,lQ,uIe,pIe,Q7,_Ie,bIe,vIe,gf,iQ,TIe,FIe,H7,CIe,MIe,EIe,hf,dQ,yIe,wIe,U7,AIe,LIe,BIe,uf,cQ,xIe,kIe,J7,RIe,SIe,PIe,pf,mQ,$Ie,IIe,Y7,DIe,jIe,NIe,_f,fQ,qIe,GIe,K7,OIe,XIe,VIe,bf,gQ,zIe,WIe,Z7,QIe,HIe,UIe,vf,hQ,JIe,YIe,e9,KIe,ZIe,eDe,Tf,uQ,oDe,rDe,o9,tDe,aDe,sDe,Ff,pQ,nDe,lDe,r9,iDe,dDe,cDe,Cf,_Q,mDe,fDe,t9,gDe,hDe,uDe,Mf,bQ,pDe,_De,a9,bDe,vDe,TDe,Ef,vQ,FDe,CDe,s9,MDe,EDe,yDe,yf,TQ,wDe,ADe,n9,LDe,BDe,xDe,wf,FQ,kDe,RDe,l9,SDe,PDe,$De,Af,CQ,IDe,DDe,i9,jDe,NDe,qDe,Lf,MQ,GDe,ODe,d9,XDe,VDe,zDe,Bf,EQ,WDe,QDe,c9,HDe,UDe,JDe,xf,yQ,YDe,KDe,m9,ZDe,eje,oje,kf,wQ,rje,tje,f9,aje,sje,nje,Rf,AQ,lje,ije,g9,dje,cje,mje,Sf,LQ,fje,gje,h9,hje,uje,pje,Pf,BQ,_je,bje,u9,vje,Tje,Fje,$f,xQ,Cje,Mje,p9,Eje,yje,wje,If,kQ,Aje,Lje,_9,Bje,xje,kje,Df,RQ,Rje,Sje,b9,Pje,$je,Ije,jf,SQ,Dje,jje,v9,Nje,qje,Gje,Nf,PQ,Oje,Xje,T9,Vje,zje,Wje,qf,$Q,Qje,Hje,F9,Uje,Jje,Yje,Gf,IQ,Kje,Zje,C9,eNe,oNe,rNe,Of,DQ,tNe,aNe,M9,sNe,nNe,lNe,Xf,jQ,iNe,dNe,E9,cNe,mNe,fNe,Vf,NQ,gNe,hNe,y9,uNe,pNe,_Ne,zf,qQ,bNe,vNe,w9,TNe,FNe,CNe,Wf,GQ,MNe,ENe,A9,yNe,wNe,ANe,Qf,OQ,LNe,BNe,L9,xNe,kNe,RNe,Hf,XQ,SNe,PNe,B9,$Ne,INe,DNe,Uf,VQ,jNe,NNe,x9,qNe,GNe,ONe,Jf,zQ,XNe,VNe,k9,zNe,WNe,QNe,Yf,WQ,HNe,UNe,R9,JNe,YNe,KNe,Kf,QQ,ZNe,eqe,S9,oqe,rqe,tqe,Zf,HQ,aqe,sqe,P9,nqe,lqe,iqe,eg,UQ,dqe,cqe,$9,mqe,fqe,gqe,og,JQ,hqe,uqe,I9,pqe,_qe,bqe,rg,YQ,vqe,Tqe,D9,Fqe,Cqe,Mqe,tg,KQ,Eqe,yqe,j9,wqe,Aqe,Lqe,ag,ZQ,Bqe,xqe,N9,kqe,Rqe,Sqe,sg,eH,Pqe,$qe,q9,Iqe,Dqe,jqe,ng,oH,Nqe,qqe,G9,Gqe,Oqe,Xqe,lg,rH,Vqe,zqe,O9,Wqe,Qqe,Hqe,ig,tH,Uqe,Jqe,X9,Yqe,Kqe,Zqe,dg,aH,eGe,oGe,V9,rGe,tGe,aGe,cg,sH,sGe,nGe,z9,lGe,iGe,dGe,mg,nH,cGe,mGe,W9,fGe,gGe,hGe,fg,lH,uGe,pGe,Q9,_Ge,bGe,vGe,gg,iH,TGe,FGe,H9,CGe,MGe,EGe,hg,dH,yGe,wGe,U9,AGe,LGe,BGe,ug,cH,xGe,kGe,J9,RGe,SGe,PGe,mH,$Ge,IGe,k5,DGe,pg,R5,jGe,fH,NGe,E9e,ji,_g,gH,S5,qGe,hH,GGe,y9e,Vo,P5,OGe,$5,XGe,Y9,VGe,zGe,WGe,I5,QGe,uH,HGe,UGe,JGe,fo,D5,YGe,pH,KGe,ZGe,Da,eOe,_H,oOe,rOe,bH,tOe,aOe,vH,sOe,nOe,lOe,M,qs,TH,iOe,dOe,K9,cOe,mOe,Z9,fOe,gOe,hOe,Gs,FH,uOe,pOe,eB,_Oe,bOe,oB,vOe,TOe,FOe,Os,CH,COe,MOe,rB,EOe,yOe,tB,wOe,AOe,LOe,bg,MH,BOe,xOe,aB,kOe,ROe,SOe,Xs,EH,POe,$Oe,sB,IOe,DOe,nB,jOe,NOe,qOe,vg,yH,GOe,OOe,lB,XOe,VOe,zOe,Tg,wH,WOe,QOe,iB,HOe,UOe,JOe,Fg,AH,YOe,KOe,dB,ZOe,eXe,oXe,Vs,LH,rXe,tXe,cB,aXe,sXe,mB,nXe,lXe,iXe,zs,BH,dXe,cXe,fB,mXe,fXe,gB,gXe,hXe,uXe,Ws,xH,pXe,_Xe,hB,bXe,vXe,uB,TXe,FXe,CXe,Cg,kH,MXe,EXe,pB,yXe,wXe,AXe,Mg,RH,LXe,BXe,_B,xXe,kXe,RXe,Qs,SH,SXe,PXe,bB,$Xe,IXe,vB,DXe,jXe,NXe,Eg,PH,qXe,GXe,TB,OXe,XXe,VXe,Hs,$H,zXe,WXe,FB,QXe,HXe,CB,UXe,JXe,YXe,Us,IH,KXe,ZXe,MB,eVe,oVe,EB,rVe,tVe,aVe,Js,DH,sVe,nVe,yB,lVe,iVe,jH,dVe,cVe,mVe,yg,NH,fVe,gVe,wB,hVe,uVe,pVe,Ys,qH,_Ve,bVe,AB,vVe,TVe,LB,FVe,CVe,MVe,wg,GH,EVe,yVe,BB,wVe,AVe,LVe,Ks,OH,BVe,xVe,xB,kVe,RVe,kB,SVe,PVe,$Ve,Zs,XH,IVe,DVe,RB,jVe,NVe,SB,qVe,GVe,OVe,en,VH,XVe,VVe,PB,zVe,WVe,$B,QVe,HVe,UVe,Ag,zH,JVe,YVe,IB,KVe,ZVe,eze,on,WH,oze,rze,DB,tze,aze,jB,sze,nze,lze,Lg,QH,ize,dze,NB,cze,mze,fze,rn,HH,gze,hze,qB,uze,pze,GB,_ze,bze,vze,tn,UH,Tze,Fze,OB,Cze,Mze,XB,Eze,yze,wze,an,JH,Aze,Lze,VB,Bze,xze,zB,kze,Rze,Sze,sn,YH,Pze,$ze,WB,Ize,Dze,QB,jze,Nze,qze,Bg,KH,Gze,Oze,HB,Xze,Vze,zze,nn,ZH,Wze,Qze,UB,Hze,Uze,JB,Jze,Yze,Kze,ln,eU,Zze,eWe,YB,oWe,rWe,KB,tWe,aWe,sWe,dn,oU,nWe,lWe,ZB,iWe,dWe,ex,cWe,mWe,fWe,cn,rU,gWe,hWe,ox,uWe,pWe,rx,_We,bWe,vWe,mn,tU,TWe,FWe,tx,CWe,MWe,ax,EWe,yWe,wWe,fn,aU,AWe,LWe,sx,BWe,xWe,nx,kWe,RWe,SWe,xg,sU,PWe,$We,lx,IWe,DWe,jWe,gn,nU,NWe,qWe,ix,GWe,OWe,dx,XWe,VWe,zWe,kg,lU,WWe,QWe,cx,HWe,UWe,JWe,Rg,iU,YWe,KWe,mx,ZWe,eQe,oQe,hn,dU,rQe,tQe,fx,aQe,sQe,gx,nQe,lQe,iQe,un,cU,dQe,cQe,hx,mQe,fQe,ux,gQe,hQe,uQe,Sg,mU,pQe,_Qe,px,bQe,vQe,TQe,pn,fU,FQe,CQe,_x,MQe,EQe,bx,yQe,wQe,AQe,_n,gU,LQe,BQe,vx,xQe,kQe,Tx,RQe,SQe,PQe,bn,hU,$Qe,IQe,Fx,DQe,jQe,Cx,NQe,qQe,GQe,vn,uU,OQe,XQe,Mx,VQe,zQe,Ex,WQe,QQe,HQe,Tn,pU,UQe,JQe,yx,YQe,KQe,wx,ZQe,eHe,oHe,Pg,_U,rHe,tHe,Ax,aHe,sHe,nHe,$g,bU,lHe,iHe,Lx,dHe,cHe,mHe,Ig,vU,fHe,gHe,Bx,hHe,uHe,pHe,Dg,TU,_He,bHe,xx,vHe,THe,FHe,Fn,FU,CHe,MHe,kx,EHe,yHe,Rx,wHe,AHe,LHe,jg,CU,BHe,xHe,Sx,kHe,RHe,SHe,Cn,MU,PHe,$He,Px,IHe,DHe,$x,jHe,NHe,qHe,Mn,EU,GHe,OHe,Ix,XHe,VHe,Dx,zHe,WHe,QHe,En,yU,HHe,UHe,jx,JHe,YHe,Nx,KHe,ZHe,eUe,yn,wU,oUe,rUe,qx,tUe,aUe,Gx,sUe,nUe,lUe,wn,AU,iUe,dUe,Ox,cUe,mUe,Xx,fUe,gUe,hUe,Ng,LU,uUe,pUe,Vx,_Ue,bUe,vUe,qg,BU,TUe,FUe,zx,CUe,MUe,EUe,An,xU,yUe,wUe,Wx,AUe,LUe,Qx,BUe,xUe,kUe,Ln,kU,RUe,SUe,Hx,PUe,$Ue,Ux,IUe,DUe,jUe,Bn,RU,NUe,qUe,Jx,GUe,OUe,Yx,XUe,VUe,zUe,Gg,SU,WUe,QUe,Kx,HUe,UUe,JUe,Og,PU,YUe,KUe,Zx,ZUe,eJe,oJe,Xg,$U,rJe,tJe,ek,aJe,sJe,nJe,Vg,IU,lJe,iJe,ok,dJe,cJe,mJe,xn,DU,fJe,gJe,rk,hJe,uJe,tk,pJe,_Je,bJe,zg,jU,vJe,TJe,ak,FJe,CJe,MJe,Wg,NU,EJe,yJe,sk,wJe,AJe,LJe,kn,qU,BJe,xJe,nk,kJe,RJe,lk,SJe,PJe,$Je,Rn,GU,IJe,DJe,ik,jJe,NJe,dk,qJe,GJe,OJe,OU,XJe,VJe,j5,zJe,Qg,N5,WJe,XU,QJe,w9e,Ni,Hg,VU,q5,HJe,zU,UJe,A9e,zo,G5,JJe,O5,YJe,ck,KJe,ZJe,eYe,X5,oYe,WU,rYe,tYe,aYe,xe,V5,sYe,QU,nYe,lYe,ja,iYe,HU,dYe,cYe,UU,mYe,fYe,JU,gYe,hYe,uYe,ne,Ug,YU,pYe,_Ye,mk,bYe,vYe,TYe,Jg,KU,FYe,CYe,fk,MYe,EYe,yYe,Yg,ZU,wYe,AYe,gk,LYe,BYe,xYe,Kg,eJ,kYe,RYe,hk,SYe,PYe,$Ye,Zg,oJ,IYe,DYe,uk,jYe,NYe,qYe,eh,rJ,GYe,OYe,pk,XYe,VYe,zYe,oh,tJ,WYe,QYe,_k,HYe,UYe,JYe,rh,aJ,YYe,KYe,bk,ZYe,eKe,oKe,th,sJ,rKe,tKe,vk,aKe,sKe,nKe,ah,nJ,lKe,iKe,Tk,dKe,cKe,mKe,sh,lJ,fKe,gKe,Fk,hKe,uKe,pKe,nh,iJ,_Ke,bKe,Ck,vKe,TKe,FKe,lh,dJ,CKe,MKe,Mk,EKe,yKe,wKe,ih,cJ,AKe,LKe,Ek,BKe,xKe,kKe,dh,mJ,RKe,SKe,yk,PKe,$Ke,IKe,ch,DKe,fJ,jKe,NKe,z5,qKe,mh,W5,GKe,gJ,OKe,L9e,qi,fh,hJ,Q5,XKe,uJ,VKe,B9e,Wo,H5,zKe,U5,WKe,wk,QKe,HKe,UKe,J5,JKe,pJ,YKe,KKe,ZKe,ke,Y5,eZe,_J,oZe,rZe,Gi,tZe,bJ,aZe,sZe,vJ,nZe,lZe,iZe,we,gh,TJ,dZe,cZe,Ak,mZe,fZe,gZe,hh,FJ,hZe,uZe,Lk,pZe,_Ze,bZe,uh,CJ,vZe,TZe,Bk,FZe,CZe,MZe,ph,MJ,EZe,yZe,xk,wZe,AZe,LZe,_h,EJ,BZe,xZe,kk,kZe,RZe,SZe,bh,yJ,PZe,$Ze,Rk,IZe,DZe,jZe,vh,wJ,NZe,qZe,Sk,GZe,OZe,XZe,Th,AJ,VZe,zZe,Pk,WZe,QZe,HZe,Fh,UZe,LJ,JZe,YZe,K5,KZe,Ch,Z5,ZZe,BJ,eeo,x9e,Oi,Mh,xJ,ey,oeo,kJ,reo,k9e,Qo,oy,teo,Xi,aeo,RJ,seo,neo,SJ,leo,ieo,deo,ry,ceo,PJ,meo,feo,geo,qr,ty,heo,$J,ueo,peo,Vi,_eo,IJ,beo,veo,DJ,Teo,Feo,Ceo,jJ,Meo,Eeo,ay,yeo,Re,sy,weo,NJ,Aeo,Leo,Na,Beo,qJ,xeo,keo,GJ,Reo,Seo,OJ,Peo,$eo,Ieo,F,Eh,XJ,Deo,jeo,$k,Neo,qeo,Geo,yh,VJ,Oeo,Xeo,Ik,Veo,zeo,Weo,wh,zJ,Qeo,Heo,Dk,Ueo,Jeo,Yeo,Ah,WJ,Keo,Zeo,jk,eoo,ooo,roo,Lh,QJ,too,aoo,Nk,soo,noo,loo,Bh,HJ,ioo,doo,qk,coo,moo,foo,xh,UJ,goo,hoo,Gk,uoo,poo,_oo,kh,JJ,boo,voo,Ok,Too,Foo,Coo,Rh,YJ,Moo,Eoo,Xk,yoo,woo,Aoo,Sh,KJ,Loo,Boo,Vk,xoo,koo,Roo,Ph,ZJ,Soo,Poo,zk,$oo,Ioo,Doo,$h,eY,joo,Noo,Wk,qoo,Goo,Ooo,Ih,oY,Xoo,Voo,Qk,zoo,Woo,Qoo,Dh,rY,Hoo,Uoo,Hk,Joo,Yoo,Koo,jh,tY,Zoo,ero,Uk,oro,rro,tro,Nh,aY,aro,sro,Jk,nro,lro,iro,qh,sY,dro,cro,Yk,mro,fro,gro,Gh,nY,hro,uro,Kk,pro,_ro,bro,Oh,lY,vro,Tro,Zk,Fro,Cro,Mro,Xh,iY,Ero,yro,eR,wro,Aro,Lro,Vh,dY,Bro,xro,oR,kro,Rro,Sro,zh,cY,Pro,$ro,rR,Iro,Dro,jro,Wh,mY,Nro,qro,tR,Gro,Oro,Xro,Qh,fY,Vro,zro,aR,Wro,Qro,Hro,Hh,gY,Uro,Jro,sR,Yro,Kro,Zro,Uh,hY,eto,oto,nR,rto,tto,ato,Jh,uY,sto,nto,lR,lto,ito,dto,Sn,pY,cto,mto,iR,fto,gto,dR,hto,uto,pto,Yh,_Y,_to,bto,cR,vto,Tto,Fto,Kh,bY,Cto,Mto,mR,Eto,yto,wto,Zh,vY,Ato,Lto,fR,Bto,xto,kto,eu,TY,Rto,Sto,gR,Pto,$to,Ito,ou,FY,Dto,jto,hR,Nto,qto,Gto,ru,CY,Oto,Xto,uR,Vto,zto,Wto,tu,MY,Qto,Hto,pR,Uto,Jto,Yto,au,EY,Kto,Zto,_R,eao,oao,rao,su,yY,tao,aao,bR,sao,nao,lao,nu,wY,iao,dao,vR,cao,mao,fao,lu,AY,gao,hao,TR,uao,pao,_ao,iu,LY,bao,vao,FR,Tao,Fao,Cao,du,BY,Mao,Eao,CR,yao,wao,Aao,cu,xY,Lao,Bao,MR,xao,kao,Rao,mu,kY,Sao,Pao,ER,$ao,Iao,Dao,fu,RY,jao,Nao,yR,qao,Gao,Oao,gu,SY,Xao,Vao,wR,zao,Wao,Qao,hu,PY,Hao,Uao,AR,Jao,Yao,Kao,uu,$Y,Zao,eso,LR,oso,rso,tso,pu,IY,aso,sso,BR,nso,lso,iso,_u,DY,dso,cso,xR,mso,fso,gso,bu,jY,hso,uso,kR,pso,_so,bso,vu,NY,vso,Tso,RR,Fso,Cso,Mso,Tu,qY,Eso,yso,SR,wso,Aso,Lso,Fu,GY,Bso,xso,PR,kso,Rso,Sso,Cu,OY,Pso,$so,$R,Iso,Dso,jso,Mu,XY,Nso,qso,IR,Gso,Oso,Xso,Eu,VY,Vso,zso,DR,Wso,Qso,Hso,yu,zY,Uso,Jso,jR,Yso,Kso,Zso,wu,WY,eno,ono,NR,rno,tno,ano,Au,QY,sno,nno,qR,lno,ino,dno,Lu,HY,cno,mno,GR,fno,gno,hno,Bu,UY,uno,pno,OR,_no,bno,vno,xu,JY,Tno,Fno,XR,Cno,Mno,Eno,ku,YY,yno,wno,VR,Ano,Lno,Bno,Ru,KY,xno,kno,zR,Rno,Sno,Pno,Su,ZY,$no,Ino,WR,Dno,jno,Nno,Pu,eK,qno,Gno,QR,Ono,Xno,Vno,$u,oK,zno,Wno,HR,Qno,Hno,Uno,Iu,rK,Jno,Yno,UR,Kno,Zno,elo,Du,tK,olo,rlo,JR,tlo,alo,slo,ju,aK,nlo,llo,YR,ilo,dlo,clo,Nu,sK,mlo,flo,KR,glo,hlo,ulo,qu,nK,plo,_lo,ZR,blo,vlo,Tlo,Gu,lK,Flo,Clo,eS,Mlo,Elo,ylo,Ou,iK,wlo,Alo,oS,Llo,Blo,xlo,Xu,dK,klo,Rlo,rS,Slo,Plo,$lo,Vu,cK,Ilo,Dlo,tS,jlo,Nlo,qlo,zu,mK,Glo,Olo,aS,Xlo,Vlo,zlo,Wu,fK,Wlo,Qlo,sS,Hlo,Ulo,Jlo,Qu,gK,Ylo,Klo,nS,Zlo,eio,oio,Hu,hK,rio,tio,lS,aio,sio,nio,Uu,uK,lio,iio,iS,dio,cio,mio,Ju,pK,fio,gio,dS,hio,uio,pio,Yu,_K,_io,bio,cS,vio,Tio,Fio,Ku,bK,Cio,Mio,mS,Eio,yio,wio,Zu,vK,Aio,Lio,fS,Bio,xio,kio,ep,TK,Rio,Sio,gS,Pio,$io,Iio,op,Dio,FK,jio,Nio,CK,qio,Gio,MK,Oio,Xio,ny,R9e,zi,rp,EK,ly,Vio,yK,zio,S9e,Ho,iy,Wio,Wi,Qio,wK,Hio,Uio,AK,Jio,Yio,Kio,dy,Zio,LK,edo,odo,rdo,Gr,cy,tdo,BK,ado,sdo,Qi,ndo,xK,ldo,ido,kK,ddo,cdo,mdo,RK,fdo,gdo,my,hdo,Se,fy,udo,SK,pdo,_do,qa,bdo,PK,vdo,Tdo,$K,Fdo,Cdo,IK,Mdo,Edo,ydo,k,tp,DK,wdo,Ado,hS,Ldo,Bdo,xdo,ap,jK,kdo,Rdo,uS,Sdo,Pdo,$do,sp,NK,Ido,Ddo,pS,jdo,Ndo,qdo,np,qK,Gdo,Odo,_S,Xdo,Vdo,zdo,lp,GK,Wdo,Qdo,bS,Hdo,Udo,Jdo,ip,OK,Ydo,Kdo,vS,Zdo,eco,oco,dp,XK,rco,tco,TS,aco,sco,nco,cp,VK,lco,ico,FS,dco,cco,mco,mp,zK,fco,gco,CS,hco,uco,pco,fp,WK,_co,bco,MS,vco,Tco,Fco,gp,QK,Cco,Mco,ES,Eco,yco,wco,hp,HK,Aco,Lco,yS,Bco,xco,kco,up,UK,Rco,Sco,wS,Pco,$co,Ico,pp,JK,Dco,jco,AS,Nco,qco,Gco,_p,YK,Oco,Xco,LS,Vco,zco,Wco,bp,KK,Qco,Hco,BS,Uco,Jco,Yco,vp,ZK,Kco,Zco,xS,emo,omo,rmo,Tp,eZ,tmo,amo,kS,smo,nmo,lmo,Fp,oZ,imo,dmo,RS,cmo,mmo,fmo,Cp,rZ,gmo,hmo,SS,umo,pmo,_mo,Mp,tZ,bmo,vmo,PS,Tmo,Fmo,Cmo,Ep,aZ,Mmo,Emo,$S,ymo,wmo,Amo,yp,sZ,Lmo,Bmo,IS,xmo,kmo,Rmo,wp,nZ,Smo,Pmo,DS,$mo,Imo,Dmo,Ap,lZ,jmo,Nmo,jS,qmo,Gmo,Omo,Lp,iZ,Xmo,Vmo,NS,zmo,Wmo,Qmo,Bp,dZ,Hmo,Umo,qS,Jmo,Ymo,Kmo,xp,cZ,Zmo,efo,GS,ofo,rfo,tfo,kp,mZ,afo,sfo,OS,nfo,lfo,ifo,Rp,fZ,dfo,cfo,XS,mfo,ffo,gfo,Sp,gZ,hfo,ufo,VS,pfo,_fo,bfo,Pp,hZ,vfo,Tfo,zS,Ffo,Cfo,Mfo,$p,uZ,Efo,yfo,WS,wfo,Afo,Lfo,Ip,pZ,Bfo,xfo,QS,kfo,Rfo,Sfo,Dp,_Z,Pfo,$fo,HS,Ifo,Dfo,jfo,jp,bZ,Nfo,qfo,US,Gfo,Ofo,Xfo,Np,vZ,Vfo,zfo,JS,Wfo,Qfo,Hfo,qp,TZ,Ufo,Jfo,YS,Yfo,Kfo,Zfo,Gp,FZ,ego,ogo,KS,rgo,tgo,ago,Op,sgo,CZ,ngo,lgo,MZ,igo,dgo,EZ,cgo,mgo,gy,P9e,Hi,Xp,yZ,hy,fgo,wZ,ggo,$9e,Uo,uy,hgo,Ui,ugo,AZ,pgo,_go,LZ,bgo,vgo,Tgo,py,Fgo,BZ,Cgo,Mgo,Ego,Or,_y,ygo,xZ,wgo,Ago,Ji,Lgo,kZ,Bgo,xgo,RZ,kgo,Rgo,Sgo,SZ,Pgo,$go,by,Igo,Pe,vy,Dgo,PZ,jgo,Ngo,Ga,qgo,$Z,Ggo,Ogo,IZ,Xgo,Vgo,DZ,zgo,Wgo,Qgo,$,Vp,jZ,Hgo,Ugo,ZS,Jgo,Ygo,Kgo,zp,NZ,Zgo,eho,eP,oho,rho,tho,Wp,qZ,aho,sho,oP,nho,lho,iho,Qp,GZ,dho,cho,rP,mho,fho,gho,Hp,OZ,hho,uho,tP,pho,_ho,bho,Up,XZ,vho,Tho,aP,Fho,Cho,Mho,Jp,VZ,Eho,yho,sP,who,Aho,Lho,Yp,zZ,Bho,xho,nP,kho,Rho,Sho,Kp,WZ,Pho,$ho,lP,Iho,Dho,jho,Zp,QZ,Nho,qho,iP,Gho,Oho,Xho,e_,HZ,Vho,zho,dP,Who,Qho,Hho,o_,UZ,Uho,Jho,cP,Yho,Kho,Zho,r_,JZ,euo,ouo,mP,ruo,tuo,auo,t_,YZ,suo,nuo,fP,luo,iuo,duo,a_,KZ,cuo,muo,gP,fuo,guo,huo,s_,ZZ,uuo,puo,hP,_uo,buo,vuo,n_,eee,Tuo,Fuo,uP,Cuo,Muo,Euo,l_,oee,yuo,wuo,pP,Auo,Luo,Buo,i_,ree,xuo,kuo,_P,Ruo,Suo,Puo,d_,tee,$uo,Iuo,bP,Duo,juo,Nuo,c_,aee,quo,Guo,vP,Ouo,Xuo,Vuo,m_,see,zuo,Wuo,TP,Quo,Huo,Uuo,f_,nee,Juo,Yuo,FP,Kuo,Zuo,epo,g_,lee,opo,rpo,CP,tpo,apo,spo,h_,iee,npo,lpo,MP,ipo,dpo,cpo,u_,dee,mpo,fpo,EP,gpo,hpo,upo,p_,cee,ppo,_po,yP,bpo,vpo,Tpo,__,mee,Fpo,Cpo,wP,Mpo,Epo,ypo,b_,fee,wpo,Apo,AP,Lpo,Bpo,xpo,v_,gee,kpo,Rpo,LP,Spo,Ppo,$po,T_,hee,Ipo,Dpo,BP,jpo,Npo,qpo,F_,uee,Gpo,Opo,xP,Xpo,Vpo,zpo,C_,pee,Wpo,Qpo,kP,Hpo,Upo,Jpo,M_,_ee,Ypo,Kpo,RP,Zpo,e_o,o_o,E_,bee,r_o,t_o,SP,a_o,s_o,n_o,y_,l_o,vee,i_o,d_o,Tee,c_o,m_o,Fee,f_o,g_o,Ty,I9e,Yi,w_,Cee,Fy,h_o,Mee,u_o,D9e,Jo,Cy,p_o,Ki,__o,Eee,b_o,v_o,yee,T_o,F_o,C_o,My,M_o,wee,E_o,y_o,w_o,Xr,Ey,A_o,Aee,L_o,B_o,Zi,x_o,Lee,k_o,R_o,Bee,S_o,P_o,$_o,xee,I_o,D_o,yy,j_o,$e,wy,N_o,kee,q_o,G_o,Oa,O_o,Ree,X_o,V_o,See,z_o,W_o,Pee,Q_o,H_o,U_o,I,A_,$ee,J_o,Y_o,PP,K_o,Z_o,ebo,L_,Iee,obo,rbo,$P,tbo,abo,sbo,B_,Dee,nbo,lbo,IP,ibo,dbo,cbo,x_,jee,mbo,fbo,DP,gbo,hbo,ubo,k_,Nee,pbo,_bo,jP,bbo,vbo,Tbo,R_,qee,Fbo,Cbo,NP,Mbo,Ebo,ybo,S_,Gee,wbo,Abo,qP,Lbo,Bbo,xbo,P_,Oee,kbo,Rbo,GP,Sbo,Pbo,$bo,$_,Xee,Ibo,Dbo,OP,jbo,Nbo,qbo,I_,Vee,Gbo,Obo,XP,Xbo,Vbo,zbo,D_,zee,Wbo,Qbo,VP,Hbo,Ubo,Jbo,j_,Wee,Ybo,Kbo,zP,Zbo,e2o,o2o,N_,Qee,r2o,t2o,WP,a2o,s2o,n2o,q_,Hee,l2o,i2o,QP,d2o,c2o,m2o,G_,Uee,f2o,g2o,HP,h2o,u2o,p2o,O_,Jee,_2o,b2o,UP,v2o,T2o,F2o,X_,Yee,C2o,M2o,JP,E2o,y2o,w2o,V_,Kee,A2o,L2o,YP,B2o,x2o,k2o,z_,Zee,R2o,S2o,KP,P2o,$2o,I2o,W_,eoe,D2o,j2o,ZP,N2o,q2o,G2o,Q_,ooe,O2o,X2o,e$,V2o,z2o,W2o,H_,roe,Q2o,H2o,o$,U2o,J2o,Y2o,U_,toe,K2o,Z2o,r$,evo,ovo,rvo,J_,aoe,tvo,avo,t$,svo,nvo,lvo,Y_,soe,ivo,dvo,a$,cvo,mvo,fvo,K_,noe,gvo,hvo,s$,uvo,pvo,_vo,Z_,loe,bvo,vvo,n$,Tvo,Fvo,Cvo,eb,ioe,Mvo,Evo,l$,yvo,wvo,Avo,ob,doe,Lvo,Bvo,i$,xvo,kvo,Rvo,rb,coe,Svo,Pvo,d$,$vo,Ivo,Dvo,tb,moe,jvo,Nvo,foe,qvo,Gvo,Ovo,ab,goe,Xvo,Vvo,c$,zvo,Wvo,Qvo,sb,hoe,Hvo,Uvo,m$,Jvo,Yvo,Kvo,nb,uoe,Zvo,eTo,f$,oTo,rTo,tTo,lb,poe,aTo,sTo,g$,nTo,lTo,iTo,ib,dTo,_oe,cTo,mTo,boe,fTo,gTo,voe,hTo,uTo,Ay,j9e,ed,db,Toe,Ly,pTo,Foe,_To,N9e,Yo,By,bTo,od,vTo,Coe,TTo,FTo,Moe,CTo,MTo,ETo,xy,yTo,Eoe,wTo,ATo,LTo,Vr,ky,BTo,yoe,xTo,kTo,rd,RTo,woe,STo,PTo,Aoe,$To,ITo,DTo,Loe,jTo,NTo,Ry,qTo,Ie,Sy,GTo,Boe,OTo,XTo,Xa,VTo,xoe,zTo,WTo,koe,QTo,HTo,Roe,UTo,JTo,YTo,ae,cb,Soe,KTo,ZTo,h$,e1o,o1o,r1o,mb,Poe,t1o,a1o,u$,s1o,n1o,l1o,fb,$oe,i1o,d1o,p$,c1o,m1o,f1o,gb,Ioe,g1o,h1o,_$,u1o,p1o,_1o,hb,Doe,b1o,v1o,b$,T1o,F1o,C1o,ub,joe,M1o,E1o,v$,y1o,w1o,A1o,pb,Noe,L1o,B1o,T$,x1o,k1o,R1o,_b,qoe,S1o,P1o,F$,$1o,I1o,D1o,bb,Goe,j1o,N1o,C$,q1o,G1o,O1o,vb,Ooe,X1o,V1o,M$,z1o,W1o,Q1o,Tb,Xoe,H1o,U1o,E$,J1o,Y1o,K1o,Fb,Voe,Z1o,eFo,y$,oFo,rFo,tFo,Cb,zoe,aFo,sFo,w$,nFo,lFo,iFo,Mb,Woe,dFo,cFo,A$,mFo,fFo,gFo,Eb,Qoe,hFo,uFo,L$,pFo,_Fo,bFo,yb,Hoe,vFo,TFo,B$,FFo,CFo,MFo,wb,EFo,Uoe,yFo,wFo,Joe,AFo,LFo,Yoe,BFo,xFo,Py,q9e,td,Ab,Koe,$y,kFo,Zoe,RFo,G9e,Ko,Iy,SFo,ad,PFo,ere,$Fo,IFo,ore,DFo,jFo,NFo,Dy,qFo,rre,GFo,OFo,XFo,zr,jy,VFo,tre,zFo,WFo,sd,QFo,are,HFo,UFo,sre,JFo,YFo,KFo,nre,ZFo,eCo,Ny,oCo,De,qy,rCo,lre,tCo,aCo,Va,sCo,ire,nCo,lCo,dre,iCo,dCo,cre,cCo,mCo,fCo,A,Lb,mre,gCo,hCo,x$,uCo,pCo,_Co,Bb,fre,bCo,vCo,k$,TCo,FCo,CCo,xb,gre,MCo,ECo,R$,yCo,wCo,ACo,kb,hre,LCo,BCo,S$,xCo,kCo,RCo,Rb,ure,SCo,PCo,P$,$Co,ICo,DCo,Sb,pre,jCo,NCo,$$,qCo,GCo,OCo,Pb,_re,XCo,VCo,I$,zCo,WCo,QCo,$b,bre,HCo,UCo,D$,JCo,YCo,KCo,Ib,vre,ZCo,e4o,j$,o4o,r4o,t4o,Db,Tre,a4o,s4o,N$,n4o,l4o,i4o,jb,Fre,d4o,c4o,q$,m4o,f4o,g4o,Nb,Cre,h4o,u4o,G$,p4o,_4o,b4o,qb,Mre,v4o,T4o,O$,F4o,C4o,M4o,Gb,Ere,E4o,y4o,X$,w4o,A4o,L4o,Ob,yre,B4o,x4o,V$,k4o,R4o,S4o,Xb,wre,P4o,$4o,z$,I4o,D4o,j4o,Vb,Are,N4o,q4o,W$,G4o,O4o,X4o,zb,Lre,V4o,z4o,Q$,W4o,Q4o,H4o,Wb,Bre,U4o,J4o,H$,Y4o,K4o,Z4o,Qb,xre,eMo,oMo,U$,rMo,tMo,aMo,Hb,kre,sMo,nMo,J$,lMo,iMo,dMo,Ub,Rre,cMo,mMo,Y$,fMo,gMo,hMo,Jb,Sre,uMo,pMo,K$,_Mo,bMo,vMo,Yb,Pre,TMo,FMo,Z$,CMo,MMo,EMo,Kb,$re,yMo,wMo,eI,AMo,LMo,BMo,Zb,Ire,xMo,kMo,oI,RMo,SMo,PMo,e2,Dre,$Mo,IMo,rI,DMo,jMo,NMo,o2,jre,qMo,GMo,tI,OMo,XMo,VMo,r2,Nre,zMo,WMo,aI,QMo,HMo,UMo,t2,qre,JMo,YMo,sI,KMo,ZMo,eEo,a2,Gre,oEo,rEo,nI,tEo,aEo,sEo,s2,Ore,nEo,lEo,lI,iEo,dEo,cEo,n2,Xre,mEo,fEo,iI,gEo,hEo,uEo,l2,Vre,pEo,_Eo,dI,bEo,vEo,TEo,i2,zre,FEo,CEo,cI,MEo,EEo,yEo,d2,Wre,wEo,AEo,mI,LEo,BEo,xEo,c2,Qre,kEo,REo,fI,SEo,PEo,$Eo,m2,Hre,IEo,DEo,gI,jEo,NEo,qEo,f2,Ure,GEo,OEo,hI,XEo,VEo,zEo,g2,Jre,WEo,QEo,uI,HEo,UEo,JEo,h2,Yre,YEo,KEo,pI,ZEo,e3o,o3o,u2,Kre,r3o,t3o,_I,a3o,s3o,n3o,p2,Zre,l3o,i3o,bI,d3o,c3o,m3o,_2,ete,f3o,g3o,vI,h3o,u3o,p3o,b2,ote,_3o,b3o,TI,v3o,T3o,F3o,v2,rte,C3o,M3o,FI,E3o,y3o,w3o,T2,A3o,tte,L3o,B3o,ate,x3o,k3o,ste,R3o,S3o,Gy,O9e,nd,F2,nte,Oy,P3o,lte,$3o,X9e,Zo,Xy,I3o,ld,D3o,ite,j3o,N3o,dte,q3o,G3o,O3o,Vy,X3o,cte,V3o,z3o,W3o,Wr,zy,Q3o,mte,H3o,U3o,id,J3o,fte,Y3o,K3o,gte,Z3o,e5o,o5o,hte,r5o,t5o,Wy,a5o,je,Qy,s5o,ute,n5o,l5o,za,i5o,pte,d5o,c5o,_te,m5o,f5o,bte,g5o,h5o,u5o,G,C2,vte,p5o,_5o,CI,b5o,v5o,T5o,M2,Tte,F5o,C5o,MI,M5o,E5o,y5o,E2,Fte,w5o,A5o,EI,L5o,B5o,x5o,y2,Cte,k5o,R5o,yI,S5o,P5o,$5o,w2,Mte,I5o,D5o,wI,j5o,N5o,q5o,A2,Ete,G5o,O5o,AI,X5o,V5o,z5o,L2,yte,W5o,Q5o,LI,H5o,U5o,J5o,B2,wte,Y5o,K5o,BI,Z5o,eyo,oyo,x2,Ate,ryo,tyo,xI,ayo,syo,nyo,k2,Lte,lyo,iyo,kI,dyo,cyo,myo,R2,Bte,fyo,gyo,RI,hyo,uyo,pyo,S2,xte,_yo,byo,SI,vyo,Tyo,Fyo,P2,kte,Cyo,Myo,PI,Eyo,yyo,wyo,$2,Rte,Ayo,Lyo,$I,Byo,xyo,kyo,I2,Ste,Ryo,Syo,II,Pyo,$yo,Iyo,D2,Pte,Dyo,jyo,DI,Nyo,qyo,Gyo,j2,$te,Oyo,Xyo,jI,Vyo,zyo,Wyo,N2,Ite,Qyo,Hyo,NI,Uyo,Jyo,Yyo,q2,Dte,Kyo,Zyo,qI,ewo,owo,rwo,G2,jte,two,awo,GI,swo,nwo,lwo,O2,Nte,iwo,dwo,OI,cwo,mwo,fwo,X2,qte,gwo,hwo,XI,uwo,pwo,_wo,V2,Gte,bwo,vwo,VI,Two,Fwo,Cwo,z2,Ote,Mwo,Ewo,zI,ywo,wwo,Awo,W2,Xte,Lwo,Bwo,WI,xwo,kwo,Rwo,Q2,Vte,Swo,Pwo,QI,$wo,Iwo,Dwo,H2,zte,jwo,Nwo,HI,qwo,Gwo,Owo,U2,Wte,Xwo,Vwo,UI,zwo,Wwo,Qwo,J2,Hwo,Qte,Uwo,Jwo,Hte,Ywo,Kwo,Ute,Zwo,e6o,Hy,V9e,dd,Y2,Jte,Uy,o6o,Yte,r6o,z9e,er,Jy,t6o,cd,a6o,Kte,s6o,n6o,Zte,l6o,i6o,d6o,Yy,c6o,eae,m6o,f6o,g6o,Qr,Ky,h6o,oae,u6o,p6o,md,_6o,rae,b6o,v6o,tae,T6o,F6o,C6o,aae,M6o,E6o,Zy,y6o,Ne,ew,w6o,sae,A6o,L6o,Wa,B6o,nae,x6o,k6o,lae,R6o,S6o,iae,P6o,$6o,I6o,sa,K2,dae,D6o,j6o,JI,N6o,q6o,G6o,Z2,cae,O6o,X6o,YI,V6o,z6o,W6o,ev,mae,Q6o,H6o,KI,U6o,J6o,Y6o,ov,fae,K6o,Z6o,ZI,eAo,oAo,rAo,rv,gae,tAo,aAo,eD,sAo,nAo,lAo,tv,iAo,hae,dAo,cAo,uae,mAo,fAo,pae,gAo,hAo,ow,W9e,fd,av,_ae,rw,uAo,bae,pAo,Q9e,or,tw,_Ao,gd,bAo,vae,vAo,TAo,Tae,FAo,CAo,MAo,aw,EAo,Fae,yAo,wAo,AAo,Hr,sw,LAo,Cae,BAo,xAo,hd,kAo,Mae,RAo,SAo,Eae,PAo,$Ao,IAo,yae,DAo,jAo,nw,NAo,qe,lw,qAo,wae,GAo,OAo,Qa,XAo,Aae,VAo,zAo,Lae,WAo,QAo,Bae,HAo,UAo,JAo,N,sv,xae,YAo,KAo,oD,ZAo,e0o,o0o,nv,kae,r0o,t0o,rD,a0o,s0o,n0o,lv,Rae,l0o,i0o,tD,d0o,c0o,m0o,iv,Sae,f0o,g0o,aD,h0o,u0o,p0o,dv,Pae,_0o,b0o,sD,v0o,T0o,F0o,cv,$ae,C0o,M0o,nD,E0o,y0o,w0o,mv,Iae,A0o,L0o,lD,B0o,x0o,k0o,fv,Dae,R0o,S0o,iD,P0o,$0o,I0o,gv,jae,D0o,j0o,dD,N0o,q0o,G0o,hv,Nae,O0o,X0o,cD,V0o,z0o,W0o,uv,qae,Q0o,H0o,mD,U0o,J0o,Y0o,pv,Gae,K0o,Z0o,fD,eLo,oLo,rLo,_v,Oae,tLo,aLo,gD,sLo,nLo,lLo,bv,Xae,iLo,dLo,hD,cLo,mLo,fLo,vv,Vae,gLo,hLo,uD,uLo,pLo,_Lo,Tv,zae,bLo,vLo,pD,TLo,FLo,CLo,Fv,Wae,MLo,ELo,_D,yLo,wLo,ALo,Cv,Qae,LLo,BLo,bD,xLo,kLo,RLo,Mv,Hae,SLo,PLo,vD,$Lo,ILo,DLo,Ev,Uae,jLo,NLo,TD,qLo,GLo,OLo,yv,Jae,XLo,VLo,FD,zLo,WLo,QLo,wv,Yae,HLo,ULo,CD,JLo,YLo,KLo,Av,Kae,ZLo,e8o,MD,o8o,r8o,t8o,Lv,Zae,a8o,s8o,ED,n8o,l8o,i8o,Bv,ese,d8o,c8o,yD,m8o,f8o,g8o,xv,ose,h8o,u8o,wD,p8o,_8o,b8o,kv,rse,v8o,T8o,AD,F8o,C8o,M8o,Rv,tse,E8o,y8o,LD,w8o,A8o,L8o,Sv,ase,B8o,x8o,BD,k8o,R8o,S8o,Pv,sse,P8o,$8o,xD,I8o,D8o,j8o,$v,nse,N8o,q8o,kD,G8o,O8o,X8o,Iv,lse,V8o,z8o,RD,W8o,Q8o,H8o,Dv,ise,U8o,J8o,SD,Y8o,K8o,Z8o,jv,e7o,dse,o7o,r7o,cse,t7o,a7o,mse,s7o,n7o,iw,H9e,ud,Nv,fse,dw,l7o,gse,i7o,U9e,rr,cw,d7o,pd,c7o,hse,m7o,f7o,use,g7o,h7o,u7o,mw,p7o,pse,_7o,b7o,v7o,Ur,fw,T7o,_se,F7o,C7o,_d,M7o,bse,E7o,y7o,vse,w7o,A7o,L7o,Tse,B7o,x7o,gw,k7o,Ge,hw,R7o,Fse,S7o,P7o,Ha,$7o,Cse,I7o,D7o,Mse,j7o,N7o,Ese,q7o,G7o,O7o,R,qv,yse,X7o,V7o,PD,z7o,W7o,Q7o,Gv,wse,H7o,U7o,$D,J7o,Y7o,K7o,Ov,Ase,Z7o,e9o,ID,o9o,r9o,t9o,Xv,Lse,a9o,s9o,DD,n9o,l9o,i9o,Vv,Bse,d9o,c9o,jD,m9o,f9o,g9o,zv,xse,h9o,u9o,ND,p9o,_9o,b9o,Wv,kse,v9o,T9o,qD,F9o,C9o,M9o,Qv,Rse,E9o,y9o,GD,w9o,A9o,L9o,Hv,Sse,B9o,x9o,OD,k9o,R9o,S9o,Uv,Pse,P9o,$9o,XD,I9o,D9o,j9o,Jv,$se,N9o,q9o,VD,G9o,O9o,X9o,Yv,Ise,V9o,z9o,zD,W9o,Q9o,H9o,Kv,Dse,U9o,J9o,WD,Y9o,K9o,Z9o,Zv,jse,eBo,oBo,QD,rBo,tBo,aBo,eT,Nse,sBo,nBo,HD,lBo,iBo,dBo,oT,qse,cBo,mBo,UD,fBo,gBo,hBo,rT,Gse,uBo,pBo,JD,_Bo,bBo,vBo,tT,Ose,TBo,FBo,YD,CBo,MBo,EBo,aT,Xse,yBo,wBo,KD,ABo,LBo,BBo,sT,Vse,xBo,kBo,ZD,RBo,SBo,PBo,nT,zse,$Bo,IBo,ej,DBo,jBo,NBo,lT,Wse,qBo,GBo,oj,OBo,XBo,VBo,iT,Qse,zBo,WBo,rj,QBo,HBo,UBo,dT,Hse,JBo,YBo,tj,KBo,ZBo,exo,cT,Use,oxo,rxo,aj,txo,axo,sxo,mT,Jse,nxo,lxo,sj,ixo,dxo,cxo,fT,Yse,mxo,fxo,nj,gxo,hxo,uxo,gT,Kse,pxo,_xo,lj,bxo,vxo,Txo,hT,Zse,Fxo,Cxo,ij,Mxo,Exo,yxo,uT,ene,wxo,Axo,dj,Lxo,Bxo,xxo,pT,one,kxo,Rxo,cj,Sxo,Pxo,$xo,_T,rne,Ixo,Dxo,mj,jxo,Nxo,qxo,bT,tne,Gxo,Oxo,fj,Xxo,Vxo,zxo,vT,ane,Wxo,Qxo,gj,Hxo,Uxo,Jxo,TT,sne,Yxo,Kxo,hj,Zxo,eko,oko,FT,nne,rko,tko,uj,ako,sko,nko,CT,lne,lko,iko,pj,dko,cko,mko,MT,ine,fko,gko,_j,hko,uko,pko,ET,dne,_ko,bko,bj,vko,Tko,Fko,yT,Cko,cne,Mko,Eko,mne,yko,wko,fne,Ako,Lko,uw,J9e,bd,wT,gne,pw,Bko,hne,xko,Y9e,tr,_w,kko,vd,Rko,une,Sko,Pko,pne,$ko,Iko,Dko,bw,jko,_ne,Nko,qko,Gko,Jr,vw,Oko,bne,Xko,Vko,Td,zko,vne,Wko,Qko,Tne,Hko,Uko,Jko,Fne,Yko,Kko,Tw,Zko,Oe,Fw,eRo,Cne,oRo,rRo,Ua,tRo,Mne,aRo,sRo,Ene,nRo,lRo,yne,iRo,dRo,cRo,wne,AT,Ane,mRo,fRo,vj,gRo,hRo,uRo,LT,pRo,Lne,_Ro,bRo,Bne,vRo,TRo,xne,FRo,CRo,Cw,K9e,Fd,BT,kne,Mw,MRo,Rne,ERo,Z9e,ar,Ew,yRo,Cd,wRo,Sne,ARo,LRo,Pne,BRo,xRo,kRo,yw,RRo,$ne,SRo,PRo,$Ro,Yr,ww,IRo,Ine,DRo,jRo,Md,NRo,Dne,qRo,GRo,jne,ORo,XRo,VRo,Nne,zRo,WRo,Aw,QRo,Xe,Lw,HRo,qne,URo,JRo,Ja,YRo,Gne,KRo,ZRo,One,eSo,oSo,Xne,rSo,tSo,aSo,be,xT,Vne,sSo,nSo,Tj,lSo,iSo,dSo,kT,zne,cSo,mSo,Fj,fSo,gSo,hSo,Pn,Wne,uSo,pSo,Cj,_So,bSo,Mj,vSo,TSo,FSo,RT,Qne,CSo,MSo,Ej,ESo,ySo,wSo,la,Hne,ASo,LSo,yj,BSo,xSo,wj,kSo,RSo,Aj,SSo,PSo,$So,ST,Une,ISo,DSo,Lj,jSo,NSo,qSo,PT,Jne,GSo,OSo,Bj,XSo,VSo,zSo,$T,Yne,WSo,QSo,xj,HSo,USo,JSo,IT,Kne,YSo,KSo,kj,ZSo,ePo,oPo,DT,rPo,Zne,tPo,aPo,ele,sPo,nPo,ole,lPo,iPo,Bw,eBe,Ed,jT,rle,xw,dPo,tle,cPo,oBe,sr,kw,mPo,yd,fPo,ale,gPo,hPo,sle,uPo,pPo,_Po,Rw,bPo,nle,vPo,TPo,FPo,Kr,Sw,CPo,lle,MPo,EPo,wd,yPo,ile,wPo,APo,dle,LPo,BPo,xPo,cle,kPo,RPo,Pw,SPo,Ve,$w,PPo,mle,$Po,IPo,Ya,DPo,fle,jPo,NPo,gle,qPo,GPo,hle,OPo,XPo,VPo,ule,NT,ple,zPo,WPo,Rj,QPo,HPo,UPo,qT,JPo,_le,YPo,KPo,ble,ZPo,e$o,vle,o$o,r$o,Iw,rBe,Ad,GT,Tle,Dw,t$o,Fle,a$o,tBe,nr,jw,s$o,Ld,n$o,Cle,l$o,i$o,Mle,d$o,c$o,m$o,Nw,f$o,Ele,g$o,h$o,u$o,Zr,qw,p$o,yle,_$o,b$o,Bd,v$o,wle,T$o,F$o,Ale,C$o,M$o,E$o,Lle,y$o,w$o,Gw,A$o,ze,Ow,L$o,Ble,B$o,x$o,Ka,k$o,xle,R$o,S$o,kle,P$o,$$o,Rle,I$o,D$o,j$o,Ae,OT,Sle,N$o,q$o,Sj,G$o,O$o,X$o,XT,Ple,V$o,z$o,Pj,W$o,Q$o,H$o,VT,$le,U$o,J$o,$j,Y$o,K$o,Z$o,zT,Ile,eIo,oIo,Ij,rIo,tIo,aIo,WT,Dle,sIo,nIo,Dj,lIo,iIo,dIo,QT,jle,cIo,mIo,jj,fIo,gIo,hIo,HT,Nle,uIo,pIo,Nj,_Io,bIo,vIo,UT,qle,TIo,FIo,qj,CIo,MIo,EIo,JT,yIo,Gle,wIo,AIo,Ole,LIo,BIo,Xle,xIo,kIo,Xw,aBe,xd,YT,Vle,Vw,RIo,zle,SIo,sBe,lr,zw,PIo,kd,$Io,Wle,IIo,DIo,Qle,jIo,NIo,qIo,Ww,GIo,Hle,OIo,XIo,VIo,et,Qw,zIo,Ule,WIo,QIo,Rd,HIo,Jle,UIo,JIo,Yle,YIo,KIo,ZIo,Kle,eDo,oDo,Hw,rDo,We,Uw,tDo,Zle,aDo,sDo,Za,nDo,eie,lDo,iDo,oie,dDo,cDo,rie,mDo,fDo,gDo,es,KT,tie,hDo,uDo,Gj,pDo,_Do,bDo,ZT,aie,vDo,TDo,Oj,FDo,CDo,MDo,e1,sie,EDo,yDo,Xj,wDo,ADo,LDo,o1,nie,BDo,xDo,Vj,kDo,RDo,SDo,r1,PDo,lie,$Do,IDo,iie,DDo,jDo,die,NDo,qDo,Jw,nBe,Sd,t1,cie,Yw,GDo,mie,ODo,lBe,ir,Kw,XDo,Pd,VDo,fie,zDo,WDo,gie,QDo,HDo,UDo,Zw,JDo,hie,YDo,KDo,ZDo,ot,e6,ejo,uie,ojo,rjo,$d,tjo,pie,ajo,sjo,_ie,njo,ljo,ijo,bie,djo,cjo,o6,mjo,Qe,r6,fjo,vie,gjo,hjo,os,ujo,Tie,pjo,_jo,Fie,bjo,vjo,Cie,Tjo,Fjo,Cjo,Le,a1,Mie,Mjo,Ejo,zj,yjo,wjo,Ajo,s1,Eie,Ljo,Bjo,Wj,xjo,kjo,Rjo,n1,yie,Sjo,Pjo,Qj,$jo,Ijo,Djo,l1,wie,jjo,Njo,Hj,qjo,Gjo,Ojo,i1,Aie,Xjo,Vjo,Uj,zjo,Wjo,Qjo,d1,Lie,Hjo,Ujo,Jj,Jjo,Yjo,Kjo,c1,Bie,Zjo,eNo,Yj,oNo,rNo,tNo,m1,xie,aNo,sNo,Kj,nNo,lNo,iNo,f1,dNo,kie,cNo,mNo,Rie,fNo,gNo,Sie,hNo,uNo,t6,iBe,Id,g1,Pie,a6,pNo,$ie,_No,dBe,dr,s6,bNo,Dd,vNo,Iie,TNo,FNo,Die,CNo,MNo,ENo,n6,yNo,jie,wNo,ANo,LNo,rt,l6,BNo,Nie,xNo,kNo,jd,RNo,qie,SNo,PNo,Gie,$No,INo,DNo,Oie,jNo,NNo,i6,qNo,He,d6,GNo,Xie,ONo,XNo,rs,VNo,Vie,zNo,WNo,zie,QNo,HNo,Wie,UNo,JNo,YNo,c6,h1,Qie,KNo,ZNo,Zj,eqo,oqo,rqo,u1,Hie,tqo,aqo,eN,sqo,nqo,lqo,p1,iqo,Uie,dqo,cqo,Jie,mqo,fqo,Yie,gqo,hqo,m6,cBe,Nd,_1,Kie,f6,uqo,Zie,pqo,mBe,cr,g6,_qo,qd,bqo,ede,vqo,Tqo,ode,Fqo,Cqo,Mqo,h6,Eqo,rde,yqo,wqo,Aqo,tt,u6,Lqo,tde,Bqo,xqo,Gd,kqo,ade,Rqo,Sqo,sde,Pqo,$qo,Iqo,nde,Dqo,jqo,p6,Nqo,Ue,_6,qqo,lde,Gqo,Oqo,ts,Xqo,ide,Vqo,zqo,dde,Wqo,Qqo,cde,Hqo,Uqo,Jqo,as,b1,mde,Yqo,Kqo,oN,Zqo,eGo,oGo,v1,fde,rGo,tGo,rN,aGo,sGo,nGo,T1,gde,lGo,iGo,tN,dGo,cGo,mGo,F1,hde,fGo,gGo,aN,hGo,uGo,pGo,C1,_Go,ude,bGo,vGo,pde,TGo,FGo,_de,CGo,MGo,b6,fBe,Od,M1,bde,v6,EGo,vde,yGo,gBe,mr,T6,wGo,Xd,AGo,Tde,LGo,BGo,Fde,xGo,kGo,RGo,F6,SGo,Cde,PGo,$Go,IGo,at,C6,DGo,Mde,jGo,NGo,Vd,qGo,Ede,GGo,OGo,yde,XGo,VGo,zGo,wde,WGo,QGo,M6,HGo,Je,E6,UGo,Ade,JGo,YGo,ss,KGo,Lde,ZGo,eOo,Bde,oOo,rOo,xde,tOo,aOo,sOo,zd,E1,kde,nOo,lOo,sN,iOo,dOo,cOo,y1,Rde,mOo,fOo,nN,gOo,hOo,uOo,w1,Sde,pOo,_Oo,lN,bOo,vOo,TOo,A1,FOo,Pde,COo,MOo,$de,EOo,yOo,Ide,wOo,AOo,y6,hBe,Wd,L1,Dde,w6,LOo,jde,BOo,uBe,fr,A6,xOo,Qd,kOo,Nde,ROo,SOo,qde,POo,$Oo,IOo,L6,DOo,Gde,jOo,NOo,qOo,st,B6,GOo,Ode,OOo,XOo,Hd,VOo,Xde,zOo,WOo,Vde,QOo,HOo,UOo,zde,JOo,YOo,x6,KOo,Ye,k6,ZOo,Wde,eXo,oXo,ns,rXo,Qde,tXo,aXo,Hde,sXo,nXo,Ude,lXo,iXo,dXo,Jde,B1,Yde,cXo,mXo,iN,fXo,gXo,hXo,x1,uXo,Kde,pXo,_Xo,Zde,bXo,vXo,ece,TXo,FXo,R6,pBe,Ud,k1,oce,S6,CXo,rce,MXo,_Be,gr,P6,EXo,Jd,yXo,tce,wXo,AXo,ace,LXo,BXo,xXo,$6,kXo,sce,RXo,SXo,PXo,nt,I6,$Xo,nce,IXo,DXo,Yd,jXo,lce,NXo,qXo,ice,GXo,OXo,XXo,dce,VXo,zXo,D6,WXo,Ke,j6,QXo,cce,HXo,UXo,ls,JXo,mce,YXo,KXo,fce,ZXo,eVo,gce,oVo,rVo,tVo,hce,R1,uce,aVo,sVo,dN,nVo,lVo,iVo,S1,dVo,pce,cVo,mVo,_ce,fVo,gVo,bce,hVo,uVo,N6,bBe,Kd,P1,vce,q6,pVo,Tce,_Vo,vBe,hr,G6,bVo,Zd,vVo,Fce,TVo,FVo,Cce,CVo,MVo,EVo,O6,yVo,Mce,wVo,AVo,LVo,lt,X6,BVo,Ece,xVo,kVo,ec,RVo,yce,SVo,PVo,wce,$Vo,IVo,DVo,Ace,jVo,NVo,V6,qVo,Ze,z6,GVo,Lce,OVo,XVo,is,VVo,Bce,zVo,WVo,xce,QVo,HVo,kce,UVo,JVo,YVo,W6,$1,Rce,KVo,ZVo,cN,ezo,ozo,rzo,I1,Sce,tzo,azo,mN,szo,nzo,lzo,D1,izo,Pce,dzo,czo,$ce,mzo,fzo,Ice,gzo,hzo,Q6,TBe,oc,j1,Dce,H6,uzo,jce,pzo,FBe,ur,U6,_zo,rc,bzo,Nce,vzo,Tzo,qce,Fzo,Czo,Mzo,J6,Ezo,Gce,yzo,wzo,Azo,it,Y6,Lzo,Oce,Bzo,xzo,tc,kzo,Xce,Rzo,Szo,Vce,Pzo,$zo,Izo,zce,Dzo,jzo,K6,Nzo,go,Z6,qzo,Wce,Gzo,Ozo,ds,Xzo,Qce,Vzo,zzo,Hce,Wzo,Qzo,Uce,Hzo,Uzo,Jzo,B,N1,Jce,Yzo,Kzo,fN,Zzo,eWo,oWo,q1,Yce,rWo,tWo,gN,aWo,sWo,nWo,G1,Kce,lWo,iWo,hN,dWo,cWo,mWo,O1,Zce,fWo,gWo,uN,hWo,uWo,pWo,X1,eme,_Wo,bWo,pN,vWo,TWo,FWo,V1,ome,CWo,MWo,_N,EWo,yWo,wWo,z1,rme,AWo,LWo,bN,BWo,xWo,kWo,W1,tme,RWo,SWo,vN,PWo,$Wo,IWo,Q1,ame,DWo,jWo,TN,NWo,qWo,GWo,H1,sme,OWo,XWo,FN,VWo,zWo,WWo,U1,nme,QWo,HWo,CN,UWo,JWo,YWo,J1,lme,KWo,ZWo,MN,eQo,oQo,rQo,Y1,ime,tQo,aQo,EN,sQo,nQo,lQo,K1,dme,iQo,dQo,yN,cQo,mQo,fQo,Z1,cme,gQo,hQo,wN,uQo,pQo,_Qo,eF,mme,bQo,vQo,AN,TQo,FQo,CQo,$n,fme,MQo,EQo,LN,yQo,wQo,BN,AQo,LQo,BQo,oF,gme,xQo,kQo,xN,RQo,SQo,PQo,rF,hme,$Qo,IQo,kN,DQo,jQo,NQo,tF,ume,qQo,GQo,RN,OQo,XQo,VQo,aF,pme,zQo,WQo,SN,QQo,HQo,UQo,sF,_me,JQo,YQo,PN,KQo,ZQo,eHo,nF,bme,oHo,rHo,$N,tHo,aHo,sHo,lF,vme,nHo,lHo,IN,iHo,dHo,cHo,iF,Tme,mHo,fHo,DN,gHo,hHo,uHo,dF,Fme,pHo,_Ho,jN,bHo,vHo,THo,cF,Cme,FHo,CHo,NN,MHo,EHo,yHo,mF,Mme,wHo,AHo,qN,LHo,BHo,xHo,fF,Eme,kHo,RHo,GN,SHo,PHo,$Ho,gF,yme,IHo,DHo,ON,jHo,NHo,qHo,hF,wme,GHo,OHo,XN,XHo,VHo,zHo,uF,Ame,WHo,QHo,VN,HHo,UHo,JHo,pF,Lme,YHo,KHo,zN,ZHo,eUo,oUo,_F,Bme,rUo,tUo,WN,aUo,sUo,nUo,bF,xme,lUo,iUo,QN,dUo,cUo,mUo,vF,kme,fUo,gUo,HN,hUo,uUo,pUo,TF,Rme,_Uo,bUo,UN,vUo,TUo,FUo,FF,Sme,CUo,MUo,JN,EUo,yUo,wUo,CF,Pme,AUo,LUo,YN,BUo,xUo,kUo,MF,$me,RUo,SUo,KN,PUo,$Uo,IUo,EF,Ime,DUo,jUo,ZN,NUo,qUo,GUo,yF,Dme,OUo,XUo,eq,VUo,zUo,WUo,jme,QUo,HUo,eA,CBe,ac,wF,Nme,oA,UUo,qme,JUo,MBe,pr,rA,YUo,sc,KUo,Gme,ZUo,eJo,Ome,oJo,rJo,tJo,tA,aJo,Xme,sJo,nJo,lJo,dt,aA,iJo,Vme,dJo,cJo,nc,mJo,zme,fJo,gJo,Wme,hJo,uJo,pJo,Qme,_Jo,bJo,sA,vJo,ho,nA,TJo,Hme,FJo,CJo,cs,MJo,Ume,EJo,yJo,Jme,wJo,AJo,Yme,LJo,BJo,xJo,H,AF,Kme,kJo,RJo,oq,SJo,PJo,$Jo,LF,Zme,IJo,DJo,rq,jJo,NJo,qJo,BF,efe,GJo,OJo,tq,XJo,VJo,zJo,xF,ofe,WJo,QJo,aq,HJo,UJo,JJo,kF,rfe,YJo,KJo,sq,ZJo,eYo,oYo,RF,tfe,rYo,tYo,nq,aYo,sYo,nYo,SF,afe,lYo,iYo,lq,dYo,cYo,mYo,PF,sfe,fYo,gYo,iq,hYo,uYo,pYo,$F,nfe,_Yo,bYo,dq,vYo,TYo,FYo,IF,lfe,CYo,MYo,cq,EYo,yYo,wYo,DF,ife,AYo,LYo,mq,BYo,xYo,kYo,jF,dfe,RYo,SYo,fq,PYo,$Yo,IYo,NF,cfe,DYo,jYo,gq,NYo,qYo,GYo,qF,mfe,OYo,XYo,hq,VYo,zYo,WYo,GF,ffe,QYo,HYo,uq,UYo,JYo,YYo,OF,gfe,KYo,ZYo,pq,eKo,oKo,rKo,XF,hfe,tKo,aKo,_q,sKo,nKo,lKo,VF,ufe,iKo,dKo,bq,cKo,mKo,fKo,zF,pfe,gKo,hKo,vq,uKo,pKo,_Ko,WF,_fe,bKo,vKo,Tq,TKo,FKo,CKo,QF,bfe,MKo,EKo,Fq,yKo,wKo,AKo,HF,vfe,LKo,BKo,Cq,xKo,kKo,RKo,Tfe,SKo,PKo,lA,EBe,lc,UF,Ffe,iA,$Ko,Cfe,IKo,yBe,_r,dA,DKo,ic,jKo,Mfe,NKo,qKo,Efe,GKo,OKo,XKo,cA,VKo,yfe,zKo,WKo,QKo,ct,mA,HKo,wfe,UKo,JKo,dc,YKo,Afe,KKo,ZKo,Lfe,eZo,oZo,rZo,Bfe,tZo,aZo,fA,sZo,uo,gA,nZo,xfe,lZo,iZo,ms,dZo,kfe,cZo,mZo,Rfe,fZo,gZo,Sfe,hZo,uZo,pZo,he,JF,Pfe,_Zo,bZo,Mq,vZo,TZo,FZo,YF,$fe,CZo,MZo,Eq,EZo,yZo,wZo,KF,Ife,AZo,LZo,yq,BZo,xZo,kZo,ZF,Dfe,RZo,SZo,wq,PZo,$Zo,IZo,eC,jfe,DZo,jZo,Aq,NZo,qZo,GZo,oC,Nfe,OZo,XZo,Lq,VZo,zZo,WZo,rC,qfe,QZo,HZo,Bq,UZo,JZo,YZo,tC,Gfe,KZo,ZZo,xq,eer,oer,rer,aC,Ofe,ter,aer,kq,ser,ner,ler,sC,Xfe,ier,der,Rq,cer,mer,fer,Vfe,ger,her,hA,wBe,cc,nC,zfe,uA,uer,Wfe,per,ABe,br,pA,_er,mc,ber,Qfe,ver,Ter,Hfe,Fer,Cer,Mer,_A,Eer,Ufe,yer,wer,Aer,mt,bA,Ler,Jfe,Ber,xer,fc,ker,Yfe,Rer,Ser,Kfe,Per,$er,Ier,Zfe,Der,jer,vA,Ner,po,TA,qer,ege,Ger,Oer,fs,Xer,oge,Ver,zer,rge,Wer,Qer,tge,Her,Uer,Jer,FA,lC,age,Yer,Ker,Sq,Zer,eor,oor,iC,sge,ror,tor,Pq,aor,sor,nor,nge,lor,ior,CA,LBe,gc,dC,lge,MA,dor,ige,cor,BBe,vr,EA,mor,hc,gor,dge,hor,uor,cge,por,_or,bor,yA,vor,mge,Tor,For,Cor,ft,wA,Mor,fge,Eor,yor,uc,wor,gge,Aor,Lor,hge,Bor,xor,kor,uge,Ror,Sor,AA,Por,_o,LA,$or,pge,Ior,Dor,gs,jor,_ge,Nor,qor,bge,Gor,Oor,vge,Xor,Vor,zor,Y,cC,Tge,Wor,Qor,$q,Hor,Uor,Jor,mC,Fge,Yor,Kor,Iq,Zor,err,orr,fC,Cge,rrr,trr,Dq,arr,srr,nrr,gC,Mge,lrr,irr,jq,drr,crr,mrr,hC,Ege,frr,grr,Nq,hrr,urr,prr,uC,yge,_rr,brr,qq,vrr,Trr,Frr,pC,wge,Crr,Mrr,Gq,Err,yrr,wrr,_C,Age,Arr,Lrr,Oq,Brr,xrr,krr,bC,Lge,Rrr,Srr,Xq,Prr,$rr,Irr,vC,Bge,Drr,jrr,Vq,Nrr,qrr,Grr,TC,xge,Orr,Xrr,zq,Vrr,zrr,Wrr,FC,kge,Qrr,Hrr,Wq,Urr,Jrr,Yrr,CC,Rge,Krr,Zrr,Qq,etr,otr,rtr,MC,Sge,ttr,atr,Hq,str,ntr,ltr,EC,Pge,itr,dtr,Uq,ctr,mtr,ftr,yC,$ge,gtr,htr,Jq,utr,ptr,_tr,wC,Ige,btr,vtr,Yq,Ttr,Ftr,Ctr,AC,Dge,Mtr,Etr,Kq,ytr,wtr,Atr,LC,jge,Ltr,Btr,Zq,xtr,ktr,Rtr,BC,Nge,Str,Ptr,eG,$tr,Itr,Dtr,qge,jtr,Ntr,BA,xBe,pc,xC,Gge,xA,qtr,Oge,Gtr,kBe,Tr,kA,Otr,_c,Xtr,Xge,Vtr,ztr,Vge,Wtr,Qtr,Htr,RA,Utr,zge,Jtr,Ytr,Ktr,gt,SA,Ztr,Wge,ear,oar,bc,rar,Qge,tar,aar,Hge,sar,nar,lar,Uge,iar,dar,PA,car,bo,$A,mar,Jge,far,gar,hs,har,Yge,uar,par,Kge,_ar,bar,Zge,Tar,Far,Car,ue,kC,ehe,Mar,Ear,oG,yar,war,Aar,RC,ohe,Lar,Bar,rG,xar,kar,Rar,SC,rhe,Sar,Par,tG,$ar,Iar,Dar,PC,the,jar,Nar,aG,qar,Gar,Oar,$C,ahe,Xar,Var,sG,zar,War,Qar,IC,she,Har,Uar,nG,Jar,Yar,Kar,DC,nhe,Zar,esr,lG,osr,rsr,tsr,jC,lhe,asr,ssr,iG,nsr,lsr,isr,NC,ihe,dsr,csr,dG,msr,fsr,gsr,qC,dhe,hsr,usr,cG,psr,_sr,bsr,che,vsr,Tsr,IA,RBe,vc,GC,mhe,DA,Fsr,fhe,Csr,SBe,Fr,jA,Msr,Tc,Esr,ghe,ysr,wsr,hhe,Asr,Lsr,Bsr,NA,xsr,uhe,ksr,Rsr,Ssr,ht,qA,Psr,phe,$sr,Isr,Fc,Dsr,_he,jsr,Nsr,bhe,qsr,Gsr,Osr,vhe,Xsr,Vsr,GA,zsr,vo,OA,Wsr,The,Qsr,Hsr,us,Usr,Fhe,Jsr,Ysr,Che,Ksr,Zsr,Mhe,enr,onr,rnr,X,OC,Ehe,tnr,anr,mG,snr,nnr,lnr,XC,yhe,inr,dnr,fG,cnr,mnr,fnr,VC,whe,gnr,hnr,gG,unr,pnr,_nr,zC,Ahe,bnr,vnr,hG,Tnr,Fnr,Cnr,WC,Lhe,Mnr,Enr,uG,ynr,wnr,Anr,QC,Bhe,Lnr,Bnr,pG,xnr,knr,Rnr,HC,xhe,Snr,Pnr,_G,$nr,Inr,Dnr,UC,khe,jnr,Nnr,bG,qnr,Gnr,Onr,JC,Rhe,Xnr,Vnr,vG,znr,Wnr,Qnr,YC,She,Hnr,Unr,TG,Jnr,Ynr,Knr,KC,Phe,Znr,elr,FG,olr,rlr,tlr,ZC,$he,alr,slr,CG,nlr,llr,ilr,e4,Ihe,dlr,clr,MG,mlr,flr,glr,o4,Dhe,hlr,ulr,EG,plr,_lr,blr,r4,jhe,vlr,Tlr,yG,Flr,Clr,Mlr,t4,Nhe,Elr,ylr,wG,wlr,Alr,Llr,a4,qhe,Blr,xlr,AG,klr,Rlr,Slr,s4,Ghe,Plr,$lr,LG,Ilr,Dlr,jlr,n4,Ohe,Nlr,qlr,BG,Glr,Olr,Xlr,l4,Xhe,Vlr,zlr,xG,Wlr,Qlr,Hlr,i4,Vhe,Ulr,Jlr,kG,Ylr,Klr,Zlr,d4,zhe,eir,oir,RG,rir,tir,air,c4,Whe,sir,nir,SG,lir,iir,dir,m4,Qhe,cir,mir,PG,fir,gir,hir,f4,Hhe,uir,pir,$G,_ir,bir,vir,Uhe,Tir,Fir,XA,PBe,Cc,g4,Jhe,VA,Cir,Yhe,Mir,$Be,Cr,zA,Eir,Mc,yir,Khe,wir,Air,Zhe,Lir,Bir,xir,WA,kir,eue,Rir,Sir,Pir,ut,QA,$ir,oue,Iir,Dir,Ec,jir,rue,Nir,qir,tue,Gir,Oir,Xir,aue,Vir,zir,HA,Wir,To,UA,Qir,sue,Hir,Uir,ps,Jir,nue,Yir,Kir,lue,Zir,edr,iue,odr,rdr,tdr,te,h4,due,adr,sdr,IG,ndr,ldr,idr,u4,cue,ddr,cdr,DG,mdr,fdr,gdr,p4,mue,hdr,udr,jG,pdr,_dr,bdr,_4,fue,vdr,Tdr,NG,Fdr,Cdr,Mdr,b4,gue,Edr,ydr,qG,wdr,Adr,Ldr,v4,hue,Bdr,xdr,GG,kdr,Rdr,Sdr,T4,uue,Pdr,$dr,OG,Idr,Ddr,jdr,F4,pue,Ndr,qdr,XG,Gdr,Odr,Xdr,C4,_ue,Vdr,zdr,VG,Wdr,Qdr,Hdr,M4,bue,Udr,Jdr,zG,Ydr,Kdr,Zdr,E4,vue,ecr,ocr,WG,rcr,tcr,acr,y4,Tue,scr,ncr,QG,lcr,icr,dcr,w4,Fue,ccr,mcr,HG,fcr,gcr,hcr,A4,Cue,ucr,pcr,UG,_cr,bcr,vcr,L4,Mue,Tcr,Fcr,JG,Ccr,Mcr,Ecr,B4,Eue,ycr,wcr,YG,Acr,Lcr,Bcr,x4,yue,xcr,kcr,KG,Rcr,Scr,Pcr,wue,$cr,Icr,JA,IBe,yc,k4,Aue,YA,Dcr,Lue,jcr,DBe,Mr,KA,Ncr,wc,qcr,Bue,Gcr,Ocr,xue,Xcr,Vcr,zcr,ZA,Wcr,kue,Qcr,Hcr,Ucr,pt,e0,Jcr,Rue,Ycr,Kcr,Ac,Zcr,Sue,emr,omr,Pue,rmr,tmr,amr,$ue,smr,nmr,o0,lmr,Fo,r0,imr,Iue,dmr,cmr,_s,mmr,Due,fmr,gmr,jue,hmr,umr,Nue,pmr,_mr,bmr,que,R4,Gue,vmr,Tmr,ZG,Fmr,Cmr,Mmr,Oue,Emr,ymr,t0,jBe,Lc,S4,Xue,a0,wmr,Vue,Amr,NBe,Er,s0,Lmr,Bc,Bmr,zue,xmr,kmr,Wue,Rmr,Smr,Pmr,n0,$mr,Que,Imr,Dmr,jmr,_t,l0,Nmr,Hue,qmr,Gmr,xc,Omr,Uue,Xmr,Vmr,Jue,zmr,Wmr,Qmr,Yue,Hmr,Umr,i0,Jmr,Co,d0,Ymr,Kue,Kmr,Zmr,bs,efr,Zue,ofr,rfr,epe,tfr,afr,ope,sfr,nfr,lfr,K,P4,rpe,ifr,dfr,eO,cfr,mfr,ffr,$4,tpe,gfr,hfr,oO,ufr,pfr,_fr,I4,ape,bfr,vfr,rO,Tfr,Ffr,Cfr,D4,spe,Mfr,Efr,tO,yfr,wfr,Afr,j4,npe,Lfr,Bfr,aO,xfr,kfr,Rfr,N4,lpe,Sfr,Pfr,sO,$fr,Ifr,Dfr,q4,ipe,jfr,Nfr,nO,qfr,Gfr,Ofr,G4,dpe,Xfr,Vfr,lO,zfr,Wfr,Qfr,O4,cpe,Hfr,Ufr,iO,Jfr,Yfr,Kfr,X4,mpe,Zfr,egr,dO,ogr,rgr,tgr,V4,fpe,agr,sgr,cO,ngr,lgr,igr,z4,gpe,dgr,cgr,mO,mgr,fgr,ggr,W4,hpe,hgr,ugr,fO,pgr,_gr,bgr,Q4,upe,vgr,Tgr,gO,Fgr,Cgr,Mgr,H4,ppe,Egr,ygr,hO,wgr,Agr,Lgr,U4,_pe,Bgr,xgr,uO,kgr,Rgr,Sgr,J4,bpe,Pgr,$gr,pO,Igr,Dgr,jgr,Y4,vpe,Ngr,qgr,_O,Ggr,Ogr,Xgr,K4,Tpe,Vgr,zgr,bO,Wgr,Qgr,Hgr,Z4,Fpe,Ugr,Jgr,vO,Ygr,Kgr,Zgr,Cpe,ehr,ohr,c0,qBe,kc,eM,Mpe,m0,rhr,Epe,thr,GBe,yr,f0,ahr,Rc,shr,ype,nhr,lhr,wpe,ihr,dhr,chr,g0,mhr,Ape,fhr,ghr,hhr,bt,h0,uhr,Lpe,phr,_hr,Sc,bhr,Bpe,vhr,Thr,xpe,Fhr,Chr,Mhr,kpe,Ehr,yhr,u0,whr,Mo,p0,Ahr,Rpe,Lhr,Bhr,vs,xhr,Spe,khr,Rhr,Ppe,Shr,Phr,$pe,$hr,Ihr,Dhr,Z,oM,Ipe,jhr,Nhr,TO,qhr,Ghr,Ohr,rM,Dpe,Xhr,Vhr,FO,zhr,Whr,Qhr,tM,jpe,Hhr,Uhr,CO,Jhr,Yhr,Khr,aM,Npe,Zhr,eur,MO,our,rur,tur,sM,qpe,aur,sur,EO,nur,lur,iur,nM,Gpe,dur,cur,yO,mur,fur,gur,lM,Ope,hur,uur,wO,pur,_ur,bur,iM,Xpe,vur,Tur,AO,Fur,Cur,Mur,dM,Vpe,Eur,yur,LO,wur,Aur,Lur,cM,zpe,Bur,xur,BO,kur,Rur,Sur,mM,Wpe,Pur,$ur,xO,Iur,Dur,jur,fM,Qpe,Nur,qur,kO,Gur,Our,Xur,gM,Hpe,Vur,zur,RO,Wur,Qur,Hur,hM,Upe,Uur,Jur,SO,Yur,Kur,Zur,uM,Jpe,epr,opr,PO,rpr,tpr,apr,pM,Ype,spr,npr,$O,lpr,ipr,dpr,_M,Kpe,cpr,mpr,IO,fpr,gpr,hpr,bM,Zpe,upr,ppr,DO,_pr,bpr,vpr,vM,e_e,Tpr,Fpr,jO,Cpr,Mpr,Epr,o_e,ypr,wpr,_0,OBe,Pc,TM,r_e,b0,Apr,t_e,Lpr,XBe,wr,v0,Bpr,$c,xpr,a_e,kpr,Rpr,s_e,Spr,Ppr,$pr,T0,Ipr,n_e,Dpr,jpr,Npr,vt,F0,qpr,l_e,Gpr,Opr,Ic,Xpr,i_e,Vpr,zpr,d_e,Wpr,Qpr,Hpr,c_e,Upr,Jpr,C0,Ypr,Eo,M0,Kpr,m_e,Zpr,e_r,Ts,o_r,f_e,r_r,t_r,g_e,a_r,s_r,h_e,n_r,l_r,i_r,u_e,FM,p_e,d_r,c_r,NO,m_r,f_r,g_r,__e,h_r,u_r,E0,VBe,Dc,CM,b_e,y0,p_r,v_e,__r,zBe,Ar,w0,b_r,jc,v_r,T_e,T_r,F_r,F_e,C_r,M_r,E_r,A0,y_r,C_e,w_r,A_r,L_r,Tt,L0,B_r,M_e,x_r,k_r,Nc,R_r,E_e,S_r,P_r,y_e,$_r,I_r,D_r,w_e,j_r,N_r,B0,q_r,yo,x0,G_r,A_e,O_r,X_r,Fs,V_r,L_e,z_r,W_r,B_e,Q_r,H_r,x_e,U_r,J_r,Y_r,k_e,MM,R_e,K_r,Z_r,qO,ebr,obr,rbr,S_e,tbr,abr,k0,WBe,qc,EM,P_e,R0,sbr,$_e,nbr,QBe,Lr,S0,lbr,Gc,ibr,I_e,dbr,cbr,D_e,mbr,fbr,gbr,P0,hbr,j_e,ubr,pbr,_br,Ft,$0,bbr,N_e,vbr,Tbr,Oc,Fbr,q_e,Cbr,Mbr,G_e,Ebr,ybr,wbr,O_e,Abr,Lbr,I0,Bbr,wo,D0,xbr,X_e,kbr,Rbr,Cs,Sbr,V_e,Pbr,$br,z_e,Ibr,Dbr,W_e,jbr,Nbr,qbr,z,yM,Q_e,Gbr,Obr,GO,Xbr,Vbr,zbr,wM,H_e,Wbr,Qbr,OO,Hbr,Ubr,Jbr,AM,U_e,Ybr,Kbr,XO,Zbr,e2r,o2r,LM,J_e,r2r,t2r,VO,a2r,s2r,n2r,BM,Y_e,l2r,i2r,zO,d2r,c2r,m2r,xM,K_e,f2r,g2r,WO,h2r,u2r,p2r,kM,Z_e,_2r,b2r,QO,v2r,T2r,F2r,RM,ebe,C2r,M2r,HO,E2r,y2r,w2r,SM,obe,A2r,L2r,UO,B2r,x2r,k2r,PM,rbe,R2r,S2r,JO,P2r,$2r,I2r,$M,tbe,D2r,j2r,YO,N2r,q2r,G2r,IM,abe,O2r,X2r,KO,V2r,z2r,W2r,DM,sbe,Q2r,H2r,ZO,U2r,J2r,Y2r,jM,nbe,K2r,Z2r,eX,evr,ovr,rvr,NM,lbe,tvr,avr,oX,svr,nvr,lvr,qM,ibe,ivr,dvr,rX,cvr,mvr,fvr,GM,dbe,gvr,hvr,tX,uvr,pvr,_vr,OM,cbe,bvr,vvr,aX,Tvr,Fvr,Cvr,XM,mbe,Mvr,Evr,sX,yvr,wvr,Avr,VM,fbe,Lvr,Bvr,nX,xvr,kvr,Rvr,zM,gbe,Svr,Pvr,lX,$vr,Ivr,Dvr,WM,hbe,jvr,Nvr,iX,qvr,Gvr,Ovr,QM,ube,Xvr,Vvr,dX,zvr,Wvr,Qvr,HM,pbe,Hvr,Uvr,cX,Jvr,Yvr,Kvr,_be,Zvr,eTr,j0,HBe,Xc,UM,bbe,N0,oTr,vbe,rTr,UBe,Br,q0,tTr,Vc,aTr,Tbe,sTr,nTr,Fbe,lTr,iTr,dTr,G0,cTr,Cbe,mTr,fTr,gTr,Ct,O0,hTr,Mbe,uTr,pTr,zc,_Tr,Ebe,bTr,vTr,ybe,TTr,FTr,CTr,wbe,MTr,ETr,X0,yTr,Ao,V0,wTr,Abe,ATr,LTr,Ms,BTr,Lbe,xTr,kTr,Bbe,RTr,STr,xbe,PTr,$Tr,ITr,Es,JM,kbe,DTr,jTr,mX,NTr,qTr,GTr,YM,Rbe,OTr,XTr,fX,VTr,zTr,WTr,KM,Sbe,QTr,HTr,gX,UTr,JTr,YTr,ZM,Pbe,KTr,ZTr,hX,e1r,o1r,r1r,$be,t1r,a1r,z0,JBe,Wc,eE,Ibe,W0,s1r,Dbe,n1r,YBe,xr,Q0,l1r,Qc,i1r,jbe,d1r,c1r,Nbe,m1r,f1r,g1r,H0,h1r,qbe,u1r,p1r,_1r,Mt,U0,b1r,Gbe,v1r,T1r,Hc,F1r,Obe,C1r,M1r,Xbe,E1r,y1r,w1r,Vbe,A1r,L1r,J0,B1r,Lo,Y0,x1r,zbe,k1r,R1r,ys,S1r,Wbe,P1r,$1r,Qbe,I1r,D1r,Hbe,j1r,N1r,q1r,me,oE,Ube,G1r,O1r,uX,X1r,V1r,z1r,rE,Jbe,W1r,Q1r,pX,H1r,U1r,J1r,tE,Ybe,Y1r,K1r,_X,Z1r,eFr,oFr,aE,Kbe,rFr,tFr,bX,aFr,sFr,nFr,sE,Zbe,lFr,iFr,vX,dFr,cFr,mFr,nE,e2e,fFr,gFr,TX,hFr,uFr,pFr,lE,o2e,_Fr,bFr,FX,vFr,TFr,FFr,iE,r2e,CFr,MFr,CX,EFr,yFr,wFr,dE,t2e,AFr,LFr,MX,BFr,xFr,kFr,cE,a2e,RFr,SFr,EX,PFr,$Fr,IFr,mE,s2e,DFr,jFr,yX,NFr,qFr,GFr,n2e,OFr,XFr,K0,KBe,Uc,fE,l2e,Z0,VFr,i2e,zFr,ZBe,kr,eL,WFr,Jc,QFr,d2e,HFr,UFr,c2e,JFr,YFr,KFr,oL,ZFr,m2e,eCr,oCr,rCr,Et,rL,tCr,f2e,aCr,sCr,Yc,nCr,g2e,lCr,iCr,h2e,dCr,cCr,mCr,u2e,fCr,gCr,tL,hCr,Bo,aL,uCr,p2e,pCr,_Cr,ws,bCr,_2e,vCr,TCr,b2e,FCr,CCr,v2e,MCr,ECr,yCr,ve,gE,T2e,wCr,ACr,wX,LCr,BCr,xCr,hE,F2e,kCr,RCr,AX,SCr,PCr,$Cr,uE,C2e,ICr,DCr,LX,jCr,NCr,qCr,pE,M2e,GCr,OCr,BX,XCr,VCr,zCr,_E,E2e,WCr,QCr,xX,HCr,UCr,JCr,bE,y2e,YCr,KCr,kX,ZCr,e4r,o4r,vE,w2e,r4r,t4r,RX,a4r,s4r,n4r,TE,A2e,l4r,i4r,SX,d4r,c4r,m4r,FE,L2e,f4r,g4r,PX,h4r,u4r,p4r,B2e,_4r,b4r,sL,exe,Kc,CE,x2e,nL,v4r,k2e,T4r,oxe,Rr,lL,F4r,Zc,C4r,R2e,M4r,E4r,S2e,y4r,w4r,A4r,iL,L4r,P2e,B4r,x4r,k4r,yt,dL,R4r,$2e,S4r,P4r,em,$4r,I2e,I4r,D4r,D2e,j4r,N4r,q4r,j2e,G4r,O4r,cL,X4r,xo,mL,V4r,N2e,z4r,W4r,As,Q4r,q2e,H4r,U4r,G2e,J4r,Y4r,O2e,K4r,Z4r,eMr,Te,ME,X2e,oMr,rMr,$X,tMr,aMr,sMr,EE,V2e,nMr,lMr,IX,iMr,dMr,cMr,yE,z2e,mMr,fMr,DX,gMr,hMr,uMr,wE,W2e,pMr,_Mr,jX,bMr,vMr,TMr,AE,Q2e,FMr,CMr,NX,MMr,EMr,yMr,LE,H2e,wMr,AMr,qX,LMr,BMr,xMr,BE,U2e,kMr,RMr,GX,SMr,PMr,$Mr,xE,J2e,IMr,DMr,OX,jMr,NMr,qMr,kE,Y2e,GMr,OMr,XX,XMr,VMr,zMr,K2e,WMr,QMr,fL,rxe,om,RE,Z2e,gL,HMr,eve,UMr,txe,Sr,hL,JMr,rm,YMr,ove,KMr,ZMr,rve,eEr,oEr,rEr,uL,tEr,tve,aEr,sEr,nEr,wt,pL,lEr,ave,iEr,dEr,tm,cEr,sve,mEr,fEr,nve,gEr,hEr,uEr,lve,pEr,_Er,_L,bEr,ko,bL,vEr,ive,TEr,FEr,Ls,CEr,dve,MEr,EEr,cve,yEr,wEr,mve,AEr,LEr,BEr,Fe,SE,fve,xEr,kEr,VX,REr,SEr,PEr,PE,gve,$Er,IEr,zX,DEr,jEr,NEr,$E,hve,qEr,GEr,WX,OEr,XEr,VEr,IE,uve,zEr,WEr,QX,QEr,HEr,UEr,DE,pve,JEr,YEr,HX,KEr,ZEr,e3r,jE,_ve,o3r,r3r,UX,t3r,a3r,s3r,NE,bve,n3r,l3r,JX,i3r,d3r,c3r,qE,vve,m3r,f3r,YX,g3r,h3r,u3r,GE,Tve,p3r,_3r,KX,b3r,v3r,T3r,Fve,F3r,C3r,vL,axe,am,OE,Cve,TL,M3r,Mve,E3r,sxe,Pr,FL,y3r,sm,w3r,Eve,A3r,L3r,yve,B3r,x3r,k3r,CL,R3r,wve,S3r,P3r,$3r,At,ML,I3r,Ave,D3r,j3r,nm,N3r,Lve,q3r,G3r,Bve,O3r,X3r,V3r,xve,z3r,W3r,EL,Q3r,Ro,yL,H3r,kve,U3r,J3r,Bs,Y3r,Rve,K3r,Z3r,Sve,e5r,o5r,Pve,r5r,t5r,a5r,Ce,XE,$ve,s5r,n5r,ZX,l5r,i5r,d5r,VE,Ive,c5r,m5r,eV,f5r,g5r,h5r,zE,Dve,u5r,p5r,oV,_5r,b5r,v5r,WE,jve,T5r,F5r,rV,C5r,M5r,E5r,QE,Nve,y5r,w5r,tV,A5r,L5r,B5r,HE,qve,x5r,k5r,aV,R5r,S5r,P5r,UE,Gve,$5r,I5r,sV,D5r,j5r,N5r,JE,Ove,q5r,G5r,nV,O5r,X5r,V5r,YE,Xve,z5r,W5r,lV,Q5r,H5r,U5r,Vve,J5r,Y5r,wL,nxe,lm,KE,zve,AL,K5r,Wve,Z5r,lxe,$r,LL,eyr,im,oyr,Qve,ryr,tyr,Hve,ayr,syr,nyr,BL,lyr,Uve,iyr,dyr,cyr,Lt,xL,myr,Jve,fyr,gyr,dm,hyr,Yve,uyr,pyr,Kve,_yr,byr,vyr,Zve,Tyr,Fyr,kL,Cyr,So,RL,Myr,eTe,Eyr,yyr,xs,wyr,oTe,Ayr,Lyr,rTe,Byr,xyr,tTe,kyr,Ryr,Syr,no,ZE,aTe,Pyr,$yr,iV,Iyr,Dyr,jyr,e3,sTe,Nyr,qyr,dV,Gyr,Oyr,Xyr,o3,nTe,Vyr,zyr,cV,Wyr,Qyr,Hyr,r3,lTe,Uyr,Jyr,mV,Yyr,Kyr,Zyr,t3,iTe,ewr,owr,fV,rwr,twr,awr,a3,dTe,swr,nwr,gV,lwr,iwr,dwr,s3,cTe,cwr,mwr,hV,fwr,gwr,hwr,mTe,uwr,pwr,SL,ixe,cm,n3,fTe,PL,_wr,gTe,bwr,dxe,Ir,$L,vwr,mm,Twr,hTe,Fwr,Cwr,uTe,Mwr,Ewr,ywr,IL,wwr,pTe,Awr,Lwr,Bwr,Bt,DL,xwr,_Te,kwr,Rwr,fm,Swr,bTe,Pwr,$wr,vTe,Iwr,Dwr,jwr,TTe,Nwr,qwr,jL,Gwr,Po,NL,Owr,FTe,Xwr,Vwr,ks,zwr,CTe,Wwr,Qwr,MTe,Hwr,Uwr,ETe,Jwr,Ywr,Kwr,lo,l3,yTe,Zwr,e6r,uV,o6r,r6r,t6r,i3,wTe,a6r,s6r,pV,n6r,l6r,i6r,d3,ATe,d6r,c6r,_V,m6r,f6r,g6r,c3,LTe,h6r,u6r,bV,p6r,_6r,b6r,m3,BTe,v6r,T6r,vV,F6r,C6r,M6r,f3,xTe,E6r,y6r,TV,w6r,A6r,L6r,g3,kTe,B6r,x6r,FV,k6r,R6r,S6r,RTe,P6r,$6r,qL,cxe,gm,h3,STe,GL,I6r,PTe,D6r,mxe,Dr,OL,j6r,hm,N6r,$Te,q6r,G6r,ITe,O6r,X6r,V6r,XL,z6r,DTe,W6r,Q6r,H6r,xt,VL,U6r,jTe,J6r,Y6r,um,K6r,NTe,Z6r,eAr,qTe,oAr,rAr,tAr,GTe,aAr,sAr,zL,nAr,$o,WL,lAr,OTe,iAr,dAr,Rs,cAr,XTe,mAr,fAr,VTe,gAr,hAr,zTe,uAr,pAr,_Ar,WTe,u3,QTe,bAr,vAr,CV,TAr,FAr,CAr,HTe,MAr,EAr,QL,fxe,pm,p3,UTe,HL,yAr,JTe,wAr,gxe,jr,UL,AAr,_m,LAr,YTe,BAr,xAr,KTe,kAr,RAr,SAr,JL,PAr,ZTe,$Ar,IAr,DAr,kt,YL,jAr,e1e,NAr,qAr,bm,GAr,o1e,OAr,XAr,r1e,VAr,zAr,WAr,t1e,QAr,HAr,KL,UAr,Io,ZL,JAr,a1e,YAr,KAr,Ss,ZAr,s1e,e0r,o0r,n1e,r0r,t0r,l1e,a0r,s0r,n0r,e8,_3,i1e,l0r,i0r,MV,d0r,c0r,m0r,b3,d1e,f0r,g0r,EV,h0r,u0r,p0r,c1e,_0r,b0r,o8,hxe,vm,v3,m1e,r8,v0r,f1e,T0r,uxe,Nr,t8,F0r,Tm,C0r,g1e,M0r,E0r,h1e,y0r,w0r,A0r,a8,L0r,u1e,B0r,x0r,k0r,Rt,s8,R0r,p1e,S0r,P0r,Fm,$0r,_1e,I0r,D0r,b1e,j0r,N0r,q0r,v1e,G0r,O0r,n8,X0r,Do,l8,V0r,T1e,z0r,W0r,Ps,Q0r,F1e,H0r,U0r,C1e,J0r,Y0r,M1e,K0r,Z0r,eLr,E1e,T3,y1e,oLr,rLr,yV,tLr,aLr,sLr,w1e,nLr,lLr,i8,pxe;return ce=new V({}),$a=new w({props:{code:'model = AutoModel.from_pretrained("bert-base-cased"),',highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)'}}),E5=new V({}),y5=new w({props:{code:`from transformers import AutoConfig, AutoModel

AutoConfig.register("new-model", NewModelConfig)
AutoModel.register(NewModelConfig, NewModel),`,highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`}}),Lm=new iLr({props:{warning:"&lcub;true}",$$slots:{default:[W2t]},$$scope:{ctx:Ai}}}),w5=new V({}),A5=new E({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L523"}}),x5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L546",parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}]}}),k5=new w({props:{code:`from transformers import AutoConfig

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-uncased")

# Download configuration from huggingface.co (user-uploaded) and cache.
config = AutoConfig.from_pretrained("dbmdz/bert-base-german-cased")

# If configuration file is in a directory (e.g., was saved using *save_pretrained('./test/saved_model/')*).
config = AutoConfig.from_pretrained("./test/bert_saved_model/")

# Load a specific configuration file.
config = AutoConfig.from_pretrained("./test/bert_saved_model/my_configuration.json")

# Change some config attributes when loading a pretrained config.
config = AutoConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
config.output_attentions

config, unused_kwargs = AutoConfig.from_pretrained(
    "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
)
config.output_attentions

config.unused_kwargs,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config.unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`}}),R5=new E({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/configuration_auto.py#L668",parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}]}}),S5=new V({}),P5=new E({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L351"}}),D5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L365",parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to dertermine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to try to load the fast version of the tokenizer.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}]}}),j5=new w({props:{code:`from transformers import AutoTokenizer

# Download vocabulary from huggingface.co and cache.
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Download vocabulary from huggingface.co (user-uploaded) and cache.
tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-german-cased")

# If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained('./test/saved_model/')*)
tokenizer = AutoTokenizer.from_pretrained("./test/bert_saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)`}}),N5=new E({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/tokenization_auto.py#L561",parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"slow_tokenizer_class"}]}}),q5=new V({}),G5=new E({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L169"}}),V5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L183",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/master/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),ch=new iLr({props:{$$slots:{default:[Q2t]},$$scope:{ctx:Ai}}}),z5=new w({props:{code:`from transformers import AutoFeatureExtractor

# Download feature extractor from huggingface.co and cache.
feature_extractor = AutoFeatureExtractor.from_pretrained("facebook/wav2vec2-base-960h")

# If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained('./test/saved_model/')*)
feature_extractor = AutoFeatureExtractor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),W5=new E({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/feature_extraction_auto.py#L310",parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}]}}),Q5=new V({}),H5=new E({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L71"}}),Y5=new E({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L85",parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co. Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or
namespaced under a user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.use_auth_token",description:`<strong>use_auth_token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>transformers-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"use_auth_token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}]}}),Fh=new iLr({props:{$$slots:{default:[H2t]},$$scope:{ctx:Ai}}}),K5=new w({props:{code:`from transformers import AutoProcessor

# Download processor from huggingface.co and cache.
processor = AutoProcessor.from_pretrained("facebook/wav2vec2-base-960h")

# If processor files are in a directory (e.g. processor was saved using *save_pretrained('./test/saved_model/')*)
processor = AutoProcessor.from_pretrained("./test/saved_model/"),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/&quot;</span>)`}}),Z5=new E({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/processing_auto.py#L238",parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}]}}),ey=new V({}),oy=new E({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L686"}}),ty=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),ay=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`}}),sy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ny=new w({props:{code:`from transformers import AutoConfig, AutoModel

# Download model and configuration from huggingface.co and cache.
model = AutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModel.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),ly=new V({}),iy=new E({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L693"}}),cy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),my=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`}}),fy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),gy=new w({props:{code:`from transformers import AutoConfig, AutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = AutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForPreTraining.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),hy=new V({}),uy=new E({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L708"}}),_y=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLMProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),by=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`}}),vy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ty=new w({props:{code:`from transformers import AutoConfig, AutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCausalLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Fy=new V({}),Cy=new E({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L715"}}),Ey=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code>(Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),yy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`}}),wy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Ay=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedLM.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Ly=new V({}),By=new E({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L722"}}),ky=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLMProphetNet model)</li>
</ul>`,name:"config"}]}}),Ry=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = AutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`}}),Sy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Py=new w({props:{code:`from transformers import AutoConfig, AutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = AutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/t5_tf_model_config.json")
model = AutoModelForSeq2SeqLM.from_pretrained(
    "./tf_model/t5_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),$y=new V({}),Iy=new E({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L731"}}),jy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Ny=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`}}),qy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Gy=new w({props:{code:`from transformers import AutoConfig, AutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSequenceClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Oy=new V({}),Xy=new E({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L765"}}),zy=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),Wy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`}}),Qy=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Hy=new w({props:{code:`from transformers import AutoConfig, AutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMultipleChoice.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Uy=new V({}),Jy=new E({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L772"}}),Ky=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}]}}),Zy=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`}}),ew=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),ow=new w({props:{code:`from transformers import AutoConfig, AutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForNextSentencePrediction.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),rw=new V({}),tw=new E({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L758"}}),sw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),nw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`}}),lw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),iw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForTokenClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),dw=new V({}),cw=new E({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L740"}}),fw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBirdPegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (Canine model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (MegatronBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystromformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}]}}),gw=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`}}),hw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),uw=new w({props:{code:`from transformers import AutoConfig, AutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForQuestionAnswering.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),pw=new V({}),_w=new E({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L747"}}),vw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),Tw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = AutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`}}),Fw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Cw=new w({props:{code:`from transformers import AutoConfig, AutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = AutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/tapas_tf_model_config.json")
model = AutoModelForTableQuestionAnswering.from_pretrained(
    "./tf_model/tapas_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Mw=new V({}),Ew=new E({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L781"}}),ww=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),Aw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`}}),Lw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Bw=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xw=new V({}),kw=new E({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L811"}}),Sw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),Pw=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`}}),$w=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Iw=new w({props:{code:`from transformers import AutoConfig, AutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForVision2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Dw=new V({}),jw=new E({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L818"}}),qw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Gw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`}}),Ow=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Xw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Vw=new V({}),zw=new E({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L841"}}),Qw=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),Hw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioFrameClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`}}),Uw=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Jw=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioFrameClassification

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioFrameClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioFrameClassification.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Yw=new V({}),Kw=new E({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L825"}}),e6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),o6=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForCTC.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`}}),r6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),t6=new w({props:{code:`from transformers import AutoConfig, AutoModelForCTC

# Download model and configuration from huggingface.co and cache.
model = AutoModelForCTC.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForCTC.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForCTC.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),a6=new V({}),s6=new E({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L832"}}),l6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
</ul>`,name:"config"}]}}),i6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`}}),d6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),m6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSpeechSeq2Seq.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),f6=new V({}),g6=new E({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L850"}}),u6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}]}}),p6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForAudioXVector.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`}}),_6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),b6=new w({props:{code:`from transformers import AutoConfig, AutoModelForAudioXVector

# Download model and configuration from huggingface.co and cache.
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForAudioXVector.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForAudioXVector.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),v6=new V({}),T6=new E({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L857"}}),C6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}]}}),M6=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForMaskedImageModeling.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`}}),E6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),y6=new w({props:{code:`from transformers import AutoConfig, AutoModelForMaskedImageModeling

# Download model and configuration from huggingface.co and cache.
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForMaskedImageModeling.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForMaskedImageModeling.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),w6=new V({}),A6=new E({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L804"}}),B6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
</ul>`,name:"config"}]}}),x6=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForObjectDetection.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`}}),k6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),R6=new w({props:{code:`from transformers import AutoConfig, AutoModelForObjectDetection

# Download model and configuration from huggingface.co and cache.
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForObjectDetection.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForObjectDetection.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),S6=new V({}),P6=new E({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L788"}}),I6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}]}}),D6=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForImageSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`}}),j6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),N6=new w({props:{code:`from transformers import AutoConfig, AutoModelForImageSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForImageSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForImageSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),q6=new V({}),G6=new E({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_auto.py#L795"}}),X6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}]}}),V6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = AutoModelForSemanticSegmentation.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`}}),z6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),Q6=new w({props:{code:`from transformers import AutoConfig, AutoModelForSemanticSegmentation

# Download model and configuration from huggingface.co and cache.
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased")

# Update configuration during loading
model = AutoModelForSemanticSegmentation.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a TF checkpoint file instead of a PyTorch model (slower)
config = AutoConfig.from_pretrained("./tf_model/bert_tf_model_config.json")
model = AutoModelForSemanticSegmentation.from_pretrained(
    "./tf_model/bert_tf_checkpoint.ckpt.index", from_tf=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),H6=new V({}),U6=new E({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L373"}}),Y6=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),K6=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`}}),Z6=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),eA=new w({props:{code:`from transformers import AutoConfig, TFAutoModel

# Download model and configuration from huggingface.co and cache.
model = TFAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),oA=new V({}),rA=new E({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L380"}}),aA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),sA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`}}),nA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),lA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),iA=new V({}),dA=new E({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L395"}}),mA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),fA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`}}),gA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),hA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),uA=new V({}),pA=new E({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L402"}}),bA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNext model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),vA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`}}),TA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),CA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),MA=new V({}),EA=new E({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L416"}}),wA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}]}}),AA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`}}),LA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),BA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),xA=new V({}),kA=new E({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L423"}}),SA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),PA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = TFAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`}}),$A=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),IA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = TFAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = TFAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),DA=new V({}),jA=new E({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L432"}}),qA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),GA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`}}),OA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),XA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),VA=new V({}),zA=new E({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L468"}}),QA=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),HA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`}}),UA=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),JA=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),YA=new V({}),KA=new E({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L448"}}),e0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}]}}),o0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("google/tapas-base-finetuned-wtq")
model = TFAutoModelForTableQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`}}),r0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),t0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTableQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq")

# Update configuration during loading
model = TFAutoModelForTableQuestionAnswering.from_pretrained("google/tapas-base-finetuned-wtq", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/tapas_pt_model_config.json")
model = TFAutoModelForTableQuestionAnswering.from_pretrained(
    "./pt_model/tapas_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),a0=new V({}),s0=new E({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L459"}}),l0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),i0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`}}),d0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),c0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),m0=new V({}),f0=new E({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L441"}}),h0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}]}}),u0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`}}),p0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),_0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),b0=new V({}),v0=new E({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L409"}}),F0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),C0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`}}),M0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),E0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),y0=new V({}),w0=new E({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_tf_auto.py#L484"}}),L0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
</ul>`,name:"config"}]}}),B0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = TFAutoModelForSpeechSeq2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`}}),x0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),k0=new w({props:{code:`from transformers import AutoConfig, TFAutoModelForSpeechSeq2Seq

# Download model and configuration from huggingface.co and cache.
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = TFAutoModelForSpeechSeq2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),R0=new V({}),S0=new E({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L229"}}),$0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),I0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModel.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`}}),D0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),j0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModel

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModel.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModel.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModel.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),N0=new V({}),q0=new E({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L243"}}),O0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
</ul>`,name:"config"}]}}),X0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForCausalLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`}}),V0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),z0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForCausalLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForCausalLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForCausalLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),W0=new V({}),Q0=new E({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L236"}}),U0=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}]}}),J0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForPreTraining.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`}}),Y0=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),K0=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForPreTraining

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForPreTraining.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForPreTraining.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),Z0=new V({}),eL=new E({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L250"}}),rL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),tL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMaskedLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`}}),aL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),sL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMaskedLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMaskedLM.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMaskedLM.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),nL=new V({}),lL=new E({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L257"}}),dL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (mT5 model)</li>
<li><a href="/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/master/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}]}}),cL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("t5-base")
model = FlaxAutoModelForSeq2SeqLM.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`}}),mL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),fL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSeq2SeqLM

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base")

# Update configuration during loading
model = FlaxAutoModelForSeq2SeqLM.from_pretrained("t5-base", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/t5_pt_model_config.json")
model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
    "./pt_model/t5_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),gL=new V({}),hL=new E({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L266"}}),pL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),_L=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForSequenceClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`}}),bL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),vL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForSequenceClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForSequenceClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForSequenceClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),TL=new V({}),FL=new E({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L275"}}),ML=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),EL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForQuestionAnswering.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`}}),yL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),wL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForQuestionAnswering

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForQuestionAnswering.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForQuestionAnswering.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),AL=new V({}),LL=new E({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L282"}}),xL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),kL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForTokenClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`}}),RL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),SL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForTokenClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForTokenClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForTokenClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),PL=new V({}),$L=new E({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L291"}}),DL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
</ul>`,name:"config"}]}}),jL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForMultipleChoice.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`}}),NL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),qL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForMultipleChoice

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForMultipleChoice.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForMultipleChoice.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),GL=new V({}),OL=new E({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L298"}}),VL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}]}}),zL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForNextSentencePrediction.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`}}),WL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),QL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForNextSentencePrediction

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForNextSentencePrediction.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),HL=new V({}),UL=new E({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L307"}}),YL=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}]}}),KL=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForImageClassification.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`}}),ZL=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),o8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForImageClassification

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForImageClassification.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForImageClassification.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),r8=new V({}),t8=new E({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/modeling_flax_auto.py#L316"}}),s8=new E({props:{name:"from_config",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L390",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}]}}),n8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download configuration from huggingface.co and cache.
config = AutoConfig.from_pretrained("bert-base-cased")
model = FlaxAutoModelForVision2Seq.from_config(config),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`}}),l8=new E({props:{name:"from_pretrained",anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/auto/auto_factory.py#L418",parametersDescription:[{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.
Valid model ids can be located at the root-level, like <code>bert-base-uncased</code>, or namespaced under a
user or organization name, like <code>dbmdz/bert-base-german-cased</code>.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.revision(str,",description:`<strong>revision(<code>str</code>,</strong> <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision(str,"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}]}}),i8=new w({props:{code:`from transformers import AutoConfig, FlaxAutoModelForVision2Seq

# Download model and configuration from huggingface.co and cache.
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased")

# Update configuration during loading
model = FlaxAutoModelForVision2Seq.from_pretrained("bert-base-cased", output_attentions=True)
model.config.output_attentions

# Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)
config = AutoConfig.from_pretrained("./pt_model/bert_pt_model_config.json")
model = FlaxAutoModelForVision2Seq.from_pretrained(
    "./pt_model/bert_pytorch_model.bin", from_pt=True, config=config
),`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`}}),{c(){J=a("meta"),Be=l(),ie=a("h1"),fe=a("a"),so=a("span"),m(ce.$$.fragment),_e=l(),Go=a("span"),Li=o("Auto Classes"),Mm=l(),na=a("p"),Bi=o(`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),xi=a("code"),T5=o("from_pretrained()"),Em=o(` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),ye=l(),io=a("p"),ki=o("Instantiating one of "),$s=a("a"),F5=o("AutoConfig"),Is=o(", "),Ds=a("a"),C5=o("AutoModel"),Ri=o(`, and
`),js=a("a"),M5=o("AutoTokenizer"),Si=o(" will directly create a class of the relevant architecture. For instance"),ym=l(),m($a.$$.fragment),co=l(),ge=a("p"),s7=o("will create a model that is an instance of "),Pi=a("a"),n7=o("BertModel"),l7=o("."),Oo=l(),Ia=a("p"),i7=o("There is one class of "),wm=a("code"),d7=o("AutoModel"),ARe=o(" for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),_9e=l(),$i=a("h2"),Am=a("a"),pW=a("span"),m(E5.$$.fragment),LRe=l(),_W=a("span"),BRe=o("Extending the Auto Classes"),b9e=l(),Ns=a("p"),xRe=o(`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),bW=a("code"),kRe=o("NewModel"),RRe=o(", make sure you have a "),vW=a("code"),SRe=o("NewModelConfig"),PRe=o(` then you can add those to the auto
classes like this:`),v9e=l(),m(y5.$$.fragment),T9e=l(),c7=a("p"),$Re=o("You will then be able to use the auto classes like you would usually do!"),F9e=l(),m(Lm.$$.fragment),C9e=l(),Ii=a("h2"),Bm=a("a"),TW=a("span"),m(w5.$$.fragment),IRe=l(),FW=a("span"),DRe=o("AutoConfig"),M9e=l(),Xo=a("div"),m(A5.$$.fragment),jRe=l(),L5=a("p"),NRe=o(`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),m7=a("a"),qRe=o("from_pretrained()"),GRe=o(" class method."),ORe=l(),B5=a("p"),XRe=o("This class cannot be instantiated directly using "),CW=a("code"),VRe=o("__init__()"),zRe=o(" (throws an error)."),WRe=l(),mo=a("div"),m(x5.$$.fragment),QRe=l(),MW=a("p"),HRe=o("Instantiate one of the configuration classes of the library from a pretrained model configuration."),URe=l(),Di=a("p"),JRe=o("The configuration class to instantiate is selected based on the "),EW=a("code"),YRe=o("model_type"),KRe=o(` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),yW=a("code"),ZRe=o("pretrained_model_name_or_path"),eSe=o(":"),oSe=l(),v=a("ul"),xm=a("li"),wW=a("strong"),rSe=o("albert"),tSe=o(" \u2014 "),f7=a("a"),aSe=o("AlbertConfig"),sSe=o(" (ALBERT model)"),nSe=l(),km=a("li"),AW=a("strong"),lSe=o("bart"),iSe=o(" \u2014 "),g7=a("a"),dSe=o("BartConfig"),cSe=o(" (BART model)"),mSe=l(),Rm=a("li"),LW=a("strong"),fSe=o("beit"),gSe=o(" \u2014 "),h7=a("a"),hSe=o("BeitConfig"),uSe=o(" (BEiT model)"),pSe=l(),Sm=a("li"),BW=a("strong"),_Se=o("bert"),bSe=o(" \u2014 "),u7=a("a"),vSe=o("BertConfig"),TSe=o(" (BERT model)"),FSe=l(),Pm=a("li"),xW=a("strong"),CSe=o("bert-generation"),MSe=o(" \u2014 "),p7=a("a"),ESe=o("BertGenerationConfig"),ySe=o(" (Bert Generation model)"),wSe=l(),$m=a("li"),kW=a("strong"),ASe=o("big_bird"),LSe=o(" \u2014 "),_7=a("a"),BSe=o("BigBirdConfig"),xSe=o(" (BigBird model)"),kSe=l(),Im=a("li"),RW=a("strong"),RSe=o("bigbird_pegasus"),SSe=o(" \u2014 "),b7=a("a"),PSe=o("BigBirdPegasusConfig"),$Se=o(" (BigBirdPegasus model)"),ISe=l(),Dm=a("li"),SW=a("strong"),DSe=o("blenderbot"),jSe=o(" \u2014 "),v7=a("a"),NSe=o("BlenderbotConfig"),qSe=o(" (Blenderbot model)"),GSe=l(),jm=a("li"),PW=a("strong"),OSe=o("blenderbot-small"),XSe=o(" \u2014 "),T7=a("a"),VSe=o("BlenderbotSmallConfig"),zSe=o(" (BlenderbotSmall model)"),WSe=l(),Nm=a("li"),$W=a("strong"),QSe=o("camembert"),HSe=o(" \u2014 "),F7=a("a"),USe=o("CamembertConfig"),JSe=o(" (CamemBERT model)"),YSe=l(),qm=a("li"),IW=a("strong"),KSe=o("canine"),ZSe=o(" \u2014 "),C7=a("a"),ePe=o("CanineConfig"),oPe=o(" (Canine model)"),rPe=l(),Gm=a("li"),DW=a("strong"),tPe=o("clip"),aPe=o(" \u2014 "),M7=a("a"),sPe=o("CLIPConfig"),nPe=o(" (CLIP model)"),lPe=l(),Om=a("li"),jW=a("strong"),iPe=o("convbert"),dPe=o(" \u2014 "),E7=a("a"),cPe=o("ConvBertConfig"),mPe=o(" (ConvBERT model)"),fPe=l(),Xm=a("li"),NW=a("strong"),gPe=o("convnext"),hPe=o(" \u2014 "),y7=a("a"),uPe=o("ConvNextConfig"),pPe=o(" (ConvNext model)"),_Pe=l(),Vm=a("li"),qW=a("strong"),bPe=o("ctrl"),vPe=o(" \u2014 "),w7=a("a"),TPe=o("CTRLConfig"),FPe=o(" (CTRL model)"),CPe=l(),zm=a("li"),GW=a("strong"),MPe=o("data2vec-audio"),EPe=o(" \u2014 "),A7=a("a"),yPe=o("Data2VecAudioConfig"),wPe=o(" (Data2VecAudio model)"),APe=l(),Wm=a("li"),OW=a("strong"),LPe=o("data2vec-text"),BPe=o(" \u2014 "),L7=a("a"),xPe=o("Data2VecTextConfig"),kPe=o(" (Data2VecText model)"),RPe=l(),Qm=a("li"),XW=a("strong"),SPe=o("deberta"),PPe=o(" \u2014 "),B7=a("a"),$Pe=o("DebertaConfig"),IPe=o(" (DeBERTa model)"),DPe=l(),Hm=a("li"),VW=a("strong"),jPe=o("deberta-v2"),NPe=o(" \u2014 "),x7=a("a"),qPe=o("DebertaV2Config"),GPe=o(" (DeBERTa-v2 model)"),OPe=l(),Um=a("li"),zW=a("strong"),XPe=o("deit"),VPe=o(" \u2014 "),k7=a("a"),zPe=o("DeiTConfig"),WPe=o(" (DeiT model)"),QPe=l(),Jm=a("li"),WW=a("strong"),HPe=o("detr"),UPe=o(" \u2014 "),R7=a("a"),JPe=o("DetrConfig"),YPe=o(" (DETR model)"),KPe=l(),Ym=a("li"),QW=a("strong"),ZPe=o("distilbert"),e$e=o(" \u2014 "),S7=a("a"),o$e=o("DistilBertConfig"),r$e=o(" (DistilBERT model)"),t$e=l(),Km=a("li"),HW=a("strong"),a$e=o("dpr"),s$e=o(" \u2014 "),P7=a("a"),n$e=o("DPRConfig"),l$e=o(" (DPR model)"),i$e=l(),Zm=a("li"),UW=a("strong"),d$e=o("electra"),c$e=o(" \u2014 "),$7=a("a"),m$e=o("ElectraConfig"),f$e=o(" (ELECTRA model)"),g$e=l(),ef=a("li"),JW=a("strong"),h$e=o("encoder-decoder"),u$e=o(" \u2014 "),I7=a("a"),p$e=o("EncoderDecoderConfig"),_$e=o(" (Encoder decoder model)"),b$e=l(),of=a("li"),YW=a("strong"),v$e=o("flaubert"),T$e=o(" \u2014 "),D7=a("a"),F$e=o("FlaubertConfig"),C$e=o(" (FlauBERT model)"),M$e=l(),rf=a("li"),KW=a("strong"),E$e=o("fnet"),y$e=o(" \u2014 "),j7=a("a"),w$e=o("FNetConfig"),A$e=o(" (FNet model)"),L$e=l(),tf=a("li"),ZW=a("strong"),B$e=o("fsmt"),x$e=o(" \u2014 "),N7=a("a"),k$e=o("FSMTConfig"),R$e=o(" (FairSeq Machine-Translation model)"),S$e=l(),af=a("li"),eQ=a("strong"),P$e=o("funnel"),$$e=o(" \u2014 "),q7=a("a"),I$e=o("FunnelConfig"),D$e=o(" (Funnel Transformer model)"),j$e=l(),sf=a("li"),oQ=a("strong"),N$e=o("gpt2"),q$e=o(" \u2014 "),G7=a("a"),G$e=o("GPT2Config"),O$e=o(" (OpenAI GPT-2 model)"),X$e=l(),nf=a("li"),rQ=a("strong"),V$e=o("gpt_neo"),z$e=o(" \u2014 "),O7=a("a"),W$e=o("GPTNeoConfig"),Q$e=o(" (GPT Neo model)"),H$e=l(),lf=a("li"),tQ=a("strong"),U$e=o("gptj"),J$e=o(" \u2014 "),X7=a("a"),Y$e=o("GPTJConfig"),K$e=o(" (GPT-J model)"),Z$e=l(),df=a("li"),aQ=a("strong"),eIe=o("hubert"),oIe=o(" \u2014 "),V7=a("a"),rIe=o("HubertConfig"),tIe=o(" (Hubert model)"),aIe=l(),cf=a("li"),sQ=a("strong"),sIe=o("ibert"),nIe=o(" \u2014 "),z7=a("a"),lIe=o("IBertConfig"),iIe=o(" (I-BERT model)"),dIe=l(),mf=a("li"),nQ=a("strong"),cIe=o("imagegpt"),mIe=o(" \u2014 "),W7=a("a"),fIe=o("ImageGPTConfig"),gIe=o(" (ImageGPT model)"),hIe=l(),ff=a("li"),lQ=a("strong"),uIe=o("layoutlm"),pIe=o(" \u2014 "),Q7=a("a"),_Ie=o("LayoutLMConfig"),bIe=o(" (LayoutLM model)"),vIe=l(),gf=a("li"),iQ=a("strong"),TIe=o("layoutlmv2"),FIe=o(" \u2014 "),H7=a("a"),CIe=o("LayoutLMv2Config"),MIe=o(" (LayoutLMv2 model)"),EIe=l(),hf=a("li"),dQ=a("strong"),yIe=o("led"),wIe=o(" \u2014 "),U7=a("a"),AIe=o("LEDConfig"),LIe=o(" (LED model)"),BIe=l(),uf=a("li"),cQ=a("strong"),xIe=o("longformer"),kIe=o(" \u2014 "),J7=a("a"),RIe=o("LongformerConfig"),SIe=o(" (Longformer model)"),PIe=l(),pf=a("li"),mQ=a("strong"),$Ie=o("luke"),IIe=o(" \u2014 "),Y7=a("a"),DIe=o("LukeConfig"),jIe=o(" (LUKE model)"),NIe=l(),_f=a("li"),fQ=a("strong"),qIe=o("lxmert"),GIe=o(" \u2014 "),K7=a("a"),OIe=o("LxmertConfig"),XIe=o(" (LXMERT model)"),VIe=l(),bf=a("li"),gQ=a("strong"),zIe=o("m2m_100"),WIe=o(" \u2014 "),Z7=a("a"),QIe=o("M2M100Config"),HIe=o(" (M2M100 model)"),UIe=l(),vf=a("li"),hQ=a("strong"),JIe=o("marian"),YIe=o(" \u2014 "),e9=a("a"),KIe=o("MarianConfig"),ZIe=o(" (Marian model)"),eDe=l(),Tf=a("li"),uQ=a("strong"),oDe=o("mbart"),rDe=o(" \u2014 "),o9=a("a"),tDe=o("MBartConfig"),aDe=o(" (mBART model)"),sDe=l(),Ff=a("li"),pQ=a("strong"),nDe=o("megatron-bert"),lDe=o(" \u2014 "),r9=a("a"),iDe=o("MegatronBertConfig"),dDe=o(" (MegatronBert model)"),cDe=l(),Cf=a("li"),_Q=a("strong"),mDe=o("mobilebert"),fDe=o(" \u2014 "),t9=a("a"),gDe=o("MobileBertConfig"),hDe=o(" (MobileBERT model)"),uDe=l(),Mf=a("li"),bQ=a("strong"),pDe=o("mpnet"),_De=o(" \u2014 "),a9=a("a"),bDe=o("MPNetConfig"),vDe=o(" (MPNet model)"),TDe=l(),Ef=a("li"),vQ=a("strong"),FDe=o("mt5"),CDe=o(" \u2014 "),s9=a("a"),MDe=o("MT5Config"),EDe=o(" (mT5 model)"),yDe=l(),yf=a("li"),TQ=a("strong"),wDe=o("nystromformer"),ADe=o(" \u2014 "),n9=a("a"),LDe=o("NystromformerConfig"),BDe=o(" (Nystromformer model)"),xDe=l(),wf=a("li"),FQ=a("strong"),kDe=o("openai-gpt"),RDe=o(" \u2014 "),l9=a("a"),SDe=o("OpenAIGPTConfig"),PDe=o(" (OpenAI GPT model)"),$De=l(),Af=a("li"),CQ=a("strong"),IDe=o("pegasus"),DDe=o(" \u2014 "),i9=a("a"),jDe=o("PegasusConfig"),NDe=o(" (Pegasus model)"),qDe=l(),Lf=a("li"),MQ=a("strong"),GDe=o("perceiver"),ODe=o(" \u2014 "),d9=a("a"),XDe=o("PerceiverConfig"),VDe=o(" (Perceiver model)"),zDe=l(),Bf=a("li"),EQ=a("strong"),WDe=o("plbart"),QDe=o(" \u2014 "),c9=a("a"),HDe=o("PLBartConfig"),UDe=o(" (PLBart model)"),JDe=l(),xf=a("li"),yQ=a("strong"),YDe=o("poolformer"),KDe=o(" \u2014 "),m9=a("a"),ZDe=o("PoolFormerConfig"),eje=o(" (PoolFormer model)"),oje=l(),kf=a("li"),wQ=a("strong"),rje=o("prophetnet"),tje=o(" \u2014 "),f9=a("a"),aje=o("ProphetNetConfig"),sje=o(" (ProphetNet model)"),nje=l(),Rf=a("li"),AQ=a("strong"),lje=o("qdqbert"),ije=o(" \u2014 "),g9=a("a"),dje=o("QDQBertConfig"),cje=o(" (QDQBert model)"),mje=l(),Sf=a("li"),LQ=a("strong"),fje=o("rag"),gje=o(" \u2014 "),h9=a("a"),hje=o("RagConfig"),uje=o(" (RAG model)"),pje=l(),Pf=a("li"),BQ=a("strong"),_je=o("realm"),bje=o(" \u2014 "),u9=a("a"),vje=o("RealmConfig"),Tje=o(" (Realm model)"),Fje=l(),$f=a("li"),xQ=a("strong"),Cje=o("reformer"),Mje=o(" \u2014 "),p9=a("a"),Eje=o("ReformerConfig"),yje=o(" (Reformer model)"),wje=l(),If=a("li"),kQ=a("strong"),Aje=o("rembert"),Lje=o(" \u2014 "),_9=a("a"),Bje=o("RemBertConfig"),xje=o(" (RemBERT model)"),kje=l(),Df=a("li"),RQ=a("strong"),Rje=o("retribert"),Sje=o(" \u2014 "),b9=a("a"),Pje=o("RetriBertConfig"),$je=o(" (RetriBERT model)"),Ije=l(),jf=a("li"),SQ=a("strong"),Dje=o("roberta"),jje=o(" \u2014 "),v9=a("a"),Nje=o("RobertaConfig"),qje=o(" (RoBERTa model)"),Gje=l(),Nf=a("li"),PQ=a("strong"),Oje=o("roformer"),Xje=o(" \u2014 "),T9=a("a"),Vje=o("RoFormerConfig"),zje=o(" (RoFormer model)"),Wje=l(),qf=a("li"),$Q=a("strong"),Qje=o("segformer"),Hje=o(" \u2014 "),F9=a("a"),Uje=o("SegformerConfig"),Jje=o(" (SegFormer model)"),Yje=l(),Gf=a("li"),IQ=a("strong"),Kje=o("sew"),Zje=o(" \u2014 "),C9=a("a"),eNe=o("SEWConfig"),oNe=o(" (SEW model)"),rNe=l(),Of=a("li"),DQ=a("strong"),tNe=o("sew-d"),aNe=o(" \u2014 "),M9=a("a"),sNe=o("SEWDConfig"),nNe=o(" (SEW-D model)"),lNe=l(),Xf=a("li"),jQ=a("strong"),iNe=o("speech-encoder-decoder"),dNe=o(" \u2014 "),E9=a("a"),cNe=o("SpeechEncoderDecoderConfig"),mNe=o(" (Speech Encoder decoder model)"),fNe=l(),Vf=a("li"),NQ=a("strong"),gNe=o("speech_to_text"),hNe=o(" \u2014 "),y9=a("a"),uNe=o("Speech2TextConfig"),pNe=o(" (Speech2Text model)"),_Ne=l(),zf=a("li"),qQ=a("strong"),bNe=o("speech_to_text_2"),vNe=o(" \u2014 "),w9=a("a"),TNe=o("Speech2Text2Config"),FNe=o(" (Speech2Text2 model)"),CNe=l(),Wf=a("li"),GQ=a("strong"),MNe=o("splinter"),ENe=o(" \u2014 "),A9=a("a"),yNe=o("SplinterConfig"),wNe=o(" (Splinter model)"),ANe=l(),Qf=a("li"),OQ=a("strong"),LNe=o("squeezebert"),BNe=o(" \u2014 "),L9=a("a"),xNe=o("SqueezeBertConfig"),kNe=o(" (SqueezeBERT model)"),RNe=l(),Hf=a("li"),XQ=a("strong"),SNe=o("swin"),PNe=o(" \u2014 "),B9=a("a"),$Ne=o("SwinConfig"),INe=o(" (Swin model)"),DNe=l(),Uf=a("li"),VQ=a("strong"),jNe=o("t5"),NNe=o(" \u2014 "),x9=a("a"),qNe=o("T5Config"),GNe=o(" (T5 model)"),ONe=l(),Jf=a("li"),zQ=a("strong"),XNe=o("tapas"),VNe=o(" \u2014 "),k9=a("a"),zNe=o("TapasConfig"),WNe=o(" (TAPAS model)"),QNe=l(),Yf=a("li"),WQ=a("strong"),HNe=o("transfo-xl"),UNe=o(" \u2014 "),R9=a("a"),JNe=o("TransfoXLConfig"),YNe=o(" (Transformer-XL model)"),KNe=l(),Kf=a("li"),QQ=a("strong"),ZNe=o("trocr"),eqe=o(" \u2014 "),S9=a("a"),oqe=o("TrOCRConfig"),rqe=o(" (TrOCR model)"),tqe=l(),Zf=a("li"),HQ=a("strong"),aqe=o("unispeech"),sqe=o(" \u2014 "),P9=a("a"),nqe=o("UniSpeechConfig"),lqe=o(" (UniSpeech model)"),iqe=l(),eg=a("li"),UQ=a("strong"),dqe=o("unispeech-sat"),cqe=o(" \u2014 "),$9=a("a"),mqe=o("UniSpeechSatConfig"),fqe=o(" (UniSpeechSat model)"),gqe=l(),og=a("li"),JQ=a("strong"),hqe=o("vilt"),uqe=o(" \u2014 "),I9=a("a"),pqe=o("ViltConfig"),_qe=o(" (ViLT model)"),bqe=l(),rg=a("li"),YQ=a("strong"),vqe=o("vision-encoder-decoder"),Tqe=o(" \u2014 "),D9=a("a"),Fqe=o("VisionEncoderDecoderConfig"),Cqe=o(" (Vision Encoder decoder model)"),Mqe=l(),tg=a("li"),KQ=a("strong"),Eqe=o("vision-text-dual-encoder"),yqe=o(" \u2014 "),j9=a("a"),wqe=o("VisionTextDualEncoderConfig"),Aqe=o(" (VisionTextDualEncoder model)"),Lqe=l(),ag=a("li"),ZQ=a("strong"),Bqe=o("visual_bert"),xqe=o(" \u2014 "),N9=a("a"),kqe=o("VisualBertConfig"),Rqe=o(" (VisualBert model)"),Sqe=l(),sg=a("li"),eH=a("strong"),Pqe=o("vit"),$qe=o(" \u2014 "),q9=a("a"),Iqe=o("ViTConfig"),Dqe=o(" (ViT model)"),jqe=l(),ng=a("li"),oH=a("strong"),Nqe=o("vit_mae"),qqe=o(" \u2014 "),G9=a("a"),Gqe=o("ViTMAEConfig"),Oqe=o(" (ViTMAE model)"),Xqe=l(),lg=a("li"),rH=a("strong"),Vqe=o("wav2vec2"),zqe=o(" \u2014 "),O9=a("a"),Wqe=o("Wav2Vec2Config"),Qqe=o(" (Wav2Vec2 model)"),Hqe=l(),ig=a("li"),tH=a("strong"),Uqe=o("wavlm"),Jqe=o(" \u2014 "),X9=a("a"),Yqe=o("WavLMConfig"),Kqe=o(" (WavLM model)"),Zqe=l(),dg=a("li"),aH=a("strong"),eGe=o("xglm"),oGe=o(" \u2014 "),V9=a("a"),rGe=o("XGLMConfig"),tGe=o(" (XGLM model)"),aGe=l(),cg=a("li"),sH=a("strong"),sGe=o("xlm"),nGe=o(" \u2014 "),z9=a("a"),lGe=o("XLMConfig"),iGe=o(" (XLM model)"),dGe=l(),mg=a("li"),nH=a("strong"),cGe=o("xlm-prophetnet"),mGe=o(" \u2014 "),W9=a("a"),fGe=o("XLMProphetNetConfig"),gGe=o(" (XLMProphetNet model)"),hGe=l(),fg=a("li"),lH=a("strong"),uGe=o("xlm-roberta"),pGe=o(" \u2014 "),Q9=a("a"),_Ge=o("XLMRobertaConfig"),bGe=o(" (XLM-RoBERTa model)"),vGe=l(),gg=a("li"),iH=a("strong"),TGe=o("xlm-roberta-xl"),FGe=o(" \u2014 "),H9=a("a"),CGe=o("XLMRobertaXLConfig"),MGe=o(" (XLM-RoBERTa-XL model)"),EGe=l(),hg=a("li"),dH=a("strong"),yGe=o("xlnet"),wGe=o(" \u2014 "),U9=a("a"),AGe=o("XLNetConfig"),LGe=o(" (XLNet model)"),BGe=l(),ug=a("li"),cH=a("strong"),xGe=o("yoso"),kGe=o(" \u2014 "),J9=a("a"),RGe=o("YosoConfig"),SGe=o(" (YOSO model)"),PGe=l(),mH=a("p"),$Ge=o("Examples:"),IGe=l(),m(k5.$$.fragment),DGe=l(),pg=a("div"),m(R5.$$.fragment),jGe=l(),fH=a("p"),NGe=o("Register a new configuration for this class."),E9e=l(),ji=a("h2"),_g=a("a"),gH=a("span"),m(S5.$$.fragment),qGe=l(),hH=a("span"),GGe=o("AutoTokenizer"),y9e=l(),Vo=a("div"),m(P5.$$.fragment),OGe=l(),$5=a("p"),XGe=o(`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),Y9=a("a"),VGe=o("AutoTokenizer.from_pretrained()"),zGe=o(" class method."),WGe=l(),I5=a("p"),QGe=o("This class cannot be instantiated directly using "),uH=a("code"),HGe=o("__init__()"),UGe=o(" (throws an error)."),JGe=l(),fo=a("div"),m(D5.$$.fragment),YGe=l(),pH=a("p"),KGe=o("Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),ZGe=l(),Da=a("p"),eOe=o("The tokenizer class to instantiate is selected based on the "),_H=a("code"),oOe=o("model_type"),rOe=o(` property of the config object (either
passed as an argument or loaded from `),bH=a("code"),tOe=o("pretrained_model_name_or_path"),aOe=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),vH=a("code"),sOe=o("pretrained_model_name_or_path"),nOe=o(":"),lOe=l(),M=a("ul"),qs=a("li"),TH=a("strong"),iOe=o("albert"),dOe=o(" \u2014 "),K9=a("a"),cOe=o("AlbertTokenizer"),mOe=o(" or "),Z9=a("a"),fOe=o("AlbertTokenizerFast"),gOe=o(" (ALBERT model)"),hOe=l(),Gs=a("li"),FH=a("strong"),uOe=o("bart"),pOe=o(" \u2014 "),eB=a("a"),_Oe=o("BartTokenizer"),bOe=o(" or "),oB=a("a"),vOe=o("BartTokenizerFast"),TOe=o(" (BART model)"),FOe=l(),Os=a("li"),CH=a("strong"),COe=o("barthez"),MOe=o(" \u2014 "),rB=a("a"),EOe=o("BarthezTokenizer"),yOe=o(" or "),tB=a("a"),wOe=o("BarthezTokenizerFast"),AOe=o(" (BARThez model)"),LOe=l(),bg=a("li"),MH=a("strong"),BOe=o("bartpho"),xOe=o(" \u2014 "),aB=a("a"),kOe=o("BartphoTokenizer"),ROe=o(" (BARTpho model)"),SOe=l(),Xs=a("li"),EH=a("strong"),POe=o("bert"),$Oe=o(" \u2014 "),sB=a("a"),IOe=o("BertTokenizer"),DOe=o(" or "),nB=a("a"),jOe=o("BertTokenizerFast"),NOe=o(" (BERT model)"),qOe=l(),vg=a("li"),yH=a("strong"),GOe=o("bert-generation"),OOe=o(" \u2014 "),lB=a("a"),XOe=o("BertGenerationTokenizer"),VOe=o(" (Bert Generation model)"),zOe=l(),Tg=a("li"),wH=a("strong"),WOe=o("bert-japanese"),QOe=o(" \u2014 "),iB=a("a"),HOe=o("BertJapaneseTokenizer"),UOe=o(" (BertJapanese model)"),JOe=l(),Fg=a("li"),AH=a("strong"),YOe=o("bertweet"),KOe=o(" \u2014 "),dB=a("a"),ZOe=o("BertweetTokenizer"),eXe=o(" (Bertweet model)"),oXe=l(),Vs=a("li"),LH=a("strong"),rXe=o("big_bird"),tXe=o(" \u2014 "),cB=a("a"),aXe=o("BigBirdTokenizer"),sXe=o(" or "),mB=a("a"),nXe=o("BigBirdTokenizerFast"),lXe=o(" (BigBird model)"),iXe=l(),zs=a("li"),BH=a("strong"),dXe=o("bigbird_pegasus"),cXe=o(" \u2014 "),fB=a("a"),mXe=o("PegasusTokenizer"),fXe=o(" or "),gB=a("a"),gXe=o("PegasusTokenizerFast"),hXe=o(" (BigBirdPegasus model)"),uXe=l(),Ws=a("li"),xH=a("strong"),pXe=o("blenderbot"),_Xe=o(" \u2014 "),hB=a("a"),bXe=o("BlenderbotTokenizer"),vXe=o(" or "),uB=a("a"),TXe=o("BlenderbotTokenizerFast"),FXe=o(" (Blenderbot model)"),CXe=l(),Cg=a("li"),kH=a("strong"),MXe=o("blenderbot-small"),EXe=o(" \u2014 "),pB=a("a"),yXe=o("BlenderbotSmallTokenizer"),wXe=o(" (BlenderbotSmall model)"),AXe=l(),Mg=a("li"),RH=a("strong"),LXe=o("byt5"),BXe=o(" \u2014 "),_B=a("a"),xXe=o("ByT5Tokenizer"),kXe=o(" (ByT5 model)"),RXe=l(),Qs=a("li"),SH=a("strong"),SXe=o("camembert"),PXe=o(" \u2014 "),bB=a("a"),$Xe=o("CamembertTokenizer"),IXe=o(" or "),vB=a("a"),DXe=o("CamembertTokenizerFast"),jXe=o(" (CamemBERT model)"),NXe=l(),Eg=a("li"),PH=a("strong"),qXe=o("canine"),GXe=o(" \u2014 "),TB=a("a"),OXe=o("CanineTokenizer"),XXe=o(" (Canine model)"),VXe=l(),Hs=a("li"),$H=a("strong"),zXe=o("clip"),WXe=o(" \u2014 "),FB=a("a"),QXe=o("CLIPTokenizer"),HXe=o(" or "),CB=a("a"),UXe=o("CLIPTokenizerFast"),JXe=o(" (CLIP model)"),YXe=l(),Us=a("li"),IH=a("strong"),KXe=o("convbert"),ZXe=o(" \u2014 "),MB=a("a"),eVe=o("ConvBertTokenizer"),oVe=o(" or "),EB=a("a"),rVe=o("ConvBertTokenizerFast"),tVe=o(" (ConvBERT model)"),aVe=l(),Js=a("li"),DH=a("strong"),sVe=o("cpm"),nVe=o(" \u2014 "),yB=a("a"),lVe=o("CpmTokenizer"),iVe=o(" or "),jH=a("code"),dVe=o("CpmTokenizerFast"),cVe=o(" (CPM model)"),mVe=l(),yg=a("li"),NH=a("strong"),fVe=o("ctrl"),gVe=o(" \u2014 "),wB=a("a"),hVe=o("CTRLTokenizer"),uVe=o(" (CTRL model)"),pVe=l(),Ys=a("li"),qH=a("strong"),_Ve=o("deberta"),bVe=o(" \u2014 "),AB=a("a"),vVe=o("DebertaTokenizer"),TVe=o(" or "),LB=a("a"),FVe=o("DebertaTokenizerFast"),CVe=o(" (DeBERTa model)"),MVe=l(),wg=a("li"),GH=a("strong"),EVe=o("deberta-v2"),yVe=o(" \u2014 "),BB=a("a"),wVe=o("DebertaV2Tokenizer"),AVe=o(" (DeBERTa-v2 model)"),LVe=l(),Ks=a("li"),OH=a("strong"),BVe=o("distilbert"),xVe=o(" \u2014 "),xB=a("a"),kVe=o("DistilBertTokenizer"),RVe=o(" or "),kB=a("a"),SVe=o("DistilBertTokenizerFast"),PVe=o(" (DistilBERT model)"),$Ve=l(),Zs=a("li"),XH=a("strong"),IVe=o("dpr"),DVe=o(" \u2014 "),RB=a("a"),jVe=o("DPRQuestionEncoderTokenizer"),NVe=o(" or "),SB=a("a"),qVe=o("DPRQuestionEncoderTokenizerFast"),GVe=o(" (DPR model)"),OVe=l(),en=a("li"),VH=a("strong"),XVe=o("electra"),VVe=o(" \u2014 "),PB=a("a"),zVe=o("ElectraTokenizer"),WVe=o(" or "),$B=a("a"),QVe=o("ElectraTokenizerFast"),HVe=o(" (ELECTRA model)"),UVe=l(),Ag=a("li"),zH=a("strong"),JVe=o("flaubert"),YVe=o(" \u2014 "),IB=a("a"),KVe=o("FlaubertTokenizer"),ZVe=o(" (FlauBERT model)"),eze=l(),on=a("li"),WH=a("strong"),oze=o("fnet"),rze=o(" \u2014 "),DB=a("a"),tze=o("FNetTokenizer"),aze=o(" or "),jB=a("a"),sze=o("FNetTokenizerFast"),nze=o(" (FNet model)"),lze=l(),Lg=a("li"),QH=a("strong"),ize=o("fsmt"),dze=o(" \u2014 "),NB=a("a"),cze=o("FSMTTokenizer"),mze=o(" (FairSeq Machine-Translation model)"),fze=l(),rn=a("li"),HH=a("strong"),gze=o("funnel"),hze=o(" \u2014 "),qB=a("a"),uze=o("FunnelTokenizer"),pze=o(" or "),GB=a("a"),_ze=o("FunnelTokenizerFast"),bze=o(" (Funnel Transformer model)"),vze=l(),tn=a("li"),UH=a("strong"),Tze=o("gpt2"),Fze=o(" \u2014 "),OB=a("a"),Cze=o("GPT2Tokenizer"),Mze=o(" or "),XB=a("a"),Eze=o("GPT2TokenizerFast"),yze=o(" (OpenAI GPT-2 model)"),wze=l(),an=a("li"),JH=a("strong"),Aze=o("gpt_neo"),Lze=o(" \u2014 "),VB=a("a"),Bze=o("GPT2Tokenizer"),xze=o(" or "),zB=a("a"),kze=o("GPT2TokenizerFast"),Rze=o(" (GPT Neo model)"),Sze=l(),sn=a("li"),YH=a("strong"),Pze=o("herbert"),$ze=o(" \u2014 "),WB=a("a"),Ize=o("HerbertTokenizer"),Dze=o(" or "),QB=a("a"),jze=o("HerbertTokenizerFast"),Nze=o(" (HerBERT model)"),qze=l(),Bg=a("li"),KH=a("strong"),Gze=o("hubert"),Oze=o(" \u2014 "),HB=a("a"),Xze=o("Wav2Vec2CTCTokenizer"),Vze=o(" (Hubert model)"),zze=l(),nn=a("li"),ZH=a("strong"),Wze=o("ibert"),Qze=o(" \u2014 "),UB=a("a"),Hze=o("RobertaTokenizer"),Uze=o(" or "),JB=a("a"),Jze=o("RobertaTokenizerFast"),Yze=o(" (I-BERT model)"),Kze=l(),ln=a("li"),eU=a("strong"),Zze=o("layoutlm"),eWe=o(" \u2014 "),YB=a("a"),oWe=o("LayoutLMTokenizer"),rWe=o(" or "),KB=a("a"),tWe=o("LayoutLMTokenizerFast"),aWe=o(" (LayoutLM model)"),sWe=l(),dn=a("li"),oU=a("strong"),nWe=o("layoutlmv2"),lWe=o(" \u2014 "),ZB=a("a"),iWe=o("LayoutLMv2Tokenizer"),dWe=o(" or "),ex=a("a"),cWe=o("LayoutLMv2TokenizerFast"),mWe=o(" (LayoutLMv2 model)"),fWe=l(),cn=a("li"),rU=a("strong"),gWe=o("layoutxlm"),hWe=o(" \u2014 "),ox=a("a"),uWe=o("LayoutXLMTokenizer"),pWe=o(" or "),rx=a("a"),_We=o("LayoutXLMTokenizerFast"),bWe=o(" (LayoutXLM model)"),vWe=l(),mn=a("li"),tU=a("strong"),TWe=o("led"),FWe=o(" \u2014 "),tx=a("a"),CWe=o("LEDTokenizer"),MWe=o(" or "),ax=a("a"),EWe=o("LEDTokenizerFast"),yWe=o(" (LED model)"),wWe=l(),fn=a("li"),aU=a("strong"),AWe=o("longformer"),LWe=o(" \u2014 "),sx=a("a"),BWe=o("LongformerTokenizer"),xWe=o(" or "),nx=a("a"),kWe=o("LongformerTokenizerFast"),RWe=o(" (Longformer model)"),SWe=l(),xg=a("li"),sU=a("strong"),PWe=o("luke"),$We=o(" \u2014 "),lx=a("a"),IWe=o("LukeTokenizer"),DWe=o(" (LUKE model)"),jWe=l(),gn=a("li"),nU=a("strong"),NWe=o("lxmert"),qWe=o(" \u2014 "),ix=a("a"),GWe=o("LxmertTokenizer"),OWe=o(" or "),dx=a("a"),XWe=o("LxmertTokenizerFast"),VWe=o(" (LXMERT model)"),zWe=l(),kg=a("li"),lU=a("strong"),WWe=o("m2m_100"),QWe=o(" \u2014 "),cx=a("a"),HWe=o("M2M100Tokenizer"),UWe=o(" (M2M100 model)"),JWe=l(),Rg=a("li"),iU=a("strong"),YWe=o("marian"),KWe=o(" \u2014 "),mx=a("a"),ZWe=o("MarianTokenizer"),eQe=o(" (Marian model)"),oQe=l(),hn=a("li"),dU=a("strong"),rQe=o("mbart"),tQe=o(" \u2014 "),fx=a("a"),aQe=o("MBartTokenizer"),sQe=o(" or "),gx=a("a"),nQe=o("MBartTokenizerFast"),lQe=o(" (mBART model)"),iQe=l(),un=a("li"),cU=a("strong"),dQe=o("mbart50"),cQe=o(" \u2014 "),hx=a("a"),mQe=o("MBart50Tokenizer"),fQe=o(" or "),ux=a("a"),gQe=o("MBart50TokenizerFast"),hQe=o(" (mBART-50 model)"),uQe=l(),Sg=a("li"),mU=a("strong"),pQe=o("mluke"),_Qe=o(" \u2014 "),px=a("a"),bQe=o("MLukeTokenizer"),vQe=o(" (mLUKE model)"),TQe=l(),pn=a("li"),fU=a("strong"),FQe=o("mobilebert"),CQe=o(" \u2014 "),_x=a("a"),MQe=o("MobileBertTokenizer"),EQe=o(" or "),bx=a("a"),yQe=o("MobileBertTokenizerFast"),wQe=o(" (MobileBERT model)"),AQe=l(),_n=a("li"),gU=a("strong"),LQe=o("mpnet"),BQe=o(" \u2014 "),vx=a("a"),xQe=o("MPNetTokenizer"),kQe=o(" or "),Tx=a("a"),RQe=o("MPNetTokenizerFast"),SQe=o(" (MPNet model)"),PQe=l(),bn=a("li"),hU=a("strong"),$Qe=o("mt5"),IQe=o(" \u2014 "),Fx=a("a"),DQe=o("MT5Tokenizer"),jQe=o(" or "),Cx=a("a"),NQe=o("MT5TokenizerFast"),qQe=o(" (mT5 model)"),GQe=l(),vn=a("li"),uU=a("strong"),OQe=o("openai-gpt"),XQe=o(" \u2014 "),Mx=a("a"),VQe=o("OpenAIGPTTokenizer"),zQe=o(" or "),Ex=a("a"),WQe=o("OpenAIGPTTokenizerFast"),QQe=o(" (OpenAI GPT model)"),HQe=l(),Tn=a("li"),pU=a("strong"),UQe=o("pegasus"),JQe=o(" \u2014 "),yx=a("a"),YQe=o("PegasusTokenizer"),KQe=o(" or "),wx=a("a"),ZQe=o("PegasusTokenizerFast"),eHe=o(" (Pegasus model)"),oHe=l(),Pg=a("li"),_U=a("strong"),rHe=o("perceiver"),tHe=o(" \u2014 "),Ax=a("a"),aHe=o("PerceiverTokenizer"),sHe=o(" (Perceiver model)"),nHe=l(),$g=a("li"),bU=a("strong"),lHe=o("phobert"),iHe=o(" \u2014 "),Lx=a("a"),dHe=o("PhobertTokenizer"),cHe=o(" (PhoBERT model)"),mHe=l(),Ig=a("li"),vU=a("strong"),fHe=o("plbart"),gHe=o(" \u2014 "),Bx=a("a"),hHe=o("PLBartTokenizer"),uHe=o(" (PLBart model)"),pHe=l(),Dg=a("li"),TU=a("strong"),_He=o("prophetnet"),bHe=o(" \u2014 "),xx=a("a"),vHe=o("ProphetNetTokenizer"),THe=o(" (ProphetNet model)"),FHe=l(),Fn=a("li"),FU=a("strong"),CHe=o("qdqbert"),MHe=o(" \u2014 "),kx=a("a"),EHe=o("BertTokenizer"),yHe=o(" or "),Rx=a("a"),wHe=o("BertTokenizerFast"),AHe=o(" (QDQBert model)"),LHe=l(),jg=a("li"),CU=a("strong"),BHe=o("rag"),xHe=o(" \u2014 "),Sx=a("a"),kHe=o("RagTokenizer"),RHe=o(" (RAG model)"),SHe=l(),Cn=a("li"),MU=a("strong"),PHe=o("reformer"),$He=o(" \u2014 "),Px=a("a"),IHe=o("ReformerTokenizer"),DHe=o(" or "),$x=a("a"),jHe=o("ReformerTokenizerFast"),NHe=o(" (Reformer model)"),qHe=l(),Mn=a("li"),EU=a("strong"),GHe=o("rembert"),OHe=o(" \u2014 "),Ix=a("a"),XHe=o("RemBertTokenizer"),VHe=o(" or "),Dx=a("a"),zHe=o("RemBertTokenizerFast"),WHe=o(" (RemBERT model)"),QHe=l(),En=a("li"),yU=a("strong"),HHe=o("retribert"),UHe=o(" \u2014 "),jx=a("a"),JHe=o("RetriBertTokenizer"),YHe=o(" or "),Nx=a("a"),KHe=o("RetriBertTokenizerFast"),ZHe=o(" (RetriBERT model)"),eUe=l(),yn=a("li"),wU=a("strong"),oUe=o("roberta"),rUe=o(" \u2014 "),qx=a("a"),tUe=o("RobertaTokenizer"),aUe=o(" or "),Gx=a("a"),sUe=o("RobertaTokenizerFast"),nUe=o(" (RoBERTa model)"),lUe=l(),wn=a("li"),AU=a("strong"),iUe=o("roformer"),dUe=o(" \u2014 "),Ox=a("a"),cUe=o("RoFormerTokenizer"),mUe=o(" or "),Xx=a("a"),fUe=o("RoFormerTokenizerFast"),gUe=o(" (RoFormer model)"),hUe=l(),Ng=a("li"),LU=a("strong"),uUe=o("speech_to_text"),pUe=o(" \u2014 "),Vx=a("a"),_Ue=o("Speech2TextTokenizer"),bUe=o(" (Speech2Text model)"),vUe=l(),qg=a("li"),BU=a("strong"),TUe=o("speech_to_text_2"),FUe=o(" \u2014 "),zx=a("a"),CUe=o("Speech2Text2Tokenizer"),MUe=o(" (Speech2Text2 model)"),EUe=l(),An=a("li"),xU=a("strong"),yUe=o("splinter"),wUe=o(" \u2014 "),Wx=a("a"),AUe=o("SplinterTokenizer"),LUe=o(" or "),Qx=a("a"),BUe=o("SplinterTokenizerFast"),xUe=o(" (Splinter model)"),kUe=l(),Ln=a("li"),kU=a("strong"),RUe=o("squeezebert"),SUe=o(" \u2014 "),Hx=a("a"),PUe=o("SqueezeBertTokenizer"),$Ue=o(" or "),Ux=a("a"),IUe=o("SqueezeBertTokenizerFast"),DUe=o(" (SqueezeBERT model)"),jUe=l(),Bn=a("li"),RU=a("strong"),NUe=o("t5"),qUe=o(" \u2014 "),Jx=a("a"),GUe=o("T5Tokenizer"),OUe=o(" or "),Yx=a("a"),XUe=o("T5TokenizerFast"),VUe=o(" (T5 model)"),zUe=l(),Gg=a("li"),SU=a("strong"),WUe=o("tapas"),QUe=o(" \u2014 "),Kx=a("a"),HUe=o("TapasTokenizer"),UUe=o(" (TAPAS model)"),JUe=l(),Og=a("li"),PU=a("strong"),YUe=o("transfo-xl"),KUe=o(" \u2014 "),Zx=a("a"),ZUe=o("TransfoXLTokenizer"),eJe=o(" (Transformer-XL model)"),oJe=l(),Xg=a("li"),$U=a("strong"),rJe=o("wav2vec2"),tJe=o(" \u2014 "),ek=a("a"),aJe=o("Wav2Vec2CTCTokenizer"),sJe=o(" (Wav2Vec2 model)"),nJe=l(),Vg=a("li"),IU=a("strong"),lJe=o("wav2vec2_phoneme"),iJe=o(" \u2014 "),ok=a("a"),dJe=o("Wav2Vec2PhonemeCTCTokenizer"),cJe=o(" (Wav2Vec2Phoneme model)"),mJe=l(),xn=a("li"),DU=a("strong"),fJe=o("xglm"),gJe=o(" \u2014 "),rk=a("a"),hJe=o("XGLMTokenizer"),uJe=o(" or "),tk=a("a"),pJe=o("XGLMTokenizerFast"),_Je=o(" (XGLM model)"),bJe=l(),zg=a("li"),jU=a("strong"),vJe=o("xlm"),TJe=o(" \u2014 "),ak=a("a"),FJe=o("XLMTokenizer"),CJe=o(" (XLM model)"),MJe=l(),Wg=a("li"),NU=a("strong"),EJe=o("xlm-prophetnet"),yJe=o(" \u2014 "),sk=a("a"),wJe=o("XLMProphetNetTokenizer"),AJe=o(" (XLMProphetNet model)"),LJe=l(),kn=a("li"),qU=a("strong"),BJe=o("xlm-roberta"),xJe=o(" \u2014 "),nk=a("a"),kJe=o("XLMRobertaTokenizer"),RJe=o(" or "),lk=a("a"),SJe=o("XLMRobertaTokenizerFast"),PJe=o(" (XLM-RoBERTa model)"),$Je=l(),Rn=a("li"),GU=a("strong"),IJe=o("xlnet"),DJe=o(" \u2014 "),ik=a("a"),jJe=o("XLNetTokenizer"),NJe=o(" or "),dk=a("a"),qJe=o("XLNetTokenizerFast"),GJe=o(" (XLNet model)"),OJe=l(),OU=a("p"),XJe=o("Examples:"),VJe=l(),m(j5.$$.fragment),zJe=l(),Qg=a("div"),m(N5.$$.fragment),WJe=l(),XU=a("p"),QJe=o("Register a new tokenizer in this mapping."),w9e=l(),Ni=a("h2"),Hg=a("a"),VU=a("span"),m(q5.$$.fragment),HJe=l(),zU=a("span"),UJe=o("AutoFeatureExtractor"),A9e=l(),zo=a("div"),m(G5.$$.fragment),JJe=l(),O5=a("p"),YJe=o(`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),ck=a("a"),KJe=o("AutoFeatureExtractor.from_pretrained()"),ZJe=o(" class method."),eYe=l(),X5=a("p"),oYe=o("This class cannot be instantiated directly using "),WU=a("code"),rYe=o("__init__()"),tYe=o(" (throws an error)."),aYe=l(),xe=a("div"),m(V5.$$.fragment),sYe=l(),QU=a("p"),nYe=o("Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),lYe=l(),ja=a("p"),iYe=o("The feature extractor class to instantiate is selected based on the "),HU=a("code"),dYe=o("model_type"),cYe=o(` property of the config object
(either passed as an argument or loaded from `),UU=a("code"),mYe=o("pretrained_model_name_or_path"),fYe=o(` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),JU=a("code"),gYe=o("pretrained_model_name_or_path"),hYe=o(":"),uYe=l(),ne=a("ul"),Ug=a("li"),YU=a("strong"),pYe=o("beit"),_Ye=o(" \u2014 "),mk=a("a"),bYe=o("BeitFeatureExtractor"),vYe=o(" (BEiT model)"),TYe=l(),Jg=a("li"),KU=a("strong"),FYe=o("clip"),CYe=o(" \u2014 "),fk=a("a"),MYe=o("CLIPFeatureExtractor"),EYe=o(" (CLIP model)"),yYe=l(),Yg=a("li"),ZU=a("strong"),wYe=o("convnext"),AYe=o(" \u2014 "),gk=a("a"),LYe=o("ConvNextFeatureExtractor"),BYe=o(" (ConvNext model)"),xYe=l(),Kg=a("li"),eJ=a("strong"),kYe=o("deit"),RYe=o(" \u2014 "),hk=a("a"),SYe=o("DeiTFeatureExtractor"),PYe=o(" (DeiT model)"),$Ye=l(),Zg=a("li"),oJ=a("strong"),IYe=o("detr"),DYe=o(" \u2014 "),uk=a("a"),jYe=o("DetrFeatureExtractor"),NYe=o(" (DETR model)"),qYe=l(),eh=a("li"),rJ=a("strong"),GYe=o("hubert"),OYe=o(" \u2014 "),pk=a("a"),XYe=o("Wav2Vec2FeatureExtractor"),VYe=o(" (Hubert model)"),zYe=l(),oh=a("li"),tJ=a("strong"),WYe=o("layoutlmv2"),QYe=o(" \u2014 "),_k=a("a"),HYe=o("LayoutLMv2FeatureExtractor"),UYe=o(" (LayoutLMv2 model)"),JYe=l(),rh=a("li"),aJ=a("strong"),YYe=o("perceiver"),KYe=o(" \u2014 "),bk=a("a"),ZYe=o("PerceiverFeatureExtractor"),eKe=o(" (Perceiver model)"),oKe=l(),th=a("li"),sJ=a("strong"),rKe=o("poolformer"),tKe=o(" \u2014 "),vk=a("a"),aKe=o("PoolFormerFeatureExtractor"),sKe=o(" (PoolFormer model)"),nKe=l(),ah=a("li"),nJ=a("strong"),lKe=o("segformer"),iKe=o(" \u2014 "),Tk=a("a"),dKe=o("SegformerFeatureExtractor"),cKe=o(" (SegFormer model)"),mKe=l(),sh=a("li"),lJ=a("strong"),fKe=o("speech_to_text"),gKe=o(" \u2014 "),Fk=a("a"),hKe=o("Speech2TextFeatureExtractor"),uKe=o(" (Speech2Text model)"),pKe=l(),nh=a("li"),iJ=a("strong"),_Ke=o("swin"),bKe=o(" \u2014 "),Ck=a("a"),vKe=o("ViTFeatureExtractor"),TKe=o(" (Swin model)"),FKe=l(),lh=a("li"),dJ=a("strong"),CKe=o("vit"),MKe=o(" \u2014 "),Mk=a("a"),EKe=o("ViTFeatureExtractor"),yKe=o(" (ViT model)"),wKe=l(),ih=a("li"),cJ=a("strong"),AKe=o("vit_mae"),LKe=o(" \u2014 "),Ek=a("a"),BKe=o("ViTFeatureExtractor"),xKe=o(" (ViTMAE model)"),kKe=l(),dh=a("li"),mJ=a("strong"),RKe=o("wav2vec2"),SKe=o(" \u2014 "),yk=a("a"),PKe=o("Wav2Vec2FeatureExtractor"),$Ke=o(" (Wav2Vec2 model)"),IKe=l(),m(ch.$$.fragment),DKe=l(),fJ=a("p"),jKe=o("Examples:"),NKe=l(),m(z5.$$.fragment),qKe=l(),mh=a("div"),m(W5.$$.fragment),GKe=l(),gJ=a("p"),OKe=o("Register a new feature extractor for this class."),L9e=l(),qi=a("h2"),fh=a("a"),hJ=a("span"),m(Q5.$$.fragment),XKe=l(),uJ=a("span"),VKe=o("AutoProcessor"),B9e=l(),Wo=a("div"),m(H5.$$.fragment),zKe=l(),U5=a("p"),WKe=o(`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),wk=a("a"),QKe=o("AutoProcessor.from_pretrained()"),HKe=o(" class method."),UKe=l(),J5=a("p"),JKe=o("This class cannot be instantiated directly using "),pJ=a("code"),YKe=o("__init__()"),KKe=o(" (throws an error)."),ZKe=l(),ke=a("div"),m(Y5.$$.fragment),eZe=l(),_J=a("p"),oZe=o("Instantiate one of the processor classes of the library from a pretrained model vocabulary."),rZe=l(),Gi=a("p"),tZe=o("The processor class to instantiate is selected based on the "),bJ=a("code"),aZe=o("model_type"),sZe=o(` property of the config object (either
passed as an argument or loaded from `),vJ=a("code"),nZe=o("pretrained_model_name_or_path"),lZe=o(" if possible):"),iZe=l(),we=a("ul"),gh=a("li"),TJ=a("strong"),dZe=o("clip"),cZe=o(" \u2014 "),Ak=a("a"),mZe=o("CLIPProcessor"),fZe=o(" (CLIP model)"),gZe=l(),hh=a("li"),FJ=a("strong"),hZe=o("layoutlmv2"),uZe=o(" \u2014 "),Lk=a("a"),pZe=o("LayoutLMv2Processor"),_Ze=o(" (LayoutLMv2 model)"),bZe=l(),uh=a("li"),CJ=a("strong"),vZe=o("layoutxlm"),TZe=o(" \u2014 "),Bk=a("a"),FZe=o("LayoutXLMProcessor"),CZe=o(" (LayoutXLM model)"),MZe=l(),ph=a("li"),MJ=a("strong"),EZe=o("speech_to_text"),yZe=o(" \u2014 "),xk=a("a"),wZe=o("Speech2TextProcessor"),AZe=o(" (Speech2Text model)"),LZe=l(),_h=a("li"),EJ=a("strong"),BZe=o("speech_to_text_2"),xZe=o(" \u2014 "),kk=a("a"),kZe=o("Speech2Text2Processor"),RZe=o(" (Speech2Text2 model)"),SZe=l(),bh=a("li"),yJ=a("strong"),PZe=o("trocr"),$Ze=o(" \u2014 "),Rk=a("a"),IZe=o("TrOCRProcessor"),DZe=o(" (TrOCR model)"),jZe=l(),vh=a("li"),wJ=a("strong"),NZe=o("vision-text-dual-encoder"),qZe=o(" \u2014 "),Sk=a("a"),GZe=o("VisionTextDualEncoderProcessor"),OZe=o(" (VisionTextDualEncoder model)"),XZe=l(),Th=a("li"),AJ=a("strong"),VZe=o("wav2vec2"),zZe=o(" \u2014 "),Pk=a("a"),WZe=o("Wav2Vec2Processor"),QZe=o(" (Wav2Vec2 model)"),HZe=l(),m(Fh.$$.fragment),UZe=l(),LJ=a("p"),JZe=o("Examples:"),YZe=l(),m(K5.$$.fragment),KZe=l(),Ch=a("div"),m(Z5.$$.fragment),ZZe=l(),BJ=a("p"),eeo=o("Register a new processor for this class."),x9e=l(),Oi=a("h2"),Mh=a("a"),xJ=a("span"),m(ey.$$.fragment),oeo=l(),kJ=a("span"),reo=o("AutoModel"),k9e=l(),Qo=a("div"),m(oy.$$.fragment),teo=l(),Xi=a("p"),aeo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),RJ=a("code"),seo=o("from_pretrained()"),neo=o("class method or the "),SJ=a("code"),leo=o("from_config()"),ieo=o(`class
method.`),deo=l(),ry=a("p"),ceo=o("This class cannot be instantiated directly using "),PJ=a("code"),meo=o("__init__()"),feo=o(" (throws an error)."),geo=l(),qr=a("div"),m(ty.$$.fragment),heo=l(),$J=a("p"),ueo=o("Instantiates one of the base model classes of the library from a configuration."),peo=l(),Vi=a("p"),_eo=o(`Note:
Loading a model from its configuration file does `),IJ=a("strong"),beo=o("not"),veo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),DJ=a("code"),Teo=o("from_pretrained()"),Feo=o("to load the model weights."),Ceo=l(),jJ=a("p"),Meo=o("Examples:"),Eeo=l(),m(ay.$$.fragment),yeo=l(),Re=a("div"),m(sy.$$.fragment),weo=l(),NJ=a("p"),Aeo=o("Instantiate one of the base model classes of the library from a pretrained model."),Leo=l(),Na=a("p"),Beo=o("The model class to instantiate is selected based on the "),qJ=a("code"),xeo=o("model_type"),keo=o(` property of the config object (either
passed as an argument or loaded from `),GJ=a("code"),Reo=o("pretrained_model_name_or_path"),Seo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),OJ=a("code"),Peo=o("pretrained_model_name_or_path"),$eo=o(":"),Ieo=l(),F=a("ul"),Eh=a("li"),XJ=a("strong"),Deo=o("albert"),jeo=o(" \u2014 "),$k=a("a"),Neo=o("AlbertModel"),qeo=o(" (ALBERT model)"),Geo=l(),yh=a("li"),VJ=a("strong"),Oeo=o("bart"),Xeo=o(" \u2014 "),Ik=a("a"),Veo=o("BartModel"),zeo=o(" (BART model)"),Weo=l(),wh=a("li"),zJ=a("strong"),Qeo=o("beit"),Heo=o(" \u2014 "),Dk=a("a"),Ueo=o("BeitModel"),Jeo=o(" (BEiT model)"),Yeo=l(),Ah=a("li"),WJ=a("strong"),Keo=o("bert"),Zeo=o(" \u2014 "),jk=a("a"),eoo=o("BertModel"),ooo=o(" (BERT model)"),roo=l(),Lh=a("li"),QJ=a("strong"),too=o("bert-generation"),aoo=o(" \u2014 "),Nk=a("a"),soo=o("BertGenerationEncoder"),noo=o(" (Bert Generation model)"),loo=l(),Bh=a("li"),HJ=a("strong"),ioo=o("big_bird"),doo=o(" \u2014 "),qk=a("a"),coo=o("BigBirdModel"),moo=o(" (BigBird model)"),foo=l(),xh=a("li"),UJ=a("strong"),goo=o("bigbird_pegasus"),hoo=o(" \u2014 "),Gk=a("a"),uoo=o("BigBirdPegasusModel"),poo=o(" (BigBirdPegasus model)"),_oo=l(),kh=a("li"),JJ=a("strong"),boo=o("blenderbot"),voo=o(" \u2014 "),Ok=a("a"),Too=o("BlenderbotModel"),Foo=o(" (Blenderbot model)"),Coo=l(),Rh=a("li"),YJ=a("strong"),Moo=o("blenderbot-small"),Eoo=o(" \u2014 "),Xk=a("a"),yoo=o("BlenderbotSmallModel"),woo=o(" (BlenderbotSmall model)"),Aoo=l(),Sh=a("li"),KJ=a("strong"),Loo=o("camembert"),Boo=o(" \u2014 "),Vk=a("a"),xoo=o("CamembertModel"),koo=o(" (CamemBERT model)"),Roo=l(),Ph=a("li"),ZJ=a("strong"),Soo=o("canine"),Poo=o(" \u2014 "),zk=a("a"),$oo=o("CanineModel"),Ioo=o(" (Canine model)"),Doo=l(),$h=a("li"),eY=a("strong"),joo=o("clip"),Noo=o(" \u2014 "),Wk=a("a"),qoo=o("CLIPModel"),Goo=o(" (CLIP model)"),Ooo=l(),Ih=a("li"),oY=a("strong"),Xoo=o("convbert"),Voo=o(" \u2014 "),Qk=a("a"),zoo=o("ConvBertModel"),Woo=o(" (ConvBERT model)"),Qoo=l(),Dh=a("li"),rY=a("strong"),Hoo=o("convnext"),Uoo=o(" \u2014 "),Hk=a("a"),Joo=o("ConvNextModel"),Yoo=o(" (ConvNext model)"),Koo=l(),jh=a("li"),tY=a("strong"),Zoo=o("ctrl"),ero=o(" \u2014 "),Uk=a("a"),oro=o("CTRLModel"),rro=o(" (CTRL model)"),tro=l(),Nh=a("li"),aY=a("strong"),aro=o("data2vec-audio"),sro=o(" \u2014 "),Jk=a("a"),nro=o("Data2VecAudioModel"),lro=o(" (Data2VecAudio model)"),iro=l(),qh=a("li"),sY=a("strong"),dro=o("data2vec-text"),cro=o(" \u2014 "),Yk=a("a"),mro=o("Data2VecTextModel"),fro=o(" (Data2VecText model)"),gro=l(),Gh=a("li"),nY=a("strong"),hro=o("deberta"),uro=o(" \u2014 "),Kk=a("a"),pro=o("DebertaModel"),_ro=o(" (DeBERTa model)"),bro=l(),Oh=a("li"),lY=a("strong"),vro=o("deberta-v2"),Tro=o(" \u2014 "),Zk=a("a"),Fro=o("DebertaV2Model"),Cro=o(" (DeBERTa-v2 model)"),Mro=l(),Xh=a("li"),iY=a("strong"),Ero=o("deit"),yro=o(" \u2014 "),eR=a("a"),wro=o("DeiTModel"),Aro=o(" (DeiT model)"),Lro=l(),Vh=a("li"),dY=a("strong"),Bro=o("detr"),xro=o(" \u2014 "),oR=a("a"),kro=o("DetrModel"),Rro=o(" (DETR model)"),Sro=l(),zh=a("li"),cY=a("strong"),Pro=o("distilbert"),$ro=o(" \u2014 "),rR=a("a"),Iro=o("DistilBertModel"),Dro=o(" (DistilBERT model)"),jro=l(),Wh=a("li"),mY=a("strong"),Nro=o("dpr"),qro=o(" \u2014 "),tR=a("a"),Gro=o("DPRQuestionEncoder"),Oro=o(" (DPR model)"),Xro=l(),Qh=a("li"),fY=a("strong"),Vro=o("electra"),zro=o(" \u2014 "),aR=a("a"),Wro=o("ElectraModel"),Qro=o(" (ELECTRA model)"),Hro=l(),Hh=a("li"),gY=a("strong"),Uro=o("flaubert"),Jro=o(" \u2014 "),sR=a("a"),Yro=o("FlaubertModel"),Kro=o(" (FlauBERT model)"),Zro=l(),Uh=a("li"),hY=a("strong"),eto=o("fnet"),oto=o(" \u2014 "),nR=a("a"),rto=o("FNetModel"),tto=o(" (FNet model)"),ato=l(),Jh=a("li"),uY=a("strong"),sto=o("fsmt"),nto=o(" \u2014 "),lR=a("a"),lto=o("FSMTModel"),ito=o(" (FairSeq Machine-Translation model)"),dto=l(),Sn=a("li"),pY=a("strong"),cto=o("funnel"),mto=o(" \u2014 "),iR=a("a"),fto=o("FunnelModel"),gto=o(" or "),dR=a("a"),hto=o("FunnelBaseModel"),uto=o(" (Funnel Transformer model)"),pto=l(),Yh=a("li"),_Y=a("strong"),_to=o("gpt2"),bto=o(" \u2014 "),cR=a("a"),vto=o("GPT2Model"),Tto=o(" (OpenAI GPT-2 model)"),Fto=l(),Kh=a("li"),bY=a("strong"),Cto=o("gpt_neo"),Mto=o(" \u2014 "),mR=a("a"),Eto=o("GPTNeoModel"),yto=o(" (GPT Neo model)"),wto=l(),Zh=a("li"),vY=a("strong"),Ato=o("gptj"),Lto=o(" \u2014 "),fR=a("a"),Bto=o("GPTJModel"),xto=o(" (GPT-J model)"),kto=l(),eu=a("li"),TY=a("strong"),Rto=o("hubert"),Sto=o(" \u2014 "),gR=a("a"),Pto=o("HubertModel"),$to=o(" (Hubert model)"),Ito=l(),ou=a("li"),FY=a("strong"),Dto=o("ibert"),jto=o(" \u2014 "),hR=a("a"),Nto=o("IBertModel"),qto=o(" (I-BERT model)"),Gto=l(),ru=a("li"),CY=a("strong"),Oto=o("imagegpt"),Xto=o(" \u2014 "),uR=a("a"),Vto=o("ImageGPTModel"),zto=o(" (ImageGPT model)"),Wto=l(),tu=a("li"),MY=a("strong"),Qto=o("layoutlm"),Hto=o(" \u2014 "),pR=a("a"),Uto=o("LayoutLMModel"),Jto=o(" (LayoutLM model)"),Yto=l(),au=a("li"),EY=a("strong"),Kto=o("layoutlmv2"),Zto=o(" \u2014 "),_R=a("a"),eao=o("LayoutLMv2Model"),oao=o(" (LayoutLMv2 model)"),rao=l(),su=a("li"),yY=a("strong"),tao=o("led"),aao=o(" \u2014 "),bR=a("a"),sao=o("LEDModel"),nao=o(" (LED model)"),lao=l(),nu=a("li"),wY=a("strong"),iao=o("longformer"),dao=o(" \u2014 "),vR=a("a"),cao=o("LongformerModel"),mao=o(" (Longformer model)"),fao=l(),lu=a("li"),AY=a("strong"),gao=o("luke"),hao=o(" \u2014 "),TR=a("a"),uao=o("LukeModel"),pao=o(" (LUKE model)"),_ao=l(),iu=a("li"),LY=a("strong"),bao=o("lxmert"),vao=o(" \u2014 "),FR=a("a"),Tao=o("LxmertModel"),Fao=o(" (LXMERT model)"),Cao=l(),du=a("li"),BY=a("strong"),Mao=o("m2m_100"),Eao=o(" \u2014 "),CR=a("a"),yao=o("M2M100Model"),wao=o(" (M2M100 model)"),Aao=l(),cu=a("li"),xY=a("strong"),Lao=o("marian"),Bao=o(" \u2014 "),MR=a("a"),xao=o("MarianModel"),kao=o(" (Marian model)"),Rao=l(),mu=a("li"),kY=a("strong"),Sao=o("mbart"),Pao=o(" \u2014 "),ER=a("a"),$ao=o("MBartModel"),Iao=o(" (mBART model)"),Dao=l(),fu=a("li"),RY=a("strong"),jao=o("megatron-bert"),Nao=o(" \u2014 "),yR=a("a"),qao=o("MegatronBertModel"),Gao=o(" (MegatronBert model)"),Oao=l(),gu=a("li"),SY=a("strong"),Xao=o("mobilebert"),Vao=o(" \u2014 "),wR=a("a"),zao=o("MobileBertModel"),Wao=o(" (MobileBERT model)"),Qao=l(),hu=a("li"),PY=a("strong"),Hao=o("mpnet"),Uao=o(" \u2014 "),AR=a("a"),Jao=o("MPNetModel"),Yao=o(" (MPNet model)"),Kao=l(),uu=a("li"),$Y=a("strong"),Zao=o("mt5"),eso=o(" \u2014 "),LR=a("a"),oso=o("MT5Model"),rso=o(" (mT5 model)"),tso=l(),pu=a("li"),IY=a("strong"),aso=o("nystromformer"),sso=o(" \u2014 "),BR=a("a"),nso=o("NystromformerModel"),lso=o(" (Nystromformer model)"),iso=l(),_u=a("li"),DY=a("strong"),dso=o("openai-gpt"),cso=o(" \u2014 "),xR=a("a"),mso=o("OpenAIGPTModel"),fso=o(" (OpenAI GPT model)"),gso=l(),bu=a("li"),jY=a("strong"),hso=o("pegasus"),uso=o(" \u2014 "),kR=a("a"),pso=o("PegasusModel"),_so=o(" (Pegasus model)"),bso=l(),vu=a("li"),NY=a("strong"),vso=o("perceiver"),Tso=o(" \u2014 "),RR=a("a"),Fso=o("PerceiverModel"),Cso=o(" (Perceiver model)"),Mso=l(),Tu=a("li"),qY=a("strong"),Eso=o("plbart"),yso=o(" \u2014 "),SR=a("a"),wso=o("PLBartModel"),Aso=o(" (PLBart model)"),Lso=l(),Fu=a("li"),GY=a("strong"),Bso=o("poolformer"),xso=o(" \u2014 "),PR=a("a"),kso=o("PoolFormerModel"),Rso=o(" (PoolFormer model)"),Sso=l(),Cu=a("li"),OY=a("strong"),Pso=o("prophetnet"),$so=o(" \u2014 "),$R=a("a"),Iso=o("ProphetNetModel"),Dso=o(" (ProphetNet model)"),jso=l(),Mu=a("li"),XY=a("strong"),Nso=o("qdqbert"),qso=o(" \u2014 "),IR=a("a"),Gso=o("QDQBertModel"),Oso=o(" (QDQBert model)"),Xso=l(),Eu=a("li"),VY=a("strong"),Vso=o("reformer"),zso=o(" \u2014 "),DR=a("a"),Wso=o("ReformerModel"),Qso=o(" (Reformer model)"),Hso=l(),yu=a("li"),zY=a("strong"),Uso=o("rembert"),Jso=o(" \u2014 "),jR=a("a"),Yso=o("RemBertModel"),Kso=o(" (RemBERT model)"),Zso=l(),wu=a("li"),WY=a("strong"),eno=o("retribert"),ono=o(" \u2014 "),NR=a("a"),rno=o("RetriBertModel"),tno=o(" (RetriBERT model)"),ano=l(),Au=a("li"),QY=a("strong"),sno=o("roberta"),nno=o(" \u2014 "),qR=a("a"),lno=o("RobertaModel"),ino=o(" (RoBERTa model)"),dno=l(),Lu=a("li"),HY=a("strong"),cno=o("roformer"),mno=o(" \u2014 "),GR=a("a"),fno=o("RoFormerModel"),gno=o(" (RoFormer model)"),hno=l(),Bu=a("li"),UY=a("strong"),uno=o("segformer"),pno=o(" \u2014 "),OR=a("a"),_no=o("SegformerModel"),bno=o(" (SegFormer model)"),vno=l(),xu=a("li"),JY=a("strong"),Tno=o("sew"),Fno=o(" \u2014 "),XR=a("a"),Cno=o("SEWModel"),Mno=o(" (SEW model)"),Eno=l(),ku=a("li"),YY=a("strong"),yno=o("sew-d"),wno=o(" \u2014 "),VR=a("a"),Ano=o("SEWDModel"),Lno=o(" (SEW-D model)"),Bno=l(),Ru=a("li"),KY=a("strong"),xno=o("speech_to_text"),kno=o(" \u2014 "),zR=a("a"),Rno=o("Speech2TextModel"),Sno=o(" (Speech2Text model)"),Pno=l(),Su=a("li"),ZY=a("strong"),$no=o("splinter"),Ino=o(" \u2014 "),WR=a("a"),Dno=o("SplinterModel"),jno=o(" (Splinter model)"),Nno=l(),Pu=a("li"),eK=a("strong"),qno=o("squeezebert"),Gno=o(" \u2014 "),QR=a("a"),Ono=o("SqueezeBertModel"),Xno=o(" (SqueezeBERT model)"),Vno=l(),$u=a("li"),oK=a("strong"),zno=o("swin"),Wno=o(" \u2014 "),HR=a("a"),Qno=o("SwinModel"),Hno=o(" (Swin model)"),Uno=l(),Iu=a("li"),rK=a("strong"),Jno=o("t5"),Yno=o(" \u2014 "),UR=a("a"),Kno=o("T5Model"),Zno=o(" (T5 model)"),elo=l(),Du=a("li"),tK=a("strong"),olo=o("tapas"),rlo=o(" \u2014 "),JR=a("a"),tlo=o("TapasModel"),alo=o(" (TAPAS model)"),slo=l(),ju=a("li"),aK=a("strong"),nlo=o("transfo-xl"),llo=o(" \u2014 "),YR=a("a"),ilo=o("TransfoXLModel"),dlo=o(" (Transformer-XL model)"),clo=l(),Nu=a("li"),sK=a("strong"),mlo=o("unispeech"),flo=o(" \u2014 "),KR=a("a"),glo=o("UniSpeechModel"),hlo=o(" (UniSpeech model)"),ulo=l(),qu=a("li"),nK=a("strong"),plo=o("unispeech-sat"),_lo=o(" \u2014 "),ZR=a("a"),blo=o("UniSpeechSatModel"),vlo=o(" (UniSpeechSat model)"),Tlo=l(),Gu=a("li"),lK=a("strong"),Flo=o("vilt"),Clo=o(" \u2014 "),eS=a("a"),Mlo=o("ViltModel"),Elo=o(" (ViLT model)"),ylo=l(),Ou=a("li"),iK=a("strong"),wlo=o("vision-text-dual-encoder"),Alo=o(" \u2014 "),oS=a("a"),Llo=o("VisionTextDualEncoderModel"),Blo=o(" (VisionTextDualEncoder model)"),xlo=l(),Xu=a("li"),dK=a("strong"),klo=o("visual_bert"),Rlo=o(" \u2014 "),rS=a("a"),Slo=o("VisualBertModel"),Plo=o(" (VisualBert model)"),$lo=l(),Vu=a("li"),cK=a("strong"),Ilo=o("vit"),Dlo=o(" \u2014 "),tS=a("a"),jlo=o("ViTModel"),Nlo=o(" (ViT model)"),qlo=l(),zu=a("li"),mK=a("strong"),Glo=o("vit_mae"),Olo=o(" \u2014 "),aS=a("a"),Xlo=o("ViTMAEModel"),Vlo=o(" (ViTMAE model)"),zlo=l(),Wu=a("li"),fK=a("strong"),Wlo=o("wav2vec2"),Qlo=o(" \u2014 "),sS=a("a"),Hlo=o("Wav2Vec2Model"),Ulo=o(" (Wav2Vec2 model)"),Jlo=l(),Qu=a("li"),gK=a("strong"),Ylo=o("wavlm"),Klo=o(" \u2014 "),nS=a("a"),Zlo=o("WavLMModel"),eio=o(" (WavLM model)"),oio=l(),Hu=a("li"),hK=a("strong"),rio=o("xglm"),tio=o(" \u2014 "),lS=a("a"),aio=o("XGLMModel"),sio=o(" (XGLM model)"),nio=l(),Uu=a("li"),uK=a("strong"),lio=o("xlm"),iio=o(" \u2014 "),iS=a("a"),dio=o("XLMModel"),cio=o(" (XLM model)"),mio=l(),Ju=a("li"),pK=a("strong"),fio=o("xlm-prophetnet"),gio=o(" \u2014 "),dS=a("a"),hio=o("XLMProphetNetModel"),uio=o(" (XLMProphetNet model)"),pio=l(),Yu=a("li"),_K=a("strong"),_io=o("xlm-roberta"),bio=o(" \u2014 "),cS=a("a"),vio=o("XLMRobertaModel"),Tio=o(" (XLM-RoBERTa model)"),Fio=l(),Ku=a("li"),bK=a("strong"),Cio=o("xlm-roberta-xl"),Mio=o(" \u2014 "),mS=a("a"),Eio=o("XLMRobertaXLModel"),yio=o(" (XLM-RoBERTa-XL model)"),wio=l(),Zu=a("li"),vK=a("strong"),Aio=o("xlnet"),Lio=o(" \u2014 "),fS=a("a"),Bio=o("XLNetModel"),xio=o(" (XLNet model)"),kio=l(),ep=a("li"),TK=a("strong"),Rio=o("yoso"),Sio=o(" \u2014 "),gS=a("a"),Pio=o("YosoModel"),$io=o(" (YOSO model)"),Iio=l(),op=a("p"),Dio=o("The model is set in evaluation mode by default using "),FK=a("code"),jio=o("model.eval()"),Nio=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),CK=a("code"),qio=o("model.train()"),Gio=l(),MK=a("p"),Oio=o("Examples:"),Xio=l(),m(ny.$$.fragment),R9e=l(),zi=a("h2"),rp=a("a"),EK=a("span"),m(ly.$$.fragment),Vio=l(),yK=a("span"),zio=o("AutoModelForPreTraining"),S9e=l(),Ho=a("div"),m(iy.$$.fragment),Wio=l(),Wi=a("p"),Qio=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),wK=a("code"),Hio=o("from_pretrained()"),Uio=o("class method or the "),AK=a("code"),Jio=o("from_config()"),Yio=o(`class
method.`),Kio=l(),dy=a("p"),Zio=o("This class cannot be instantiated directly using "),LK=a("code"),edo=o("__init__()"),odo=o(" (throws an error)."),rdo=l(),Gr=a("div"),m(cy.$$.fragment),tdo=l(),BK=a("p"),ado=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),sdo=l(),Qi=a("p"),ndo=o(`Note:
Loading a model from its configuration file does `),xK=a("strong"),ldo=o("not"),ido=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),kK=a("code"),ddo=o("from_pretrained()"),cdo=o("to load the model weights."),mdo=l(),RK=a("p"),fdo=o("Examples:"),gdo=l(),m(my.$$.fragment),hdo=l(),Se=a("div"),m(fy.$$.fragment),udo=l(),SK=a("p"),pdo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),_do=l(),qa=a("p"),bdo=o("The model class to instantiate is selected based on the "),PK=a("code"),vdo=o("model_type"),Tdo=o(` property of the config object (either
passed as an argument or loaded from `),$K=a("code"),Fdo=o("pretrained_model_name_or_path"),Cdo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IK=a("code"),Mdo=o("pretrained_model_name_or_path"),Edo=o(":"),ydo=l(),k=a("ul"),tp=a("li"),DK=a("strong"),wdo=o("albert"),Ado=o(" \u2014 "),hS=a("a"),Ldo=o("AlbertForPreTraining"),Bdo=o(" (ALBERT model)"),xdo=l(),ap=a("li"),jK=a("strong"),kdo=o("bart"),Rdo=o(" \u2014 "),uS=a("a"),Sdo=o("BartForConditionalGeneration"),Pdo=o(" (BART model)"),$do=l(),sp=a("li"),NK=a("strong"),Ido=o("bert"),Ddo=o(" \u2014 "),pS=a("a"),jdo=o("BertForPreTraining"),Ndo=o(" (BERT model)"),qdo=l(),np=a("li"),qK=a("strong"),Gdo=o("big_bird"),Odo=o(" \u2014 "),_S=a("a"),Xdo=o("BigBirdForPreTraining"),Vdo=o(" (BigBird model)"),zdo=l(),lp=a("li"),GK=a("strong"),Wdo=o("camembert"),Qdo=o(" \u2014 "),bS=a("a"),Hdo=o("CamembertForMaskedLM"),Udo=o(" (CamemBERT model)"),Jdo=l(),ip=a("li"),OK=a("strong"),Ydo=o("ctrl"),Kdo=o(" \u2014 "),vS=a("a"),Zdo=o("CTRLLMHeadModel"),eco=o(" (CTRL model)"),oco=l(),dp=a("li"),XK=a("strong"),rco=o("data2vec-text"),tco=o(" \u2014 "),TS=a("a"),aco=o("Data2VecTextForMaskedLM"),sco=o(" (Data2VecText model)"),nco=l(),cp=a("li"),VK=a("strong"),lco=o("deberta"),ico=o(" \u2014 "),FS=a("a"),dco=o("DebertaForMaskedLM"),cco=o(" (DeBERTa model)"),mco=l(),mp=a("li"),zK=a("strong"),fco=o("deberta-v2"),gco=o(" \u2014 "),CS=a("a"),hco=o("DebertaV2ForMaskedLM"),uco=o(" (DeBERTa-v2 model)"),pco=l(),fp=a("li"),WK=a("strong"),_co=o("distilbert"),bco=o(" \u2014 "),MS=a("a"),vco=o("DistilBertForMaskedLM"),Tco=o(" (DistilBERT model)"),Fco=l(),gp=a("li"),QK=a("strong"),Cco=o("electra"),Mco=o(" \u2014 "),ES=a("a"),Eco=o("ElectraForPreTraining"),yco=o(" (ELECTRA model)"),wco=l(),hp=a("li"),HK=a("strong"),Aco=o("flaubert"),Lco=o(" \u2014 "),yS=a("a"),Bco=o("FlaubertWithLMHeadModel"),xco=o(" (FlauBERT model)"),kco=l(),up=a("li"),UK=a("strong"),Rco=o("fnet"),Sco=o(" \u2014 "),wS=a("a"),Pco=o("FNetForPreTraining"),$co=o(" (FNet model)"),Ico=l(),pp=a("li"),JK=a("strong"),Dco=o("fsmt"),jco=o(" \u2014 "),AS=a("a"),Nco=o("FSMTForConditionalGeneration"),qco=o(" (FairSeq Machine-Translation model)"),Gco=l(),_p=a("li"),YK=a("strong"),Oco=o("funnel"),Xco=o(" \u2014 "),LS=a("a"),Vco=o("FunnelForPreTraining"),zco=o(" (Funnel Transformer model)"),Wco=l(),bp=a("li"),KK=a("strong"),Qco=o("gpt2"),Hco=o(" \u2014 "),BS=a("a"),Uco=o("GPT2LMHeadModel"),Jco=o(" (OpenAI GPT-2 model)"),Yco=l(),vp=a("li"),ZK=a("strong"),Kco=o("ibert"),Zco=o(" \u2014 "),xS=a("a"),emo=o("IBertForMaskedLM"),omo=o(" (I-BERT model)"),rmo=l(),Tp=a("li"),eZ=a("strong"),tmo=o("layoutlm"),amo=o(" \u2014 "),kS=a("a"),smo=o("LayoutLMForMaskedLM"),nmo=o(" (LayoutLM model)"),lmo=l(),Fp=a("li"),oZ=a("strong"),imo=o("longformer"),dmo=o(" \u2014 "),RS=a("a"),cmo=o("LongformerForMaskedLM"),mmo=o(" (Longformer model)"),fmo=l(),Cp=a("li"),rZ=a("strong"),gmo=o("lxmert"),hmo=o(" \u2014 "),SS=a("a"),umo=o("LxmertForPreTraining"),pmo=o(" (LXMERT model)"),_mo=l(),Mp=a("li"),tZ=a("strong"),bmo=o("megatron-bert"),vmo=o(" \u2014 "),PS=a("a"),Tmo=o("MegatronBertForPreTraining"),Fmo=o(" (MegatronBert model)"),Cmo=l(),Ep=a("li"),aZ=a("strong"),Mmo=o("mobilebert"),Emo=o(" \u2014 "),$S=a("a"),ymo=o("MobileBertForPreTraining"),wmo=o(" (MobileBERT model)"),Amo=l(),yp=a("li"),sZ=a("strong"),Lmo=o("mpnet"),Bmo=o(" \u2014 "),IS=a("a"),xmo=o("MPNetForMaskedLM"),kmo=o(" (MPNet model)"),Rmo=l(),wp=a("li"),nZ=a("strong"),Smo=o("openai-gpt"),Pmo=o(" \u2014 "),DS=a("a"),$mo=o("OpenAIGPTLMHeadModel"),Imo=o(" (OpenAI GPT model)"),Dmo=l(),Ap=a("li"),lZ=a("strong"),jmo=o("retribert"),Nmo=o(" \u2014 "),jS=a("a"),qmo=o("RetriBertModel"),Gmo=o(" (RetriBERT model)"),Omo=l(),Lp=a("li"),iZ=a("strong"),Xmo=o("roberta"),Vmo=o(" \u2014 "),NS=a("a"),zmo=o("RobertaForMaskedLM"),Wmo=o(" (RoBERTa model)"),Qmo=l(),Bp=a("li"),dZ=a("strong"),Hmo=o("squeezebert"),Umo=o(" \u2014 "),qS=a("a"),Jmo=o("SqueezeBertForMaskedLM"),Ymo=o(" (SqueezeBERT model)"),Kmo=l(),xp=a("li"),cZ=a("strong"),Zmo=o("t5"),efo=o(" \u2014 "),GS=a("a"),ofo=o("T5ForConditionalGeneration"),rfo=o(" (T5 model)"),tfo=l(),kp=a("li"),mZ=a("strong"),afo=o("tapas"),sfo=o(" \u2014 "),OS=a("a"),nfo=o("TapasForMaskedLM"),lfo=o(" (TAPAS model)"),ifo=l(),Rp=a("li"),fZ=a("strong"),dfo=o("transfo-xl"),cfo=o(" \u2014 "),XS=a("a"),mfo=o("TransfoXLLMHeadModel"),ffo=o(" (Transformer-XL model)"),gfo=l(),Sp=a("li"),gZ=a("strong"),hfo=o("unispeech"),ufo=o(" \u2014 "),VS=a("a"),pfo=o("UniSpeechForPreTraining"),_fo=o(" (UniSpeech model)"),bfo=l(),Pp=a("li"),hZ=a("strong"),vfo=o("unispeech-sat"),Tfo=o(" \u2014 "),zS=a("a"),Ffo=o("UniSpeechSatForPreTraining"),Cfo=o(" (UniSpeechSat model)"),Mfo=l(),$p=a("li"),uZ=a("strong"),Efo=o("visual_bert"),yfo=o(" \u2014 "),WS=a("a"),wfo=o("VisualBertForPreTraining"),Afo=o(" (VisualBert model)"),Lfo=l(),Ip=a("li"),pZ=a("strong"),Bfo=o("vit_mae"),xfo=o(" \u2014 "),QS=a("a"),kfo=o("ViTMAEForPreTraining"),Rfo=o(" (ViTMAE model)"),Sfo=l(),Dp=a("li"),_Z=a("strong"),Pfo=o("wav2vec2"),$fo=o(" \u2014 "),HS=a("a"),Ifo=o("Wav2Vec2ForPreTraining"),Dfo=o(" (Wav2Vec2 model)"),jfo=l(),jp=a("li"),bZ=a("strong"),Nfo=o("xlm"),qfo=o(" \u2014 "),US=a("a"),Gfo=o("XLMWithLMHeadModel"),Ofo=o(" (XLM model)"),Xfo=l(),Np=a("li"),vZ=a("strong"),Vfo=o("xlm-roberta"),zfo=o(" \u2014 "),JS=a("a"),Wfo=o("XLMRobertaForMaskedLM"),Qfo=o(" (XLM-RoBERTa model)"),Hfo=l(),qp=a("li"),TZ=a("strong"),Ufo=o("xlm-roberta-xl"),Jfo=o(" \u2014 "),YS=a("a"),Yfo=o("XLMRobertaXLForMaskedLM"),Kfo=o(" (XLM-RoBERTa-XL model)"),Zfo=l(),Gp=a("li"),FZ=a("strong"),ego=o("xlnet"),ogo=o(" \u2014 "),KS=a("a"),rgo=o("XLNetLMHeadModel"),tgo=o(" (XLNet model)"),ago=l(),Op=a("p"),sgo=o("The model is set in evaluation mode by default using "),CZ=a("code"),ngo=o("model.eval()"),lgo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),MZ=a("code"),igo=o("model.train()"),dgo=l(),EZ=a("p"),cgo=o("Examples:"),mgo=l(),m(gy.$$.fragment),P9e=l(),Hi=a("h2"),Xp=a("a"),yZ=a("span"),m(hy.$$.fragment),fgo=l(),wZ=a("span"),ggo=o("AutoModelForCausalLM"),$9e=l(),Uo=a("div"),m(uy.$$.fragment),hgo=l(),Ui=a("p"),ugo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),AZ=a("code"),pgo=o("from_pretrained()"),_go=o("class method or the "),LZ=a("code"),bgo=o("from_config()"),vgo=o(`class
method.`),Tgo=l(),py=a("p"),Fgo=o("This class cannot be instantiated directly using "),BZ=a("code"),Cgo=o("__init__()"),Mgo=o(" (throws an error)."),Ego=l(),Or=a("div"),m(_y.$$.fragment),ygo=l(),xZ=a("p"),wgo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),Ago=l(),Ji=a("p"),Lgo=o(`Note:
Loading a model from its configuration file does `),kZ=a("strong"),Bgo=o("not"),xgo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),RZ=a("code"),kgo=o("from_pretrained()"),Rgo=o("to load the model weights."),Sgo=l(),SZ=a("p"),Pgo=o("Examples:"),$go=l(),m(by.$$.fragment),Igo=l(),Pe=a("div"),m(vy.$$.fragment),Dgo=l(),PZ=a("p"),jgo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Ngo=l(),Ga=a("p"),qgo=o("The model class to instantiate is selected based on the "),$Z=a("code"),Ggo=o("model_type"),Ogo=o(` property of the config object (either
passed as an argument or loaded from `),IZ=a("code"),Xgo=o("pretrained_model_name_or_path"),Vgo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),DZ=a("code"),zgo=o("pretrained_model_name_or_path"),Wgo=o(":"),Qgo=l(),$=a("ul"),Vp=a("li"),jZ=a("strong"),Hgo=o("bart"),Ugo=o(" \u2014 "),ZS=a("a"),Jgo=o("BartForCausalLM"),Ygo=o(" (BART model)"),Kgo=l(),zp=a("li"),NZ=a("strong"),Zgo=o("bert"),eho=o(" \u2014 "),eP=a("a"),oho=o("BertLMHeadModel"),rho=o(" (BERT model)"),tho=l(),Wp=a("li"),qZ=a("strong"),aho=o("bert-generation"),sho=o(" \u2014 "),oP=a("a"),nho=o("BertGenerationDecoder"),lho=o(" (Bert Generation model)"),iho=l(),Qp=a("li"),GZ=a("strong"),dho=o("big_bird"),cho=o(" \u2014 "),rP=a("a"),mho=o("BigBirdForCausalLM"),fho=o(" (BigBird model)"),gho=l(),Hp=a("li"),OZ=a("strong"),hho=o("bigbird_pegasus"),uho=o(" \u2014 "),tP=a("a"),pho=o("BigBirdPegasusForCausalLM"),_ho=o(" (BigBirdPegasus model)"),bho=l(),Up=a("li"),XZ=a("strong"),vho=o("blenderbot"),Tho=o(" \u2014 "),aP=a("a"),Fho=o("BlenderbotForCausalLM"),Cho=o(" (Blenderbot model)"),Mho=l(),Jp=a("li"),VZ=a("strong"),Eho=o("blenderbot-small"),yho=o(" \u2014 "),sP=a("a"),who=o("BlenderbotSmallForCausalLM"),Aho=o(" (BlenderbotSmall model)"),Lho=l(),Yp=a("li"),zZ=a("strong"),Bho=o("camembert"),xho=o(" \u2014 "),nP=a("a"),kho=o("CamembertForCausalLM"),Rho=o(" (CamemBERT model)"),Sho=l(),Kp=a("li"),WZ=a("strong"),Pho=o("ctrl"),$ho=o(" \u2014 "),lP=a("a"),Iho=o("CTRLLMHeadModel"),Dho=o(" (CTRL model)"),jho=l(),Zp=a("li"),QZ=a("strong"),Nho=o("data2vec-text"),qho=o(" \u2014 "),iP=a("a"),Gho=o("Data2VecTextForCausalLM"),Oho=o(" (Data2VecText model)"),Xho=l(),e_=a("li"),HZ=a("strong"),Vho=o("electra"),zho=o(" \u2014 "),dP=a("a"),Who=o("ElectraForCausalLM"),Qho=o(" (ELECTRA model)"),Hho=l(),o_=a("li"),UZ=a("strong"),Uho=o("gpt2"),Jho=o(" \u2014 "),cP=a("a"),Yho=o("GPT2LMHeadModel"),Kho=o(" (OpenAI GPT-2 model)"),Zho=l(),r_=a("li"),JZ=a("strong"),euo=o("gpt_neo"),ouo=o(" \u2014 "),mP=a("a"),ruo=o("GPTNeoForCausalLM"),tuo=o(" (GPT Neo model)"),auo=l(),t_=a("li"),YZ=a("strong"),suo=o("gptj"),nuo=o(" \u2014 "),fP=a("a"),luo=o("GPTJForCausalLM"),iuo=o(" (GPT-J model)"),duo=l(),a_=a("li"),KZ=a("strong"),cuo=o("marian"),muo=o(" \u2014 "),gP=a("a"),fuo=o("MarianForCausalLM"),guo=o(" (Marian model)"),huo=l(),s_=a("li"),ZZ=a("strong"),uuo=o("mbart"),puo=o(" \u2014 "),hP=a("a"),_uo=o("MBartForCausalLM"),buo=o(" (mBART model)"),vuo=l(),n_=a("li"),eee=a("strong"),Tuo=o("megatron-bert"),Fuo=o(" \u2014 "),uP=a("a"),Cuo=o("MegatronBertForCausalLM"),Muo=o(" (MegatronBert model)"),Euo=l(),l_=a("li"),oee=a("strong"),yuo=o("openai-gpt"),wuo=o(" \u2014 "),pP=a("a"),Auo=o("OpenAIGPTLMHeadModel"),Luo=o(" (OpenAI GPT model)"),Buo=l(),i_=a("li"),ree=a("strong"),xuo=o("pegasus"),kuo=o(" \u2014 "),_P=a("a"),Ruo=o("PegasusForCausalLM"),Suo=o(" (Pegasus model)"),Puo=l(),d_=a("li"),tee=a("strong"),$uo=o("plbart"),Iuo=o(" \u2014 "),bP=a("a"),Duo=o("PLBartForCausalLM"),juo=o(" (PLBart model)"),Nuo=l(),c_=a("li"),aee=a("strong"),quo=o("prophetnet"),Guo=o(" \u2014 "),vP=a("a"),Ouo=o("ProphetNetForCausalLM"),Xuo=o(" (ProphetNet model)"),Vuo=l(),m_=a("li"),see=a("strong"),zuo=o("qdqbert"),Wuo=o(" \u2014 "),TP=a("a"),Quo=o("QDQBertLMHeadModel"),Huo=o(" (QDQBert model)"),Uuo=l(),f_=a("li"),nee=a("strong"),Juo=o("reformer"),Yuo=o(" \u2014 "),FP=a("a"),Kuo=o("ReformerModelWithLMHead"),Zuo=o(" (Reformer model)"),epo=l(),g_=a("li"),lee=a("strong"),opo=o("rembert"),rpo=o(" \u2014 "),CP=a("a"),tpo=o("RemBertForCausalLM"),apo=o(" (RemBERT model)"),spo=l(),h_=a("li"),iee=a("strong"),npo=o("roberta"),lpo=o(" \u2014 "),MP=a("a"),ipo=o("RobertaForCausalLM"),dpo=o(" (RoBERTa model)"),cpo=l(),u_=a("li"),dee=a("strong"),mpo=o("roformer"),fpo=o(" \u2014 "),EP=a("a"),gpo=o("RoFormerForCausalLM"),hpo=o(" (RoFormer model)"),upo=l(),p_=a("li"),cee=a("strong"),ppo=o("speech_to_text_2"),_po=o(" \u2014 "),yP=a("a"),bpo=o("Speech2Text2ForCausalLM"),vpo=o(" (Speech2Text2 model)"),Tpo=l(),__=a("li"),mee=a("strong"),Fpo=o("transfo-xl"),Cpo=o(" \u2014 "),wP=a("a"),Mpo=o("TransfoXLLMHeadModel"),Epo=o(" (Transformer-XL model)"),ypo=l(),b_=a("li"),fee=a("strong"),wpo=o("trocr"),Apo=o(" \u2014 "),AP=a("a"),Lpo=o("TrOCRForCausalLM"),Bpo=o(" (TrOCR model)"),xpo=l(),v_=a("li"),gee=a("strong"),kpo=o("xglm"),Rpo=o(" \u2014 "),LP=a("a"),Spo=o("XGLMForCausalLM"),Ppo=o(" (XGLM model)"),$po=l(),T_=a("li"),hee=a("strong"),Ipo=o("xlm"),Dpo=o(" \u2014 "),BP=a("a"),jpo=o("XLMWithLMHeadModel"),Npo=o(" (XLM model)"),qpo=l(),F_=a("li"),uee=a("strong"),Gpo=o("xlm-prophetnet"),Opo=o(" \u2014 "),xP=a("a"),Xpo=o("XLMProphetNetForCausalLM"),Vpo=o(" (XLMProphetNet model)"),zpo=l(),C_=a("li"),pee=a("strong"),Wpo=o("xlm-roberta"),Qpo=o(" \u2014 "),kP=a("a"),Hpo=o("XLMRobertaForCausalLM"),Upo=o(" (XLM-RoBERTa model)"),Jpo=l(),M_=a("li"),_ee=a("strong"),Ypo=o("xlm-roberta-xl"),Kpo=o(" \u2014 "),RP=a("a"),Zpo=o("XLMRobertaXLForCausalLM"),e_o=o(" (XLM-RoBERTa-XL model)"),o_o=l(),E_=a("li"),bee=a("strong"),r_o=o("xlnet"),t_o=o(" \u2014 "),SP=a("a"),a_o=o("XLNetLMHeadModel"),s_o=o(" (XLNet model)"),n_o=l(),y_=a("p"),l_o=o("The model is set in evaluation mode by default using "),vee=a("code"),i_o=o("model.eval()"),d_o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tee=a("code"),c_o=o("model.train()"),m_o=l(),Fee=a("p"),f_o=o("Examples:"),g_o=l(),m(Ty.$$.fragment),I9e=l(),Yi=a("h2"),w_=a("a"),Cee=a("span"),m(Fy.$$.fragment),h_o=l(),Mee=a("span"),u_o=o("AutoModelForMaskedLM"),D9e=l(),Jo=a("div"),m(Cy.$$.fragment),p_o=l(),Ki=a("p"),__o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Eee=a("code"),b_o=o("from_pretrained()"),v_o=o("class method or the "),yee=a("code"),T_o=o("from_config()"),F_o=o(`class
method.`),C_o=l(),My=a("p"),M_o=o("This class cannot be instantiated directly using "),wee=a("code"),E_o=o("__init__()"),y_o=o(" (throws an error)."),w_o=l(),Xr=a("div"),m(Ey.$$.fragment),A_o=l(),Aee=a("p"),L_o=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),B_o=l(),Zi=a("p"),x_o=o(`Note:
Loading a model from its configuration file does `),Lee=a("strong"),k_o=o("not"),R_o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bee=a("code"),S_o=o("from_pretrained()"),P_o=o("to load the model weights."),$_o=l(),xee=a("p"),I_o=o("Examples:"),D_o=l(),m(yy.$$.fragment),j_o=l(),$e=a("div"),m(wy.$$.fragment),N_o=l(),kee=a("p"),q_o=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),G_o=l(),Oa=a("p"),O_o=o("The model class to instantiate is selected based on the "),Ree=a("code"),X_o=o("model_type"),V_o=o(` property of the config object (either
passed as an argument or loaded from `),See=a("code"),z_o=o("pretrained_model_name_or_path"),W_o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pee=a("code"),Q_o=o("pretrained_model_name_or_path"),H_o=o(":"),U_o=l(),I=a("ul"),A_=a("li"),$ee=a("strong"),J_o=o("albert"),Y_o=o(" \u2014 "),PP=a("a"),K_o=o("AlbertForMaskedLM"),Z_o=o(" (ALBERT model)"),ebo=l(),L_=a("li"),Iee=a("strong"),obo=o("bart"),rbo=o(" \u2014 "),$P=a("a"),tbo=o("BartForConditionalGeneration"),abo=o(" (BART model)"),sbo=l(),B_=a("li"),Dee=a("strong"),nbo=o("bert"),lbo=o(" \u2014 "),IP=a("a"),ibo=o("BertForMaskedLM"),dbo=o(" (BERT model)"),cbo=l(),x_=a("li"),jee=a("strong"),mbo=o("big_bird"),fbo=o(" \u2014 "),DP=a("a"),gbo=o("BigBirdForMaskedLM"),hbo=o(" (BigBird model)"),ubo=l(),k_=a("li"),Nee=a("strong"),pbo=o("camembert"),_bo=o(" \u2014 "),jP=a("a"),bbo=o("CamembertForMaskedLM"),vbo=o(" (CamemBERT model)"),Tbo=l(),R_=a("li"),qee=a("strong"),Fbo=o("convbert"),Cbo=o(" \u2014 "),NP=a("a"),Mbo=o("ConvBertForMaskedLM"),Ebo=o(" (ConvBERT model)"),ybo=l(),S_=a("li"),Gee=a("strong"),wbo=o("data2vec-text"),Abo=o(" \u2014 "),qP=a("a"),Lbo=o("Data2VecTextForMaskedLM"),Bbo=o(" (Data2VecText model)"),xbo=l(),P_=a("li"),Oee=a("strong"),kbo=o("deberta"),Rbo=o(" \u2014 "),GP=a("a"),Sbo=o("DebertaForMaskedLM"),Pbo=o(" (DeBERTa model)"),$bo=l(),$_=a("li"),Xee=a("strong"),Ibo=o("deberta-v2"),Dbo=o(" \u2014 "),OP=a("a"),jbo=o("DebertaV2ForMaskedLM"),Nbo=o(" (DeBERTa-v2 model)"),qbo=l(),I_=a("li"),Vee=a("strong"),Gbo=o("distilbert"),Obo=o(" \u2014 "),XP=a("a"),Xbo=o("DistilBertForMaskedLM"),Vbo=o(" (DistilBERT model)"),zbo=l(),D_=a("li"),zee=a("strong"),Wbo=o("electra"),Qbo=o(" \u2014 "),VP=a("a"),Hbo=o("ElectraForMaskedLM"),Ubo=o(" (ELECTRA model)"),Jbo=l(),j_=a("li"),Wee=a("strong"),Ybo=o("flaubert"),Kbo=o(" \u2014 "),zP=a("a"),Zbo=o("FlaubertWithLMHeadModel"),e2o=o(" (FlauBERT model)"),o2o=l(),N_=a("li"),Qee=a("strong"),r2o=o("fnet"),t2o=o(" \u2014 "),WP=a("a"),a2o=o("FNetForMaskedLM"),s2o=o(" (FNet model)"),n2o=l(),q_=a("li"),Hee=a("strong"),l2o=o("funnel"),i2o=o(" \u2014 "),QP=a("a"),d2o=o("FunnelForMaskedLM"),c2o=o(" (Funnel Transformer model)"),m2o=l(),G_=a("li"),Uee=a("strong"),f2o=o("ibert"),g2o=o(" \u2014 "),HP=a("a"),h2o=o("IBertForMaskedLM"),u2o=o(" (I-BERT model)"),p2o=l(),O_=a("li"),Jee=a("strong"),_2o=o("layoutlm"),b2o=o(" \u2014 "),UP=a("a"),v2o=o("LayoutLMForMaskedLM"),T2o=o(" (LayoutLM model)"),F2o=l(),X_=a("li"),Yee=a("strong"),C2o=o("longformer"),M2o=o(" \u2014 "),JP=a("a"),E2o=o("LongformerForMaskedLM"),y2o=o(" (Longformer model)"),w2o=l(),V_=a("li"),Kee=a("strong"),A2o=o("mbart"),L2o=o(" \u2014 "),YP=a("a"),B2o=o("MBartForConditionalGeneration"),x2o=o(" (mBART model)"),k2o=l(),z_=a("li"),Zee=a("strong"),R2o=o("megatron-bert"),S2o=o(" \u2014 "),KP=a("a"),P2o=o("MegatronBertForMaskedLM"),$2o=o(" (MegatronBert model)"),I2o=l(),W_=a("li"),eoe=a("strong"),D2o=o("mobilebert"),j2o=o(" \u2014 "),ZP=a("a"),N2o=o("MobileBertForMaskedLM"),q2o=o(" (MobileBERT model)"),G2o=l(),Q_=a("li"),ooe=a("strong"),O2o=o("mpnet"),X2o=o(" \u2014 "),e$=a("a"),V2o=o("MPNetForMaskedLM"),z2o=o(" (MPNet model)"),W2o=l(),H_=a("li"),roe=a("strong"),Q2o=o("nystromformer"),H2o=o(" \u2014 "),o$=a("a"),U2o=o("NystromformerForMaskedLM"),J2o=o(" (Nystromformer model)"),Y2o=l(),U_=a("li"),toe=a("strong"),K2o=o("perceiver"),Z2o=o(" \u2014 "),r$=a("a"),evo=o("PerceiverForMaskedLM"),ovo=o(" (Perceiver model)"),rvo=l(),J_=a("li"),aoe=a("strong"),tvo=o("qdqbert"),avo=o(" \u2014 "),t$=a("a"),svo=o("QDQBertForMaskedLM"),nvo=o(" (QDQBert model)"),lvo=l(),Y_=a("li"),soe=a("strong"),ivo=o("reformer"),dvo=o(" \u2014 "),a$=a("a"),cvo=o("ReformerForMaskedLM"),mvo=o(" (Reformer model)"),fvo=l(),K_=a("li"),noe=a("strong"),gvo=o("rembert"),hvo=o(" \u2014 "),s$=a("a"),uvo=o("RemBertForMaskedLM"),pvo=o(" (RemBERT model)"),_vo=l(),Z_=a("li"),loe=a("strong"),bvo=o("roberta"),vvo=o(" \u2014 "),n$=a("a"),Tvo=o("RobertaForMaskedLM"),Fvo=o(" (RoBERTa model)"),Cvo=l(),eb=a("li"),ioe=a("strong"),Mvo=o("roformer"),Evo=o(" \u2014 "),l$=a("a"),yvo=o("RoFormerForMaskedLM"),wvo=o(" (RoFormer model)"),Avo=l(),ob=a("li"),doe=a("strong"),Lvo=o("squeezebert"),Bvo=o(" \u2014 "),i$=a("a"),xvo=o("SqueezeBertForMaskedLM"),kvo=o(" (SqueezeBERT model)"),Rvo=l(),rb=a("li"),coe=a("strong"),Svo=o("tapas"),Pvo=o(" \u2014 "),d$=a("a"),$vo=o("TapasForMaskedLM"),Ivo=o(" (TAPAS model)"),Dvo=l(),tb=a("li"),moe=a("strong"),jvo=o("wav2vec2"),Nvo=o(" \u2014 "),foe=a("code"),qvo=o("Wav2Vec2ForMaskedLM"),Gvo=o("(Wav2Vec2 model)"),Ovo=l(),ab=a("li"),goe=a("strong"),Xvo=o("xlm"),Vvo=o(" \u2014 "),c$=a("a"),zvo=o("XLMWithLMHeadModel"),Wvo=o(" (XLM model)"),Qvo=l(),sb=a("li"),hoe=a("strong"),Hvo=o("xlm-roberta"),Uvo=o(" \u2014 "),m$=a("a"),Jvo=o("XLMRobertaForMaskedLM"),Yvo=o(" (XLM-RoBERTa model)"),Kvo=l(),nb=a("li"),uoe=a("strong"),Zvo=o("xlm-roberta-xl"),eTo=o(" \u2014 "),f$=a("a"),oTo=o("XLMRobertaXLForMaskedLM"),rTo=o(" (XLM-RoBERTa-XL model)"),tTo=l(),lb=a("li"),poe=a("strong"),aTo=o("yoso"),sTo=o(" \u2014 "),g$=a("a"),nTo=o("YosoForMaskedLM"),lTo=o(" (YOSO model)"),iTo=l(),ib=a("p"),dTo=o("The model is set in evaluation mode by default using "),_oe=a("code"),cTo=o("model.eval()"),mTo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),boe=a("code"),fTo=o("model.train()"),gTo=l(),voe=a("p"),hTo=o("Examples:"),uTo=l(),m(Ay.$$.fragment),j9e=l(),ed=a("h2"),db=a("a"),Toe=a("span"),m(Ly.$$.fragment),pTo=l(),Foe=a("span"),_To=o("AutoModelForSeq2SeqLM"),N9e=l(),Yo=a("div"),m(By.$$.fragment),bTo=l(),od=a("p"),vTo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Coe=a("code"),TTo=o("from_pretrained()"),FTo=o("class method or the "),Moe=a("code"),CTo=o("from_config()"),MTo=o(`class
method.`),ETo=l(),xy=a("p"),yTo=o("This class cannot be instantiated directly using "),Eoe=a("code"),wTo=o("__init__()"),ATo=o(" (throws an error)."),LTo=l(),Vr=a("div"),m(ky.$$.fragment),BTo=l(),yoe=a("p"),xTo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),kTo=l(),rd=a("p"),RTo=o(`Note:
Loading a model from its configuration file does `),woe=a("strong"),STo=o("not"),PTo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Aoe=a("code"),$To=o("from_pretrained()"),ITo=o("to load the model weights."),DTo=l(),Loe=a("p"),jTo=o("Examples:"),NTo=l(),m(Ry.$$.fragment),qTo=l(),Ie=a("div"),m(Sy.$$.fragment),GTo=l(),Boe=a("p"),OTo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),XTo=l(),Xa=a("p"),VTo=o("The model class to instantiate is selected based on the "),xoe=a("code"),zTo=o("model_type"),WTo=o(` property of the config object (either
passed as an argument or loaded from `),koe=a("code"),QTo=o("pretrained_model_name_or_path"),HTo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Roe=a("code"),UTo=o("pretrained_model_name_or_path"),JTo=o(":"),YTo=l(),ae=a("ul"),cb=a("li"),Soe=a("strong"),KTo=o("bart"),ZTo=o(" \u2014 "),h$=a("a"),e1o=o("BartForConditionalGeneration"),o1o=o(" (BART model)"),r1o=l(),mb=a("li"),Poe=a("strong"),t1o=o("bigbird_pegasus"),a1o=o(" \u2014 "),u$=a("a"),s1o=o("BigBirdPegasusForConditionalGeneration"),n1o=o(" (BigBirdPegasus model)"),l1o=l(),fb=a("li"),$oe=a("strong"),i1o=o("blenderbot"),d1o=o(" \u2014 "),p$=a("a"),c1o=o("BlenderbotForConditionalGeneration"),m1o=o(" (Blenderbot model)"),f1o=l(),gb=a("li"),Ioe=a("strong"),g1o=o("blenderbot-small"),h1o=o(" \u2014 "),_$=a("a"),u1o=o("BlenderbotSmallForConditionalGeneration"),p1o=o(" (BlenderbotSmall model)"),_1o=l(),hb=a("li"),Doe=a("strong"),b1o=o("encoder-decoder"),v1o=o(" \u2014 "),b$=a("a"),T1o=o("EncoderDecoderModel"),F1o=o(" (Encoder decoder model)"),C1o=l(),ub=a("li"),joe=a("strong"),M1o=o("fsmt"),E1o=o(" \u2014 "),v$=a("a"),y1o=o("FSMTForConditionalGeneration"),w1o=o(" (FairSeq Machine-Translation model)"),A1o=l(),pb=a("li"),Noe=a("strong"),L1o=o("led"),B1o=o(" \u2014 "),T$=a("a"),x1o=o("LEDForConditionalGeneration"),k1o=o(" (LED model)"),R1o=l(),_b=a("li"),qoe=a("strong"),S1o=o("m2m_100"),P1o=o(" \u2014 "),F$=a("a"),$1o=o("M2M100ForConditionalGeneration"),I1o=o(" (M2M100 model)"),D1o=l(),bb=a("li"),Goe=a("strong"),j1o=o("marian"),N1o=o(" \u2014 "),C$=a("a"),q1o=o("MarianMTModel"),G1o=o(" (Marian model)"),O1o=l(),vb=a("li"),Ooe=a("strong"),X1o=o("mbart"),V1o=o(" \u2014 "),M$=a("a"),z1o=o("MBartForConditionalGeneration"),W1o=o(" (mBART model)"),Q1o=l(),Tb=a("li"),Xoe=a("strong"),H1o=o("mt5"),U1o=o(" \u2014 "),E$=a("a"),J1o=o("MT5ForConditionalGeneration"),Y1o=o(" (mT5 model)"),K1o=l(),Fb=a("li"),Voe=a("strong"),Z1o=o("pegasus"),eFo=o(" \u2014 "),y$=a("a"),oFo=o("PegasusForConditionalGeneration"),rFo=o(" (Pegasus model)"),tFo=l(),Cb=a("li"),zoe=a("strong"),aFo=o("plbart"),sFo=o(" \u2014 "),w$=a("a"),nFo=o("PLBartForConditionalGeneration"),lFo=o(" (PLBart model)"),iFo=l(),Mb=a("li"),Woe=a("strong"),dFo=o("prophetnet"),cFo=o(" \u2014 "),A$=a("a"),mFo=o("ProphetNetForConditionalGeneration"),fFo=o(" (ProphetNet model)"),gFo=l(),Eb=a("li"),Qoe=a("strong"),hFo=o("t5"),uFo=o(" \u2014 "),L$=a("a"),pFo=o("T5ForConditionalGeneration"),_Fo=o(" (T5 model)"),bFo=l(),yb=a("li"),Hoe=a("strong"),vFo=o("xlm-prophetnet"),TFo=o(" \u2014 "),B$=a("a"),FFo=o("XLMProphetNetForConditionalGeneration"),CFo=o(" (XLMProphetNet model)"),MFo=l(),wb=a("p"),EFo=o("The model is set in evaluation mode by default using "),Uoe=a("code"),yFo=o("model.eval()"),wFo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Joe=a("code"),AFo=o("model.train()"),LFo=l(),Yoe=a("p"),BFo=o("Examples:"),xFo=l(),m(Py.$$.fragment),q9e=l(),td=a("h2"),Ab=a("a"),Koe=a("span"),m($y.$$.fragment),kFo=l(),Zoe=a("span"),RFo=o("AutoModelForSequenceClassification"),G9e=l(),Ko=a("div"),m(Iy.$$.fragment),SFo=l(),ad=a("p"),PFo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),ere=a("code"),$Fo=o("from_pretrained()"),IFo=o("class method or the "),ore=a("code"),DFo=o("from_config()"),jFo=o(`class
method.`),NFo=l(),Dy=a("p"),qFo=o("This class cannot be instantiated directly using "),rre=a("code"),GFo=o("__init__()"),OFo=o(" (throws an error)."),XFo=l(),zr=a("div"),m(jy.$$.fragment),VFo=l(),tre=a("p"),zFo=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),WFo=l(),sd=a("p"),QFo=o(`Note:
Loading a model from its configuration file does `),are=a("strong"),HFo=o("not"),UFo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sre=a("code"),JFo=o("from_pretrained()"),YFo=o("to load the model weights."),KFo=l(),nre=a("p"),ZFo=o("Examples:"),eCo=l(),m(Ny.$$.fragment),oCo=l(),De=a("div"),m(qy.$$.fragment),rCo=l(),lre=a("p"),tCo=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),aCo=l(),Va=a("p"),sCo=o("The model class to instantiate is selected based on the "),ire=a("code"),nCo=o("model_type"),lCo=o(` property of the config object (either
passed as an argument or loaded from `),dre=a("code"),iCo=o("pretrained_model_name_or_path"),dCo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cre=a("code"),cCo=o("pretrained_model_name_or_path"),mCo=o(":"),fCo=l(),A=a("ul"),Lb=a("li"),mre=a("strong"),gCo=o("albert"),hCo=o(" \u2014 "),x$=a("a"),uCo=o("AlbertForSequenceClassification"),pCo=o(" (ALBERT model)"),_Co=l(),Bb=a("li"),fre=a("strong"),bCo=o("bart"),vCo=o(" \u2014 "),k$=a("a"),TCo=o("BartForSequenceClassification"),FCo=o(" (BART model)"),CCo=l(),xb=a("li"),gre=a("strong"),MCo=o("bert"),ECo=o(" \u2014 "),R$=a("a"),yCo=o("BertForSequenceClassification"),wCo=o(" (BERT model)"),ACo=l(),kb=a("li"),hre=a("strong"),LCo=o("big_bird"),BCo=o(" \u2014 "),S$=a("a"),xCo=o("BigBirdForSequenceClassification"),kCo=o(" (BigBird model)"),RCo=l(),Rb=a("li"),ure=a("strong"),SCo=o("bigbird_pegasus"),PCo=o(" \u2014 "),P$=a("a"),$Co=o("BigBirdPegasusForSequenceClassification"),ICo=o(" (BigBirdPegasus model)"),DCo=l(),Sb=a("li"),pre=a("strong"),jCo=o("camembert"),NCo=o(" \u2014 "),$$=a("a"),qCo=o("CamembertForSequenceClassification"),GCo=o(" (CamemBERT model)"),OCo=l(),Pb=a("li"),_re=a("strong"),XCo=o("canine"),VCo=o(" \u2014 "),I$=a("a"),zCo=o("CanineForSequenceClassification"),WCo=o(" (Canine model)"),QCo=l(),$b=a("li"),bre=a("strong"),HCo=o("convbert"),UCo=o(" \u2014 "),D$=a("a"),JCo=o("ConvBertForSequenceClassification"),YCo=o(" (ConvBERT model)"),KCo=l(),Ib=a("li"),vre=a("strong"),ZCo=o("ctrl"),e4o=o(" \u2014 "),j$=a("a"),o4o=o("CTRLForSequenceClassification"),r4o=o(" (CTRL model)"),t4o=l(),Db=a("li"),Tre=a("strong"),a4o=o("data2vec-text"),s4o=o(" \u2014 "),N$=a("a"),n4o=o("Data2VecTextForSequenceClassification"),l4o=o(" (Data2VecText model)"),i4o=l(),jb=a("li"),Fre=a("strong"),d4o=o("deberta"),c4o=o(" \u2014 "),q$=a("a"),m4o=o("DebertaForSequenceClassification"),f4o=o(" (DeBERTa model)"),g4o=l(),Nb=a("li"),Cre=a("strong"),h4o=o("deberta-v2"),u4o=o(" \u2014 "),G$=a("a"),p4o=o("DebertaV2ForSequenceClassification"),_4o=o(" (DeBERTa-v2 model)"),b4o=l(),qb=a("li"),Mre=a("strong"),v4o=o("distilbert"),T4o=o(" \u2014 "),O$=a("a"),F4o=o("DistilBertForSequenceClassification"),C4o=o(" (DistilBERT model)"),M4o=l(),Gb=a("li"),Ere=a("strong"),E4o=o("electra"),y4o=o(" \u2014 "),X$=a("a"),w4o=o("ElectraForSequenceClassification"),A4o=o(" (ELECTRA model)"),L4o=l(),Ob=a("li"),yre=a("strong"),B4o=o("flaubert"),x4o=o(" \u2014 "),V$=a("a"),k4o=o("FlaubertForSequenceClassification"),R4o=o(" (FlauBERT model)"),S4o=l(),Xb=a("li"),wre=a("strong"),P4o=o("fnet"),$4o=o(" \u2014 "),z$=a("a"),I4o=o("FNetForSequenceClassification"),D4o=o(" (FNet model)"),j4o=l(),Vb=a("li"),Are=a("strong"),N4o=o("funnel"),q4o=o(" \u2014 "),W$=a("a"),G4o=o("FunnelForSequenceClassification"),O4o=o(" (Funnel Transformer model)"),X4o=l(),zb=a("li"),Lre=a("strong"),V4o=o("gpt2"),z4o=o(" \u2014 "),Q$=a("a"),W4o=o("GPT2ForSequenceClassification"),Q4o=o(" (OpenAI GPT-2 model)"),H4o=l(),Wb=a("li"),Bre=a("strong"),U4o=o("gpt_neo"),J4o=o(" \u2014 "),H$=a("a"),Y4o=o("GPTNeoForSequenceClassification"),K4o=o(" (GPT Neo model)"),Z4o=l(),Qb=a("li"),xre=a("strong"),eMo=o("gptj"),oMo=o(" \u2014 "),U$=a("a"),rMo=o("GPTJForSequenceClassification"),tMo=o(" (GPT-J model)"),aMo=l(),Hb=a("li"),kre=a("strong"),sMo=o("ibert"),nMo=o(" \u2014 "),J$=a("a"),lMo=o("IBertForSequenceClassification"),iMo=o(" (I-BERT model)"),dMo=l(),Ub=a("li"),Rre=a("strong"),cMo=o("layoutlm"),mMo=o(" \u2014 "),Y$=a("a"),fMo=o("LayoutLMForSequenceClassification"),gMo=o(" (LayoutLM model)"),hMo=l(),Jb=a("li"),Sre=a("strong"),uMo=o("layoutlmv2"),pMo=o(" \u2014 "),K$=a("a"),_Mo=o("LayoutLMv2ForSequenceClassification"),bMo=o(" (LayoutLMv2 model)"),vMo=l(),Yb=a("li"),Pre=a("strong"),TMo=o("led"),FMo=o(" \u2014 "),Z$=a("a"),CMo=o("LEDForSequenceClassification"),MMo=o(" (LED model)"),EMo=l(),Kb=a("li"),$re=a("strong"),yMo=o("longformer"),wMo=o(" \u2014 "),eI=a("a"),AMo=o("LongformerForSequenceClassification"),LMo=o(" (Longformer model)"),BMo=l(),Zb=a("li"),Ire=a("strong"),xMo=o("mbart"),kMo=o(" \u2014 "),oI=a("a"),RMo=o("MBartForSequenceClassification"),SMo=o(" (mBART model)"),PMo=l(),e2=a("li"),Dre=a("strong"),$Mo=o("megatron-bert"),IMo=o(" \u2014 "),rI=a("a"),DMo=o("MegatronBertForSequenceClassification"),jMo=o(" (MegatronBert model)"),NMo=l(),o2=a("li"),jre=a("strong"),qMo=o("mobilebert"),GMo=o(" \u2014 "),tI=a("a"),OMo=o("MobileBertForSequenceClassification"),XMo=o(" (MobileBERT model)"),VMo=l(),r2=a("li"),Nre=a("strong"),zMo=o("mpnet"),WMo=o(" \u2014 "),aI=a("a"),QMo=o("MPNetForSequenceClassification"),HMo=o(" (MPNet model)"),UMo=l(),t2=a("li"),qre=a("strong"),JMo=o("nystromformer"),YMo=o(" \u2014 "),sI=a("a"),KMo=o("NystromformerForSequenceClassification"),ZMo=o(" (Nystromformer model)"),eEo=l(),a2=a("li"),Gre=a("strong"),oEo=o("openai-gpt"),rEo=o(" \u2014 "),nI=a("a"),tEo=o("OpenAIGPTForSequenceClassification"),aEo=o(" (OpenAI GPT model)"),sEo=l(),s2=a("li"),Ore=a("strong"),nEo=o("perceiver"),lEo=o(" \u2014 "),lI=a("a"),iEo=o("PerceiverForSequenceClassification"),dEo=o(" (Perceiver model)"),cEo=l(),n2=a("li"),Xre=a("strong"),mEo=o("plbart"),fEo=o(" \u2014 "),iI=a("a"),gEo=o("PLBartForSequenceClassification"),hEo=o(" (PLBart model)"),uEo=l(),l2=a("li"),Vre=a("strong"),pEo=o("qdqbert"),_Eo=o(" \u2014 "),dI=a("a"),bEo=o("QDQBertForSequenceClassification"),vEo=o(" (QDQBert model)"),TEo=l(),i2=a("li"),zre=a("strong"),FEo=o("reformer"),CEo=o(" \u2014 "),cI=a("a"),MEo=o("ReformerForSequenceClassification"),EEo=o(" (Reformer model)"),yEo=l(),d2=a("li"),Wre=a("strong"),wEo=o("rembert"),AEo=o(" \u2014 "),mI=a("a"),LEo=o("RemBertForSequenceClassification"),BEo=o(" (RemBERT model)"),xEo=l(),c2=a("li"),Qre=a("strong"),kEo=o("roberta"),REo=o(" \u2014 "),fI=a("a"),SEo=o("RobertaForSequenceClassification"),PEo=o(" (RoBERTa model)"),$Eo=l(),m2=a("li"),Hre=a("strong"),IEo=o("roformer"),DEo=o(" \u2014 "),gI=a("a"),jEo=o("RoFormerForSequenceClassification"),NEo=o(" (RoFormer model)"),qEo=l(),f2=a("li"),Ure=a("strong"),GEo=o("squeezebert"),OEo=o(" \u2014 "),hI=a("a"),XEo=o("SqueezeBertForSequenceClassification"),VEo=o(" (SqueezeBERT model)"),zEo=l(),g2=a("li"),Jre=a("strong"),WEo=o("tapas"),QEo=o(" \u2014 "),uI=a("a"),HEo=o("TapasForSequenceClassification"),UEo=o(" (TAPAS model)"),JEo=l(),h2=a("li"),Yre=a("strong"),YEo=o("transfo-xl"),KEo=o(" \u2014 "),pI=a("a"),ZEo=o("TransfoXLForSequenceClassification"),e3o=o(" (Transformer-XL model)"),o3o=l(),u2=a("li"),Kre=a("strong"),r3o=o("xlm"),t3o=o(" \u2014 "),_I=a("a"),a3o=o("XLMForSequenceClassification"),s3o=o(" (XLM model)"),n3o=l(),p2=a("li"),Zre=a("strong"),l3o=o("xlm-roberta"),i3o=o(" \u2014 "),bI=a("a"),d3o=o("XLMRobertaForSequenceClassification"),c3o=o(" (XLM-RoBERTa model)"),m3o=l(),_2=a("li"),ete=a("strong"),f3o=o("xlm-roberta-xl"),g3o=o(" \u2014 "),vI=a("a"),h3o=o("XLMRobertaXLForSequenceClassification"),u3o=o(" (XLM-RoBERTa-XL model)"),p3o=l(),b2=a("li"),ote=a("strong"),_3o=o("xlnet"),b3o=o(" \u2014 "),TI=a("a"),v3o=o("XLNetForSequenceClassification"),T3o=o(" (XLNet model)"),F3o=l(),v2=a("li"),rte=a("strong"),C3o=o("yoso"),M3o=o(" \u2014 "),FI=a("a"),E3o=o("YosoForSequenceClassification"),y3o=o(" (YOSO model)"),w3o=l(),T2=a("p"),A3o=o("The model is set in evaluation mode by default using "),tte=a("code"),L3o=o("model.eval()"),B3o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ate=a("code"),x3o=o("model.train()"),k3o=l(),ste=a("p"),R3o=o("Examples:"),S3o=l(),m(Gy.$$.fragment),O9e=l(),nd=a("h2"),F2=a("a"),nte=a("span"),m(Oy.$$.fragment),P3o=l(),lte=a("span"),$3o=o("AutoModelForMultipleChoice"),X9e=l(),Zo=a("div"),m(Xy.$$.fragment),I3o=l(),ld=a("p"),D3o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ite=a("code"),j3o=o("from_pretrained()"),N3o=o("class method or the "),dte=a("code"),q3o=o("from_config()"),G3o=o(`class
method.`),O3o=l(),Vy=a("p"),X3o=o("This class cannot be instantiated directly using "),cte=a("code"),V3o=o("__init__()"),z3o=o(" (throws an error)."),W3o=l(),Wr=a("div"),m(zy.$$.fragment),Q3o=l(),mte=a("p"),H3o=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),U3o=l(),id=a("p"),J3o=o(`Note:
Loading a model from its configuration file does `),fte=a("strong"),Y3o=o("not"),K3o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),gte=a("code"),Z3o=o("from_pretrained()"),e5o=o("to load the model weights."),o5o=l(),hte=a("p"),r5o=o("Examples:"),t5o=l(),m(Wy.$$.fragment),a5o=l(),je=a("div"),m(Qy.$$.fragment),s5o=l(),ute=a("p"),n5o=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),l5o=l(),za=a("p"),i5o=o("The model class to instantiate is selected based on the "),pte=a("code"),d5o=o("model_type"),c5o=o(` property of the config object (either
passed as an argument or loaded from `),_te=a("code"),m5o=o("pretrained_model_name_or_path"),f5o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bte=a("code"),g5o=o("pretrained_model_name_or_path"),h5o=o(":"),u5o=l(),G=a("ul"),C2=a("li"),vte=a("strong"),p5o=o("albert"),_5o=o(" \u2014 "),CI=a("a"),b5o=o("AlbertForMultipleChoice"),v5o=o(" (ALBERT model)"),T5o=l(),M2=a("li"),Tte=a("strong"),F5o=o("bert"),C5o=o(" \u2014 "),MI=a("a"),M5o=o("BertForMultipleChoice"),E5o=o(" (BERT model)"),y5o=l(),E2=a("li"),Fte=a("strong"),w5o=o("big_bird"),A5o=o(" \u2014 "),EI=a("a"),L5o=o("BigBirdForMultipleChoice"),B5o=o(" (BigBird model)"),x5o=l(),y2=a("li"),Cte=a("strong"),k5o=o("camembert"),R5o=o(" \u2014 "),yI=a("a"),S5o=o("CamembertForMultipleChoice"),P5o=o(" (CamemBERT model)"),$5o=l(),w2=a("li"),Mte=a("strong"),I5o=o("canine"),D5o=o(" \u2014 "),wI=a("a"),j5o=o("CanineForMultipleChoice"),N5o=o(" (Canine model)"),q5o=l(),A2=a("li"),Ete=a("strong"),G5o=o("convbert"),O5o=o(" \u2014 "),AI=a("a"),X5o=o("ConvBertForMultipleChoice"),V5o=o(" (ConvBERT model)"),z5o=l(),L2=a("li"),yte=a("strong"),W5o=o("data2vec-text"),Q5o=o(" \u2014 "),LI=a("a"),H5o=o("Data2VecTextForMultipleChoice"),U5o=o(" (Data2VecText model)"),J5o=l(),B2=a("li"),wte=a("strong"),Y5o=o("distilbert"),K5o=o(" \u2014 "),BI=a("a"),Z5o=o("DistilBertForMultipleChoice"),eyo=o(" (DistilBERT model)"),oyo=l(),x2=a("li"),Ate=a("strong"),ryo=o("electra"),tyo=o(" \u2014 "),xI=a("a"),ayo=o("ElectraForMultipleChoice"),syo=o(" (ELECTRA model)"),nyo=l(),k2=a("li"),Lte=a("strong"),lyo=o("flaubert"),iyo=o(" \u2014 "),kI=a("a"),dyo=o("FlaubertForMultipleChoice"),cyo=o(" (FlauBERT model)"),myo=l(),R2=a("li"),Bte=a("strong"),fyo=o("fnet"),gyo=o(" \u2014 "),RI=a("a"),hyo=o("FNetForMultipleChoice"),uyo=o(" (FNet model)"),pyo=l(),S2=a("li"),xte=a("strong"),_yo=o("funnel"),byo=o(" \u2014 "),SI=a("a"),vyo=o("FunnelForMultipleChoice"),Tyo=o(" (Funnel Transformer model)"),Fyo=l(),P2=a("li"),kte=a("strong"),Cyo=o("ibert"),Myo=o(" \u2014 "),PI=a("a"),Eyo=o("IBertForMultipleChoice"),yyo=o(" (I-BERT model)"),wyo=l(),$2=a("li"),Rte=a("strong"),Ayo=o("longformer"),Lyo=o(" \u2014 "),$I=a("a"),Byo=o("LongformerForMultipleChoice"),xyo=o(" (Longformer model)"),kyo=l(),I2=a("li"),Ste=a("strong"),Ryo=o("megatron-bert"),Syo=o(" \u2014 "),II=a("a"),Pyo=o("MegatronBertForMultipleChoice"),$yo=o(" (MegatronBert model)"),Iyo=l(),D2=a("li"),Pte=a("strong"),Dyo=o("mobilebert"),jyo=o(" \u2014 "),DI=a("a"),Nyo=o("MobileBertForMultipleChoice"),qyo=o(" (MobileBERT model)"),Gyo=l(),j2=a("li"),$te=a("strong"),Oyo=o("mpnet"),Xyo=o(" \u2014 "),jI=a("a"),Vyo=o("MPNetForMultipleChoice"),zyo=o(" (MPNet model)"),Wyo=l(),N2=a("li"),Ite=a("strong"),Qyo=o("nystromformer"),Hyo=o(" \u2014 "),NI=a("a"),Uyo=o("NystromformerForMultipleChoice"),Jyo=o(" (Nystromformer model)"),Yyo=l(),q2=a("li"),Dte=a("strong"),Kyo=o("qdqbert"),Zyo=o(" \u2014 "),qI=a("a"),ewo=o("QDQBertForMultipleChoice"),owo=o(" (QDQBert model)"),rwo=l(),G2=a("li"),jte=a("strong"),two=o("rembert"),awo=o(" \u2014 "),GI=a("a"),swo=o("RemBertForMultipleChoice"),nwo=o(" (RemBERT model)"),lwo=l(),O2=a("li"),Nte=a("strong"),iwo=o("roberta"),dwo=o(" \u2014 "),OI=a("a"),cwo=o("RobertaForMultipleChoice"),mwo=o(" (RoBERTa model)"),fwo=l(),X2=a("li"),qte=a("strong"),gwo=o("roformer"),hwo=o(" \u2014 "),XI=a("a"),uwo=o("RoFormerForMultipleChoice"),pwo=o(" (RoFormer model)"),_wo=l(),V2=a("li"),Gte=a("strong"),bwo=o("squeezebert"),vwo=o(" \u2014 "),VI=a("a"),Two=o("SqueezeBertForMultipleChoice"),Fwo=o(" (SqueezeBERT model)"),Cwo=l(),z2=a("li"),Ote=a("strong"),Mwo=o("xlm"),Ewo=o(" \u2014 "),zI=a("a"),ywo=o("XLMForMultipleChoice"),wwo=o(" (XLM model)"),Awo=l(),W2=a("li"),Xte=a("strong"),Lwo=o("xlm-roberta"),Bwo=o(" \u2014 "),WI=a("a"),xwo=o("XLMRobertaForMultipleChoice"),kwo=o(" (XLM-RoBERTa model)"),Rwo=l(),Q2=a("li"),Vte=a("strong"),Swo=o("xlm-roberta-xl"),Pwo=o(" \u2014 "),QI=a("a"),$wo=o("XLMRobertaXLForMultipleChoice"),Iwo=o(" (XLM-RoBERTa-XL model)"),Dwo=l(),H2=a("li"),zte=a("strong"),jwo=o("xlnet"),Nwo=o(" \u2014 "),HI=a("a"),qwo=o("XLNetForMultipleChoice"),Gwo=o(" (XLNet model)"),Owo=l(),U2=a("li"),Wte=a("strong"),Xwo=o("yoso"),Vwo=o(" \u2014 "),UI=a("a"),zwo=o("YosoForMultipleChoice"),Wwo=o(" (YOSO model)"),Qwo=l(),J2=a("p"),Hwo=o("The model is set in evaluation mode by default using "),Qte=a("code"),Uwo=o("model.eval()"),Jwo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hte=a("code"),Ywo=o("model.train()"),Kwo=l(),Ute=a("p"),Zwo=o("Examples:"),e6o=l(),m(Hy.$$.fragment),V9e=l(),dd=a("h2"),Y2=a("a"),Jte=a("span"),m(Uy.$$.fragment),o6o=l(),Yte=a("span"),r6o=o("AutoModelForNextSentencePrediction"),z9e=l(),er=a("div"),m(Jy.$$.fragment),t6o=l(),cd=a("p"),a6o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Kte=a("code"),s6o=o("from_pretrained()"),n6o=o("class method or the "),Zte=a("code"),l6o=o("from_config()"),i6o=o(`class
method.`),d6o=l(),Yy=a("p"),c6o=o("This class cannot be instantiated directly using "),eae=a("code"),m6o=o("__init__()"),f6o=o(" (throws an error)."),g6o=l(),Qr=a("div"),m(Ky.$$.fragment),h6o=l(),oae=a("p"),u6o=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),p6o=l(),md=a("p"),_6o=o(`Note:
Loading a model from its configuration file does `),rae=a("strong"),b6o=o("not"),v6o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tae=a("code"),T6o=o("from_pretrained()"),F6o=o("to load the model weights."),C6o=l(),aae=a("p"),M6o=o("Examples:"),E6o=l(),m(Zy.$$.fragment),y6o=l(),Ne=a("div"),m(ew.$$.fragment),w6o=l(),sae=a("p"),A6o=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),L6o=l(),Wa=a("p"),B6o=o("The model class to instantiate is selected based on the "),nae=a("code"),x6o=o("model_type"),k6o=o(` property of the config object (either
passed as an argument or loaded from `),lae=a("code"),R6o=o("pretrained_model_name_or_path"),S6o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iae=a("code"),P6o=o("pretrained_model_name_or_path"),$6o=o(":"),I6o=l(),sa=a("ul"),K2=a("li"),dae=a("strong"),D6o=o("bert"),j6o=o(" \u2014 "),JI=a("a"),N6o=o("BertForNextSentencePrediction"),q6o=o(" (BERT model)"),G6o=l(),Z2=a("li"),cae=a("strong"),O6o=o("fnet"),X6o=o(" \u2014 "),YI=a("a"),V6o=o("FNetForNextSentencePrediction"),z6o=o(" (FNet model)"),W6o=l(),ev=a("li"),mae=a("strong"),Q6o=o("megatron-bert"),H6o=o(" \u2014 "),KI=a("a"),U6o=o("MegatronBertForNextSentencePrediction"),J6o=o(" (MegatronBert model)"),Y6o=l(),ov=a("li"),fae=a("strong"),K6o=o("mobilebert"),Z6o=o(" \u2014 "),ZI=a("a"),eAo=o("MobileBertForNextSentencePrediction"),oAo=o(" (MobileBERT model)"),rAo=l(),rv=a("li"),gae=a("strong"),tAo=o("qdqbert"),aAo=o(" \u2014 "),eD=a("a"),sAo=o("QDQBertForNextSentencePrediction"),nAo=o(" (QDQBert model)"),lAo=l(),tv=a("p"),iAo=o("The model is set in evaluation mode by default using "),hae=a("code"),dAo=o("model.eval()"),cAo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),uae=a("code"),mAo=o("model.train()"),fAo=l(),pae=a("p"),gAo=o("Examples:"),hAo=l(),m(ow.$$.fragment),W9e=l(),fd=a("h2"),av=a("a"),_ae=a("span"),m(rw.$$.fragment),uAo=l(),bae=a("span"),pAo=o("AutoModelForTokenClassification"),Q9e=l(),or=a("div"),m(tw.$$.fragment),_Ao=l(),gd=a("p"),bAo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),vae=a("code"),vAo=o("from_pretrained()"),TAo=o("class method or the "),Tae=a("code"),FAo=o("from_config()"),CAo=o(`class
method.`),MAo=l(),aw=a("p"),EAo=o("This class cannot be instantiated directly using "),Fae=a("code"),yAo=o("__init__()"),wAo=o(" (throws an error)."),AAo=l(),Hr=a("div"),m(sw.$$.fragment),LAo=l(),Cae=a("p"),BAo=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),xAo=l(),hd=a("p"),kAo=o(`Note:
Loading a model from its configuration file does `),Mae=a("strong"),RAo=o("not"),SAo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Eae=a("code"),PAo=o("from_pretrained()"),$Ao=o("to load the model weights."),IAo=l(),yae=a("p"),DAo=o("Examples:"),jAo=l(),m(nw.$$.fragment),NAo=l(),qe=a("div"),m(lw.$$.fragment),qAo=l(),wae=a("p"),GAo=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),OAo=l(),Qa=a("p"),XAo=o("The model class to instantiate is selected based on the "),Aae=a("code"),VAo=o("model_type"),zAo=o(` property of the config object (either
passed as an argument or loaded from `),Lae=a("code"),WAo=o("pretrained_model_name_or_path"),QAo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bae=a("code"),HAo=o("pretrained_model_name_or_path"),UAo=o(":"),JAo=l(),N=a("ul"),sv=a("li"),xae=a("strong"),YAo=o("albert"),KAo=o(" \u2014 "),oD=a("a"),ZAo=o("AlbertForTokenClassification"),e0o=o(" (ALBERT model)"),o0o=l(),nv=a("li"),kae=a("strong"),r0o=o("bert"),t0o=o(" \u2014 "),rD=a("a"),a0o=o("BertForTokenClassification"),s0o=o(" (BERT model)"),n0o=l(),lv=a("li"),Rae=a("strong"),l0o=o("big_bird"),i0o=o(" \u2014 "),tD=a("a"),d0o=o("BigBirdForTokenClassification"),c0o=o(" (BigBird model)"),m0o=l(),iv=a("li"),Sae=a("strong"),f0o=o("camembert"),g0o=o(" \u2014 "),aD=a("a"),h0o=o("CamembertForTokenClassification"),u0o=o(" (CamemBERT model)"),p0o=l(),dv=a("li"),Pae=a("strong"),_0o=o("canine"),b0o=o(" \u2014 "),sD=a("a"),v0o=o("CanineForTokenClassification"),T0o=o(" (Canine model)"),F0o=l(),cv=a("li"),$ae=a("strong"),C0o=o("convbert"),M0o=o(" \u2014 "),nD=a("a"),E0o=o("ConvBertForTokenClassification"),y0o=o(" (ConvBERT model)"),w0o=l(),mv=a("li"),Iae=a("strong"),A0o=o("data2vec-text"),L0o=o(" \u2014 "),lD=a("a"),B0o=o("Data2VecTextForTokenClassification"),x0o=o(" (Data2VecText model)"),k0o=l(),fv=a("li"),Dae=a("strong"),R0o=o("deberta"),S0o=o(" \u2014 "),iD=a("a"),P0o=o("DebertaForTokenClassification"),$0o=o(" (DeBERTa model)"),I0o=l(),gv=a("li"),jae=a("strong"),D0o=o("deberta-v2"),j0o=o(" \u2014 "),dD=a("a"),N0o=o("DebertaV2ForTokenClassification"),q0o=o(" (DeBERTa-v2 model)"),G0o=l(),hv=a("li"),Nae=a("strong"),O0o=o("distilbert"),X0o=o(" \u2014 "),cD=a("a"),V0o=o("DistilBertForTokenClassification"),z0o=o(" (DistilBERT model)"),W0o=l(),uv=a("li"),qae=a("strong"),Q0o=o("electra"),H0o=o(" \u2014 "),mD=a("a"),U0o=o("ElectraForTokenClassification"),J0o=o(" (ELECTRA model)"),Y0o=l(),pv=a("li"),Gae=a("strong"),K0o=o("flaubert"),Z0o=o(" \u2014 "),fD=a("a"),eLo=o("FlaubertForTokenClassification"),oLo=o(" (FlauBERT model)"),rLo=l(),_v=a("li"),Oae=a("strong"),tLo=o("fnet"),aLo=o(" \u2014 "),gD=a("a"),sLo=o("FNetForTokenClassification"),nLo=o(" (FNet model)"),lLo=l(),bv=a("li"),Xae=a("strong"),iLo=o("funnel"),dLo=o(" \u2014 "),hD=a("a"),cLo=o("FunnelForTokenClassification"),mLo=o(" (Funnel Transformer model)"),fLo=l(),vv=a("li"),Vae=a("strong"),gLo=o("gpt2"),hLo=o(" \u2014 "),uD=a("a"),uLo=o("GPT2ForTokenClassification"),pLo=o(" (OpenAI GPT-2 model)"),_Lo=l(),Tv=a("li"),zae=a("strong"),bLo=o("ibert"),vLo=o(" \u2014 "),pD=a("a"),TLo=o("IBertForTokenClassification"),FLo=o(" (I-BERT model)"),CLo=l(),Fv=a("li"),Wae=a("strong"),MLo=o("layoutlm"),ELo=o(" \u2014 "),_D=a("a"),yLo=o("LayoutLMForTokenClassification"),wLo=o(" (LayoutLM model)"),ALo=l(),Cv=a("li"),Qae=a("strong"),LLo=o("layoutlmv2"),BLo=o(" \u2014 "),bD=a("a"),xLo=o("LayoutLMv2ForTokenClassification"),kLo=o(" (LayoutLMv2 model)"),RLo=l(),Mv=a("li"),Hae=a("strong"),SLo=o("longformer"),PLo=o(" \u2014 "),vD=a("a"),$Lo=o("LongformerForTokenClassification"),ILo=o(" (Longformer model)"),DLo=l(),Ev=a("li"),Uae=a("strong"),jLo=o("megatron-bert"),NLo=o(" \u2014 "),TD=a("a"),qLo=o("MegatronBertForTokenClassification"),GLo=o(" (MegatronBert model)"),OLo=l(),yv=a("li"),Jae=a("strong"),XLo=o("mobilebert"),VLo=o(" \u2014 "),FD=a("a"),zLo=o("MobileBertForTokenClassification"),WLo=o(" (MobileBERT model)"),QLo=l(),wv=a("li"),Yae=a("strong"),HLo=o("mpnet"),ULo=o(" \u2014 "),CD=a("a"),JLo=o("MPNetForTokenClassification"),YLo=o(" (MPNet model)"),KLo=l(),Av=a("li"),Kae=a("strong"),ZLo=o("nystromformer"),e8o=o(" \u2014 "),MD=a("a"),o8o=o("NystromformerForTokenClassification"),r8o=o(" (Nystromformer model)"),t8o=l(),Lv=a("li"),Zae=a("strong"),a8o=o("qdqbert"),s8o=o(" \u2014 "),ED=a("a"),n8o=o("QDQBertForTokenClassification"),l8o=o(" (QDQBert model)"),i8o=l(),Bv=a("li"),ese=a("strong"),d8o=o("rembert"),c8o=o(" \u2014 "),yD=a("a"),m8o=o("RemBertForTokenClassification"),f8o=o(" (RemBERT model)"),g8o=l(),xv=a("li"),ose=a("strong"),h8o=o("roberta"),u8o=o(" \u2014 "),wD=a("a"),p8o=o("RobertaForTokenClassification"),_8o=o(" (RoBERTa model)"),b8o=l(),kv=a("li"),rse=a("strong"),v8o=o("roformer"),T8o=o(" \u2014 "),AD=a("a"),F8o=o("RoFormerForTokenClassification"),C8o=o(" (RoFormer model)"),M8o=l(),Rv=a("li"),tse=a("strong"),E8o=o("squeezebert"),y8o=o(" \u2014 "),LD=a("a"),w8o=o("SqueezeBertForTokenClassification"),A8o=o(" (SqueezeBERT model)"),L8o=l(),Sv=a("li"),ase=a("strong"),B8o=o("xlm"),x8o=o(" \u2014 "),BD=a("a"),k8o=o("XLMForTokenClassification"),R8o=o(" (XLM model)"),S8o=l(),Pv=a("li"),sse=a("strong"),P8o=o("xlm-roberta"),$8o=o(" \u2014 "),xD=a("a"),I8o=o("XLMRobertaForTokenClassification"),D8o=o(" (XLM-RoBERTa model)"),j8o=l(),$v=a("li"),nse=a("strong"),N8o=o("xlm-roberta-xl"),q8o=o(" \u2014 "),kD=a("a"),G8o=o("XLMRobertaXLForTokenClassification"),O8o=o(" (XLM-RoBERTa-XL model)"),X8o=l(),Iv=a("li"),lse=a("strong"),V8o=o("xlnet"),z8o=o(" \u2014 "),RD=a("a"),W8o=o("XLNetForTokenClassification"),Q8o=o(" (XLNet model)"),H8o=l(),Dv=a("li"),ise=a("strong"),U8o=o("yoso"),J8o=o(" \u2014 "),SD=a("a"),Y8o=o("YosoForTokenClassification"),K8o=o(" (YOSO model)"),Z8o=l(),jv=a("p"),e7o=o("The model is set in evaluation mode by default using "),dse=a("code"),o7o=o("model.eval()"),r7o=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),cse=a("code"),t7o=o("model.train()"),a7o=l(),mse=a("p"),s7o=o("Examples:"),n7o=l(),m(iw.$$.fragment),H9e=l(),ud=a("h2"),Nv=a("a"),fse=a("span"),m(dw.$$.fragment),l7o=l(),gse=a("span"),i7o=o("AutoModelForQuestionAnswering"),U9e=l(),rr=a("div"),m(cw.$$.fragment),d7o=l(),pd=a("p"),c7o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),hse=a("code"),m7o=o("from_pretrained()"),f7o=o("class method or the "),use=a("code"),g7o=o("from_config()"),h7o=o(`class
method.`),u7o=l(),mw=a("p"),p7o=o("This class cannot be instantiated directly using "),pse=a("code"),_7o=o("__init__()"),b7o=o(" (throws an error)."),v7o=l(),Ur=a("div"),m(fw.$$.fragment),T7o=l(),_se=a("p"),F7o=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),C7o=l(),_d=a("p"),M7o=o(`Note:
Loading a model from its configuration file does `),bse=a("strong"),E7o=o("not"),y7o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),vse=a("code"),w7o=o("from_pretrained()"),A7o=o("to load the model weights."),L7o=l(),Tse=a("p"),B7o=o("Examples:"),x7o=l(),m(gw.$$.fragment),k7o=l(),Ge=a("div"),m(hw.$$.fragment),R7o=l(),Fse=a("p"),S7o=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),P7o=l(),Ha=a("p"),$7o=o("The model class to instantiate is selected based on the "),Cse=a("code"),I7o=o("model_type"),D7o=o(` property of the config object (either
passed as an argument or loaded from `),Mse=a("code"),j7o=o("pretrained_model_name_or_path"),N7o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ese=a("code"),q7o=o("pretrained_model_name_or_path"),G7o=o(":"),O7o=l(),R=a("ul"),qv=a("li"),yse=a("strong"),X7o=o("albert"),V7o=o(" \u2014 "),PD=a("a"),z7o=o("AlbertForQuestionAnswering"),W7o=o(" (ALBERT model)"),Q7o=l(),Gv=a("li"),wse=a("strong"),H7o=o("bart"),U7o=o(" \u2014 "),$D=a("a"),J7o=o("BartForQuestionAnswering"),Y7o=o(" (BART model)"),K7o=l(),Ov=a("li"),Ase=a("strong"),Z7o=o("bert"),e9o=o(" \u2014 "),ID=a("a"),o9o=o("BertForQuestionAnswering"),r9o=o(" (BERT model)"),t9o=l(),Xv=a("li"),Lse=a("strong"),a9o=o("big_bird"),s9o=o(" \u2014 "),DD=a("a"),n9o=o("BigBirdForQuestionAnswering"),l9o=o(" (BigBird model)"),i9o=l(),Vv=a("li"),Bse=a("strong"),d9o=o("bigbird_pegasus"),c9o=o(" \u2014 "),jD=a("a"),m9o=o("BigBirdPegasusForQuestionAnswering"),f9o=o(" (BigBirdPegasus model)"),g9o=l(),zv=a("li"),xse=a("strong"),h9o=o("camembert"),u9o=o(" \u2014 "),ND=a("a"),p9o=o("CamembertForQuestionAnswering"),_9o=o(" (CamemBERT model)"),b9o=l(),Wv=a("li"),kse=a("strong"),v9o=o("canine"),T9o=o(" \u2014 "),qD=a("a"),F9o=o("CanineForQuestionAnswering"),C9o=o(" (Canine model)"),M9o=l(),Qv=a("li"),Rse=a("strong"),E9o=o("convbert"),y9o=o(" \u2014 "),GD=a("a"),w9o=o("ConvBertForQuestionAnswering"),A9o=o(" (ConvBERT model)"),L9o=l(),Hv=a("li"),Sse=a("strong"),B9o=o("data2vec-text"),x9o=o(" \u2014 "),OD=a("a"),k9o=o("Data2VecTextForQuestionAnswering"),R9o=o(" (Data2VecText model)"),S9o=l(),Uv=a("li"),Pse=a("strong"),P9o=o("deberta"),$9o=o(" \u2014 "),XD=a("a"),I9o=o("DebertaForQuestionAnswering"),D9o=o(" (DeBERTa model)"),j9o=l(),Jv=a("li"),$se=a("strong"),N9o=o("deberta-v2"),q9o=o(" \u2014 "),VD=a("a"),G9o=o("DebertaV2ForQuestionAnswering"),O9o=o(" (DeBERTa-v2 model)"),X9o=l(),Yv=a("li"),Ise=a("strong"),V9o=o("distilbert"),z9o=o(" \u2014 "),zD=a("a"),W9o=o("DistilBertForQuestionAnswering"),Q9o=o(" (DistilBERT model)"),H9o=l(),Kv=a("li"),Dse=a("strong"),U9o=o("electra"),J9o=o(" \u2014 "),WD=a("a"),Y9o=o("ElectraForQuestionAnswering"),K9o=o(" (ELECTRA model)"),Z9o=l(),Zv=a("li"),jse=a("strong"),eBo=o("flaubert"),oBo=o(" \u2014 "),QD=a("a"),rBo=o("FlaubertForQuestionAnsweringSimple"),tBo=o(" (FlauBERT model)"),aBo=l(),eT=a("li"),Nse=a("strong"),sBo=o("fnet"),nBo=o(" \u2014 "),HD=a("a"),lBo=o("FNetForQuestionAnswering"),iBo=o(" (FNet model)"),dBo=l(),oT=a("li"),qse=a("strong"),cBo=o("funnel"),mBo=o(" \u2014 "),UD=a("a"),fBo=o("FunnelForQuestionAnswering"),gBo=o(" (Funnel Transformer model)"),hBo=l(),rT=a("li"),Gse=a("strong"),uBo=o("gptj"),pBo=o(" \u2014 "),JD=a("a"),_Bo=o("GPTJForQuestionAnswering"),bBo=o(" (GPT-J model)"),vBo=l(),tT=a("li"),Ose=a("strong"),TBo=o("ibert"),FBo=o(" \u2014 "),YD=a("a"),CBo=o("IBertForQuestionAnswering"),MBo=o(" (I-BERT model)"),EBo=l(),aT=a("li"),Xse=a("strong"),yBo=o("layoutlmv2"),wBo=o(" \u2014 "),KD=a("a"),ABo=o("LayoutLMv2ForQuestionAnswering"),LBo=o(" (LayoutLMv2 model)"),BBo=l(),sT=a("li"),Vse=a("strong"),xBo=o("led"),kBo=o(" \u2014 "),ZD=a("a"),RBo=o("LEDForQuestionAnswering"),SBo=o(" (LED model)"),PBo=l(),nT=a("li"),zse=a("strong"),$Bo=o("longformer"),IBo=o(" \u2014 "),ej=a("a"),DBo=o("LongformerForQuestionAnswering"),jBo=o(" (Longformer model)"),NBo=l(),lT=a("li"),Wse=a("strong"),qBo=o("lxmert"),GBo=o(" \u2014 "),oj=a("a"),OBo=o("LxmertForQuestionAnswering"),XBo=o(" (LXMERT model)"),VBo=l(),iT=a("li"),Qse=a("strong"),zBo=o("mbart"),WBo=o(" \u2014 "),rj=a("a"),QBo=o("MBartForQuestionAnswering"),HBo=o(" (mBART model)"),UBo=l(),dT=a("li"),Hse=a("strong"),JBo=o("megatron-bert"),YBo=o(" \u2014 "),tj=a("a"),KBo=o("MegatronBertForQuestionAnswering"),ZBo=o(" (MegatronBert model)"),exo=l(),cT=a("li"),Use=a("strong"),oxo=o("mobilebert"),rxo=o(" \u2014 "),aj=a("a"),txo=o("MobileBertForQuestionAnswering"),axo=o(" (MobileBERT model)"),sxo=l(),mT=a("li"),Jse=a("strong"),nxo=o("mpnet"),lxo=o(" \u2014 "),sj=a("a"),ixo=o("MPNetForQuestionAnswering"),dxo=o(" (MPNet model)"),cxo=l(),fT=a("li"),Yse=a("strong"),mxo=o("nystromformer"),fxo=o(" \u2014 "),nj=a("a"),gxo=o("NystromformerForQuestionAnswering"),hxo=o(" (Nystromformer model)"),uxo=l(),gT=a("li"),Kse=a("strong"),pxo=o("qdqbert"),_xo=o(" \u2014 "),lj=a("a"),bxo=o("QDQBertForQuestionAnswering"),vxo=o(" (QDQBert model)"),Txo=l(),hT=a("li"),Zse=a("strong"),Fxo=o("reformer"),Cxo=o(" \u2014 "),ij=a("a"),Mxo=o("ReformerForQuestionAnswering"),Exo=o(" (Reformer model)"),yxo=l(),uT=a("li"),ene=a("strong"),wxo=o("rembert"),Axo=o(" \u2014 "),dj=a("a"),Lxo=o("RemBertForQuestionAnswering"),Bxo=o(" (RemBERT model)"),xxo=l(),pT=a("li"),one=a("strong"),kxo=o("roberta"),Rxo=o(" \u2014 "),cj=a("a"),Sxo=o("RobertaForQuestionAnswering"),Pxo=o(" (RoBERTa model)"),$xo=l(),_T=a("li"),rne=a("strong"),Ixo=o("roformer"),Dxo=o(" \u2014 "),mj=a("a"),jxo=o("RoFormerForQuestionAnswering"),Nxo=o(" (RoFormer model)"),qxo=l(),bT=a("li"),tne=a("strong"),Gxo=o("splinter"),Oxo=o(" \u2014 "),fj=a("a"),Xxo=o("SplinterForQuestionAnswering"),Vxo=o(" (Splinter model)"),zxo=l(),vT=a("li"),ane=a("strong"),Wxo=o("squeezebert"),Qxo=o(" \u2014 "),gj=a("a"),Hxo=o("SqueezeBertForQuestionAnswering"),Uxo=o(" (SqueezeBERT model)"),Jxo=l(),TT=a("li"),sne=a("strong"),Yxo=o("xlm"),Kxo=o(" \u2014 "),hj=a("a"),Zxo=o("XLMForQuestionAnsweringSimple"),eko=o(" (XLM model)"),oko=l(),FT=a("li"),nne=a("strong"),rko=o("xlm-roberta"),tko=o(" \u2014 "),uj=a("a"),ako=o("XLMRobertaForQuestionAnswering"),sko=o(" (XLM-RoBERTa model)"),nko=l(),CT=a("li"),lne=a("strong"),lko=o("xlm-roberta-xl"),iko=o(" \u2014 "),pj=a("a"),dko=o("XLMRobertaXLForQuestionAnswering"),cko=o(" (XLM-RoBERTa-XL model)"),mko=l(),MT=a("li"),ine=a("strong"),fko=o("xlnet"),gko=o(" \u2014 "),_j=a("a"),hko=o("XLNetForQuestionAnsweringSimple"),uko=o(" (XLNet model)"),pko=l(),ET=a("li"),dne=a("strong"),_ko=o("yoso"),bko=o(" \u2014 "),bj=a("a"),vko=o("YosoForQuestionAnswering"),Tko=o(" (YOSO model)"),Fko=l(),yT=a("p"),Cko=o("The model is set in evaluation mode by default using "),cne=a("code"),Mko=o("model.eval()"),Eko=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mne=a("code"),yko=o("model.train()"),wko=l(),fne=a("p"),Ako=o("Examples:"),Lko=l(),m(uw.$$.fragment),J9e=l(),bd=a("h2"),wT=a("a"),gne=a("span"),m(pw.$$.fragment),Bko=l(),hne=a("span"),xko=o("AutoModelForTableQuestionAnswering"),Y9e=l(),tr=a("div"),m(_w.$$.fragment),kko=l(),vd=a("p"),Rko=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),une=a("code"),Sko=o("from_pretrained()"),Pko=o("class method or the "),pne=a("code"),$ko=o("from_config()"),Iko=o(`class
method.`),Dko=l(),bw=a("p"),jko=o("This class cannot be instantiated directly using "),_ne=a("code"),Nko=o("__init__()"),qko=o(" (throws an error)."),Gko=l(),Jr=a("div"),m(vw.$$.fragment),Oko=l(),bne=a("p"),Xko=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Vko=l(),Td=a("p"),zko=o(`Note:
Loading a model from its configuration file does `),vne=a("strong"),Wko=o("not"),Qko=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Tne=a("code"),Hko=o("from_pretrained()"),Uko=o("to load the model weights."),Jko=l(),Fne=a("p"),Yko=o("Examples:"),Kko=l(),m(Tw.$$.fragment),Zko=l(),Oe=a("div"),m(Fw.$$.fragment),eRo=l(),Cne=a("p"),oRo=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),rRo=l(),Ua=a("p"),tRo=o("The model class to instantiate is selected based on the "),Mne=a("code"),aRo=o("model_type"),sRo=o(` property of the config object (either
passed as an argument or loaded from `),Ene=a("code"),nRo=o("pretrained_model_name_or_path"),lRo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yne=a("code"),iRo=o("pretrained_model_name_or_path"),dRo=o(":"),cRo=l(),wne=a("ul"),AT=a("li"),Ane=a("strong"),mRo=o("tapas"),fRo=o(" \u2014 "),vj=a("a"),gRo=o("TapasForQuestionAnswering"),hRo=o(" (TAPAS model)"),uRo=l(),LT=a("p"),pRo=o("The model is set in evaluation mode by default using "),Lne=a("code"),_Ro=o("model.eval()"),bRo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Bne=a("code"),vRo=o("model.train()"),TRo=l(),xne=a("p"),FRo=o("Examples:"),CRo=l(),m(Cw.$$.fragment),K9e=l(),Fd=a("h2"),BT=a("a"),kne=a("span"),m(Mw.$$.fragment),MRo=l(),Rne=a("span"),ERo=o("AutoModelForImageClassification"),Z9e=l(),ar=a("div"),m(Ew.$$.fragment),yRo=l(),Cd=a("p"),wRo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Sne=a("code"),ARo=o("from_pretrained()"),LRo=o("class method or the "),Pne=a("code"),BRo=o("from_config()"),xRo=o(`class
method.`),kRo=l(),yw=a("p"),RRo=o("This class cannot be instantiated directly using "),$ne=a("code"),SRo=o("__init__()"),PRo=o(" (throws an error)."),$Ro=l(),Yr=a("div"),m(ww.$$.fragment),IRo=l(),Ine=a("p"),DRo=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),jRo=l(),Md=a("p"),NRo=o(`Note:
Loading a model from its configuration file does `),Dne=a("strong"),qRo=o("not"),GRo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),jne=a("code"),ORo=o("from_pretrained()"),XRo=o("to load the model weights."),VRo=l(),Nne=a("p"),zRo=o("Examples:"),WRo=l(),m(Aw.$$.fragment),QRo=l(),Xe=a("div"),m(Lw.$$.fragment),HRo=l(),qne=a("p"),URo=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),JRo=l(),Ja=a("p"),YRo=o("The model class to instantiate is selected based on the "),Gne=a("code"),KRo=o("model_type"),ZRo=o(` property of the config object (either
passed as an argument or loaded from `),One=a("code"),eSo=o("pretrained_model_name_or_path"),oSo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xne=a("code"),rSo=o("pretrained_model_name_or_path"),tSo=o(":"),aSo=l(),be=a("ul"),xT=a("li"),Vne=a("strong"),sSo=o("beit"),nSo=o(" \u2014 "),Tj=a("a"),lSo=o("BeitForImageClassification"),iSo=o(" (BEiT model)"),dSo=l(),kT=a("li"),zne=a("strong"),cSo=o("convnext"),mSo=o(" \u2014 "),Fj=a("a"),fSo=o("ConvNextForImageClassification"),gSo=o(" (ConvNext model)"),hSo=l(),Pn=a("li"),Wne=a("strong"),uSo=o("deit"),pSo=o(" \u2014 "),Cj=a("a"),_So=o("DeiTForImageClassification"),bSo=o(" or "),Mj=a("a"),vSo=o("DeiTForImageClassificationWithTeacher"),TSo=o(" (DeiT model)"),FSo=l(),RT=a("li"),Qne=a("strong"),CSo=o("imagegpt"),MSo=o(" \u2014 "),Ej=a("a"),ESo=o("ImageGPTForImageClassification"),ySo=o(" (ImageGPT model)"),wSo=l(),la=a("li"),Hne=a("strong"),ASo=o("perceiver"),LSo=o(" \u2014 "),yj=a("a"),BSo=o("PerceiverForImageClassificationLearned"),xSo=o(" or "),wj=a("a"),kSo=o("PerceiverForImageClassificationFourier"),RSo=o(" or "),Aj=a("a"),SSo=o("PerceiverForImageClassificationConvProcessing"),PSo=o(" (Perceiver model)"),$So=l(),ST=a("li"),Une=a("strong"),ISo=o("poolformer"),DSo=o(" \u2014 "),Lj=a("a"),jSo=o("PoolFormerForImageClassification"),NSo=o(" (PoolFormer model)"),qSo=l(),PT=a("li"),Jne=a("strong"),GSo=o("segformer"),OSo=o(" \u2014 "),Bj=a("a"),XSo=o("SegformerForImageClassification"),VSo=o(" (SegFormer model)"),zSo=l(),$T=a("li"),Yne=a("strong"),WSo=o("swin"),QSo=o(" \u2014 "),xj=a("a"),HSo=o("SwinForImageClassification"),USo=o(" (Swin model)"),JSo=l(),IT=a("li"),Kne=a("strong"),YSo=o("vit"),KSo=o(" \u2014 "),kj=a("a"),ZSo=o("ViTForImageClassification"),ePo=o(" (ViT model)"),oPo=l(),DT=a("p"),rPo=o("The model is set in evaluation mode by default using "),Zne=a("code"),tPo=o("model.eval()"),aPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ele=a("code"),sPo=o("model.train()"),nPo=l(),ole=a("p"),lPo=o("Examples:"),iPo=l(),m(Bw.$$.fragment),eBe=l(),Ed=a("h2"),jT=a("a"),rle=a("span"),m(xw.$$.fragment),dPo=l(),tle=a("span"),cPo=o("AutoModelForVision2Seq"),oBe=l(),sr=a("div"),m(kw.$$.fragment),mPo=l(),yd=a("p"),fPo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),ale=a("code"),gPo=o("from_pretrained()"),hPo=o("class method or the "),sle=a("code"),uPo=o("from_config()"),pPo=o(`class
method.`),_Po=l(),Rw=a("p"),bPo=o("This class cannot be instantiated directly using "),nle=a("code"),vPo=o("__init__()"),TPo=o(" (throws an error)."),FPo=l(),Kr=a("div"),m(Sw.$$.fragment),CPo=l(),lle=a("p"),MPo=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),EPo=l(),wd=a("p"),yPo=o(`Note:
Loading a model from its configuration file does `),ile=a("strong"),wPo=o("not"),APo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),dle=a("code"),LPo=o("from_pretrained()"),BPo=o("to load the model weights."),xPo=l(),cle=a("p"),kPo=o("Examples:"),RPo=l(),m(Pw.$$.fragment),SPo=l(),Ve=a("div"),m($w.$$.fragment),PPo=l(),mle=a("p"),$Po=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),IPo=l(),Ya=a("p"),DPo=o("The model class to instantiate is selected based on the "),fle=a("code"),jPo=o("model_type"),NPo=o(` property of the config object (either
passed as an argument or loaded from `),gle=a("code"),qPo=o("pretrained_model_name_or_path"),GPo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hle=a("code"),OPo=o("pretrained_model_name_or_path"),XPo=o(":"),VPo=l(),ule=a("ul"),NT=a("li"),ple=a("strong"),zPo=o("vision-encoder-decoder"),WPo=o(" \u2014 "),Rj=a("a"),QPo=o("VisionEncoderDecoderModel"),HPo=o(" (Vision Encoder decoder model)"),UPo=l(),qT=a("p"),JPo=o("The model is set in evaluation mode by default using "),_le=a("code"),YPo=o("model.eval()"),KPo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ble=a("code"),ZPo=o("model.train()"),e$o=l(),vle=a("p"),o$o=o("Examples:"),r$o=l(),m(Iw.$$.fragment),rBe=l(),Ad=a("h2"),GT=a("a"),Tle=a("span"),m(Dw.$$.fragment),t$o=l(),Fle=a("span"),a$o=o("AutoModelForAudioClassification"),tBe=l(),nr=a("div"),m(jw.$$.fragment),s$o=l(),Ld=a("p"),n$o=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Cle=a("code"),l$o=o("from_pretrained()"),i$o=o("class method or the "),Mle=a("code"),d$o=o("from_config()"),c$o=o(`class
method.`),m$o=l(),Nw=a("p"),f$o=o("This class cannot be instantiated directly using "),Ele=a("code"),g$o=o("__init__()"),h$o=o(" (throws an error)."),u$o=l(),Zr=a("div"),m(qw.$$.fragment),p$o=l(),yle=a("p"),_$o=o("Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),b$o=l(),Bd=a("p"),v$o=o(`Note:
Loading a model from its configuration file does `),wle=a("strong"),T$o=o("not"),F$o=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Ale=a("code"),C$o=o("from_pretrained()"),M$o=o("to load the model weights."),E$o=l(),Lle=a("p"),y$o=o("Examples:"),w$o=l(),m(Gw.$$.fragment),A$o=l(),ze=a("div"),m(Ow.$$.fragment),L$o=l(),Ble=a("p"),B$o=o("Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),x$o=l(),Ka=a("p"),k$o=o("The model class to instantiate is selected based on the "),xle=a("code"),R$o=o("model_type"),S$o=o(` property of the config object (either
passed as an argument or loaded from `),kle=a("code"),P$o=o("pretrained_model_name_or_path"),$$o=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rle=a("code"),I$o=o("pretrained_model_name_or_path"),D$o=o(":"),j$o=l(),Ae=a("ul"),OT=a("li"),Sle=a("strong"),N$o=o("data2vec-audio"),q$o=o(" \u2014 "),Sj=a("a"),G$o=o("Data2VecAudioForSequenceClassification"),O$o=o(" (Data2VecAudio model)"),X$o=l(),XT=a("li"),Ple=a("strong"),V$o=o("hubert"),z$o=o(" \u2014 "),Pj=a("a"),W$o=o("HubertForSequenceClassification"),Q$o=o(" (Hubert model)"),H$o=l(),VT=a("li"),$le=a("strong"),U$o=o("sew"),J$o=o(" \u2014 "),$j=a("a"),Y$o=o("SEWForSequenceClassification"),K$o=o(" (SEW model)"),Z$o=l(),zT=a("li"),Ile=a("strong"),eIo=o("sew-d"),oIo=o(" \u2014 "),Ij=a("a"),rIo=o("SEWDForSequenceClassification"),tIo=o(" (SEW-D model)"),aIo=l(),WT=a("li"),Dle=a("strong"),sIo=o("unispeech"),nIo=o(" \u2014 "),Dj=a("a"),lIo=o("UniSpeechForSequenceClassification"),iIo=o(" (UniSpeech model)"),dIo=l(),QT=a("li"),jle=a("strong"),cIo=o("unispeech-sat"),mIo=o(" \u2014 "),jj=a("a"),fIo=o("UniSpeechSatForSequenceClassification"),gIo=o(" (UniSpeechSat model)"),hIo=l(),HT=a("li"),Nle=a("strong"),uIo=o("wav2vec2"),pIo=o(" \u2014 "),Nj=a("a"),_Io=o("Wav2Vec2ForSequenceClassification"),bIo=o(" (Wav2Vec2 model)"),vIo=l(),UT=a("li"),qle=a("strong"),TIo=o("wavlm"),FIo=o(" \u2014 "),qj=a("a"),CIo=o("WavLMForSequenceClassification"),MIo=o(" (WavLM model)"),EIo=l(),JT=a("p"),yIo=o("The model is set in evaluation mode by default using "),Gle=a("code"),wIo=o("model.eval()"),AIo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ole=a("code"),LIo=o("model.train()"),BIo=l(),Xle=a("p"),xIo=o("Examples:"),kIo=l(),m(Xw.$$.fragment),aBe=l(),xd=a("h2"),YT=a("a"),Vle=a("span"),m(Vw.$$.fragment),RIo=l(),zle=a("span"),SIo=o("AutoModelForAudioFrameClassification"),sBe=l(),lr=a("div"),m(zw.$$.fragment),PIo=l(),kd=a("p"),$Io=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Wle=a("code"),IIo=o("from_pretrained()"),DIo=o("class method or the "),Qle=a("code"),jIo=o("from_config()"),NIo=o(`class
method.`),qIo=l(),Ww=a("p"),GIo=o("This class cannot be instantiated directly using "),Hle=a("code"),OIo=o("__init__()"),XIo=o(" (throws an error)."),VIo=l(),et=a("div"),m(Qw.$$.fragment),zIo=l(),Ule=a("p"),WIo=o("Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),QIo=l(),Rd=a("p"),HIo=o(`Note:
Loading a model from its configuration file does `),Jle=a("strong"),UIo=o("not"),JIo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Yle=a("code"),YIo=o("from_pretrained()"),KIo=o("to load the model weights."),ZIo=l(),Kle=a("p"),eDo=o("Examples:"),oDo=l(),m(Hw.$$.fragment),rDo=l(),We=a("div"),m(Uw.$$.fragment),tDo=l(),Zle=a("p"),aDo=o("Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),sDo=l(),Za=a("p"),nDo=o("The model class to instantiate is selected based on the "),eie=a("code"),lDo=o("model_type"),iDo=o(` property of the config object (either
passed as an argument or loaded from `),oie=a("code"),dDo=o("pretrained_model_name_or_path"),cDo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rie=a("code"),mDo=o("pretrained_model_name_or_path"),fDo=o(":"),gDo=l(),es=a("ul"),KT=a("li"),tie=a("strong"),hDo=o("data2vec-audio"),uDo=o(" \u2014 "),Gj=a("a"),pDo=o("Data2VecAudioForAudioFrameClassification"),_Do=o(" (Data2VecAudio model)"),bDo=l(),ZT=a("li"),aie=a("strong"),vDo=o("unispeech-sat"),TDo=o(" \u2014 "),Oj=a("a"),FDo=o("UniSpeechSatForAudioFrameClassification"),CDo=o(" (UniSpeechSat model)"),MDo=l(),e1=a("li"),sie=a("strong"),EDo=o("wav2vec2"),yDo=o(" \u2014 "),Xj=a("a"),wDo=o("Wav2Vec2ForAudioFrameClassification"),ADo=o(" (Wav2Vec2 model)"),LDo=l(),o1=a("li"),nie=a("strong"),BDo=o("wavlm"),xDo=o(" \u2014 "),Vj=a("a"),kDo=o("WavLMForAudioFrameClassification"),RDo=o(" (WavLM model)"),SDo=l(),r1=a("p"),PDo=o("The model is set in evaluation mode by default using "),lie=a("code"),$Do=o("model.eval()"),IDo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),iie=a("code"),DDo=o("model.train()"),jDo=l(),die=a("p"),NDo=o("Examples:"),qDo=l(),m(Jw.$$.fragment),nBe=l(),Sd=a("h2"),t1=a("a"),cie=a("span"),m(Yw.$$.fragment),GDo=l(),mie=a("span"),ODo=o("AutoModelForCTC"),lBe=l(),ir=a("div"),m(Kw.$$.fragment),XDo=l(),Pd=a("p"),VDo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),fie=a("code"),zDo=o("from_pretrained()"),WDo=o("class method or the "),gie=a("code"),QDo=o("from_config()"),HDo=o(`class
method.`),UDo=l(),Zw=a("p"),JDo=o("This class cannot be instantiated directly using "),hie=a("code"),YDo=o("__init__()"),KDo=o(" (throws an error)."),ZDo=l(),ot=a("div"),m(e6.$$.fragment),ejo=l(),uie=a("p"),ojo=o("Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),rjo=l(),$d=a("p"),tjo=o(`Note:
Loading a model from its configuration file does `),pie=a("strong"),ajo=o("not"),sjo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),_ie=a("code"),njo=o("from_pretrained()"),ljo=o("to load the model weights."),ijo=l(),bie=a("p"),djo=o("Examples:"),cjo=l(),m(o6.$$.fragment),mjo=l(),Qe=a("div"),m(r6.$$.fragment),fjo=l(),vie=a("p"),gjo=o("Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),hjo=l(),os=a("p"),ujo=o("The model class to instantiate is selected based on the "),Tie=a("code"),pjo=o("model_type"),_jo=o(` property of the config object (either
passed as an argument or loaded from `),Fie=a("code"),bjo=o("pretrained_model_name_or_path"),vjo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cie=a("code"),Tjo=o("pretrained_model_name_or_path"),Fjo=o(":"),Cjo=l(),Le=a("ul"),a1=a("li"),Mie=a("strong"),Mjo=o("data2vec-audio"),Ejo=o(" \u2014 "),zj=a("a"),yjo=o("Data2VecAudioForCTC"),wjo=o(" (Data2VecAudio model)"),Ajo=l(),s1=a("li"),Eie=a("strong"),Ljo=o("hubert"),Bjo=o(" \u2014 "),Wj=a("a"),xjo=o("HubertForCTC"),kjo=o(" (Hubert model)"),Rjo=l(),n1=a("li"),yie=a("strong"),Sjo=o("sew"),Pjo=o(" \u2014 "),Qj=a("a"),$jo=o("SEWForCTC"),Ijo=o(" (SEW model)"),Djo=l(),l1=a("li"),wie=a("strong"),jjo=o("sew-d"),Njo=o(" \u2014 "),Hj=a("a"),qjo=o("SEWDForCTC"),Gjo=o(" (SEW-D model)"),Ojo=l(),i1=a("li"),Aie=a("strong"),Xjo=o("unispeech"),Vjo=o(" \u2014 "),Uj=a("a"),zjo=o("UniSpeechForCTC"),Wjo=o(" (UniSpeech model)"),Qjo=l(),d1=a("li"),Lie=a("strong"),Hjo=o("unispeech-sat"),Ujo=o(" \u2014 "),Jj=a("a"),Jjo=o("UniSpeechSatForCTC"),Yjo=o(" (UniSpeechSat model)"),Kjo=l(),c1=a("li"),Bie=a("strong"),Zjo=o("wav2vec2"),eNo=o(" \u2014 "),Yj=a("a"),oNo=o("Wav2Vec2ForCTC"),rNo=o(" (Wav2Vec2 model)"),tNo=l(),m1=a("li"),xie=a("strong"),aNo=o("wavlm"),sNo=o(" \u2014 "),Kj=a("a"),nNo=o("WavLMForCTC"),lNo=o(" (WavLM model)"),iNo=l(),f1=a("p"),dNo=o("The model is set in evaluation mode by default using "),kie=a("code"),cNo=o("model.eval()"),mNo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rie=a("code"),fNo=o("model.train()"),gNo=l(),Sie=a("p"),hNo=o("Examples:"),uNo=l(),m(t6.$$.fragment),iBe=l(),Id=a("h2"),g1=a("a"),Pie=a("span"),m(a6.$$.fragment),pNo=l(),$ie=a("span"),_No=o("AutoModelForSpeechSeq2Seq"),dBe=l(),dr=a("div"),m(s6.$$.fragment),bNo=l(),Dd=a("p"),vNo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Iie=a("code"),TNo=o("from_pretrained()"),FNo=o("class method or the "),Die=a("code"),CNo=o("from_config()"),MNo=o(`class
method.`),ENo=l(),n6=a("p"),yNo=o("This class cannot be instantiated directly using "),jie=a("code"),wNo=o("__init__()"),ANo=o(" (throws an error)."),LNo=l(),rt=a("div"),m(l6.$$.fragment),BNo=l(),Nie=a("p"),xNo=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),kNo=l(),jd=a("p"),RNo=o(`Note:
Loading a model from its configuration file does `),qie=a("strong"),SNo=o("not"),PNo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Gie=a("code"),$No=o("from_pretrained()"),INo=o("to load the model weights."),DNo=l(),Oie=a("p"),jNo=o("Examples:"),NNo=l(),m(i6.$$.fragment),qNo=l(),He=a("div"),m(d6.$$.fragment),GNo=l(),Xie=a("p"),ONo=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),XNo=l(),rs=a("p"),VNo=o("The model class to instantiate is selected based on the "),Vie=a("code"),zNo=o("model_type"),WNo=o(` property of the config object (either
passed as an argument or loaded from `),zie=a("code"),QNo=o("pretrained_model_name_or_path"),HNo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wie=a("code"),UNo=o("pretrained_model_name_or_path"),JNo=o(":"),YNo=l(),c6=a("ul"),h1=a("li"),Qie=a("strong"),KNo=o("speech-encoder-decoder"),ZNo=o(" \u2014 "),Zj=a("a"),eqo=o("SpeechEncoderDecoderModel"),oqo=o(" (Speech Encoder decoder model)"),rqo=l(),u1=a("li"),Hie=a("strong"),tqo=o("speech_to_text"),aqo=o(" \u2014 "),eN=a("a"),sqo=o("Speech2TextForConditionalGeneration"),nqo=o(" (Speech2Text model)"),lqo=l(),p1=a("p"),iqo=o("The model is set in evaluation mode by default using "),Uie=a("code"),dqo=o("model.eval()"),cqo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Jie=a("code"),mqo=o("model.train()"),fqo=l(),Yie=a("p"),gqo=o("Examples:"),hqo=l(),m(m6.$$.fragment),cBe=l(),Nd=a("h2"),_1=a("a"),Kie=a("span"),m(f6.$$.fragment),uqo=l(),Zie=a("span"),pqo=o("AutoModelForAudioXVector"),mBe=l(),cr=a("div"),m(g6.$$.fragment),_qo=l(),qd=a("p"),bqo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),ede=a("code"),vqo=o("from_pretrained()"),Tqo=o("class method or the "),ode=a("code"),Fqo=o("from_config()"),Cqo=o(`class
method.`),Mqo=l(),h6=a("p"),Eqo=o("This class cannot be instantiated directly using "),rde=a("code"),yqo=o("__init__()"),wqo=o(" (throws an error)."),Aqo=l(),tt=a("div"),m(u6.$$.fragment),Lqo=l(),tde=a("p"),Bqo=o("Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),xqo=l(),Gd=a("p"),kqo=o(`Note:
Loading a model from its configuration file does `),ade=a("strong"),Rqo=o("not"),Sqo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),sde=a("code"),Pqo=o("from_pretrained()"),$qo=o("to load the model weights."),Iqo=l(),nde=a("p"),Dqo=o("Examples:"),jqo=l(),m(p6.$$.fragment),Nqo=l(),Ue=a("div"),m(_6.$$.fragment),qqo=l(),lde=a("p"),Gqo=o("Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),Oqo=l(),ts=a("p"),Xqo=o("The model class to instantiate is selected based on the "),ide=a("code"),Vqo=o("model_type"),zqo=o(` property of the config object (either
passed as an argument or loaded from `),dde=a("code"),Wqo=o("pretrained_model_name_or_path"),Qqo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cde=a("code"),Hqo=o("pretrained_model_name_or_path"),Uqo=o(":"),Jqo=l(),as=a("ul"),b1=a("li"),mde=a("strong"),Yqo=o("data2vec-audio"),Kqo=o(" \u2014 "),oN=a("a"),Zqo=o("Data2VecAudioForXVector"),eGo=o(" (Data2VecAudio model)"),oGo=l(),v1=a("li"),fde=a("strong"),rGo=o("unispeech-sat"),tGo=o(" \u2014 "),rN=a("a"),aGo=o("UniSpeechSatForXVector"),sGo=o(" (UniSpeechSat model)"),nGo=l(),T1=a("li"),gde=a("strong"),lGo=o("wav2vec2"),iGo=o(" \u2014 "),tN=a("a"),dGo=o("Wav2Vec2ForXVector"),cGo=o(" (Wav2Vec2 model)"),mGo=l(),F1=a("li"),hde=a("strong"),fGo=o("wavlm"),gGo=o(" \u2014 "),aN=a("a"),hGo=o("WavLMForXVector"),uGo=o(" (WavLM model)"),pGo=l(),C1=a("p"),_Go=o("The model is set in evaluation mode by default using "),ude=a("code"),bGo=o("model.eval()"),vGo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pde=a("code"),TGo=o("model.train()"),FGo=l(),_de=a("p"),CGo=o("Examples:"),MGo=l(),m(b6.$$.fragment),fBe=l(),Od=a("h2"),M1=a("a"),bde=a("span"),m(v6.$$.fragment),EGo=l(),vde=a("span"),yGo=o("AutoModelForMaskedImageModeling"),gBe=l(),mr=a("div"),m(T6.$$.fragment),wGo=l(),Xd=a("p"),AGo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Tde=a("code"),LGo=o("from_pretrained()"),BGo=o("class method or the "),Fde=a("code"),xGo=o("from_config()"),kGo=o(`class
method.`),RGo=l(),F6=a("p"),SGo=o("This class cannot be instantiated directly using "),Cde=a("code"),PGo=o("__init__()"),$Go=o(" (throws an error)."),IGo=l(),at=a("div"),m(C6.$$.fragment),DGo=l(),Mde=a("p"),jGo=o("Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),NGo=l(),Vd=a("p"),qGo=o(`Note:
Loading a model from its configuration file does `),Ede=a("strong"),GGo=o("not"),OGo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),yde=a("code"),XGo=o("from_pretrained()"),VGo=o("to load the model weights."),zGo=l(),wde=a("p"),WGo=o("Examples:"),QGo=l(),m(M6.$$.fragment),HGo=l(),Je=a("div"),m(E6.$$.fragment),UGo=l(),Ade=a("p"),JGo=o("Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),YGo=l(),ss=a("p"),KGo=o("The model class to instantiate is selected based on the "),Lde=a("code"),ZGo=o("model_type"),eOo=o(` property of the config object (either
passed as an argument or loaded from `),Bde=a("code"),oOo=o("pretrained_model_name_or_path"),rOo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xde=a("code"),tOo=o("pretrained_model_name_or_path"),aOo=o(":"),sOo=l(),zd=a("ul"),E1=a("li"),kde=a("strong"),nOo=o("deit"),lOo=o(" \u2014 "),sN=a("a"),iOo=o("DeiTForMaskedImageModeling"),dOo=o(" (DeiT model)"),cOo=l(),y1=a("li"),Rde=a("strong"),mOo=o("swin"),fOo=o(" \u2014 "),nN=a("a"),gOo=o("SwinForMaskedImageModeling"),hOo=o(" (Swin model)"),uOo=l(),w1=a("li"),Sde=a("strong"),pOo=o("vit"),_Oo=o(" \u2014 "),lN=a("a"),bOo=o("ViTForMaskedImageModeling"),vOo=o(" (ViT model)"),TOo=l(),A1=a("p"),FOo=o("The model is set in evaluation mode by default using "),Pde=a("code"),COo=o("model.eval()"),MOo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$de=a("code"),EOo=o("model.train()"),yOo=l(),Ide=a("p"),wOo=o("Examples:"),AOo=l(),m(y6.$$.fragment),hBe=l(),Wd=a("h2"),L1=a("a"),Dde=a("span"),m(w6.$$.fragment),LOo=l(),jde=a("span"),BOo=o("AutoModelForObjectDetection"),uBe=l(),fr=a("div"),m(A6.$$.fragment),xOo=l(),Qd=a("p"),kOo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Nde=a("code"),ROo=o("from_pretrained()"),SOo=o("class method or the "),qde=a("code"),POo=o("from_config()"),$Oo=o(`class
method.`),IOo=l(),L6=a("p"),DOo=o("This class cannot be instantiated directly using "),Gde=a("code"),jOo=o("__init__()"),NOo=o(" (throws an error)."),qOo=l(),st=a("div"),m(B6.$$.fragment),GOo=l(),Ode=a("p"),OOo=o("Instantiates one of the model classes of the library (with a object detection head) from a configuration."),XOo=l(),Hd=a("p"),VOo=o(`Note:
Loading a model from its configuration file does `),Xde=a("strong"),zOo=o("not"),WOo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vde=a("code"),QOo=o("from_pretrained()"),HOo=o("to load the model weights."),UOo=l(),zde=a("p"),JOo=o("Examples:"),YOo=l(),m(x6.$$.fragment),KOo=l(),Ye=a("div"),m(k6.$$.fragment),ZOo=l(),Wde=a("p"),eXo=o("Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),oXo=l(),ns=a("p"),rXo=o("The model class to instantiate is selected based on the "),Qde=a("code"),tXo=o("model_type"),aXo=o(` property of the config object (either
passed as an argument or loaded from `),Hde=a("code"),sXo=o("pretrained_model_name_or_path"),nXo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ude=a("code"),lXo=o("pretrained_model_name_or_path"),iXo=o(":"),dXo=l(),Jde=a("ul"),B1=a("li"),Yde=a("strong"),cXo=o("detr"),mXo=o(" \u2014 "),iN=a("a"),fXo=o("DetrForObjectDetection"),gXo=o(" (DETR model)"),hXo=l(),x1=a("p"),uXo=o("The model is set in evaluation mode by default using "),Kde=a("code"),pXo=o("model.eval()"),_Xo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zde=a("code"),bXo=o("model.train()"),vXo=l(),ece=a("p"),TXo=o("Examples:"),FXo=l(),m(R6.$$.fragment),pBe=l(),Ud=a("h2"),k1=a("a"),oce=a("span"),m(S6.$$.fragment),CXo=l(),rce=a("span"),MXo=o("AutoModelForImageSegmentation"),_Be=l(),gr=a("div"),m(P6.$$.fragment),EXo=l(),Jd=a("p"),yXo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),tce=a("code"),wXo=o("from_pretrained()"),AXo=o("class method or the "),ace=a("code"),LXo=o("from_config()"),BXo=o(`class
method.`),xXo=l(),$6=a("p"),kXo=o("This class cannot be instantiated directly using "),sce=a("code"),RXo=o("__init__()"),SXo=o(" (throws an error)."),PXo=l(),nt=a("div"),m(I6.$$.fragment),$Xo=l(),nce=a("p"),IXo=o("Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),DXo=l(),Yd=a("p"),jXo=o(`Note:
Loading a model from its configuration file does `),lce=a("strong"),NXo=o("not"),qXo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ice=a("code"),GXo=o("from_pretrained()"),OXo=o("to load the model weights."),XXo=l(),dce=a("p"),VXo=o("Examples:"),zXo=l(),m(D6.$$.fragment),WXo=l(),Ke=a("div"),m(j6.$$.fragment),QXo=l(),cce=a("p"),HXo=o("Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),UXo=l(),ls=a("p"),JXo=o("The model class to instantiate is selected based on the "),mce=a("code"),YXo=o("model_type"),KXo=o(` property of the config object (either
passed as an argument or loaded from `),fce=a("code"),ZXo=o("pretrained_model_name_or_path"),eVo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gce=a("code"),oVo=o("pretrained_model_name_or_path"),rVo=o(":"),tVo=l(),hce=a("ul"),R1=a("li"),uce=a("strong"),aVo=o("detr"),sVo=o(" \u2014 "),dN=a("a"),nVo=o("DetrForSegmentation"),lVo=o(" (DETR model)"),iVo=l(),S1=a("p"),dVo=o("The model is set in evaluation mode by default using "),pce=a("code"),cVo=o("model.eval()"),mVo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),_ce=a("code"),fVo=o("model.train()"),gVo=l(),bce=a("p"),hVo=o("Examples:"),uVo=l(),m(N6.$$.fragment),bBe=l(),Kd=a("h2"),P1=a("a"),vce=a("span"),m(q6.$$.fragment),pVo=l(),Tce=a("span"),_Vo=o("AutoModelForSemanticSegmentation"),vBe=l(),hr=a("div"),m(G6.$$.fragment),bVo=l(),Zd=a("p"),vVo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Fce=a("code"),TVo=o("from_pretrained()"),FVo=o("class method or the "),Cce=a("code"),CVo=o("from_config()"),MVo=o(`class
method.`),EVo=l(),O6=a("p"),yVo=o("This class cannot be instantiated directly using "),Mce=a("code"),wVo=o("__init__()"),AVo=o(" (throws an error)."),LVo=l(),lt=a("div"),m(X6.$$.fragment),BVo=l(),Ece=a("p"),xVo=o("Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),kVo=l(),ec=a("p"),RVo=o(`Note:
Loading a model from its configuration file does `),yce=a("strong"),SVo=o("not"),PVo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),wce=a("code"),$Vo=o("from_pretrained()"),IVo=o("to load the model weights."),DVo=l(),Ace=a("p"),jVo=o("Examples:"),NVo=l(),m(V6.$$.fragment),qVo=l(),Ze=a("div"),m(z6.$$.fragment),GVo=l(),Lce=a("p"),OVo=o("Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),XVo=l(),is=a("p"),VVo=o("The model class to instantiate is selected based on the "),Bce=a("code"),zVo=o("model_type"),WVo=o(` property of the config object (either
passed as an argument or loaded from `),xce=a("code"),QVo=o("pretrained_model_name_or_path"),HVo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),kce=a("code"),UVo=o("pretrained_model_name_or_path"),JVo=o(":"),YVo=l(),W6=a("ul"),$1=a("li"),Rce=a("strong"),KVo=o("beit"),ZVo=o(" \u2014 "),cN=a("a"),ezo=o("BeitForSemanticSegmentation"),ozo=o(" (BEiT model)"),rzo=l(),I1=a("li"),Sce=a("strong"),tzo=o("segformer"),azo=o(" \u2014 "),mN=a("a"),szo=o("SegformerForSemanticSegmentation"),nzo=o(" (SegFormer model)"),lzo=l(),D1=a("p"),izo=o("The model is set in evaluation mode by default using "),Pce=a("code"),dzo=o("model.eval()"),czo=o(` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$ce=a("code"),mzo=o("model.train()"),fzo=l(),Ice=a("p"),gzo=o("Examples:"),hzo=l(),m(Q6.$$.fragment),TBe=l(),oc=a("h2"),j1=a("a"),Dce=a("span"),m(H6.$$.fragment),uzo=l(),jce=a("span"),pzo=o("TFAutoModel"),FBe=l(),ur=a("div"),m(U6.$$.fragment),_zo=l(),rc=a("p"),bzo=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Nce=a("code"),vzo=o("from_pretrained()"),Tzo=o("class method or the "),qce=a("code"),Fzo=o("from_config()"),Czo=o(`class
method.`),Mzo=l(),J6=a("p"),Ezo=o("This class cannot be instantiated directly using "),Gce=a("code"),yzo=o("__init__()"),wzo=o(" (throws an error)."),Azo=l(),it=a("div"),m(Y6.$$.fragment),Lzo=l(),Oce=a("p"),Bzo=o("Instantiates one of the base model classes of the library from a configuration."),xzo=l(),tc=a("p"),kzo=o(`Note:
Loading a model from its configuration file does `),Xce=a("strong"),Rzo=o("not"),Szo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Vce=a("code"),Pzo=o("from_pretrained()"),$zo=o("to load the model weights."),Izo=l(),zce=a("p"),Dzo=o("Examples:"),jzo=l(),m(K6.$$.fragment),Nzo=l(),go=a("div"),m(Z6.$$.fragment),qzo=l(),Wce=a("p"),Gzo=o("Instantiate one of the base model classes of the library from a pretrained model."),Ozo=l(),ds=a("p"),Xzo=o("The model class to instantiate is selected based on the "),Qce=a("code"),Vzo=o("model_type"),zzo=o(` property of the config object (either
passed as an argument or loaded from `),Hce=a("code"),Wzo=o("pretrained_model_name_or_path"),Qzo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Uce=a("code"),Hzo=o("pretrained_model_name_or_path"),Uzo=o(":"),Jzo=l(),B=a("ul"),N1=a("li"),Jce=a("strong"),Yzo=o("albert"),Kzo=o(" \u2014 "),fN=a("a"),Zzo=o("TFAlbertModel"),eWo=o(" (ALBERT model)"),oWo=l(),q1=a("li"),Yce=a("strong"),rWo=o("bart"),tWo=o(" \u2014 "),gN=a("a"),aWo=o("TFBartModel"),sWo=o(" (BART model)"),nWo=l(),G1=a("li"),Kce=a("strong"),lWo=o("bert"),iWo=o(" \u2014 "),hN=a("a"),dWo=o("TFBertModel"),cWo=o(" (BERT model)"),mWo=l(),O1=a("li"),Zce=a("strong"),fWo=o("blenderbot"),gWo=o(" \u2014 "),uN=a("a"),hWo=o("TFBlenderbotModel"),uWo=o(" (Blenderbot model)"),pWo=l(),X1=a("li"),eme=a("strong"),_Wo=o("blenderbot-small"),bWo=o(" \u2014 "),pN=a("a"),vWo=o("TFBlenderbotSmallModel"),TWo=o(" (BlenderbotSmall model)"),FWo=l(),V1=a("li"),ome=a("strong"),CWo=o("camembert"),MWo=o(" \u2014 "),_N=a("a"),EWo=o("TFCamembertModel"),yWo=o(" (CamemBERT model)"),wWo=l(),z1=a("li"),rme=a("strong"),AWo=o("clip"),LWo=o(" \u2014 "),bN=a("a"),BWo=o("TFCLIPModel"),xWo=o(" (CLIP model)"),kWo=l(),W1=a("li"),tme=a("strong"),RWo=o("convbert"),SWo=o(" \u2014 "),vN=a("a"),PWo=o("TFConvBertModel"),$Wo=o(" (ConvBERT model)"),IWo=l(),Q1=a("li"),ame=a("strong"),DWo=o("convnext"),jWo=o(" \u2014 "),TN=a("a"),NWo=o("TFConvNextModel"),qWo=o(" (ConvNext model)"),GWo=l(),H1=a("li"),sme=a("strong"),OWo=o("ctrl"),XWo=o(" \u2014 "),FN=a("a"),VWo=o("TFCTRLModel"),zWo=o(" (CTRL model)"),WWo=l(),U1=a("li"),nme=a("strong"),QWo=o("deberta"),HWo=o(" \u2014 "),CN=a("a"),UWo=o("TFDebertaModel"),JWo=o(" (DeBERTa model)"),YWo=l(),J1=a("li"),lme=a("strong"),KWo=o("deberta-v2"),ZWo=o(" \u2014 "),MN=a("a"),eQo=o("TFDebertaV2Model"),oQo=o(" (DeBERTa-v2 model)"),rQo=l(),Y1=a("li"),ime=a("strong"),tQo=o("distilbert"),aQo=o(" \u2014 "),EN=a("a"),sQo=o("TFDistilBertModel"),nQo=o(" (DistilBERT model)"),lQo=l(),K1=a("li"),dme=a("strong"),iQo=o("dpr"),dQo=o(" \u2014 "),yN=a("a"),cQo=o("TFDPRQuestionEncoder"),mQo=o(" (DPR model)"),fQo=l(),Z1=a("li"),cme=a("strong"),gQo=o("electra"),hQo=o(" \u2014 "),wN=a("a"),uQo=o("TFElectraModel"),pQo=o(" (ELECTRA model)"),_Qo=l(),eF=a("li"),mme=a("strong"),bQo=o("flaubert"),vQo=o(" \u2014 "),AN=a("a"),TQo=o("TFFlaubertModel"),FQo=o(" (FlauBERT model)"),CQo=l(),$n=a("li"),fme=a("strong"),MQo=o("funnel"),EQo=o(" \u2014 "),LN=a("a"),yQo=o("TFFunnelModel"),wQo=o(" or "),BN=a("a"),AQo=o("TFFunnelBaseModel"),LQo=o(" (Funnel Transformer model)"),BQo=l(),oF=a("li"),gme=a("strong"),xQo=o("gpt2"),kQo=o(" \u2014 "),xN=a("a"),RQo=o("TFGPT2Model"),SQo=o(" (OpenAI GPT-2 model)"),PQo=l(),rF=a("li"),hme=a("strong"),$Qo=o("hubert"),IQo=o(" \u2014 "),kN=a("a"),DQo=o("TFHubertModel"),jQo=o(" (Hubert model)"),NQo=l(),tF=a("li"),ume=a("strong"),qQo=o("layoutlm"),GQo=o(" \u2014 "),RN=a("a"),OQo=o("TFLayoutLMModel"),XQo=o(" (LayoutLM model)"),VQo=l(),aF=a("li"),pme=a("strong"),zQo=o("led"),WQo=o(" \u2014 "),SN=a("a"),QQo=o("TFLEDModel"),HQo=o(" (LED model)"),UQo=l(),sF=a("li"),_me=a("strong"),JQo=o("longformer"),YQo=o(" \u2014 "),PN=a("a"),KQo=o("TFLongformerModel"),ZQo=o(" (Longformer model)"),eHo=l(),nF=a("li"),bme=a("strong"),oHo=o("lxmert"),rHo=o(" \u2014 "),$N=a("a"),tHo=o("TFLxmertModel"),aHo=o(" (LXMERT model)"),sHo=l(),lF=a("li"),vme=a("strong"),nHo=o("marian"),lHo=o(" \u2014 "),IN=a("a"),iHo=o("TFMarianModel"),dHo=o(" (Marian model)"),cHo=l(),iF=a("li"),Tme=a("strong"),mHo=o("mbart"),fHo=o(" \u2014 "),DN=a("a"),gHo=o("TFMBartModel"),hHo=o(" (mBART model)"),uHo=l(),dF=a("li"),Fme=a("strong"),pHo=o("mobilebert"),_Ho=o(" \u2014 "),jN=a("a"),bHo=o("TFMobileBertModel"),vHo=o(" (MobileBERT model)"),THo=l(),cF=a("li"),Cme=a("strong"),FHo=o("mpnet"),CHo=o(" \u2014 "),NN=a("a"),MHo=o("TFMPNetModel"),EHo=o(" (MPNet model)"),yHo=l(),mF=a("li"),Mme=a("strong"),wHo=o("mt5"),AHo=o(" \u2014 "),qN=a("a"),LHo=o("TFMT5Model"),BHo=o(" (mT5 model)"),xHo=l(),fF=a("li"),Eme=a("strong"),kHo=o("openai-gpt"),RHo=o(" \u2014 "),GN=a("a"),SHo=o("TFOpenAIGPTModel"),PHo=o(" (OpenAI GPT model)"),$Ho=l(),gF=a("li"),yme=a("strong"),IHo=o("pegasus"),DHo=o(" \u2014 "),ON=a("a"),jHo=o("TFPegasusModel"),NHo=o(" (Pegasus model)"),qHo=l(),hF=a("li"),wme=a("strong"),GHo=o("rembert"),OHo=o(" \u2014 "),XN=a("a"),XHo=o("TFRemBertModel"),VHo=o(" (RemBERT model)"),zHo=l(),uF=a("li"),Ame=a("strong"),WHo=o("roberta"),QHo=o(" \u2014 "),VN=a("a"),HHo=o("TFRobertaModel"),UHo=o(" (RoBERTa model)"),JHo=l(),pF=a("li"),Lme=a("strong"),YHo=o("roformer"),KHo=o(" \u2014 "),zN=a("a"),ZHo=o("TFRoFormerModel"),eUo=o(" (RoFormer model)"),oUo=l(),_F=a("li"),Bme=a("strong"),rUo=o("speech_to_text"),tUo=o(" \u2014 "),WN=a("a"),aUo=o("TFSpeech2TextModel"),sUo=o(" (Speech2Text model)"),nUo=l(),bF=a("li"),xme=a("strong"),lUo=o("t5"),iUo=o(" \u2014 "),QN=a("a"),dUo=o("TFT5Model"),cUo=o(" (T5 model)"),mUo=l(),vF=a("li"),kme=a("strong"),fUo=o("tapas"),gUo=o(" \u2014 "),HN=a("a"),hUo=o("TFTapasModel"),uUo=o(" (TAPAS model)"),pUo=l(),TF=a("li"),Rme=a("strong"),_Uo=o("transfo-xl"),bUo=o(" \u2014 "),UN=a("a"),vUo=o("TFTransfoXLModel"),TUo=o(" (Transformer-XL model)"),FUo=l(),FF=a("li"),Sme=a("strong"),CUo=o("vit"),MUo=o(" \u2014 "),JN=a("a"),EUo=o("TFViTModel"),yUo=o(" (ViT model)"),wUo=l(),CF=a("li"),Pme=a("strong"),AUo=o("wav2vec2"),LUo=o(" \u2014 "),YN=a("a"),BUo=o("TFWav2Vec2Model"),xUo=o(" (Wav2Vec2 model)"),kUo=l(),MF=a("li"),$me=a("strong"),RUo=o("xlm"),SUo=o(" \u2014 "),KN=a("a"),PUo=o("TFXLMModel"),$Uo=o(" (XLM model)"),IUo=l(),EF=a("li"),Ime=a("strong"),DUo=o("xlm-roberta"),jUo=o(" \u2014 "),ZN=a("a"),NUo=o("TFXLMRobertaModel"),qUo=o(" (XLM-RoBERTa model)"),GUo=l(),yF=a("li"),Dme=a("strong"),OUo=o("xlnet"),XUo=o(" \u2014 "),eq=a("a"),VUo=o("TFXLNetModel"),zUo=o(" (XLNet model)"),WUo=l(),jme=a("p"),QUo=o("Examples:"),HUo=l(),m(eA.$$.fragment),CBe=l(),ac=a("h2"),wF=a("a"),Nme=a("span"),m(oA.$$.fragment),UUo=l(),qme=a("span"),JUo=o("TFAutoModelForPreTraining"),MBe=l(),pr=a("div"),m(rA.$$.fragment),YUo=l(),sc=a("p"),KUo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Gme=a("code"),ZUo=o("from_pretrained()"),eJo=o("class method or the "),Ome=a("code"),oJo=o("from_config()"),rJo=o(`class
method.`),tJo=l(),tA=a("p"),aJo=o("This class cannot be instantiated directly using "),Xme=a("code"),sJo=o("__init__()"),nJo=o(" (throws an error)."),lJo=l(),dt=a("div"),m(aA.$$.fragment),iJo=l(),Vme=a("p"),dJo=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),cJo=l(),nc=a("p"),mJo=o(`Note:
Loading a model from its configuration file does `),zme=a("strong"),fJo=o("not"),gJo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Wme=a("code"),hJo=o("from_pretrained()"),uJo=o("to load the model weights."),pJo=l(),Qme=a("p"),_Jo=o("Examples:"),bJo=l(),m(sA.$$.fragment),vJo=l(),ho=a("div"),m(nA.$$.fragment),TJo=l(),Hme=a("p"),FJo=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),CJo=l(),cs=a("p"),MJo=o("The model class to instantiate is selected based on the "),Ume=a("code"),EJo=o("model_type"),yJo=o(` property of the config object (either
passed as an argument or loaded from `),Jme=a("code"),wJo=o("pretrained_model_name_or_path"),AJo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yme=a("code"),LJo=o("pretrained_model_name_or_path"),BJo=o(":"),xJo=l(),H=a("ul"),AF=a("li"),Kme=a("strong"),kJo=o("albert"),RJo=o(" \u2014 "),oq=a("a"),SJo=o("TFAlbertForPreTraining"),PJo=o(" (ALBERT model)"),$Jo=l(),LF=a("li"),Zme=a("strong"),IJo=o("bart"),DJo=o(" \u2014 "),rq=a("a"),jJo=o("TFBartForConditionalGeneration"),NJo=o(" (BART model)"),qJo=l(),BF=a("li"),efe=a("strong"),GJo=o("bert"),OJo=o(" \u2014 "),tq=a("a"),XJo=o("TFBertForPreTraining"),VJo=o(" (BERT model)"),zJo=l(),xF=a("li"),ofe=a("strong"),WJo=o("camembert"),QJo=o(" \u2014 "),aq=a("a"),HJo=o("TFCamembertForMaskedLM"),UJo=o(" (CamemBERT model)"),JJo=l(),kF=a("li"),rfe=a("strong"),YJo=o("ctrl"),KJo=o(" \u2014 "),sq=a("a"),ZJo=o("TFCTRLLMHeadModel"),eYo=o(" (CTRL model)"),oYo=l(),RF=a("li"),tfe=a("strong"),rYo=o("distilbert"),tYo=o(" \u2014 "),nq=a("a"),aYo=o("TFDistilBertForMaskedLM"),sYo=o(" (DistilBERT model)"),nYo=l(),SF=a("li"),afe=a("strong"),lYo=o("electra"),iYo=o(" \u2014 "),lq=a("a"),dYo=o("TFElectraForPreTraining"),cYo=o(" (ELECTRA model)"),mYo=l(),PF=a("li"),sfe=a("strong"),fYo=o("flaubert"),gYo=o(" \u2014 "),iq=a("a"),hYo=o("TFFlaubertWithLMHeadModel"),uYo=o(" (FlauBERT model)"),pYo=l(),$F=a("li"),nfe=a("strong"),_Yo=o("funnel"),bYo=o(" \u2014 "),dq=a("a"),vYo=o("TFFunnelForPreTraining"),TYo=o(" (Funnel Transformer model)"),FYo=l(),IF=a("li"),lfe=a("strong"),CYo=o("gpt2"),MYo=o(" \u2014 "),cq=a("a"),EYo=o("TFGPT2LMHeadModel"),yYo=o(" (OpenAI GPT-2 model)"),wYo=l(),DF=a("li"),ife=a("strong"),AYo=o("layoutlm"),LYo=o(" \u2014 "),mq=a("a"),BYo=o("TFLayoutLMForMaskedLM"),xYo=o(" (LayoutLM model)"),kYo=l(),jF=a("li"),dfe=a("strong"),RYo=o("lxmert"),SYo=o(" \u2014 "),fq=a("a"),PYo=o("TFLxmertForPreTraining"),$Yo=o(" (LXMERT model)"),IYo=l(),NF=a("li"),cfe=a("strong"),DYo=o("mobilebert"),jYo=o(" \u2014 "),gq=a("a"),NYo=o("TFMobileBertForPreTraining"),qYo=o(" (MobileBERT model)"),GYo=l(),qF=a("li"),mfe=a("strong"),OYo=o("mpnet"),XYo=o(" \u2014 "),hq=a("a"),VYo=o("TFMPNetForMaskedLM"),zYo=o(" (MPNet model)"),WYo=l(),GF=a("li"),ffe=a("strong"),QYo=o("openai-gpt"),HYo=o(" \u2014 "),uq=a("a"),UYo=o("TFOpenAIGPTLMHeadModel"),JYo=o(" (OpenAI GPT model)"),YYo=l(),OF=a("li"),gfe=a("strong"),KYo=o("roberta"),ZYo=o(" \u2014 "),pq=a("a"),eKo=o("TFRobertaForMaskedLM"),oKo=o(" (RoBERTa model)"),rKo=l(),XF=a("li"),hfe=a("strong"),tKo=o("t5"),aKo=o(" \u2014 "),_q=a("a"),sKo=o("TFT5ForConditionalGeneration"),nKo=o(" (T5 model)"),lKo=l(),VF=a("li"),ufe=a("strong"),iKo=o("tapas"),dKo=o(" \u2014 "),bq=a("a"),cKo=o("TFTapasForMaskedLM"),mKo=o(" (TAPAS model)"),fKo=l(),zF=a("li"),pfe=a("strong"),gKo=o("transfo-xl"),hKo=o(" \u2014 "),vq=a("a"),uKo=o("TFTransfoXLLMHeadModel"),pKo=o(" (Transformer-XL model)"),_Ko=l(),WF=a("li"),_fe=a("strong"),bKo=o("xlm"),vKo=o(" \u2014 "),Tq=a("a"),TKo=o("TFXLMWithLMHeadModel"),FKo=o(" (XLM model)"),CKo=l(),QF=a("li"),bfe=a("strong"),MKo=o("xlm-roberta"),EKo=o(" \u2014 "),Fq=a("a"),yKo=o("TFXLMRobertaForMaskedLM"),wKo=o(" (XLM-RoBERTa model)"),AKo=l(),HF=a("li"),vfe=a("strong"),LKo=o("xlnet"),BKo=o(" \u2014 "),Cq=a("a"),xKo=o("TFXLNetLMHeadModel"),kKo=o(" (XLNet model)"),RKo=l(),Tfe=a("p"),SKo=o("Examples:"),PKo=l(),m(lA.$$.fragment),EBe=l(),lc=a("h2"),UF=a("a"),Ffe=a("span"),m(iA.$$.fragment),$Ko=l(),Cfe=a("span"),IKo=o("TFAutoModelForCausalLM"),yBe=l(),_r=a("div"),m(dA.$$.fragment),DKo=l(),ic=a("p"),jKo=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Mfe=a("code"),NKo=o("from_pretrained()"),qKo=o("class method or the "),Efe=a("code"),GKo=o("from_config()"),OKo=o(`class
method.`),XKo=l(),cA=a("p"),VKo=o("This class cannot be instantiated directly using "),yfe=a("code"),zKo=o("__init__()"),WKo=o(" (throws an error)."),QKo=l(),ct=a("div"),m(mA.$$.fragment),HKo=l(),wfe=a("p"),UKo=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),JKo=l(),dc=a("p"),YKo=o(`Note:
Loading a model from its configuration file does `),Afe=a("strong"),KKo=o("not"),ZKo=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Lfe=a("code"),eZo=o("from_pretrained()"),oZo=o("to load the model weights."),rZo=l(),Bfe=a("p"),tZo=o("Examples:"),aZo=l(),m(fA.$$.fragment),sZo=l(),uo=a("div"),m(gA.$$.fragment),nZo=l(),xfe=a("p"),lZo=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),iZo=l(),ms=a("p"),dZo=o("The model class to instantiate is selected based on the "),kfe=a("code"),cZo=o("model_type"),mZo=o(` property of the config object (either
passed as an argument or loaded from `),Rfe=a("code"),fZo=o("pretrained_model_name_or_path"),gZo=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sfe=a("code"),hZo=o("pretrained_model_name_or_path"),uZo=o(":"),pZo=l(),he=a("ul"),JF=a("li"),Pfe=a("strong"),_Zo=o("bert"),bZo=o(" \u2014 "),Mq=a("a"),vZo=o("TFBertLMHeadModel"),TZo=o(" (BERT model)"),FZo=l(),YF=a("li"),$fe=a("strong"),CZo=o("ctrl"),MZo=o(" \u2014 "),Eq=a("a"),EZo=o("TFCTRLLMHeadModel"),yZo=o(" (CTRL model)"),wZo=l(),KF=a("li"),Ife=a("strong"),AZo=o("gpt2"),LZo=o(" \u2014 "),yq=a("a"),BZo=o("TFGPT2LMHeadModel"),xZo=o(" (OpenAI GPT-2 model)"),kZo=l(),ZF=a("li"),Dfe=a("strong"),RZo=o("openai-gpt"),SZo=o(" \u2014 "),wq=a("a"),PZo=o("TFOpenAIGPTLMHeadModel"),$Zo=o(" (OpenAI GPT model)"),IZo=l(),eC=a("li"),jfe=a("strong"),DZo=o("rembert"),jZo=o(" \u2014 "),Aq=a("a"),NZo=o("TFRemBertForCausalLM"),qZo=o(" (RemBERT model)"),GZo=l(),oC=a("li"),Nfe=a("strong"),OZo=o("roberta"),XZo=o(" \u2014 "),Lq=a("a"),VZo=o("TFRobertaForCausalLM"),zZo=o(" (RoBERTa model)"),WZo=l(),rC=a("li"),qfe=a("strong"),QZo=o("roformer"),HZo=o(" \u2014 "),Bq=a("a"),UZo=o("TFRoFormerForCausalLM"),JZo=o(" (RoFormer model)"),YZo=l(),tC=a("li"),Gfe=a("strong"),KZo=o("transfo-xl"),ZZo=o(" \u2014 "),xq=a("a"),eer=o("TFTransfoXLLMHeadModel"),oer=o(" (Transformer-XL model)"),rer=l(),aC=a("li"),Ofe=a("strong"),ter=o("xlm"),aer=o(" \u2014 "),kq=a("a"),ser=o("TFXLMWithLMHeadModel"),ner=o(" (XLM model)"),ler=l(),sC=a("li"),Xfe=a("strong"),ier=o("xlnet"),der=o(" \u2014 "),Rq=a("a"),cer=o("TFXLNetLMHeadModel"),mer=o(" (XLNet model)"),fer=l(),Vfe=a("p"),ger=o("Examples:"),her=l(),m(hA.$$.fragment),wBe=l(),cc=a("h2"),nC=a("a"),zfe=a("span"),m(uA.$$.fragment),uer=l(),Wfe=a("span"),per=o("TFAutoModelForImageClassification"),ABe=l(),br=a("div"),m(pA.$$.fragment),_er=l(),mc=a("p"),ber=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Qfe=a("code"),ver=o("from_pretrained()"),Ter=o("class method or the "),Hfe=a("code"),Fer=o("from_config()"),Cer=o(`class
method.`),Mer=l(),_A=a("p"),Eer=o("This class cannot be instantiated directly using "),Ufe=a("code"),yer=o("__init__()"),wer=o(" (throws an error)."),Aer=l(),mt=a("div"),m(bA.$$.fragment),Ler=l(),Jfe=a("p"),Ber=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),xer=l(),fc=a("p"),ker=o(`Note:
Loading a model from its configuration file does `),Yfe=a("strong"),Rer=o("not"),Ser=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kfe=a("code"),Per=o("from_pretrained()"),$er=o("to load the model weights."),Ier=l(),Zfe=a("p"),Der=o("Examples:"),jer=l(),m(vA.$$.fragment),Ner=l(),po=a("div"),m(TA.$$.fragment),qer=l(),ege=a("p"),Ger=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),Oer=l(),fs=a("p"),Xer=o("The model class to instantiate is selected based on the "),oge=a("code"),Ver=o("model_type"),zer=o(` property of the config object (either
passed as an argument or loaded from `),rge=a("code"),Wer=o("pretrained_model_name_or_path"),Qer=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tge=a("code"),Her=o("pretrained_model_name_or_path"),Uer=o(":"),Jer=l(),FA=a("ul"),lC=a("li"),age=a("strong"),Yer=o("convnext"),Ker=o(" \u2014 "),Sq=a("a"),Zer=o("TFConvNextForImageClassification"),eor=o(" (ConvNext model)"),oor=l(),iC=a("li"),sge=a("strong"),ror=o("vit"),tor=o(" \u2014 "),Pq=a("a"),aor=o("TFViTForImageClassification"),sor=o(" (ViT model)"),nor=l(),nge=a("p"),lor=o("Examples:"),ior=l(),m(CA.$$.fragment),LBe=l(),gc=a("h2"),dC=a("a"),lge=a("span"),m(MA.$$.fragment),dor=l(),ige=a("span"),cor=o("TFAutoModelForMaskedLM"),BBe=l(),vr=a("div"),m(EA.$$.fragment),mor=l(),hc=a("p"),gor=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),dge=a("code"),hor=o("from_pretrained()"),uor=o("class method or the "),cge=a("code"),por=o("from_config()"),_or=o(`class
method.`),bor=l(),yA=a("p"),vor=o("This class cannot be instantiated directly using "),mge=a("code"),Tor=o("__init__()"),For=o(" (throws an error)."),Cor=l(),ft=a("div"),m(wA.$$.fragment),Mor=l(),fge=a("p"),Eor=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),yor=l(),uc=a("p"),wor=o(`Note:
Loading a model from its configuration file does `),gge=a("strong"),Aor=o("not"),Lor=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),hge=a("code"),Bor=o("from_pretrained()"),xor=o("to load the model weights."),kor=l(),uge=a("p"),Ror=o("Examples:"),Sor=l(),m(AA.$$.fragment),Por=l(),_o=a("div"),m(LA.$$.fragment),$or=l(),pge=a("p"),Ior=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Dor=l(),gs=a("p"),jor=o("The model class to instantiate is selected based on the "),_ge=a("code"),Nor=o("model_type"),qor=o(` property of the config object (either
passed as an argument or loaded from `),bge=a("code"),Gor=o("pretrained_model_name_or_path"),Oor=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),vge=a("code"),Xor=o("pretrained_model_name_or_path"),Vor=o(":"),zor=l(),Y=a("ul"),cC=a("li"),Tge=a("strong"),Wor=o("albert"),Qor=o(" \u2014 "),$q=a("a"),Hor=o("TFAlbertForMaskedLM"),Uor=o(" (ALBERT model)"),Jor=l(),mC=a("li"),Fge=a("strong"),Yor=o("bert"),Kor=o(" \u2014 "),Iq=a("a"),Zor=o("TFBertForMaskedLM"),err=o(" (BERT model)"),orr=l(),fC=a("li"),Cge=a("strong"),rrr=o("camembert"),trr=o(" \u2014 "),Dq=a("a"),arr=o("TFCamembertForMaskedLM"),srr=o(" (CamemBERT model)"),nrr=l(),gC=a("li"),Mge=a("strong"),lrr=o("convbert"),irr=o(" \u2014 "),jq=a("a"),drr=o("TFConvBertForMaskedLM"),crr=o(" (ConvBERT model)"),mrr=l(),hC=a("li"),Ege=a("strong"),frr=o("deberta"),grr=o(" \u2014 "),Nq=a("a"),hrr=o("TFDebertaForMaskedLM"),urr=o(" (DeBERTa model)"),prr=l(),uC=a("li"),yge=a("strong"),_rr=o("deberta-v2"),brr=o(" \u2014 "),qq=a("a"),vrr=o("TFDebertaV2ForMaskedLM"),Trr=o(" (DeBERTa-v2 model)"),Frr=l(),pC=a("li"),wge=a("strong"),Crr=o("distilbert"),Mrr=o(" \u2014 "),Gq=a("a"),Err=o("TFDistilBertForMaskedLM"),yrr=o(" (DistilBERT model)"),wrr=l(),_C=a("li"),Age=a("strong"),Arr=o("electra"),Lrr=o(" \u2014 "),Oq=a("a"),Brr=o("TFElectraForMaskedLM"),xrr=o(" (ELECTRA model)"),krr=l(),bC=a("li"),Lge=a("strong"),Rrr=o("flaubert"),Srr=o(" \u2014 "),Xq=a("a"),Prr=o("TFFlaubertWithLMHeadModel"),$rr=o(" (FlauBERT model)"),Irr=l(),vC=a("li"),Bge=a("strong"),Drr=o("funnel"),jrr=o(" \u2014 "),Vq=a("a"),Nrr=o("TFFunnelForMaskedLM"),qrr=o(" (Funnel Transformer model)"),Grr=l(),TC=a("li"),xge=a("strong"),Orr=o("layoutlm"),Xrr=o(" \u2014 "),zq=a("a"),Vrr=o("TFLayoutLMForMaskedLM"),zrr=o(" (LayoutLM model)"),Wrr=l(),FC=a("li"),kge=a("strong"),Qrr=o("longformer"),Hrr=o(" \u2014 "),Wq=a("a"),Urr=o("TFLongformerForMaskedLM"),Jrr=o(" (Longformer model)"),Yrr=l(),CC=a("li"),Rge=a("strong"),Krr=o("mobilebert"),Zrr=o(" \u2014 "),Qq=a("a"),etr=o("TFMobileBertForMaskedLM"),otr=o(" (MobileBERT model)"),rtr=l(),MC=a("li"),Sge=a("strong"),ttr=o("mpnet"),atr=o(" \u2014 "),Hq=a("a"),str=o("TFMPNetForMaskedLM"),ntr=o(" (MPNet model)"),ltr=l(),EC=a("li"),Pge=a("strong"),itr=o("rembert"),dtr=o(" \u2014 "),Uq=a("a"),ctr=o("TFRemBertForMaskedLM"),mtr=o(" (RemBERT model)"),ftr=l(),yC=a("li"),$ge=a("strong"),gtr=o("roberta"),htr=o(" \u2014 "),Jq=a("a"),utr=o("TFRobertaForMaskedLM"),ptr=o(" (RoBERTa model)"),_tr=l(),wC=a("li"),Ige=a("strong"),btr=o("roformer"),vtr=o(" \u2014 "),Yq=a("a"),Ttr=o("TFRoFormerForMaskedLM"),Ftr=o(" (RoFormer model)"),Ctr=l(),AC=a("li"),Dge=a("strong"),Mtr=o("tapas"),Etr=o(" \u2014 "),Kq=a("a"),ytr=o("TFTapasForMaskedLM"),wtr=o(" (TAPAS model)"),Atr=l(),LC=a("li"),jge=a("strong"),Ltr=o("xlm"),Btr=o(" \u2014 "),Zq=a("a"),xtr=o("TFXLMWithLMHeadModel"),ktr=o(" (XLM model)"),Rtr=l(),BC=a("li"),Nge=a("strong"),Str=o("xlm-roberta"),Ptr=o(" \u2014 "),eG=a("a"),$tr=o("TFXLMRobertaForMaskedLM"),Itr=o(" (XLM-RoBERTa model)"),Dtr=l(),qge=a("p"),jtr=o("Examples:"),Ntr=l(),m(BA.$$.fragment),xBe=l(),pc=a("h2"),xC=a("a"),Gge=a("span"),m(xA.$$.fragment),qtr=l(),Oge=a("span"),Gtr=o("TFAutoModelForSeq2SeqLM"),kBe=l(),Tr=a("div"),m(kA.$$.fragment),Otr=l(),_c=a("p"),Xtr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Xge=a("code"),Vtr=o("from_pretrained()"),ztr=o("class method or the "),Vge=a("code"),Wtr=o("from_config()"),Qtr=o(`class
method.`),Htr=l(),RA=a("p"),Utr=o("This class cannot be instantiated directly using "),zge=a("code"),Jtr=o("__init__()"),Ytr=o(" (throws an error)."),Ktr=l(),gt=a("div"),m(SA.$$.fragment),Ztr=l(),Wge=a("p"),ear=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),oar=l(),bc=a("p"),rar=o(`Note:
Loading a model from its configuration file does `),Qge=a("strong"),tar=o("not"),aar=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Hge=a("code"),sar=o("from_pretrained()"),nar=o("to load the model weights."),lar=l(),Uge=a("p"),iar=o("Examples:"),dar=l(),m(PA.$$.fragment),car=l(),bo=a("div"),m($A.$$.fragment),mar=l(),Jge=a("p"),far=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),gar=l(),hs=a("p"),har=o("The model class to instantiate is selected based on the "),Yge=a("code"),uar=o("model_type"),par=o(` property of the config object (either
passed as an argument or loaded from `),Kge=a("code"),_ar=o("pretrained_model_name_or_path"),bar=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zge=a("code"),Tar=o("pretrained_model_name_or_path"),Far=o(":"),Car=l(),ue=a("ul"),kC=a("li"),ehe=a("strong"),Mar=o("bart"),Ear=o(" \u2014 "),oG=a("a"),yar=o("TFBartForConditionalGeneration"),war=o(" (BART model)"),Aar=l(),RC=a("li"),ohe=a("strong"),Lar=o("blenderbot"),Bar=o(" \u2014 "),rG=a("a"),xar=o("TFBlenderbotForConditionalGeneration"),kar=o(" (Blenderbot model)"),Rar=l(),SC=a("li"),rhe=a("strong"),Sar=o("blenderbot-small"),Par=o(" \u2014 "),tG=a("a"),$ar=o("TFBlenderbotSmallForConditionalGeneration"),Iar=o(" (BlenderbotSmall model)"),Dar=l(),PC=a("li"),the=a("strong"),jar=o("encoder-decoder"),Nar=o(" \u2014 "),aG=a("a"),qar=o("TFEncoderDecoderModel"),Gar=o(" (Encoder decoder model)"),Oar=l(),$C=a("li"),ahe=a("strong"),Xar=o("led"),Var=o(" \u2014 "),sG=a("a"),zar=o("TFLEDForConditionalGeneration"),War=o(" (LED model)"),Qar=l(),IC=a("li"),she=a("strong"),Har=o("marian"),Uar=o(" \u2014 "),nG=a("a"),Jar=o("TFMarianMTModel"),Yar=o(" (Marian model)"),Kar=l(),DC=a("li"),nhe=a("strong"),Zar=o("mbart"),esr=o(" \u2014 "),lG=a("a"),osr=o("TFMBartForConditionalGeneration"),rsr=o(" (mBART model)"),tsr=l(),jC=a("li"),lhe=a("strong"),asr=o("mt5"),ssr=o(" \u2014 "),iG=a("a"),nsr=o("TFMT5ForConditionalGeneration"),lsr=o(" (mT5 model)"),isr=l(),NC=a("li"),ihe=a("strong"),dsr=o("pegasus"),csr=o(" \u2014 "),dG=a("a"),msr=o("TFPegasusForConditionalGeneration"),fsr=o(" (Pegasus model)"),gsr=l(),qC=a("li"),dhe=a("strong"),hsr=o("t5"),usr=o(" \u2014 "),cG=a("a"),psr=o("TFT5ForConditionalGeneration"),_sr=o(" (T5 model)"),bsr=l(),che=a("p"),vsr=o("Examples:"),Tsr=l(),m(IA.$$.fragment),RBe=l(),vc=a("h2"),GC=a("a"),mhe=a("span"),m(DA.$$.fragment),Fsr=l(),fhe=a("span"),Csr=o("TFAutoModelForSequenceClassification"),SBe=l(),Fr=a("div"),m(jA.$$.fragment),Msr=l(),Tc=a("p"),Esr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),ghe=a("code"),ysr=o("from_pretrained()"),wsr=o("class method or the "),hhe=a("code"),Asr=o("from_config()"),Lsr=o(`class
method.`),Bsr=l(),NA=a("p"),xsr=o("This class cannot be instantiated directly using "),uhe=a("code"),ksr=o("__init__()"),Rsr=o(" (throws an error)."),Ssr=l(),ht=a("div"),m(qA.$$.fragment),Psr=l(),phe=a("p"),$sr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Isr=l(),Fc=a("p"),Dsr=o(`Note:
Loading a model from its configuration file does `),_he=a("strong"),jsr=o("not"),Nsr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),bhe=a("code"),qsr=o("from_pretrained()"),Gsr=o("to load the model weights."),Osr=l(),vhe=a("p"),Xsr=o("Examples:"),Vsr=l(),m(GA.$$.fragment),zsr=l(),vo=a("div"),m(OA.$$.fragment),Wsr=l(),The=a("p"),Qsr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),Hsr=l(),us=a("p"),Usr=o("The model class to instantiate is selected based on the "),Fhe=a("code"),Jsr=o("model_type"),Ysr=o(` property of the config object (either
passed as an argument or loaded from `),Che=a("code"),Ksr=o("pretrained_model_name_or_path"),Zsr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mhe=a("code"),enr=o("pretrained_model_name_or_path"),onr=o(":"),rnr=l(),X=a("ul"),OC=a("li"),Ehe=a("strong"),tnr=o("albert"),anr=o(" \u2014 "),mG=a("a"),snr=o("TFAlbertForSequenceClassification"),nnr=o(" (ALBERT model)"),lnr=l(),XC=a("li"),yhe=a("strong"),inr=o("bert"),dnr=o(" \u2014 "),fG=a("a"),cnr=o("TFBertForSequenceClassification"),mnr=o(" (BERT model)"),fnr=l(),VC=a("li"),whe=a("strong"),gnr=o("camembert"),hnr=o(" \u2014 "),gG=a("a"),unr=o("TFCamembertForSequenceClassification"),pnr=o(" (CamemBERT model)"),_nr=l(),zC=a("li"),Ahe=a("strong"),bnr=o("convbert"),vnr=o(" \u2014 "),hG=a("a"),Tnr=o("TFConvBertForSequenceClassification"),Fnr=o(" (ConvBERT model)"),Cnr=l(),WC=a("li"),Lhe=a("strong"),Mnr=o("ctrl"),Enr=o(" \u2014 "),uG=a("a"),ynr=o("TFCTRLForSequenceClassification"),wnr=o(" (CTRL model)"),Anr=l(),QC=a("li"),Bhe=a("strong"),Lnr=o("deberta"),Bnr=o(" \u2014 "),pG=a("a"),xnr=o("TFDebertaForSequenceClassification"),knr=o(" (DeBERTa model)"),Rnr=l(),HC=a("li"),xhe=a("strong"),Snr=o("deberta-v2"),Pnr=o(" \u2014 "),_G=a("a"),$nr=o("TFDebertaV2ForSequenceClassification"),Inr=o(" (DeBERTa-v2 model)"),Dnr=l(),UC=a("li"),khe=a("strong"),jnr=o("distilbert"),Nnr=o(" \u2014 "),bG=a("a"),qnr=o("TFDistilBertForSequenceClassification"),Gnr=o(" (DistilBERT model)"),Onr=l(),JC=a("li"),Rhe=a("strong"),Xnr=o("electra"),Vnr=o(" \u2014 "),vG=a("a"),znr=o("TFElectraForSequenceClassification"),Wnr=o(" (ELECTRA model)"),Qnr=l(),YC=a("li"),She=a("strong"),Hnr=o("flaubert"),Unr=o(" \u2014 "),TG=a("a"),Jnr=o("TFFlaubertForSequenceClassification"),Ynr=o(" (FlauBERT model)"),Knr=l(),KC=a("li"),Phe=a("strong"),Znr=o("funnel"),elr=o(" \u2014 "),FG=a("a"),olr=o("TFFunnelForSequenceClassification"),rlr=o(" (Funnel Transformer model)"),tlr=l(),ZC=a("li"),$he=a("strong"),alr=o("gpt2"),slr=o(" \u2014 "),CG=a("a"),nlr=o("TFGPT2ForSequenceClassification"),llr=o(" (OpenAI GPT-2 model)"),ilr=l(),e4=a("li"),Ihe=a("strong"),dlr=o("layoutlm"),clr=o(" \u2014 "),MG=a("a"),mlr=o("TFLayoutLMForSequenceClassification"),flr=o(" (LayoutLM model)"),glr=l(),o4=a("li"),Dhe=a("strong"),hlr=o("longformer"),ulr=o(" \u2014 "),EG=a("a"),plr=o("TFLongformerForSequenceClassification"),_lr=o(" (Longformer model)"),blr=l(),r4=a("li"),jhe=a("strong"),vlr=o("mobilebert"),Tlr=o(" \u2014 "),yG=a("a"),Flr=o("TFMobileBertForSequenceClassification"),Clr=o(" (MobileBERT model)"),Mlr=l(),t4=a("li"),Nhe=a("strong"),Elr=o("mpnet"),ylr=o(" \u2014 "),wG=a("a"),wlr=o("TFMPNetForSequenceClassification"),Alr=o(" (MPNet model)"),Llr=l(),a4=a("li"),qhe=a("strong"),Blr=o("openai-gpt"),xlr=o(" \u2014 "),AG=a("a"),klr=o("TFOpenAIGPTForSequenceClassification"),Rlr=o(" (OpenAI GPT model)"),Slr=l(),s4=a("li"),Ghe=a("strong"),Plr=o("rembert"),$lr=o(" \u2014 "),LG=a("a"),Ilr=o("TFRemBertForSequenceClassification"),Dlr=o(" (RemBERT model)"),jlr=l(),n4=a("li"),Ohe=a("strong"),Nlr=o("roberta"),qlr=o(" \u2014 "),BG=a("a"),Glr=o("TFRobertaForSequenceClassification"),Olr=o(" (RoBERTa model)"),Xlr=l(),l4=a("li"),Xhe=a("strong"),Vlr=o("roformer"),zlr=o(" \u2014 "),xG=a("a"),Wlr=o("TFRoFormerForSequenceClassification"),Qlr=o(" (RoFormer model)"),Hlr=l(),i4=a("li"),Vhe=a("strong"),Ulr=o("tapas"),Jlr=o(" \u2014 "),kG=a("a"),Ylr=o("TFTapasForSequenceClassification"),Klr=o(" (TAPAS model)"),Zlr=l(),d4=a("li"),zhe=a("strong"),eir=o("transfo-xl"),oir=o(" \u2014 "),RG=a("a"),rir=o("TFTransfoXLForSequenceClassification"),tir=o(" (Transformer-XL model)"),air=l(),c4=a("li"),Whe=a("strong"),sir=o("xlm"),nir=o(" \u2014 "),SG=a("a"),lir=o("TFXLMForSequenceClassification"),iir=o(" (XLM model)"),dir=l(),m4=a("li"),Qhe=a("strong"),cir=o("xlm-roberta"),mir=o(" \u2014 "),PG=a("a"),fir=o("TFXLMRobertaForSequenceClassification"),gir=o(" (XLM-RoBERTa model)"),hir=l(),f4=a("li"),Hhe=a("strong"),uir=o("xlnet"),pir=o(" \u2014 "),$G=a("a"),_ir=o("TFXLNetForSequenceClassification"),bir=o(" (XLNet model)"),vir=l(),Uhe=a("p"),Tir=o("Examples:"),Fir=l(),m(XA.$$.fragment),PBe=l(),Cc=a("h2"),g4=a("a"),Jhe=a("span"),m(VA.$$.fragment),Cir=l(),Yhe=a("span"),Mir=o("TFAutoModelForMultipleChoice"),$Be=l(),Cr=a("div"),m(zA.$$.fragment),Eir=l(),Mc=a("p"),yir=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Khe=a("code"),wir=o("from_pretrained()"),Air=o("class method or the "),Zhe=a("code"),Lir=o("from_config()"),Bir=o(`class
method.`),xir=l(),WA=a("p"),kir=o("This class cannot be instantiated directly using "),eue=a("code"),Rir=o("__init__()"),Sir=o(" (throws an error)."),Pir=l(),ut=a("div"),m(QA.$$.fragment),$ir=l(),oue=a("p"),Iir=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Dir=l(),Ec=a("p"),jir=o(`Note:
Loading a model from its configuration file does `),rue=a("strong"),Nir=o("not"),qir=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),tue=a("code"),Gir=o("from_pretrained()"),Oir=o("to load the model weights."),Xir=l(),aue=a("p"),Vir=o("Examples:"),zir=l(),m(HA.$$.fragment),Wir=l(),To=a("div"),m(UA.$$.fragment),Qir=l(),sue=a("p"),Hir=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Uir=l(),ps=a("p"),Jir=o("The model class to instantiate is selected based on the "),nue=a("code"),Yir=o("model_type"),Kir=o(` property of the config object (either
passed as an argument or loaded from `),lue=a("code"),Zir=o("pretrained_model_name_or_path"),edr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iue=a("code"),odr=o("pretrained_model_name_or_path"),rdr=o(":"),tdr=l(),te=a("ul"),h4=a("li"),due=a("strong"),adr=o("albert"),sdr=o(" \u2014 "),IG=a("a"),ndr=o("TFAlbertForMultipleChoice"),ldr=o(" (ALBERT model)"),idr=l(),u4=a("li"),cue=a("strong"),ddr=o("bert"),cdr=o(" \u2014 "),DG=a("a"),mdr=o("TFBertForMultipleChoice"),fdr=o(" (BERT model)"),gdr=l(),p4=a("li"),mue=a("strong"),hdr=o("camembert"),udr=o(" \u2014 "),jG=a("a"),pdr=o("TFCamembertForMultipleChoice"),_dr=o(" (CamemBERT model)"),bdr=l(),_4=a("li"),fue=a("strong"),vdr=o("convbert"),Tdr=o(" \u2014 "),NG=a("a"),Fdr=o("TFConvBertForMultipleChoice"),Cdr=o(" (ConvBERT model)"),Mdr=l(),b4=a("li"),gue=a("strong"),Edr=o("distilbert"),ydr=o(" \u2014 "),qG=a("a"),wdr=o("TFDistilBertForMultipleChoice"),Adr=o(" (DistilBERT model)"),Ldr=l(),v4=a("li"),hue=a("strong"),Bdr=o("electra"),xdr=o(" \u2014 "),GG=a("a"),kdr=o("TFElectraForMultipleChoice"),Rdr=o(" (ELECTRA model)"),Sdr=l(),T4=a("li"),uue=a("strong"),Pdr=o("flaubert"),$dr=o(" \u2014 "),OG=a("a"),Idr=o("TFFlaubertForMultipleChoice"),Ddr=o(" (FlauBERT model)"),jdr=l(),F4=a("li"),pue=a("strong"),Ndr=o("funnel"),qdr=o(" \u2014 "),XG=a("a"),Gdr=o("TFFunnelForMultipleChoice"),Odr=o(" (Funnel Transformer model)"),Xdr=l(),C4=a("li"),_ue=a("strong"),Vdr=o("longformer"),zdr=o(" \u2014 "),VG=a("a"),Wdr=o("TFLongformerForMultipleChoice"),Qdr=o(" (Longformer model)"),Hdr=l(),M4=a("li"),bue=a("strong"),Udr=o("mobilebert"),Jdr=o(" \u2014 "),zG=a("a"),Ydr=o("TFMobileBertForMultipleChoice"),Kdr=o(" (MobileBERT model)"),Zdr=l(),E4=a("li"),vue=a("strong"),ecr=o("mpnet"),ocr=o(" \u2014 "),WG=a("a"),rcr=o("TFMPNetForMultipleChoice"),tcr=o(" (MPNet model)"),acr=l(),y4=a("li"),Tue=a("strong"),scr=o("rembert"),ncr=o(" \u2014 "),QG=a("a"),lcr=o("TFRemBertForMultipleChoice"),icr=o(" (RemBERT model)"),dcr=l(),w4=a("li"),Fue=a("strong"),ccr=o("roberta"),mcr=o(" \u2014 "),HG=a("a"),fcr=o("TFRobertaForMultipleChoice"),gcr=o(" (RoBERTa model)"),hcr=l(),A4=a("li"),Cue=a("strong"),ucr=o("roformer"),pcr=o(" \u2014 "),UG=a("a"),_cr=o("TFRoFormerForMultipleChoice"),bcr=o(" (RoFormer model)"),vcr=l(),L4=a("li"),Mue=a("strong"),Tcr=o("xlm"),Fcr=o(" \u2014 "),JG=a("a"),Ccr=o("TFXLMForMultipleChoice"),Mcr=o(" (XLM model)"),Ecr=l(),B4=a("li"),Eue=a("strong"),ycr=o("xlm-roberta"),wcr=o(" \u2014 "),YG=a("a"),Acr=o("TFXLMRobertaForMultipleChoice"),Lcr=o(" (XLM-RoBERTa model)"),Bcr=l(),x4=a("li"),yue=a("strong"),xcr=o("xlnet"),kcr=o(" \u2014 "),KG=a("a"),Rcr=o("TFXLNetForMultipleChoice"),Scr=o(" (XLNet model)"),Pcr=l(),wue=a("p"),$cr=o("Examples:"),Icr=l(),m(JA.$$.fragment),IBe=l(),yc=a("h2"),k4=a("a"),Aue=a("span"),m(YA.$$.fragment),Dcr=l(),Lue=a("span"),jcr=o("TFAutoModelForTableQuestionAnswering"),DBe=l(),Mr=a("div"),m(KA.$$.fragment),Ncr=l(),wc=a("p"),qcr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Bue=a("code"),Gcr=o("from_pretrained()"),Ocr=o("class method or the "),xue=a("code"),Xcr=o("from_config()"),Vcr=o(`class
method.`),zcr=l(),ZA=a("p"),Wcr=o("This class cannot be instantiated directly using "),kue=a("code"),Qcr=o("__init__()"),Hcr=o(" (throws an error)."),Ucr=l(),pt=a("div"),m(e0.$$.fragment),Jcr=l(),Rue=a("p"),Ycr=o("Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),Kcr=l(),Ac=a("p"),Zcr=o(`Note:
Loading a model from its configuration file does `),Sue=a("strong"),emr=o("not"),omr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Pue=a("code"),rmr=o("from_pretrained()"),tmr=o("to load the model weights."),amr=l(),$ue=a("p"),smr=o("Examples:"),nmr=l(),m(o0.$$.fragment),lmr=l(),Fo=a("div"),m(r0.$$.fragment),imr=l(),Iue=a("p"),dmr=o("Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),cmr=l(),_s=a("p"),mmr=o("The model class to instantiate is selected based on the "),Due=a("code"),fmr=o("model_type"),gmr=o(` property of the config object (either
passed as an argument or loaded from `),jue=a("code"),hmr=o("pretrained_model_name_or_path"),umr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nue=a("code"),pmr=o("pretrained_model_name_or_path"),_mr=o(":"),bmr=l(),que=a("ul"),R4=a("li"),Gue=a("strong"),vmr=o("tapas"),Tmr=o(" \u2014 "),ZG=a("a"),Fmr=o("TFTapasForQuestionAnswering"),Cmr=o(" (TAPAS model)"),Mmr=l(),Oue=a("p"),Emr=o("Examples:"),ymr=l(),m(t0.$$.fragment),jBe=l(),Lc=a("h2"),S4=a("a"),Xue=a("span"),m(a0.$$.fragment),wmr=l(),Vue=a("span"),Amr=o("TFAutoModelForTokenClassification"),NBe=l(),Er=a("div"),m(s0.$$.fragment),Lmr=l(),Bc=a("p"),Bmr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),zue=a("code"),xmr=o("from_pretrained()"),kmr=o("class method or the "),Wue=a("code"),Rmr=o("from_config()"),Smr=o(`class
method.`),Pmr=l(),n0=a("p"),$mr=o("This class cannot be instantiated directly using "),Que=a("code"),Imr=o("__init__()"),Dmr=o(" (throws an error)."),jmr=l(),_t=a("div"),m(l0.$$.fragment),Nmr=l(),Hue=a("p"),qmr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Gmr=l(),xc=a("p"),Omr=o(`Note:
Loading a model from its configuration file does `),Uue=a("strong"),Xmr=o("not"),Vmr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Jue=a("code"),zmr=o("from_pretrained()"),Wmr=o("to load the model weights."),Qmr=l(),Yue=a("p"),Hmr=o("Examples:"),Umr=l(),m(i0.$$.fragment),Jmr=l(),Co=a("div"),m(d0.$$.fragment),Ymr=l(),Kue=a("p"),Kmr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Zmr=l(),bs=a("p"),efr=o("The model class to instantiate is selected based on the "),Zue=a("code"),ofr=o("model_type"),rfr=o(` property of the config object (either
passed as an argument or loaded from `),epe=a("code"),tfr=o("pretrained_model_name_or_path"),afr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ope=a("code"),sfr=o("pretrained_model_name_or_path"),nfr=o(":"),lfr=l(),K=a("ul"),P4=a("li"),rpe=a("strong"),ifr=o("albert"),dfr=o(" \u2014 "),eO=a("a"),cfr=o("TFAlbertForTokenClassification"),mfr=o(" (ALBERT model)"),ffr=l(),$4=a("li"),tpe=a("strong"),gfr=o("bert"),hfr=o(" \u2014 "),oO=a("a"),ufr=o("TFBertForTokenClassification"),pfr=o(" (BERT model)"),_fr=l(),I4=a("li"),ape=a("strong"),bfr=o("camembert"),vfr=o(" \u2014 "),rO=a("a"),Tfr=o("TFCamembertForTokenClassification"),Ffr=o(" (CamemBERT model)"),Cfr=l(),D4=a("li"),spe=a("strong"),Mfr=o("convbert"),Efr=o(" \u2014 "),tO=a("a"),yfr=o("TFConvBertForTokenClassification"),wfr=o(" (ConvBERT model)"),Afr=l(),j4=a("li"),npe=a("strong"),Lfr=o("deberta"),Bfr=o(" \u2014 "),aO=a("a"),xfr=o("TFDebertaForTokenClassification"),kfr=o(" (DeBERTa model)"),Rfr=l(),N4=a("li"),lpe=a("strong"),Sfr=o("deberta-v2"),Pfr=o(" \u2014 "),sO=a("a"),$fr=o("TFDebertaV2ForTokenClassification"),Ifr=o(" (DeBERTa-v2 model)"),Dfr=l(),q4=a("li"),ipe=a("strong"),jfr=o("distilbert"),Nfr=o(" \u2014 "),nO=a("a"),qfr=o("TFDistilBertForTokenClassification"),Gfr=o(" (DistilBERT model)"),Ofr=l(),G4=a("li"),dpe=a("strong"),Xfr=o("electra"),Vfr=o(" \u2014 "),lO=a("a"),zfr=o("TFElectraForTokenClassification"),Wfr=o(" (ELECTRA model)"),Qfr=l(),O4=a("li"),cpe=a("strong"),Hfr=o("flaubert"),Ufr=o(" \u2014 "),iO=a("a"),Jfr=o("TFFlaubertForTokenClassification"),Yfr=o(" (FlauBERT model)"),Kfr=l(),X4=a("li"),mpe=a("strong"),Zfr=o("funnel"),egr=o(" \u2014 "),dO=a("a"),ogr=o("TFFunnelForTokenClassification"),rgr=o(" (Funnel Transformer model)"),tgr=l(),V4=a("li"),fpe=a("strong"),agr=o("layoutlm"),sgr=o(" \u2014 "),cO=a("a"),ngr=o("TFLayoutLMForTokenClassification"),lgr=o(" (LayoutLM model)"),igr=l(),z4=a("li"),gpe=a("strong"),dgr=o("longformer"),cgr=o(" \u2014 "),mO=a("a"),mgr=o("TFLongformerForTokenClassification"),fgr=o(" (Longformer model)"),ggr=l(),W4=a("li"),hpe=a("strong"),hgr=o("mobilebert"),ugr=o(" \u2014 "),fO=a("a"),pgr=o("TFMobileBertForTokenClassification"),_gr=o(" (MobileBERT model)"),bgr=l(),Q4=a("li"),upe=a("strong"),vgr=o("mpnet"),Tgr=o(" \u2014 "),gO=a("a"),Fgr=o("TFMPNetForTokenClassification"),Cgr=o(" (MPNet model)"),Mgr=l(),H4=a("li"),ppe=a("strong"),Egr=o("rembert"),ygr=o(" \u2014 "),hO=a("a"),wgr=o("TFRemBertForTokenClassification"),Agr=o(" (RemBERT model)"),Lgr=l(),U4=a("li"),_pe=a("strong"),Bgr=o("roberta"),xgr=o(" \u2014 "),uO=a("a"),kgr=o("TFRobertaForTokenClassification"),Rgr=o(" (RoBERTa model)"),Sgr=l(),J4=a("li"),bpe=a("strong"),Pgr=o("roformer"),$gr=o(" \u2014 "),pO=a("a"),Igr=o("TFRoFormerForTokenClassification"),Dgr=o(" (RoFormer model)"),jgr=l(),Y4=a("li"),vpe=a("strong"),Ngr=o("xlm"),qgr=o(" \u2014 "),_O=a("a"),Ggr=o("TFXLMForTokenClassification"),Ogr=o(" (XLM model)"),Xgr=l(),K4=a("li"),Tpe=a("strong"),Vgr=o("xlm-roberta"),zgr=o(" \u2014 "),bO=a("a"),Wgr=o("TFXLMRobertaForTokenClassification"),Qgr=o(" (XLM-RoBERTa model)"),Hgr=l(),Z4=a("li"),Fpe=a("strong"),Ugr=o("xlnet"),Jgr=o(" \u2014 "),vO=a("a"),Ygr=o("TFXLNetForTokenClassification"),Kgr=o(" (XLNet model)"),Zgr=l(),Cpe=a("p"),ehr=o("Examples:"),ohr=l(),m(c0.$$.fragment),qBe=l(),kc=a("h2"),eM=a("a"),Mpe=a("span"),m(m0.$$.fragment),rhr=l(),Epe=a("span"),thr=o("TFAutoModelForQuestionAnswering"),GBe=l(),yr=a("div"),m(f0.$$.fragment),ahr=l(),Rc=a("p"),shr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),ype=a("code"),nhr=o("from_pretrained()"),lhr=o("class method or the "),wpe=a("code"),ihr=o("from_config()"),dhr=o(`class
method.`),chr=l(),g0=a("p"),mhr=o("This class cannot be instantiated directly using "),Ape=a("code"),fhr=o("__init__()"),ghr=o(" (throws an error)."),hhr=l(),bt=a("div"),m(h0.$$.fragment),uhr=l(),Lpe=a("p"),phr=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),_hr=l(),Sc=a("p"),bhr=o(`Note:
Loading a model from its configuration file does `),Bpe=a("strong"),vhr=o("not"),Thr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),xpe=a("code"),Fhr=o("from_pretrained()"),Chr=o("to load the model weights."),Mhr=l(),kpe=a("p"),Ehr=o("Examples:"),yhr=l(),m(u0.$$.fragment),whr=l(),Mo=a("div"),m(p0.$$.fragment),Ahr=l(),Rpe=a("p"),Lhr=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Bhr=l(),vs=a("p"),xhr=o("The model class to instantiate is selected based on the "),Spe=a("code"),khr=o("model_type"),Rhr=o(` property of the config object (either
passed as an argument or loaded from `),Ppe=a("code"),Shr=o("pretrained_model_name_or_path"),Phr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$pe=a("code"),$hr=o("pretrained_model_name_or_path"),Ihr=o(":"),Dhr=l(),Z=a("ul"),oM=a("li"),Ipe=a("strong"),jhr=o("albert"),Nhr=o(" \u2014 "),TO=a("a"),qhr=o("TFAlbertForQuestionAnswering"),Ghr=o(" (ALBERT model)"),Ohr=l(),rM=a("li"),Dpe=a("strong"),Xhr=o("bert"),Vhr=o(" \u2014 "),FO=a("a"),zhr=o("TFBertForQuestionAnswering"),Whr=o(" (BERT model)"),Qhr=l(),tM=a("li"),jpe=a("strong"),Hhr=o("camembert"),Uhr=o(" \u2014 "),CO=a("a"),Jhr=o("TFCamembertForQuestionAnswering"),Yhr=o(" (CamemBERT model)"),Khr=l(),aM=a("li"),Npe=a("strong"),Zhr=o("convbert"),eur=o(" \u2014 "),MO=a("a"),our=o("TFConvBertForQuestionAnswering"),rur=o(" (ConvBERT model)"),tur=l(),sM=a("li"),qpe=a("strong"),aur=o("deberta"),sur=o(" \u2014 "),EO=a("a"),nur=o("TFDebertaForQuestionAnswering"),lur=o(" (DeBERTa model)"),iur=l(),nM=a("li"),Gpe=a("strong"),dur=o("deberta-v2"),cur=o(" \u2014 "),yO=a("a"),mur=o("TFDebertaV2ForQuestionAnswering"),fur=o(" (DeBERTa-v2 model)"),gur=l(),lM=a("li"),Ope=a("strong"),hur=o("distilbert"),uur=o(" \u2014 "),wO=a("a"),pur=o("TFDistilBertForQuestionAnswering"),_ur=o(" (DistilBERT model)"),bur=l(),iM=a("li"),Xpe=a("strong"),vur=o("electra"),Tur=o(" \u2014 "),AO=a("a"),Fur=o("TFElectraForQuestionAnswering"),Cur=o(" (ELECTRA model)"),Mur=l(),dM=a("li"),Vpe=a("strong"),Eur=o("flaubert"),yur=o(" \u2014 "),LO=a("a"),wur=o("TFFlaubertForQuestionAnsweringSimple"),Aur=o(" (FlauBERT model)"),Lur=l(),cM=a("li"),zpe=a("strong"),Bur=o("funnel"),xur=o(" \u2014 "),BO=a("a"),kur=o("TFFunnelForQuestionAnswering"),Rur=o(" (Funnel Transformer model)"),Sur=l(),mM=a("li"),Wpe=a("strong"),Pur=o("longformer"),$ur=o(" \u2014 "),xO=a("a"),Iur=o("TFLongformerForQuestionAnswering"),Dur=o(" (Longformer model)"),jur=l(),fM=a("li"),Qpe=a("strong"),Nur=o("mobilebert"),qur=o(" \u2014 "),kO=a("a"),Gur=o("TFMobileBertForQuestionAnswering"),Our=o(" (MobileBERT model)"),Xur=l(),gM=a("li"),Hpe=a("strong"),Vur=o("mpnet"),zur=o(" \u2014 "),RO=a("a"),Wur=o("TFMPNetForQuestionAnswering"),Qur=o(" (MPNet model)"),Hur=l(),hM=a("li"),Upe=a("strong"),Uur=o("rembert"),Jur=o(" \u2014 "),SO=a("a"),Yur=o("TFRemBertForQuestionAnswering"),Kur=o(" (RemBERT model)"),Zur=l(),uM=a("li"),Jpe=a("strong"),epr=o("roberta"),opr=o(" \u2014 "),PO=a("a"),rpr=o("TFRobertaForQuestionAnswering"),tpr=o(" (RoBERTa model)"),apr=l(),pM=a("li"),Ype=a("strong"),spr=o("roformer"),npr=o(" \u2014 "),$O=a("a"),lpr=o("TFRoFormerForQuestionAnswering"),ipr=o(" (RoFormer model)"),dpr=l(),_M=a("li"),Kpe=a("strong"),cpr=o("xlm"),mpr=o(" \u2014 "),IO=a("a"),fpr=o("TFXLMForQuestionAnsweringSimple"),gpr=o(" (XLM model)"),hpr=l(),bM=a("li"),Zpe=a("strong"),upr=o("xlm-roberta"),ppr=o(" \u2014 "),DO=a("a"),_pr=o("TFXLMRobertaForQuestionAnswering"),bpr=o(" (XLM-RoBERTa model)"),vpr=l(),vM=a("li"),e_e=a("strong"),Tpr=o("xlnet"),Fpr=o(" \u2014 "),jO=a("a"),Cpr=o("TFXLNetForQuestionAnsweringSimple"),Mpr=o(" (XLNet model)"),Epr=l(),o_e=a("p"),ypr=o("Examples:"),wpr=l(),m(_0.$$.fragment),OBe=l(),Pc=a("h2"),TM=a("a"),r_e=a("span"),m(b0.$$.fragment),Apr=l(),t_e=a("span"),Lpr=o("TFAutoModelForVision2Seq"),XBe=l(),wr=a("div"),m(v0.$$.fragment),Bpr=l(),$c=a("p"),xpr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),a_e=a("code"),kpr=o("from_pretrained()"),Rpr=o("class method or the "),s_e=a("code"),Spr=o("from_config()"),Ppr=o(`class
method.`),$pr=l(),T0=a("p"),Ipr=o("This class cannot be instantiated directly using "),n_e=a("code"),Dpr=o("__init__()"),jpr=o(" (throws an error)."),Npr=l(),vt=a("div"),m(F0.$$.fragment),qpr=l(),l_e=a("p"),Gpr=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Opr=l(),Ic=a("p"),Xpr=o(`Note:
Loading a model from its configuration file does `),i_e=a("strong"),Vpr=o("not"),zpr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),d_e=a("code"),Wpr=o("from_pretrained()"),Qpr=o("to load the model weights."),Hpr=l(),c_e=a("p"),Upr=o("Examples:"),Jpr=l(),m(C0.$$.fragment),Ypr=l(),Eo=a("div"),m(M0.$$.fragment),Kpr=l(),m_e=a("p"),Zpr=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),e_r=l(),Ts=a("p"),o_r=o("The model class to instantiate is selected based on the "),f_e=a("code"),r_r=o("model_type"),t_r=o(` property of the config object (either
passed as an argument or loaded from `),g_e=a("code"),a_r=o("pretrained_model_name_or_path"),s_r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h_e=a("code"),n_r=o("pretrained_model_name_or_path"),l_r=o(":"),i_r=l(),u_e=a("ul"),FM=a("li"),p_e=a("strong"),d_r=o("vision-encoder-decoder"),c_r=o(" \u2014 "),NO=a("a"),m_r=o("TFVisionEncoderDecoderModel"),f_r=o(" (Vision Encoder decoder model)"),g_r=l(),__e=a("p"),h_r=o("Examples:"),u_r=l(),m(E0.$$.fragment),VBe=l(),Dc=a("h2"),CM=a("a"),b_e=a("span"),m(y0.$$.fragment),p_r=l(),v_e=a("span"),__r=o("TFAutoModelForSpeechSeq2Seq"),zBe=l(),Ar=a("div"),m(w0.$$.fragment),b_r=l(),jc=a("p"),v_r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),T_e=a("code"),T_r=o("from_pretrained()"),F_r=o("class method or the "),F_e=a("code"),C_r=o("from_config()"),M_r=o(`class
method.`),E_r=l(),A0=a("p"),y_r=o("This class cannot be instantiated directly using "),C_e=a("code"),w_r=o("__init__()"),A_r=o(" (throws an error)."),L_r=l(),Tt=a("div"),m(L0.$$.fragment),B_r=l(),M_e=a("p"),x_r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),k_r=l(),Nc=a("p"),R_r=o(`Note:
Loading a model from its configuration file does `),E_e=a("strong"),S_r=o("not"),P_r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),y_e=a("code"),$_r=o("from_pretrained()"),I_r=o("to load the model weights."),D_r=l(),w_e=a("p"),j_r=o("Examples:"),N_r=l(),m(B0.$$.fragment),q_r=l(),yo=a("div"),m(x0.$$.fragment),G_r=l(),A_e=a("p"),O_r=o("Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),X_r=l(),Fs=a("p"),V_r=o("The model class to instantiate is selected based on the "),L_e=a("code"),z_r=o("model_type"),W_r=o(` property of the config object (either
passed as an argument or loaded from `),B_e=a("code"),Q_r=o("pretrained_model_name_or_path"),H_r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),x_e=a("code"),U_r=o("pretrained_model_name_or_path"),J_r=o(":"),Y_r=l(),k_e=a("ul"),MM=a("li"),R_e=a("strong"),K_r=o("speech_to_text"),Z_r=o(" \u2014 "),qO=a("a"),ebr=o("TFSpeech2TextForConditionalGeneration"),obr=o(" (Speech2Text model)"),rbr=l(),S_e=a("p"),tbr=o("Examples:"),abr=l(),m(k0.$$.fragment),WBe=l(),qc=a("h2"),EM=a("a"),P_e=a("span"),m(R0.$$.fragment),sbr=l(),$_e=a("span"),nbr=o("FlaxAutoModel"),QBe=l(),Lr=a("div"),m(S0.$$.fragment),lbr=l(),Gc=a("p"),ibr=o(`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),I_e=a("code"),dbr=o("from_pretrained()"),cbr=o("class method or the "),D_e=a("code"),mbr=o("from_config()"),fbr=o(`class
method.`),gbr=l(),P0=a("p"),hbr=o("This class cannot be instantiated directly using "),j_e=a("code"),ubr=o("__init__()"),pbr=o(" (throws an error)."),_br=l(),Ft=a("div"),m($0.$$.fragment),bbr=l(),N_e=a("p"),vbr=o("Instantiates one of the base model classes of the library from a configuration."),Tbr=l(),Oc=a("p"),Fbr=o(`Note:
Loading a model from its configuration file does `),q_e=a("strong"),Cbr=o("not"),Mbr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),G_e=a("code"),Ebr=o("from_pretrained()"),ybr=o("to load the model weights."),wbr=l(),O_e=a("p"),Abr=o("Examples:"),Lbr=l(),m(I0.$$.fragment),Bbr=l(),wo=a("div"),m(D0.$$.fragment),xbr=l(),X_e=a("p"),kbr=o("Instantiate one of the base model classes of the library from a pretrained model."),Rbr=l(),Cs=a("p"),Sbr=o("The model class to instantiate is selected based on the "),V_e=a("code"),Pbr=o("model_type"),$br=o(` property of the config object (either
passed as an argument or loaded from `),z_e=a("code"),Ibr=o("pretrained_model_name_or_path"),Dbr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),W_e=a("code"),jbr=o("pretrained_model_name_or_path"),Nbr=o(":"),qbr=l(),z=a("ul"),yM=a("li"),Q_e=a("strong"),Gbr=o("albert"),Obr=o(" \u2014 "),GO=a("a"),Xbr=o("FlaxAlbertModel"),Vbr=o(" (ALBERT model)"),zbr=l(),wM=a("li"),H_e=a("strong"),Wbr=o("bart"),Qbr=o(" \u2014 "),OO=a("a"),Hbr=o("FlaxBartModel"),Ubr=o(" (BART model)"),Jbr=l(),AM=a("li"),U_e=a("strong"),Ybr=o("beit"),Kbr=o(" \u2014 "),XO=a("a"),Zbr=o("FlaxBeitModel"),e2r=o(" (BEiT model)"),o2r=l(),LM=a("li"),J_e=a("strong"),r2r=o("bert"),t2r=o(" \u2014 "),VO=a("a"),a2r=o("FlaxBertModel"),s2r=o(" (BERT model)"),n2r=l(),BM=a("li"),Y_e=a("strong"),l2r=o("big_bird"),i2r=o(" \u2014 "),zO=a("a"),d2r=o("FlaxBigBirdModel"),c2r=o(" (BigBird model)"),m2r=l(),xM=a("li"),K_e=a("strong"),f2r=o("blenderbot"),g2r=o(" \u2014 "),WO=a("a"),h2r=o("FlaxBlenderbotModel"),u2r=o(" (Blenderbot model)"),p2r=l(),kM=a("li"),Z_e=a("strong"),_2r=o("blenderbot-small"),b2r=o(" \u2014 "),QO=a("a"),v2r=o("FlaxBlenderbotSmallModel"),T2r=o(" (BlenderbotSmall model)"),F2r=l(),RM=a("li"),ebe=a("strong"),C2r=o("clip"),M2r=o(" \u2014 "),HO=a("a"),E2r=o("FlaxCLIPModel"),y2r=o(" (CLIP model)"),w2r=l(),SM=a("li"),obe=a("strong"),A2r=o("distilbert"),L2r=o(" \u2014 "),UO=a("a"),B2r=o("FlaxDistilBertModel"),x2r=o(" (DistilBERT model)"),k2r=l(),PM=a("li"),rbe=a("strong"),R2r=o("electra"),S2r=o(" \u2014 "),JO=a("a"),P2r=o("FlaxElectraModel"),$2r=o(" (ELECTRA model)"),I2r=l(),$M=a("li"),tbe=a("strong"),D2r=o("gpt2"),j2r=o(" \u2014 "),YO=a("a"),N2r=o("FlaxGPT2Model"),q2r=o(" (OpenAI GPT-2 model)"),G2r=l(),IM=a("li"),abe=a("strong"),O2r=o("gpt_neo"),X2r=o(" \u2014 "),KO=a("a"),V2r=o("FlaxGPTNeoModel"),z2r=o(" (GPT Neo model)"),W2r=l(),DM=a("li"),sbe=a("strong"),Q2r=o("gptj"),H2r=o(" \u2014 "),ZO=a("a"),U2r=o("FlaxGPTJModel"),J2r=o(" (GPT-J model)"),Y2r=l(),jM=a("li"),nbe=a("strong"),K2r=o("marian"),Z2r=o(" \u2014 "),eX=a("a"),evr=o("FlaxMarianModel"),ovr=o(" (Marian model)"),rvr=l(),NM=a("li"),lbe=a("strong"),tvr=o("mbart"),avr=o(" \u2014 "),oX=a("a"),svr=o("FlaxMBartModel"),nvr=o(" (mBART model)"),lvr=l(),qM=a("li"),ibe=a("strong"),ivr=o("mt5"),dvr=o(" \u2014 "),rX=a("a"),cvr=o("FlaxMT5Model"),mvr=o(" (mT5 model)"),fvr=l(),GM=a("li"),dbe=a("strong"),gvr=o("pegasus"),hvr=o(" \u2014 "),tX=a("a"),uvr=o("FlaxPegasusModel"),pvr=o(" (Pegasus model)"),_vr=l(),OM=a("li"),cbe=a("strong"),bvr=o("roberta"),vvr=o(" \u2014 "),aX=a("a"),Tvr=o("FlaxRobertaModel"),Fvr=o(" (RoBERTa model)"),Cvr=l(),XM=a("li"),mbe=a("strong"),Mvr=o("roformer"),Evr=o(" \u2014 "),sX=a("a"),yvr=o("FlaxRoFormerModel"),wvr=o(" (RoFormer model)"),Avr=l(),VM=a("li"),fbe=a("strong"),Lvr=o("t5"),Bvr=o(" \u2014 "),nX=a("a"),xvr=o("FlaxT5Model"),kvr=o(" (T5 model)"),Rvr=l(),zM=a("li"),gbe=a("strong"),Svr=o("vision-text-dual-encoder"),Pvr=o(" \u2014 "),lX=a("a"),$vr=o("FlaxVisionTextDualEncoderModel"),Ivr=o(" (VisionTextDualEncoder model)"),Dvr=l(),WM=a("li"),hbe=a("strong"),jvr=o("vit"),Nvr=o(" \u2014 "),iX=a("a"),qvr=o("FlaxViTModel"),Gvr=o(" (ViT model)"),Ovr=l(),QM=a("li"),ube=a("strong"),Xvr=o("wav2vec2"),Vvr=o(" \u2014 "),dX=a("a"),zvr=o("FlaxWav2Vec2Model"),Wvr=o(" (Wav2Vec2 model)"),Qvr=l(),HM=a("li"),pbe=a("strong"),Hvr=o("xglm"),Uvr=o(" \u2014 "),cX=a("a"),Jvr=o("FlaxXGLMModel"),Yvr=o(" (XGLM model)"),Kvr=l(),_be=a("p"),Zvr=o("Examples:"),eTr=l(),m(j0.$$.fragment),HBe=l(),Xc=a("h2"),UM=a("a"),bbe=a("span"),m(N0.$$.fragment),oTr=l(),vbe=a("span"),rTr=o("FlaxAutoModelForCausalLM"),UBe=l(),Br=a("div"),m(q0.$$.fragment),tTr=l(),Vc=a("p"),aTr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Tbe=a("code"),sTr=o("from_pretrained()"),nTr=o("class method or the "),Fbe=a("code"),lTr=o("from_config()"),iTr=o(`class
method.`),dTr=l(),G0=a("p"),cTr=o("This class cannot be instantiated directly using "),Cbe=a("code"),mTr=o("__init__()"),fTr=o(" (throws an error)."),gTr=l(),Ct=a("div"),m(O0.$$.fragment),hTr=l(),Mbe=a("p"),uTr=o("Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),pTr=l(),zc=a("p"),_Tr=o(`Note:
Loading a model from its configuration file does `),Ebe=a("strong"),bTr=o("not"),vTr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),ybe=a("code"),TTr=o("from_pretrained()"),FTr=o("to load the model weights."),CTr=l(),wbe=a("p"),MTr=o("Examples:"),ETr=l(),m(X0.$$.fragment),yTr=l(),Ao=a("div"),m(V0.$$.fragment),wTr=l(),Abe=a("p"),ATr=o("Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),LTr=l(),Ms=a("p"),BTr=o("The model class to instantiate is selected based on the "),Lbe=a("code"),xTr=o("model_type"),kTr=o(` property of the config object (either
passed as an argument or loaded from `),Bbe=a("code"),RTr=o("pretrained_model_name_or_path"),STr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xbe=a("code"),PTr=o("pretrained_model_name_or_path"),$Tr=o(":"),ITr=l(),Es=a("ul"),JM=a("li"),kbe=a("strong"),DTr=o("gpt2"),jTr=o(" \u2014 "),mX=a("a"),NTr=o("FlaxGPT2LMHeadModel"),qTr=o(" (OpenAI GPT-2 model)"),GTr=l(),YM=a("li"),Rbe=a("strong"),OTr=o("gpt_neo"),XTr=o(" \u2014 "),fX=a("a"),VTr=o("FlaxGPTNeoForCausalLM"),zTr=o(" (GPT Neo model)"),WTr=l(),KM=a("li"),Sbe=a("strong"),QTr=o("gptj"),HTr=o(" \u2014 "),gX=a("a"),UTr=o("FlaxGPTJForCausalLM"),JTr=o(" (GPT-J model)"),YTr=l(),ZM=a("li"),Pbe=a("strong"),KTr=o("xglm"),ZTr=o(" \u2014 "),hX=a("a"),e1r=o("FlaxXGLMForCausalLM"),o1r=o(" (XGLM model)"),r1r=l(),$be=a("p"),t1r=o("Examples:"),a1r=l(),m(z0.$$.fragment),JBe=l(),Wc=a("h2"),eE=a("a"),Ibe=a("span"),m(W0.$$.fragment),s1r=l(),Dbe=a("span"),n1r=o("FlaxAutoModelForPreTraining"),YBe=l(),xr=a("div"),m(Q0.$$.fragment),l1r=l(),Qc=a("p"),i1r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),jbe=a("code"),d1r=o("from_pretrained()"),c1r=o("class method or the "),Nbe=a("code"),m1r=o("from_config()"),f1r=o(`class
method.`),g1r=l(),H0=a("p"),h1r=o("This class cannot be instantiated directly using "),qbe=a("code"),u1r=o("__init__()"),p1r=o(" (throws an error)."),_1r=l(),Mt=a("div"),m(U0.$$.fragment),b1r=l(),Gbe=a("p"),v1r=o("Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),T1r=l(),Hc=a("p"),F1r=o(`Note:
Loading a model from its configuration file does `),Obe=a("strong"),C1r=o("not"),M1r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Xbe=a("code"),E1r=o("from_pretrained()"),y1r=o("to load the model weights."),w1r=l(),Vbe=a("p"),A1r=o("Examples:"),L1r=l(),m(J0.$$.fragment),B1r=l(),Lo=a("div"),m(Y0.$$.fragment),x1r=l(),zbe=a("p"),k1r=o("Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),R1r=l(),ys=a("p"),S1r=o("The model class to instantiate is selected based on the "),Wbe=a("code"),P1r=o("model_type"),$1r=o(` property of the config object (either
passed as an argument or loaded from `),Qbe=a("code"),I1r=o("pretrained_model_name_or_path"),D1r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hbe=a("code"),j1r=o("pretrained_model_name_or_path"),N1r=o(":"),q1r=l(),me=a("ul"),oE=a("li"),Ube=a("strong"),G1r=o("albert"),O1r=o(" \u2014 "),uX=a("a"),X1r=o("FlaxAlbertForPreTraining"),V1r=o(" (ALBERT model)"),z1r=l(),rE=a("li"),Jbe=a("strong"),W1r=o("bart"),Q1r=o(" \u2014 "),pX=a("a"),H1r=o("FlaxBartForConditionalGeneration"),U1r=o(" (BART model)"),J1r=l(),tE=a("li"),Ybe=a("strong"),Y1r=o("bert"),K1r=o(" \u2014 "),_X=a("a"),Z1r=o("FlaxBertForPreTraining"),eFr=o(" (BERT model)"),oFr=l(),aE=a("li"),Kbe=a("strong"),rFr=o("big_bird"),tFr=o(" \u2014 "),bX=a("a"),aFr=o("FlaxBigBirdForPreTraining"),sFr=o(" (BigBird model)"),nFr=l(),sE=a("li"),Zbe=a("strong"),lFr=o("electra"),iFr=o(" \u2014 "),vX=a("a"),dFr=o("FlaxElectraForPreTraining"),cFr=o(" (ELECTRA model)"),mFr=l(),nE=a("li"),e2e=a("strong"),fFr=o("mbart"),gFr=o(" \u2014 "),TX=a("a"),hFr=o("FlaxMBartForConditionalGeneration"),uFr=o(" (mBART model)"),pFr=l(),lE=a("li"),o2e=a("strong"),_Fr=o("mt5"),bFr=o(" \u2014 "),FX=a("a"),vFr=o("FlaxMT5ForConditionalGeneration"),TFr=o(" (mT5 model)"),FFr=l(),iE=a("li"),r2e=a("strong"),CFr=o("roberta"),MFr=o(" \u2014 "),CX=a("a"),EFr=o("FlaxRobertaForMaskedLM"),yFr=o(" (RoBERTa model)"),wFr=l(),dE=a("li"),t2e=a("strong"),AFr=o("roformer"),LFr=o(" \u2014 "),MX=a("a"),BFr=o("FlaxRoFormerForMaskedLM"),xFr=o(" (RoFormer model)"),kFr=l(),cE=a("li"),a2e=a("strong"),RFr=o("t5"),SFr=o(" \u2014 "),EX=a("a"),PFr=o("FlaxT5ForConditionalGeneration"),$Fr=o(" (T5 model)"),IFr=l(),mE=a("li"),s2e=a("strong"),DFr=o("wav2vec2"),jFr=o(" \u2014 "),yX=a("a"),NFr=o("FlaxWav2Vec2ForPreTraining"),qFr=o(" (Wav2Vec2 model)"),GFr=l(),n2e=a("p"),OFr=o("Examples:"),XFr=l(),m(K0.$$.fragment),KBe=l(),Uc=a("h2"),fE=a("a"),l2e=a("span"),m(Z0.$$.fragment),VFr=l(),i2e=a("span"),zFr=o("FlaxAutoModelForMaskedLM"),ZBe=l(),kr=a("div"),m(eL.$$.fragment),WFr=l(),Jc=a("p"),QFr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),d2e=a("code"),HFr=o("from_pretrained()"),UFr=o("class method or the "),c2e=a("code"),JFr=o("from_config()"),YFr=o(`class
method.`),KFr=l(),oL=a("p"),ZFr=o("This class cannot be instantiated directly using "),m2e=a("code"),eCr=o("__init__()"),oCr=o(" (throws an error)."),rCr=l(),Et=a("div"),m(rL.$$.fragment),tCr=l(),f2e=a("p"),aCr=o("Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),sCr=l(),Yc=a("p"),nCr=o(`Note:
Loading a model from its configuration file does `),g2e=a("strong"),lCr=o("not"),iCr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),h2e=a("code"),dCr=o("from_pretrained()"),cCr=o("to load the model weights."),mCr=l(),u2e=a("p"),fCr=o("Examples:"),gCr=l(),m(tL.$$.fragment),hCr=l(),Bo=a("div"),m(aL.$$.fragment),uCr=l(),p2e=a("p"),pCr=o("Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),_Cr=l(),ws=a("p"),bCr=o("The model class to instantiate is selected based on the "),_2e=a("code"),vCr=o("model_type"),TCr=o(` property of the config object (either
passed as an argument or loaded from `),b2e=a("code"),FCr=o("pretrained_model_name_or_path"),CCr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),v2e=a("code"),MCr=o("pretrained_model_name_or_path"),ECr=o(":"),yCr=l(),ve=a("ul"),gE=a("li"),T2e=a("strong"),wCr=o("albert"),ACr=o(" \u2014 "),wX=a("a"),LCr=o("FlaxAlbertForMaskedLM"),BCr=o(" (ALBERT model)"),xCr=l(),hE=a("li"),F2e=a("strong"),kCr=o("bart"),RCr=o(" \u2014 "),AX=a("a"),SCr=o("FlaxBartForConditionalGeneration"),PCr=o(" (BART model)"),$Cr=l(),uE=a("li"),C2e=a("strong"),ICr=o("bert"),DCr=o(" \u2014 "),LX=a("a"),jCr=o("FlaxBertForMaskedLM"),NCr=o(" (BERT model)"),qCr=l(),pE=a("li"),M2e=a("strong"),GCr=o("big_bird"),OCr=o(" \u2014 "),BX=a("a"),XCr=o("FlaxBigBirdForMaskedLM"),VCr=o(" (BigBird model)"),zCr=l(),_E=a("li"),E2e=a("strong"),WCr=o("distilbert"),QCr=o(" \u2014 "),xX=a("a"),HCr=o("FlaxDistilBertForMaskedLM"),UCr=o(" (DistilBERT model)"),JCr=l(),bE=a("li"),y2e=a("strong"),YCr=o("electra"),KCr=o(" \u2014 "),kX=a("a"),ZCr=o("FlaxElectraForMaskedLM"),e4r=o(" (ELECTRA model)"),o4r=l(),vE=a("li"),w2e=a("strong"),r4r=o("mbart"),t4r=o(" \u2014 "),RX=a("a"),a4r=o("FlaxMBartForConditionalGeneration"),s4r=o(" (mBART model)"),n4r=l(),TE=a("li"),A2e=a("strong"),l4r=o("roberta"),i4r=o(" \u2014 "),SX=a("a"),d4r=o("FlaxRobertaForMaskedLM"),c4r=o(" (RoBERTa model)"),m4r=l(),FE=a("li"),L2e=a("strong"),f4r=o("roformer"),g4r=o(" \u2014 "),PX=a("a"),h4r=o("FlaxRoFormerForMaskedLM"),u4r=o(" (RoFormer model)"),p4r=l(),B2e=a("p"),_4r=o("Examples:"),b4r=l(),m(sL.$$.fragment),exe=l(),Kc=a("h2"),CE=a("a"),x2e=a("span"),m(nL.$$.fragment),v4r=l(),k2e=a("span"),T4r=o("FlaxAutoModelForSeq2SeqLM"),oxe=l(),Rr=a("div"),m(lL.$$.fragment),F4r=l(),Zc=a("p"),C4r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),R2e=a("code"),M4r=o("from_pretrained()"),E4r=o("class method or the "),S2e=a("code"),y4r=o("from_config()"),w4r=o(`class
method.`),A4r=l(),iL=a("p"),L4r=o("This class cannot be instantiated directly using "),P2e=a("code"),B4r=o("__init__()"),x4r=o(" (throws an error)."),k4r=l(),yt=a("div"),m(dL.$$.fragment),R4r=l(),$2e=a("p"),S4r=o("Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),P4r=l(),em=a("p"),$4r=o(`Note:
Loading a model from its configuration file does `),I2e=a("strong"),I4r=o("not"),D4r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),D2e=a("code"),j4r=o("from_pretrained()"),N4r=o("to load the model weights."),q4r=l(),j2e=a("p"),G4r=o("Examples:"),O4r=l(),m(cL.$$.fragment),X4r=l(),xo=a("div"),m(mL.$$.fragment),V4r=l(),N2e=a("p"),z4r=o("Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),W4r=l(),As=a("p"),Q4r=o("The model class to instantiate is selected based on the "),q2e=a("code"),H4r=o("model_type"),U4r=o(` property of the config object (either
passed as an argument or loaded from `),G2e=a("code"),J4r=o("pretrained_model_name_or_path"),Y4r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),O2e=a("code"),K4r=o("pretrained_model_name_or_path"),Z4r=o(":"),eMr=l(),Te=a("ul"),ME=a("li"),X2e=a("strong"),oMr=o("bart"),rMr=o(" \u2014 "),$X=a("a"),tMr=o("FlaxBartForConditionalGeneration"),aMr=o(" (BART model)"),sMr=l(),EE=a("li"),V2e=a("strong"),nMr=o("blenderbot"),lMr=o(" \u2014 "),IX=a("a"),iMr=o("FlaxBlenderbotForConditionalGeneration"),dMr=o(" (Blenderbot model)"),cMr=l(),yE=a("li"),z2e=a("strong"),mMr=o("blenderbot-small"),fMr=o(" \u2014 "),DX=a("a"),gMr=o("FlaxBlenderbotSmallForConditionalGeneration"),hMr=o(" (BlenderbotSmall model)"),uMr=l(),wE=a("li"),W2e=a("strong"),pMr=o("encoder-decoder"),_Mr=o(" \u2014 "),jX=a("a"),bMr=o("FlaxEncoderDecoderModel"),vMr=o(" (Encoder decoder model)"),TMr=l(),AE=a("li"),Q2e=a("strong"),FMr=o("marian"),CMr=o(" \u2014 "),NX=a("a"),MMr=o("FlaxMarianMTModel"),EMr=o(" (Marian model)"),yMr=l(),LE=a("li"),H2e=a("strong"),wMr=o("mbart"),AMr=o(" \u2014 "),qX=a("a"),LMr=o("FlaxMBartForConditionalGeneration"),BMr=o(" (mBART model)"),xMr=l(),BE=a("li"),U2e=a("strong"),kMr=o("mt5"),RMr=o(" \u2014 "),GX=a("a"),SMr=o("FlaxMT5ForConditionalGeneration"),PMr=o(" (mT5 model)"),$Mr=l(),xE=a("li"),J2e=a("strong"),IMr=o("pegasus"),DMr=o(" \u2014 "),OX=a("a"),jMr=o("FlaxPegasusForConditionalGeneration"),NMr=o(" (Pegasus model)"),qMr=l(),kE=a("li"),Y2e=a("strong"),GMr=o("t5"),OMr=o(" \u2014 "),XX=a("a"),XMr=o("FlaxT5ForConditionalGeneration"),VMr=o(" (T5 model)"),zMr=l(),K2e=a("p"),WMr=o("Examples:"),QMr=l(),m(fL.$$.fragment),rxe=l(),om=a("h2"),RE=a("a"),Z2e=a("span"),m(gL.$$.fragment),HMr=l(),eve=a("span"),UMr=o("FlaxAutoModelForSequenceClassification"),txe=l(),Sr=a("div"),m(hL.$$.fragment),JMr=l(),rm=a("p"),YMr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),ove=a("code"),KMr=o("from_pretrained()"),ZMr=o("class method or the "),rve=a("code"),eEr=o("from_config()"),oEr=o(`class
method.`),rEr=l(),uL=a("p"),tEr=o("This class cannot be instantiated directly using "),tve=a("code"),aEr=o("__init__()"),sEr=o(" (throws an error)."),nEr=l(),wt=a("div"),m(pL.$$.fragment),lEr=l(),ave=a("p"),iEr=o("Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),dEr=l(),tm=a("p"),cEr=o(`Note:
Loading a model from its configuration file does `),sve=a("strong"),mEr=o("not"),fEr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),nve=a("code"),gEr=o("from_pretrained()"),hEr=o("to load the model weights."),uEr=l(),lve=a("p"),pEr=o("Examples:"),_Er=l(),m(_L.$$.fragment),bEr=l(),ko=a("div"),m(bL.$$.fragment),vEr=l(),ive=a("p"),TEr=o("Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),FEr=l(),Ls=a("p"),CEr=o("The model class to instantiate is selected based on the "),dve=a("code"),MEr=o("model_type"),EEr=o(` property of the config object (either
passed as an argument or loaded from `),cve=a("code"),yEr=o("pretrained_model_name_or_path"),wEr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mve=a("code"),AEr=o("pretrained_model_name_or_path"),LEr=o(":"),BEr=l(),Fe=a("ul"),SE=a("li"),fve=a("strong"),xEr=o("albert"),kEr=o(" \u2014 "),VX=a("a"),REr=o("FlaxAlbertForSequenceClassification"),SEr=o(" (ALBERT model)"),PEr=l(),PE=a("li"),gve=a("strong"),$Er=o("bart"),IEr=o(" \u2014 "),zX=a("a"),DEr=o("FlaxBartForSequenceClassification"),jEr=o(" (BART model)"),NEr=l(),$E=a("li"),hve=a("strong"),qEr=o("bert"),GEr=o(" \u2014 "),WX=a("a"),OEr=o("FlaxBertForSequenceClassification"),XEr=o(" (BERT model)"),VEr=l(),IE=a("li"),uve=a("strong"),zEr=o("big_bird"),WEr=o(" \u2014 "),QX=a("a"),QEr=o("FlaxBigBirdForSequenceClassification"),HEr=o(" (BigBird model)"),UEr=l(),DE=a("li"),pve=a("strong"),JEr=o("distilbert"),YEr=o(" \u2014 "),HX=a("a"),KEr=o("FlaxDistilBertForSequenceClassification"),ZEr=o(" (DistilBERT model)"),e3r=l(),jE=a("li"),_ve=a("strong"),o3r=o("electra"),r3r=o(" \u2014 "),UX=a("a"),t3r=o("FlaxElectraForSequenceClassification"),a3r=o(" (ELECTRA model)"),s3r=l(),NE=a("li"),bve=a("strong"),n3r=o("mbart"),l3r=o(" \u2014 "),JX=a("a"),i3r=o("FlaxMBartForSequenceClassification"),d3r=o(" (mBART model)"),c3r=l(),qE=a("li"),vve=a("strong"),m3r=o("roberta"),f3r=o(" \u2014 "),YX=a("a"),g3r=o("FlaxRobertaForSequenceClassification"),h3r=o(" (RoBERTa model)"),u3r=l(),GE=a("li"),Tve=a("strong"),p3r=o("roformer"),_3r=o(" \u2014 "),KX=a("a"),b3r=o("FlaxRoFormerForSequenceClassification"),v3r=o(" (RoFormer model)"),T3r=l(),Fve=a("p"),F3r=o("Examples:"),C3r=l(),m(vL.$$.fragment),axe=l(),am=a("h2"),OE=a("a"),Cve=a("span"),m(TL.$$.fragment),M3r=l(),Mve=a("span"),E3r=o("FlaxAutoModelForQuestionAnswering"),sxe=l(),Pr=a("div"),m(FL.$$.fragment),y3r=l(),sm=a("p"),w3r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Eve=a("code"),A3r=o("from_pretrained()"),L3r=o("class method or the "),yve=a("code"),B3r=o("from_config()"),x3r=o(`class
method.`),k3r=l(),CL=a("p"),R3r=o("This class cannot be instantiated directly using "),wve=a("code"),S3r=o("__init__()"),P3r=o(" (throws an error)."),$3r=l(),At=a("div"),m(ML.$$.fragment),I3r=l(),Ave=a("p"),D3r=o("Instantiates one of the model classes of the library (with a question answering head) from a configuration."),j3r=l(),nm=a("p"),N3r=o(`Note:
Loading a model from its configuration file does `),Lve=a("strong"),q3r=o("not"),G3r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Bve=a("code"),O3r=o("from_pretrained()"),X3r=o("to load the model weights."),V3r=l(),xve=a("p"),z3r=o("Examples:"),W3r=l(),m(EL.$$.fragment),Q3r=l(),Ro=a("div"),m(yL.$$.fragment),H3r=l(),kve=a("p"),U3r=o("Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),J3r=l(),Bs=a("p"),Y3r=o("The model class to instantiate is selected based on the "),Rve=a("code"),K3r=o("model_type"),Z3r=o(` property of the config object (either
passed as an argument or loaded from `),Sve=a("code"),e5r=o("pretrained_model_name_or_path"),o5r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pve=a("code"),r5r=o("pretrained_model_name_or_path"),t5r=o(":"),a5r=l(),Ce=a("ul"),XE=a("li"),$ve=a("strong"),s5r=o("albert"),n5r=o(" \u2014 "),ZX=a("a"),l5r=o("FlaxAlbertForQuestionAnswering"),i5r=o(" (ALBERT model)"),d5r=l(),VE=a("li"),Ive=a("strong"),c5r=o("bart"),m5r=o(" \u2014 "),eV=a("a"),f5r=o("FlaxBartForQuestionAnswering"),g5r=o(" (BART model)"),h5r=l(),zE=a("li"),Dve=a("strong"),u5r=o("bert"),p5r=o(" \u2014 "),oV=a("a"),_5r=o("FlaxBertForQuestionAnswering"),b5r=o(" (BERT model)"),v5r=l(),WE=a("li"),jve=a("strong"),T5r=o("big_bird"),F5r=o(" \u2014 "),rV=a("a"),C5r=o("FlaxBigBirdForQuestionAnswering"),M5r=o(" (BigBird model)"),E5r=l(),QE=a("li"),Nve=a("strong"),y5r=o("distilbert"),w5r=o(" \u2014 "),tV=a("a"),A5r=o("FlaxDistilBertForQuestionAnswering"),L5r=o(" (DistilBERT model)"),B5r=l(),HE=a("li"),qve=a("strong"),x5r=o("electra"),k5r=o(" \u2014 "),aV=a("a"),R5r=o("FlaxElectraForQuestionAnswering"),S5r=o(" (ELECTRA model)"),P5r=l(),UE=a("li"),Gve=a("strong"),$5r=o("mbart"),I5r=o(" \u2014 "),sV=a("a"),D5r=o("FlaxMBartForQuestionAnswering"),j5r=o(" (mBART model)"),N5r=l(),JE=a("li"),Ove=a("strong"),q5r=o("roberta"),G5r=o(" \u2014 "),nV=a("a"),O5r=o("FlaxRobertaForQuestionAnswering"),X5r=o(" (RoBERTa model)"),V5r=l(),YE=a("li"),Xve=a("strong"),z5r=o("roformer"),W5r=o(" \u2014 "),lV=a("a"),Q5r=o("FlaxRoFormerForQuestionAnswering"),H5r=o(" (RoFormer model)"),U5r=l(),Vve=a("p"),J5r=o("Examples:"),Y5r=l(),m(wL.$$.fragment),nxe=l(),lm=a("h2"),KE=a("a"),zve=a("span"),m(AL.$$.fragment),K5r=l(),Wve=a("span"),Z5r=o("FlaxAutoModelForTokenClassification"),lxe=l(),$r=a("div"),m(LL.$$.fragment),eyr=l(),im=a("p"),oyr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Qve=a("code"),ryr=o("from_pretrained()"),tyr=o("class method or the "),Hve=a("code"),ayr=o("from_config()"),syr=o(`class
method.`),nyr=l(),BL=a("p"),lyr=o("This class cannot be instantiated directly using "),Uve=a("code"),iyr=o("__init__()"),dyr=o(" (throws an error)."),cyr=l(),Lt=a("div"),m(xL.$$.fragment),myr=l(),Jve=a("p"),fyr=o("Instantiates one of the model classes of the library (with a token classification head) from a configuration."),gyr=l(),dm=a("p"),hyr=o(`Note:
Loading a model from its configuration file does `),Yve=a("strong"),uyr=o("not"),pyr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),Kve=a("code"),_yr=o("from_pretrained()"),byr=o("to load the model weights."),vyr=l(),Zve=a("p"),Tyr=o("Examples:"),Fyr=l(),m(kL.$$.fragment),Cyr=l(),So=a("div"),m(RL.$$.fragment),Myr=l(),eTe=a("p"),Eyr=o("Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),yyr=l(),xs=a("p"),wyr=o("The model class to instantiate is selected based on the "),oTe=a("code"),Ayr=o("model_type"),Lyr=o(` property of the config object (either
passed as an argument or loaded from `),rTe=a("code"),Byr=o("pretrained_model_name_or_path"),xyr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tTe=a("code"),kyr=o("pretrained_model_name_or_path"),Ryr=o(":"),Syr=l(),no=a("ul"),ZE=a("li"),aTe=a("strong"),Pyr=o("albert"),$yr=o(" \u2014 "),iV=a("a"),Iyr=o("FlaxAlbertForTokenClassification"),Dyr=o(" (ALBERT model)"),jyr=l(),e3=a("li"),sTe=a("strong"),Nyr=o("bert"),qyr=o(" \u2014 "),dV=a("a"),Gyr=o("FlaxBertForTokenClassification"),Oyr=o(" (BERT model)"),Xyr=l(),o3=a("li"),nTe=a("strong"),Vyr=o("big_bird"),zyr=o(" \u2014 "),cV=a("a"),Wyr=o("FlaxBigBirdForTokenClassification"),Qyr=o(" (BigBird model)"),Hyr=l(),r3=a("li"),lTe=a("strong"),Uyr=o("distilbert"),Jyr=o(" \u2014 "),mV=a("a"),Yyr=o("FlaxDistilBertForTokenClassification"),Kyr=o(" (DistilBERT model)"),Zyr=l(),t3=a("li"),iTe=a("strong"),ewr=o("electra"),owr=o(" \u2014 "),fV=a("a"),rwr=o("FlaxElectraForTokenClassification"),twr=o(" (ELECTRA model)"),awr=l(),a3=a("li"),dTe=a("strong"),swr=o("roberta"),nwr=o(" \u2014 "),gV=a("a"),lwr=o("FlaxRobertaForTokenClassification"),iwr=o(" (RoBERTa model)"),dwr=l(),s3=a("li"),cTe=a("strong"),cwr=o("roformer"),mwr=o(" \u2014 "),hV=a("a"),fwr=o("FlaxRoFormerForTokenClassification"),gwr=o(" (RoFormer model)"),hwr=l(),mTe=a("p"),uwr=o("Examples:"),pwr=l(),m(SL.$$.fragment),ixe=l(),cm=a("h2"),n3=a("a"),fTe=a("span"),m(PL.$$.fragment),_wr=l(),gTe=a("span"),bwr=o("FlaxAutoModelForMultipleChoice"),dxe=l(),Ir=a("div"),m($L.$$.fragment),vwr=l(),mm=a("p"),Twr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),hTe=a("code"),Fwr=o("from_pretrained()"),Cwr=o("class method or the "),uTe=a("code"),Mwr=o("from_config()"),Ewr=o(`class
method.`),ywr=l(),IL=a("p"),wwr=o("This class cannot be instantiated directly using "),pTe=a("code"),Awr=o("__init__()"),Lwr=o(" (throws an error)."),Bwr=l(),Bt=a("div"),m(DL.$$.fragment),xwr=l(),_Te=a("p"),kwr=o("Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),Rwr=l(),fm=a("p"),Swr=o(`Note:
Loading a model from its configuration file does `),bTe=a("strong"),Pwr=o("not"),$wr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),vTe=a("code"),Iwr=o("from_pretrained()"),Dwr=o("to load the model weights."),jwr=l(),TTe=a("p"),Nwr=o("Examples:"),qwr=l(),m(jL.$$.fragment),Gwr=l(),Po=a("div"),m(NL.$$.fragment),Owr=l(),FTe=a("p"),Xwr=o("Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Vwr=l(),ks=a("p"),zwr=o("The model class to instantiate is selected based on the "),CTe=a("code"),Wwr=o("model_type"),Qwr=o(` property of the config object (either
passed as an argument or loaded from `),MTe=a("code"),Hwr=o("pretrained_model_name_or_path"),Uwr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ETe=a("code"),Jwr=o("pretrained_model_name_or_path"),Ywr=o(":"),Kwr=l(),lo=a("ul"),l3=a("li"),yTe=a("strong"),Zwr=o("albert"),e6r=o(" \u2014 "),uV=a("a"),o6r=o("FlaxAlbertForMultipleChoice"),r6r=o(" (ALBERT model)"),t6r=l(),i3=a("li"),wTe=a("strong"),a6r=o("bert"),s6r=o(" \u2014 "),pV=a("a"),n6r=o("FlaxBertForMultipleChoice"),l6r=o(" (BERT model)"),i6r=l(),d3=a("li"),ATe=a("strong"),d6r=o("big_bird"),c6r=o(" \u2014 "),_V=a("a"),m6r=o("FlaxBigBirdForMultipleChoice"),f6r=o(" (BigBird model)"),g6r=l(),c3=a("li"),LTe=a("strong"),h6r=o("distilbert"),u6r=o(" \u2014 "),bV=a("a"),p6r=o("FlaxDistilBertForMultipleChoice"),_6r=o(" (DistilBERT model)"),b6r=l(),m3=a("li"),BTe=a("strong"),v6r=o("electra"),T6r=o(" \u2014 "),vV=a("a"),F6r=o("FlaxElectraForMultipleChoice"),C6r=o(" (ELECTRA model)"),M6r=l(),f3=a("li"),xTe=a("strong"),E6r=o("roberta"),y6r=o(" \u2014 "),TV=a("a"),w6r=o("FlaxRobertaForMultipleChoice"),A6r=o(" (RoBERTa model)"),L6r=l(),g3=a("li"),kTe=a("strong"),B6r=o("roformer"),x6r=o(" \u2014 "),FV=a("a"),k6r=o("FlaxRoFormerForMultipleChoice"),R6r=o(" (RoFormer model)"),S6r=l(),RTe=a("p"),P6r=o("Examples:"),$6r=l(),m(qL.$$.fragment),cxe=l(),gm=a("h2"),h3=a("a"),STe=a("span"),m(GL.$$.fragment),I6r=l(),PTe=a("span"),D6r=o("FlaxAutoModelForNextSentencePrediction"),mxe=l(),Dr=a("div"),m(OL.$$.fragment),j6r=l(),hm=a("p"),N6r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),$Te=a("code"),q6r=o("from_pretrained()"),G6r=o("class method or the "),ITe=a("code"),O6r=o("from_config()"),X6r=o(`class
method.`),V6r=l(),XL=a("p"),z6r=o("This class cannot be instantiated directly using "),DTe=a("code"),W6r=o("__init__()"),Q6r=o(" (throws an error)."),H6r=l(),xt=a("div"),m(VL.$$.fragment),U6r=l(),jTe=a("p"),J6r=o("Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Y6r=l(),um=a("p"),K6r=o(`Note:
Loading a model from its configuration file does `),NTe=a("strong"),Z6r=o("not"),eAr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),qTe=a("code"),oAr=o("from_pretrained()"),rAr=o("to load the model weights."),tAr=l(),GTe=a("p"),aAr=o("Examples:"),sAr=l(),m(zL.$$.fragment),nAr=l(),$o=a("div"),m(WL.$$.fragment),lAr=l(),OTe=a("p"),iAr=o("Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),dAr=l(),Rs=a("p"),cAr=o("The model class to instantiate is selected based on the "),XTe=a("code"),mAr=o("model_type"),fAr=o(` property of the config object (either
passed as an argument or loaded from `),VTe=a("code"),gAr=o("pretrained_model_name_or_path"),hAr=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zTe=a("code"),uAr=o("pretrained_model_name_or_path"),pAr=o(":"),_Ar=l(),WTe=a("ul"),u3=a("li"),QTe=a("strong"),bAr=o("bert"),vAr=o(" \u2014 "),CV=a("a"),TAr=o("FlaxBertForNextSentencePrediction"),FAr=o(" (BERT model)"),CAr=l(),HTe=a("p"),MAr=o("Examples:"),EAr=l(),m(QL.$$.fragment),fxe=l(),pm=a("h2"),p3=a("a"),UTe=a("span"),m(HL.$$.fragment),yAr=l(),JTe=a("span"),wAr=o("FlaxAutoModelForImageClassification"),gxe=l(),jr=a("div"),m(UL.$$.fragment),AAr=l(),_m=a("p"),LAr=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),YTe=a("code"),BAr=o("from_pretrained()"),xAr=o("class method or the "),KTe=a("code"),kAr=o("from_config()"),RAr=o(`class
method.`),SAr=l(),JL=a("p"),PAr=o("This class cannot be instantiated directly using "),ZTe=a("code"),$Ar=o("__init__()"),IAr=o(" (throws an error)."),DAr=l(),kt=a("div"),m(YL.$$.fragment),jAr=l(),e1e=a("p"),NAr=o("Instantiates one of the model classes of the library (with a image classification head) from a configuration."),qAr=l(),bm=a("p"),GAr=o(`Note:
Loading a model from its configuration file does `),o1e=a("strong"),OAr=o("not"),XAr=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),r1e=a("code"),VAr=o("from_pretrained()"),zAr=o("to load the model weights."),WAr=l(),t1e=a("p"),QAr=o("Examples:"),HAr=l(),m(KL.$$.fragment),UAr=l(),Io=a("div"),m(ZL.$$.fragment),JAr=l(),a1e=a("p"),YAr=o("Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),KAr=l(),Ss=a("p"),ZAr=o("The model class to instantiate is selected based on the "),s1e=a("code"),e0r=o("model_type"),o0r=o(` property of the config object (either
passed as an argument or loaded from `),n1e=a("code"),r0r=o("pretrained_model_name_or_path"),t0r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),l1e=a("code"),a0r=o("pretrained_model_name_or_path"),s0r=o(":"),n0r=l(),e8=a("ul"),_3=a("li"),i1e=a("strong"),l0r=o("beit"),i0r=o(" \u2014 "),MV=a("a"),d0r=o("FlaxBeitForImageClassification"),c0r=o(" (BEiT model)"),m0r=l(),b3=a("li"),d1e=a("strong"),f0r=o("vit"),g0r=o(" \u2014 "),EV=a("a"),h0r=o("FlaxViTForImageClassification"),u0r=o(" (ViT model)"),p0r=l(),c1e=a("p"),_0r=o("Examples:"),b0r=l(),m(o8.$$.fragment),hxe=l(),vm=a("h2"),v3=a("a"),m1e=a("span"),m(r8.$$.fragment),v0r=l(),f1e=a("span"),T0r=o("FlaxAutoModelForVision2Seq"),uxe=l(),Nr=a("div"),m(t8.$$.fragment),F0r=l(),Tm=a("p"),C0r=o(`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),g1e=a("code"),M0r=o("from_pretrained()"),E0r=o("class method or the "),h1e=a("code"),y0r=o("from_config()"),w0r=o(`class
method.`),A0r=l(),a8=a("p"),L0r=o("This class cannot be instantiated directly using "),u1e=a("code"),B0r=o("__init__()"),x0r=o(" (throws an error)."),k0r=l(),Rt=a("div"),m(s8.$$.fragment),R0r=l(),p1e=a("p"),S0r=o("Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),P0r=l(),Fm=a("p"),$0r=o(`Note:
Loading a model from its configuration file does `),_1e=a("strong"),I0r=o("not"),D0r=o(` load the model weights. It only affects the
model\u2019s configuration. Use `),b1e=a("code"),j0r=o("from_pretrained()"),N0r=o("to load the model weights."),q0r=l(),v1e=a("p"),G0r=o("Examples:"),O0r=l(),m(n8.$$.fragment),X0r=l(),Do=a("div"),m(l8.$$.fragment),V0r=l(),T1e=a("p"),z0r=o("Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),W0r=l(),Ps=a("p"),Q0r=o("The model class to instantiate is selected based on the "),F1e=a("code"),H0r=o("model_type"),U0r=o(` property of the config object (either
passed as an argument or loaded from `),C1e=a("code"),J0r=o("pretrained_model_name_or_path"),Y0r=o(` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),M1e=a("code"),K0r=o("pretrained_model_name_or_path"),Z0r=o(":"),eLr=l(),E1e=a("ul"),T3=a("li"),y1e=a("strong"),oLr=o("vision-encoder-decoder"),rLr=o(" \u2014 "),yV=a("a"),tLr=o("FlaxVisionEncoderDecoderModel"),aLr=o(" (Vision Encoder decoder model)"),sLr=l(),w1e=a("p"),nLr=o("Examples:"),lLr=l(),m(i8.$$.fragment),this.h()},l(c){const _=z2t('[data-svelte="svelte-1phssyn"]',document.head);J=s(_,"META",{name:!0,content:!0}),_.forEach(t),Be=i(c),ie=s(c,"H1",{class:!0});var d8=n(ie);fe=s(d8,"A",{id:!0,class:!0,href:!0});var A1e=n(fe);so=s(A1e,"SPAN",{});var L1e=n(so);f(ce.$$.fragment,L1e),L1e.forEach(t),A1e.forEach(t),_e=i(d8),Go=s(d8,"SPAN",{});var dLr=n(Go);Li=r(dLr,"Auto Classes"),dLr.forEach(t),d8.forEach(t),Mm=i(c),na=s(c,"P",{});var _xe=n(na);Bi=r(_xe,`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the `),xi=s(_xe,"CODE",{});var cLr=n(xi);T5=r(cLr,"from_pretrained()"),cLr.forEach(t),Em=r(_xe,` method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`),_xe.forEach(t),ye=i(c),io=s(c,"P",{});var F3=n(io);ki=r(F3,"Instantiating one of "),$s=s(F3,"A",{href:!0});var mLr=n($s);F5=r(mLr,"AutoConfig"),mLr.forEach(t),Is=r(F3,", "),Ds=s(F3,"A",{href:!0});var fLr=n(Ds);C5=r(fLr,"AutoModel"),fLr.forEach(t),Ri=r(F3,`, and
`),js=s(F3,"A",{href:!0});var gLr=n(js);M5=r(gLr,"AutoTokenizer"),gLr.forEach(t),Si=r(F3," will directly create a class of the relevant architecture. For instance"),F3.forEach(t),ym=i(c),f($a.$$.fragment,c),co=i(c),ge=s(c,"P",{});var bxe=n(ge);s7=r(bxe,"will create a model that is an instance of "),Pi=s(bxe,"A",{href:!0});var hLr=n(Pi);n7=r(hLr,"BertModel"),hLr.forEach(t),l7=r(bxe,"."),bxe.forEach(t),Oo=i(c),Ia=s(c,"P",{});var vxe=n(Ia);i7=r(vxe,"There is one class of "),wm=s(vxe,"CODE",{});var uLr=n(wm);d7=r(uLr,"AutoModel"),uLr.forEach(t),ARe=r(vxe," for each task, and for each backend (PyTorch, TensorFlow, or Flax)."),vxe.forEach(t),_9e=i(c),$i=s(c,"H2",{class:!0});var Txe=n($i);Am=s(Txe,"A",{id:!0,class:!0,href:!0});var pLr=n(Am);pW=s(pLr,"SPAN",{});var _Lr=n(pW);f(E5.$$.fragment,_Lr),_Lr.forEach(t),pLr.forEach(t),LRe=i(Txe),_W=s(Txe,"SPAN",{});var bLr=n(_W);BRe=r(bLr,"Extending the Auto Classes"),bLr.forEach(t),Txe.forEach(t),b9e=i(c),Ns=s(c,"P",{});var wV=n(Ns);xRe=r(wV,`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model `),bW=s(wV,"CODE",{});var vLr=n(bW);kRe=r(vLr,"NewModel"),vLr.forEach(t),RRe=r(wV,", make sure you have a "),vW=s(wV,"CODE",{});var TLr=n(vW);SRe=r(TLr,"NewModelConfig"),TLr.forEach(t),PRe=r(wV,` then you can add those to the auto
classes like this:`),wV.forEach(t),v9e=i(c),f(y5.$$.fragment,c),T9e=i(c),c7=s(c,"P",{});var FLr=n(c7);$Re=r(FLr,"You will then be able to use the auto classes like you would usually do!"),FLr.forEach(t),F9e=i(c),f(Lm.$$.fragment,c),C9e=i(c),Ii=s(c,"H2",{class:!0});var Fxe=n(Ii);Bm=s(Fxe,"A",{id:!0,class:!0,href:!0});var CLr=n(Bm);TW=s(CLr,"SPAN",{});var MLr=n(TW);f(w5.$$.fragment,MLr),MLr.forEach(t),CLr.forEach(t),IRe=i(Fxe),FW=s(Fxe,"SPAN",{});var ELr=n(FW);DRe=r(ELr,"AutoConfig"),ELr.forEach(t),Fxe.forEach(t),M9e=i(c),Xo=s(c,"DIV",{class:!0});var In=n(Xo);f(A5.$$.fragment,In),jRe=i(In),L5=s(In,"P",{});var Cxe=n(L5);NRe=r(Cxe,`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the `),m7=s(Cxe,"A",{href:!0});var yLr=n(m7);qRe=r(yLr,"from_pretrained()"),yLr.forEach(t),GRe=r(Cxe," class method."),Cxe.forEach(t),ORe=i(In),B5=s(In,"P",{});var Mxe=n(B5);XRe=r(Mxe,"This class cannot be instantiated directly using "),CW=s(Mxe,"CODE",{});var wLr=n(CW);VRe=r(wLr,"__init__()"),wLr.forEach(t),zRe=r(Mxe," (throws an error)."),Mxe.forEach(t),WRe=i(In),mo=s(In,"DIV",{class:!0});var ia=n(mo);f(x5.$$.fragment,ia),QRe=i(ia),MW=s(ia,"P",{});var ALr=n(MW);HRe=r(ALr,"Instantiate one of the configuration classes of the library from a pretrained model configuration."),ALr.forEach(t),URe=i(ia),Di=s(ia,"P",{});var AV=n(Di);JRe=r(AV,"The configuration class to instantiate is selected based on the "),EW=s(AV,"CODE",{});var LLr=n(EW);YRe=r(LLr,"model_type"),LLr.forEach(t),KRe=r(AV,` property of the config object that
is loaded, or when it\u2019s missing, by falling back to using pattern matching on `),yW=s(AV,"CODE",{});var BLr=n(yW);ZRe=r(BLr,"pretrained_model_name_or_path"),BLr.forEach(t),eSe=r(AV,":"),AV.forEach(t),oSe=i(ia),v=s(ia,"UL",{});var T=n(v);xm=s(T,"LI",{});var B1e=n(xm);wW=s(B1e,"STRONG",{});var xLr=n(wW);rSe=r(xLr,"albert"),xLr.forEach(t),tSe=r(B1e," \u2014 "),f7=s(B1e,"A",{href:!0});var kLr=n(f7);aSe=r(kLr,"AlbertConfig"),kLr.forEach(t),sSe=r(B1e," (ALBERT model)"),B1e.forEach(t),nSe=i(T),km=s(T,"LI",{});var x1e=n(km);AW=s(x1e,"STRONG",{});var RLr=n(AW);lSe=r(RLr,"bart"),RLr.forEach(t),iSe=r(x1e," \u2014 "),g7=s(x1e,"A",{href:!0});var SLr=n(g7);dSe=r(SLr,"BartConfig"),SLr.forEach(t),cSe=r(x1e," (BART model)"),x1e.forEach(t),mSe=i(T),Rm=s(T,"LI",{});var k1e=n(Rm);LW=s(k1e,"STRONG",{});var PLr=n(LW);fSe=r(PLr,"beit"),PLr.forEach(t),gSe=r(k1e," \u2014 "),h7=s(k1e,"A",{href:!0});var $Lr=n(h7);hSe=r($Lr,"BeitConfig"),$Lr.forEach(t),uSe=r(k1e," (BEiT model)"),k1e.forEach(t),pSe=i(T),Sm=s(T,"LI",{});var R1e=n(Sm);BW=s(R1e,"STRONG",{});var ILr=n(BW);_Se=r(ILr,"bert"),ILr.forEach(t),bSe=r(R1e," \u2014 "),u7=s(R1e,"A",{href:!0});var DLr=n(u7);vSe=r(DLr,"BertConfig"),DLr.forEach(t),TSe=r(R1e," (BERT model)"),R1e.forEach(t),FSe=i(T),Pm=s(T,"LI",{});var S1e=n(Pm);xW=s(S1e,"STRONG",{});var jLr=n(xW);CSe=r(jLr,"bert-generation"),jLr.forEach(t),MSe=r(S1e," \u2014 "),p7=s(S1e,"A",{href:!0});var NLr=n(p7);ESe=r(NLr,"BertGenerationConfig"),NLr.forEach(t),ySe=r(S1e," (Bert Generation model)"),S1e.forEach(t),wSe=i(T),$m=s(T,"LI",{});var P1e=n($m);kW=s(P1e,"STRONG",{});var qLr=n(kW);ASe=r(qLr,"big_bird"),qLr.forEach(t),LSe=r(P1e," \u2014 "),_7=s(P1e,"A",{href:!0});var GLr=n(_7);BSe=r(GLr,"BigBirdConfig"),GLr.forEach(t),xSe=r(P1e," (BigBird model)"),P1e.forEach(t),kSe=i(T),Im=s(T,"LI",{});var $1e=n(Im);RW=s($1e,"STRONG",{});var OLr=n(RW);RSe=r(OLr,"bigbird_pegasus"),OLr.forEach(t),SSe=r($1e," \u2014 "),b7=s($1e,"A",{href:!0});var XLr=n(b7);PSe=r(XLr,"BigBirdPegasusConfig"),XLr.forEach(t),$Se=r($1e," (BigBirdPegasus model)"),$1e.forEach(t),ISe=i(T),Dm=s(T,"LI",{});var I1e=n(Dm);SW=s(I1e,"STRONG",{});var VLr=n(SW);DSe=r(VLr,"blenderbot"),VLr.forEach(t),jSe=r(I1e," \u2014 "),v7=s(I1e,"A",{href:!0});var zLr=n(v7);NSe=r(zLr,"BlenderbotConfig"),zLr.forEach(t),qSe=r(I1e," (Blenderbot model)"),I1e.forEach(t),GSe=i(T),jm=s(T,"LI",{});var D1e=n(jm);PW=s(D1e,"STRONG",{});var WLr=n(PW);OSe=r(WLr,"blenderbot-small"),WLr.forEach(t),XSe=r(D1e," \u2014 "),T7=s(D1e,"A",{href:!0});var QLr=n(T7);VSe=r(QLr,"BlenderbotSmallConfig"),QLr.forEach(t),zSe=r(D1e," (BlenderbotSmall model)"),D1e.forEach(t),WSe=i(T),Nm=s(T,"LI",{});var j1e=n(Nm);$W=s(j1e,"STRONG",{});var HLr=n($W);QSe=r(HLr,"camembert"),HLr.forEach(t),HSe=r(j1e," \u2014 "),F7=s(j1e,"A",{href:!0});var ULr=n(F7);USe=r(ULr,"CamembertConfig"),ULr.forEach(t),JSe=r(j1e," (CamemBERT model)"),j1e.forEach(t),YSe=i(T),qm=s(T,"LI",{});var N1e=n(qm);IW=s(N1e,"STRONG",{});var JLr=n(IW);KSe=r(JLr,"canine"),JLr.forEach(t),ZSe=r(N1e," \u2014 "),C7=s(N1e,"A",{href:!0});var YLr=n(C7);ePe=r(YLr,"CanineConfig"),YLr.forEach(t),oPe=r(N1e," (Canine model)"),N1e.forEach(t),rPe=i(T),Gm=s(T,"LI",{});var q1e=n(Gm);DW=s(q1e,"STRONG",{});var KLr=n(DW);tPe=r(KLr,"clip"),KLr.forEach(t),aPe=r(q1e," \u2014 "),M7=s(q1e,"A",{href:!0});var ZLr=n(M7);sPe=r(ZLr,"CLIPConfig"),ZLr.forEach(t),nPe=r(q1e," (CLIP model)"),q1e.forEach(t),lPe=i(T),Om=s(T,"LI",{});var G1e=n(Om);jW=s(G1e,"STRONG",{});var e8r=n(jW);iPe=r(e8r,"convbert"),e8r.forEach(t),dPe=r(G1e," \u2014 "),E7=s(G1e,"A",{href:!0});var o8r=n(E7);cPe=r(o8r,"ConvBertConfig"),o8r.forEach(t),mPe=r(G1e," (ConvBERT model)"),G1e.forEach(t),fPe=i(T),Xm=s(T,"LI",{});var O1e=n(Xm);NW=s(O1e,"STRONG",{});var r8r=n(NW);gPe=r(r8r,"convnext"),r8r.forEach(t),hPe=r(O1e," \u2014 "),y7=s(O1e,"A",{href:!0});var t8r=n(y7);uPe=r(t8r,"ConvNextConfig"),t8r.forEach(t),pPe=r(O1e," (ConvNext model)"),O1e.forEach(t),_Pe=i(T),Vm=s(T,"LI",{});var X1e=n(Vm);qW=s(X1e,"STRONG",{});var a8r=n(qW);bPe=r(a8r,"ctrl"),a8r.forEach(t),vPe=r(X1e," \u2014 "),w7=s(X1e,"A",{href:!0});var s8r=n(w7);TPe=r(s8r,"CTRLConfig"),s8r.forEach(t),FPe=r(X1e," (CTRL model)"),X1e.forEach(t),CPe=i(T),zm=s(T,"LI",{});var V1e=n(zm);GW=s(V1e,"STRONG",{});var n8r=n(GW);MPe=r(n8r,"data2vec-audio"),n8r.forEach(t),EPe=r(V1e," \u2014 "),A7=s(V1e,"A",{href:!0});var l8r=n(A7);yPe=r(l8r,"Data2VecAudioConfig"),l8r.forEach(t),wPe=r(V1e," (Data2VecAudio model)"),V1e.forEach(t),APe=i(T),Wm=s(T,"LI",{});var z1e=n(Wm);OW=s(z1e,"STRONG",{});var i8r=n(OW);LPe=r(i8r,"data2vec-text"),i8r.forEach(t),BPe=r(z1e," \u2014 "),L7=s(z1e,"A",{href:!0});var d8r=n(L7);xPe=r(d8r,"Data2VecTextConfig"),d8r.forEach(t),kPe=r(z1e," (Data2VecText model)"),z1e.forEach(t),RPe=i(T),Qm=s(T,"LI",{});var W1e=n(Qm);XW=s(W1e,"STRONG",{});var c8r=n(XW);SPe=r(c8r,"deberta"),c8r.forEach(t),PPe=r(W1e," \u2014 "),B7=s(W1e,"A",{href:!0});var m8r=n(B7);$Pe=r(m8r,"DebertaConfig"),m8r.forEach(t),IPe=r(W1e," (DeBERTa model)"),W1e.forEach(t),DPe=i(T),Hm=s(T,"LI",{});var Q1e=n(Hm);VW=s(Q1e,"STRONG",{});var f8r=n(VW);jPe=r(f8r,"deberta-v2"),f8r.forEach(t),NPe=r(Q1e," \u2014 "),x7=s(Q1e,"A",{href:!0});var g8r=n(x7);qPe=r(g8r,"DebertaV2Config"),g8r.forEach(t),GPe=r(Q1e," (DeBERTa-v2 model)"),Q1e.forEach(t),OPe=i(T),Um=s(T,"LI",{});var H1e=n(Um);zW=s(H1e,"STRONG",{});var h8r=n(zW);XPe=r(h8r,"deit"),h8r.forEach(t),VPe=r(H1e," \u2014 "),k7=s(H1e,"A",{href:!0});var u8r=n(k7);zPe=r(u8r,"DeiTConfig"),u8r.forEach(t),WPe=r(H1e," (DeiT model)"),H1e.forEach(t),QPe=i(T),Jm=s(T,"LI",{});var U1e=n(Jm);WW=s(U1e,"STRONG",{});var p8r=n(WW);HPe=r(p8r,"detr"),p8r.forEach(t),UPe=r(U1e," \u2014 "),R7=s(U1e,"A",{href:!0});var _8r=n(R7);JPe=r(_8r,"DetrConfig"),_8r.forEach(t),YPe=r(U1e," (DETR model)"),U1e.forEach(t),KPe=i(T),Ym=s(T,"LI",{});var J1e=n(Ym);QW=s(J1e,"STRONG",{});var b8r=n(QW);ZPe=r(b8r,"distilbert"),b8r.forEach(t),e$e=r(J1e," \u2014 "),S7=s(J1e,"A",{href:!0});var v8r=n(S7);o$e=r(v8r,"DistilBertConfig"),v8r.forEach(t),r$e=r(J1e," (DistilBERT model)"),J1e.forEach(t),t$e=i(T),Km=s(T,"LI",{});var Y1e=n(Km);HW=s(Y1e,"STRONG",{});var T8r=n(HW);a$e=r(T8r,"dpr"),T8r.forEach(t),s$e=r(Y1e," \u2014 "),P7=s(Y1e,"A",{href:!0});var F8r=n(P7);n$e=r(F8r,"DPRConfig"),F8r.forEach(t),l$e=r(Y1e," (DPR model)"),Y1e.forEach(t),i$e=i(T),Zm=s(T,"LI",{});var K1e=n(Zm);UW=s(K1e,"STRONG",{});var C8r=n(UW);d$e=r(C8r,"electra"),C8r.forEach(t),c$e=r(K1e," \u2014 "),$7=s(K1e,"A",{href:!0});var M8r=n($7);m$e=r(M8r,"ElectraConfig"),M8r.forEach(t),f$e=r(K1e," (ELECTRA model)"),K1e.forEach(t),g$e=i(T),ef=s(T,"LI",{});var Z1e=n(ef);JW=s(Z1e,"STRONG",{});var E8r=n(JW);h$e=r(E8r,"encoder-decoder"),E8r.forEach(t),u$e=r(Z1e," \u2014 "),I7=s(Z1e,"A",{href:!0});var y8r=n(I7);p$e=r(y8r,"EncoderDecoderConfig"),y8r.forEach(t),_$e=r(Z1e," (Encoder decoder model)"),Z1e.forEach(t),b$e=i(T),of=s(T,"LI",{});var eFe=n(of);YW=s(eFe,"STRONG",{});var w8r=n(YW);v$e=r(w8r,"flaubert"),w8r.forEach(t),T$e=r(eFe," \u2014 "),D7=s(eFe,"A",{href:!0});var A8r=n(D7);F$e=r(A8r,"FlaubertConfig"),A8r.forEach(t),C$e=r(eFe," (FlauBERT model)"),eFe.forEach(t),M$e=i(T),rf=s(T,"LI",{});var oFe=n(rf);KW=s(oFe,"STRONG",{});var L8r=n(KW);E$e=r(L8r,"fnet"),L8r.forEach(t),y$e=r(oFe," \u2014 "),j7=s(oFe,"A",{href:!0});var B8r=n(j7);w$e=r(B8r,"FNetConfig"),B8r.forEach(t),A$e=r(oFe," (FNet model)"),oFe.forEach(t),L$e=i(T),tf=s(T,"LI",{});var rFe=n(tf);ZW=s(rFe,"STRONG",{});var x8r=n(ZW);B$e=r(x8r,"fsmt"),x8r.forEach(t),x$e=r(rFe," \u2014 "),N7=s(rFe,"A",{href:!0});var k8r=n(N7);k$e=r(k8r,"FSMTConfig"),k8r.forEach(t),R$e=r(rFe," (FairSeq Machine-Translation model)"),rFe.forEach(t),S$e=i(T),af=s(T,"LI",{});var tFe=n(af);eQ=s(tFe,"STRONG",{});var R8r=n(eQ);P$e=r(R8r,"funnel"),R8r.forEach(t),$$e=r(tFe," \u2014 "),q7=s(tFe,"A",{href:!0});var S8r=n(q7);I$e=r(S8r,"FunnelConfig"),S8r.forEach(t),D$e=r(tFe," (Funnel Transformer model)"),tFe.forEach(t),j$e=i(T),sf=s(T,"LI",{});var aFe=n(sf);oQ=s(aFe,"STRONG",{});var P8r=n(oQ);N$e=r(P8r,"gpt2"),P8r.forEach(t),q$e=r(aFe," \u2014 "),G7=s(aFe,"A",{href:!0});var $8r=n(G7);G$e=r($8r,"GPT2Config"),$8r.forEach(t),O$e=r(aFe," (OpenAI GPT-2 model)"),aFe.forEach(t),X$e=i(T),nf=s(T,"LI",{});var sFe=n(nf);rQ=s(sFe,"STRONG",{});var I8r=n(rQ);V$e=r(I8r,"gpt_neo"),I8r.forEach(t),z$e=r(sFe," \u2014 "),O7=s(sFe,"A",{href:!0});var D8r=n(O7);W$e=r(D8r,"GPTNeoConfig"),D8r.forEach(t),Q$e=r(sFe," (GPT Neo model)"),sFe.forEach(t),H$e=i(T),lf=s(T,"LI",{});var nFe=n(lf);tQ=s(nFe,"STRONG",{});var j8r=n(tQ);U$e=r(j8r,"gptj"),j8r.forEach(t),J$e=r(nFe," \u2014 "),X7=s(nFe,"A",{href:!0});var N8r=n(X7);Y$e=r(N8r,"GPTJConfig"),N8r.forEach(t),K$e=r(nFe," (GPT-J model)"),nFe.forEach(t),Z$e=i(T),df=s(T,"LI",{});var lFe=n(df);aQ=s(lFe,"STRONG",{});var q8r=n(aQ);eIe=r(q8r,"hubert"),q8r.forEach(t),oIe=r(lFe," \u2014 "),V7=s(lFe,"A",{href:!0});var G8r=n(V7);rIe=r(G8r,"HubertConfig"),G8r.forEach(t),tIe=r(lFe," (Hubert model)"),lFe.forEach(t),aIe=i(T),cf=s(T,"LI",{});var iFe=n(cf);sQ=s(iFe,"STRONG",{});var O8r=n(sQ);sIe=r(O8r,"ibert"),O8r.forEach(t),nIe=r(iFe," \u2014 "),z7=s(iFe,"A",{href:!0});var X8r=n(z7);lIe=r(X8r,"IBertConfig"),X8r.forEach(t),iIe=r(iFe," (I-BERT model)"),iFe.forEach(t),dIe=i(T),mf=s(T,"LI",{});var dFe=n(mf);nQ=s(dFe,"STRONG",{});var V8r=n(nQ);cIe=r(V8r,"imagegpt"),V8r.forEach(t),mIe=r(dFe," \u2014 "),W7=s(dFe,"A",{href:!0});var z8r=n(W7);fIe=r(z8r,"ImageGPTConfig"),z8r.forEach(t),gIe=r(dFe," (ImageGPT model)"),dFe.forEach(t),hIe=i(T),ff=s(T,"LI",{});var cFe=n(ff);lQ=s(cFe,"STRONG",{});var W8r=n(lQ);uIe=r(W8r,"layoutlm"),W8r.forEach(t),pIe=r(cFe," \u2014 "),Q7=s(cFe,"A",{href:!0});var Q8r=n(Q7);_Ie=r(Q8r,"LayoutLMConfig"),Q8r.forEach(t),bIe=r(cFe," (LayoutLM model)"),cFe.forEach(t),vIe=i(T),gf=s(T,"LI",{});var mFe=n(gf);iQ=s(mFe,"STRONG",{});var H8r=n(iQ);TIe=r(H8r,"layoutlmv2"),H8r.forEach(t),FIe=r(mFe," \u2014 "),H7=s(mFe,"A",{href:!0});var U8r=n(H7);CIe=r(U8r,"LayoutLMv2Config"),U8r.forEach(t),MIe=r(mFe," (LayoutLMv2 model)"),mFe.forEach(t),EIe=i(T),hf=s(T,"LI",{});var fFe=n(hf);dQ=s(fFe,"STRONG",{});var J8r=n(dQ);yIe=r(J8r,"led"),J8r.forEach(t),wIe=r(fFe," \u2014 "),U7=s(fFe,"A",{href:!0});var Y8r=n(U7);AIe=r(Y8r,"LEDConfig"),Y8r.forEach(t),LIe=r(fFe," (LED model)"),fFe.forEach(t),BIe=i(T),uf=s(T,"LI",{});var gFe=n(uf);cQ=s(gFe,"STRONG",{});var K8r=n(cQ);xIe=r(K8r,"longformer"),K8r.forEach(t),kIe=r(gFe," \u2014 "),J7=s(gFe,"A",{href:!0});var Z8r=n(J7);RIe=r(Z8r,"LongformerConfig"),Z8r.forEach(t),SIe=r(gFe," (Longformer model)"),gFe.forEach(t),PIe=i(T),pf=s(T,"LI",{});var hFe=n(pf);mQ=s(hFe,"STRONG",{});var e7r=n(mQ);$Ie=r(e7r,"luke"),e7r.forEach(t),IIe=r(hFe," \u2014 "),Y7=s(hFe,"A",{href:!0});var o7r=n(Y7);DIe=r(o7r,"LukeConfig"),o7r.forEach(t),jIe=r(hFe," (LUKE model)"),hFe.forEach(t),NIe=i(T),_f=s(T,"LI",{});var uFe=n(_f);fQ=s(uFe,"STRONG",{});var r7r=n(fQ);qIe=r(r7r,"lxmert"),r7r.forEach(t),GIe=r(uFe," \u2014 "),K7=s(uFe,"A",{href:!0});var t7r=n(K7);OIe=r(t7r,"LxmertConfig"),t7r.forEach(t),XIe=r(uFe," (LXMERT model)"),uFe.forEach(t),VIe=i(T),bf=s(T,"LI",{});var pFe=n(bf);gQ=s(pFe,"STRONG",{});var a7r=n(gQ);zIe=r(a7r,"m2m_100"),a7r.forEach(t),WIe=r(pFe," \u2014 "),Z7=s(pFe,"A",{href:!0});var s7r=n(Z7);QIe=r(s7r,"M2M100Config"),s7r.forEach(t),HIe=r(pFe," (M2M100 model)"),pFe.forEach(t),UIe=i(T),vf=s(T,"LI",{});var _Fe=n(vf);hQ=s(_Fe,"STRONG",{});var n7r=n(hQ);JIe=r(n7r,"marian"),n7r.forEach(t),YIe=r(_Fe," \u2014 "),e9=s(_Fe,"A",{href:!0});var l7r=n(e9);KIe=r(l7r,"MarianConfig"),l7r.forEach(t),ZIe=r(_Fe," (Marian model)"),_Fe.forEach(t),eDe=i(T),Tf=s(T,"LI",{});var bFe=n(Tf);uQ=s(bFe,"STRONG",{});var i7r=n(uQ);oDe=r(i7r,"mbart"),i7r.forEach(t),rDe=r(bFe," \u2014 "),o9=s(bFe,"A",{href:!0});var d7r=n(o9);tDe=r(d7r,"MBartConfig"),d7r.forEach(t),aDe=r(bFe," (mBART model)"),bFe.forEach(t),sDe=i(T),Ff=s(T,"LI",{});var vFe=n(Ff);pQ=s(vFe,"STRONG",{});var c7r=n(pQ);nDe=r(c7r,"megatron-bert"),c7r.forEach(t),lDe=r(vFe," \u2014 "),r9=s(vFe,"A",{href:!0});var m7r=n(r9);iDe=r(m7r,"MegatronBertConfig"),m7r.forEach(t),dDe=r(vFe," (MegatronBert model)"),vFe.forEach(t),cDe=i(T),Cf=s(T,"LI",{});var TFe=n(Cf);_Q=s(TFe,"STRONG",{});var f7r=n(_Q);mDe=r(f7r,"mobilebert"),f7r.forEach(t),fDe=r(TFe," \u2014 "),t9=s(TFe,"A",{href:!0});var g7r=n(t9);gDe=r(g7r,"MobileBertConfig"),g7r.forEach(t),hDe=r(TFe," (MobileBERT model)"),TFe.forEach(t),uDe=i(T),Mf=s(T,"LI",{});var FFe=n(Mf);bQ=s(FFe,"STRONG",{});var h7r=n(bQ);pDe=r(h7r,"mpnet"),h7r.forEach(t),_De=r(FFe," \u2014 "),a9=s(FFe,"A",{href:!0});var u7r=n(a9);bDe=r(u7r,"MPNetConfig"),u7r.forEach(t),vDe=r(FFe," (MPNet model)"),FFe.forEach(t),TDe=i(T),Ef=s(T,"LI",{});var CFe=n(Ef);vQ=s(CFe,"STRONG",{});var p7r=n(vQ);FDe=r(p7r,"mt5"),p7r.forEach(t),CDe=r(CFe," \u2014 "),s9=s(CFe,"A",{href:!0});var _7r=n(s9);MDe=r(_7r,"MT5Config"),_7r.forEach(t),EDe=r(CFe," (mT5 model)"),CFe.forEach(t),yDe=i(T),yf=s(T,"LI",{});var MFe=n(yf);TQ=s(MFe,"STRONG",{});var b7r=n(TQ);wDe=r(b7r,"nystromformer"),b7r.forEach(t),ADe=r(MFe," \u2014 "),n9=s(MFe,"A",{href:!0});var v7r=n(n9);LDe=r(v7r,"NystromformerConfig"),v7r.forEach(t),BDe=r(MFe," (Nystromformer model)"),MFe.forEach(t),xDe=i(T),wf=s(T,"LI",{});var EFe=n(wf);FQ=s(EFe,"STRONG",{});var T7r=n(FQ);kDe=r(T7r,"openai-gpt"),T7r.forEach(t),RDe=r(EFe," \u2014 "),l9=s(EFe,"A",{href:!0});var F7r=n(l9);SDe=r(F7r,"OpenAIGPTConfig"),F7r.forEach(t),PDe=r(EFe," (OpenAI GPT model)"),EFe.forEach(t),$De=i(T),Af=s(T,"LI",{});var yFe=n(Af);CQ=s(yFe,"STRONG",{});var C7r=n(CQ);IDe=r(C7r,"pegasus"),C7r.forEach(t),DDe=r(yFe," \u2014 "),i9=s(yFe,"A",{href:!0});var M7r=n(i9);jDe=r(M7r,"PegasusConfig"),M7r.forEach(t),NDe=r(yFe," (Pegasus model)"),yFe.forEach(t),qDe=i(T),Lf=s(T,"LI",{});var wFe=n(Lf);MQ=s(wFe,"STRONG",{});var E7r=n(MQ);GDe=r(E7r,"perceiver"),E7r.forEach(t),ODe=r(wFe," \u2014 "),d9=s(wFe,"A",{href:!0});var y7r=n(d9);XDe=r(y7r,"PerceiverConfig"),y7r.forEach(t),VDe=r(wFe," (Perceiver model)"),wFe.forEach(t),zDe=i(T),Bf=s(T,"LI",{});var AFe=n(Bf);EQ=s(AFe,"STRONG",{});var w7r=n(EQ);WDe=r(w7r,"plbart"),w7r.forEach(t),QDe=r(AFe," \u2014 "),c9=s(AFe,"A",{href:!0});var A7r=n(c9);HDe=r(A7r,"PLBartConfig"),A7r.forEach(t),UDe=r(AFe," (PLBart model)"),AFe.forEach(t),JDe=i(T),xf=s(T,"LI",{});var LFe=n(xf);yQ=s(LFe,"STRONG",{});var L7r=n(yQ);YDe=r(L7r,"poolformer"),L7r.forEach(t),KDe=r(LFe," \u2014 "),m9=s(LFe,"A",{href:!0});var B7r=n(m9);ZDe=r(B7r,"PoolFormerConfig"),B7r.forEach(t),eje=r(LFe," (PoolFormer model)"),LFe.forEach(t),oje=i(T),kf=s(T,"LI",{});var BFe=n(kf);wQ=s(BFe,"STRONG",{});var x7r=n(wQ);rje=r(x7r,"prophetnet"),x7r.forEach(t),tje=r(BFe," \u2014 "),f9=s(BFe,"A",{href:!0});var k7r=n(f9);aje=r(k7r,"ProphetNetConfig"),k7r.forEach(t),sje=r(BFe," (ProphetNet model)"),BFe.forEach(t),nje=i(T),Rf=s(T,"LI",{});var xFe=n(Rf);AQ=s(xFe,"STRONG",{});var R7r=n(AQ);lje=r(R7r,"qdqbert"),R7r.forEach(t),ije=r(xFe," \u2014 "),g9=s(xFe,"A",{href:!0});var S7r=n(g9);dje=r(S7r,"QDQBertConfig"),S7r.forEach(t),cje=r(xFe," (QDQBert model)"),xFe.forEach(t),mje=i(T),Sf=s(T,"LI",{});var kFe=n(Sf);LQ=s(kFe,"STRONG",{});var P7r=n(LQ);fje=r(P7r,"rag"),P7r.forEach(t),gje=r(kFe," \u2014 "),h9=s(kFe,"A",{href:!0});var $7r=n(h9);hje=r($7r,"RagConfig"),$7r.forEach(t),uje=r(kFe," (RAG model)"),kFe.forEach(t),pje=i(T),Pf=s(T,"LI",{});var RFe=n(Pf);BQ=s(RFe,"STRONG",{});var I7r=n(BQ);_je=r(I7r,"realm"),I7r.forEach(t),bje=r(RFe," \u2014 "),u9=s(RFe,"A",{href:!0});var D7r=n(u9);vje=r(D7r,"RealmConfig"),D7r.forEach(t),Tje=r(RFe," (Realm model)"),RFe.forEach(t),Fje=i(T),$f=s(T,"LI",{});var SFe=n($f);xQ=s(SFe,"STRONG",{});var j7r=n(xQ);Cje=r(j7r,"reformer"),j7r.forEach(t),Mje=r(SFe," \u2014 "),p9=s(SFe,"A",{href:!0});var N7r=n(p9);Eje=r(N7r,"ReformerConfig"),N7r.forEach(t),yje=r(SFe," (Reformer model)"),SFe.forEach(t),wje=i(T),If=s(T,"LI",{});var PFe=n(If);kQ=s(PFe,"STRONG",{});var q7r=n(kQ);Aje=r(q7r,"rembert"),q7r.forEach(t),Lje=r(PFe," \u2014 "),_9=s(PFe,"A",{href:!0});var G7r=n(_9);Bje=r(G7r,"RemBertConfig"),G7r.forEach(t),xje=r(PFe," (RemBERT model)"),PFe.forEach(t),kje=i(T),Df=s(T,"LI",{});var $Fe=n(Df);RQ=s($Fe,"STRONG",{});var O7r=n(RQ);Rje=r(O7r,"retribert"),O7r.forEach(t),Sje=r($Fe," \u2014 "),b9=s($Fe,"A",{href:!0});var X7r=n(b9);Pje=r(X7r,"RetriBertConfig"),X7r.forEach(t),$je=r($Fe," (RetriBERT model)"),$Fe.forEach(t),Ije=i(T),jf=s(T,"LI",{});var IFe=n(jf);SQ=s(IFe,"STRONG",{});var V7r=n(SQ);Dje=r(V7r,"roberta"),V7r.forEach(t),jje=r(IFe," \u2014 "),v9=s(IFe,"A",{href:!0});var z7r=n(v9);Nje=r(z7r,"RobertaConfig"),z7r.forEach(t),qje=r(IFe," (RoBERTa model)"),IFe.forEach(t),Gje=i(T),Nf=s(T,"LI",{});var DFe=n(Nf);PQ=s(DFe,"STRONG",{});var W7r=n(PQ);Oje=r(W7r,"roformer"),W7r.forEach(t),Xje=r(DFe," \u2014 "),T9=s(DFe,"A",{href:!0});var Q7r=n(T9);Vje=r(Q7r,"RoFormerConfig"),Q7r.forEach(t),zje=r(DFe," (RoFormer model)"),DFe.forEach(t),Wje=i(T),qf=s(T,"LI",{});var jFe=n(qf);$Q=s(jFe,"STRONG",{});var H7r=n($Q);Qje=r(H7r,"segformer"),H7r.forEach(t),Hje=r(jFe," \u2014 "),F9=s(jFe,"A",{href:!0});var U7r=n(F9);Uje=r(U7r,"SegformerConfig"),U7r.forEach(t),Jje=r(jFe," (SegFormer model)"),jFe.forEach(t),Yje=i(T),Gf=s(T,"LI",{});var NFe=n(Gf);IQ=s(NFe,"STRONG",{});var J7r=n(IQ);Kje=r(J7r,"sew"),J7r.forEach(t),Zje=r(NFe," \u2014 "),C9=s(NFe,"A",{href:!0});var Y7r=n(C9);eNe=r(Y7r,"SEWConfig"),Y7r.forEach(t),oNe=r(NFe," (SEW model)"),NFe.forEach(t),rNe=i(T),Of=s(T,"LI",{});var qFe=n(Of);DQ=s(qFe,"STRONG",{});var K7r=n(DQ);tNe=r(K7r,"sew-d"),K7r.forEach(t),aNe=r(qFe," \u2014 "),M9=s(qFe,"A",{href:!0});var Z7r=n(M9);sNe=r(Z7r,"SEWDConfig"),Z7r.forEach(t),nNe=r(qFe," (SEW-D model)"),qFe.forEach(t),lNe=i(T),Xf=s(T,"LI",{});var GFe=n(Xf);jQ=s(GFe,"STRONG",{});var e9r=n(jQ);iNe=r(e9r,"speech-encoder-decoder"),e9r.forEach(t),dNe=r(GFe," \u2014 "),E9=s(GFe,"A",{href:!0});var o9r=n(E9);cNe=r(o9r,"SpeechEncoderDecoderConfig"),o9r.forEach(t),mNe=r(GFe," (Speech Encoder decoder model)"),GFe.forEach(t),fNe=i(T),Vf=s(T,"LI",{});var OFe=n(Vf);NQ=s(OFe,"STRONG",{});var r9r=n(NQ);gNe=r(r9r,"speech_to_text"),r9r.forEach(t),hNe=r(OFe," \u2014 "),y9=s(OFe,"A",{href:!0});var t9r=n(y9);uNe=r(t9r,"Speech2TextConfig"),t9r.forEach(t),pNe=r(OFe," (Speech2Text model)"),OFe.forEach(t),_Ne=i(T),zf=s(T,"LI",{});var XFe=n(zf);qQ=s(XFe,"STRONG",{});var a9r=n(qQ);bNe=r(a9r,"speech_to_text_2"),a9r.forEach(t),vNe=r(XFe," \u2014 "),w9=s(XFe,"A",{href:!0});var s9r=n(w9);TNe=r(s9r,"Speech2Text2Config"),s9r.forEach(t),FNe=r(XFe," (Speech2Text2 model)"),XFe.forEach(t),CNe=i(T),Wf=s(T,"LI",{});var VFe=n(Wf);GQ=s(VFe,"STRONG",{});var n9r=n(GQ);MNe=r(n9r,"splinter"),n9r.forEach(t),ENe=r(VFe," \u2014 "),A9=s(VFe,"A",{href:!0});var l9r=n(A9);yNe=r(l9r,"SplinterConfig"),l9r.forEach(t),wNe=r(VFe," (Splinter model)"),VFe.forEach(t),ANe=i(T),Qf=s(T,"LI",{});var zFe=n(Qf);OQ=s(zFe,"STRONG",{});var i9r=n(OQ);LNe=r(i9r,"squeezebert"),i9r.forEach(t),BNe=r(zFe," \u2014 "),L9=s(zFe,"A",{href:!0});var d9r=n(L9);xNe=r(d9r,"SqueezeBertConfig"),d9r.forEach(t),kNe=r(zFe," (SqueezeBERT model)"),zFe.forEach(t),RNe=i(T),Hf=s(T,"LI",{});var WFe=n(Hf);XQ=s(WFe,"STRONG",{});var c9r=n(XQ);SNe=r(c9r,"swin"),c9r.forEach(t),PNe=r(WFe," \u2014 "),B9=s(WFe,"A",{href:!0});var m9r=n(B9);$Ne=r(m9r,"SwinConfig"),m9r.forEach(t),INe=r(WFe," (Swin model)"),WFe.forEach(t),DNe=i(T),Uf=s(T,"LI",{});var QFe=n(Uf);VQ=s(QFe,"STRONG",{});var f9r=n(VQ);jNe=r(f9r,"t5"),f9r.forEach(t),NNe=r(QFe," \u2014 "),x9=s(QFe,"A",{href:!0});var g9r=n(x9);qNe=r(g9r,"T5Config"),g9r.forEach(t),GNe=r(QFe," (T5 model)"),QFe.forEach(t),ONe=i(T),Jf=s(T,"LI",{});var HFe=n(Jf);zQ=s(HFe,"STRONG",{});var h9r=n(zQ);XNe=r(h9r,"tapas"),h9r.forEach(t),VNe=r(HFe," \u2014 "),k9=s(HFe,"A",{href:!0});var u9r=n(k9);zNe=r(u9r,"TapasConfig"),u9r.forEach(t),WNe=r(HFe," (TAPAS model)"),HFe.forEach(t),QNe=i(T),Yf=s(T,"LI",{});var UFe=n(Yf);WQ=s(UFe,"STRONG",{});var p9r=n(WQ);HNe=r(p9r,"transfo-xl"),p9r.forEach(t),UNe=r(UFe," \u2014 "),R9=s(UFe,"A",{href:!0});var _9r=n(R9);JNe=r(_9r,"TransfoXLConfig"),_9r.forEach(t),YNe=r(UFe," (Transformer-XL model)"),UFe.forEach(t),KNe=i(T),Kf=s(T,"LI",{});var JFe=n(Kf);QQ=s(JFe,"STRONG",{});var b9r=n(QQ);ZNe=r(b9r,"trocr"),b9r.forEach(t),eqe=r(JFe," \u2014 "),S9=s(JFe,"A",{href:!0});var v9r=n(S9);oqe=r(v9r,"TrOCRConfig"),v9r.forEach(t),rqe=r(JFe," (TrOCR model)"),JFe.forEach(t),tqe=i(T),Zf=s(T,"LI",{});var YFe=n(Zf);HQ=s(YFe,"STRONG",{});var T9r=n(HQ);aqe=r(T9r,"unispeech"),T9r.forEach(t),sqe=r(YFe," \u2014 "),P9=s(YFe,"A",{href:!0});var F9r=n(P9);nqe=r(F9r,"UniSpeechConfig"),F9r.forEach(t),lqe=r(YFe," (UniSpeech model)"),YFe.forEach(t),iqe=i(T),eg=s(T,"LI",{});var KFe=n(eg);UQ=s(KFe,"STRONG",{});var C9r=n(UQ);dqe=r(C9r,"unispeech-sat"),C9r.forEach(t),cqe=r(KFe," \u2014 "),$9=s(KFe,"A",{href:!0});var M9r=n($9);mqe=r(M9r,"UniSpeechSatConfig"),M9r.forEach(t),fqe=r(KFe," (UniSpeechSat model)"),KFe.forEach(t),gqe=i(T),og=s(T,"LI",{});var ZFe=n(og);JQ=s(ZFe,"STRONG",{});var E9r=n(JQ);hqe=r(E9r,"vilt"),E9r.forEach(t),uqe=r(ZFe," \u2014 "),I9=s(ZFe,"A",{href:!0});var y9r=n(I9);pqe=r(y9r,"ViltConfig"),y9r.forEach(t),_qe=r(ZFe," (ViLT model)"),ZFe.forEach(t),bqe=i(T),rg=s(T,"LI",{});var eCe=n(rg);YQ=s(eCe,"STRONG",{});var w9r=n(YQ);vqe=r(w9r,"vision-encoder-decoder"),w9r.forEach(t),Tqe=r(eCe," \u2014 "),D9=s(eCe,"A",{href:!0});var A9r=n(D9);Fqe=r(A9r,"VisionEncoderDecoderConfig"),A9r.forEach(t),Cqe=r(eCe," (Vision Encoder decoder model)"),eCe.forEach(t),Mqe=i(T),tg=s(T,"LI",{});var oCe=n(tg);KQ=s(oCe,"STRONG",{});var L9r=n(KQ);Eqe=r(L9r,"vision-text-dual-encoder"),L9r.forEach(t),yqe=r(oCe," \u2014 "),j9=s(oCe,"A",{href:!0});var B9r=n(j9);wqe=r(B9r,"VisionTextDualEncoderConfig"),B9r.forEach(t),Aqe=r(oCe," (VisionTextDualEncoder model)"),oCe.forEach(t),Lqe=i(T),ag=s(T,"LI",{});var rCe=n(ag);ZQ=s(rCe,"STRONG",{});var x9r=n(ZQ);Bqe=r(x9r,"visual_bert"),x9r.forEach(t),xqe=r(rCe," \u2014 "),N9=s(rCe,"A",{href:!0});var k9r=n(N9);kqe=r(k9r,"VisualBertConfig"),k9r.forEach(t),Rqe=r(rCe," (VisualBert model)"),rCe.forEach(t),Sqe=i(T),sg=s(T,"LI",{});var tCe=n(sg);eH=s(tCe,"STRONG",{});var R9r=n(eH);Pqe=r(R9r,"vit"),R9r.forEach(t),$qe=r(tCe," \u2014 "),q9=s(tCe,"A",{href:!0});var S9r=n(q9);Iqe=r(S9r,"ViTConfig"),S9r.forEach(t),Dqe=r(tCe," (ViT model)"),tCe.forEach(t),jqe=i(T),ng=s(T,"LI",{});var aCe=n(ng);oH=s(aCe,"STRONG",{});var P9r=n(oH);Nqe=r(P9r,"vit_mae"),P9r.forEach(t),qqe=r(aCe," \u2014 "),G9=s(aCe,"A",{href:!0});var $9r=n(G9);Gqe=r($9r,"ViTMAEConfig"),$9r.forEach(t),Oqe=r(aCe," (ViTMAE model)"),aCe.forEach(t),Xqe=i(T),lg=s(T,"LI",{});var sCe=n(lg);rH=s(sCe,"STRONG",{});var I9r=n(rH);Vqe=r(I9r,"wav2vec2"),I9r.forEach(t),zqe=r(sCe," \u2014 "),O9=s(sCe,"A",{href:!0});var D9r=n(O9);Wqe=r(D9r,"Wav2Vec2Config"),D9r.forEach(t),Qqe=r(sCe," (Wav2Vec2 model)"),sCe.forEach(t),Hqe=i(T),ig=s(T,"LI",{});var nCe=n(ig);tH=s(nCe,"STRONG",{});var j9r=n(tH);Uqe=r(j9r,"wavlm"),j9r.forEach(t),Jqe=r(nCe," \u2014 "),X9=s(nCe,"A",{href:!0});var N9r=n(X9);Yqe=r(N9r,"WavLMConfig"),N9r.forEach(t),Kqe=r(nCe," (WavLM model)"),nCe.forEach(t),Zqe=i(T),dg=s(T,"LI",{});var lCe=n(dg);aH=s(lCe,"STRONG",{});var q9r=n(aH);eGe=r(q9r,"xglm"),q9r.forEach(t),oGe=r(lCe," \u2014 "),V9=s(lCe,"A",{href:!0});var G9r=n(V9);rGe=r(G9r,"XGLMConfig"),G9r.forEach(t),tGe=r(lCe," (XGLM model)"),lCe.forEach(t),aGe=i(T),cg=s(T,"LI",{});var iCe=n(cg);sH=s(iCe,"STRONG",{});var O9r=n(sH);sGe=r(O9r,"xlm"),O9r.forEach(t),nGe=r(iCe," \u2014 "),z9=s(iCe,"A",{href:!0});var X9r=n(z9);lGe=r(X9r,"XLMConfig"),X9r.forEach(t),iGe=r(iCe," (XLM model)"),iCe.forEach(t),dGe=i(T),mg=s(T,"LI",{});var dCe=n(mg);nH=s(dCe,"STRONG",{});var V9r=n(nH);cGe=r(V9r,"xlm-prophetnet"),V9r.forEach(t),mGe=r(dCe," \u2014 "),W9=s(dCe,"A",{href:!0});var z9r=n(W9);fGe=r(z9r,"XLMProphetNetConfig"),z9r.forEach(t),gGe=r(dCe," (XLMProphetNet model)"),dCe.forEach(t),hGe=i(T),fg=s(T,"LI",{});var cCe=n(fg);lH=s(cCe,"STRONG",{});var W9r=n(lH);uGe=r(W9r,"xlm-roberta"),W9r.forEach(t),pGe=r(cCe," \u2014 "),Q9=s(cCe,"A",{href:!0});var Q9r=n(Q9);_Ge=r(Q9r,"XLMRobertaConfig"),Q9r.forEach(t),bGe=r(cCe," (XLM-RoBERTa model)"),cCe.forEach(t),vGe=i(T),gg=s(T,"LI",{});var mCe=n(gg);iH=s(mCe,"STRONG",{});var H9r=n(iH);TGe=r(H9r,"xlm-roberta-xl"),H9r.forEach(t),FGe=r(mCe," \u2014 "),H9=s(mCe,"A",{href:!0});var U9r=n(H9);CGe=r(U9r,"XLMRobertaXLConfig"),U9r.forEach(t),MGe=r(mCe," (XLM-RoBERTa-XL model)"),mCe.forEach(t),EGe=i(T),hg=s(T,"LI",{});var fCe=n(hg);dH=s(fCe,"STRONG",{});var J9r=n(dH);yGe=r(J9r,"xlnet"),J9r.forEach(t),wGe=r(fCe," \u2014 "),U9=s(fCe,"A",{href:!0});var Y9r=n(U9);AGe=r(Y9r,"XLNetConfig"),Y9r.forEach(t),LGe=r(fCe," (XLNet model)"),fCe.forEach(t),BGe=i(T),ug=s(T,"LI",{});var gCe=n(ug);cH=s(gCe,"STRONG",{});var K9r=n(cH);xGe=r(K9r,"yoso"),K9r.forEach(t),kGe=r(gCe," \u2014 "),J9=s(gCe,"A",{href:!0});var Z9r=n(J9);RGe=r(Z9r,"YosoConfig"),Z9r.forEach(t),SGe=r(gCe," (YOSO model)"),gCe.forEach(t),T.forEach(t),PGe=i(ia),mH=s(ia,"P",{});var eBr=n(mH);$Ge=r(eBr,"Examples:"),eBr.forEach(t),IGe=i(ia),f(k5.$$.fragment,ia),ia.forEach(t),DGe=i(In),pg=s(In,"DIV",{class:!0});var Exe=n(pg);f(R5.$$.fragment,Exe),jGe=i(Exe),fH=s(Exe,"P",{});var oBr=n(fH);NGe=r(oBr,"Register a new configuration for this class."),oBr.forEach(t),Exe.forEach(t),In.forEach(t),E9e=i(c),ji=s(c,"H2",{class:!0});var yxe=n(ji);_g=s(yxe,"A",{id:!0,class:!0,href:!0});var rBr=n(_g);gH=s(rBr,"SPAN",{});var tBr=n(gH);f(S5.$$.fragment,tBr),tBr.forEach(t),rBr.forEach(t),qGe=i(yxe),hH=s(yxe,"SPAN",{});var aBr=n(hH);GGe=r(aBr,"AutoTokenizer"),aBr.forEach(t),yxe.forEach(t),y9e=i(c),Vo=s(c,"DIV",{class:!0});var Dn=n(Vo);f(P5.$$.fragment,Dn),OGe=i(Dn),$5=s(Dn,"P",{});var wxe=n($5);XGe=r(wxe,`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the `),Y9=s(wxe,"A",{href:!0});var sBr=n(Y9);VGe=r(sBr,"AutoTokenizer.from_pretrained()"),sBr.forEach(t),zGe=r(wxe," class method."),wxe.forEach(t),WGe=i(Dn),I5=s(Dn,"P",{});var Axe=n(I5);QGe=r(Axe,"This class cannot be instantiated directly using "),uH=s(Axe,"CODE",{});var nBr=n(uH);HGe=r(nBr,"__init__()"),nBr.forEach(t),UGe=r(Axe," (throws an error)."),Axe.forEach(t),JGe=i(Dn),fo=s(Dn,"DIV",{class:!0});var da=n(fo);f(D5.$$.fragment,da),YGe=i(da),pH=s(da,"P",{});var lBr=n(pH);KGe=r(lBr,"Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary."),lBr.forEach(t),ZGe=i(da),Da=s(da,"P",{});var C3=n(Da);eOe=r(C3,"The tokenizer class to instantiate is selected based on the "),_H=s(C3,"CODE",{});var iBr=n(_H);oOe=r(iBr,"model_type"),iBr.forEach(t),rOe=r(C3,` property of the config object (either
passed as an argument or loaded from `),bH=s(C3,"CODE",{});var dBr=n(bH);tOe=r(dBr,"pretrained_model_name_or_path"),dBr.forEach(t),aOe=r(C3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),vH=s(C3,"CODE",{});var cBr=n(vH);sOe=r(cBr,"pretrained_model_name_or_path"),cBr.forEach(t),nOe=r(C3,":"),C3.forEach(t),lOe=i(da),M=s(da,"UL",{});var y=n(M);qs=s(y,"LI",{});var c8=n(qs);TH=s(c8,"STRONG",{});var mBr=n(TH);iOe=r(mBr,"albert"),mBr.forEach(t),dOe=r(c8," \u2014 "),K9=s(c8,"A",{href:!0});var fBr=n(K9);cOe=r(fBr,"AlbertTokenizer"),fBr.forEach(t),mOe=r(c8," or "),Z9=s(c8,"A",{href:!0});var gBr=n(Z9);fOe=r(gBr,"AlbertTokenizerFast"),gBr.forEach(t),gOe=r(c8," (ALBERT model)"),c8.forEach(t),hOe=i(y),Gs=s(y,"LI",{});var m8=n(Gs);FH=s(m8,"STRONG",{});var hBr=n(FH);uOe=r(hBr,"bart"),hBr.forEach(t),pOe=r(m8," \u2014 "),eB=s(m8,"A",{href:!0});var uBr=n(eB);_Oe=r(uBr,"BartTokenizer"),uBr.forEach(t),bOe=r(m8," or "),oB=s(m8,"A",{href:!0});var pBr=n(oB);vOe=r(pBr,"BartTokenizerFast"),pBr.forEach(t),TOe=r(m8," (BART model)"),m8.forEach(t),FOe=i(y),Os=s(y,"LI",{});var f8=n(Os);CH=s(f8,"STRONG",{});var _Br=n(CH);COe=r(_Br,"barthez"),_Br.forEach(t),MOe=r(f8," \u2014 "),rB=s(f8,"A",{href:!0});var bBr=n(rB);EOe=r(bBr,"BarthezTokenizer"),bBr.forEach(t),yOe=r(f8," or "),tB=s(f8,"A",{href:!0});var vBr=n(tB);wOe=r(vBr,"BarthezTokenizerFast"),vBr.forEach(t),AOe=r(f8," (BARThez model)"),f8.forEach(t),LOe=i(y),bg=s(y,"LI",{});var hCe=n(bg);MH=s(hCe,"STRONG",{});var TBr=n(MH);BOe=r(TBr,"bartpho"),TBr.forEach(t),xOe=r(hCe," \u2014 "),aB=s(hCe,"A",{href:!0});var FBr=n(aB);kOe=r(FBr,"BartphoTokenizer"),FBr.forEach(t),ROe=r(hCe," (BARTpho model)"),hCe.forEach(t),SOe=i(y),Xs=s(y,"LI",{});var g8=n(Xs);EH=s(g8,"STRONG",{});var CBr=n(EH);POe=r(CBr,"bert"),CBr.forEach(t),$Oe=r(g8," \u2014 "),sB=s(g8,"A",{href:!0});var MBr=n(sB);IOe=r(MBr,"BertTokenizer"),MBr.forEach(t),DOe=r(g8," or "),nB=s(g8,"A",{href:!0});var EBr=n(nB);jOe=r(EBr,"BertTokenizerFast"),EBr.forEach(t),NOe=r(g8," (BERT model)"),g8.forEach(t),qOe=i(y),vg=s(y,"LI",{});var uCe=n(vg);yH=s(uCe,"STRONG",{});var yBr=n(yH);GOe=r(yBr,"bert-generation"),yBr.forEach(t),OOe=r(uCe," \u2014 "),lB=s(uCe,"A",{href:!0});var wBr=n(lB);XOe=r(wBr,"BertGenerationTokenizer"),wBr.forEach(t),VOe=r(uCe," (Bert Generation model)"),uCe.forEach(t),zOe=i(y),Tg=s(y,"LI",{});var pCe=n(Tg);wH=s(pCe,"STRONG",{});var ABr=n(wH);WOe=r(ABr,"bert-japanese"),ABr.forEach(t),QOe=r(pCe," \u2014 "),iB=s(pCe,"A",{href:!0});var LBr=n(iB);HOe=r(LBr,"BertJapaneseTokenizer"),LBr.forEach(t),UOe=r(pCe," (BertJapanese model)"),pCe.forEach(t),JOe=i(y),Fg=s(y,"LI",{});var _Ce=n(Fg);AH=s(_Ce,"STRONG",{});var BBr=n(AH);YOe=r(BBr,"bertweet"),BBr.forEach(t),KOe=r(_Ce," \u2014 "),dB=s(_Ce,"A",{href:!0});var xBr=n(dB);ZOe=r(xBr,"BertweetTokenizer"),xBr.forEach(t),eXe=r(_Ce," (Bertweet model)"),_Ce.forEach(t),oXe=i(y),Vs=s(y,"LI",{});var h8=n(Vs);LH=s(h8,"STRONG",{});var kBr=n(LH);rXe=r(kBr,"big_bird"),kBr.forEach(t),tXe=r(h8," \u2014 "),cB=s(h8,"A",{href:!0});var RBr=n(cB);aXe=r(RBr,"BigBirdTokenizer"),RBr.forEach(t),sXe=r(h8," or "),mB=s(h8,"A",{href:!0});var SBr=n(mB);nXe=r(SBr,"BigBirdTokenizerFast"),SBr.forEach(t),lXe=r(h8," (BigBird model)"),h8.forEach(t),iXe=i(y),zs=s(y,"LI",{});var u8=n(zs);BH=s(u8,"STRONG",{});var PBr=n(BH);dXe=r(PBr,"bigbird_pegasus"),PBr.forEach(t),cXe=r(u8," \u2014 "),fB=s(u8,"A",{href:!0});var $Br=n(fB);mXe=r($Br,"PegasusTokenizer"),$Br.forEach(t),fXe=r(u8," or "),gB=s(u8,"A",{href:!0});var IBr=n(gB);gXe=r(IBr,"PegasusTokenizerFast"),IBr.forEach(t),hXe=r(u8," (BigBirdPegasus model)"),u8.forEach(t),uXe=i(y),Ws=s(y,"LI",{});var p8=n(Ws);xH=s(p8,"STRONG",{});var DBr=n(xH);pXe=r(DBr,"blenderbot"),DBr.forEach(t),_Xe=r(p8," \u2014 "),hB=s(p8,"A",{href:!0});var jBr=n(hB);bXe=r(jBr,"BlenderbotTokenizer"),jBr.forEach(t),vXe=r(p8," or "),uB=s(p8,"A",{href:!0});var NBr=n(uB);TXe=r(NBr,"BlenderbotTokenizerFast"),NBr.forEach(t),FXe=r(p8," (Blenderbot model)"),p8.forEach(t),CXe=i(y),Cg=s(y,"LI",{});var bCe=n(Cg);kH=s(bCe,"STRONG",{});var qBr=n(kH);MXe=r(qBr,"blenderbot-small"),qBr.forEach(t),EXe=r(bCe," \u2014 "),pB=s(bCe,"A",{href:!0});var GBr=n(pB);yXe=r(GBr,"BlenderbotSmallTokenizer"),GBr.forEach(t),wXe=r(bCe," (BlenderbotSmall model)"),bCe.forEach(t),AXe=i(y),Mg=s(y,"LI",{});var vCe=n(Mg);RH=s(vCe,"STRONG",{});var OBr=n(RH);LXe=r(OBr,"byt5"),OBr.forEach(t),BXe=r(vCe," \u2014 "),_B=s(vCe,"A",{href:!0});var XBr=n(_B);xXe=r(XBr,"ByT5Tokenizer"),XBr.forEach(t),kXe=r(vCe," (ByT5 model)"),vCe.forEach(t),RXe=i(y),Qs=s(y,"LI",{});var _8=n(Qs);SH=s(_8,"STRONG",{});var VBr=n(SH);SXe=r(VBr,"camembert"),VBr.forEach(t),PXe=r(_8," \u2014 "),bB=s(_8,"A",{href:!0});var zBr=n(bB);$Xe=r(zBr,"CamembertTokenizer"),zBr.forEach(t),IXe=r(_8," or "),vB=s(_8,"A",{href:!0});var WBr=n(vB);DXe=r(WBr,"CamembertTokenizerFast"),WBr.forEach(t),jXe=r(_8," (CamemBERT model)"),_8.forEach(t),NXe=i(y),Eg=s(y,"LI",{});var TCe=n(Eg);PH=s(TCe,"STRONG",{});var QBr=n(PH);qXe=r(QBr,"canine"),QBr.forEach(t),GXe=r(TCe," \u2014 "),TB=s(TCe,"A",{href:!0});var HBr=n(TB);OXe=r(HBr,"CanineTokenizer"),HBr.forEach(t),XXe=r(TCe," (Canine model)"),TCe.forEach(t),VXe=i(y),Hs=s(y,"LI",{});var b8=n(Hs);$H=s(b8,"STRONG",{});var UBr=n($H);zXe=r(UBr,"clip"),UBr.forEach(t),WXe=r(b8," \u2014 "),FB=s(b8,"A",{href:!0});var JBr=n(FB);QXe=r(JBr,"CLIPTokenizer"),JBr.forEach(t),HXe=r(b8," or "),CB=s(b8,"A",{href:!0});var YBr=n(CB);UXe=r(YBr,"CLIPTokenizerFast"),YBr.forEach(t),JXe=r(b8," (CLIP model)"),b8.forEach(t),YXe=i(y),Us=s(y,"LI",{});var v8=n(Us);IH=s(v8,"STRONG",{});var KBr=n(IH);KXe=r(KBr,"convbert"),KBr.forEach(t),ZXe=r(v8," \u2014 "),MB=s(v8,"A",{href:!0});var ZBr=n(MB);eVe=r(ZBr,"ConvBertTokenizer"),ZBr.forEach(t),oVe=r(v8," or "),EB=s(v8,"A",{href:!0});var exr=n(EB);rVe=r(exr,"ConvBertTokenizerFast"),exr.forEach(t),tVe=r(v8," (ConvBERT model)"),v8.forEach(t),aVe=i(y),Js=s(y,"LI",{});var T8=n(Js);DH=s(T8,"STRONG",{});var oxr=n(DH);sVe=r(oxr,"cpm"),oxr.forEach(t),nVe=r(T8," \u2014 "),yB=s(T8,"A",{href:!0});var rxr=n(yB);lVe=r(rxr,"CpmTokenizer"),rxr.forEach(t),iVe=r(T8," or "),jH=s(T8,"CODE",{});var txr=n(jH);dVe=r(txr,"CpmTokenizerFast"),txr.forEach(t),cVe=r(T8," (CPM model)"),T8.forEach(t),mVe=i(y),yg=s(y,"LI",{});var FCe=n(yg);NH=s(FCe,"STRONG",{});var axr=n(NH);fVe=r(axr,"ctrl"),axr.forEach(t),gVe=r(FCe," \u2014 "),wB=s(FCe,"A",{href:!0});var sxr=n(wB);hVe=r(sxr,"CTRLTokenizer"),sxr.forEach(t),uVe=r(FCe," (CTRL model)"),FCe.forEach(t),pVe=i(y),Ys=s(y,"LI",{});var F8=n(Ys);qH=s(F8,"STRONG",{});var nxr=n(qH);_Ve=r(nxr,"deberta"),nxr.forEach(t),bVe=r(F8," \u2014 "),AB=s(F8,"A",{href:!0});var lxr=n(AB);vVe=r(lxr,"DebertaTokenizer"),lxr.forEach(t),TVe=r(F8," or "),LB=s(F8,"A",{href:!0});var ixr=n(LB);FVe=r(ixr,"DebertaTokenizerFast"),ixr.forEach(t),CVe=r(F8," (DeBERTa model)"),F8.forEach(t),MVe=i(y),wg=s(y,"LI",{});var CCe=n(wg);GH=s(CCe,"STRONG",{});var dxr=n(GH);EVe=r(dxr,"deberta-v2"),dxr.forEach(t),yVe=r(CCe," \u2014 "),BB=s(CCe,"A",{href:!0});var cxr=n(BB);wVe=r(cxr,"DebertaV2Tokenizer"),cxr.forEach(t),AVe=r(CCe," (DeBERTa-v2 model)"),CCe.forEach(t),LVe=i(y),Ks=s(y,"LI",{});var C8=n(Ks);OH=s(C8,"STRONG",{});var mxr=n(OH);BVe=r(mxr,"distilbert"),mxr.forEach(t),xVe=r(C8," \u2014 "),xB=s(C8,"A",{href:!0});var fxr=n(xB);kVe=r(fxr,"DistilBertTokenizer"),fxr.forEach(t),RVe=r(C8," or "),kB=s(C8,"A",{href:!0});var gxr=n(kB);SVe=r(gxr,"DistilBertTokenizerFast"),gxr.forEach(t),PVe=r(C8," (DistilBERT model)"),C8.forEach(t),$Ve=i(y),Zs=s(y,"LI",{});var M8=n(Zs);XH=s(M8,"STRONG",{});var hxr=n(XH);IVe=r(hxr,"dpr"),hxr.forEach(t),DVe=r(M8," \u2014 "),RB=s(M8,"A",{href:!0});var uxr=n(RB);jVe=r(uxr,"DPRQuestionEncoderTokenizer"),uxr.forEach(t),NVe=r(M8," or "),SB=s(M8,"A",{href:!0});var pxr=n(SB);qVe=r(pxr,"DPRQuestionEncoderTokenizerFast"),pxr.forEach(t),GVe=r(M8," (DPR model)"),M8.forEach(t),OVe=i(y),en=s(y,"LI",{});var E8=n(en);VH=s(E8,"STRONG",{});var _xr=n(VH);XVe=r(_xr,"electra"),_xr.forEach(t),VVe=r(E8," \u2014 "),PB=s(E8,"A",{href:!0});var bxr=n(PB);zVe=r(bxr,"ElectraTokenizer"),bxr.forEach(t),WVe=r(E8," or "),$B=s(E8,"A",{href:!0});var vxr=n($B);QVe=r(vxr,"ElectraTokenizerFast"),vxr.forEach(t),HVe=r(E8," (ELECTRA model)"),E8.forEach(t),UVe=i(y),Ag=s(y,"LI",{});var MCe=n(Ag);zH=s(MCe,"STRONG",{});var Txr=n(zH);JVe=r(Txr,"flaubert"),Txr.forEach(t),YVe=r(MCe," \u2014 "),IB=s(MCe,"A",{href:!0});var Fxr=n(IB);KVe=r(Fxr,"FlaubertTokenizer"),Fxr.forEach(t),ZVe=r(MCe," (FlauBERT model)"),MCe.forEach(t),eze=i(y),on=s(y,"LI",{});var y8=n(on);WH=s(y8,"STRONG",{});var Cxr=n(WH);oze=r(Cxr,"fnet"),Cxr.forEach(t),rze=r(y8," \u2014 "),DB=s(y8,"A",{href:!0});var Mxr=n(DB);tze=r(Mxr,"FNetTokenizer"),Mxr.forEach(t),aze=r(y8," or "),jB=s(y8,"A",{href:!0});var Exr=n(jB);sze=r(Exr,"FNetTokenizerFast"),Exr.forEach(t),nze=r(y8," (FNet model)"),y8.forEach(t),lze=i(y),Lg=s(y,"LI",{});var ECe=n(Lg);QH=s(ECe,"STRONG",{});var yxr=n(QH);ize=r(yxr,"fsmt"),yxr.forEach(t),dze=r(ECe," \u2014 "),NB=s(ECe,"A",{href:!0});var wxr=n(NB);cze=r(wxr,"FSMTTokenizer"),wxr.forEach(t),mze=r(ECe," (FairSeq Machine-Translation model)"),ECe.forEach(t),fze=i(y),rn=s(y,"LI",{});var w8=n(rn);HH=s(w8,"STRONG",{});var Axr=n(HH);gze=r(Axr,"funnel"),Axr.forEach(t),hze=r(w8," \u2014 "),qB=s(w8,"A",{href:!0});var Lxr=n(qB);uze=r(Lxr,"FunnelTokenizer"),Lxr.forEach(t),pze=r(w8," or "),GB=s(w8,"A",{href:!0});var Bxr=n(GB);_ze=r(Bxr,"FunnelTokenizerFast"),Bxr.forEach(t),bze=r(w8," (Funnel Transformer model)"),w8.forEach(t),vze=i(y),tn=s(y,"LI",{});var A8=n(tn);UH=s(A8,"STRONG",{});var xxr=n(UH);Tze=r(xxr,"gpt2"),xxr.forEach(t),Fze=r(A8," \u2014 "),OB=s(A8,"A",{href:!0});var kxr=n(OB);Cze=r(kxr,"GPT2Tokenizer"),kxr.forEach(t),Mze=r(A8," or "),XB=s(A8,"A",{href:!0});var Rxr=n(XB);Eze=r(Rxr,"GPT2TokenizerFast"),Rxr.forEach(t),yze=r(A8," (OpenAI GPT-2 model)"),A8.forEach(t),wze=i(y),an=s(y,"LI",{});var L8=n(an);JH=s(L8,"STRONG",{});var Sxr=n(JH);Aze=r(Sxr,"gpt_neo"),Sxr.forEach(t),Lze=r(L8," \u2014 "),VB=s(L8,"A",{href:!0});var Pxr=n(VB);Bze=r(Pxr,"GPT2Tokenizer"),Pxr.forEach(t),xze=r(L8," or "),zB=s(L8,"A",{href:!0});var $xr=n(zB);kze=r($xr,"GPT2TokenizerFast"),$xr.forEach(t),Rze=r(L8," (GPT Neo model)"),L8.forEach(t),Sze=i(y),sn=s(y,"LI",{});var B8=n(sn);YH=s(B8,"STRONG",{});var Ixr=n(YH);Pze=r(Ixr,"herbert"),Ixr.forEach(t),$ze=r(B8," \u2014 "),WB=s(B8,"A",{href:!0});var Dxr=n(WB);Ize=r(Dxr,"HerbertTokenizer"),Dxr.forEach(t),Dze=r(B8," or "),QB=s(B8,"A",{href:!0});var jxr=n(QB);jze=r(jxr,"HerbertTokenizerFast"),jxr.forEach(t),Nze=r(B8," (HerBERT model)"),B8.forEach(t),qze=i(y),Bg=s(y,"LI",{});var yCe=n(Bg);KH=s(yCe,"STRONG",{});var Nxr=n(KH);Gze=r(Nxr,"hubert"),Nxr.forEach(t),Oze=r(yCe," \u2014 "),HB=s(yCe,"A",{href:!0});var qxr=n(HB);Xze=r(qxr,"Wav2Vec2CTCTokenizer"),qxr.forEach(t),Vze=r(yCe," (Hubert model)"),yCe.forEach(t),zze=i(y),nn=s(y,"LI",{});var x8=n(nn);ZH=s(x8,"STRONG",{});var Gxr=n(ZH);Wze=r(Gxr,"ibert"),Gxr.forEach(t),Qze=r(x8," \u2014 "),UB=s(x8,"A",{href:!0});var Oxr=n(UB);Hze=r(Oxr,"RobertaTokenizer"),Oxr.forEach(t),Uze=r(x8," or "),JB=s(x8,"A",{href:!0});var Xxr=n(JB);Jze=r(Xxr,"RobertaTokenizerFast"),Xxr.forEach(t),Yze=r(x8," (I-BERT model)"),x8.forEach(t),Kze=i(y),ln=s(y,"LI",{});var k8=n(ln);eU=s(k8,"STRONG",{});var Vxr=n(eU);Zze=r(Vxr,"layoutlm"),Vxr.forEach(t),eWe=r(k8," \u2014 "),YB=s(k8,"A",{href:!0});var zxr=n(YB);oWe=r(zxr,"LayoutLMTokenizer"),zxr.forEach(t),rWe=r(k8," or "),KB=s(k8,"A",{href:!0});var Wxr=n(KB);tWe=r(Wxr,"LayoutLMTokenizerFast"),Wxr.forEach(t),aWe=r(k8," (LayoutLM model)"),k8.forEach(t),sWe=i(y),dn=s(y,"LI",{});var R8=n(dn);oU=s(R8,"STRONG",{});var Qxr=n(oU);nWe=r(Qxr,"layoutlmv2"),Qxr.forEach(t),lWe=r(R8," \u2014 "),ZB=s(R8,"A",{href:!0});var Hxr=n(ZB);iWe=r(Hxr,"LayoutLMv2Tokenizer"),Hxr.forEach(t),dWe=r(R8," or "),ex=s(R8,"A",{href:!0});var Uxr=n(ex);cWe=r(Uxr,"LayoutLMv2TokenizerFast"),Uxr.forEach(t),mWe=r(R8," (LayoutLMv2 model)"),R8.forEach(t),fWe=i(y),cn=s(y,"LI",{});var S8=n(cn);rU=s(S8,"STRONG",{});var Jxr=n(rU);gWe=r(Jxr,"layoutxlm"),Jxr.forEach(t),hWe=r(S8," \u2014 "),ox=s(S8,"A",{href:!0});var Yxr=n(ox);uWe=r(Yxr,"LayoutXLMTokenizer"),Yxr.forEach(t),pWe=r(S8," or "),rx=s(S8,"A",{href:!0});var Kxr=n(rx);_We=r(Kxr,"LayoutXLMTokenizerFast"),Kxr.forEach(t),bWe=r(S8," (LayoutXLM model)"),S8.forEach(t),vWe=i(y),mn=s(y,"LI",{});var P8=n(mn);tU=s(P8,"STRONG",{});var Zxr=n(tU);TWe=r(Zxr,"led"),Zxr.forEach(t),FWe=r(P8," \u2014 "),tx=s(P8,"A",{href:!0});var ekr=n(tx);CWe=r(ekr,"LEDTokenizer"),ekr.forEach(t),MWe=r(P8," or "),ax=s(P8,"A",{href:!0});var okr=n(ax);EWe=r(okr,"LEDTokenizerFast"),okr.forEach(t),yWe=r(P8," (LED model)"),P8.forEach(t),wWe=i(y),fn=s(y,"LI",{});var $8=n(fn);aU=s($8,"STRONG",{});var rkr=n(aU);AWe=r(rkr,"longformer"),rkr.forEach(t),LWe=r($8," \u2014 "),sx=s($8,"A",{href:!0});var tkr=n(sx);BWe=r(tkr,"LongformerTokenizer"),tkr.forEach(t),xWe=r($8," or "),nx=s($8,"A",{href:!0});var akr=n(nx);kWe=r(akr,"LongformerTokenizerFast"),akr.forEach(t),RWe=r($8," (Longformer model)"),$8.forEach(t),SWe=i(y),xg=s(y,"LI",{});var wCe=n(xg);sU=s(wCe,"STRONG",{});var skr=n(sU);PWe=r(skr,"luke"),skr.forEach(t),$We=r(wCe," \u2014 "),lx=s(wCe,"A",{href:!0});var nkr=n(lx);IWe=r(nkr,"LukeTokenizer"),nkr.forEach(t),DWe=r(wCe," (LUKE model)"),wCe.forEach(t),jWe=i(y),gn=s(y,"LI",{});var I8=n(gn);nU=s(I8,"STRONG",{});var lkr=n(nU);NWe=r(lkr,"lxmert"),lkr.forEach(t),qWe=r(I8," \u2014 "),ix=s(I8,"A",{href:!0});var ikr=n(ix);GWe=r(ikr,"LxmertTokenizer"),ikr.forEach(t),OWe=r(I8," or "),dx=s(I8,"A",{href:!0});var dkr=n(dx);XWe=r(dkr,"LxmertTokenizerFast"),dkr.forEach(t),VWe=r(I8," (LXMERT model)"),I8.forEach(t),zWe=i(y),kg=s(y,"LI",{});var ACe=n(kg);lU=s(ACe,"STRONG",{});var ckr=n(lU);WWe=r(ckr,"m2m_100"),ckr.forEach(t),QWe=r(ACe," \u2014 "),cx=s(ACe,"A",{href:!0});var mkr=n(cx);HWe=r(mkr,"M2M100Tokenizer"),mkr.forEach(t),UWe=r(ACe," (M2M100 model)"),ACe.forEach(t),JWe=i(y),Rg=s(y,"LI",{});var LCe=n(Rg);iU=s(LCe,"STRONG",{});var fkr=n(iU);YWe=r(fkr,"marian"),fkr.forEach(t),KWe=r(LCe," \u2014 "),mx=s(LCe,"A",{href:!0});var gkr=n(mx);ZWe=r(gkr,"MarianTokenizer"),gkr.forEach(t),eQe=r(LCe," (Marian model)"),LCe.forEach(t),oQe=i(y),hn=s(y,"LI",{});var D8=n(hn);dU=s(D8,"STRONG",{});var hkr=n(dU);rQe=r(hkr,"mbart"),hkr.forEach(t),tQe=r(D8," \u2014 "),fx=s(D8,"A",{href:!0});var ukr=n(fx);aQe=r(ukr,"MBartTokenizer"),ukr.forEach(t),sQe=r(D8," or "),gx=s(D8,"A",{href:!0});var pkr=n(gx);nQe=r(pkr,"MBartTokenizerFast"),pkr.forEach(t),lQe=r(D8," (mBART model)"),D8.forEach(t),iQe=i(y),un=s(y,"LI",{});var j8=n(un);cU=s(j8,"STRONG",{});var _kr=n(cU);dQe=r(_kr,"mbart50"),_kr.forEach(t),cQe=r(j8," \u2014 "),hx=s(j8,"A",{href:!0});var bkr=n(hx);mQe=r(bkr,"MBart50Tokenizer"),bkr.forEach(t),fQe=r(j8," or "),ux=s(j8,"A",{href:!0});var vkr=n(ux);gQe=r(vkr,"MBart50TokenizerFast"),vkr.forEach(t),hQe=r(j8," (mBART-50 model)"),j8.forEach(t),uQe=i(y),Sg=s(y,"LI",{});var BCe=n(Sg);mU=s(BCe,"STRONG",{});var Tkr=n(mU);pQe=r(Tkr,"mluke"),Tkr.forEach(t),_Qe=r(BCe," \u2014 "),px=s(BCe,"A",{href:!0});var Fkr=n(px);bQe=r(Fkr,"MLukeTokenizer"),Fkr.forEach(t),vQe=r(BCe," (mLUKE model)"),BCe.forEach(t),TQe=i(y),pn=s(y,"LI",{});var N8=n(pn);fU=s(N8,"STRONG",{});var Ckr=n(fU);FQe=r(Ckr,"mobilebert"),Ckr.forEach(t),CQe=r(N8," \u2014 "),_x=s(N8,"A",{href:!0});var Mkr=n(_x);MQe=r(Mkr,"MobileBertTokenizer"),Mkr.forEach(t),EQe=r(N8," or "),bx=s(N8,"A",{href:!0});var Ekr=n(bx);yQe=r(Ekr,"MobileBertTokenizerFast"),Ekr.forEach(t),wQe=r(N8," (MobileBERT model)"),N8.forEach(t),AQe=i(y),_n=s(y,"LI",{});var q8=n(_n);gU=s(q8,"STRONG",{});var ykr=n(gU);LQe=r(ykr,"mpnet"),ykr.forEach(t),BQe=r(q8," \u2014 "),vx=s(q8,"A",{href:!0});var wkr=n(vx);xQe=r(wkr,"MPNetTokenizer"),wkr.forEach(t),kQe=r(q8," or "),Tx=s(q8,"A",{href:!0});var Akr=n(Tx);RQe=r(Akr,"MPNetTokenizerFast"),Akr.forEach(t),SQe=r(q8," (MPNet model)"),q8.forEach(t),PQe=i(y),bn=s(y,"LI",{});var G8=n(bn);hU=s(G8,"STRONG",{});var Lkr=n(hU);$Qe=r(Lkr,"mt5"),Lkr.forEach(t),IQe=r(G8," \u2014 "),Fx=s(G8,"A",{href:!0});var Bkr=n(Fx);DQe=r(Bkr,"MT5Tokenizer"),Bkr.forEach(t),jQe=r(G8," or "),Cx=s(G8,"A",{href:!0});var xkr=n(Cx);NQe=r(xkr,"MT5TokenizerFast"),xkr.forEach(t),qQe=r(G8," (mT5 model)"),G8.forEach(t),GQe=i(y),vn=s(y,"LI",{});var O8=n(vn);uU=s(O8,"STRONG",{});var kkr=n(uU);OQe=r(kkr,"openai-gpt"),kkr.forEach(t),XQe=r(O8," \u2014 "),Mx=s(O8,"A",{href:!0});var Rkr=n(Mx);VQe=r(Rkr,"OpenAIGPTTokenizer"),Rkr.forEach(t),zQe=r(O8," or "),Ex=s(O8,"A",{href:!0});var Skr=n(Ex);WQe=r(Skr,"OpenAIGPTTokenizerFast"),Skr.forEach(t),QQe=r(O8," (OpenAI GPT model)"),O8.forEach(t),HQe=i(y),Tn=s(y,"LI",{});var X8=n(Tn);pU=s(X8,"STRONG",{});var Pkr=n(pU);UQe=r(Pkr,"pegasus"),Pkr.forEach(t),JQe=r(X8," \u2014 "),yx=s(X8,"A",{href:!0});var $kr=n(yx);YQe=r($kr,"PegasusTokenizer"),$kr.forEach(t),KQe=r(X8," or "),wx=s(X8,"A",{href:!0});var Ikr=n(wx);ZQe=r(Ikr,"PegasusTokenizerFast"),Ikr.forEach(t),eHe=r(X8," (Pegasus model)"),X8.forEach(t),oHe=i(y),Pg=s(y,"LI",{});var xCe=n(Pg);_U=s(xCe,"STRONG",{});var Dkr=n(_U);rHe=r(Dkr,"perceiver"),Dkr.forEach(t),tHe=r(xCe," \u2014 "),Ax=s(xCe,"A",{href:!0});var jkr=n(Ax);aHe=r(jkr,"PerceiverTokenizer"),jkr.forEach(t),sHe=r(xCe," (Perceiver model)"),xCe.forEach(t),nHe=i(y),$g=s(y,"LI",{});var kCe=n($g);bU=s(kCe,"STRONG",{});var Nkr=n(bU);lHe=r(Nkr,"phobert"),Nkr.forEach(t),iHe=r(kCe," \u2014 "),Lx=s(kCe,"A",{href:!0});var qkr=n(Lx);dHe=r(qkr,"PhobertTokenizer"),qkr.forEach(t),cHe=r(kCe," (PhoBERT model)"),kCe.forEach(t),mHe=i(y),Ig=s(y,"LI",{});var RCe=n(Ig);vU=s(RCe,"STRONG",{});var Gkr=n(vU);fHe=r(Gkr,"plbart"),Gkr.forEach(t),gHe=r(RCe," \u2014 "),Bx=s(RCe,"A",{href:!0});var Okr=n(Bx);hHe=r(Okr,"PLBartTokenizer"),Okr.forEach(t),uHe=r(RCe," (PLBart model)"),RCe.forEach(t),pHe=i(y),Dg=s(y,"LI",{});var SCe=n(Dg);TU=s(SCe,"STRONG",{});var Xkr=n(TU);_He=r(Xkr,"prophetnet"),Xkr.forEach(t),bHe=r(SCe," \u2014 "),xx=s(SCe,"A",{href:!0});var Vkr=n(xx);vHe=r(Vkr,"ProphetNetTokenizer"),Vkr.forEach(t),THe=r(SCe," (ProphetNet model)"),SCe.forEach(t),FHe=i(y),Fn=s(y,"LI",{});var V8=n(Fn);FU=s(V8,"STRONG",{});var zkr=n(FU);CHe=r(zkr,"qdqbert"),zkr.forEach(t),MHe=r(V8," \u2014 "),kx=s(V8,"A",{href:!0});var Wkr=n(kx);EHe=r(Wkr,"BertTokenizer"),Wkr.forEach(t),yHe=r(V8," or "),Rx=s(V8,"A",{href:!0});var Qkr=n(Rx);wHe=r(Qkr,"BertTokenizerFast"),Qkr.forEach(t),AHe=r(V8," (QDQBert model)"),V8.forEach(t),LHe=i(y),jg=s(y,"LI",{});var PCe=n(jg);CU=s(PCe,"STRONG",{});var Hkr=n(CU);BHe=r(Hkr,"rag"),Hkr.forEach(t),xHe=r(PCe," \u2014 "),Sx=s(PCe,"A",{href:!0});var Ukr=n(Sx);kHe=r(Ukr,"RagTokenizer"),Ukr.forEach(t),RHe=r(PCe," (RAG model)"),PCe.forEach(t),SHe=i(y),Cn=s(y,"LI",{});var z8=n(Cn);MU=s(z8,"STRONG",{});var Jkr=n(MU);PHe=r(Jkr,"reformer"),Jkr.forEach(t),$He=r(z8," \u2014 "),Px=s(z8,"A",{href:!0});var Ykr=n(Px);IHe=r(Ykr,"ReformerTokenizer"),Ykr.forEach(t),DHe=r(z8," or "),$x=s(z8,"A",{href:!0});var Kkr=n($x);jHe=r(Kkr,"ReformerTokenizerFast"),Kkr.forEach(t),NHe=r(z8," (Reformer model)"),z8.forEach(t),qHe=i(y),Mn=s(y,"LI",{});var W8=n(Mn);EU=s(W8,"STRONG",{});var Zkr=n(EU);GHe=r(Zkr,"rembert"),Zkr.forEach(t),OHe=r(W8," \u2014 "),Ix=s(W8,"A",{href:!0});var eRr=n(Ix);XHe=r(eRr,"RemBertTokenizer"),eRr.forEach(t),VHe=r(W8," or "),Dx=s(W8,"A",{href:!0});var oRr=n(Dx);zHe=r(oRr,"RemBertTokenizerFast"),oRr.forEach(t),WHe=r(W8," (RemBERT model)"),W8.forEach(t),QHe=i(y),En=s(y,"LI",{});var Q8=n(En);yU=s(Q8,"STRONG",{});var rRr=n(yU);HHe=r(rRr,"retribert"),rRr.forEach(t),UHe=r(Q8," \u2014 "),jx=s(Q8,"A",{href:!0});var tRr=n(jx);JHe=r(tRr,"RetriBertTokenizer"),tRr.forEach(t),YHe=r(Q8," or "),Nx=s(Q8,"A",{href:!0});var aRr=n(Nx);KHe=r(aRr,"RetriBertTokenizerFast"),aRr.forEach(t),ZHe=r(Q8," (RetriBERT model)"),Q8.forEach(t),eUe=i(y),yn=s(y,"LI",{});var H8=n(yn);wU=s(H8,"STRONG",{});var sRr=n(wU);oUe=r(sRr,"roberta"),sRr.forEach(t),rUe=r(H8," \u2014 "),qx=s(H8,"A",{href:!0});var nRr=n(qx);tUe=r(nRr,"RobertaTokenizer"),nRr.forEach(t),aUe=r(H8," or "),Gx=s(H8,"A",{href:!0});var lRr=n(Gx);sUe=r(lRr,"RobertaTokenizerFast"),lRr.forEach(t),nUe=r(H8," (RoBERTa model)"),H8.forEach(t),lUe=i(y),wn=s(y,"LI",{});var U8=n(wn);AU=s(U8,"STRONG",{});var iRr=n(AU);iUe=r(iRr,"roformer"),iRr.forEach(t),dUe=r(U8," \u2014 "),Ox=s(U8,"A",{href:!0});var dRr=n(Ox);cUe=r(dRr,"RoFormerTokenizer"),dRr.forEach(t),mUe=r(U8," or "),Xx=s(U8,"A",{href:!0});var cRr=n(Xx);fUe=r(cRr,"RoFormerTokenizerFast"),cRr.forEach(t),gUe=r(U8," (RoFormer model)"),U8.forEach(t),hUe=i(y),Ng=s(y,"LI",{});var $Ce=n(Ng);LU=s($Ce,"STRONG",{});var mRr=n(LU);uUe=r(mRr,"speech_to_text"),mRr.forEach(t),pUe=r($Ce," \u2014 "),Vx=s($Ce,"A",{href:!0});var fRr=n(Vx);_Ue=r(fRr,"Speech2TextTokenizer"),fRr.forEach(t),bUe=r($Ce," (Speech2Text model)"),$Ce.forEach(t),vUe=i(y),qg=s(y,"LI",{});var ICe=n(qg);BU=s(ICe,"STRONG",{});var gRr=n(BU);TUe=r(gRr,"speech_to_text_2"),gRr.forEach(t),FUe=r(ICe," \u2014 "),zx=s(ICe,"A",{href:!0});var hRr=n(zx);CUe=r(hRr,"Speech2Text2Tokenizer"),hRr.forEach(t),MUe=r(ICe," (Speech2Text2 model)"),ICe.forEach(t),EUe=i(y),An=s(y,"LI",{});var J8=n(An);xU=s(J8,"STRONG",{});var uRr=n(xU);yUe=r(uRr,"splinter"),uRr.forEach(t),wUe=r(J8," \u2014 "),Wx=s(J8,"A",{href:!0});var pRr=n(Wx);AUe=r(pRr,"SplinterTokenizer"),pRr.forEach(t),LUe=r(J8," or "),Qx=s(J8,"A",{href:!0});var _Rr=n(Qx);BUe=r(_Rr,"SplinterTokenizerFast"),_Rr.forEach(t),xUe=r(J8," (Splinter model)"),J8.forEach(t),kUe=i(y),Ln=s(y,"LI",{});var Y8=n(Ln);kU=s(Y8,"STRONG",{});var bRr=n(kU);RUe=r(bRr,"squeezebert"),bRr.forEach(t),SUe=r(Y8," \u2014 "),Hx=s(Y8,"A",{href:!0});var vRr=n(Hx);PUe=r(vRr,"SqueezeBertTokenizer"),vRr.forEach(t),$Ue=r(Y8," or "),Ux=s(Y8,"A",{href:!0});var TRr=n(Ux);IUe=r(TRr,"SqueezeBertTokenizerFast"),TRr.forEach(t),DUe=r(Y8," (SqueezeBERT model)"),Y8.forEach(t),jUe=i(y),Bn=s(y,"LI",{});var K8=n(Bn);RU=s(K8,"STRONG",{});var FRr=n(RU);NUe=r(FRr,"t5"),FRr.forEach(t),qUe=r(K8," \u2014 "),Jx=s(K8,"A",{href:!0});var CRr=n(Jx);GUe=r(CRr,"T5Tokenizer"),CRr.forEach(t),OUe=r(K8," or "),Yx=s(K8,"A",{href:!0});var MRr=n(Yx);XUe=r(MRr,"T5TokenizerFast"),MRr.forEach(t),VUe=r(K8," (T5 model)"),K8.forEach(t),zUe=i(y),Gg=s(y,"LI",{});var DCe=n(Gg);SU=s(DCe,"STRONG",{});var ERr=n(SU);WUe=r(ERr,"tapas"),ERr.forEach(t),QUe=r(DCe," \u2014 "),Kx=s(DCe,"A",{href:!0});var yRr=n(Kx);HUe=r(yRr,"TapasTokenizer"),yRr.forEach(t),UUe=r(DCe," (TAPAS model)"),DCe.forEach(t),JUe=i(y),Og=s(y,"LI",{});var jCe=n(Og);PU=s(jCe,"STRONG",{});var wRr=n(PU);YUe=r(wRr,"transfo-xl"),wRr.forEach(t),KUe=r(jCe," \u2014 "),Zx=s(jCe,"A",{href:!0});var ARr=n(Zx);ZUe=r(ARr,"TransfoXLTokenizer"),ARr.forEach(t),eJe=r(jCe," (Transformer-XL model)"),jCe.forEach(t),oJe=i(y),Xg=s(y,"LI",{});var NCe=n(Xg);$U=s(NCe,"STRONG",{});var LRr=n($U);rJe=r(LRr,"wav2vec2"),LRr.forEach(t),tJe=r(NCe," \u2014 "),ek=s(NCe,"A",{href:!0});var BRr=n(ek);aJe=r(BRr,"Wav2Vec2CTCTokenizer"),BRr.forEach(t),sJe=r(NCe," (Wav2Vec2 model)"),NCe.forEach(t),nJe=i(y),Vg=s(y,"LI",{});var qCe=n(Vg);IU=s(qCe,"STRONG",{});var xRr=n(IU);lJe=r(xRr,"wav2vec2_phoneme"),xRr.forEach(t),iJe=r(qCe," \u2014 "),ok=s(qCe,"A",{href:!0});var kRr=n(ok);dJe=r(kRr,"Wav2Vec2PhonemeCTCTokenizer"),kRr.forEach(t),cJe=r(qCe," (Wav2Vec2Phoneme model)"),qCe.forEach(t),mJe=i(y),xn=s(y,"LI",{});var Z8=n(xn);DU=s(Z8,"STRONG",{});var RRr=n(DU);fJe=r(RRr,"xglm"),RRr.forEach(t),gJe=r(Z8," \u2014 "),rk=s(Z8,"A",{href:!0});var SRr=n(rk);hJe=r(SRr,"XGLMTokenizer"),SRr.forEach(t),uJe=r(Z8," or "),tk=s(Z8,"A",{href:!0});var PRr=n(tk);pJe=r(PRr,"XGLMTokenizerFast"),PRr.forEach(t),_Je=r(Z8," (XGLM model)"),Z8.forEach(t),bJe=i(y),zg=s(y,"LI",{});var GCe=n(zg);jU=s(GCe,"STRONG",{});var $Rr=n(jU);vJe=r($Rr,"xlm"),$Rr.forEach(t),TJe=r(GCe," \u2014 "),ak=s(GCe,"A",{href:!0});var IRr=n(ak);FJe=r(IRr,"XLMTokenizer"),IRr.forEach(t),CJe=r(GCe," (XLM model)"),GCe.forEach(t),MJe=i(y),Wg=s(y,"LI",{});var OCe=n(Wg);NU=s(OCe,"STRONG",{});var DRr=n(NU);EJe=r(DRr,"xlm-prophetnet"),DRr.forEach(t),yJe=r(OCe," \u2014 "),sk=s(OCe,"A",{href:!0});var jRr=n(sk);wJe=r(jRr,"XLMProphetNetTokenizer"),jRr.forEach(t),AJe=r(OCe," (XLMProphetNet model)"),OCe.forEach(t),LJe=i(y),kn=s(y,"LI",{});var e7=n(kn);qU=s(e7,"STRONG",{});var NRr=n(qU);BJe=r(NRr,"xlm-roberta"),NRr.forEach(t),xJe=r(e7," \u2014 "),nk=s(e7,"A",{href:!0});var qRr=n(nk);kJe=r(qRr,"XLMRobertaTokenizer"),qRr.forEach(t),RJe=r(e7," or "),lk=s(e7,"A",{href:!0});var GRr=n(lk);SJe=r(GRr,"XLMRobertaTokenizerFast"),GRr.forEach(t),PJe=r(e7," (XLM-RoBERTa model)"),e7.forEach(t),$Je=i(y),Rn=s(y,"LI",{});var o7=n(Rn);GU=s(o7,"STRONG",{});var ORr=n(GU);IJe=r(ORr,"xlnet"),ORr.forEach(t),DJe=r(o7," \u2014 "),ik=s(o7,"A",{href:!0});var XRr=n(ik);jJe=r(XRr,"XLNetTokenizer"),XRr.forEach(t),NJe=r(o7," or "),dk=s(o7,"A",{href:!0});var VRr=n(dk);qJe=r(VRr,"XLNetTokenizerFast"),VRr.forEach(t),GJe=r(o7," (XLNet model)"),o7.forEach(t),y.forEach(t),OJe=i(da),OU=s(da,"P",{});var zRr=n(OU);XJe=r(zRr,"Examples:"),zRr.forEach(t),VJe=i(da),f(j5.$$.fragment,da),da.forEach(t),zJe=i(Dn),Qg=s(Dn,"DIV",{class:!0});var Lxe=n(Qg);f(N5.$$.fragment,Lxe),WJe=i(Lxe),XU=s(Lxe,"P",{});var WRr=n(XU);QJe=r(WRr,"Register a new tokenizer in this mapping."),WRr.forEach(t),Lxe.forEach(t),Dn.forEach(t),w9e=i(c),Ni=s(c,"H2",{class:!0});var Bxe=n(Ni);Hg=s(Bxe,"A",{id:!0,class:!0,href:!0});var QRr=n(Hg);VU=s(QRr,"SPAN",{});var HRr=n(VU);f(q5.$$.fragment,HRr),HRr.forEach(t),QRr.forEach(t),HJe=i(Bxe),zU=s(Bxe,"SPAN",{});var URr=n(zU);UJe=r(URr,"AutoFeatureExtractor"),URr.forEach(t),Bxe.forEach(t),A9e=i(c),zo=s(c,"DIV",{class:!0});var jn=n(zo);f(G5.$$.fragment,jn),JJe=i(jn),O5=s(jn,"P",{});var xxe=n(O5);YJe=r(xxe,`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the `),ck=s(xxe,"A",{href:!0});var JRr=n(ck);KJe=r(JRr,"AutoFeatureExtractor.from_pretrained()"),JRr.forEach(t),ZJe=r(xxe," class method."),xxe.forEach(t),eYe=i(jn),X5=s(jn,"P",{});var kxe=n(X5);oYe=r(kxe,"This class cannot be instantiated directly using "),WU=s(kxe,"CODE",{});var YRr=n(WU);rYe=r(YRr,"__init__()"),YRr.forEach(t),tYe=r(kxe," (throws an error)."),kxe.forEach(t),aYe=i(jn),xe=s(jn,"DIV",{class:!0});var St=n(xe);f(V5.$$.fragment,St),sYe=i(St),QU=s(St,"P",{});var KRr=n(QU);nYe=r(KRr,"Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary."),KRr.forEach(t),lYe=i(St),ja=s(St,"P",{});var M3=n(ja);iYe=r(M3,"The feature extractor class to instantiate is selected based on the "),HU=s(M3,"CODE",{});var ZRr=n(HU);dYe=r(ZRr,"model_type"),ZRr.forEach(t),cYe=r(M3,` property of the config object
(either passed as an argument or loaded from `),UU=s(M3,"CODE",{});var eSr=n(UU);mYe=r(eSr,"pretrained_model_name_or_path"),eSr.forEach(t),fYe=r(M3,` if possible), or when it\u2019s
missing, by falling back to using pattern matching on `),JU=s(M3,"CODE",{});var oSr=n(JU);gYe=r(oSr,"pretrained_model_name_or_path"),oSr.forEach(t),hYe=r(M3,":"),M3.forEach(t),uYe=i(St),ne=s(St,"UL",{});var de=n(ne);Ug=s(de,"LI",{});var XCe=n(Ug);YU=s(XCe,"STRONG",{});var rSr=n(YU);pYe=r(rSr,"beit"),rSr.forEach(t),_Ye=r(XCe," \u2014 "),mk=s(XCe,"A",{href:!0});var tSr=n(mk);bYe=r(tSr,"BeitFeatureExtractor"),tSr.forEach(t),vYe=r(XCe," (BEiT model)"),XCe.forEach(t),TYe=i(de),Jg=s(de,"LI",{});var VCe=n(Jg);KU=s(VCe,"STRONG",{});var aSr=n(KU);FYe=r(aSr,"clip"),aSr.forEach(t),CYe=r(VCe," \u2014 "),fk=s(VCe,"A",{href:!0});var sSr=n(fk);MYe=r(sSr,"CLIPFeatureExtractor"),sSr.forEach(t),EYe=r(VCe," (CLIP model)"),VCe.forEach(t),yYe=i(de),Yg=s(de,"LI",{});var zCe=n(Yg);ZU=s(zCe,"STRONG",{});var nSr=n(ZU);wYe=r(nSr,"convnext"),nSr.forEach(t),AYe=r(zCe," \u2014 "),gk=s(zCe,"A",{href:!0});var lSr=n(gk);LYe=r(lSr,"ConvNextFeatureExtractor"),lSr.forEach(t),BYe=r(zCe," (ConvNext model)"),zCe.forEach(t),xYe=i(de),Kg=s(de,"LI",{});var WCe=n(Kg);eJ=s(WCe,"STRONG",{});var iSr=n(eJ);kYe=r(iSr,"deit"),iSr.forEach(t),RYe=r(WCe," \u2014 "),hk=s(WCe,"A",{href:!0});var dSr=n(hk);SYe=r(dSr,"DeiTFeatureExtractor"),dSr.forEach(t),PYe=r(WCe," (DeiT model)"),WCe.forEach(t),$Ye=i(de),Zg=s(de,"LI",{});var QCe=n(Zg);oJ=s(QCe,"STRONG",{});var cSr=n(oJ);IYe=r(cSr,"detr"),cSr.forEach(t),DYe=r(QCe," \u2014 "),uk=s(QCe,"A",{href:!0});var mSr=n(uk);jYe=r(mSr,"DetrFeatureExtractor"),mSr.forEach(t),NYe=r(QCe," (DETR model)"),QCe.forEach(t),qYe=i(de),eh=s(de,"LI",{});var HCe=n(eh);rJ=s(HCe,"STRONG",{});var fSr=n(rJ);GYe=r(fSr,"hubert"),fSr.forEach(t),OYe=r(HCe," \u2014 "),pk=s(HCe,"A",{href:!0});var gSr=n(pk);XYe=r(gSr,"Wav2Vec2FeatureExtractor"),gSr.forEach(t),VYe=r(HCe," (Hubert model)"),HCe.forEach(t),zYe=i(de),oh=s(de,"LI",{});var UCe=n(oh);tJ=s(UCe,"STRONG",{});var hSr=n(tJ);WYe=r(hSr,"layoutlmv2"),hSr.forEach(t),QYe=r(UCe," \u2014 "),_k=s(UCe,"A",{href:!0});var uSr=n(_k);HYe=r(uSr,"LayoutLMv2FeatureExtractor"),uSr.forEach(t),UYe=r(UCe," (LayoutLMv2 model)"),UCe.forEach(t),JYe=i(de),rh=s(de,"LI",{});var JCe=n(rh);aJ=s(JCe,"STRONG",{});var pSr=n(aJ);YYe=r(pSr,"perceiver"),pSr.forEach(t),KYe=r(JCe," \u2014 "),bk=s(JCe,"A",{href:!0});var _Sr=n(bk);ZYe=r(_Sr,"PerceiverFeatureExtractor"),_Sr.forEach(t),eKe=r(JCe," (Perceiver model)"),JCe.forEach(t),oKe=i(de),th=s(de,"LI",{});var YCe=n(th);sJ=s(YCe,"STRONG",{});var bSr=n(sJ);rKe=r(bSr,"poolformer"),bSr.forEach(t),tKe=r(YCe," \u2014 "),vk=s(YCe,"A",{href:!0});var vSr=n(vk);aKe=r(vSr,"PoolFormerFeatureExtractor"),vSr.forEach(t),sKe=r(YCe," (PoolFormer model)"),YCe.forEach(t),nKe=i(de),ah=s(de,"LI",{});var KCe=n(ah);nJ=s(KCe,"STRONG",{});var TSr=n(nJ);lKe=r(TSr,"segformer"),TSr.forEach(t),iKe=r(KCe," \u2014 "),Tk=s(KCe,"A",{href:!0});var FSr=n(Tk);dKe=r(FSr,"SegformerFeatureExtractor"),FSr.forEach(t),cKe=r(KCe," (SegFormer model)"),KCe.forEach(t),mKe=i(de),sh=s(de,"LI",{});var ZCe=n(sh);lJ=s(ZCe,"STRONG",{});var CSr=n(lJ);fKe=r(CSr,"speech_to_text"),CSr.forEach(t),gKe=r(ZCe," \u2014 "),Fk=s(ZCe,"A",{href:!0});var MSr=n(Fk);hKe=r(MSr,"Speech2TextFeatureExtractor"),MSr.forEach(t),uKe=r(ZCe," (Speech2Text model)"),ZCe.forEach(t),pKe=i(de),nh=s(de,"LI",{});var e4e=n(nh);iJ=s(e4e,"STRONG",{});var ESr=n(iJ);_Ke=r(ESr,"swin"),ESr.forEach(t),bKe=r(e4e," \u2014 "),Ck=s(e4e,"A",{href:!0});var ySr=n(Ck);vKe=r(ySr,"ViTFeatureExtractor"),ySr.forEach(t),TKe=r(e4e," (Swin model)"),e4e.forEach(t),FKe=i(de),lh=s(de,"LI",{});var o4e=n(lh);dJ=s(o4e,"STRONG",{});var wSr=n(dJ);CKe=r(wSr,"vit"),wSr.forEach(t),MKe=r(o4e," \u2014 "),Mk=s(o4e,"A",{href:!0});var ASr=n(Mk);EKe=r(ASr,"ViTFeatureExtractor"),ASr.forEach(t),yKe=r(o4e," (ViT model)"),o4e.forEach(t),wKe=i(de),ih=s(de,"LI",{});var r4e=n(ih);cJ=s(r4e,"STRONG",{});var LSr=n(cJ);AKe=r(LSr,"vit_mae"),LSr.forEach(t),LKe=r(r4e," \u2014 "),Ek=s(r4e,"A",{href:!0});var BSr=n(Ek);BKe=r(BSr,"ViTFeatureExtractor"),BSr.forEach(t),xKe=r(r4e," (ViTMAE model)"),r4e.forEach(t),kKe=i(de),dh=s(de,"LI",{});var t4e=n(dh);mJ=s(t4e,"STRONG",{});var xSr=n(mJ);RKe=r(xSr,"wav2vec2"),xSr.forEach(t),SKe=r(t4e," \u2014 "),yk=s(t4e,"A",{href:!0});var kSr=n(yk);PKe=r(kSr,"Wav2Vec2FeatureExtractor"),kSr.forEach(t),$Ke=r(t4e," (Wav2Vec2 model)"),t4e.forEach(t),de.forEach(t),IKe=i(St),f(ch.$$.fragment,St),DKe=i(St),fJ=s(St,"P",{});var RSr=n(fJ);jKe=r(RSr,"Examples:"),RSr.forEach(t),NKe=i(St),f(z5.$$.fragment,St),St.forEach(t),qKe=i(jn),mh=s(jn,"DIV",{class:!0});var Rxe=n(mh);f(W5.$$.fragment,Rxe),GKe=i(Rxe),gJ=s(Rxe,"P",{});var SSr=n(gJ);OKe=r(SSr,"Register a new feature extractor for this class."),SSr.forEach(t),Rxe.forEach(t),jn.forEach(t),L9e=i(c),qi=s(c,"H2",{class:!0});var Sxe=n(qi);fh=s(Sxe,"A",{id:!0,class:!0,href:!0});var PSr=n(fh);hJ=s(PSr,"SPAN",{});var $Sr=n(hJ);f(Q5.$$.fragment,$Sr),$Sr.forEach(t),PSr.forEach(t),XKe=i(Sxe),uJ=s(Sxe,"SPAN",{});var ISr=n(uJ);VKe=r(ISr,"AutoProcessor"),ISr.forEach(t),Sxe.forEach(t),B9e=i(c),Wo=s(c,"DIV",{class:!0});var Nn=n(Wo);f(H5.$$.fragment,Nn),zKe=i(Nn),U5=s(Nn,"P",{});var Pxe=n(U5);WKe=r(Pxe,`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the `),wk=s(Pxe,"A",{href:!0});var DSr=n(wk);QKe=r(DSr,"AutoProcessor.from_pretrained()"),DSr.forEach(t),HKe=r(Pxe," class method."),Pxe.forEach(t),UKe=i(Nn),J5=s(Nn,"P",{});var $xe=n(J5);JKe=r($xe,"This class cannot be instantiated directly using "),pJ=s($xe,"CODE",{});var jSr=n(pJ);YKe=r(jSr,"__init__()"),jSr.forEach(t),KKe=r($xe," (throws an error)."),$xe.forEach(t),ZKe=i(Nn),ke=s(Nn,"DIV",{class:!0});var Pt=n(ke);f(Y5.$$.fragment,Pt),eZe=i(Pt),_J=s(Pt,"P",{});var NSr=n(_J);oZe=r(NSr,"Instantiate one of the processor classes of the library from a pretrained model vocabulary."),NSr.forEach(t),rZe=i(Pt),Gi=s(Pt,"P",{});var LV=n(Gi);tZe=r(LV,"The processor class to instantiate is selected based on the "),bJ=s(LV,"CODE",{});var qSr=n(bJ);aZe=r(qSr,"model_type"),qSr.forEach(t),sZe=r(LV,` property of the config object (either
passed as an argument or loaded from `),vJ=s(LV,"CODE",{});var GSr=n(vJ);nZe=r(GSr,"pretrained_model_name_or_path"),GSr.forEach(t),lZe=r(LV," if possible):"),LV.forEach(t),iZe=i(Pt),we=s(Pt,"UL",{});var jo=n(we);gh=s(jo,"LI",{});var a4e=n(gh);TJ=s(a4e,"STRONG",{});var OSr=n(TJ);dZe=r(OSr,"clip"),OSr.forEach(t),cZe=r(a4e," \u2014 "),Ak=s(a4e,"A",{href:!0});var XSr=n(Ak);mZe=r(XSr,"CLIPProcessor"),XSr.forEach(t),fZe=r(a4e," (CLIP model)"),a4e.forEach(t),gZe=i(jo),hh=s(jo,"LI",{});var s4e=n(hh);FJ=s(s4e,"STRONG",{});var VSr=n(FJ);hZe=r(VSr,"layoutlmv2"),VSr.forEach(t),uZe=r(s4e," \u2014 "),Lk=s(s4e,"A",{href:!0});var zSr=n(Lk);pZe=r(zSr,"LayoutLMv2Processor"),zSr.forEach(t),_Ze=r(s4e," (LayoutLMv2 model)"),s4e.forEach(t),bZe=i(jo),uh=s(jo,"LI",{});var n4e=n(uh);CJ=s(n4e,"STRONG",{});var WSr=n(CJ);vZe=r(WSr,"layoutxlm"),WSr.forEach(t),TZe=r(n4e," \u2014 "),Bk=s(n4e,"A",{href:!0});var QSr=n(Bk);FZe=r(QSr,"LayoutXLMProcessor"),QSr.forEach(t),CZe=r(n4e," (LayoutXLM model)"),n4e.forEach(t),MZe=i(jo),ph=s(jo,"LI",{});var l4e=n(ph);MJ=s(l4e,"STRONG",{});var HSr=n(MJ);EZe=r(HSr,"speech_to_text"),HSr.forEach(t),yZe=r(l4e," \u2014 "),xk=s(l4e,"A",{href:!0});var USr=n(xk);wZe=r(USr,"Speech2TextProcessor"),USr.forEach(t),AZe=r(l4e," (Speech2Text model)"),l4e.forEach(t),LZe=i(jo),_h=s(jo,"LI",{});var i4e=n(_h);EJ=s(i4e,"STRONG",{});var JSr=n(EJ);BZe=r(JSr,"speech_to_text_2"),JSr.forEach(t),xZe=r(i4e," \u2014 "),kk=s(i4e,"A",{href:!0});var YSr=n(kk);kZe=r(YSr,"Speech2Text2Processor"),YSr.forEach(t),RZe=r(i4e," (Speech2Text2 model)"),i4e.forEach(t),SZe=i(jo),bh=s(jo,"LI",{});var d4e=n(bh);yJ=s(d4e,"STRONG",{});var KSr=n(yJ);PZe=r(KSr,"trocr"),KSr.forEach(t),$Ze=r(d4e," \u2014 "),Rk=s(d4e,"A",{href:!0});var ZSr=n(Rk);IZe=r(ZSr,"TrOCRProcessor"),ZSr.forEach(t),DZe=r(d4e," (TrOCR model)"),d4e.forEach(t),jZe=i(jo),vh=s(jo,"LI",{});var c4e=n(vh);wJ=s(c4e,"STRONG",{});var ePr=n(wJ);NZe=r(ePr,"vision-text-dual-encoder"),ePr.forEach(t),qZe=r(c4e," \u2014 "),Sk=s(c4e,"A",{href:!0});var oPr=n(Sk);GZe=r(oPr,"VisionTextDualEncoderProcessor"),oPr.forEach(t),OZe=r(c4e," (VisionTextDualEncoder model)"),c4e.forEach(t),XZe=i(jo),Th=s(jo,"LI",{});var m4e=n(Th);AJ=s(m4e,"STRONG",{});var rPr=n(AJ);VZe=r(rPr,"wav2vec2"),rPr.forEach(t),zZe=r(m4e," \u2014 "),Pk=s(m4e,"A",{href:!0});var tPr=n(Pk);WZe=r(tPr,"Wav2Vec2Processor"),tPr.forEach(t),QZe=r(m4e," (Wav2Vec2 model)"),m4e.forEach(t),jo.forEach(t),HZe=i(Pt),f(Fh.$$.fragment,Pt),UZe=i(Pt),LJ=s(Pt,"P",{});var aPr=n(LJ);JZe=r(aPr,"Examples:"),aPr.forEach(t),YZe=i(Pt),f(K5.$$.fragment,Pt),Pt.forEach(t),KZe=i(Nn),Ch=s(Nn,"DIV",{class:!0});var Ixe=n(Ch);f(Z5.$$.fragment,Ixe),ZZe=i(Ixe),BJ=s(Ixe,"P",{});var sPr=n(BJ);eeo=r(sPr,"Register a new processor for this class."),sPr.forEach(t),Ixe.forEach(t),Nn.forEach(t),x9e=i(c),Oi=s(c,"H2",{class:!0});var Dxe=n(Oi);Mh=s(Dxe,"A",{id:!0,class:!0,href:!0});var nPr=n(Mh);xJ=s(nPr,"SPAN",{});var lPr=n(xJ);f(ey.$$.fragment,lPr),lPr.forEach(t),nPr.forEach(t),oeo=i(Dxe),kJ=s(Dxe,"SPAN",{});var iPr=n(kJ);reo=r(iPr,"AutoModel"),iPr.forEach(t),Dxe.forEach(t),k9e=i(c),Qo=s(c,"DIV",{class:!0});var qn=n(Qo);f(oy.$$.fragment,qn),teo=i(qn),Xi=s(qn,"P",{});var BV=n(Xi);aeo=r(BV,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),RJ=s(BV,"CODE",{});var dPr=n(RJ);seo=r(dPr,"from_pretrained()"),dPr.forEach(t),neo=r(BV,"class method or the "),SJ=s(BV,"CODE",{});var cPr=n(SJ);leo=r(cPr,"from_config()"),cPr.forEach(t),ieo=r(BV,`class
method.`),BV.forEach(t),deo=i(qn),ry=s(qn,"P",{});var jxe=n(ry);ceo=r(jxe,"This class cannot be instantiated directly using "),PJ=s(jxe,"CODE",{});var mPr=n(PJ);meo=r(mPr,"__init__()"),mPr.forEach(t),feo=r(jxe," (throws an error)."),jxe.forEach(t),geo=i(qn),qr=s(qn,"DIV",{class:!0});var Gn=n(qr);f(ty.$$.fragment,Gn),heo=i(Gn),$J=s(Gn,"P",{});var fPr=n($J);ueo=r(fPr,"Instantiates one of the base model classes of the library from a configuration."),fPr.forEach(t),peo=i(Gn),Vi=s(Gn,"P",{});var xV=n(Vi);_eo=r(xV,`Note:
Loading a model from its configuration file does `),IJ=s(xV,"STRONG",{});var gPr=n(IJ);beo=r(gPr,"not"),gPr.forEach(t),veo=r(xV,` load the model weights. It only affects the
model\u2019s configuration. Use `),DJ=s(xV,"CODE",{});var hPr=n(DJ);Teo=r(hPr,"from_pretrained()"),hPr.forEach(t),Feo=r(xV,"to load the model weights."),xV.forEach(t),Ceo=i(Gn),jJ=s(Gn,"P",{});var uPr=n(jJ);Meo=r(uPr,"Examples:"),uPr.forEach(t),Eeo=i(Gn),f(ay.$$.fragment,Gn),Gn.forEach(t),yeo=i(qn),Re=s(qn,"DIV",{class:!0});var $t=n(Re);f(sy.$$.fragment,$t),weo=i($t),NJ=s($t,"P",{});var pPr=n(NJ);Aeo=r(pPr,"Instantiate one of the base model classes of the library from a pretrained model."),pPr.forEach(t),Leo=i($t),Na=s($t,"P",{});var E3=n(Na);Beo=r(E3,"The model class to instantiate is selected based on the "),qJ=s(E3,"CODE",{});var _Pr=n(qJ);xeo=r(_Pr,"model_type"),_Pr.forEach(t),keo=r(E3,` property of the config object (either
passed as an argument or loaded from `),GJ=s(E3,"CODE",{});var bPr=n(GJ);Reo=r(bPr,"pretrained_model_name_or_path"),bPr.forEach(t),Seo=r(E3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),OJ=s(E3,"CODE",{});var vPr=n(OJ);Peo=r(vPr,"pretrained_model_name_or_path"),vPr.forEach(t),$eo=r(E3,":"),E3.forEach(t),Ieo=i($t),F=s($t,"UL",{});var C=n(F);Eh=s(C,"LI",{});var f4e=n(Eh);XJ=s(f4e,"STRONG",{});var TPr=n(XJ);Deo=r(TPr,"albert"),TPr.forEach(t),jeo=r(f4e," \u2014 "),$k=s(f4e,"A",{href:!0});var FPr=n($k);Neo=r(FPr,"AlbertModel"),FPr.forEach(t),qeo=r(f4e," (ALBERT model)"),f4e.forEach(t),Geo=i(C),yh=s(C,"LI",{});var g4e=n(yh);VJ=s(g4e,"STRONG",{});var CPr=n(VJ);Oeo=r(CPr,"bart"),CPr.forEach(t),Xeo=r(g4e," \u2014 "),Ik=s(g4e,"A",{href:!0});var MPr=n(Ik);Veo=r(MPr,"BartModel"),MPr.forEach(t),zeo=r(g4e," (BART model)"),g4e.forEach(t),Weo=i(C),wh=s(C,"LI",{});var h4e=n(wh);zJ=s(h4e,"STRONG",{});var EPr=n(zJ);Qeo=r(EPr,"beit"),EPr.forEach(t),Heo=r(h4e," \u2014 "),Dk=s(h4e,"A",{href:!0});var yPr=n(Dk);Ueo=r(yPr,"BeitModel"),yPr.forEach(t),Jeo=r(h4e," (BEiT model)"),h4e.forEach(t),Yeo=i(C),Ah=s(C,"LI",{});var u4e=n(Ah);WJ=s(u4e,"STRONG",{});var wPr=n(WJ);Keo=r(wPr,"bert"),wPr.forEach(t),Zeo=r(u4e," \u2014 "),jk=s(u4e,"A",{href:!0});var APr=n(jk);eoo=r(APr,"BertModel"),APr.forEach(t),ooo=r(u4e," (BERT model)"),u4e.forEach(t),roo=i(C),Lh=s(C,"LI",{});var p4e=n(Lh);QJ=s(p4e,"STRONG",{});var LPr=n(QJ);too=r(LPr,"bert-generation"),LPr.forEach(t),aoo=r(p4e," \u2014 "),Nk=s(p4e,"A",{href:!0});var BPr=n(Nk);soo=r(BPr,"BertGenerationEncoder"),BPr.forEach(t),noo=r(p4e," (Bert Generation model)"),p4e.forEach(t),loo=i(C),Bh=s(C,"LI",{});var _4e=n(Bh);HJ=s(_4e,"STRONG",{});var xPr=n(HJ);ioo=r(xPr,"big_bird"),xPr.forEach(t),doo=r(_4e," \u2014 "),qk=s(_4e,"A",{href:!0});var kPr=n(qk);coo=r(kPr,"BigBirdModel"),kPr.forEach(t),moo=r(_4e," (BigBird model)"),_4e.forEach(t),foo=i(C),xh=s(C,"LI",{});var b4e=n(xh);UJ=s(b4e,"STRONG",{});var RPr=n(UJ);goo=r(RPr,"bigbird_pegasus"),RPr.forEach(t),hoo=r(b4e," \u2014 "),Gk=s(b4e,"A",{href:!0});var SPr=n(Gk);uoo=r(SPr,"BigBirdPegasusModel"),SPr.forEach(t),poo=r(b4e," (BigBirdPegasus model)"),b4e.forEach(t),_oo=i(C),kh=s(C,"LI",{});var v4e=n(kh);JJ=s(v4e,"STRONG",{});var PPr=n(JJ);boo=r(PPr,"blenderbot"),PPr.forEach(t),voo=r(v4e," \u2014 "),Ok=s(v4e,"A",{href:!0});var $Pr=n(Ok);Too=r($Pr,"BlenderbotModel"),$Pr.forEach(t),Foo=r(v4e," (Blenderbot model)"),v4e.forEach(t),Coo=i(C),Rh=s(C,"LI",{});var T4e=n(Rh);YJ=s(T4e,"STRONG",{});var IPr=n(YJ);Moo=r(IPr,"blenderbot-small"),IPr.forEach(t),Eoo=r(T4e," \u2014 "),Xk=s(T4e,"A",{href:!0});var DPr=n(Xk);yoo=r(DPr,"BlenderbotSmallModel"),DPr.forEach(t),woo=r(T4e," (BlenderbotSmall model)"),T4e.forEach(t),Aoo=i(C),Sh=s(C,"LI",{});var F4e=n(Sh);KJ=s(F4e,"STRONG",{});var jPr=n(KJ);Loo=r(jPr,"camembert"),jPr.forEach(t),Boo=r(F4e," \u2014 "),Vk=s(F4e,"A",{href:!0});var NPr=n(Vk);xoo=r(NPr,"CamembertModel"),NPr.forEach(t),koo=r(F4e," (CamemBERT model)"),F4e.forEach(t),Roo=i(C),Ph=s(C,"LI",{});var C4e=n(Ph);ZJ=s(C4e,"STRONG",{});var qPr=n(ZJ);Soo=r(qPr,"canine"),qPr.forEach(t),Poo=r(C4e," \u2014 "),zk=s(C4e,"A",{href:!0});var GPr=n(zk);$oo=r(GPr,"CanineModel"),GPr.forEach(t),Ioo=r(C4e," (Canine model)"),C4e.forEach(t),Doo=i(C),$h=s(C,"LI",{});var M4e=n($h);eY=s(M4e,"STRONG",{});var OPr=n(eY);joo=r(OPr,"clip"),OPr.forEach(t),Noo=r(M4e," \u2014 "),Wk=s(M4e,"A",{href:!0});var XPr=n(Wk);qoo=r(XPr,"CLIPModel"),XPr.forEach(t),Goo=r(M4e," (CLIP model)"),M4e.forEach(t),Ooo=i(C),Ih=s(C,"LI",{});var E4e=n(Ih);oY=s(E4e,"STRONG",{});var VPr=n(oY);Xoo=r(VPr,"convbert"),VPr.forEach(t),Voo=r(E4e," \u2014 "),Qk=s(E4e,"A",{href:!0});var zPr=n(Qk);zoo=r(zPr,"ConvBertModel"),zPr.forEach(t),Woo=r(E4e," (ConvBERT model)"),E4e.forEach(t),Qoo=i(C),Dh=s(C,"LI",{});var y4e=n(Dh);rY=s(y4e,"STRONG",{});var WPr=n(rY);Hoo=r(WPr,"convnext"),WPr.forEach(t),Uoo=r(y4e," \u2014 "),Hk=s(y4e,"A",{href:!0});var QPr=n(Hk);Joo=r(QPr,"ConvNextModel"),QPr.forEach(t),Yoo=r(y4e," (ConvNext model)"),y4e.forEach(t),Koo=i(C),jh=s(C,"LI",{});var w4e=n(jh);tY=s(w4e,"STRONG",{});var HPr=n(tY);Zoo=r(HPr,"ctrl"),HPr.forEach(t),ero=r(w4e," \u2014 "),Uk=s(w4e,"A",{href:!0});var UPr=n(Uk);oro=r(UPr,"CTRLModel"),UPr.forEach(t),rro=r(w4e," (CTRL model)"),w4e.forEach(t),tro=i(C),Nh=s(C,"LI",{});var A4e=n(Nh);aY=s(A4e,"STRONG",{});var JPr=n(aY);aro=r(JPr,"data2vec-audio"),JPr.forEach(t),sro=r(A4e," \u2014 "),Jk=s(A4e,"A",{href:!0});var YPr=n(Jk);nro=r(YPr,"Data2VecAudioModel"),YPr.forEach(t),lro=r(A4e," (Data2VecAudio model)"),A4e.forEach(t),iro=i(C),qh=s(C,"LI",{});var L4e=n(qh);sY=s(L4e,"STRONG",{});var KPr=n(sY);dro=r(KPr,"data2vec-text"),KPr.forEach(t),cro=r(L4e," \u2014 "),Yk=s(L4e,"A",{href:!0});var ZPr=n(Yk);mro=r(ZPr,"Data2VecTextModel"),ZPr.forEach(t),fro=r(L4e," (Data2VecText model)"),L4e.forEach(t),gro=i(C),Gh=s(C,"LI",{});var B4e=n(Gh);nY=s(B4e,"STRONG",{});var e$r=n(nY);hro=r(e$r,"deberta"),e$r.forEach(t),uro=r(B4e," \u2014 "),Kk=s(B4e,"A",{href:!0});var o$r=n(Kk);pro=r(o$r,"DebertaModel"),o$r.forEach(t),_ro=r(B4e," (DeBERTa model)"),B4e.forEach(t),bro=i(C),Oh=s(C,"LI",{});var x4e=n(Oh);lY=s(x4e,"STRONG",{});var r$r=n(lY);vro=r(r$r,"deberta-v2"),r$r.forEach(t),Tro=r(x4e," \u2014 "),Zk=s(x4e,"A",{href:!0});var t$r=n(Zk);Fro=r(t$r,"DebertaV2Model"),t$r.forEach(t),Cro=r(x4e," (DeBERTa-v2 model)"),x4e.forEach(t),Mro=i(C),Xh=s(C,"LI",{});var k4e=n(Xh);iY=s(k4e,"STRONG",{});var a$r=n(iY);Ero=r(a$r,"deit"),a$r.forEach(t),yro=r(k4e," \u2014 "),eR=s(k4e,"A",{href:!0});var s$r=n(eR);wro=r(s$r,"DeiTModel"),s$r.forEach(t),Aro=r(k4e," (DeiT model)"),k4e.forEach(t),Lro=i(C),Vh=s(C,"LI",{});var R4e=n(Vh);dY=s(R4e,"STRONG",{});var n$r=n(dY);Bro=r(n$r,"detr"),n$r.forEach(t),xro=r(R4e," \u2014 "),oR=s(R4e,"A",{href:!0});var l$r=n(oR);kro=r(l$r,"DetrModel"),l$r.forEach(t),Rro=r(R4e," (DETR model)"),R4e.forEach(t),Sro=i(C),zh=s(C,"LI",{});var S4e=n(zh);cY=s(S4e,"STRONG",{});var i$r=n(cY);Pro=r(i$r,"distilbert"),i$r.forEach(t),$ro=r(S4e," \u2014 "),rR=s(S4e,"A",{href:!0});var d$r=n(rR);Iro=r(d$r,"DistilBertModel"),d$r.forEach(t),Dro=r(S4e," (DistilBERT model)"),S4e.forEach(t),jro=i(C),Wh=s(C,"LI",{});var P4e=n(Wh);mY=s(P4e,"STRONG",{});var c$r=n(mY);Nro=r(c$r,"dpr"),c$r.forEach(t),qro=r(P4e," \u2014 "),tR=s(P4e,"A",{href:!0});var m$r=n(tR);Gro=r(m$r,"DPRQuestionEncoder"),m$r.forEach(t),Oro=r(P4e," (DPR model)"),P4e.forEach(t),Xro=i(C),Qh=s(C,"LI",{});var $4e=n(Qh);fY=s($4e,"STRONG",{});var f$r=n(fY);Vro=r(f$r,"electra"),f$r.forEach(t),zro=r($4e," \u2014 "),aR=s($4e,"A",{href:!0});var g$r=n(aR);Wro=r(g$r,"ElectraModel"),g$r.forEach(t),Qro=r($4e," (ELECTRA model)"),$4e.forEach(t),Hro=i(C),Hh=s(C,"LI",{});var I4e=n(Hh);gY=s(I4e,"STRONG",{});var h$r=n(gY);Uro=r(h$r,"flaubert"),h$r.forEach(t),Jro=r(I4e," \u2014 "),sR=s(I4e,"A",{href:!0});var u$r=n(sR);Yro=r(u$r,"FlaubertModel"),u$r.forEach(t),Kro=r(I4e," (FlauBERT model)"),I4e.forEach(t),Zro=i(C),Uh=s(C,"LI",{});var D4e=n(Uh);hY=s(D4e,"STRONG",{});var p$r=n(hY);eto=r(p$r,"fnet"),p$r.forEach(t),oto=r(D4e," \u2014 "),nR=s(D4e,"A",{href:!0});var _$r=n(nR);rto=r(_$r,"FNetModel"),_$r.forEach(t),tto=r(D4e," (FNet model)"),D4e.forEach(t),ato=i(C),Jh=s(C,"LI",{});var j4e=n(Jh);uY=s(j4e,"STRONG",{});var b$r=n(uY);sto=r(b$r,"fsmt"),b$r.forEach(t),nto=r(j4e," \u2014 "),lR=s(j4e,"A",{href:!0});var v$r=n(lR);lto=r(v$r,"FSMTModel"),v$r.forEach(t),ito=r(j4e," (FairSeq Machine-Translation model)"),j4e.forEach(t),dto=i(C),Sn=s(C,"LI",{});var r7=n(Sn);pY=s(r7,"STRONG",{});var T$r=n(pY);cto=r(T$r,"funnel"),T$r.forEach(t),mto=r(r7," \u2014 "),iR=s(r7,"A",{href:!0});var F$r=n(iR);fto=r(F$r,"FunnelModel"),F$r.forEach(t),gto=r(r7," or "),dR=s(r7,"A",{href:!0});var C$r=n(dR);hto=r(C$r,"FunnelBaseModel"),C$r.forEach(t),uto=r(r7," (Funnel Transformer model)"),r7.forEach(t),pto=i(C),Yh=s(C,"LI",{});var N4e=n(Yh);_Y=s(N4e,"STRONG",{});var M$r=n(_Y);_to=r(M$r,"gpt2"),M$r.forEach(t),bto=r(N4e," \u2014 "),cR=s(N4e,"A",{href:!0});var E$r=n(cR);vto=r(E$r,"GPT2Model"),E$r.forEach(t),Tto=r(N4e," (OpenAI GPT-2 model)"),N4e.forEach(t),Fto=i(C),Kh=s(C,"LI",{});var q4e=n(Kh);bY=s(q4e,"STRONG",{});var y$r=n(bY);Cto=r(y$r,"gpt_neo"),y$r.forEach(t),Mto=r(q4e," \u2014 "),mR=s(q4e,"A",{href:!0});var w$r=n(mR);Eto=r(w$r,"GPTNeoModel"),w$r.forEach(t),yto=r(q4e," (GPT Neo model)"),q4e.forEach(t),wto=i(C),Zh=s(C,"LI",{});var G4e=n(Zh);vY=s(G4e,"STRONG",{});var A$r=n(vY);Ato=r(A$r,"gptj"),A$r.forEach(t),Lto=r(G4e," \u2014 "),fR=s(G4e,"A",{href:!0});var L$r=n(fR);Bto=r(L$r,"GPTJModel"),L$r.forEach(t),xto=r(G4e," (GPT-J model)"),G4e.forEach(t),kto=i(C),eu=s(C,"LI",{});var O4e=n(eu);TY=s(O4e,"STRONG",{});var B$r=n(TY);Rto=r(B$r,"hubert"),B$r.forEach(t),Sto=r(O4e," \u2014 "),gR=s(O4e,"A",{href:!0});var x$r=n(gR);Pto=r(x$r,"HubertModel"),x$r.forEach(t),$to=r(O4e," (Hubert model)"),O4e.forEach(t),Ito=i(C),ou=s(C,"LI",{});var X4e=n(ou);FY=s(X4e,"STRONG",{});var k$r=n(FY);Dto=r(k$r,"ibert"),k$r.forEach(t),jto=r(X4e," \u2014 "),hR=s(X4e,"A",{href:!0});var R$r=n(hR);Nto=r(R$r,"IBertModel"),R$r.forEach(t),qto=r(X4e," (I-BERT model)"),X4e.forEach(t),Gto=i(C),ru=s(C,"LI",{});var V4e=n(ru);CY=s(V4e,"STRONG",{});var S$r=n(CY);Oto=r(S$r,"imagegpt"),S$r.forEach(t),Xto=r(V4e," \u2014 "),uR=s(V4e,"A",{href:!0});var P$r=n(uR);Vto=r(P$r,"ImageGPTModel"),P$r.forEach(t),zto=r(V4e," (ImageGPT model)"),V4e.forEach(t),Wto=i(C),tu=s(C,"LI",{});var z4e=n(tu);MY=s(z4e,"STRONG",{});var $$r=n(MY);Qto=r($$r,"layoutlm"),$$r.forEach(t),Hto=r(z4e," \u2014 "),pR=s(z4e,"A",{href:!0});var I$r=n(pR);Uto=r(I$r,"LayoutLMModel"),I$r.forEach(t),Jto=r(z4e," (LayoutLM model)"),z4e.forEach(t),Yto=i(C),au=s(C,"LI",{});var W4e=n(au);EY=s(W4e,"STRONG",{});var D$r=n(EY);Kto=r(D$r,"layoutlmv2"),D$r.forEach(t),Zto=r(W4e," \u2014 "),_R=s(W4e,"A",{href:!0});var j$r=n(_R);eao=r(j$r,"LayoutLMv2Model"),j$r.forEach(t),oao=r(W4e," (LayoutLMv2 model)"),W4e.forEach(t),rao=i(C),su=s(C,"LI",{});var Q4e=n(su);yY=s(Q4e,"STRONG",{});var N$r=n(yY);tao=r(N$r,"led"),N$r.forEach(t),aao=r(Q4e," \u2014 "),bR=s(Q4e,"A",{href:!0});var q$r=n(bR);sao=r(q$r,"LEDModel"),q$r.forEach(t),nao=r(Q4e," (LED model)"),Q4e.forEach(t),lao=i(C),nu=s(C,"LI",{});var H4e=n(nu);wY=s(H4e,"STRONG",{});var G$r=n(wY);iao=r(G$r,"longformer"),G$r.forEach(t),dao=r(H4e," \u2014 "),vR=s(H4e,"A",{href:!0});var O$r=n(vR);cao=r(O$r,"LongformerModel"),O$r.forEach(t),mao=r(H4e," (Longformer model)"),H4e.forEach(t),fao=i(C),lu=s(C,"LI",{});var U4e=n(lu);AY=s(U4e,"STRONG",{});var X$r=n(AY);gao=r(X$r,"luke"),X$r.forEach(t),hao=r(U4e," \u2014 "),TR=s(U4e,"A",{href:!0});var V$r=n(TR);uao=r(V$r,"LukeModel"),V$r.forEach(t),pao=r(U4e," (LUKE model)"),U4e.forEach(t),_ao=i(C),iu=s(C,"LI",{});var J4e=n(iu);LY=s(J4e,"STRONG",{});var z$r=n(LY);bao=r(z$r,"lxmert"),z$r.forEach(t),vao=r(J4e," \u2014 "),FR=s(J4e,"A",{href:!0});var W$r=n(FR);Tao=r(W$r,"LxmertModel"),W$r.forEach(t),Fao=r(J4e," (LXMERT model)"),J4e.forEach(t),Cao=i(C),du=s(C,"LI",{});var Y4e=n(du);BY=s(Y4e,"STRONG",{});var Q$r=n(BY);Mao=r(Q$r,"m2m_100"),Q$r.forEach(t),Eao=r(Y4e," \u2014 "),CR=s(Y4e,"A",{href:!0});var H$r=n(CR);yao=r(H$r,"M2M100Model"),H$r.forEach(t),wao=r(Y4e," (M2M100 model)"),Y4e.forEach(t),Aao=i(C),cu=s(C,"LI",{});var K4e=n(cu);xY=s(K4e,"STRONG",{});var U$r=n(xY);Lao=r(U$r,"marian"),U$r.forEach(t),Bao=r(K4e," \u2014 "),MR=s(K4e,"A",{href:!0});var J$r=n(MR);xao=r(J$r,"MarianModel"),J$r.forEach(t),kao=r(K4e," (Marian model)"),K4e.forEach(t),Rao=i(C),mu=s(C,"LI",{});var Z4e=n(mu);kY=s(Z4e,"STRONG",{});var Y$r=n(kY);Sao=r(Y$r,"mbart"),Y$r.forEach(t),Pao=r(Z4e," \u2014 "),ER=s(Z4e,"A",{href:!0});var K$r=n(ER);$ao=r(K$r,"MBartModel"),K$r.forEach(t),Iao=r(Z4e," (mBART model)"),Z4e.forEach(t),Dao=i(C),fu=s(C,"LI",{});var eMe=n(fu);RY=s(eMe,"STRONG",{});var Z$r=n(RY);jao=r(Z$r,"megatron-bert"),Z$r.forEach(t),Nao=r(eMe," \u2014 "),yR=s(eMe,"A",{href:!0});var eIr=n(yR);qao=r(eIr,"MegatronBertModel"),eIr.forEach(t),Gao=r(eMe," (MegatronBert model)"),eMe.forEach(t),Oao=i(C),gu=s(C,"LI",{});var oMe=n(gu);SY=s(oMe,"STRONG",{});var oIr=n(SY);Xao=r(oIr,"mobilebert"),oIr.forEach(t),Vao=r(oMe," \u2014 "),wR=s(oMe,"A",{href:!0});var rIr=n(wR);zao=r(rIr,"MobileBertModel"),rIr.forEach(t),Wao=r(oMe," (MobileBERT model)"),oMe.forEach(t),Qao=i(C),hu=s(C,"LI",{});var rMe=n(hu);PY=s(rMe,"STRONG",{});var tIr=n(PY);Hao=r(tIr,"mpnet"),tIr.forEach(t),Uao=r(rMe," \u2014 "),AR=s(rMe,"A",{href:!0});var aIr=n(AR);Jao=r(aIr,"MPNetModel"),aIr.forEach(t),Yao=r(rMe," (MPNet model)"),rMe.forEach(t),Kao=i(C),uu=s(C,"LI",{});var tMe=n(uu);$Y=s(tMe,"STRONG",{});var sIr=n($Y);Zao=r(sIr,"mt5"),sIr.forEach(t),eso=r(tMe," \u2014 "),LR=s(tMe,"A",{href:!0});var nIr=n(LR);oso=r(nIr,"MT5Model"),nIr.forEach(t),rso=r(tMe," (mT5 model)"),tMe.forEach(t),tso=i(C),pu=s(C,"LI",{});var aMe=n(pu);IY=s(aMe,"STRONG",{});var lIr=n(IY);aso=r(lIr,"nystromformer"),lIr.forEach(t),sso=r(aMe," \u2014 "),BR=s(aMe,"A",{href:!0});var iIr=n(BR);nso=r(iIr,"NystromformerModel"),iIr.forEach(t),lso=r(aMe," (Nystromformer model)"),aMe.forEach(t),iso=i(C),_u=s(C,"LI",{});var sMe=n(_u);DY=s(sMe,"STRONG",{});var dIr=n(DY);dso=r(dIr,"openai-gpt"),dIr.forEach(t),cso=r(sMe," \u2014 "),xR=s(sMe,"A",{href:!0});var cIr=n(xR);mso=r(cIr,"OpenAIGPTModel"),cIr.forEach(t),fso=r(sMe," (OpenAI GPT model)"),sMe.forEach(t),gso=i(C),bu=s(C,"LI",{});var nMe=n(bu);jY=s(nMe,"STRONG",{});var mIr=n(jY);hso=r(mIr,"pegasus"),mIr.forEach(t),uso=r(nMe," \u2014 "),kR=s(nMe,"A",{href:!0});var fIr=n(kR);pso=r(fIr,"PegasusModel"),fIr.forEach(t),_so=r(nMe," (Pegasus model)"),nMe.forEach(t),bso=i(C),vu=s(C,"LI",{});var lMe=n(vu);NY=s(lMe,"STRONG",{});var gIr=n(NY);vso=r(gIr,"perceiver"),gIr.forEach(t),Tso=r(lMe," \u2014 "),RR=s(lMe,"A",{href:!0});var hIr=n(RR);Fso=r(hIr,"PerceiverModel"),hIr.forEach(t),Cso=r(lMe," (Perceiver model)"),lMe.forEach(t),Mso=i(C),Tu=s(C,"LI",{});var iMe=n(Tu);qY=s(iMe,"STRONG",{});var uIr=n(qY);Eso=r(uIr,"plbart"),uIr.forEach(t),yso=r(iMe," \u2014 "),SR=s(iMe,"A",{href:!0});var pIr=n(SR);wso=r(pIr,"PLBartModel"),pIr.forEach(t),Aso=r(iMe," (PLBart model)"),iMe.forEach(t),Lso=i(C),Fu=s(C,"LI",{});var dMe=n(Fu);GY=s(dMe,"STRONG",{});var _Ir=n(GY);Bso=r(_Ir,"poolformer"),_Ir.forEach(t),xso=r(dMe," \u2014 "),PR=s(dMe,"A",{href:!0});var bIr=n(PR);kso=r(bIr,"PoolFormerModel"),bIr.forEach(t),Rso=r(dMe," (PoolFormer model)"),dMe.forEach(t),Sso=i(C),Cu=s(C,"LI",{});var cMe=n(Cu);OY=s(cMe,"STRONG",{});var vIr=n(OY);Pso=r(vIr,"prophetnet"),vIr.forEach(t),$so=r(cMe," \u2014 "),$R=s(cMe,"A",{href:!0});var TIr=n($R);Iso=r(TIr,"ProphetNetModel"),TIr.forEach(t),Dso=r(cMe," (ProphetNet model)"),cMe.forEach(t),jso=i(C),Mu=s(C,"LI",{});var mMe=n(Mu);XY=s(mMe,"STRONG",{});var FIr=n(XY);Nso=r(FIr,"qdqbert"),FIr.forEach(t),qso=r(mMe," \u2014 "),IR=s(mMe,"A",{href:!0});var CIr=n(IR);Gso=r(CIr,"QDQBertModel"),CIr.forEach(t),Oso=r(mMe," (QDQBert model)"),mMe.forEach(t),Xso=i(C),Eu=s(C,"LI",{});var fMe=n(Eu);VY=s(fMe,"STRONG",{});var MIr=n(VY);Vso=r(MIr,"reformer"),MIr.forEach(t),zso=r(fMe," \u2014 "),DR=s(fMe,"A",{href:!0});var EIr=n(DR);Wso=r(EIr,"ReformerModel"),EIr.forEach(t),Qso=r(fMe," (Reformer model)"),fMe.forEach(t),Hso=i(C),yu=s(C,"LI",{});var gMe=n(yu);zY=s(gMe,"STRONG",{});var yIr=n(zY);Uso=r(yIr,"rembert"),yIr.forEach(t),Jso=r(gMe," \u2014 "),jR=s(gMe,"A",{href:!0});var wIr=n(jR);Yso=r(wIr,"RemBertModel"),wIr.forEach(t),Kso=r(gMe," (RemBERT model)"),gMe.forEach(t),Zso=i(C),wu=s(C,"LI",{});var hMe=n(wu);WY=s(hMe,"STRONG",{});var AIr=n(WY);eno=r(AIr,"retribert"),AIr.forEach(t),ono=r(hMe," \u2014 "),NR=s(hMe,"A",{href:!0});var LIr=n(NR);rno=r(LIr,"RetriBertModel"),LIr.forEach(t),tno=r(hMe," (RetriBERT model)"),hMe.forEach(t),ano=i(C),Au=s(C,"LI",{});var uMe=n(Au);QY=s(uMe,"STRONG",{});var BIr=n(QY);sno=r(BIr,"roberta"),BIr.forEach(t),nno=r(uMe," \u2014 "),qR=s(uMe,"A",{href:!0});var xIr=n(qR);lno=r(xIr,"RobertaModel"),xIr.forEach(t),ino=r(uMe," (RoBERTa model)"),uMe.forEach(t),dno=i(C),Lu=s(C,"LI",{});var pMe=n(Lu);HY=s(pMe,"STRONG",{});var kIr=n(HY);cno=r(kIr,"roformer"),kIr.forEach(t),mno=r(pMe," \u2014 "),GR=s(pMe,"A",{href:!0});var RIr=n(GR);fno=r(RIr,"RoFormerModel"),RIr.forEach(t),gno=r(pMe," (RoFormer model)"),pMe.forEach(t),hno=i(C),Bu=s(C,"LI",{});var _Me=n(Bu);UY=s(_Me,"STRONG",{});var SIr=n(UY);uno=r(SIr,"segformer"),SIr.forEach(t),pno=r(_Me," \u2014 "),OR=s(_Me,"A",{href:!0});var PIr=n(OR);_no=r(PIr,"SegformerModel"),PIr.forEach(t),bno=r(_Me," (SegFormer model)"),_Me.forEach(t),vno=i(C),xu=s(C,"LI",{});var bMe=n(xu);JY=s(bMe,"STRONG",{});var $Ir=n(JY);Tno=r($Ir,"sew"),$Ir.forEach(t),Fno=r(bMe," \u2014 "),XR=s(bMe,"A",{href:!0});var IIr=n(XR);Cno=r(IIr,"SEWModel"),IIr.forEach(t),Mno=r(bMe," (SEW model)"),bMe.forEach(t),Eno=i(C),ku=s(C,"LI",{});var vMe=n(ku);YY=s(vMe,"STRONG",{});var DIr=n(YY);yno=r(DIr,"sew-d"),DIr.forEach(t),wno=r(vMe," \u2014 "),VR=s(vMe,"A",{href:!0});var jIr=n(VR);Ano=r(jIr,"SEWDModel"),jIr.forEach(t),Lno=r(vMe," (SEW-D model)"),vMe.forEach(t),Bno=i(C),Ru=s(C,"LI",{});var TMe=n(Ru);KY=s(TMe,"STRONG",{});var NIr=n(KY);xno=r(NIr,"speech_to_text"),NIr.forEach(t),kno=r(TMe," \u2014 "),zR=s(TMe,"A",{href:!0});var qIr=n(zR);Rno=r(qIr,"Speech2TextModel"),qIr.forEach(t),Sno=r(TMe," (Speech2Text model)"),TMe.forEach(t),Pno=i(C),Su=s(C,"LI",{});var FMe=n(Su);ZY=s(FMe,"STRONG",{});var GIr=n(ZY);$no=r(GIr,"splinter"),GIr.forEach(t),Ino=r(FMe," \u2014 "),WR=s(FMe,"A",{href:!0});var OIr=n(WR);Dno=r(OIr,"SplinterModel"),OIr.forEach(t),jno=r(FMe," (Splinter model)"),FMe.forEach(t),Nno=i(C),Pu=s(C,"LI",{});var CMe=n(Pu);eK=s(CMe,"STRONG",{});var XIr=n(eK);qno=r(XIr,"squeezebert"),XIr.forEach(t),Gno=r(CMe," \u2014 "),QR=s(CMe,"A",{href:!0});var VIr=n(QR);Ono=r(VIr,"SqueezeBertModel"),VIr.forEach(t),Xno=r(CMe," (SqueezeBERT model)"),CMe.forEach(t),Vno=i(C),$u=s(C,"LI",{});var MMe=n($u);oK=s(MMe,"STRONG",{});var zIr=n(oK);zno=r(zIr,"swin"),zIr.forEach(t),Wno=r(MMe," \u2014 "),HR=s(MMe,"A",{href:!0});var WIr=n(HR);Qno=r(WIr,"SwinModel"),WIr.forEach(t),Hno=r(MMe," (Swin model)"),MMe.forEach(t),Uno=i(C),Iu=s(C,"LI",{});var EMe=n(Iu);rK=s(EMe,"STRONG",{});var QIr=n(rK);Jno=r(QIr,"t5"),QIr.forEach(t),Yno=r(EMe," \u2014 "),UR=s(EMe,"A",{href:!0});var HIr=n(UR);Kno=r(HIr,"T5Model"),HIr.forEach(t),Zno=r(EMe," (T5 model)"),EMe.forEach(t),elo=i(C),Du=s(C,"LI",{});var yMe=n(Du);tK=s(yMe,"STRONG",{});var UIr=n(tK);olo=r(UIr,"tapas"),UIr.forEach(t),rlo=r(yMe," \u2014 "),JR=s(yMe,"A",{href:!0});var JIr=n(JR);tlo=r(JIr,"TapasModel"),JIr.forEach(t),alo=r(yMe," (TAPAS model)"),yMe.forEach(t),slo=i(C),ju=s(C,"LI",{});var wMe=n(ju);aK=s(wMe,"STRONG",{});var YIr=n(aK);nlo=r(YIr,"transfo-xl"),YIr.forEach(t),llo=r(wMe," \u2014 "),YR=s(wMe,"A",{href:!0});var KIr=n(YR);ilo=r(KIr,"TransfoXLModel"),KIr.forEach(t),dlo=r(wMe," (Transformer-XL model)"),wMe.forEach(t),clo=i(C),Nu=s(C,"LI",{});var AMe=n(Nu);sK=s(AMe,"STRONG",{});var ZIr=n(sK);mlo=r(ZIr,"unispeech"),ZIr.forEach(t),flo=r(AMe," \u2014 "),KR=s(AMe,"A",{href:!0});var eDr=n(KR);glo=r(eDr,"UniSpeechModel"),eDr.forEach(t),hlo=r(AMe," (UniSpeech model)"),AMe.forEach(t),ulo=i(C),qu=s(C,"LI",{});var LMe=n(qu);nK=s(LMe,"STRONG",{});var oDr=n(nK);plo=r(oDr,"unispeech-sat"),oDr.forEach(t),_lo=r(LMe," \u2014 "),ZR=s(LMe,"A",{href:!0});var rDr=n(ZR);blo=r(rDr,"UniSpeechSatModel"),rDr.forEach(t),vlo=r(LMe," (UniSpeechSat model)"),LMe.forEach(t),Tlo=i(C),Gu=s(C,"LI",{});var BMe=n(Gu);lK=s(BMe,"STRONG",{});var tDr=n(lK);Flo=r(tDr,"vilt"),tDr.forEach(t),Clo=r(BMe," \u2014 "),eS=s(BMe,"A",{href:!0});var aDr=n(eS);Mlo=r(aDr,"ViltModel"),aDr.forEach(t),Elo=r(BMe," (ViLT model)"),BMe.forEach(t),ylo=i(C),Ou=s(C,"LI",{});var xMe=n(Ou);iK=s(xMe,"STRONG",{});var sDr=n(iK);wlo=r(sDr,"vision-text-dual-encoder"),sDr.forEach(t),Alo=r(xMe," \u2014 "),oS=s(xMe,"A",{href:!0});var nDr=n(oS);Llo=r(nDr,"VisionTextDualEncoderModel"),nDr.forEach(t),Blo=r(xMe," (VisionTextDualEncoder model)"),xMe.forEach(t),xlo=i(C),Xu=s(C,"LI",{});var kMe=n(Xu);dK=s(kMe,"STRONG",{});var lDr=n(dK);klo=r(lDr,"visual_bert"),lDr.forEach(t),Rlo=r(kMe," \u2014 "),rS=s(kMe,"A",{href:!0});var iDr=n(rS);Slo=r(iDr,"VisualBertModel"),iDr.forEach(t),Plo=r(kMe," (VisualBert model)"),kMe.forEach(t),$lo=i(C),Vu=s(C,"LI",{});var RMe=n(Vu);cK=s(RMe,"STRONG",{});var dDr=n(cK);Ilo=r(dDr,"vit"),dDr.forEach(t),Dlo=r(RMe," \u2014 "),tS=s(RMe,"A",{href:!0});var cDr=n(tS);jlo=r(cDr,"ViTModel"),cDr.forEach(t),Nlo=r(RMe," (ViT model)"),RMe.forEach(t),qlo=i(C),zu=s(C,"LI",{});var SMe=n(zu);mK=s(SMe,"STRONG",{});var mDr=n(mK);Glo=r(mDr,"vit_mae"),mDr.forEach(t),Olo=r(SMe," \u2014 "),aS=s(SMe,"A",{href:!0});var fDr=n(aS);Xlo=r(fDr,"ViTMAEModel"),fDr.forEach(t),Vlo=r(SMe," (ViTMAE model)"),SMe.forEach(t),zlo=i(C),Wu=s(C,"LI",{});var PMe=n(Wu);fK=s(PMe,"STRONG",{});var gDr=n(fK);Wlo=r(gDr,"wav2vec2"),gDr.forEach(t),Qlo=r(PMe," \u2014 "),sS=s(PMe,"A",{href:!0});var hDr=n(sS);Hlo=r(hDr,"Wav2Vec2Model"),hDr.forEach(t),Ulo=r(PMe," (Wav2Vec2 model)"),PMe.forEach(t),Jlo=i(C),Qu=s(C,"LI",{});var $Me=n(Qu);gK=s($Me,"STRONG",{});var uDr=n(gK);Ylo=r(uDr,"wavlm"),uDr.forEach(t),Klo=r($Me," \u2014 "),nS=s($Me,"A",{href:!0});var pDr=n(nS);Zlo=r(pDr,"WavLMModel"),pDr.forEach(t),eio=r($Me," (WavLM model)"),$Me.forEach(t),oio=i(C),Hu=s(C,"LI",{});var IMe=n(Hu);hK=s(IMe,"STRONG",{});var _Dr=n(hK);rio=r(_Dr,"xglm"),_Dr.forEach(t),tio=r(IMe," \u2014 "),lS=s(IMe,"A",{href:!0});var bDr=n(lS);aio=r(bDr,"XGLMModel"),bDr.forEach(t),sio=r(IMe," (XGLM model)"),IMe.forEach(t),nio=i(C),Uu=s(C,"LI",{});var DMe=n(Uu);uK=s(DMe,"STRONG",{});var vDr=n(uK);lio=r(vDr,"xlm"),vDr.forEach(t),iio=r(DMe," \u2014 "),iS=s(DMe,"A",{href:!0});var TDr=n(iS);dio=r(TDr,"XLMModel"),TDr.forEach(t),cio=r(DMe," (XLM model)"),DMe.forEach(t),mio=i(C),Ju=s(C,"LI",{});var jMe=n(Ju);pK=s(jMe,"STRONG",{});var FDr=n(pK);fio=r(FDr,"xlm-prophetnet"),FDr.forEach(t),gio=r(jMe," \u2014 "),dS=s(jMe,"A",{href:!0});var CDr=n(dS);hio=r(CDr,"XLMProphetNetModel"),CDr.forEach(t),uio=r(jMe," (XLMProphetNet model)"),jMe.forEach(t),pio=i(C),Yu=s(C,"LI",{});var NMe=n(Yu);_K=s(NMe,"STRONG",{});var MDr=n(_K);_io=r(MDr,"xlm-roberta"),MDr.forEach(t),bio=r(NMe," \u2014 "),cS=s(NMe,"A",{href:!0});var EDr=n(cS);vio=r(EDr,"XLMRobertaModel"),EDr.forEach(t),Tio=r(NMe," (XLM-RoBERTa model)"),NMe.forEach(t),Fio=i(C),Ku=s(C,"LI",{});var qMe=n(Ku);bK=s(qMe,"STRONG",{});var yDr=n(bK);Cio=r(yDr,"xlm-roberta-xl"),yDr.forEach(t),Mio=r(qMe," \u2014 "),mS=s(qMe,"A",{href:!0});var wDr=n(mS);Eio=r(wDr,"XLMRobertaXLModel"),wDr.forEach(t),yio=r(qMe," (XLM-RoBERTa-XL model)"),qMe.forEach(t),wio=i(C),Zu=s(C,"LI",{});var GMe=n(Zu);vK=s(GMe,"STRONG",{});var ADr=n(vK);Aio=r(ADr,"xlnet"),ADr.forEach(t),Lio=r(GMe," \u2014 "),fS=s(GMe,"A",{href:!0});var LDr=n(fS);Bio=r(LDr,"XLNetModel"),LDr.forEach(t),xio=r(GMe," (XLNet model)"),GMe.forEach(t),kio=i(C),ep=s(C,"LI",{});var OMe=n(ep);TK=s(OMe,"STRONG",{});var BDr=n(TK);Rio=r(BDr,"yoso"),BDr.forEach(t),Sio=r(OMe," \u2014 "),gS=s(OMe,"A",{href:!0});var xDr=n(gS);Pio=r(xDr,"YosoModel"),xDr.forEach(t),$io=r(OMe," (YOSO model)"),OMe.forEach(t),C.forEach(t),Iio=i($t),op=s($t,"P",{});var XMe=n(op);Dio=r(XMe,"The model is set in evaluation mode by default using "),FK=s(XMe,"CODE",{});var kDr=n(FK);jio=r(kDr,"model.eval()"),kDr.forEach(t),Nio=r(XMe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),CK=s(XMe,"CODE",{});var RDr=n(CK);qio=r(RDr,"model.train()"),RDr.forEach(t),XMe.forEach(t),Gio=i($t),MK=s($t,"P",{});var SDr=n(MK);Oio=r(SDr,"Examples:"),SDr.forEach(t),Xio=i($t),f(ny.$$.fragment,$t),$t.forEach(t),qn.forEach(t),R9e=i(c),zi=s(c,"H2",{class:!0});var Nxe=n(zi);rp=s(Nxe,"A",{id:!0,class:!0,href:!0});var PDr=n(rp);EK=s(PDr,"SPAN",{});var $Dr=n(EK);f(ly.$$.fragment,$Dr),$Dr.forEach(t),PDr.forEach(t),Vio=i(Nxe),yK=s(Nxe,"SPAN",{});var IDr=n(yK);zio=r(IDr,"AutoModelForPreTraining"),IDr.forEach(t),Nxe.forEach(t),S9e=i(c),Ho=s(c,"DIV",{class:!0});var On=n(Ho);f(iy.$$.fragment,On),Wio=i(On),Wi=s(On,"P",{});var kV=n(Wi);Qio=r(kV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),wK=s(kV,"CODE",{});var DDr=n(wK);Hio=r(DDr,"from_pretrained()"),DDr.forEach(t),Uio=r(kV,"class method or the "),AK=s(kV,"CODE",{});var jDr=n(AK);Jio=r(jDr,"from_config()"),jDr.forEach(t),Yio=r(kV,`class
method.`),kV.forEach(t),Kio=i(On),dy=s(On,"P",{});var qxe=n(dy);Zio=r(qxe,"This class cannot be instantiated directly using "),LK=s(qxe,"CODE",{});var NDr=n(LK);edo=r(NDr,"__init__()"),NDr.forEach(t),odo=r(qxe," (throws an error)."),qxe.forEach(t),rdo=i(On),Gr=s(On,"DIV",{class:!0});var Xn=n(Gr);f(cy.$$.fragment,Xn),tdo=i(Xn),BK=s(Xn,"P",{});var qDr=n(BK);ado=r(qDr,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),qDr.forEach(t),sdo=i(Xn),Qi=s(Xn,"P",{});var RV=n(Qi);ndo=r(RV,`Note:
Loading a model from its configuration file does `),xK=s(RV,"STRONG",{});var GDr=n(xK);ldo=r(GDr,"not"),GDr.forEach(t),ido=r(RV,` load the model weights. It only affects the
model\u2019s configuration. Use `),kK=s(RV,"CODE",{});var ODr=n(kK);ddo=r(ODr,"from_pretrained()"),ODr.forEach(t),cdo=r(RV,"to load the model weights."),RV.forEach(t),mdo=i(Xn),RK=s(Xn,"P",{});var XDr=n(RK);fdo=r(XDr,"Examples:"),XDr.forEach(t),gdo=i(Xn),f(my.$$.fragment,Xn),Xn.forEach(t),hdo=i(On),Se=s(On,"DIV",{class:!0});var It=n(Se);f(fy.$$.fragment,It),udo=i(It),SK=s(It,"P",{});var VDr=n(SK);pdo=r(VDr,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),VDr.forEach(t),_do=i(It),qa=s(It,"P",{});var y3=n(qa);bdo=r(y3,"The model class to instantiate is selected based on the "),PK=s(y3,"CODE",{});var zDr=n(PK);vdo=r(zDr,"model_type"),zDr.forEach(t),Tdo=r(y3,` property of the config object (either
passed as an argument or loaded from `),$K=s(y3,"CODE",{});var WDr=n($K);Fdo=r(WDr,"pretrained_model_name_or_path"),WDr.forEach(t),Cdo=r(y3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),IK=s(y3,"CODE",{});var QDr=n(IK);Mdo=r(QDr,"pretrained_model_name_or_path"),QDr.forEach(t),Edo=r(y3,":"),y3.forEach(t),ydo=i(It),k=s(It,"UL",{});var S=n(k);tp=s(S,"LI",{});var VMe=n(tp);DK=s(VMe,"STRONG",{});var HDr=n(DK);wdo=r(HDr,"albert"),HDr.forEach(t),Ado=r(VMe," \u2014 "),hS=s(VMe,"A",{href:!0});var UDr=n(hS);Ldo=r(UDr,"AlbertForPreTraining"),UDr.forEach(t),Bdo=r(VMe," (ALBERT model)"),VMe.forEach(t),xdo=i(S),ap=s(S,"LI",{});var zMe=n(ap);jK=s(zMe,"STRONG",{});var JDr=n(jK);kdo=r(JDr,"bart"),JDr.forEach(t),Rdo=r(zMe," \u2014 "),uS=s(zMe,"A",{href:!0});var YDr=n(uS);Sdo=r(YDr,"BartForConditionalGeneration"),YDr.forEach(t),Pdo=r(zMe," (BART model)"),zMe.forEach(t),$do=i(S),sp=s(S,"LI",{});var WMe=n(sp);NK=s(WMe,"STRONG",{});var KDr=n(NK);Ido=r(KDr,"bert"),KDr.forEach(t),Ddo=r(WMe," \u2014 "),pS=s(WMe,"A",{href:!0});var ZDr=n(pS);jdo=r(ZDr,"BertForPreTraining"),ZDr.forEach(t),Ndo=r(WMe," (BERT model)"),WMe.forEach(t),qdo=i(S),np=s(S,"LI",{});var QMe=n(np);qK=s(QMe,"STRONG",{});var ejr=n(qK);Gdo=r(ejr,"big_bird"),ejr.forEach(t),Odo=r(QMe," \u2014 "),_S=s(QMe,"A",{href:!0});var ojr=n(_S);Xdo=r(ojr,"BigBirdForPreTraining"),ojr.forEach(t),Vdo=r(QMe," (BigBird model)"),QMe.forEach(t),zdo=i(S),lp=s(S,"LI",{});var HMe=n(lp);GK=s(HMe,"STRONG",{});var rjr=n(GK);Wdo=r(rjr,"camembert"),rjr.forEach(t),Qdo=r(HMe," \u2014 "),bS=s(HMe,"A",{href:!0});var tjr=n(bS);Hdo=r(tjr,"CamembertForMaskedLM"),tjr.forEach(t),Udo=r(HMe," (CamemBERT model)"),HMe.forEach(t),Jdo=i(S),ip=s(S,"LI",{});var UMe=n(ip);OK=s(UMe,"STRONG",{});var ajr=n(OK);Ydo=r(ajr,"ctrl"),ajr.forEach(t),Kdo=r(UMe," \u2014 "),vS=s(UMe,"A",{href:!0});var sjr=n(vS);Zdo=r(sjr,"CTRLLMHeadModel"),sjr.forEach(t),eco=r(UMe," (CTRL model)"),UMe.forEach(t),oco=i(S),dp=s(S,"LI",{});var JMe=n(dp);XK=s(JMe,"STRONG",{});var njr=n(XK);rco=r(njr,"data2vec-text"),njr.forEach(t),tco=r(JMe," \u2014 "),TS=s(JMe,"A",{href:!0});var ljr=n(TS);aco=r(ljr,"Data2VecTextForMaskedLM"),ljr.forEach(t),sco=r(JMe," (Data2VecText model)"),JMe.forEach(t),nco=i(S),cp=s(S,"LI",{});var YMe=n(cp);VK=s(YMe,"STRONG",{});var ijr=n(VK);lco=r(ijr,"deberta"),ijr.forEach(t),ico=r(YMe," \u2014 "),FS=s(YMe,"A",{href:!0});var djr=n(FS);dco=r(djr,"DebertaForMaskedLM"),djr.forEach(t),cco=r(YMe," (DeBERTa model)"),YMe.forEach(t),mco=i(S),mp=s(S,"LI",{});var KMe=n(mp);zK=s(KMe,"STRONG",{});var cjr=n(zK);fco=r(cjr,"deberta-v2"),cjr.forEach(t),gco=r(KMe," \u2014 "),CS=s(KMe,"A",{href:!0});var mjr=n(CS);hco=r(mjr,"DebertaV2ForMaskedLM"),mjr.forEach(t),uco=r(KMe," (DeBERTa-v2 model)"),KMe.forEach(t),pco=i(S),fp=s(S,"LI",{});var ZMe=n(fp);WK=s(ZMe,"STRONG",{});var fjr=n(WK);_co=r(fjr,"distilbert"),fjr.forEach(t),bco=r(ZMe," \u2014 "),MS=s(ZMe,"A",{href:!0});var gjr=n(MS);vco=r(gjr,"DistilBertForMaskedLM"),gjr.forEach(t),Tco=r(ZMe," (DistilBERT model)"),ZMe.forEach(t),Fco=i(S),gp=s(S,"LI",{});var eEe=n(gp);QK=s(eEe,"STRONG",{});var hjr=n(QK);Cco=r(hjr,"electra"),hjr.forEach(t),Mco=r(eEe," \u2014 "),ES=s(eEe,"A",{href:!0});var ujr=n(ES);Eco=r(ujr,"ElectraForPreTraining"),ujr.forEach(t),yco=r(eEe," (ELECTRA model)"),eEe.forEach(t),wco=i(S),hp=s(S,"LI",{});var oEe=n(hp);HK=s(oEe,"STRONG",{});var pjr=n(HK);Aco=r(pjr,"flaubert"),pjr.forEach(t),Lco=r(oEe," \u2014 "),yS=s(oEe,"A",{href:!0});var _jr=n(yS);Bco=r(_jr,"FlaubertWithLMHeadModel"),_jr.forEach(t),xco=r(oEe," (FlauBERT model)"),oEe.forEach(t),kco=i(S),up=s(S,"LI",{});var rEe=n(up);UK=s(rEe,"STRONG",{});var bjr=n(UK);Rco=r(bjr,"fnet"),bjr.forEach(t),Sco=r(rEe," \u2014 "),wS=s(rEe,"A",{href:!0});var vjr=n(wS);Pco=r(vjr,"FNetForPreTraining"),vjr.forEach(t),$co=r(rEe," (FNet model)"),rEe.forEach(t),Ico=i(S),pp=s(S,"LI",{});var tEe=n(pp);JK=s(tEe,"STRONG",{});var Tjr=n(JK);Dco=r(Tjr,"fsmt"),Tjr.forEach(t),jco=r(tEe," \u2014 "),AS=s(tEe,"A",{href:!0});var Fjr=n(AS);Nco=r(Fjr,"FSMTForConditionalGeneration"),Fjr.forEach(t),qco=r(tEe," (FairSeq Machine-Translation model)"),tEe.forEach(t),Gco=i(S),_p=s(S,"LI",{});var aEe=n(_p);YK=s(aEe,"STRONG",{});var Cjr=n(YK);Oco=r(Cjr,"funnel"),Cjr.forEach(t),Xco=r(aEe," \u2014 "),LS=s(aEe,"A",{href:!0});var Mjr=n(LS);Vco=r(Mjr,"FunnelForPreTraining"),Mjr.forEach(t),zco=r(aEe," (Funnel Transformer model)"),aEe.forEach(t),Wco=i(S),bp=s(S,"LI",{});var sEe=n(bp);KK=s(sEe,"STRONG",{});var Ejr=n(KK);Qco=r(Ejr,"gpt2"),Ejr.forEach(t),Hco=r(sEe," \u2014 "),BS=s(sEe,"A",{href:!0});var yjr=n(BS);Uco=r(yjr,"GPT2LMHeadModel"),yjr.forEach(t),Jco=r(sEe," (OpenAI GPT-2 model)"),sEe.forEach(t),Yco=i(S),vp=s(S,"LI",{});var nEe=n(vp);ZK=s(nEe,"STRONG",{});var wjr=n(ZK);Kco=r(wjr,"ibert"),wjr.forEach(t),Zco=r(nEe," \u2014 "),xS=s(nEe,"A",{href:!0});var Ajr=n(xS);emo=r(Ajr,"IBertForMaskedLM"),Ajr.forEach(t),omo=r(nEe," (I-BERT model)"),nEe.forEach(t),rmo=i(S),Tp=s(S,"LI",{});var lEe=n(Tp);eZ=s(lEe,"STRONG",{});var Ljr=n(eZ);tmo=r(Ljr,"layoutlm"),Ljr.forEach(t),amo=r(lEe," \u2014 "),kS=s(lEe,"A",{href:!0});var Bjr=n(kS);smo=r(Bjr,"LayoutLMForMaskedLM"),Bjr.forEach(t),nmo=r(lEe," (LayoutLM model)"),lEe.forEach(t),lmo=i(S),Fp=s(S,"LI",{});var iEe=n(Fp);oZ=s(iEe,"STRONG",{});var xjr=n(oZ);imo=r(xjr,"longformer"),xjr.forEach(t),dmo=r(iEe," \u2014 "),RS=s(iEe,"A",{href:!0});var kjr=n(RS);cmo=r(kjr,"LongformerForMaskedLM"),kjr.forEach(t),mmo=r(iEe," (Longformer model)"),iEe.forEach(t),fmo=i(S),Cp=s(S,"LI",{});var dEe=n(Cp);rZ=s(dEe,"STRONG",{});var Rjr=n(rZ);gmo=r(Rjr,"lxmert"),Rjr.forEach(t),hmo=r(dEe," \u2014 "),SS=s(dEe,"A",{href:!0});var Sjr=n(SS);umo=r(Sjr,"LxmertForPreTraining"),Sjr.forEach(t),pmo=r(dEe," (LXMERT model)"),dEe.forEach(t),_mo=i(S),Mp=s(S,"LI",{});var cEe=n(Mp);tZ=s(cEe,"STRONG",{});var Pjr=n(tZ);bmo=r(Pjr,"megatron-bert"),Pjr.forEach(t),vmo=r(cEe," \u2014 "),PS=s(cEe,"A",{href:!0});var $jr=n(PS);Tmo=r($jr,"MegatronBertForPreTraining"),$jr.forEach(t),Fmo=r(cEe," (MegatronBert model)"),cEe.forEach(t),Cmo=i(S),Ep=s(S,"LI",{});var mEe=n(Ep);aZ=s(mEe,"STRONG",{});var Ijr=n(aZ);Mmo=r(Ijr,"mobilebert"),Ijr.forEach(t),Emo=r(mEe," \u2014 "),$S=s(mEe,"A",{href:!0});var Djr=n($S);ymo=r(Djr,"MobileBertForPreTraining"),Djr.forEach(t),wmo=r(mEe," (MobileBERT model)"),mEe.forEach(t),Amo=i(S),yp=s(S,"LI",{});var fEe=n(yp);sZ=s(fEe,"STRONG",{});var jjr=n(sZ);Lmo=r(jjr,"mpnet"),jjr.forEach(t),Bmo=r(fEe," \u2014 "),IS=s(fEe,"A",{href:!0});var Njr=n(IS);xmo=r(Njr,"MPNetForMaskedLM"),Njr.forEach(t),kmo=r(fEe," (MPNet model)"),fEe.forEach(t),Rmo=i(S),wp=s(S,"LI",{});var gEe=n(wp);nZ=s(gEe,"STRONG",{});var qjr=n(nZ);Smo=r(qjr,"openai-gpt"),qjr.forEach(t),Pmo=r(gEe," \u2014 "),DS=s(gEe,"A",{href:!0});var Gjr=n(DS);$mo=r(Gjr,"OpenAIGPTLMHeadModel"),Gjr.forEach(t),Imo=r(gEe," (OpenAI GPT model)"),gEe.forEach(t),Dmo=i(S),Ap=s(S,"LI",{});var hEe=n(Ap);lZ=s(hEe,"STRONG",{});var Ojr=n(lZ);jmo=r(Ojr,"retribert"),Ojr.forEach(t),Nmo=r(hEe," \u2014 "),jS=s(hEe,"A",{href:!0});var Xjr=n(jS);qmo=r(Xjr,"RetriBertModel"),Xjr.forEach(t),Gmo=r(hEe," (RetriBERT model)"),hEe.forEach(t),Omo=i(S),Lp=s(S,"LI",{});var uEe=n(Lp);iZ=s(uEe,"STRONG",{});var Vjr=n(iZ);Xmo=r(Vjr,"roberta"),Vjr.forEach(t),Vmo=r(uEe," \u2014 "),NS=s(uEe,"A",{href:!0});var zjr=n(NS);zmo=r(zjr,"RobertaForMaskedLM"),zjr.forEach(t),Wmo=r(uEe," (RoBERTa model)"),uEe.forEach(t),Qmo=i(S),Bp=s(S,"LI",{});var pEe=n(Bp);dZ=s(pEe,"STRONG",{});var Wjr=n(dZ);Hmo=r(Wjr,"squeezebert"),Wjr.forEach(t),Umo=r(pEe," \u2014 "),qS=s(pEe,"A",{href:!0});var Qjr=n(qS);Jmo=r(Qjr,"SqueezeBertForMaskedLM"),Qjr.forEach(t),Ymo=r(pEe," (SqueezeBERT model)"),pEe.forEach(t),Kmo=i(S),xp=s(S,"LI",{});var _Ee=n(xp);cZ=s(_Ee,"STRONG",{});var Hjr=n(cZ);Zmo=r(Hjr,"t5"),Hjr.forEach(t),efo=r(_Ee," \u2014 "),GS=s(_Ee,"A",{href:!0});var Ujr=n(GS);ofo=r(Ujr,"T5ForConditionalGeneration"),Ujr.forEach(t),rfo=r(_Ee," (T5 model)"),_Ee.forEach(t),tfo=i(S),kp=s(S,"LI",{});var bEe=n(kp);mZ=s(bEe,"STRONG",{});var Jjr=n(mZ);afo=r(Jjr,"tapas"),Jjr.forEach(t),sfo=r(bEe," \u2014 "),OS=s(bEe,"A",{href:!0});var Yjr=n(OS);nfo=r(Yjr,"TapasForMaskedLM"),Yjr.forEach(t),lfo=r(bEe," (TAPAS model)"),bEe.forEach(t),ifo=i(S),Rp=s(S,"LI",{});var vEe=n(Rp);fZ=s(vEe,"STRONG",{});var Kjr=n(fZ);dfo=r(Kjr,"transfo-xl"),Kjr.forEach(t),cfo=r(vEe," \u2014 "),XS=s(vEe,"A",{href:!0});var Zjr=n(XS);mfo=r(Zjr,"TransfoXLLMHeadModel"),Zjr.forEach(t),ffo=r(vEe," (Transformer-XL model)"),vEe.forEach(t),gfo=i(S),Sp=s(S,"LI",{});var TEe=n(Sp);gZ=s(TEe,"STRONG",{});var eNr=n(gZ);hfo=r(eNr,"unispeech"),eNr.forEach(t),ufo=r(TEe," \u2014 "),VS=s(TEe,"A",{href:!0});var oNr=n(VS);pfo=r(oNr,"UniSpeechForPreTraining"),oNr.forEach(t),_fo=r(TEe," (UniSpeech model)"),TEe.forEach(t),bfo=i(S),Pp=s(S,"LI",{});var FEe=n(Pp);hZ=s(FEe,"STRONG",{});var rNr=n(hZ);vfo=r(rNr,"unispeech-sat"),rNr.forEach(t),Tfo=r(FEe," \u2014 "),zS=s(FEe,"A",{href:!0});var tNr=n(zS);Ffo=r(tNr,"UniSpeechSatForPreTraining"),tNr.forEach(t),Cfo=r(FEe," (UniSpeechSat model)"),FEe.forEach(t),Mfo=i(S),$p=s(S,"LI",{});var CEe=n($p);uZ=s(CEe,"STRONG",{});var aNr=n(uZ);Efo=r(aNr,"visual_bert"),aNr.forEach(t),yfo=r(CEe," \u2014 "),WS=s(CEe,"A",{href:!0});var sNr=n(WS);wfo=r(sNr,"VisualBertForPreTraining"),sNr.forEach(t),Afo=r(CEe," (VisualBert model)"),CEe.forEach(t),Lfo=i(S),Ip=s(S,"LI",{});var MEe=n(Ip);pZ=s(MEe,"STRONG",{});var nNr=n(pZ);Bfo=r(nNr,"vit_mae"),nNr.forEach(t),xfo=r(MEe," \u2014 "),QS=s(MEe,"A",{href:!0});var lNr=n(QS);kfo=r(lNr,"ViTMAEForPreTraining"),lNr.forEach(t),Rfo=r(MEe," (ViTMAE model)"),MEe.forEach(t),Sfo=i(S),Dp=s(S,"LI",{});var EEe=n(Dp);_Z=s(EEe,"STRONG",{});var iNr=n(_Z);Pfo=r(iNr,"wav2vec2"),iNr.forEach(t),$fo=r(EEe," \u2014 "),HS=s(EEe,"A",{href:!0});var dNr=n(HS);Ifo=r(dNr,"Wav2Vec2ForPreTraining"),dNr.forEach(t),Dfo=r(EEe," (Wav2Vec2 model)"),EEe.forEach(t),jfo=i(S),jp=s(S,"LI",{});var yEe=n(jp);bZ=s(yEe,"STRONG",{});var cNr=n(bZ);Nfo=r(cNr,"xlm"),cNr.forEach(t),qfo=r(yEe," \u2014 "),US=s(yEe,"A",{href:!0});var mNr=n(US);Gfo=r(mNr,"XLMWithLMHeadModel"),mNr.forEach(t),Ofo=r(yEe," (XLM model)"),yEe.forEach(t),Xfo=i(S),Np=s(S,"LI",{});var wEe=n(Np);vZ=s(wEe,"STRONG",{});var fNr=n(vZ);Vfo=r(fNr,"xlm-roberta"),fNr.forEach(t),zfo=r(wEe," \u2014 "),JS=s(wEe,"A",{href:!0});var gNr=n(JS);Wfo=r(gNr,"XLMRobertaForMaskedLM"),gNr.forEach(t),Qfo=r(wEe," (XLM-RoBERTa model)"),wEe.forEach(t),Hfo=i(S),qp=s(S,"LI",{});var AEe=n(qp);TZ=s(AEe,"STRONG",{});var hNr=n(TZ);Ufo=r(hNr,"xlm-roberta-xl"),hNr.forEach(t),Jfo=r(AEe," \u2014 "),YS=s(AEe,"A",{href:!0});var uNr=n(YS);Yfo=r(uNr,"XLMRobertaXLForMaskedLM"),uNr.forEach(t),Kfo=r(AEe," (XLM-RoBERTa-XL model)"),AEe.forEach(t),Zfo=i(S),Gp=s(S,"LI",{});var LEe=n(Gp);FZ=s(LEe,"STRONG",{});var pNr=n(FZ);ego=r(pNr,"xlnet"),pNr.forEach(t),ogo=r(LEe," \u2014 "),KS=s(LEe,"A",{href:!0});var _Nr=n(KS);rgo=r(_Nr,"XLNetLMHeadModel"),_Nr.forEach(t),tgo=r(LEe," (XLNet model)"),LEe.forEach(t),S.forEach(t),ago=i(It),Op=s(It,"P",{});var BEe=n(Op);sgo=r(BEe,"The model is set in evaluation mode by default using "),CZ=s(BEe,"CODE",{});var bNr=n(CZ);ngo=r(bNr,"model.eval()"),bNr.forEach(t),lgo=r(BEe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),MZ=s(BEe,"CODE",{});var vNr=n(MZ);igo=r(vNr,"model.train()"),vNr.forEach(t),BEe.forEach(t),dgo=i(It),EZ=s(It,"P",{});var TNr=n(EZ);cgo=r(TNr,"Examples:"),TNr.forEach(t),mgo=i(It),f(gy.$$.fragment,It),It.forEach(t),On.forEach(t),P9e=i(c),Hi=s(c,"H2",{class:!0});var Gxe=n(Hi);Xp=s(Gxe,"A",{id:!0,class:!0,href:!0});var FNr=n(Xp);yZ=s(FNr,"SPAN",{});var CNr=n(yZ);f(hy.$$.fragment,CNr),CNr.forEach(t),FNr.forEach(t),fgo=i(Gxe),wZ=s(Gxe,"SPAN",{});var MNr=n(wZ);ggo=r(MNr,"AutoModelForCausalLM"),MNr.forEach(t),Gxe.forEach(t),$9e=i(c),Uo=s(c,"DIV",{class:!0});var Vn=n(Uo);f(uy.$$.fragment,Vn),hgo=i(Vn),Ui=s(Vn,"P",{});var SV=n(Ui);ugo=r(SV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),AZ=s(SV,"CODE",{});var ENr=n(AZ);pgo=r(ENr,"from_pretrained()"),ENr.forEach(t),_go=r(SV,"class method or the "),LZ=s(SV,"CODE",{});var yNr=n(LZ);bgo=r(yNr,"from_config()"),yNr.forEach(t),vgo=r(SV,`class
method.`),SV.forEach(t),Tgo=i(Vn),py=s(Vn,"P",{});var Oxe=n(py);Fgo=r(Oxe,"This class cannot be instantiated directly using "),BZ=s(Oxe,"CODE",{});var wNr=n(BZ);Cgo=r(wNr,"__init__()"),wNr.forEach(t),Mgo=r(Oxe," (throws an error)."),Oxe.forEach(t),Ego=i(Vn),Or=s(Vn,"DIV",{class:!0});var zn=n(Or);f(_y.$$.fragment,zn),ygo=i(zn),xZ=s(zn,"P",{});var ANr=n(xZ);wgo=r(ANr,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),ANr.forEach(t),Ago=i(zn),Ji=s(zn,"P",{});var PV=n(Ji);Lgo=r(PV,`Note:
Loading a model from its configuration file does `),kZ=s(PV,"STRONG",{});var LNr=n(kZ);Bgo=r(LNr,"not"),LNr.forEach(t),xgo=r(PV,` load the model weights. It only affects the
model\u2019s configuration. Use `),RZ=s(PV,"CODE",{});var BNr=n(RZ);kgo=r(BNr,"from_pretrained()"),BNr.forEach(t),Rgo=r(PV,"to load the model weights."),PV.forEach(t),Sgo=i(zn),SZ=s(zn,"P",{});var xNr=n(SZ);Pgo=r(xNr,"Examples:"),xNr.forEach(t),$go=i(zn),f(by.$$.fragment,zn),zn.forEach(t),Igo=i(Vn),Pe=s(Vn,"DIV",{class:!0});var Dt=n(Pe);f(vy.$$.fragment,Dt),Dgo=i(Dt),PZ=s(Dt,"P",{});var kNr=n(PZ);jgo=r(kNr,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),kNr.forEach(t),Ngo=i(Dt),Ga=s(Dt,"P",{});var w3=n(Ga);qgo=r(w3,"The model class to instantiate is selected based on the "),$Z=s(w3,"CODE",{});var RNr=n($Z);Ggo=r(RNr,"model_type"),RNr.forEach(t),Ogo=r(w3,` property of the config object (either
passed as an argument or loaded from `),IZ=s(w3,"CODE",{});var SNr=n(IZ);Xgo=r(SNr,"pretrained_model_name_or_path"),SNr.forEach(t),Vgo=r(w3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),DZ=s(w3,"CODE",{});var PNr=n(DZ);zgo=r(PNr,"pretrained_model_name_or_path"),PNr.forEach(t),Wgo=r(w3,":"),w3.forEach(t),Qgo=i(Dt),$=s(Dt,"UL",{});var D=n($);Vp=s(D,"LI",{});var xEe=n(Vp);jZ=s(xEe,"STRONG",{});var $Nr=n(jZ);Hgo=r($Nr,"bart"),$Nr.forEach(t),Ugo=r(xEe," \u2014 "),ZS=s(xEe,"A",{href:!0});var INr=n(ZS);Jgo=r(INr,"BartForCausalLM"),INr.forEach(t),Ygo=r(xEe," (BART model)"),xEe.forEach(t),Kgo=i(D),zp=s(D,"LI",{});var kEe=n(zp);NZ=s(kEe,"STRONG",{});var DNr=n(NZ);Zgo=r(DNr,"bert"),DNr.forEach(t),eho=r(kEe," \u2014 "),eP=s(kEe,"A",{href:!0});var jNr=n(eP);oho=r(jNr,"BertLMHeadModel"),jNr.forEach(t),rho=r(kEe," (BERT model)"),kEe.forEach(t),tho=i(D),Wp=s(D,"LI",{});var REe=n(Wp);qZ=s(REe,"STRONG",{});var NNr=n(qZ);aho=r(NNr,"bert-generation"),NNr.forEach(t),sho=r(REe," \u2014 "),oP=s(REe,"A",{href:!0});var qNr=n(oP);nho=r(qNr,"BertGenerationDecoder"),qNr.forEach(t),lho=r(REe," (Bert Generation model)"),REe.forEach(t),iho=i(D),Qp=s(D,"LI",{});var SEe=n(Qp);GZ=s(SEe,"STRONG",{});var GNr=n(GZ);dho=r(GNr,"big_bird"),GNr.forEach(t),cho=r(SEe," \u2014 "),rP=s(SEe,"A",{href:!0});var ONr=n(rP);mho=r(ONr,"BigBirdForCausalLM"),ONr.forEach(t),fho=r(SEe," (BigBird model)"),SEe.forEach(t),gho=i(D),Hp=s(D,"LI",{});var PEe=n(Hp);OZ=s(PEe,"STRONG",{});var XNr=n(OZ);hho=r(XNr,"bigbird_pegasus"),XNr.forEach(t),uho=r(PEe," \u2014 "),tP=s(PEe,"A",{href:!0});var VNr=n(tP);pho=r(VNr,"BigBirdPegasusForCausalLM"),VNr.forEach(t),_ho=r(PEe," (BigBirdPegasus model)"),PEe.forEach(t),bho=i(D),Up=s(D,"LI",{});var $Ee=n(Up);XZ=s($Ee,"STRONG",{});var zNr=n(XZ);vho=r(zNr,"blenderbot"),zNr.forEach(t),Tho=r($Ee," \u2014 "),aP=s($Ee,"A",{href:!0});var WNr=n(aP);Fho=r(WNr,"BlenderbotForCausalLM"),WNr.forEach(t),Cho=r($Ee," (Blenderbot model)"),$Ee.forEach(t),Mho=i(D),Jp=s(D,"LI",{});var IEe=n(Jp);VZ=s(IEe,"STRONG",{});var QNr=n(VZ);Eho=r(QNr,"blenderbot-small"),QNr.forEach(t),yho=r(IEe," \u2014 "),sP=s(IEe,"A",{href:!0});var HNr=n(sP);who=r(HNr,"BlenderbotSmallForCausalLM"),HNr.forEach(t),Aho=r(IEe," (BlenderbotSmall model)"),IEe.forEach(t),Lho=i(D),Yp=s(D,"LI",{});var DEe=n(Yp);zZ=s(DEe,"STRONG",{});var UNr=n(zZ);Bho=r(UNr,"camembert"),UNr.forEach(t),xho=r(DEe," \u2014 "),nP=s(DEe,"A",{href:!0});var JNr=n(nP);kho=r(JNr,"CamembertForCausalLM"),JNr.forEach(t),Rho=r(DEe," (CamemBERT model)"),DEe.forEach(t),Sho=i(D),Kp=s(D,"LI",{});var jEe=n(Kp);WZ=s(jEe,"STRONG",{});var YNr=n(WZ);Pho=r(YNr,"ctrl"),YNr.forEach(t),$ho=r(jEe," \u2014 "),lP=s(jEe,"A",{href:!0});var KNr=n(lP);Iho=r(KNr,"CTRLLMHeadModel"),KNr.forEach(t),Dho=r(jEe," (CTRL model)"),jEe.forEach(t),jho=i(D),Zp=s(D,"LI",{});var NEe=n(Zp);QZ=s(NEe,"STRONG",{});var ZNr=n(QZ);Nho=r(ZNr,"data2vec-text"),ZNr.forEach(t),qho=r(NEe," \u2014 "),iP=s(NEe,"A",{href:!0});var eqr=n(iP);Gho=r(eqr,"Data2VecTextForCausalLM"),eqr.forEach(t),Oho=r(NEe," (Data2VecText model)"),NEe.forEach(t),Xho=i(D),e_=s(D,"LI",{});var qEe=n(e_);HZ=s(qEe,"STRONG",{});var oqr=n(HZ);Vho=r(oqr,"electra"),oqr.forEach(t),zho=r(qEe," \u2014 "),dP=s(qEe,"A",{href:!0});var rqr=n(dP);Who=r(rqr,"ElectraForCausalLM"),rqr.forEach(t),Qho=r(qEe," (ELECTRA model)"),qEe.forEach(t),Hho=i(D),o_=s(D,"LI",{});var GEe=n(o_);UZ=s(GEe,"STRONG",{});var tqr=n(UZ);Uho=r(tqr,"gpt2"),tqr.forEach(t),Jho=r(GEe," \u2014 "),cP=s(GEe,"A",{href:!0});var aqr=n(cP);Yho=r(aqr,"GPT2LMHeadModel"),aqr.forEach(t),Kho=r(GEe," (OpenAI GPT-2 model)"),GEe.forEach(t),Zho=i(D),r_=s(D,"LI",{});var OEe=n(r_);JZ=s(OEe,"STRONG",{});var sqr=n(JZ);euo=r(sqr,"gpt_neo"),sqr.forEach(t),ouo=r(OEe," \u2014 "),mP=s(OEe,"A",{href:!0});var nqr=n(mP);ruo=r(nqr,"GPTNeoForCausalLM"),nqr.forEach(t),tuo=r(OEe," (GPT Neo model)"),OEe.forEach(t),auo=i(D),t_=s(D,"LI",{});var XEe=n(t_);YZ=s(XEe,"STRONG",{});var lqr=n(YZ);suo=r(lqr,"gptj"),lqr.forEach(t),nuo=r(XEe," \u2014 "),fP=s(XEe,"A",{href:!0});var iqr=n(fP);luo=r(iqr,"GPTJForCausalLM"),iqr.forEach(t),iuo=r(XEe," (GPT-J model)"),XEe.forEach(t),duo=i(D),a_=s(D,"LI",{});var VEe=n(a_);KZ=s(VEe,"STRONG",{});var dqr=n(KZ);cuo=r(dqr,"marian"),dqr.forEach(t),muo=r(VEe," \u2014 "),gP=s(VEe,"A",{href:!0});var cqr=n(gP);fuo=r(cqr,"MarianForCausalLM"),cqr.forEach(t),guo=r(VEe," (Marian model)"),VEe.forEach(t),huo=i(D),s_=s(D,"LI",{});var zEe=n(s_);ZZ=s(zEe,"STRONG",{});var mqr=n(ZZ);uuo=r(mqr,"mbart"),mqr.forEach(t),puo=r(zEe," \u2014 "),hP=s(zEe,"A",{href:!0});var fqr=n(hP);_uo=r(fqr,"MBartForCausalLM"),fqr.forEach(t),buo=r(zEe," (mBART model)"),zEe.forEach(t),vuo=i(D),n_=s(D,"LI",{});var WEe=n(n_);eee=s(WEe,"STRONG",{});var gqr=n(eee);Tuo=r(gqr,"megatron-bert"),gqr.forEach(t),Fuo=r(WEe," \u2014 "),uP=s(WEe,"A",{href:!0});var hqr=n(uP);Cuo=r(hqr,"MegatronBertForCausalLM"),hqr.forEach(t),Muo=r(WEe," (MegatronBert model)"),WEe.forEach(t),Euo=i(D),l_=s(D,"LI",{});var QEe=n(l_);oee=s(QEe,"STRONG",{});var uqr=n(oee);yuo=r(uqr,"openai-gpt"),uqr.forEach(t),wuo=r(QEe," \u2014 "),pP=s(QEe,"A",{href:!0});var pqr=n(pP);Auo=r(pqr,"OpenAIGPTLMHeadModel"),pqr.forEach(t),Luo=r(QEe," (OpenAI GPT model)"),QEe.forEach(t),Buo=i(D),i_=s(D,"LI",{});var HEe=n(i_);ree=s(HEe,"STRONG",{});var _qr=n(ree);xuo=r(_qr,"pegasus"),_qr.forEach(t),kuo=r(HEe," \u2014 "),_P=s(HEe,"A",{href:!0});var bqr=n(_P);Ruo=r(bqr,"PegasusForCausalLM"),bqr.forEach(t),Suo=r(HEe," (Pegasus model)"),HEe.forEach(t),Puo=i(D),d_=s(D,"LI",{});var UEe=n(d_);tee=s(UEe,"STRONG",{});var vqr=n(tee);$uo=r(vqr,"plbart"),vqr.forEach(t),Iuo=r(UEe," \u2014 "),bP=s(UEe,"A",{href:!0});var Tqr=n(bP);Duo=r(Tqr,"PLBartForCausalLM"),Tqr.forEach(t),juo=r(UEe," (PLBart model)"),UEe.forEach(t),Nuo=i(D),c_=s(D,"LI",{});var JEe=n(c_);aee=s(JEe,"STRONG",{});var Fqr=n(aee);quo=r(Fqr,"prophetnet"),Fqr.forEach(t),Guo=r(JEe," \u2014 "),vP=s(JEe,"A",{href:!0});var Cqr=n(vP);Ouo=r(Cqr,"ProphetNetForCausalLM"),Cqr.forEach(t),Xuo=r(JEe," (ProphetNet model)"),JEe.forEach(t),Vuo=i(D),m_=s(D,"LI",{});var YEe=n(m_);see=s(YEe,"STRONG",{});var Mqr=n(see);zuo=r(Mqr,"qdqbert"),Mqr.forEach(t),Wuo=r(YEe," \u2014 "),TP=s(YEe,"A",{href:!0});var Eqr=n(TP);Quo=r(Eqr,"QDQBertLMHeadModel"),Eqr.forEach(t),Huo=r(YEe," (QDQBert model)"),YEe.forEach(t),Uuo=i(D),f_=s(D,"LI",{});var KEe=n(f_);nee=s(KEe,"STRONG",{});var yqr=n(nee);Juo=r(yqr,"reformer"),yqr.forEach(t),Yuo=r(KEe," \u2014 "),FP=s(KEe,"A",{href:!0});var wqr=n(FP);Kuo=r(wqr,"ReformerModelWithLMHead"),wqr.forEach(t),Zuo=r(KEe," (Reformer model)"),KEe.forEach(t),epo=i(D),g_=s(D,"LI",{});var ZEe=n(g_);lee=s(ZEe,"STRONG",{});var Aqr=n(lee);opo=r(Aqr,"rembert"),Aqr.forEach(t),rpo=r(ZEe," \u2014 "),CP=s(ZEe,"A",{href:!0});var Lqr=n(CP);tpo=r(Lqr,"RemBertForCausalLM"),Lqr.forEach(t),apo=r(ZEe," (RemBERT model)"),ZEe.forEach(t),spo=i(D),h_=s(D,"LI",{});var e3e=n(h_);iee=s(e3e,"STRONG",{});var Bqr=n(iee);npo=r(Bqr,"roberta"),Bqr.forEach(t),lpo=r(e3e," \u2014 "),MP=s(e3e,"A",{href:!0});var xqr=n(MP);ipo=r(xqr,"RobertaForCausalLM"),xqr.forEach(t),dpo=r(e3e," (RoBERTa model)"),e3e.forEach(t),cpo=i(D),u_=s(D,"LI",{});var o3e=n(u_);dee=s(o3e,"STRONG",{});var kqr=n(dee);mpo=r(kqr,"roformer"),kqr.forEach(t),fpo=r(o3e," \u2014 "),EP=s(o3e,"A",{href:!0});var Rqr=n(EP);gpo=r(Rqr,"RoFormerForCausalLM"),Rqr.forEach(t),hpo=r(o3e," (RoFormer model)"),o3e.forEach(t),upo=i(D),p_=s(D,"LI",{});var r3e=n(p_);cee=s(r3e,"STRONG",{});var Sqr=n(cee);ppo=r(Sqr,"speech_to_text_2"),Sqr.forEach(t),_po=r(r3e," \u2014 "),yP=s(r3e,"A",{href:!0});var Pqr=n(yP);bpo=r(Pqr,"Speech2Text2ForCausalLM"),Pqr.forEach(t),vpo=r(r3e," (Speech2Text2 model)"),r3e.forEach(t),Tpo=i(D),__=s(D,"LI",{});var t3e=n(__);mee=s(t3e,"STRONG",{});var $qr=n(mee);Fpo=r($qr,"transfo-xl"),$qr.forEach(t),Cpo=r(t3e," \u2014 "),wP=s(t3e,"A",{href:!0});var Iqr=n(wP);Mpo=r(Iqr,"TransfoXLLMHeadModel"),Iqr.forEach(t),Epo=r(t3e," (Transformer-XL model)"),t3e.forEach(t),ypo=i(D),b_=s(D,"LI",{});var a3e=n(b_);fee=s(a3e,"STRONG",{});var Dqr=n(fee);wpo=r(Dqr,"trocr"),Dqr.forEach(t),Apo=r(a3e," \u2014 "),AP=s(a3e,"A",{href:!0});var jqr=n(AP);Lpo=r(jqr,"TrOCRForCausalLM"),jqr.forEach(t),Bpo=r(a3e," (TrOCR model)"),a3e.forEach(t),xpo=i(D),v_=s(D,"LI",{});var s3e=n(v_);gee=s(s3e,"STRONG",{});var Nqr=n(gee);kpo=r(Nqr,"xglm"),Nqr.forEach(t),Rpo=r(s3e," \u2014 "),LP=s(s3e,"A",{href:!0});var qqr=n(LP);Spo=r(qqr,"XGLMForCausalLM"),qqr.forEach(t),Ppo=r(s3e," (XGLM model)"),s3e.forEach(t),$po=i(D),T_=s(D,"LI",{});var n3e=n(T_);hee=s(n3e,"STRONG",{});var Gqr=n(hee);Ipo=r(Gqr,"xlm"),Gqr.forEach(t),Dpo=r(n3e," \u2014 "),BP=s(n3e,"A",{href:!0});var Oqr=n(BP);jpo=r(Oqr,"XLMWithLMHeadModel"),Oqr.forEach(t),Npo=r(n3e," (XLM model)"),n3e.forEach(t),qpo=i(D),F_=s(D,"LI",{});var l3e=n(F_);uee=s(l3e,"STRONG",{});var Xqr=n(uee);Gpo=r(Xqr,"xlm-prophetnet"),Xqr.forEach(t),Opo=r(l3e," \u2014 "),xP=s(l3e,"A",{href:!0});var Vqr=n(xP);Xpo=r(Vqr,"XLMProphetNetForCausalLM"),Vqr.forEach(t),Vpo=r(l3e," (XLMProphetNet model)"),l3e.forEach(t),zpo=i(D),C_=s(D,"LI",{});var i3e=n(C_);pee=s(i3e,"STRONG",{});var zqr=n(pee);Wpo=r(zqr,"xlm-roberta"),zqr.forEach(t),Qpo=r(i3e," \u2014 "),kP=s(i3e,"A",{href:!0});var Wqr=n(kP);Hpo=r(Wqr,"XLMRobertaForCausalLM"),Wqr.forEach(t),Upo=r(i3e," (XLM-RoBERTa model)"),i3e.forEach(t),Jpo=i(D),M_=s(D,"LI",{});var d3e=n(M_);_ee=s(d3e,"STRONG",{});var Qqr=n(_ee);Ypo=r(Qqr,"xlm-roberta-xl"),Qqr.forEach(t),Kpo=r(d3e," \u2014 "),RP=s(d3e,"A",{href:!0});var Hqr=n(RP);Zpo=r(Hqr,"XLMRobertaXLForCausalLM"),Hqr.forEach(t),e_o=r(d3e," (XLM-RoBERTa-XL model)"),d3e.forEach(t),o_o=i(D),E_=s(D,"LI",{});var c3e=n(E_);bee=s(c3e,"STRONG",{});var Uqr=n(bee);r_o=r(Uqr,"xlnet"),Uqr.forEach(t),t_o=r(c3e," \u2014 "),SP=s(c3e,"A",{href:!0});var Jqr=n(SP);a_o=r(Jqr,"XLNetLMHeadModel"),Jqr.forEach(t),s_o=r(c3e," (XLNet model)"),c3e.forEach(t),D.forEach(t),n_o=i(Dt),y_=s(Dt,"P",{});var m3e=n(y_);l_o=r(m3e,"The model is set in evaluation mode by default using "),vee=s(m3e,"CODE",{});var Yqr=n(vee);i_o=r(Yqr,"model.eval()"),Yqr.forEach(t),d_o=r(m3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Tee=s(m3e,"CODE",{});var Kqr=n(Tee);c_o=r(Kqr,"model.train()"),Kqr.forEach(t),m3e.forEach(t),m_o=i(Dt),Fee=s(Dt,"P",{});var Zqr=n(Fee);f_o=r(Zqr,"Examples:"),Zqr.forEach(t),g_o=i(Dt),f(Ty.$$.fragment,Dt),Dt.forEach(t),Vn.forEach(t),I9e=i(c),Yi=s(c,"H2",{class:!0});var Xxe=n(Yi);w_=s(Xxe,"A",{id:!0,class:!0,href:!0});var eGr=n(w_);Cee=s(eGr,"SPAN",{});var oGr=n(Cee);f(Fy.$$.fragment,oGr),oGr.forEach(t),eGr.forEach(t),h_o=i(Xxe),Mee=s(Xxe,"SPAN",{});var rGr=n(Mee);u_o=r(rGr,"AutoModelForMaskedLM"),rGr.forEach(t),Xxe.forEach(t),D9e=i(c),Jo=s(c,"DIV",{class:!0});var Wn=n(Jo);f(Cy.$$.fragment,Wn),p_o=i(Wn),Ki=s(Wn,"P",{});var $V=n(Ki);__o=r($V,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),Eee=s($V,"CODE",{});var tGr=n(Eee);b_o=r(tGr,"from_pretrained()"),tGr.forEach(t),v_o=r($V,"class method or the "),yee=s($V,"CODE",{});var aGr=n(yee);T_o=r(aGr,"from_config()"),aGr.forEach(t),F_o=r($V,`class
method.`),$V.forEach(t),C_o=i(Wn),My=s(Wn,"P",{});var Vxe=n(My);M_o=r(Vxe,"This class cannot be instantiated directly using "),wee=s(Vxe,"CODE",{});var sGr=n(wee);E_o=r(sGr,"__init__()"),sGr.forEach(t),y_o=r(Vxe," (throws an error)."),Vxe.forEach(t),w_o=i(Wn),Xr=s(Wn,"DIV",{class:!0});var Qn=n(Xr);f(Ey.$$.fragment,Qn),A_o=i(Qn),Aee=s(Qn,"P",{});var nGr=n(Aee);L_o=r(nGr,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),nGr.forEach(t),B_o=i(Qn),Zi=s(Qn,"P",{});var IV=n(Zi);x_o=r(IV,`Note:
Loading a model from its configuration file does `),Lee=s(IV,"STRONG",{});var lGr=n(Lee);k_o=r(lGr,"not"),lGr.forEach(t),R_o=r(IV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bee=s(IV,"CODE",{});var iGr=n(Bee);S_o=r(iGr,"from_pretrained()"),iGr.forEach(t),P_o=r(IV,"to load the model weights."),IV.forEach(t),$_o=i(Qn),xee=s(Qn,"P",{});var dGr=n(xee);I_o=r(dGr,"Examples:"),dGr.forEach(t),D_o=i(Qn),f(yy.$$.fragment,Qn),Qn.forEach(t),j_o=i(Wn),$e=s(Wn,"DIV",{class:!0});var jt=n($e);f(wy.$$.fragment,jt),N_o=i(jt),kee=s(jt,"P",{});var cGr=n(kee);q_o=r(cGr,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),cGr.forEach(t),G_o=i(jt),Oa=s(jt,"P",{});var A3=n(Oa);O_o=r(A3,"The model class to instantiate is selected based on the "),Ree=s(A3,"CODE",{});var mGr=n(Ree);X_o=r(mGr,"model_type"),mGr.forEach(t),V_o=r(A3,` property of the config object (either
passed as an argument or loaded from `),See=s(A3,"CODE",{});var fGr=n(See);z_o=r(fGr,"pretrained_model_name_or_path"),fGr.forEach(t),W_o=r(A3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pee=s(A3,"CODE",{});var gGr=n(Pee);Q_o=r(gGr,"pretrained_model_name_or_path"),gGr.forEach(t),H_o=r(A3,":"),A3.forEach(t),U_o=i(jt),I=s(jt,"UL",{});var j=n(I);A_=s(j,"LI",{});var f3e=n(A_);$ee=s(f3e,"STRONG",{});var hGr=n($ee);J_o=r(hGr,"albert"),hGr.forEach(t),Y_o=r(f3e," \u2014 "),PP=s(f3e,"A",{href:!0});var uGr=n(PP);K_o=r(uGr,"AlbertForMaskedLM"),uGr.forEach(t),Z_o=r(f3e," (ALBERT model)"),f3e.forEach(t),ebo=i(j),L_=s(j,"LI",{});var g3e=n(L_);Iee=s(g3e,"STRONG",{});var pGr=n(Iee);obo=r(pGr,"bart"),pGr.forEach(t),rbo=r(g3e," \u2014 "),$P=s(g3e,"A",{href:!0});var _Gr=n($P);tbo=r(_Gr,"BartForConditionalGeneration"),_Gr.forEach(t),abo=r(g3e," (BART model)"),g3e.forEach(t),sbo=i(j),B_=s(j,"LI",{});var h3e=n(B_);Dee=s(h3e,"STRONG",{});var bGr=n(Dee);nbo=r(bGr,"bert"),bGr.forEach(t),lbo=r(h3e," \u2014 "),IP=s(h3e,"A",{href:!0});var vGr=n(IP);ibo=r(vGr,"BertForMaskedLM"),vGr.forEach(t),dbo=r(h3e," (BERT model)"),h3e.forEach(t),cbo=i(j),x_=s(j,"LI",{});var u3e=n(x_);jee=s(u3e,"STRONG",{});var TGr=n(jee);mbo=r(TGr,"big_bird"),TGr.forEach(t),fbo=r(u3e," \u2014 "),DP=s(u3e,"A",{href:!0});var FGr=n(DP);gbo=r(FGr,"BigBirdForMaskedLM"),FGr.forEach(t),hbo=r(u3e," (BigBird model)"),u3e.forEach(t),ubo=i(j),k_=s(j,"LI",{});var p3e=n(k_);Nee=s(p3e,"STRONG",{});var CGr=n(Nee);pbo=r(CGr,"camembert"),CGr.forEach(t),_bo=r(p3e," \u2014 "),jP=s(p3e,"A",{href:!0});var MGr=n(jP);bbo=r(MGr,"CamembertForMaskedLM"),MGr.forEach(t),vbo=r(p3e," (CamemBERT model)"),p3e.forEach(t),Tbo=i(j),R_=s(j,"LI",{});var _3e=n(R_);qee=s(_3e,"STRONG",{});var EGr=n(qee);Fbo=r(EGr,"convbert"),EGr.forEach(t),Cbo=r(_3e," \u2014 "),NP=s(_3e,"A",{href:!0});var yGr=n(NP);Mbo=r(yGr,"ConvBertForMaskedLM"),yGr.forEach(t),Ebo=r(_3e," (ConvBERT model)"),_3e.forEach(t),ybo=i(j),S_=s(j,"LI",{});var b3e=n(S_);Gee=s(b3e,"STRONG",{});var wGr=n(Gee);wbo=r(wGr,"data2vec-text"),wGr.forEach(t),Abo=r(b3e," \u2014 "),qP=s(b3e,"A",{href:!0});var AGr=n(qP);Lbo=r(AGr,"Data2VecTextForMaskedLM"),AGr.forEach(t),Bbo=r(b3e," (Data2VecText model)"),b3e.forEach(t),xbo=i(j),P_=s(j,"LI",{});var v3e=n(P_);Oee=s(v3e,"STRONG",{});var LGr=n(Oee);kbo=r(LGr,"deberta"),LGr.forEach(t),Rbo=r(v3e," \u2014 "),GP=s(v3e,"A",{href:!0});var BGr=n(GP);Sbo=r(BGr,"DebertaForMaskedLM"),BGr.forEach(t),Pbo=r(v3e," (DeBERTa model)"),v3e.forEach(t),$bo=i(j),$_=s(j,"LI",{});var T3e=n($_);Xee=s(T3e,"STRONG",{});var xGr=n(Xee);Ibo=r(xGr,"deberta-v2"),xGr.forEach(t),Dbo=r(T3e," \u2014 "),OP=s(T3e,"A",{href:!0});var kGr=n(OP);jbo=r(kGr,"DebertaV2ForMaskedLM"),kGr.forEach(t),Nbo=r(T3e," (DeBERTa-v2 model)"),T3e.forEach(t),qbo=i(j),I_=s(j,"LI",{});var F3e=n(I_);Vee=s(F3e,"STRONG",{});var RGr=n(Vee);Gbo=r(RGr,"distilbert"),RGr.forEach(t),Obo=r(F3e," \u2014 "),XP=s(F3e,"A",{href:!0});var SGr=n(XP);Xbo=r(SGr,"DistilBertForMaskedLM"),SGr.forEach(t),Vbo=r(F3e," (DistilBERT model)"),F3e.forEach(t),zbo=i(j),D_=s(j,"LI",{});var C3e=n(D_);zee=s(C3e,"STRONG",{});var PGr=n(zee);Wbo=r(PGr,"electra"),PGr.forEach(t),Qbo=r(C3e," \u2014 "),VP=s(C3e,"A",{href:!0});var $Gr=n(VP);Hbo=r($Gr,"ElectraForMaskedLM"),$Gr.forEach(t),Ubo=r(C3e," (ELECTRA model)"),C3e.forEach(t),Jbo=i(j),j_=s(j,"LI",{});var M3e=n(j_);Wee=s(M3e,"STRONG",{});var IGr=n(Wee);Ybo=r(IGr,"flaubert"),IGr.forEach(t),Kbo=r(M3e," \u2014 "),zP=s(M3e,"A",{href:!0});var DGr=n(zP);Zbo=r(DGr,"FlaubertWithLMHeadModel"),DGr.forEach(t),e2o=r(M3e," (FlauBERT model)"),M3e.forEach(t),o2o=i(j),N_=s(j,"LI",{});var E3e=n(N_);Qee=s(E3e,"STRONG",{});var jGr=n(Qee);r2o=r(jGr,"fnet"),jGr.forEach(t),t2o=r(E3e," \u2014 "),WP=s(E3e,"A",{href:!0});var NGr=n(WP);a2o=r(NGr,"FNetForMaskedLM"),NGr.forEach(t),s2o=r(E3e," (FNet model)"),E3e.forEach(t),n2o=i(j),q_=s(j,"LI",{});var y3e=n(q_);Hee=s(y3e,"STRONG",{});var qGr=n(Hee);l2o=r(qGr,"funnel"),qGr.forEach(t),i2o=r(y3e," \u2014 "),QP=s(y3e,"A",{href:!0});var GGr=n(QP);d2o=r(GGr,"FunnelForMaskedLM"),GGr.forEach(t),c2o=r(y3e," (Funnel Transformer model)"),y3e.forEach(t),m2o=i(j),G_=s(j,"LI",{});var w3e=n(G_);Uee=s(w3e,"STRONG",{});var OGr=n(Uee);f2o=r(OGr,"ibert"),OGr.forEach(t),g2o=r(w3e," \u2014 "),HP=s(w3e,"A",{href:!0});var XGr=n(HP);h2o=r(XGr,"IBertForMaskedLM"),XGr.forEach(t),u2o=r(w3e," (I-BERT model)"),w3e.forEach(t),p2o=i(j),O_=s(j,"LI",{});var A3e=n(O_);Jee=s(A3e,"STRONG",{});var VGr=n(Jee);_2o=r(VGr,"layoutlm"),VGr.forEach(t),b2o=r(A3e," \u2014 "),UP=s(A3e,"A",{href:!0});var zGr=n(UP);v2o=r(zGr,"LayoutLMForMaskedLM"),zGr.forEach(t),T2o=r(A3e," (LayoutLM model)"),A3e.forEach(t),F2o=i(j),X_=s(j,"LI",{});var L3e=n(X_);Yee=s(L3e,"STRONG",{});var WGr=n(Yee);C2o=r(WGr,"longformer"),WGr.forEach(t),M2o=r(L3e," \u2014 "),JP=s(L3e,"A",{href:!0});var QGr=n(JP);E2o=r(QGr,"LongformerForMaskedLM"),QGr.forEach(t),y2o=r(L3e," (Longformer model)"),L3e.forEach(t),w2o=i(j),V_=s(j,"LI",{});var B3e=n(V_);Kee=s(B3e,"STRONG",{});var HGr=n(Kee);A2o=r(HGr,"mbart"),HGr.forEach(t),L2o=r(B3e," \u2014 "),YP=s(B3e,"A",{href:!0});var UGr=n(YP);B2o=r(UGr,"MBartForConditionalGeneration"),UGr.forEach(t),x2o=r(B3e," (mBART model)"),B3e.forEach(t),k2o=i(j),z_=s(j,"LI",{});var x3e=n(z_);Zee=s(x3e,"STRONG",{});var JGr=n(Zee);R2o=r(JGr,"megatron-bert"),JGr.forEach(t),S2o=r(x3e," \u2014 "),KP=s(x3e,"A",{href:!0});var YGr=n(KP);P2o=r(YGr,"MegatronBertForMaskedLM"),YGr.forEach(t),$2o=r(x3e," (MegatronBert model)"),x3e.forEach(t),I2o=i(j),W_=s(j,"LI",{});var k3e=n(W_);eoe=s(k3e,"STRONG",{});var KGr=n(eoe);D2o=r(KGr,"mobilebert"),KGr.forEach(t),j2o=r(k3e," \u2014 "),ZP=s(k3e,"A",{href:!0});var ZGr=n(ZP);N2o=r(ZGr,"MobileBertForMaskedLM"),ZGr.forEach(t),q2o=r(k3e," (MobileBERT model)"),k3e.forEach(t),G2o=i(j),Q_=s(j,"LI",{});var R3e=n(Q_);ooe=s(R3e,"STRONG",{});var eOr=n(ooe);O2o=r(eOr,"mpnet"),eOr.forEach(t),X2o=r(R3e," \u2014 "),e$=s(R3e,"A",{href:!0});var oOr=n(e$);V2o=r(oOr,"MPNetForMaskedLM"),oOr.forEach(t),z2o=r(R3e," (MPNet model)"),R3e.forEach(t),W2o=i(j),H_=s(j,"LI",{});var S3e=n(H_);roe=s(S3e,"STRONG",{});var rOr=n(roe);Q2o=r(rOr,"nystromformer"),rOr.forEach(t),H2o=r(S3e," \u2014 "),o$=s(S3e,"A",{href:!0});var tOr=n(o$);U2o=r(tOr,"NystromformerForMaskedLM"),tOr.forEach(t),J2o=r(S3e," (Nystromformer model)"),S3e.forEach(t),Y2o=i(j),U_=s(j,"LI",{});var P3e=n(U_);toe=s(P3e,"STRONG",{});var aOr=n(toe);K2o=r(aOr,"perceiver"),aOr.forEach(t),Z2o=r(P3e," \u2014 "),r$=s(P3e,"A",{href:!0});var sOr=n(r$);evo=r(sOr,"PerceiverForMaskedLM"),sOr.forEach(t),ovo=r(P3e," (Perceiver model)"),P3e.forEach(t),rvo=i(j),J_=s(j,"LI",{});var $3e=n(J_);aoe=s($3e,"STRONG",{});var nOr=n(aoe);tvo=r(nOr,"qdqbert"),nOr.forEach(t),avo=r($3e," \u2014 "),t$=s($3e,"A",{href:!0});var lOr=n(t$);svo=r(lOr,"QDQBertForMaskedLM"),lOr.forEach(t),nvo=r($3e," (QDQBert model)"),$3e.forEach(t),lvo=i(j),Y_=s(j,"LI",{});var I3e=n(Y_);soe=s(I3e,"STRONG",{});var iOr=n(soe);ivo=r(iOr,"reformer"),iOr.forEach(t),dvo=r(I3e," \u2014 "),a$=s(I3e,"A",{href:!0});var dOr=n(a$);cvo=r(dOr,"ReformerForMaskedLM"),dOr.forEach(t),mvo=r(I3e," (Reformer model)"),I3e.forEach(t),fvo=i(j),K_=s(j,"LI",{});var D3e=n(K_);noe=s(D3e,"STRONG",{});var cOr=n(noe);gvo=r(cOr,"rembert"),cOr.forEach(t),hvo=r(D3e," \u2014 "),s$=s(D3e,"A",{href:!0});var mOr=n(s$);uvo=r(mOr,"RemBertForMaskedLM"),mOr.forEach(t),pvo=r(D3e," (RemBERT model)"),D3e.forEach(t),_vo=i(j),Z_=s(j,"LI",{});var j3e=n(Z_);loe=s(j3e,"STRONG",{});var fOr=n(loe);bvo=r(fOr,"roberta"),fOr.forEach(t),vvo=r(j3e," \u2014 "),n$=s(j3e,"A",{href:!0});var gOr=n(n$);Tvo=r(gOr,"RobertaForMaskedLM"),gOr.forEach(t),Fvo=r(j3e," (RoBERTa model)"),j3e.forEach(t),Cvo=i(j),eb=s(j,"LI",{});var N3e=n(eb);ioe=s(N3e,"STRONG",{});var hOr=n(ioe);Mvo=r(hOr,"roformer"),hOr.forEach(t),Evo=r(N3e," \u2014 "),l$=s(N3e,"A",{href:!0});var uOr=n(l$);yvo=r(uOr,"RoFormerForMaskedLM"),uOr.forEach(t),wvo=r(N3e," (RoFormer model)"),N3e.forEach(t),Avo=i(j),ob=s(j,"LI",{});var q3e=n(ob);doe=s(q3e,"STRONG",{});var pOr=n(doe);Lvo=r(pOr,"squeezebert"),pOr.forEach(t),Bvo=r(q3e," \u2014 "),i$=s(q3e,"A",{href:!0});var _Or=n(i$);xvo=r(_Or,"SqueezeBertForMaskedLM"),_Or.forEach(t),kvo=r(q3e," (SqueezeBERT model)"),q3e.forEach(t),Rvo=i(j),rb=s(j,"LI",{});var G3e=n(rb);coe=s(G3e,"STRONG",{});var bOr=n(coe);Svo=r(bOr,"tapas"),bOr.forEach(t),Pvo=r(G3e," \u2014 "),d$=s(G3e,"A",{href:!0});var vOr=n(d$);$vo=r(vOr,"TapasForMaskedLM"),vOr.forEach(t),Ivo=r(G3e," (TAPAS model)"),G3e.forEach(t),Dvo=i(j),tb=s(j,"LI",{});var O3e=n(tb);moe=s(O3e,"STRONG",{});var TOr=n(moe);jvo=r(TOr,"wav2vec2"),TOr.forEach(t),Nvo=r(O3e," \u2014 "),foe=s(O3e,"CODE",{});var FOr=n(foe);qvo=r(FOr,"Wav2Vec2ForMaskedLM"),FOr.forEach(t),Gvo=r(O3e,"(Wav2Vec2 model)"),O3e.forEach(t),Ovo=i(j),ab=s(j,"LI",{});var X3e=n(ab);goe=s(X3e,"STRONG",{});var COr=n(goe);Xvo=r(COr,"xlm"),COr.forEach(t),Vvo=r(X3e," \u2014 "),c$=s(X3e,"A",{href:!0});var MOr=n(c$);zvo=r(MOr,"XLMWithLMHeadModel"),MOr.forEach(t),Wvo=r(X3e," (XLM model)"),X3e.forEach(t),Qvo=i(j),sb=s(j,"LI",{});var V3e=n(sb);hoe=s(V3e,"STRONG",{});var EOr=n(hoe);Hvo=r(EOr,"xlm-roberta"),EOr.forEach(t),Uvo=r(V3e," \u2014 "),m$=s(V3e,"A",{href:!0});var yOr=n(m$);Jvo=r(yOr,"XLMRobertaForMaskedLM"),yOr.forEach(t),Yvo=r(V3e," (XLM-RoBERTa model)"),V3e.forEach(t),Kvo=i(j),nb=s(j,"LI",{});var z3e=n(nb);uoe=s(z3e,"STRONG",{});var wOr=n(uoe);Zvo=r(wOr,"xlm-roberta-xl"),wOr.forEach(t),eTo=r(z3e," \u2014 "),f$=s(z3e,"A",{href:!0});var AOr=n(f$);oTo=r(AOr,"XLMRobertaXLForMaskedLM"),AOr.forEach(t),rTo=r(z3e," (XLM-RoBERTa-XL model)"),z3e.forEach(t),tTo=i(j),lb=s(j,"LI",{});var W3e=n(lb);poe=s(W3e,"STRONG",{});var LOr=n(poe);aTo=r(LOr,"yoso"),LOr.forEach(t),sTo=r(W3e," \u2014 "),g$=s(W3e,"A",{href:!0});var BOr=n(g$);nTo=r(BOr,"YosoForMaskedLM"),BOr.forEach(t),lTo=r(W3e," (YOSO model)"),W3e.forEach(t),j.forEach(t),iTo=i(jt),ib=s(jt,"P",{});var Q3e=n(ib);dTo=r(Q3e,"The model is set in evaluation mode by default using "),_oe=s(Q3e,"CODE",{});var xOr=n(_oe);cTo=r(xOr,"model.eval()"),xOr.forEach(t),mTo=r(Q3e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),boe=s(Q3e,"CODE",{});var kOr=n(boe);fTo=r(kOr,"model.train()"),kOr.forEach(t),Q3e.forEach(t),gTo=i(jt),voe=s(jt,"P",{});var ROr=n(voe);hTo=r(ROr,"Examples:"),ROr.forEach(t),uTo=i(jt),f(Ay.$$.fragment,jt),jt.forEach(t),Wn.forEach(t),j9e=i(c),ed=s(c,"H2",{class:!0});var zxe=n(ed);db=s(zxe,"A",{id:!0,class:!0,href:!0});var SOr=n(db);Toe=s(SOr,"SPAN",{});var POr=n(Toe);f(Ly.$$.fragment,POr),POr.forEach(t),SOr.forEach(t),pTo=i(zxe),Foe=s(zxe,"SPAN",{});var $Or=n(Foe);_To=r($Or,"AutoModelForSeq2SeqLM"),$Or.forEach(t),zxe.forEach(t),N9e=i(c),Yo=s(c,"DIV",{class:!0});var Hn=n(Yo);f(By.$$.fragment,Hn),bTo=i(Hn),od=s(Hn,"P",{});var DV=n(od);vTo=r(DV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Coe=s(DV,"CODE",{});var IOr=n(Coe);TTo=r(IOr,"from_pretrained()"),IOr.forEach(t),FTo=r(DV,"class method or the "),Moe=s(DV,"CODE",{});var DOr=n(Moe);CTo=r(DOr,"from_config()"),DOr.forEach(t),MTo=r(DV,`class
method.`),DV.forEach(t),ETo=i(Hn),xy=s(Hn,"P",{});var Wxe=n(xy);yTo=r(Wxe,"This class cannot be instantiated directly using "),Eoe=s(Wxe,"CODE",{});var jOr=n(Eoe);wTo=r(jOr,"__init__()"),jOr.forEach(t),ATo=r(Wxe," (throws an error)."),Wxe.forEach(t),LTo=i(Hn),Vr=s(Hn,"DIV",{class:!0});var Un=n(Vr);f(ky.$$.fragment,Un),BTo=i(Un),yoe=s(Un,"P",{});var NOr=n(yoe);xTo=r(NOr,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),NOr.forEach(t),kTo=i(Un),rd=s(Un,"P",{});var jV=n(rd);RTo=r(jV,`Note:
Loading a model from its configuration file does `),woe=s(jV,"STRONG",{});var qOr=n(woe);STo=r(qOr,"not"),qOr.forEach(t),PTo=r(jV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Aoe=s(jV,"CODE",{});var GOr=n(Aoe);$To=r(GOr,"from_pretrained()"),GOr.forEach(t),ITo=r(jV,"to load the model weights."),jV.forEach(t),DTo=i(Un),Loe=s(Un,"P",{});var OOr=n(Loe);jTo=r(OOr,"Examples:"),OOr.forEach(t),NTo=i(Un),f(Ry.$$.fragment,Un),Un.forEach(t),qTo=i(Hn),Ie=s(Hn,"DIV",{class:!0});var Nt=n(Ie);f(Sy.$$.fragment,Nt),GTo=i(Nt),Boe=s(Nt,"P",{});var XOr=n(Boe);OTo=r(XOr,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),XOr.forEach(t),XTo=i(Nt),Xa=s(Nt,"P",{});var L3=n(Xa);VTo=r(L3,"The model class to instantiate is selected based on the "),xoe=s(L3,"CODE",{});var VOr=n(xoe);zTo=r(VOr,"model_type"),VOr.forEach(t),WTo=r(L3,` property of the config object (either
passed as an argument or loaded from `),koe=s(L3,"CODE",{});var zOr=n(koe);QTo=r(zOr,"pretrained_model_name_or_path"),zOr.forEach(t),HTo=r(L3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Roe=s(L3,"CODE",{});var WOr=n(Roe);UTo=r(WOr,"pretrained_model_name_or_path"),WOr.forEach(t),JTo=r(L3,":"),L3.forEach(t),YTo=i(Nt),ae=s(Nt,"UL",{});var le=n(ae);cb=s(le,"LI",{});var H3e=n(cb);Soe=s(H3e,"STRONG",{});var QOr=n(Soe);KTo=r(QOr,"bart"),QOr.forEach(t),ZTo=r(H3e," \u2014 "),h$=s(H3e,"A",{href:!0});var HOr=n(h$);e1o=r(HOr,"BartForConditionalGeneration"),HOr.forEach(t),o1o=r(H3e," (BART model)"),H3e.forEach(t),r1o=i(le),mb=s(le,"LI",{});var U3e=n(mb);Poe=s(U3e,"STRONG",{});var UOr=n(Poe);t1o=r(UOr,"bigbird_pegasus"),UOr.forEach(t),a1o=r(U3e," \u2014 "),u$=s(U3e,"A",{href:!0});var JOr=n(u$);s1o=r(JOr,"BigBirdPegasusForConditionalGeneration"),JOr.forEach(t),n1o=r(U3e," (BigBirdPegasus model)"),U3e.forEach(t),l1o=i(le),fb=s(le,"LI",{});var J3e=n(fb);$oe=s(J3e,"STRONG",{});var YOr=n($oe);i1o=r(YOr,"blenderbot"),YOr.forEach(t),d1o=r(J3e," \u2014 "),p$=s(J3e,"A",{href:!0});var KOr=n(p$);c1o=r(KOr,"BlenderbotForConditionalGeneration"),KOr.forEach(t),m1o=r(J3e," (Blenderbot model)"),J3e.forEach(t),f1o=i(le),gb=s(le,"LI",{});var Y3e=n(gb);Ioe=s(Y3e,"STRONG",{});var ZOr=n(Ioe);g1o=r(ZOr,"blenderbot-small"),ZOr.forEach(t),h1o=r(Y3e," \u2014 "),_$=s(Y3e,"A",{href:!0});var eXr=n(_$);u1o=r(eXr,"BlenderbotSmallForConditionalGeneration"),eXr.forEach(t),p1o=r(Y3e," (BlenderbotSmall model)"),Y3e.forEach(t),_1o=i(le),hb=s(le,"LI",{});var K3e=n(hb);Doe=s(K3e,"STRONG",{});var oXr=n(Doe);b1o=r(oXr,"encoder-decoder"),oXr.forEach(t),v1o=r(K3e," \u2014 "),b$=s(K3e,"A",{href:!0});var rXr=n(b$);T1o=r(rXr,"EncoderDecoderModel"),rXr.forEach(t),F1o=r(K3e," (Encoder decoder model)"),K3e.forEach(t),C1o=i(le),ub=s(le,"LI",{});var Z3e=n(ub);joe=s(Z3e,"STRONG",{});var tXr=n(joe);M1o=r(tXr,"fsmt"),tXr.forEach(t),E1o=r(Z3e," \u2014 "),v$=s(Z3e,"A",{href:!0});var aXr=n(v$);y1o=r(aXr,"FSMTForConditionalGeneration"),aXr.forEach(t),w1o=r(Z3e," (FairSeq Machine-Translation model)"),Z3e.forEach(t),A1o=i(le),pb=s(le,"LI",{});var e5e=n(pb);Noe=s(e5e,"STRONG",{});var sXr=n(Noe);L1o=r(sXr,"led"),sXr.forEach(t),B1o=r(e5e," \u2014 "),T$=s(e5e,"A",{href:!0});var nXr=n(T$);x1o=r(nXr,"LEDForConditionalGeneration"),nXr.forEach(t),k1o=r(e5e," (LED model)"),e5e.forEach(t),R1o=i(le),_b=s(le,"LI",{});var o5e=n(_b);qoe=s(o5e,"STRONG",{});var lXr=n(qoe);S1o=r(lXr,"m2m_100"),lXr.forEach(t),P1o=r(o5e," \u2014 "),F$=s(o5e,"A",{href:!0});var iXr=n(F$);$1o=r(iXr,"M2M100ForConditionalGeneration"),iXr.forEach(t),I1o=r(o5e," (M2M100 model)"),o5e.forEach(t),D1o=i(le),bb=s(le,"LI",{});var r5e=n(bb);Goe=s(r5e,"STRONG",{});var dXr=n(Goe);j1o=r(dXr,"marian"),dXr.forEach(t),N1o=r(r5e," \u2014 "),C$=s(r5e,"A",{href:!0});var cXr=n(C$);q1o=r(cXr,"MarianMTModel"),cXr.forEach(t),G1o=r(r5e," (Marian model)"),r5e.forEach(t),O1o=i(le),vb=s(le,"LI",{});var t5e=n(vb);Ooe=s(t5e,"STRONG",{});var mXr=n(Ooe);X1o=r(mXr,"mbart"),mXr.forEach(t),V1o=r(t5e," \u2014 "),M$=s(t5e,"A",{href:!0});var fXr=n(M$);z1o=r(fXr,"MBartForConditionalGeneration"),fXr.forEach(t),W1o=r(t5e," (mBART model)"),t5e.forEach(t),Q1o=i(le),Tb=s(le,"LI",{});var a5e=n(Tb);Xoe=s(a5e,"STRONG",{});var gXr=n(Xoe);H1o=r(gXr,"mt5"),gXr.forEach(t),U1o=r(a5e," \u2014 "),E$=s(a5e,"A",{href:!0});var hXr=n(E$);J1o=r(hXr,"MT5ForConditionalGeneration"),hXr.forEach(t),Y1o=r(a5e," (mT5 model)"),a5e.forEach(t),K1o=i(le),Fb=s(le,"LI",{});var s5e=n(Fb);Voe=s(s5e,"STRONG",{});var uXr=n(Voe);Z1o=r(uXr,"pegasus"),uXr.forEach(t),eFo=r(s5e," \u2014 "),y$=s(s5e,"A",{href:!0});var pXr=n(y$);oFo=r(pXr,"PegasusForConditionalGeneration"),pXr.forEach(t),rFo=r(s5e," (Pegasus model)"),s5e.forEach(t),tFo=i(le),Cb=s(le,"LI",{});var n5e=n(Cb);zoe=s(n5e,"STRONG",{});var _Xr=n(zoe);aFo=r(_Xr,"plbart"),_Xr.forEach(t),sFo=r(n5e," \u2014 "),w$=s(n5e,"A",{href:!0});var bXr=n(w$);nFo=r(bXr,"PLBartForConditionalGeneration"),bXr.forEach(t),lFo=r(n5e," (PLBart model)"),n5e.forEach(t),iFo=i(le),Mb=s(le,"LI",{});var l5e=n(Mb);Woe=s(l5e,"STRONG",{});var vXr=n(Woe);dFo=r(vXr,"prophetnet"),vXr.forEach(t),cFo=r(l5e," \u2014 "),A$=s(l5e,"A",{href:!0});var TXr=n(A$);mFo=r(TXr,"ProphetNetForConditionalGeneration"),TXr.forEach(t),fFo=r(l5e," (ProphetNet model)"),l5e.forEach(t),gFo=i(le),Eb=s(le,"LI",{});var i5e=n(Eb);Qoe=s(i5e,"STRONG",{});var FXr=n(Qoe);hFo=r(FXr,"t5"),FXr.forEach(t),uFo=r(i5e," \u2014 "),L$=s(i5e,"A",{href:!0});var CXr=n(L$);pFo=r(CXr,"T5ForConditionalGeneration"),CXr.forEach(t),_Fo=r(i5e," (T5 model)"),i5e.forEach(t),bFo=i(le),yb=s(le,"LI",{});var d5e=n(yb);Hoe=s(d5e,"STRONG",{});var MXr=n(Hoe);vFo=r(MXr,"xlm-prophetnet"),MXr.forEach(t),TFo=r(d5e," \u2014 "),B$=s(d5e,"A",{href:!0});var EXr=n(B$);FFo=r(EXr,"XLMProphetNetForConditionalGeneration"),EXr.forEach(t),CFo=r(d5e," (XLMProphetNet model)"),d5e.forEach(t),le.forEach(t),MFo=i(Nt),wb=s(Nt,"P",{});var c5e=n(wb);EFo=r(c5e,"The model is set in evaluation mode by default using "),Uoe=s(c5e,"CODE",{});var yXr=n(Uoe);yFo=r(yXr,"model.eval()"),yXr.forEach(t),wFo=r(c5e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Joe=s(c5e,"CODE",{});var wXr=n(Joe);AFo=r(wXr,"model.train()"),wXr.forEach(t),c5e.forEach(t),LFo=i(Nt),Yoe=s(Nt,"P",{});var AXr=n(Yoe);BFo=r(AXr,"Examples:"),AXr.forEach(t),xFo=i(Nt),f(Py.$$.fragment,Nt),Nt.forEach(t),Hn.forEach(t),q9e=i(c),td=s(c,"H2",{class:!0});var Qxe=n(td);Ab=s(Qxe,"A",{id:!0,class:!0,href:!0});var LXr=n(Ab);Koe=s(LXr,"SPAN",{});var BXr=n(Koe);f($y.$$.fragment,BXr),BXr.forEach(t),LXr.forEach(t),kFo=i(Qxe),Zoe=s(Qxe,"SPAN",{});var xXr=n(Zoe);RFo=r(xXr,"AutoModelForSequenceClassification"),xXr.forEach(t),Qxe.forEach(t),G9e=i(c),Ko=s(c,"DIV",{class:!0});var Jn=n(Ko);f(Iy.$$.fragment,Jn),SFo=i(Jn),ad=s(Jn,"P",{});var NV=n(ad);PFo=r(NV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),ere=s(NV,"CODE",{});var kXr=n(ere);$Fo=r(kXr,"from_pretrained()"),kXr.forEach(t),IFo=r(NV,"class method or the "),ore=s(NV,"CODE",{});var RXr=n(ore);DFo=r(RXr,"from_config()"),RXr.forEach(t),jFo=r(NV,`class
method.`),NV.forEach(t),NFo=i(Jn),Dy=s(Jn,"P",{});var Hxe=n(Dy);qFo=r(Hxe,"This class cannot be instantiated directly using "),rre=s(Hxe,"CODE",{});var SXr=n(rre);GFo=r(SXr,"__init__()"),SXr.forEach(t),OFo=r(Hxe," (throws an error)."),Hxe.forEach(t),XFo=i(Jn),zr=s(Jn,"DIV",{class:!0});var Yn=n(zr);f(jy.$$.fragment,Yn),VFo=i(Yn),tre=s(Yn,"P",{});var PXr=n(tre);zFo=r(PXr,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),PXr.forEach(t),WFo=i(Yn),sd=s(Yn,"P",{});var qV=n(sd);QFo=r(qV,`Note:
Loading a model from its configuration file does `),are=s(qV,"STRONG",{});var $Xr=n(are);HFo=r($Xr,"not"),$Xr.forEach(t),UFo=r(qV,` load the model weights. It only affects the
model\u2019s configuration. Use `),sre=s(qV,"CODE",{});var IXr=n(sre);JFo=r(IXr,"from_pretrained()"),IXr.forEach(t),YFo=r(qV,"to load the model weights."),qV.forEach(t),KFo=i(Yn),nre=s(Yn,"P",{});var DXr=n(nre);ZFo=r(DXr,"Examples:"),DXr.forEach(t),eCo=i(Yn),f(Ny.$$.fragment,Yn),Yn.forEach(t),oCo=i(Jn),De=s(Jn,"DIV",{class:!0});var qt=n(De);f(qy.$$.fragment,qt),rCo=i(qt),lre=s(qt,"P",{});var jXr=n(lre);tCo=r(jXr,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),jXr.forEach(t),aCo=i(qt),Va=s(qt,"P",{});var B3=n(Va);sCo=r(B3,"The model class to instantiate is selected based on the "),ire=s(B3,"CODE",{});var NXr=n(ire);nCo=r(NXr,"model_type"),NXr.forEach(t),lCo=r(B3,` property of the config object (either
passed as an argument or loaded from `),dre=s(B3,"CODE",{});var qXr=n(dre);iCo=r(qXr,"pretrained_model_name_or_path"),qXr.forEach(t),dCo=r(B3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cre=s(B3,"CODE",{});var GXr=n(cre);cCo=r(GXr,"pretrained_model_name_or_path"),GXr.forEach(t),mCo=r(B3,":"),B3.forEach(t),fCo=i(qt),A=s(qt,"UL",{});var L=n(A);Lb=s(L,"LI",{});var m5e=n(Lb);mre=s(m5e,"STRONG",{});var OXr=n(mre);gCo=r(OXr,"albert"),OXr.forEach(t),hCo=r(m5e," \u2014 "),x$=s(m5e,"A",{href:!0});var XXr=n(x$);uCo=r(XXr,"AlbertForSequenceClassification"),XXr.forEach(t),pCo=r(m5e," (ALBERT model)"),m5e.forEach(t),_Co=i(L),Bb=s(L,"LI",{});var f5e=n(Bb);fre=s(f5e,"STRONG",{});var VXr=n(fre);bCo=r(VXr,"bart"),VXr.forEach(t),vCo=r(f5e," \u2014 "),k$=s(f5e,"A",{href:!0});var zXr=n(k$);TCo=r(zXr,"BartForSequenceClassification"),zXr.forEach(t),FCo=r(f5e," (BART model)"),f5e.forEach(t),CCo=i(L),xb=s(L,"LI",{});var g5e=n(xb);gre=s(g5e,"STRONG",{});var WXr=n(gre);MCo=r(WXr,"bert"),WXr.forEach(t),ECo=r(g5e," \u2014 "),R$=s(g5e,"A",{href:!0});var QXr=n(R$);yCo=r(QXr,"BertForSequenceClassification"),QXr.forEach(t),wCo=r(g5e," (BERT model)"),g5e.forEach(t),ACo=i(L),kb=s(L,"LI",{});var h5e=n(kb);hre=s(h5e,"STRONG",{});var HXr=n(hre);LCo=r(HXr,"big_bird"),HXr.forEach(t),BCo=r(h5e," \u2014 "),S$=s(h5e,"A",{href:!0});var UXr=n(S$);xCo=r(UXr,"BigBirdForSequenceClassification"),UXr.forEach(t),kCo=r(h5e," (BigBird model)"),h5e.forEach(t),RCo=i(L),Rb=s(L,"LI",{});var u5e=n(Rb);ure=s(u5e,"STRONG",{});var JXr=n(ure);SCo=r(JXr,"bigbird_pegasus"),JXr.forEach(t),PCo=r(u5e," \u2014 "),P$=s(u5e,"A",{href:!0});var YXr=n(P$);$Co=r(YXr,"BigBirdPegasusForSequenceClassification"),YXr.forEach(t),ICo=r(u5e," (BigBirdPegasus model)"),u5e.forEach(t),DCo=i(L),Sb=s(L,"LI",{});var p5e=n(Sb);pre=s(p5e,"STRONG",{});var KXr=n(pre);jCo=r(KXr,"camembert"),KXr.forEach(t),NCo=r(p5e," \u2014 "),$$=s(p5e,"A",{href:!0});var ZXr=n($$);qCo=r(ZXr,"CamembertForSequenceClassification"),ZXr.forEach(t),GCo=r(p5e," (CamemBERT model)"),p5e.forEach(t),OCo=i(L),Pb=s(L,"LI",{});var _5e=n(Pb);_re=s(_5e,"STRONG",{});var eVr=n(_re);XCo=r(eVr,"canine"),eVr.forEach(t),VCo=r(_5e," \u2014 "),I$=s(_5e,"A",{href:!0});var oVr=n(I$);zCo=r(oVr,"CanineForSequenceClassification"),oVr.forEach(t),WCo=r(_5e," (Canine model)"),_5e.forEach(t),QCo=i(L),$b=s(L,"LI",{});var b5e=n($b);bre=s(b5e,"STRONG",{});var rVr=n(bre);HCo=r(rVr,"convbert"),rVr.forEach(t),UCo=r(b5e," \u2014 "),D$=s(b5e,"A",{href:!0});var tVr=n(D$);JCo=r(tVr,"ConvBertForSequenceClassification"),tVr.forEach(t),YCo=r(b5e," (ConvBERT model)"),b5e.forEach(t),KCo=i(L),Ib=s(L,"LI",{});var v5e=n(Ib);vre=s(v5e,"STRONG",{});var aVr=n(vre);ZCo=r(aVr,"ctrl"),aVr.forEach(t),e4o=r(v5e," \u2014 "),j$=s(v5e,"A",{href:!0});var sVr=n(j$);o4o=r(sVr,"CTRLForSequenceClassification"),sVr.forEach(t),r4o=r(v5e," (CTRL model)"),v5e.forEach(t),t4o=i(L),Db=s(L,"LI",{});var T5e=n(Db);Tre=s(T5e,"STRONG",{});var nVr=n(Tre);a4o=r(nVr,"data2vec-text"),nVr.forEach(t),s4o=r(T5e," \u2014 "),N$=s(T5e,"A",{href:!0});var lVr=n(N$);n4o=r(lVr,"Data2VecTextForSequenceClassification"),lVr.forEach(t),l4o=r(T5e," (Data2VecText model)"),T5e.forEach(t),i4o=i(L),jb=s(L,"LI",{});var F5e=n(jb);Fre=s(F5e,"STRONG",{});var iVr=n(Fre);d4o=r(iVr,"deberta"),iVr.forEach(t),c4o=r(F5e," \u2014 "),q$=s(F5e,"A",{href:!0});var dVr=n(q$);m4o=r(dVr,"DebertaForSequenceClassification"),dVr.forEach(t),f4o=r(F5e," (DeBERTa model)"),F5e.forEach(t),g4o=i(L),Nb=s(L,"LI",{});var C5e=n(Nb);Cre=s(C5e,"STRONG",{});var cVr=n(Cre);h4o=r(cVr,"deberta-v2"),cVr.forEach(t),u4o=r(C5e," \u2014 "),G$=s(C5e,"A",{href:!0});var mVr=n(G$);p4o=r(mVr,"DebertaV2ForSequenceClassification"),mVr.forEach(t),_4o=r(C5e," (DeBERTa-v2 model)"),C5e.forEach(t),b4o=i(L),qb=s(L,"LI",{});var M5e=n(qb);Mre=s(M5e,"STRONG",{});var fVr=n(Mre);v4o=r(fVr,"distilbert"),fVr.forEach(t),T4o=r(M5e," \u2014 "),O$=s(M5e,"A",{href:!0});var gVr=n(O$);F4o=r(gVr,"DistilBertForSequenceClassification"),gVr.forEach(t),C4o=r(M5e," (DistilBERT model)"),M5e.forEach(t),M4o=i(L),Gb=s(L,"LI",{});var E5e=n(Gb);Ere=s(E5e,"STRONG",{});var hVr=n(Ere);E4o=r(hVr,"electra"),hVr.forEach(t),y4o=r(E5e," \u2014 "),X$=s(E5e,"A",{href:!0});var uVr=n(X$);w4o=r(uVr,"ElectraForSequenceClassification"),uVr.forEach(t),A4o=r(E5e," (ELECTRA model)"),E5e.forEach(t),L4o=i(L),Ob=s(L,"LI",{});var y5e=n(Ob);yre=s(y5e,"STRONG",{});var pVr=n(yre);B4o=r(pVr,"flaubert"),pVr.forEach(t),x4o=r(y5e," \u2014 "),V$=s(y5e,"A",{href:!0});var _Vr=n(V$);k4o=r(_Vr,"FlaubertForSequenceClassification"),_Vr.forEach(t),R4o=r(y5e," (FlauBERT model)"),y5e.forEach(t),S4o=i(L),Xb=s(L,"LI",{});var w5e=n(Xb);wre=s(w5e,"STRONG",{});var bVr=n(wre);P4o=r(bVr,"fnet"),bVr.forEach(t),$4o=r(w5e," \u2014 "),z$=s(w5e,"A",{href:!0});var vVr=n(z$);I4o=r(vVr,"FNetForSequenceClassification"),vVr.forEach(t),D4o=r(w5e," (FNet model)"),w5e.forEach(t),j4o=i(L),Vb=s(L,"LI",{});var A5e=n(Vb);Are=s(A5e,"STRONG",{});var TVr=n(Are);N4o=r(TVr,"funnel"),TVr.forEach(t),q4o=r(A5e," \u2014 "),W$=s(A5e,"A",{href:!0});var FVr=n(W$);G4o=r(FVr,"FunnelForSequenceClassification"),FVr.forEach(t),O4o=r(A5e," (Funnel Transformer model)"),A5e.forEach(t),X4o=i(L),zb=s(L,"LI",{});var L5e=n(zb);Lre=s(L5e,"STRONG",{});var CVr=n(Lre);V4o=r(CVr,"gpt2"),CVr.forEach(t),z4o=r(L5e," \u2014 "),Q$=s(L5e,"A",{href:!0});var MVr=n(Q$);W4o=r(MVr,"GPT2ForSequenceClassification"),MVr.forEach(t),Q4o=r(L5e," (OpenAI GPT-2 model)"),L5e.forEach(t),H4o=i(L),Wb=s(L,"LI",{});var B5e=n(Wb);Bre=s(B5e,"STRONG",{});var EVr=n(Bre);U4o=r(EVr,"gpt_neo"),EVr.forEach(t),J4o=r(B5e," \u2014 "),H$=s(B5e,"A",{href:!0});var yVr=n(H$);Y4o=r(yVr,"GPTNeoForSequenceClassification"),yVr.forEach(t),K4o=r(B5e," (GPT Neo model)"),B5e.forEach(t),Z4o=i(L),Qb=s(L,"LI",{});var x5e=n(Qb);xre=s(x5e,"STRONG",{});var wVr=n(xre);eMo=r(wVr,"gptj"),wVr.forEach(t),oMo=r(x5e," \u2014 "),U$=s(x5e,"A",{href:!0});var AVr=n(U$);rMo=r(AVr,"GPTJForSequenceClassification"),AVr.forEach(t),tMo=r(x5e," (GPT-J model)"),x5e.forEach(t),aMo=i(L),Hb=s(L,"LI",{});var k5e=n(Hb);kre=s(k5e,"STRONG",{});var LVr=n(kre);sMo=r(LVr,"ibert"),LVr.forEach(t),nMo=r(k5e," \u2014 "),J$=s(k5e,"A",{href:!0});var BVr=n(J$);lMo=r(BVr,"IBertForSequenceClassification"),BVr.forEach(t),iMo=r(k5e," (I-BERT model)"),k5e.forEach(t),dMo=i(L),Ub=s(L,"LI",{});var R5e=n(Ub);Rre=s(R5e,"STRONG",{});var xVr=n(Rre);cMo=r(xVr,"layoutlm"),xVr.forEach(t),mMo=r(R5e," \u2014 "),Y$=s(R5e,"A",{href:!0});var kVr=n(Y$);fMo=r(kVr,"LayoutLMForSequenceClassification"),kVr.forEach(t),gMo=r(R5e," (LayoutLM model)"),R5e.forEach(t),hMo=i(L),Jb=s(L,"LI",{});var S5e=n(Jb);Sre=s(S5e,"STRONG",{});var RVr=n(Sre);uMo=r(RVr,"layoutlmv2"),RVr.forEach(t),pMo=r(S5e," \u2014 "),K$=s(S5e,"A",{href:!0});var SVr=n(K$);_Mo=r(SVr,"LayoutLMv2ForSequenceClassification"),SVr.forEach(t),bMo=r(S5e," (LayoutLMv2 model)"),S5e.forEach(t),vMo=i(L),Yb=s(L,"LI",{});var P5e=n(Yb);Pre=s(P5e,"STRONG",{});var PVr=n(Pre);TMo=r(PVr,"led"),PVr.forEach(t),FMo=r(P5e," \u2014 "),Z$=s(P5e,"A",{href:!0});var $Vr=n(Z$);CMo=r($Vr,"LEDForSequenceClassification"),$Vr.forEach(t),MMo=r(P5e," (LED model)"),P5e.forEach(t),EMo=i(L),Kb=s(L,"LI",{});var $5e=n(Kb);$re=s($5e,"STRONG",{});var IVr=n($re);yMo=r(IVr,"longformer"),IVr.forEach(t),wMo=r($5e," \u2014 "),eI=s($5e,"A",{href:!0});var DVr=n(eI);AMo=r(DVr,"LongformerForSequenceClassification"),DVr.forEach(t),LMo=r($5e," (Longformer model)"),$5e.forEach(t),BMo=i(L),Zb=s(L,"LI",{});var I5e=n(Zb);Ire=s(I5e,"STRONG",{});var jVr=n(Ire);xMo=r(jVr,"mbart"),jVr.forEach(t),kMo=r(I5e," \u2014 "),oI=s(I5e,"A",{href:!0});var NVr=n(oI);RMo=r(NVr,"MBartForSequenceClassification"),NVr.forEach(t),SMo=r(I5e," (mBART model)"),I5e.forEach(t),PMo=i(L),e2=s(L,"LI",{});var D5e=n(e2);Dre=s(D5e,"STRONG",{});var qVr=n(Dre);$Mo=r(qVr,"megatron-bert"),qVr.forEach(t),IMo=r(D5e," \u2014 "),rI=s(D5e,"A",{href:!0});var GVr=n(rI);DMo=r(GVr,"MegatronBertForSequenceClassification"),GVr.forEach(t),jMo=r(D5e," (MegatronBert model)"),D5e.forEach(t),NMo=i(L),o2=s(L,"LI",{});var j5e=n(o2);jre=s(j5e,"STRONG",{});var OVr=n(jre);qMo=r(OVr,"mobilebert"),OVr.forEach(t),GMo=r(j5e," \u2014 "),tI=s(j5e,"A",{href:!0});var XVr=n(tI);OMo=r(XVr,"MobileBertForSequenceClassification"),XVr.forEach(t),XMo=r(j5e," (MobileBERT model)"),j5e.forEach(t),VMo=i(L),r2=s(L,"LI",{});var N5e=n(r2);Nre=s(N5e,"STRONG",{});var VVr=n(Nre);zMo=r(VVr,"mpnet"),VVr.forEach(t),WMo=r(N5e," \u2014 "),aI=s(N5e,"A",{href:!0});var zVr=n(aI);QMo=r(zVr,"MPNetForSequenceClassification"),zVr.forEach(t),HMo=r(N5e," (MPNet model)"),N5e.forEach(t),UMo=i(L),t2=s(L,"LI",{});var q5e=n(t2);qre=s(q5e,"STRONG",{});var WVr=n(qre);JMo=r(WVr,"nystromformer"),WVr.forEach(t),YMo=r(q5e," \u2014 "),sI=s(q5e,"A",{href:!0});var QVr=n(sI);KMo=r(QVr,"NystromformerForSequenceClassification"),QVr.forEach(t),ZMo=r(q5e," (Nystromformer model)"),q5e.forEach(t),eEo=i(L),a2=s(L,"LI",{});var G5e=n(a2);Gre=s(G5e,"STRONG",{});var HVr=n(Gre);oEo=r(HVr,"openai-gpt"),HVr.forEach(t),rEo=r(G5e," \u2014 "),nI=s(G5e,"A",{href:!0});var UVr=n(nI);tEo=r(UVr,"OpenAIGPTForSequenceClassification"),UVr.forEach(t),aEo=r(G5e," (OpenAI GPT model)"),G5e.forEach(t),sEo=i(L),s2=s(L,"LI",{});var O5e=n(s2);Ore=s(O5e,"STRONG",{});var JVr=n(Ore);nEo=r(JVr,"perceiver"),JVr.forEach(t),lEo=r(O5e," \u2014 "),lI=s(O5e,"A",{href:!0});var YVr=n(lI);iEo=r(YVr,"PerceiverForSequenceClassification"),YVr.forEach(t),dEo=r(O5e," (Perceiver model)"),O5e.forEach(t),cEo=i(L),n2=s(L,"LI",{});var X5e=n(n2);Xre=s(X5e,"STRONG",{});var KVr=n(Xre);mEo=r(KVr,"plbart"),KVr.forEach(t),fEo=r(X5e," \u2014 "),iI=s(X5e,"A",{href:!0});var ZVr=n(iI);gEo=r(ZVr,"PLBartForSequenceClassification"),ZVr.forEach(t),hEo=r(X5e," (PLBart model)"),X5e.forEach(t),uEo=i(L),l2=s(L,"LI",{});var V5e=n(l2);Vre=s(V5e,"STRONG",{});var ezr=n(Vre);pEo=r(ezr,"qdqbert"),ezr.forEach(t),_Eo=r(V5e," \u2014 "),dI=s(V5e,"A",{href:!0});var ozr=n(dI);bEo=r(ozr,"QDQBertForSequenceClassification"),ozr.forEach(t),vEo=r(V5e," (QDQBert model)"),V5e.forEach(t),TEo=i(L),i2=s(L,"LI",{});var z5e=n(i2);zre=s(z5e,"STRONG",{});var rzr=n(zre);FEo=r(rzr,"reformer"),rzr.forEach(t),CEo=r(z5e," \u2014 "),cI=s(z5e,"A",{href:!0});var tzr=n(cI);MEo=r(tzr,"ReformerForSequenceClassification"),tzr.forEach(t),EEo=r(z5e," (Reformer model)"),z5e.forEach(t),yEo=i(L),d2=s(L,"LI",{});var W5e=n(d2);Wre=s(W5e,"STRONG",{});var azr=n(Wre);wEo=r(azr,"rembert"),azr.forEach(t),AEo=r(W5e," \u2014 "),mI=s(W5e,"A",{href:!0});var szr=n(mI);LEo=r(szr,"RemBertForSequenceClassification"),szr.forEach(t),BEo=r(W5e," (RemBERT model)"),W5e.forEach(t),xEo=i(L),c2=s(L,"LI",{});var Q5e=n(c2);Qre=s(Q5e,"STRONG",{});var nzr=n(Qre);kEo=r(nzr,"roberta"),nzr.forEach(t),REo=r(Q5e," \u2014 "),fI=s(Q5e,"A",{href:!0});var lzr=n(fI);SEo=r(lzr,"RobertaForSequenceClassification"),lzr.forEach(t),PEo=r(Q5e," (RoBERTa model)"),Q5e.forEach(t),$Eo=i(L),m2=s(L,"LI",{});var H5e=n(m2);Hre=s(H5e,"STRONG",{});var izr=n(Hre);IEo=r(izr,"roformer"),izr.forEach(t),DEo=r(H5e," \u2014 "),gI=s(H5e,"A",{href:!0});var dzr=n(gI);jEo=r(dzr,"RoFormerForSequenceClassification"),dzr.forEach(t),NEo=r(H5e," (RoFormer model)"),H5e.forEach(t),qEo=i(L),f2=s(L,"LI",{});var U5e=n(f2);Ure=s(U5e,"STRONG",{});var czr=n(Ure);GEo=r(czr,"squeezebert"),czr.forEach(t),OEo=r(U5e," \u2014 "),hI=s(U5e,"A",{href:!0});var mzr=n(hI);XEo=r(mzr,"SqueezeBertForSequenceClassification"),mzr.forEach(t),VEo=r(U5e," (SqueezeBERT model)"),U5e.forEach(t),zEo=i(L),g2=s(L,"LI",{});var J5e=n(g2);Jre=s(J5e,"STRONG",{});var fzr=n(Jre);WEo=r(fzr,"tapas"),fzr.forEach(t),QEo=r(J5e," \u2014 "),uI=s(J5e,"A",{href:!0});var gzr=n(uI);HEo=r(gzr,"TapasForSequenceClassification"),gzr.forEach(t),UEo=r(J5e," (TAPAS model)"),J5e.forEach(t),JEo=i(L),h2=s(L,"LI",{});var Y5e=n(h2);Yre=s(Y5e,"STRONG",{});var hzr=n(Yre);YEo=r(hzr,"transfo-xl"),hzr.forEach(t),KEo=r(Y5e," \u2014 "),pI=s(Y5e,"A",{href:!0});var uzr=n(pI);ZEo=r(uzr,"TransfoXLForSequenceClassification"),uzr.forEach(t),e3o=r(Y5e," (Transformer-XL model)"),Y5e.forEach(t),o3o=i(L),u2=s(L,"LI",{});var K5e=n(u2);Kre=s(K5e,"STRONG",{});var pzr=n(Kre);r3o=r(pzr,"xlm"),pzr.forEach(t),t3o=r(K5e," \u2014 "),_I=s(K5e,"A",{href:!0});var _zr=n(_I);a3o=r(_zr,"XLMForSequenceClassification"),_zr.forEach(t),s3o=r(K5e," (XLM model)"),K5e.forEach(t),n3o=i(L),p2=s(L,"LI",{});var Z5e=n(p2);Zre=s(Z5e,"STRONG",{});var bzr=n(Zre);l3o=r(bzr,"xlm-roberta"),bzr.forEach(t),i3o=r(Z5e," \u2014 "),bI=s(Z5e,"A",{href:!0});var vzr=n(bI);d3o=r(vzr,"XLMRobertaForSequenceClassification"),vzr.forEach(t),c3o=r(Z5e," (XLM-RoBERTa model)"),Z5e.forEach(t),m3o=i(L),_2=s(L,"LI",{});var eye=n(_2);ete=s(eye,"STRONG",{});var Tzr=n(ete);f3o=r(Tzr,"xlm-roberta-xl"),Tzr.forEach(t),g3o=r(eye," \u2014 "),vI=s(eye,"A",{href:!0});var Fzr=n(vI);h3o=r(Fzr,"XLMRobertaXLForSequenceClassification"),Fzr.forEach(t),u3o=r(eye," (XLM-RoBERTa-XL model)"),eye.forEach(t),p3o=i(L),b2=s(L,"LI",{});var oye=n(b2);ote=s(oye,"STRONG",{});var Czr=n(ote);_3o=r(Czr,"xlnet"),Czr.forEach(t),b3o=r(oye," \u2014 "),TI=s(oye,"A",{href:!0});var Mzr=n(TI);v3o=r(Mzr,"XLNetForSequenceClassification"),Mzr.forEach(t),T3o=r(oye," (XLNet model)"),oye.forEach(t),F3o=i(L),v2=s(L,"LI",{});var rye=n(v2);rte=s(rye,"STRONG",{});var Ezr=n(rte);C3o=r(Ezr,"yoso"),Ezr.forEach(t),M3o=r(rye," \u2014 "),FI=s(rye,"A",{href:!0});var yzr=n(FI);E3o=r(yzr,"YosoForSequenceClassification"),yzr.forEach(t),y3o=r(rye," (YOSO model)"),rye.forEach(t),L.forEach(t),w3o=i(qt),T2=s(qt,"P",{});var tye=n(T2);A3o=r(tye,"The model is set in evaluation mode by default using "),tte=s(tye,"CODE",{});var wzr=n(tte);L3o=r(wzr,"model.eval()"),wzr.forEach(t),B3o=r(tye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ate=s(tye,"CODE",{});var Azr=n(ate);x3o=r(Azr,"model.train()"),Azr.forEach(t),tye.forEach(t),k3o=i(qt),ste=s(qt,"P",{});var Lzr=n(ste);R3o=r(Lzr,"Examples:"),Lzr.forEach(t),S3o=i(qt),f(Gy.$$.fragment,qt),qt.forEach(t),Jn.forEach(t),O9e=i(c),nd=s(c,"H2",{class:!0});var Uxe=n(nd);F2=s(Uxe,"A",{id:!0,class:!0,href:!0});var Bzr=n(F2);nte=s(Bzr,"SPAN",{});var xzr=n(nte);f(Oy.$$.fragment,xzr),xzr.forEach(t),Bzr.forEach(t),P3o=i(Uxe),lte=s(Uxe,"SPAN",{});var kzr=n(lte);$3o=r(kzr,"AutoModelForMultipleChoice"),kzr.forEach(t),Uxe.forEach(t),X9e=i(c),Zo=s(c,"DIV",{class:!0});var Kn=n(Zo);f(Xy.$$.fragment,Kn),I3o=i(Kn),ld=s(Kn,"P",{});var GV=n(ld);D3o=r(GV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),ite=s(GV,"CODE",{});var Rzr=n(ite);j3o=r(Rzr,"from_pretrained()"),Rzr.forEach(t),N3o=r(GV,"class method or the "),dte=s(GV,"CODE",{});var Szr=n(dte);q3o=r(Szr,"from_config()"),Szr.forEach(t),G3o=r(GV,`class
method.`),GV.forEach(t),O3o=i(Kn),Vy=s(Kn,"P",{});var Jxe=n(Vy);X3o=r(Jxe,"This class cannot be instantiated directly using "),cte=s(Jxe,"CODE",{});var Pzr=n(cte);V3o=r(Pzr,"__init__()"),Pzr.forEach(t),z3o=r(Jxe," (throws an error)."),Jxe.forEach(t),W3o=i(Kn),Wr=s(Kn,"DIV",{class:!0});var Zn=n(Wr);f(zy.$$.fragment,Zn),Q3o=i(Zn),mte=s(Zn,"P",{});var $zr=n(mte);H3o=r($zr,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),$zr.forEach(t),U3o=i(Zn),id=s(Zn,"P",{});var OV=n(id);J3o=r(OV,`Note:
Loading a model from its configuration file does `),fte=s(OV,"STRONG",{});var Izr=n(fte);Y3o=r(Izr,"not"),Izr.forEach(t),K3o=r(OV,` load the model weights. It only affects the
model\u2019s configuration. Use `),gte=s(OV,"CODE",{});var Dzr=n(gte);Z3o=r(Dzr,"from_pretrained()"),Dzr.forEach(t),e5o=r(OV,"to load the model weights."),OV.forEach(t),o5o=i(Zn),hte=s(Zn,"P",{});var jzr=n(hte);r5o=r(jzr,"Examples:"),jzr.forEach(t),t5o=i(Zn),f(Wy.$$.fragment,Zn),Zn.forEach(t),a5o=i(Kn),je=s(Kn,"DIV",{class:!0});var Gt=n(je);f(Qy.$$.fragment,Gt),s5o=i(Gt),ute=s(Gt,"P",{});var Nzr=n(ute);n5o=r(Nzr,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Nzr.forEach(t),l5o=i(Gt),za=s(Gt,"P",{});var x3=n(za);i5o=r(x3,"The model class to instantiate is selected based on the "),pte=s(x3,"CODE",{});var qzr=n(pte);d5o=r(qzr,"model_type"),qzr.forEach(t),c5o=r(x3,` property of the config object (either
passed as an argument or loaded from `),_te=s(x3,"CODE",{});var Gzr=n(_te);m5o=r(Gzr,"pretrained_model_name_or_path"),Gzr.forEach(t),f5o=r(x3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),bte=s(x3,"CODE",{});var Ozr=n(bte);g5o=r(Ozr,"pretrained_model_name_or_path"),Ozr.forEach(t),h5o=r(x3,":"),x3.forEach(t),u5o=i(Gt),G=s(Gt,"UL",{});var O=n(G);C2=s(O,"LI",{});var aye=n(C2);vte=s(aye,"STRONG",{});var Xzr=n(vte);p5o=r(Xzr,"albert"),Xzr.forEach(t),_5o=r(aye," \u2014 "),CI=s(aye,"A",{href:!0});var Vzr=n(CI);b5o=r(Vzr,"AlbertForMultipleChoice"),Vzr.forEach(t),v5o=r(aye," (ALBERT model)"),aye.forEach(t),T5o=i(O),M2=s(O,"LI",{});var sye=n(M2);Tte=s(sye,"STRONG",{});var zzr=n(Tte);F5o=r(zzr,"bert"),zzr.forEach(t),C5o=r(sye," \u2014 "),MI=s(sye,"A",{href:!0});var Wzr=n(MI);M5o=r(Wzr,"BertForMultipleChoice"),Wzr.forEach(t),E5o=r(sye," (BERT model)"),sye.forEach(t),y5o=i(O),E2=s(O,"LI",{});var nye=n(E2);Fte=s(nye,"STRONG",{});var Qzr=n(Fte);w5o=r(Qzr,"big_bird"),Qzr.forEach(t),A5o=r(nye," \u2014 "),EI=s(nye,"A",{href:!0});var Hzr=n(EI);L5o=r(Hzr,"BigBirdForMultipleChoice"),Hzr.forEach(t),B5o=r(nye," (BigBird model)"),nye.forEach(t),x5o=i(O),y2=s(O,"LI",{});var lye=n(y2);Cte=s(lye,"STRONG",{});var Uzr=n(Cte);k5o=r(Uzr,"camembert"),Uzr.forEach(t),R5o=r(lye," \u2014 "),yI=s(lye,"A",{href:!0});var Jzr=n(yI);S5o=r(Jzr,"CamembertForMultipleChoice"),Jzr.forEach(t),P5o=r(lye," (CamemBERT model)"),lye.forEach(t),$5o=i(O),w2=s(O,"LI",{});var iye=n(w2);Mte=s(iye,"STRONG",{});var Yzr=n(Mte);I5o=r(Yzr,"canine"),Yzr.forEach(t),D5o=r(iye," \u2014 "),wI=s(iye,"A",{href:!0});var Kzr=n(wI);j5o=r(Kzr,"CanineForMultipleChoice"),Kzr.forEach(t),N5o=r(iye," (Canine model)"),iye.forEach(t),q5o=i(O),A2=s(O,"LI",{});var dye=n(A2);Ete=s(dye,"STRONG",{});var Zzr=n(Ete);G5o=r(Zzr,"convbert"),Zzr.forEach(t),O5o=r(dye," \u2014 "),AI=s(dye,"A",{href:!0});var eWr=n(AI);X5o=r(eWr,"ConvBertForMultipleChoice"),eWr.forEach(t),V5o=r(dye," (ConvBERT model)"),dye.forEach(t),z5o=i(O),L2=s(O,"LI",{});var cye=n(L2);yte=s(cye,"STRONG",{});var oWr=n(yte);W5o=r(oWr,"data2vec-text"),oWr.forEach(t),Q5o=r(cye," \u2014 "),LI=s(cye,"A",{href:!0});var rWr=n(LI);H5o=r(rWr,"Data2VecTextForMultipleChoice"),rWr.forEach(t),U5o=r(cye," (Data2VecText model)"),cye.forEach(t),J5o=i(O),B2=s(O,"LI",{});var mye=n(B2);wte=s(mye,"STRONG",{});var tWr=n(wte);Y5o=r(tWr,"distilbert"),tWr.forEach(t),K5o=r(mye," \u2014 "),BI=s(mye,"A",{href:!0});var aWr=n(BI);Z5o=r(aWr,"DistilBertForMultipleChoice"),aWr.forEach(t),eyo=r(mye," (DistilBERT model)"),mye.forEach(t),oyo=i(O),x2=s(O,"LI",{});var fye=n(x2);Ate=s(fye,"STRONG",{});var sWr=n(Ate);ryo=r(sWr,"electra"),sWr.forEach(t),tyo=r(fye," \u2014 "),xI=s(fye,"A",{href:!0});var nWr=n(xI);ayo=r(nWr,"ElectraForMultipleChoice"),nWr.forEach(t),syo=r(fye," (ELECTRA model)"),fye.forEach(t),nyo=i(O),k2=s(O,"LI",{});var gye=n(k2);Lte=s(gye,"STRONG",{});var lWr=n(Lte);lyo=r(lWr,"flaubert"),lWr.forEach(t),iyo=r(gye," \u2014 "),kI=s(gye,"A",{href:!0});var iWr=n(kI);dyo=r(iWr,"FlaubertForMultipleChoice"),iWr.forEach(t),cyo=r(gye," (FlauBERT model)"),gye.forEach(t),myo=i(O),R2=s(O,"LI",{});var hye=n(R2);Bte=s(hye,"STRONG",{});var dWr=n(Bte);fyo=r(dWr,"fnet"),dWr.forEach(t),gyo=r(hye," \u2014 "),RI=s(hye,"A",{href:!0});var cWr=n(RI);hyo=r(cWr,"FNetForMultipleChoice"),cWr.forEach(t),uyo=r(hye," (FNet model)"),hye.forEach(t),pyo=i(O),S2=s(O,"LI",{});var uye=n(S2);xte=s(uye,"STRONG",{});var mWr=n(xte);_yo=r(mWr,"funnel"),mWr.forEach(t),byo=r(uye," \u2014 "),SI=s(uye,"A",{href:!0});var fWr=n(SI);vyo=r(fWr,"FunnelForMultipleChoice"),fWr.forEach(t),Tyo=r(uye," (Funnel Transformer model)"),uye.forEach(t),Fyo=i(O),P2=s(O,"LI",{});var pye=n(P2);kte=s(pye,"STRONG",{});var gWr=n(kte);Cyo=r(gWr,"ibert"),gWr.forEach(t),Myo=r(pye," \u2014 "),PI=s(pye,"A",{href:!0});var hWr=n(PI);Eyo=r(hWr,"IBertForMultipleChoice"),hWr.forEach(t),yyo=r(pye," (I-BERT model)"),pye.forEach(t),wyo=i(O),$2=s(O,"LI",{});var _ye=n($2);Rte=s(_ye,"STRONG",{});var uWr=n(Rte);Ayo=r(uWr,"longformer"),uWr.forEach(t),Lyo=r(_ye," \u2014 "),$I=s(_ye,"A",{href:!0});var pWr=n($I);Byo=r(pWr,"LongformerForMultipleChoice"),pWr.forEach(t),xyo=r(_ye," (Longformer model)"),_ye.forEach(t),kyo=i(O),I2=s(O,"LI",{});var bye=n(I2);Ste=s(bye,"STRONG",{});var _Wr=n(Ste);Ryo=r(_Wr,"megatron-bert"),_Wr.forEach(t),Syo=r(bye," \u2014 "),II=s(bye,"A",{href:!0});var bWr=n(II);Pyo=r(bWr,"MegatronBertForMultipleChoice"),bWr.forEach(t),$yo=r(bye," (MegatronBert model)"),bye.forEach(t),Iyo=i(O),D2=s(O,"LI",{});var vye=n(D2);Pte=s(vye,"STRONG",{});var vWr=n(Pte);Dyo=r(vWr,"mobilebert"),vWr.forEach(t),jyo=r(vye," \u2014 "),DI=s(vye,"A",{href:!0});var TWr=n(DI);Nyo=r(TWr,"MobileBertForMultipleChoice"),TWr.forEach(t),qyo=r(vye," (MobileBERT model)"),vye.forEach(t),Gyo=i(O),j2=s(O,"LI",{});var Tye=n(j2);$te=s(Tye,"STRONG",{});var FWr=n($te);Oyo=r(FWr,"mpnet"),FWr.forEach(t),Xyo=r(Tye," \u2014 "),jI=s(Tye,"A",{href:!0});var CWr=n(jI);Vyo=r(CWr,"MPNetForMultipleChoice"),CWr.forEach(t),zyo=r(Tye," (MPNet model)"),Tye.forEach(t),Wyo=i(O),N2=s(O,"LI",{});var Fye=n(N2);Ite=s(Fye,"STRONG",{});var MWr=n(Ite);Qyo=r(MWr,"nystromformer"),MWr.forEach(t),Hyo=r(Fye," \u2014 "),NI=s(Fye,"A",{href:!0});var EWr=n(NI);Uyo=r(EWr,"NystromformerForMultipleChoice"),EWr.forEach(t),Jyo=r(Fye," (Nystromformer model)"),Fye.forEach(t),Yyo=i(O),q2=s(O,"LI",{});var Cye=n(q2);Dte=s(Cye,"STRONG",{});var yWr=n(Dte);Kyo=r(yWr,"qdqbert"),yWr.forEach(t),Zyo=r(Cye," \u2014 "),qI=s(Cye,"A",{href:!0});var wWr=n(qI);ewo=r(wWr,"QDQBertForMultipleChoice"),wWr.forEach(t),owo=r(Cye," (QDQBert model)"),Cye.forEach(t),rwo=i(O),G2=s(O,"LI",{});var Mye=n(G2);jte=s(Mye,"STRONG",{});var AWr=n(jte);two=r(AWr,"rembert"),AWr.forEach(t),awo=r(Mye," \u2014 "),GI=s(Mye,"A",{href:!0});var LWr=n(GI);swo=r(LWr,"RemBertForMultipleChoice"),LWr.forEach(t),nwo=r(Mye," (RemBERT model)"),Mye.forEach(t),lwo=i(O),O2=s(O,"LI",{});var Eye=n(O2);Nte=s(Eye,"STRONG",{});var BWr=n(Nte);iwo=r(BWr,"roberta"),BWr.forEach(t),dwo=r(Eye," \u2014 "),OI=s(Eye,"A",{href:!0});var xWr=n(OI);cwo=r(xWr,"RobertaForMultipleChoice"),xWr.forEach(t),mwo=r(Eye," (RoBERTa model)"),Eye.forEach(t),fwo=i(O),X2=s(O,"LI",{});var yye=n(X2);qte=s(yye,"STRONG",{});var kWr=n(qte);gwo=r(kWr,"roformer"),kWr.forEach(t),hwo=r(yye," \u2014 "),XI=s(yye,"A",{href:!0});var RWr=n(XI);uwo=r(RWr,"RoFormerForMultipleChoice"),RWr.forEach(t),pwo=r(yye," (RoFormer model)"),yye.forEach(t),_wo=i(O),V2=s(O,"LI",{});var wye=n(V2);Gte=s(wye,"STRONG",{});var SWr=n(Gte);bwo=r(SWr,"squeezebert"),SWr.forEach(t),vwo=r(wye," \u2014 "),VI=s(wye,"A",{href:!0});var PWr=n(VI);Two=r(PWr,"SqueezeBertForMultipleChoice"),PWr.forEach(t),Fwo=r(wye," (SqueezeBERT model)"),wye.forEach(t),Cwo=i(O),z2=s(O,"LI",{});var Aye=n(z2);Ote=s(Aye,"STRONG",{});var $Wr=n(Ote);Mwo=r($Wr,"xlm"),$Wr.forEach(t),Ewo=r(Aye," \u2014 "),zI=s(Aye,"A",{href:!0});var IWr=n(zI);ywo=r(IWr,"XLMForMultipleChoice"),IWr.forEach(t),wwo=r(Aye," (XLM model)"),Aye.forEach(t),Awo=i(O),W2=s(O,"LI",{});var Lye=n(W2);Xte=s(Lye,"STRONG",{});var DWr=n(Xte);Lwo=r(DWr,"xlm-roberta"),DWr.forEach(t),Bwo=r(Lye," \u2014 "),WI=s(Lye,"A",{href:!0});var jWr=n(WI);xwo=r(jWr,"XLMRobertaForMultipleChoice"),jWr.forEach(t),kwo=r(Lye," (XLM-RoBERTa model)"),Lye.forEach(t),Rwo=i(O),Q2=s(O,"LI",{});var Bye=n(Q2);Vte=s(Bye,"STRONG",{});var NWr=n(Vte);Swo=r(NWr,"xlm-roberta-xl"),NWr.forEach(t),Pwo=r(Bye," \u2014 "),QI=s(Bye,"A",{href:!0});var qWr=n(QI);$wo=r(qWr,"XLMRobertaXLForMultipleChoice"),qWr.forEach(t),Iwo=r(Bye," (XLM-RoBERTa-XL model)"),Bye.forEach(t),Dwo=i(O),H2=s(O,"LI",{});var xye=n(H2);zte=s(xye,"STRONG",{});var GWr=n(zte);jwo=r(GWr,"xlnet"),GWr.forEach(t),Nwo=r(xye," \u2014 "),HI=s(xye,"A",{href:!0});var OWr=n(HI);qwo=r(OWr,"XLNetForMultipleChoice"),OWr.forEach(t),Gwo=r(xye," (XLNet model)"),xye.forEach(t),Owo=i(O),U2=s(O,"LI",{});var kye=n(U2);Wte=s(kye,"STRONG",{});var XWr=n(Wte);Xwo=r(XWr,"yoso"),XWr.forEach(t),Vwo=r(kye," \u2014 "),UI=s(kye,"A",{href:!0});var VWr=n(UI);zwo=r(VWr,"YosoForMultipleChoice"),VWr.forEach(t),Wwo=r(kye," (YOSO model)"),kye.forEach(t),O.forEach(t),Qwo=i(Gt),J2=s(Gt,"P",{});var Rye=n(J2);Hwo=r(Rye,"The model is set in evaluation mode by default using "),Qte=s(Rye,"CODE",{});var zWr=n(Qte);Uwo=r(zWr,"model.eval()"),zWr.forEach(t),Jwo=r(Rye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Hte=s(Rye,"CODE",{});var WWr=n(Hte);Ywo=r(WWr,"model.train()"),WWr.forEach(t),Rye.forEach(t),Kwo=i(Gt),Ute=s(Gt,"P",{});var QWr=n(Ute);Zwo=r(QWr,"Examples:"),QWr.forEach(t),e6o=i(Gt),f(Hy.$$.fragment,Gt),Gt.forEach(t),Kn.forEach(t),V9e=i(c),dd=s(c,"H2",{class:!0});var Yxe=n(dd);Y2=s(Yxe,"A",{id:!0,class:!0,href:!0});var HWr=n(Y2);Jte=s(HWr,"SPAN",{});var UWr=n(Jte);f(Uy.$$.fragment,UWr),UWr.forEach(t),HWr.forEach(t),o6o=i(Yxe),Yte=s(Yxe,"SPAN",{});var JWr=n(Yte);r6o=r(JWr,"AutoModelForNextSentencePrediction"),JWr.forEach(t),Yxe.forEach(t),z9e=i(c),er=s(c,"DIV",{class:!0});var el=n(er);f(Jy.$$.fragment,el),t6o=i(el),cd=s(el,"P",{});var XV=n(cd);a6o=r(XV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),Kte=s(XV,"CODE",{});var YWr=n(Kte);s6o=r(YWr,"from_pretrained()"),YWr.forEach(t),n6o=r(XV,"class method or the "),Zte=s(XV,"CODE",{});var KWr=n(Zte);l6o=r(KWr,"from_config()"),KWr.forEach(t),i6o=r(XV,`class
method.`),XV.forEach(t),d6o=i(el),Yy=s(el,"P",{});var Kxe=n(Yy);c6o=r(Kxe,"This class cannot be instantiated directly using "),eae=s(Kxe,"CODE",{});var ZWr=n(eae);m6o=r(ZWr,"__init__()"),ZWr.forEach(t),f6o=r(Kxe," (throws an error)."),Kxe.forEach(t),g6o=i(el),Qr=s(el,"DIV",{class:!0});var ol=n(Qr);f(Ky.$$.fragment,ol),h6o=i(ol),oae=s(ol,"P",{});var eQr=n(oae);u6o=r(eQr,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),eQr.forEach(t),p6o=i(ol),md=s(ol,"P",{});var VV=n(md);_6o=r(VV,`Note:
Loading a model from its configuration file does `),rae=s(VV,"STRONG",{});var oQr=n(rae);b6o=r(oQr,"not"),oQr.forEach(t),v6o=r(VV,` load the model weights. It only affects the
model\u2019s configuration. Use `),tae=s(VV,"CODE",{});var rQr=n(tae);T6o=r(rQr,"from_pretrained()"),rQr.forEach(t),F6o=r(VV,"to load the model weights."),VV.forEach(t),C6o=i(ol),aae=s(ol,"P",{});var tQr=n(aae);M6o=r(tQr,"Examples:"),tQr.forEach(t),E6o=i(ol),f(Zy.$$.fragment,ol),ol.forEach(t),y6o=i(el),Ne=s(el,"DIV",{class:!0});var Ot=n(Ne);f(ew.$$.fragment,Ot),w6o=i(Ot),sae=s(Ot,"P",{});var aQr=n(sae);A6o=r(aQr,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),aQr.forEach(t),L6o=i(Ot),Wa=s(Ot,"P",{});var k3=n(Wa);B6o=r(k3,"The model class to instantiate is selected based on the "),nae=s(k3,"CODE",{});var sQr=n(nae);x6o=r(sQr,"model_type"),sQr.forEach(t),k6o=r(k3,` property of the config object (either
passed as an argument or loaded from `),lae=s(k3,"CODE",{});var nQr=n(lae);R6o=r(nQr,"pretrained_model_name_or_path"),nQr.forEach(t),S6o=r(k3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iae=s(k3,"CODE",{});var lQr=n(iae);P6o=r(lQr,"pretrained_model_name_or_path"),lQr.forEach(t),$6o=r(k3,":"),k3.forEach(t),I6o=i(Ot),sa=s(Ot,"UL",{});var rl=n(sa);K2=s(rl,"LI",{});var Sye=n(K2);dae=s(Sye,"STRONG",{});var iQr=n(dae);D6o=r(iQr,"bert"),iQr.forEach(t),j6o=r(Sye," \u2014 "),JI=s(Sye,"A",{href:!0});var dQr=n(JI);N6o=r(dQr,"BertForNextSentencePrediction"),dQr.forEach(t),q6o=r(Sye," (BERT model)"),Sye.forEach(t),G6o=i(rl),Z2=s(rl,"LI",{});var Pye=n(Z2);cae=s(Pye,"STRONG",{});var cQr=n(cae);O6o=r(cQr,"fnet"),cQr.forEach(t),X6o=r(Pye," \u2014 "),YI=s(Pye,"A",{href:!0});var mQr=n(YI);V6o=r(mQr,"FNetForNextSentencePrediction"),mQr.forEach(t),z6o=r(Pye," (FNet model)"),Pye.forEach(t),W6o=i(rl),ev=s(rl,"LI",{});var $ye=n(ev);mae=s($ye,"STRONG",{});var fQr=n(mae);Q6o=r(fQr,"megatron-bert"),fQr.forEach(t),H6o=r($ye," \u2014 "),KI=s($ye,"A",{href:!0});var gQr=n(KI);U6o=r(gQr,"MegatronBertForNextSentencePrediction"),gQr.forEach(t),J6o=r($ye," (MegatronBert model)"),$ye.forEach(t),Y6o=i(rl),ov=s(rl,"LI",{});var Iye=n(ov);fae=s(Iye,"STRONG",{});var hQr=n(fae);K6o=r(hQr,"mobilebert"),hQr.forEach(t),Z6o=r(Iye," \u2014 "),ZI=s(Iye,"A",{href:!0});var uQr=n(ZI);eAo=r(uQr,"MobileBertForNextSentencePrediction"),uQr.forEach(t),oAo=r(Iye," (MobileBERT model)"),Iye.forEach(t),rAo=i(rl),rv=s(rl,"LI",{});var Dye=n(rv);gae=s(Dye,"STRONG",{});var pQr=n(gae);tAo=r(pQr,"qdqbert"),pQr.forEach(t),aAo=r(Dye," \u2014 "),eD=s(Dye,"A",{href:!0});var _Qr=n(eD);sAo=r(_Qr,"QDQBertForNextSentencePrediction"),_Qr.forEach(t),nAo=r(Dye," (QDQBert model)"),Dye.forEach(t),rl.forEach(t),lAo=i(Ot),tv=s(Ot,"P",{});var jye=n(tv);iAo=r(jye,"The model is set in evaluation mode by default using "),hae=s(jye,"CODE",{});var bQr=n(hae);dAo=r(bQr,"model.eval()"),bQr.forEach(t),cAo=r(jye,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),uae=s(jye,"CODE",{});var vQr=n(uae);mAo=r(vQr,"model.train()"),vQr.forEach(t),jye.forEach(t),fAo=i(Ot),pae=s(Ot,"P",{});var TQr=n(pae);gAo=r(TQr,"Examples:"),TQr.forEach(t),hAo=i(Ot),f(ow.$$.fragment,Ot),Ot.forEach(t),el.forEach(t),W9e=i(c),fd=s(c,"H2",{class:!0});var Zxe=n(fd);av=s(Zxe,"A",{id:!0,class:!0,href:!0});var FQr=n(av);_ae=s(FQr,"SPAN",{});var CQr=n(_ae);f(rw.$$.fragment,CQr),CQr.forEach(t),FQr.forEach(t),uAo=i(Zxe),bae=s(Zxe,"SPAN",{});var MQr=n(bae);pAo=r(MQr,"AutoModelForTokenClassification"),MQr.forEach(t),Zxe.forEach(t),Q9e=i(c),or=s(c,"DIV",{class:!0});var tl=n(or);f(tw.$$.fragment,tl),_Ao=i(tl),gd=s(tl,"P",{});var zV=n(gd);bAo=r(zV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),vae=s(zV,"CODE",{});var EQr=n(vae);vAo=r(EQr,"from_pretrained()"),EQr.forEach(t),TAo=r(zV,"class method or the "),Tae=s(zV,"CODE",{});var yQr=n(Tae);FAo=r(yQr,"from_config()"),yQr.forEach(t),CAo=r(zV,`class
method.`),zV.forEach(t),MAo=i(tl),aw=s(tl,"P",{});var eke=n(aw);EAo=r(eke,"This class cannot be instantiated directly using "),Fae=s(eke,"CODE",{});var wQr=n(Fae);yAo=r(wQr,"__init__()"),wQr.forEach(t),wAo=r(eke," (throws an error)."),eke.forEach(t),AAo=i(tl),Hr=s(tl,"DIV",{class:!0});var al=n(Hr);f(sw.$$.fragment,al),LAo=i(al),Cae=s(al,"P",{});var AQr=n(Cae);BAo=r(AQr,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),AQr.forEach(t),xAo=i(al),hd=s(al,"P",{});var WV=n(hd);kAo=r(WV,`Note:
Loading a model from its configuration file does `),Mae=s(WV,"STRONG",{});var LQr=n(Mae);RAo=r(LQr,"not"),LQr.forEach(t),SAo=r(WV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Eae=s(WV,"CODE",{});var BQr=n(Eae);PAo=r(BQr,"from_pretrained()"),BQr.forEach(t),$Ao=r(WV,"to load the model weights."),WV.forEach(t),IAo=i(al),yae=s(al,"P",{});var xQr=n(yae);DAo=r(xQr,"Examples:"),xQr.forEach(t),jAo=i(al),f(nw.$$.fragment,al),al.forEach(t),NAo=i(tl),qe=s(tl,"DIV",{class:!0});var Xt=n(qe);f(lw.$$.fragment,Xt),qAo=i(Xt),wae=s(Xt,"P",{});var kQr=n(wae);GAo=r(kQr,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),kQr.forEach(t),OAo=i(Xt),Qa=s(Xt,"P",{});var R3=n(Qa);XAo=r(R3,"The model class to instantiate is selected based on the "),Aae=s(R3,"CODE",{});var RQr=n(Aae);VAo=r(RQr,"model_type"),RQr.forEach(t),zAo=r(R3,` property of the config object (either
passed as an argument or loaded from `),Lae=s(R3,"CODE",{});var SQr=n(Lae);WAo=r(SQr,"pretrained_model_name_or_path"),SQr.forEach(t),QAo=r(R3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Bae=s(R3,"CODE",{});var PQr=n(Bae);HAo=r(PQr,"pretrained_model_name_or_path"),PQr.forEach(t),UAo=r(R3,":"),R3.forEach(t),JAo=i(Xt),N=s(Xt,"UL",{});var q=n(N);sv=s(q,"LI",{});var Nye=n(sv);xae=s(Nye,"STRONG",{});var $Qr=n(xae);YAo=r($Qr,"albert"),$Qr.forEach(t),KAo=r(Nye," \u2014 "),oD=s(Nye,"A",{href:!0});var IQr=n(oD);ZAo=r(IQr,"AlbertForTokenClassification"),IQr.forEach(t),e0o=r(Nye," (ALBERT model)"),Nye.forEach(t),o0o=i(q),nv=s(q,"LI",{});var qye=n(nv);kae=s(qye,"STRONG",{});var DQr=n(kae);r0o=r(DQr,"bert"),DQr.forEach(t),t0o=r(qye," \u2014 "),rD=s(qye,"A",{href:!0});var jQr=n(rD);a0o=r(jQr,"BertForTokenClassification"),jQr.forEach(t),s0o=r(qye," (BERT model)"),qye.forEach(t),n0o=i(q),lv=s(q,"LI",{});var Gye=n(lv);Rae=s(Gye,"STRONG",{});var NQr=n(Rae);l0o=r(NQr,"big_bird"),NQr.forEach(t),i0o=r(Gye," \u2014 "),tD=s(Gye,"A",{href:!0});var qQr=n(tD);d0o=r(qQr,"BigBirdForTokenClassification"),qQr.forEach(t),c0o=r(Gye," (BigBird model)"),Gye.forEach(t),m0o=i(q),iv=s(q,"LI",{});var Oye=n(iv);Sae=s(Oye,"STRONG",{});var GQr=n(Sae);f0o=r(GQr,"camembert"),GQr.forEach(t),g0o=r(Oye," \u2014 "),aD=s(Oye,"A",{href:!0});var OQr=n(aD);h0o=r(OQr,"CamembertForTokenClassification"),OQr.forEach(t),u0o=r(Oye," (CamemBERT model)"),Oye.forEach(t),p0o=i(q),dv=s(q,"LI",{});var Xye=n(dv);Pae=s(Xye,"STRONG",{});var XQr=n(Pae);_0o=r(XQr,"canine"),XQr.forEach(t),b0o=r(Xye," \u2014 "),sD=s(Xye,"A",{href:!0});var VQr=n(sD);v0o=r(VQr,"CanineForTokenClassification"),VQr.forEach(t),T0o=r(Xye," (Canine model)"),Xye.forEach(t),F0o=i(q),cv=s(q,"LI",{});var Vye=n(cv);$ae=s(Vye,"STRONG",{});var zQr=n($ae);C0o=r(zQr,"convbert"),zQr.forEach(t),M0o=r(Vye," \u2014 "),nD=s(Vye,"A",{href:!0});var WQr=n(nD);E0o=r(WQr,"ConvBertForTokenClassification"),WQr.forEach(t),y0o=r(Vye," (ConvBERT model)"),Vye.forEach(t),w0o=i(q),mv=s(q,"LI",{});var zye=n(mv);Iae=s(zye,"STRONG",{});var QQr=n(Iae);A0o=r(QQr,"data2vec-text"),QQr.forEach(t),L0o=r(zye," \u2014 "),lD=s(zye,"A",{href:!0});var HQr=n(lD);B0o=r(HQr,"Data2VecTextForTokenClassification"),HQr.forEach(t),x0o=r(zye," (Data2VecText model)"),zye.forEach(t),k0o=i(q),fv=s(q,"LI",{});var Wye=n(fv);Dae=s(Wye,"STRONG",{});var UQr=n(Dae);R0o=r(UQr,"deberta"),UQr.forEach(t),S0o=r(Wye," \u2014 "),iD=s(Wye,"A",{href:!0});var JQr=n(iD);P0o=r(JQr,"DebertaForTokenClassification"),JQr.forEach(t),$0o=r(Wye," (DeBERTa model)"),Wye.forEach(t),I0o=i(q),gv=s(q,"LI",{});var Qye=n(gv);jae=s(Qye,"STRONG",{});var YQr=n(jae);D0o=r(YQr,"deberta-v2"),YQr.forEach(t),j0o=r(Qye," \u2014 "),dD=s(Qye,"A",{href:!0});var KQr=n(dD);N0o=r(KQr,"DebertaV2ForTokenClassification"),KQr.forEach(t),q0o=r(Qye," (DeBERTa-v2 model)"),Qye.forEach(t),G0o=i(q),hv=s(q,"LI",{});var Hye=n(hv);Nae=s(Hye,"STRONG",{});var ZQr=n(Nae);O0o=r(ZQr,"distilbert"),ZQr.forEach(t),X0o=r(Hye," \u2014 "),cD=s(Hye,"A",{href:!0});var eHr=n(cD);V0o=r(eHr,"DistilBertForTokenClassification"),eHr.forEach(t),z0o=r(Hye," (DistilBERT model)"),Hye.forEach(t),W0o=i(q),uv=s(q,"LI",{});var Uye=n(uv);qae=s(Uye,"STRONG",{});var oHr=n(qae);Q0o=r(oHr,"electra"),oHr.forEach(t),H0o=r(Uye," \u2014 "),mD=s(Uye,"A",{href:!0});var rHr=n(mD);U0o=r(rHr,"ElectraForTokenClassification"),rHr.forEach(t),J0o=r(Uye," (ELECTRA model)"),Uye.forEach(t),Y0o=i(q),pv=s(q,"LI",{});var Jye=n(pv);Gae=s(Jye,"STRONG",{});var tHr=n(Gae);K0o=r(tHr,"flaubert"),tHr.forEach(t),Z0o=r(Jye," \u2014 "),fD=s(Jye,"A",{href:!0});var aHr=n(fD);eLo=r(aHr,"FlaubertForTokenClassification"),aHr.forEach(t),oLo=r(Jye," (FlauBERT model)"),Jye.forEach(t),rLo=i(q),_v=s(q,"LI",{});var Yye=n(_v);Oae=s(Yye,"STRONG",{});var sHr=n(Oae);tLo=r(sHr,"fnet"),sHr.forEach(t),aLo=r(Yye," \u2014 "),gD=s(Yye,"A",{href:!0});var nHr=n(gD);sLo=r(nHr,"FNetForTokenClassification"),nHr.forEach(t),nLo=r(Yye," (FNet model)"),Yye.forEach(t),lLo=i(q),bv=s(q,"LI",{});var Kye=n(bv);Xae=s(Kye,"STRONG",{});var lHr=n(Xae);iLo=r(lHr,"funnel"),lHr.forEach(t),dLo=r(Kye," \u2014 "),hD=s(Kye,"A",{href:!0});var iHr=n(hD);cLo=r(iHr,"FunnelForTokenClassification"),iHr.forEach(t),mLo=r(Kye," (Funnel Transformer model)"),Kye.forEach(t),fLo=i(q),vv=s(q,"LI",{});var Zye=n(vv);Vae=s(Zye,"STRONG",{});var dHr=n(Vae);gLo=r(dHr,"gpt2"),dHr.forEach(t),hLo=r(Zye," \u2014 "),uD=s(Zye,"A",{href:!0});var cHr=n(uD);uLo=r(cHr,"GPT2ForTokenClassification"),cHr.forEach(t),pLo=r(Zye," (OpenAI GPT-2 model)"),Zye.forEach(t),_Lo=i(q),Tv=s(q,"LI",{});var ewe=n(Tv);zae=s(ewe,"STRONG",{});var mHr=n(zae);bLo=r(mHr,"ibert"),mHr.forEach(t),vLo=r(ewe," \u2014 "),pD=s(ewe,"A",{href:!0});var fHr=n(pD);TLo=r(fHr,"IBertForTokenClassification"),fHr.forEach(t),FLo=r(ewe," (I-BERT model)"),ewe.forEach(t),CLo=i(q),Fv=s(q,"LI",{});var owe=n(Fv);Wae=s(owe,"STRONG",{});var gHr=n(Wae);MLo=r(gHr,"layoutlm"),gHr.forEach(t),ELo=r(owe," \u2014 "),_D=s(owe,"A",{href:!0});var hHr=n(_D);yLo=r(hHr,"LayoutLMForTokenClassification"),hHr.forEach(t),wLo=r(owe," (LayoutLM model)"),owe.forEach(t),ALo=i(q),Cv=s(q,"LI",{});var rwe=n(Cv);Qae=s(rwe,"STRONG",{});var uHr=n(Qae);LLo=r(uHr,"layoutlmv2"),uHr.forEach(t),BLo=r(rwe," \u2014 "),bD=s(rwe,"A",{href:!0});var pHr=n(bD);xLo=r(pHr,"LayoutLMv2ForTokenClassification"),pHr.forEach(t),kLo=r(rwe," (LayoutLMv2 model)"),rwe.forEach(t),RLo=i(q),Mv=s(q,"LI",{});var twe=n(Mv);Hae=s(twe,"STRONG",{});var _Hr=n(Hae);SLo=r(_Hr,"longformer"),_Hr.forEach(t),PLo=r(twe," \u2014 "),vD=s(twe,"A",{href:!0});var bHr=n(vD);$Lo=r(bHr,"LongformerForTokenClassification"),bHr.forEach(t),ILo=r(twe," (Longformer model)"),twe.forEach(t),DLo=i(q),Ev=s(q,"LI",{});var awe=n(Ev);Uae=s(awe,"STRONG",{});var vHr=n(Uae);jLo=r(vHr,"megatron-bert"),vHr.forEach(t),NLo=r(awe," \u2014 "),TD=s(awe,"A",{href:!0});var THr=n(TD);qLo=r(THr,"MegatronBertForTokenClassification"),THr.forEach(t),GLo=r(awe," (MegatronBert model)"),awe.forEach(t),OLo=i(q),yv=s(q,"LI",{});var swe=n(yv);Jae=s(swe,"STRONG",{});var FHr=n(Jae);XLo=r(FHr,"mobilebert"),FHr.forEach(t),VLo=r(swe," \u2014 "),FD=s(swe,"A",{href:!0});var CHr=n(FD);zLo=r(CHr,"MobileBertForTokenClassification"),CHr.forEach(t),WLo=r(swe," (MobileBERT model)"),swe.forEach(t),QLo=i(q),wv=s(q,"LI",{});var nwe=n(wv);Yae=s(nwe,"STRONG",{});var MHr=n(Yae);HLo=r(MHr,"mpnet"),MHr.forEach(t),ULo=r(nwe," \u2014 "),CD=s(nwe,"A",{href:!0});var EHr=n(CD);JLo=r(EHr,"MPNetForTokenClassification"),EHr.forEach(t),YLo=r(nwe," (MPNet model)"),nwe.forEach(t),KLo=i(q),Av=s(q,"LI",{});var lwe=n(Av);Kae=s(lwe,"STRONG",{});var yHr=n(Kae);ZLo=r(yHr,"nystromformer"),yHr.forEach(t),e8o=r(lwe," \u2014 "),MD=s(lwe,"A",{href:!0});var wHr=n(MD);o8o=r(wHr,"NystromformerForTokenClassification"),wHr.forEach(t),r8o=r(lwe," (Nystromformer model)"),lwe.forEach(t),t8o=i(q),Lv=s(q,"LI",{});var iwe=n(Lv);Zae=s(iwe,"STRONG",{});var AHr=n(Zae);a8o=r(AHr,"qdqbert"),AHr.forEach(t),s8o=r(iwe," \u2014 "),ED=s(iwe,"A",{href:!0});var LHr=n(ED);n8o=r(LHr,"QDQBertForTokenClassification"),LHr.forEach(t),l8o=r(iwe," (QDQBert model)"),iwe.forEach(t),i8o=i(q),Bv=s(q,"LI",{});var dwe=n(Bv);ese=s(dwe,"STRONG",{});var BHr=n(ese);d8o=r(BHr,"rembert"),BHr.forEach(t),c8o=r(dwe," \u2014 "),yD=s(dwe,"A",{href:!0});var xHr=n(yD);m8o=r(xHr,"RemBertForTokenClassification"),xHr.forEach(t),f8o=r(dwe," (RemBERT model)"),dwe.forEach(t),g8o=i(q),xv=s(q,"LI",{});var cwe=n(xv);ose=s(cwe,"STRONG",{});var kHr=n(ose);h8o=r(kHr,"roberta"),kHr.forEach(t),u8o=r(cwe," \u2014 "),wD=s(cwe,"A",{href:!0});var RHr=n(wD);p8o=r(RHr,"RobertaForTokenClassification"),RHr.forEach(t),_8o=r(cwe," (RoBERTa model)"),cwe.forEach(t),b8o=i(q),kv=s(q,"LI",{});var mwe=n(kv);rse=s(mwe,"STRONG",{});var SHr=n(rse);v8o=r(SHr,"roformer"),SHr.forEach(t),T8o=r(mwe," \u2014 "),AD=s(mwe,"A",{href:!0});var PHr=n(AD);F8o=r(PHr,"RoFormerForTokenClassification"),PHr.forEach(t),C8o=r(mwe," (RoFormer model)"),mwe.forEach(t),M8o=i(q),Rv=s(q,"LI",{});var fwe=n(Rv);tse=s(fwe,"STRONG",{});var $Hr=n(tse);E8o=r($Hr,"squeezebert"),$Hr.forEach(t),y8o=r(fwe," \u2014 "),LD=s(fwe,"A",{href:!0});var IHr=n(LD);w8o=r(IHr,"SqueezeBertForTokenClassification"),IHr.forEach(t),A8o=r(fwe," (SqueezeBERT model)"),fwe.forEach(t),L8o=i(q),Sv=s(q,"LI",{});var gwe=n(Sv);ase=s(gwe,"STRONG",{});var DHr=n(ase);B8o=r(DHr,"xlm"),DHr.forEach(t),x8o=r(gwe," \u2014 "),BD=s(gwe,"A",{href:!0});var jHr=n(BD);k8o=r(jHr,"XLMForTokenClassification"),jHr.forEach(t),R8o=r(gwe," (XLM model)"),gwe.forEach(t),S8o=i(q),Pv=s(q,"LI",{});var hwe=n(Pv);sse=s(hwe,"STRONG",{});var NHr=n(sse);P8o=r(NHr,"xlm-roberta"),NHr.forEach(t),$8o=r(hwe," \u2014 "),xD=s(hwe,"A",{href:!0});var qHr=n(xD);I8o=r(qHr,"XLMRobertaForTokenClassification"),qHr.forEach(t),D8o=r(hwe," (XLM-RoBERTa model)"),hwe.forEach(t),j8o=i(q),$v=s(q,"LI",{});var uwe=n($v);nse=s(uwe,"STRONG",{});var GHr=n(nse);N8o=r(GHr,"xlm-roberta-xl"),GHr.forEach(t),q8o=r(uwe," \u2014 "),kD=s(uwe,"A",{href:!0});var OHr=n(kD);G8o=r(OHr,"XLMRobertaXLForTokenClassification"),OHr.forEach(t),O8o=r(uwe," (XLM-RoBERTa-XL model)"),uwe.forEach(t),X8o=i(q),Iv=s(q,"LI",{});var pwe=n(Iv);lse=s(pwe,"STRONG",{});var XHr=n(lse);V8o=r(XHr,"xlnet"),XHr.forEach(t),z8o=r(pwe," \u2014 "),RD=s(pwe,"A",{href:!0});var VHr=n(RD);W8o=r(VHr,"XLNetForTokenClassification"),VHr.forEach(t),Q8o=r(pwe," (XLNet model)"),pwe.forEach(t),H8o=i(q),Dv=s(q,"LI",{});var _we=n(Dv);ise=s(_we,"STRONG",{});var zHr=n(ise);U8o=r(zHr,"yoso"),zHr.forEach(t),J8o=r(_we," \u2014 "),SD=s(_we,"A",{href:!0});var WHr=n(SD);Y8o=r(WHr,"YosoForTokenClassification"),WHr.forEach(t),K8o=r(_we," (YOSO model)"),_we.forEach(t),q.forEach(t),Z8o=i(Xt),jv=s(Xt,"P",{});var bwe=n(jv);e7o=r(bwe,"The model is set in evaluation mode by default using "),dse=s(bwe,"CODE",{});var QHr=n(dse);o7o=r(QHr,"model.eval()"),QHr.forEach(t),r7o=r(bwe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),cse=s(bwe,"CODE",{});var HHr=n(cse);t7o=r(HHr,"model.train()"),HHr.forEach(t),bwe.forEach(t),a7o=i(Xt),mse=s(Xt,"P",{});var UHr=n(mse);s7o=r(UHr,"Examples:"),UHr.forEach(t),n7o=i(Xt),f(iw.$$.fragment,Xt),Xt.forEach(t),tl.forEach(t),H9e=i(c),ud=s(c,"H2",{class:!0});var oke=n(ud);Nv=s(oke,"A",{id:!0,class:!0,href:!0});var JHr=n(Nv);fse=s(JHr,"SPAN",{});var YHr=n(fse);f(dw.$$.fragment,YHr),YHr.forEach(t),JHr.forEach(t),l7o=i(oke),gse=s(oke,"SPAN",{});var KHr=n(gse);i7o=r(KHr,"AutoModelForQuestionAnswering"),KHr.forEach(t),oke.forEach(t),U9e=i(c),rr=s(c,"DIV",{class:!0});var sl=n(rr);f(cw.$$.fragment,sl),d7o=i(sl),pd=s(sl,"P",{});var QV=n(pd);c7o=r(QV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),hse=s(QV,"CODE",{});var ZHr=n(hse);m7o=r(ZHr,"from_pretrained()"),ZHr.forEach(t),f7o=r(QV,"class method or the "),use=s(QV,"CODE",{});var eUr=n(use);g7o=r(eUr,"from_config()"),eUr.forEach(t),h7o=r(QV,`class
method.`),QV.forEach(t),u7o=i(sl),mw=s(sl,"P",{});var rke=n(mw);p7o=r(rke,"This class cannot be instantiated directly using "),pse=s(rke,"CODE",{});var oUr=n(pse);_7o=r(oUr,"__init__()"),oUr.forEach(t),b7o=r(rke," (throws an error)."),rke.forEach(t),v7o=i(sl),Ur=s(sl,"DIV",{class:!0});var nl=n(Ur);f(fw.$$.fragment,nl),T7o=i(nl),_se=s(nl,"P",{});var rUr=n(_se);F7o=r(rUr,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),rUr.forEach(t),C7o=i(nl),_d=s(nl,"P",{});var HV=n(_d);M7o=r(HV,`Note:
Loading a model from its configuration file does `),bse=s(HV,"STRONG",{});var tUr=n(bse);E7o=r(tUr,"not"),tUr.forEach(t),y7o=r(HV,` load the model weights. It only affects the
model\u2019s configuration. Use `),vse=s(HV,"CODE",{});var aUr=n(vse);w7o=r(aUr,"from_pretrained()"),aUr.forEach(t),A7o=r(HV,"to load the model weights."),HV.forEach(t),L7o=i(nl),Tse=s(nl,"P",{});var sUr=n(Tse);B7o=r(sUr,"Examples:"),sUr.forEach(t),x7o=i(nl),f(gw.$$.fragment,nl),nl.forEach(t),k7o=i(sl),Ge=s(sl,"DIV",{class:!0});var Vt=n(Ge);f(hw.$$.fragment,Vt),R7o=i(Vt),Fse=s(Vt,"P",{});var nUr=n(Fse);S7o=r(nUr,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),nUr.forEach(t),P7o=i(Vt),Ha=s(Vt,"P",{});var S3=n(Ha);$7o=r(S3,"The model class to instantiate is selected based on the "),Cse=s(S3,"CODE",{});var lUr=n(Cse);I7o=r(lUr,"model_type"),lUr.forEach(t),D7o=r(S3,` property of the config object (either
passed as an argument or loaded from `),Mse=s(S3,"CODE",{});var iUr=n(Mse);j7o=r(iUr,"pretrained_model_name_or_path"),iUr.forEach(t),N7o=r(S3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ese=s(S3,"CODE",{});var dUr=n(Ese);q7o=r(dUr,"pretrained_model_name_or_path"),dUr.forEach(t),G7o=r(S3,":"),S3.forEach(t),O7o=i(Vt),R=s(Vt,"UL",{});var P=n(R);qv=s(P,"LI",{});var vwe=n(qv);yse=s(vwe,"STRONG",{});var cUr=n(yse);X7o=r(cUr,"albert"),cUr.forEach(t),V7o=r(vwe," \u2014 "),PD=s(vwe,"A",{href:!0});var mUr=n(PD);z7o=r(mUr,"AlbertForQuestionAnswering"),mUr.forEach(t),W7o=r(vwe," (ALBERT model)"),vwe.forEach(t),Q7o=i(P),Gv=s(P,"LI",{});var Twe=n(Gv);wse=s(Twe,"STRONG",{});var fUr=n(wse);H7o=r(fUr,"bart"),fUr.forEach(t),U7o=r(Twe," \u2014 "),$D=s(Twe,"A",{href:!0});var gUr=n($D);J7o=r(gUr,"BartForQuestionAnswering"),gUr.forEach(t),Y7o=r(Twe," (BART model)"),Twe.forEach(t),K7o=i(P),Ov=s(P,"LI",{});var Fwe=n(Ov);Ase=s(Fwe,"STRONG",{});var hUr=n(Ase);Z7o=r(hUr,"bert"),hUr.forEach(t),e9o=r(Fwe," \u2014 "),ID=s(Fwe,"A",{href:!0});var uUr=n(ID);o9o=r(uUr,"BertForQuestionAnswering"),uUr.forEach(t),r9o=r(Fwe," (BERT model)"),Fwe.forEach(t),t9o=i(P),Xv=s(P,"LI",{});var Cwe=n(Xv);Lse=s(Cwe,"STRONG",{});var pUr=n(Lse);a9o=r(pUr,"big_bird"),pUr.forEach(t),s9o=r(Cwe," \u2014 "),DD=s(Cwe,"A",{href:!0});var _Ur=n(DD);n9o=r(_Ur,"BigBirdForQuestionAnswering"),_Ur.forEach(t),l9o=r(Cwe," (BigBird model)"),Cwe.forEach(t),i9o=i(P),Vv=s(P,"LI",{});var Mwe=n(Vv);Bse=s(Mwe,"STRONG",{});var bUr=n(Bse);d9o=r(bUr,"bigbird_pegasus"),bUr.forEach(t),c9o=r(Mwe," \u2014 "),jD=s(Mwe,"A",{href:!0});var vUr=n(jD);m9o=r(vUr,"BigBirdPegasusForQuestionAnswering"),vUr.forEach(t),f9o=r(Mwe," (BigBirdPegasus model)"),Mwe.forEach(t),g9o=i(P),zv=s(P,"LI",{});var Ewe=n(zv);xse=s(Ewe,"STRONG",{});var TUr=n(xse);h9o=r(TUr,"camembert"),TUr.forEach(t),u9o=r(Ewe," \u2014 "),ND=s(Ewe,"A",{href:!0});var FUr=n(ND);p9o=r(FUr,"CamembertForQuestionAnswering"),FUr.forEach(t),_9o=r(Ewe," (CamemBERT model)"),Ewe.forEach(t),b9o=i(P),Wv=s(P,"LI",{});var ywe=n(Wv);kse=s(ywe,"STRONG",{});var CUr=n(kse);v9o=r(CUr,"canine"),CUr.forEach(t),T9o=r(ywe," \u2014 "),qD=s(ywe,"A",{href:!0});var MUr=n(qD);F9o=r(MUr,"CanineForQuestionAnswering"),MUr.forEach(t),C9o=r(ywe," (Canine model)"),ywe.forEach(t),M9o=i(P),Qv=s(P,"LI",{});var wwe=n(Qv);Rse=s(wwe,"STRONG",{});var EUr=n(Rse);E9o=r(EUr,"convbert"),EUr.forEach(t),y9o=r(wwe," \u2014 "),GD=s(wwe,"A",{href:!0});var yUr=n(GD);w9o=r(yUr,"ConvBertForQuestionAnswering"),yUr.forEach(t),A9o=r(wwe," (ConvBERT model)"),wwe.forEach(t),L9o=i(P),Hv=s(P,"LI",{});var Awe=n(Hv);Sse=s(Awe,"STRONG",{});var wUr=n(Sse);B9o=r(wUr,"data2vec-text"),wUr.forEach(t),x9o=r(Awe," \u2014 "),OD=s(Awe,"A",{href:!0});var AUr=n(OD);k9o=r(AUr,"Data2VecTextForQuestionAnswering"),AUr.forEach(t),R9o=r(Awe," (Data2VecText model)"),Awe.forEach(t),S9o=i(P),Uv=s(P,"LI",{});var Lwe=n(Uv);Pse=s(Lwe,"STRONG",{});var LUr=n(Pse);P9o=r(LUr,"deberta"),LUr.forEach(t),$9o=r(Lwe," \u2014 "),XD=s(Lwe,"A",{href:!0});var BUr=n(XD);I9o=r(BUr,"DebertaForQuestionAnswering"),BUr.forEach(t),D9o=r(Lwe," (DeBERTa model)"),Lwe.forEach(t),j9o=i(P),Jv=s(P,"LI",{});var Bwe=n(Jv);$se=s(Bwe,"STRONG",{});var xUr=n($se);N9o=r(xUr,"deberta-v2"),xUr.forEach(t),q9o=r(Bwe," \u2014 "),VD=s(Bwe,"A",{href:!0});var kUr=n(VD);G9o=r(kUr,"DebertaV2ForQuestionAnswering"),kUr.forEach(t),O9o=r(Bwe," (DeBERTa-v2 model)"),Bwe.forEach(t),X9o=i(P),Yv=s(P,"LI",{});var xwe=n(Yv);Ise=s(xwe,"STRONG",{});var RUr=n(Ise);V9o=r(RUr,"distilbert"),RUr.forEach(t),z9o=r(xwe," \u2014 "),zD=s(xwe,"A",{href:!0});var SUr=n(zD);W9o=r(SUr,"DistilBertForQuestionAnswering"),SUr.forEach(t),Q9o=r(xwe," (DistilBERT model)"),xwe.forEach(t),H9o=i(P),Kv=s(P,"LI",{});var kwe=n(Kv);Dse=s(kwe,"STRONG",{});var PUr=n(Dse);U9o=r(PUr,"electra"),PUr.forEach(t),J9o=r(kwe," \u2014 "),WD=s(kwe,"A",{href:!0});var $Ur=n(WD);Y9o=r($Ur,"ElectraForQuestionAnswering"),$Ur.forEach(t),K9o=r(kwe," (ELECTRA model)"),kwe.forEach(t),Z9o=i(P),Zv=s(P,"LI",{});var Rwe=n(Zv);jse=s(Rwe,"STRONG",{});var IUr=n(jse);eBo=r(IUr,"flaubert"),IUr.forEach(t),oBo=r(Rwe," \u2014 "),QD=s(Rwe,"A",{href:!0});var DUr=n(QD);rBo=r(DUr,"FlaubertForQuestionAnsweringSimple"),DUr.forEach(t),tBo=r(Rwe," (FlauBERT model)"),Rwe.forEach(t),aBo=i(P),eT=s(P,"LI",{});var Swe=n(eT);Nse=s(Swe,"STRONG",{});var jUr=n(Nse);sBo=r(jUr,"fnet"),jUr.forEach(t),nBo=r(Swe," \u2014 "),HD=s(Swe,"A",{href:!0});var NUr=n(HD);lBo=r(NUr,"FNetForQuestionAnswering"),NUr.forEach(t),iBo=r(Swe," (FNet model)"),Swe.forEach(t),dBo=i(P),oT=s(P,"LI",{});var Pwe=n(oT);qse=s(Pwe,"STRONG",{});var qUr=n(qse);cBo=r(qUr,"funnel"),qUr.forEach(t),mBo=r(Pwe," \u2014 "),UD=s(Pwe,"A",{href:!0});var GUr=n(UD);fBo=r(GUr,"FunnelForQuestionAnswering"),GUr.forEach(t),gBo=r(Pwe," (Funnel Transformer model)"),Pwe.forEach(t),hBo=i(P),rT=s(P,"LI",{});var $we=n(rT);Gse=s($we,"STRONG",{});var OUr=n(Gse);uBo=r(OUr,"gptj"),OUr.forEach(t),pBo=r($we," \u2014 "),JD=s($we,"A",{href:!0});var XUr=n(JD);_Bo=r(XUr,"GPTJForQuestionAnswering"),XUr.forEach(t),bBo=r($we," (GPT-J model)"),$we.forEach(t),vBo=i(P),tT=s(P,"LI",{});var Iwe=n(tT);Ose=s(Iwe,"STRONG",{});var VUr=n(Ose);TBo=r(VUr,"ibert"),VUr.forEach(t),FBo=r(Iwe," \u2014 "),YD=s(Iwe,"A",{href:!0});var zUr=n(YD);CBo=r(zUr,"IBertForQuestionAnswering"),zUr.forEach(t),MBo=r(Iwe," (I-BERT model)"),Iwe.forEach(t),EBo=i(P),aT=s(P,"LI",{});var Dwe=n(aT);Xse=s(Dwe,"STRONG",{});var WUr=n(Xse);yBo=r(WUr,"layoutlmv2"),WUr.forEach(t),wBo=r(Dwe," \u2014 "),KD=s(Dwe,"A",{href:!0});var QUr=n(KD);ABo=r(QUr,"LayoutLMv2ForQuestionAnswering"),QUr.forEach(t),LBo=r(Dwe," (LayoutLMv2 model)"),Dwe.forEach(t),BBo=i(P),sT=s(P,"LI",{});var jwe=n(sT);Vse=s(jwe,"STRONG",{});var HUr=n(Vse);xBo=r(HUr,"led"),HUr.forEach(t),kBo=r(jwe," \u2014 "),ZD=s(jwe,"A",{href:!0});var UUr=n(ZD);RBo=r(UUr,"LEDForQuestionAnswering"),UUr.forEach(t),SBo=r(jwe," (LED model)"),jwe.forEach(t),PBo=i(P),nT=s(P,"LI",{});var Nwe=n(nT);zse=s(Nwe,"STRONG",{});var JUr=n(zse);$Bo=r(JUr,"longformer"),JUr.forEach(t),IBo=r(Nwe," \u2014 "),ej=s(Nwe,"A",{href:!0});var YUr=n(ej);DBo=r(YUr,"LongformerForQuestionAnswering"),YUr.forEach(t),jBo=r(Nwe," (Longformer model)"),Nwe.forEach(t),NBo=i(P),lT=s(P,"LI",{});var qwe=n(lT);Wse=s(qwe,"STRONG",{});var KUr=n(Wse);qBo=r(KUr,"lxmert"),KUr.forEach(t),GBo=r(qwe," \u2014 "),oj=s(qwe,"A",{href:!0});var ZUr=n(oj);OBo=r(ZUr,"LxmertForQuestionAnswering"),ZUr.forEach(t),XBo=r(qwe," (LXMERT model)"),qwe.forEach(t),VBo=i(P),iT=s(P,"LI",{});var Gwe=n(iT);Qse=s(Gwe,"STRONG",{});var eJr=n(Qse);zBo=r(eJr,"mbart"),eJr.forEach(t),WBo=r(Gwe," \u2014 "),rj=s(Gwe,"A",{href:!0});var oJr=n(rj);QBo=r(oJr,"MBartForQuestionAnswering"),oJr.forEach(t),HBo=r(Gwe," (mBART model)"),Gwe.forEach(t),UBo=i(P),dT=s(P,"LI",{});var Owe=n(dT);Hse=s(Owe,"STRONG",{});var rJr=n(Hse);JBo=r(rJr,"megatron-bert"),rJr.forEach(t),YBo=r(Owe," \u2014 "),tj=s(Owe,"A",{href:!0});var tJr=n(tj);KBo=r(tJr,"MegatronBertForQuestionAnswering"),tJr.forEach(t),ZBo=r(Owe," (MegatronBert model)"),Owe.forEach(t),exo=i(P),cT=s(P,"LI",{});var Xwe=n(cT);Use=s(Xwe,"STRONG",{});var aJr=n(Use);oxo=r(aJr,"mobilebert"),aJr.forEach(t),rxo=r(Xwe," \u2014 "),aj=s(Xwe,"A",{href:!0});var sJr=n(aj);txo=r(sJr,"MobileBertForQuestionAnswering"),sJr.forEach(t),axo=r(Xwe," (MobileBERT model)"),Xwe.forEach(t),sxo=i(P),mT=s(P,"LI",{});var Vwe=n(mT);Jse=s(Vwe,"STRONG",{});var nJr=n(Jse);nxo=r(nJr,"mpnet"),nJr.forEach(t),lxo=r(Vwe," \u2014 "),sj=s(Vwe,"A",{href:!0});var lJr=n(sj);ixo=r(lJr,"MPNetForQuestionAnswering"),lJr.forEach(t),dxo=r(Vwe," (MPNet model)"),Vwe.forEach(t),cxo=i(P),fT=s(P,"LI",{});var zwe=n(fT);Yse=s(zwe,"STRONG",{});var iJr=n(Yse);mxo=r(iJr,"nystromformer"),iJr.forEach(t),fxo=r(zwe," \u2014 "),nj=s(zwe,"A",{href:!0});var dJr=n(nj);gxo=r(dJr,"NystromformerForQuestionAnswering"),dJr.forEach(t),hxo=r(zwe," (Nystromformer model)"),zwe.forEach(t),uxo=i(P),gT=s(P,"LI",{});var Wwe=n(gT);Kse=s(Wwe,"STRONG",{});var cJr=n(Kse);pxo=r(cJr,"qdqbert"),cJr.forEach(t),_xo=r(Wwe," \u2014 "),lj=s(Wwe,"A",{href:!0});var mJr=n(lj);bxo=r(mJr,"QDQBertForQuestionAnswering"),mJr.forEach(t),vxo=r(Wwe," (QDQBert model)"),Wwe.forEach(t),Txo=i(P),hT=s(P,"LI",{});var Qwe=n(hT);Zse=s(Qwe,"STRONG",{});var fJr=n(Zse);Fxo=r(fJr,"reformer"),fJr.forEach(t),Cxo=r(Qwe," \u2014 "),ij=s(Qwe,"A",{href:!0});var gJr=n(ij);Mxo=r(gJr,"ReformerForQuestionAnswering"),gJr.forEach(t),Exo=r(Qwe," (Reformer model)"),Qwe.forEach(t),yxo=i(P),uT=s(P,"LI",{});var Hwe=n(uT);ene=s(Hwe,"STRONG",{});var hJr=n(ene);wxo=r(hJr,"rembert"),hJr.forEach(t),Axo=r(Hwe," \u2014 "),dj=s(Hwe,"A",{href:!0});var uJr=n(dj);Lxo=r(uJr,"RemBertForQuestionAnswering"),uJr.forEach(t),Bxo=r(Hwe," (RemBERT model)"),Hwe.forEach(t),xxo=i(P),pT=s(P,"LI",{});var Uwe=n(pT);one=s(Uwe,"STRONG",{});var pJr=n(one);kxo=r(pJr,"roberta"),pJr.forEach(t),Rxo=r(Uwe," \u2014 "),cj=s(Uwe,"A",{href:!0});var _Jr=n(cj);Sxo=r(_Jr,"RobertaForQuestionAnswering"),_Jr.forEach(t),Pxo=r(Uwe," (RoBERTa model)"),Uwe.forEach(t),$xo=i(P),_T=s(P,"LI",{});var Jwe=n(_T);rne=s(Jwe,"STRONG",{});var bJr=n(rne);Ixo=r(bJr,"roformer"),bJr.forEach(t),Dxo=r(Jwe," \u2014 "),mj=s(Jwe,"A",{href:!0});var vJr=n(mj);jxo=r(vJr,"RoFormerForQuestionAnswering"),vJr.forEach(t),Nxo=r(Jwe," (RoFormer model)"),Jwe.forEach(t),qxo=i(P),bT=s(P,"LI",{});var Ywe=n(bT);tne=s(Ywe,"STRONG",{});var TJr=n(tne);Gxo=r(TJr,"splinter"),TJr.forEach(t),Oxo=r(Ywe," \u2014 "),fj=s(Ywe,"A",{href:!0});var FJr=n(fj);Xxo=r(FJr,"SplinterForQuestionAnswering"),FJr.forEach(t),Vxo=r(Ywe," (Splinter model)"),Ywe.forEach(t),zxo=i(P),vT=s(P,"LI",{});var Kwe=n(vT);ane=s(Kwe,"STRONG",{});var CJr=n(ane);Wxo=r(CJr,"squeezebert"),CJr.forEach(t),Qxo=r(Kwe," \u2014 "),gj=s(Kwe,"A",{href:!0});var MJr=n(gj);Hxo=r(MJr,"SqueezeBertForQuestionAnswering"),MJr.forEach(t),Uxo=r(Kwe," (SqueezeBERT model)"),Kwe.forEach(t),Jxo=i(P),TT=s(P,"LI",{});var Zwe=n(TT);sne=s(Zwe,"STRONG",{});var EJr=n(sne);Yxo=r(EJr,"xlm"),EJr.forEach(t),Kxo=r(Zwe," \u2014 "),hj=s(Zwe,"A",{href:!0});var yJr=n(hj);Zxo=r(yJr,"XLMForQuestionAnsweringSimple"),yJr.forEach(t),eko=r(Zwe," (XLM model)"),Zwe.forEach(t),oko=i(P),FT=s(P,"LI",{});var e6e=n(FT);nne=s(e6e,"STRONG",{});var wJr=n(nne);rko=r(wJr,"xlm-roberta"),wJr.forEach(t),tko=r(e6e," \u2014 "),uj=s(e6e,"A",{href:!0});var AJr=n(uj);ako=r(AJr,"XLMRobertaForQuestionAnswering"),AJr.forEach(t),sko=r(e6e," (XLM-RoBERTa model)"),e6e.forEach(t),nko=i(P),CT=s(P,"LI",{});var o6e=n(CT);lne=s(o6e,"STRONG",{});var LJr=n(lne);lko=r(LJr,"xlm-roberta-xl"),LJr.forEach(t),iko=r(o6e," \u2014 "),pj=s(o6e,"A",{href:!0});var BJr=n(pj);dko=r(BJr,"XLMRobertaXLForQuestionAnswering"),BJr.forEach(t),cko=r(o6e," (XLM-RoBERTa-XL model)"),o6e.forEach(t),mko=i(P),MT=s(P,"LI",{});var r6e=n(MT);ine=s(r6e,"STRONG",{});var xJr=n(ine);fko=r(xJr,"xlnet"),xJr.forEach(t),gko=r(r6e," \u2014 "),_j=s(r6e,"A",{href:!0});var kJr=n(_j);hko=r(kJr,"XLNetForQuestionAnsweringSimple"),kJr.forEach(t),uko=r(r6e," (XLNet model)"),r6e.forEach(t),pko=i(P),ET=s(P,"LI",{});var t6e=n(ET);dne=s(t6e,"STRONG",{});var RJr=n(dne);_ko=r(RJr,"yoso"),RJr.forEach(t),bko=r(t6e," \u2014 "),bj=s(t6e,"A",{href:!0});var SJr=n(bj);vko=r(SJr,"YosoForQuestionAnswering"),SJr.forEach(t),Tko=r(t6e," (YOSO model)"),t6e.forEach(t),P.forEach(t),Fko=i(Vt),yT=s(Vt,"P",{});var a6e=n(yT);Cko=r(a6e,"The model is set in evaluation mode by default using "),cne=s(a6e,"CODE",{});var PJr=n(cne);Mko=r(PJr,"model.eval()"),PJr.forEach(t),Eko=r(a6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),mne=s(a6e,"CODE",{});var $Jr=n(mne);yko=r($Jr,"model.train()"),$Jr.forEach(t),a6e.forEach(t),wko=i(Vt),fne=s(Vt,"P",{});var IJr=n(fne);Ako=r(IJr,"Examples:"),IJr.forEach(t),Lko=i(Vt),f(uw.$$.fragment,Vt),Vt.forEach(t),sl.forEach(t),J9e=i(c),bd=s(c,"H2",{class:!0});var tke=n(bd);wT=s(tke,"A",{id:!0,class:!0,href:!0});var DJr=n(wT);gne=s(DJr,"SPAN",{});var jJr=n(gne);f(pw.$$.fragment,jJr),jJr.forEach(t),DJr.forEach(t),Bko=i(tke),hne=s(tke,"SPAN",{});var NJr=n(hne);xko=r(NJr,"AutoModelForTableQuestionAnswering"),NJr.forEach(t),tke.forEach(t),Y9e=i(c),tr=s(c,"DIV",{class:!0});var ll=n(tr);f(_w.$$.fragment,ll),kko=i(ll),vd=s(ll,"P",{});var UV=n(vd);Rko=r(UV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),une=s(UV,"CODE",{});var qJr=n(une);Sko=r(qJr,"from_pretrained()"),qJr.forEach(t),Pko=r(UV,"class method or the "),pne=s(UV,"CODE",{});var GJr=n(pne);$ko=r(GJr,"from_config()"),GJr.forEach(t),Iko=r(UV,`class
method.`),UV.forEach(t),Dko=i(ll),bw=s(ll,"P",{});var ake=n(bw);jko=r(ake,"This class cannot be instantiated directly using "),_ne=s(ake,"CODE",{});var OJr=n(_ne);Nko=r(OJr,"__init__()"),OJr.forEach(t),qko=r(ake," (throws an error)."),ake.forEach(t),Gko=i(ll),Jr=s(ll,"DIV",{class:!0});var il=n(Jr);f(vw.$$.fragment,il),Oko=i(il),bne=s(il,"P",{});var XJr=n(bne);Xko=r(XJr,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),XJr.forEach(t),Vko=i(il),Td=s(il,"P",{});var JV=n(Td);zko=r(JV,`Note:
Loading a model from its configuration file does `),vne=s(JV,"STRONG",{});var VJr=n(vne);Wko=r(VJr,"not"),VJr.forEach(t),Qko=r(JV,` load the model weights. It only affects the
model\u2019s configuration. Use `),Tne=s(JV,"CODE",{});var zJr=n(Tne);Hko=r(zJr,"from_pretrained()"),zJr.forEach(t),Uko=r(JV,"to load the model weights."),JV.forEach(t),Jko=i(il),Fne=s(il,"P",{});var WJr=n(Fne);Yko=r(WJr,"Examples:"),WJr.forEach(t),Kko=i(il),f(Tw.$$.fragment,il),il.forEach(t),Zko=i(ll),Oe=s(ll,"DIV",{class:!0});var zt=n(Oe);f(Fw.$$.fragment,zt),eRo=i(zt),Cne=s(zt,"P",{});var QJr=n(Cne);oRo=r(QJr,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),QJr.forEach(t),rRo=i(zt),Ua=s(zt,"P",{});var P3=n(Ua);tRo=r(P3,"The model class to instantiate is selected based on the "),Mne=s(P3,"CODE",{});var HJr=n(Mne);aRo=r(HJr,"model_type"),HJr.forEach(t),sRo=r(P3,` property of the config object (either
passed as an argument or loaded from `),Ene=s(P3,"CODE",{});var UJr=n(Ene);nRo=r(UJr,"pretrained_model_name_or_path"),UJr.forEach(t),lRo=r(P3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),yne=s(P3,"CODE",{});var JJr=n(yne);iRo=r(JJr,"pretrained_model_name_or_path"),JJr.forEach(t),dRo=r(P3,":"),P3.forEach(t),cRo=i(zt),wne=s(zt,"UL",{});var YJr=n(wne);AT=s(YJr,"LI",{});var s6e=n(AT);Ane=s(s6e,"STRONG",{});var KJr=n(Ane);mRo=r(KJr,"tapas"),KJr.forEach(t),fRo=r(s6e," \u2014 "),vj=s(s6e,"A",{href:!0});var ZJr=n(vj);gRo=r(ZJr,"TapasForQuestionAnswering"),ZJr.forEach(t),hRo=r(s6e," (TAPAS model)"),s6e.forEach(t),YJr.forEach(t),uRo=i(zt),LT=s(zt,"P",{});var n6e=n(LT);pRo=r(n6e,"The model is set in evaluation mode by default using "),Lne=s(n6e,"CODE",{});var eYr=n(Lne);_Ro=r(eYr,"model.eval()"),eYr.forEach(t),bRo=r(n6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Bne=s(n6e,"CODE",{});var oYr=n(Bne);vRo=r(oYr,"model.train()"),oYr.forEach(t),n6e.forEach(t),TRo=i(zt),xne=s(zt,"P",{});var rYr=n(xne);FRo=r(rYr,"Examples:"),rYr.forEach(t),CRo=i(zt),f(Cw.$$.fragment,zt),zt.forEach(t),ll.forEach(t),K9e=i(c),Fd=s(c,"H2",{class:!0});var ske=n(Fd);BT=s(ske,"A",{id:!0,class:!0,href:!0});var tYr=n(BT);kne=s(tYr,"SPAN",{});var aYr=n(kne);f(Mw.$$.fragment,aYr),aYr.forEach(t),tYr.forEach(t),MRo=i(ske),Rne=s(ske,"SPAN",{});var sYr=n(Rne);ERo=r(sYr,"AutoModelForImageClassification"),sYr.forEach(t),ske.forEach(t),Z9e=i(c),ar=s(c,"DIV",{class:!0});var dl=n(ar);f(Ew.$$.fragment,dl),yRo=i(dl),Cd=s(dl,"P",{});var YV=n(Cd);wRo=r(YV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Sne=s(YV,"CODE",{});var nYr=n(Sne);ARo=r(nYr,"from_pretrained()"),nYr.forEach(t),LRo=r(YV,"class method or the "),Pne=s(YV,"CODE",{});var lYr=n(Pne);BRo=r(lYr,"from_config()"),lYr.forEach(t),xRo=r(YV,`class
method.`),YV.forEach(t),kRo=i(dl),yw=s(dl,"P",{});var nke=n(yw);RRo=r(nke,"This class cannot be instantiated directly using "),$ne=s(nke,"CODE",{});var iYr=n($ne);SRo=r(iYr,"__init__()"),iYr.forEach(t),PRo=r(nke," (throws an error)."),nke.forEach(t),$Ro=i(dl),Yr=s(dl,"DIV",{class:!0});var cl=n(Yr);f(ww.$$.fragment,cl),IRo=i(cl),Ine=s(cl,"P",{});var dYr=n(Ine);DRo=r(dYr,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),dYr.forEach(t),jRo=i(cl),Md=s(cl,"P",{});var KV=n(Md);NRo=r(KV,`Note:
Loading a model from its configuration file does `),Dne=s(KV,"STRONG",{});var cYr=n(Dne);qRo=r(cYr,"not"),cYr.forEach(t),GRo=r(KV,` load the model weights. It only affects the
model\u2019s configuration. Use `),jne=s(KV,"CODE",{});var mYr=n(jne);ORo=r(mYr,"from_pretrained()"),mYr.forEach(t),XRo=r(KV,"to load the model weights."),KV.forEach(t),VRo=i(cl),Nne=s(cl,"P",{});var fYr=n(Nne);zRo=r(fYr,"Examples:"),fYr.forEach(t),WRo=i(cl),f(Aw.$$.fragment,cl),cl.forEach(t),QRo=i(dl),Xe=s(dl,"DIV",{class:!0});var Wt=n(Xe);f(Lw.$$.fragment,Wt),HRo=i(Wt),qne=s(Wt,"P",{});var gYr=n(qne);URo=r(gYr,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),gYr.forEach(t),JRo=i(Wt),Ja=s(Wt,"P",{});var $3=n(Ja);YRo=r($3,"The model class to instantiate is selected based on the "),Gne=s($3,"CODE",{});var hYr=n(Gne);KRo=r(hYr,"model_type"),hYr.forEach(t),ZRo=r($3,` property of the config object (either
passed as an argument or loaded from `),One=s($3,"CODE",{});var uYr=n(One);eSo=r(uYr,"pretrained_model_name_or_path"),uYr.forEach(t),oSo=r($3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Xne=s($3,"CODE",{});var pYr=n(Xne);rSo=r(pYr,"pretrained_model_name_or_path"),pYr.forEach(t),tSo=r($3,":"),$3.forEach(t),aSo=i(Wt),be=s(Wt,"UL",{});var eo=n(be);xT=s(eo,"LI",{});var l6e=n(xT);Vne=s(l6e,"STRONG",{});var _Yr=n(Vne);sSo=r(_Yr,"beit"),_Yr.forEach(t),nSo=r(l6e," \u2014 "),Tj=s(l6e,"A",{href:!0});var bYr=n(Tj);lSo=r(bYr,"BeitForImageClassification"),bYr.forEach(t),iSo=r(l6e," (BEiT model)"),l6e.forEach(t),dSo=i(eo),kT=s(eo,"LI",{});var i6e=n(kT);zne=s(i6e,"STRONG",{});var vYr=n(zne);cSo=r(vYr,"convnext"),vYr.forEach(t),mSo=r(i6e," \u2014 "),Fj=s(i6e,"A",{href:!0});var TYr=n(Fj);fSo=r(TYr,"ConvNextForImageClassification"),TYr.forEach(t),gSo=r(i6e," (ConvNext model)"),i6e.forEach(t),hSo=i(eo),Pn=s(eo,"LI",{});var t7=n(Pn);Wne=s(t7,"STRONG",{});var FYr=n(Wne);uSo=r(FYr,"deit"),FYr.forEach(t),pSo=r(t7," \u2014 "),Cj=s(t7,"A",{href:!0});var CYr=n(Cj);_So=r(CYr,"DeiTForImageClassification"),CYr.forEach(t),bSo=r(t7," or "),Mj=s(t7,"A",{href:!0});var MYr=n(Mj);vSo=r(MYr,"DeiTForImageClassificationWithTeacher"),MYr.forEach(t),TSo=r(t7," (DeiT model)"),t7.forEach(t),FSo=i(eo),RT=s(eo,"LI",{});var d6e=n(RT);Qne=s(d6e,"STRONG",{});var EYr=n(Qne);CSo=r(EYr,"imagegpt"),EYr.forEach(t),MSo=r(d6e," \u2014 "),Ej=s(d6e,"A",{href:!0});var yYr=n(Ej);ESo=r(yYr,"ImageGPTForImageClassification"),yYr.forEach(t),ySo=r(d6e," (ImageGPT model)"),d6e.forEach(t),wSo=i(eo),la=s(eo,"LI",{});var Cm=n(la);Hne=s(Cm,"STRONG",{});var wYr=n(Hne);ASo=r(wYr,"perceiver"),wYr.forEach(t),LSo=r(Cm," \u2014 "),yj=s(Cm,"A",{href:!0});var AYr=n(yj);BSo=r(AYr,"PerceiverForImageClassificationLearned"),AYr.forEach(t),xSo=r(Cm," or "),wj=s(Cm,"A",{href:!0});var LYr=n(wj);kSo=r(LYr,"PerceiverForImageClassificationFourier"),LYr.forEach(t),RSo=r(Cm," or "),Aj=s(Cm,"A",{href:!0});var BYr=n(Aj);SSo=r(BYr,"PerceiverForImageClassificationConvProcessing"),BYr.forEach(t),PSo=r(Cm," (Perceiver model)"),Cm.forEach(t),$So=i(eo),ST=s(eo,"LI",{});var c6e=n(ST);Une=s(c6e,"STRONG",{});var xYr=n(Une);ISo=r(xYr,"poolformer"),xYr.forEach(t),DSo=r(c6e," \u2014 "),Lj=s(c6e,"A",{href:!0});var kYr=n(Lj);jSo=r(kYr,"PoolFormerForImageClassification"),kYr.forEach(t),NSo=r(c6e," (PoolFormer model)"),c6e.forEach(t),qSo=i(eo),PT=s(eo,"LI",{});var m6e=n(PT);Jne=s(m6e,"STRONG",{});var RYr=n(Jne);GSo=r(RYr,"segformer"),RYr.forEach(t),OSo=r(m6e," \u2014 "),Bj=s(m6e,"A",{href:!0});var SYr=n(Bj);XSo=r(SYr,"SegformerForImageClassification"),SYr.forEach(t),VSo=r(m6e," (SegFormer model)"),m6e.forEach(t),zSo=i(eo),$T=s(eo,"LI",{});var f6e=n($T);Yne=s(f6e,"STRONG",{});var PYr=n(Yne);WSo=r(PYr,"swin"),PYr.forEach(t),QSo=r(f6e," \u2014 "),xj=s(f6e,"A",{href:!0});var $Yr=n(xj);HSo=r($Yr,"SwinForImageClassification"),$Yr.forEach(t),USo=r(f6e," (Swin model)"),f6e.forEach(t),JSo=i(eo),IT=s(eo,"LI",{});var g6e=n(IT);Kne=s(g6e,"STRONG",{});var IYr=n(Kne);YSo=r(IYr,"vit"),IYr.forEach(t),KSo=r(g6e," \u2014 "),kj=s(g6e,"A",{href:!0});var DYr=n(kj);ZSo=r(DYr,"ViTForImageClassification"),DYr.forEach(t),ePo=r(g6e," (ViT model)"),g6e.forEach(t),eo.forEach(t),oPo=i(Wt),DT=s(Wt,"P",{});var h6e=n(DT);rPo=r(h6e,"The model is set in evaluation mode by default using "),Zne=s(h6e,"CODE",{});var jYr=n(Zne);tPo=r(jYr,"model.eval()"),jYr.forEach(t),aPo=r(h6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ele=s(h6e,"CODE",{});var NYr=n(ele);sPo=r(NYr,"model.train()"),NYr.forEach(t),h6e.forEach(t),nPo=i(Wt),ole=s(Wt,"P",{});var qYr=n(ole);lPo=r(qYr,"Examples:"),qYr.forEach(t),iPo=i(Wt),f(Bw.$$.fragment,Wt),Wt.forEach(t),dl.forEach(t),eBe=i(c),Ed=s(c,"H2",{class:!0});var lke=n(Ed);jT=s(lke,"A",{id:!0,class:!0,href:!0});var GYr=n(jT);rle=s(GYr,"SPAN",{});var OYr=n(rle);f(xw.$$.fragment,OYr),OYr.forEach(t),GYr.forEach(t),dPo=i(lke),tle=s(lke,"SPAN",{});var XYr=n(tle);cPo=r(XYr,"AutoModelForVision2Seq"),XYr.forEach(t),lke.forEach(t),oBe=i(c),sr=s(c,"DIV",{class:!0});var ml=n(sr);f(kw.$$.fragment,ml),mPo=i(ml),yd=s(ml,"P",{});var ZV=n(yd);fPo=r(ZV,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),ale=s(ZV,"CODE",{});var VYr=n(ale);gPo=r(VYr,"from_pretrained()"),VYr.forEach(t),hPo=r(ZV,"class method or the "),sle=s(ZV,"CODE",{});var zYr=n(sle);uPo=r(zYr,"from_config()"),zYr.forEach(t),pPo=r(ZV,`class
method.`),ZV.forEach(t),_Po=i(ml),Rw=s(ml,"P",{});var ike=n(Rw);bPo=r(ike,"This class cannot be instantiated directly using "),nle=s(ike,"CODE",{});var WYr=n(nle);vPo=r(WYr,"__init__()"),WYr.forEach(t),TPo=r(ike," (throws an error)."),ike.forEach(t),FPo=i(ml),Kr=s(ml,"DIV",{class:!0});var fl=n(Kr);f(Sw.$$.fragment,fl),CPo=i(fl),lle=s(fl,"P",{});var QYr=n(lle);MPo=r(QYr,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),QYr.forEach(t),EPo=i(fl),wd=s(fl,"P",{});var ez=n(wd);yPo=r(ez,`Note:
Loading a model from its configuration file does `),ile=s(ez,"STRONG",{});var HYr=n(ile);wPo=r(HYr,"not"),HYr.forEach(t),APo=r(ez,` load the model weights. It only affects the
model\u2019s configuration. Use `),dle=s(ez,"CODE",{});var UYr=n(dle);LPo=r(UYr,"from_pretrained()"),UYr.forEach(t),BPo=r(ez,"to load the model weights."),ez.forEach(t),xPo=i(fl),cle=s(fl,"P",{});var JYr=n(cle);kPo=r(JYr,"Examples:"),JYr.forEach(t),RPo=i(fl),f(Pw.$$.fragment,fl),fl.forEach(t),SPo=i(ml),Ve=s(ml,"DIV",{class:!0});var Qt=n(Ve);f($w.$$.fragment,Qt),PPo=i(Qt),mle=s(Qt,"P",{});var YYr=n(mle);$Po=r(YYr,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),YYr.forEach(t),IPo=i(Qt),Ya=s(Qt,"P",{});var I3=n(Ya);DPo=r(I3,"The model class to instantiate is selected based on the "),fle=s(I3,"CODE",{});var KYr=n(fle);jPo=r(KYr,"model_type"),KYr.forEach(t),NPo=r(I3,` property of the config object (either
passed as an argument or loaded from `),gle=s(I3,"CODE",{});var ZYr=n(gle);qPo=r(ZYr,"pretrained_model_name_or_path"),ZYr.forEach(t),GPo=r(I3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),hle=s(I3,"CODE",{});var eKr=n(hle);OPo=r(eKr,"pretrained_model_name_or_path"),eKr.forEach(t),XPo=r(I3,":"),I3.forEach(t),VPo=i(Qt),ule=s(Qt,"UL",{});var oKr=n(ule);NT=s(oKr,"LI",{});var u6e=n(NT);ple=s(u6e,"STRONG",{});var rKr=n(ple);zPo=r(rKr,"vision-encoder-decoder"),rKr.forEach(t),WPo=r(u6e," \u2014 "),Rj=s(u6e,"A",{href:!0});var tKr=n(Rj);QPo=r(tKr,"VisionEncoderDecoderModel"),tKr.forEach(t),HPo=r(u6e," (Vision Encoder decoder model)"),u6e.forEach(t),oKr.forEach(t),UPo=i(Qt),qT=s(Qt,"P",{});var p6e=n(qT);JPo=r(p6e,"The model is set in evaluation mode by default using "),_le=s(p6e,"CODE",{});var aKr=n(_le);YPo=r(aKr,"model.eval()"),aKr.forEach(t),KPo=r(p6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),ble=s(p6e,"CODE",{});var sKr=n(ble);ZPo=r(sKr,"model.train()"),sKr.forEach(t),p6e.forEach(t),e$o=i(Qt),vle=s(Qt,"P",{});var nKr=n(vle);o$o=r(nKr,"Examples:"),nKr.forEach(t),r$o=i(Qt),f(Iw.$$.fragment,Qt),Qt.forEach(t),ml.forEach(t),rBe=i(c),Ad=s(c,"H2",{class:!0});var dke=n(Ad);GT=s(dke,"A",{id:!0,class:!0,href:!0});var lKr=n(GT);Tle=s(lKr,"SPAN",{});var iKr=n(Tle);f(Dw.$$.fragment,iKr),iKr.forEach(t),lKr.forEach(t),t$o=i(dke),Fle=s(dke,"SPAN",{});var dKr=n(Fle);a$o=r(dKr,"AutoModelForAudioClassification"),dKr.forEach(t),dke.forEach(t),tBe=i(c),nr=s(c,"DIV",{class:!0});var gl=n(nr);f(jw.$$.fragment,gl),s$o=i(gl),Ld=s(gl,"P",{});var oz=n(Ld);n$o=r(oz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the `),Cle=s(oz,"CODE",{});var cKr=n(Cle);l$o=r(cKr,"from_pretrained()"),cKr.forEach(t),i$o=r(oz,"class method or the "),Mle=s(oz,"CODE",{});var mKr=n(Mle);d$o=r(mKr,"from_config()"),mKr.forEach(t),c$o=r(oz,`class
method.`),oz.forEach(t),m$o=i(gl),Nw=s(gl,"P",{});var cke=n(Nw);f$o=r(cke,"This class cannot be instantiated directly using "),Ele=s(cke,"CODE",{});var fKr=n(Ele);g$o=r(fKr,"__init__()"),fKr.forEach(t),h$o=r(cke," (throws an error)."),cke.forEach(t),u$o=i(gl),Zr=s(gl,"DIV",{class:!0});var hl=n(Zr);f(qw.$$.fragment,hl),p$o=i(hl),yle=s(hl,"P",{});var gKr=n(yle);_$o=r(gKr,"Instantiates one of the model classes of the library (with a audio classification head) from a configuration."),gKr.forEach(t),b$o=i(hl),Bd=s(hl,"P",{});var rz=n(Bd);v$o=r(rz,`Note:
Loading a model from its configuration file does `),wle=s(rz,"STRONG",{});var hKr=n(wle);T$o=r(hKr,"not"),hKr.forEach(t),F$o=r(rz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Ale=s(rz,"CODE",{});var uKr=n(Ale);C$o=r(uKr,"from_pretrained()"),uKr.forEach(t),M$o=r(rz,"to load the model weights."),rz.forEach(t),E$o=i(hl),Lle=s(hl,"P",{});var pKr=n(Lle);y$o=r(pKr,"Examples:"),pKr.forEach(t),w$o=i(hl),f(Gw.$$.fragment,hl),hl.forEach(t),A$o=i(gl),ze=s(gl,"DIV",{class:!0});var Ht=n(ze);f(Ow.$$.fragment,Ht),L$o=i(Ht),Ble=s(Ht,"P",{});var _Kr=n(Ble);B$o=r(_Kr,"Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model."),_Kr.forEach(t),x$o=i(Ht),Ka=s(Ht,"P",{});var D3=n(Ka);k$o=r(D3,"The model class to instantiate is selected based on the "),xle=s(D3,"CODE",{});var bKr=n(xle);R$o=r(bKr,"model_type"),bKr.forEach(t),S$o=r(D3,` property of the config object (either
passed as an argument or loaded from `),kle=s(D3,"CODE",{});var vKr=n(kle);P$o=r(vKr,"pretrained_model_name_or_path"),vKr.forEach(t),$$o=r(D3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Rle=s(D3,"CODE",{});var TKr=n(Rle);I$o=r(TKr,"pretrained_model_name_or_path"),TKr.forEach(t),D$o=r(D3,":"),D3.forEach(t),j$o=i(Ht),Ae=s(Ht,"UL",{});var No=n(Ae);OT=s(No,"LI",{});var _6e=n(OT);Sle=s(_6e,"STRONG",{});var FKr=n(Sle);N$o=r(FKr,"data2vec-audio"),FKr.forEach(t),q$o=r(_6e," \u2014 "),Sj=s(_6e,"A",{href:!0});var CKr=n(Sj);G$o=r(CKr,"Data2VecAudioForSequenceClassification"),CKr.forEach(t),O$o=r(_6e," (Data2VecAudio model)"),_6e.forEach(t),X$o=i(No),XT=s(No,"LI",{});var b6e=n(XT);Ple=s(b6e,"STRONG",{});var MKr=n(Ple);V$o=r(MKr,"hubert"),MKr.forEach(t),z$o=r(b6e," \u2014 "),Pj=s(b6e,"A",{href:!0});var EKr=n(Pj);W$o=r(EKr,"HubertForSequenceClassification"),EKr.forEach(t),Q$o=r(b6e," (Hubert model)"),b6e.forEach(t),H$o=i(No),VT=s(No,"LI",{});var v6e=n(VT);$le=s(v6e,"STRONG",{});var yKr=n($le);U$o=r(yKr,"sew"),yKr.forEach(t),J$o=r(v6e," \u2014 "),$j=s(v6e,"A",{href:!0});var wKr=n($j);Y$o=r(wKr,"SEWForSequenceClassification"),wKr.forEach(t),K$o=r(v6e," (SEW model)"),v6e.forEach(t),Z$o=i(No),zT=s(No,"LI",{});var T6e=n(zT);Ile=s(T6e,"STRONG",{});var AKr=n(Ile);eIo=r(AKr,"sew-d"),AKr.forEach(t),oIo=r(T6e," \u2014 "),Ij=s(T6e,"A",{href:!0});var LKr=n(Ij);rIo=r(LKr,"SEWDForSequenceClassification"),LKr.forEach(t),tIo=r(T6e," (SEW-D model)"),T6e.forEach(t),aIo=i(No),WT=s(No,"LI",{});var F6e=n(WT);Dle=s(F6e,"STRONG",{});var BKr=n(Dle);sIo=r(BKr,"unispeech"),BKr.forEach(t),nIo=r(F6e," \u2014 "),Dj=s(F6e,"A",{href:!0});var xKr=n(Dj);lIo=r(xKr,"UniSpeechForSequenceClassification"),xKr.forEach(t),iIo=r(F6e," (UniSpeech model)"),F6e.forEach(t),dIo=i(No),QT=s(No,"LI",{});var C6e=n(QT);jle=s(C6e,"STRONG",{});var kKr=n(jle);cIo=r(kKr,"unispeech-sat"),kKr.forEach(t),mIo=r(C6e," \u2014 "),jj=s(C6e,"A",{href:!0});var RKr=n(jj);fIo=r(RKr,"UniSpeechSatForSequenceClassification"),RKr.forEach(t),gIo=r(C6e," (UniSpeechSat model)"),C6e.forEach(t),hIo=i(No),HT=s(No,"LI",{});var M6e=n(HT);Nle=s(M6e,"STRONG",{});var SKr=n(Nle);uIo=r(SKr,"wav2vec2"),SKr.forEach(t),pIo=r(M6e," \u2014 "),Nj=s(M6e,"A",{href:!0});var PKr=n(Nj);_Io=r(PKr,"Wav2Vec2ForSequenceClassification"),PKr.forEach(t),bIo=r(M6e," (Wav2Vec2 model)"),M6e.forEach(t),vIo=i(No),UT=s(No,"LI",{});var E6e=n(UT);qle=s(E6e,"STRONG",{});var $Kr=n(qle);TIo=r($Kr,"wavlm"),$Kr.forEach(t),FIo=r(E6e," \u2014 "),qj=s(E6e,"A",{href:!0});var IKr=n(qj);CIo=r(IKr,"WavLMForSequenceClassification"),IKr.forEach(t),MIo=r(E6e," (WavLM model)"),E6e.forEach(t),No.forEach(t),EIo=i(Ht),JT=s(Ht,"P",{});var y6e=n(JT);yIo=r(y6e,"The model is set in evaluation mode by default using "),Gle=s(y6e,"CODE",{});var DKr=n(Gle);wIo=r(DKr,"model.eval()"),DKr.forEach(t),AIo=r(y6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Ole=s(y6e,"CODE",{});var jKr=n(Ole);LIo=r(jKr,"model.train()"),jKr.forEach(t),y6e.forEach(t),BIo=i(Ht),Xle=s(Ht,"P",{});var NKr=n(Xle);xIo=r(NKr,"Examples:"),NKr.forEach(t),kIo=i(Ht),f(Xw.$$.fragment,Ht),Ht.forEach(t),gl.forEach(t),aBe=i(c),xd=s(c,"H2",{class:!0});var mke=n(xd);YT=s(mke,"A",{id:!0,class:!0,href:!0});var qKr=n(YT);Vle=s(qKr,"SPAN",{});var GKr=n(Vle);f(Vw.$$.fragment,GKr),GKr.forEach(t),qKr.forEach(t),RIo=i(mke),zle=s(mke,"SPAN",{});var OKr=n(zle);SIo=r(OKr,"AutoModelForAudioFrameClassification"),OKr.forEach(t),mke.forEach(t),sBe=i(c),lr=s(c,"DIV",{class:!0});var ul=n(lr);f(zw.$$.fragment,ul),PIo=i(ul),kd=s(ul,"P",{});var tz=n(kd);$Io=r(tz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the `),Wle=s(tz,"CODE",{});var XKr=n(Wle);IIo=r(XKr,"from_pretrained()"),XKr.forEach(t),DIo=r(tz,"class method or the "),Qle=s(tz,"CODE",{});var VKr=n(Qle);jIo=r(VKr,"from_config()"),VKr.forEach(t),NIo=r(tz,`class
method.`),tz.forEach(t),qIo=i(ul),Ww=s(ul,"P",{});var fke=n(Ww);GIo=r(fke,"This class cannot be instantiated directly using "),Hle=s(fke,"CODE",{});var zKr=n(Hle);OIo=r(zKr,"__init__()"),zKr.forEach(t),XIo=r(fke," (throws an error)."),fke.forEach(t),VIo=i(ul),et=s(ul,"DIV",{class:!0});var pl=n(et);f(Qw.$$.fragment,pl),zIo=i(pl),Ule=s(pl,"P",{});var WKr=n(Ule);WIo=r(WKr,"Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration."),WKr.forEach(t),QIo=i(pl),Rd=s(pl,"P",{});var az=n(Rd);HIo=r(az,`Note:
Loading a model from its configuration file does `),Jle=s(az,"STRONG",{});var QKr=n(Jle);UIo=r(QKr,"not"),QKr.forEach(t),JIo=r(az,` load the model weights. It only affects the
model\u2019s configuration. Use `),Yle=s(az,"CODE",{});var HKr=n(Yle);YIo=r(HKr,"from_pretrained()"),HKr.forEach(t),KIo=r(az,"to load the model weights."),az.forEach(t),ZIo=i(pl),Kle=s(pl,"P",{});var UKr=n(Kle);eDo=r(UKr,"Examples:"),UKr.forEach(t),oDo=i(pl),f(Hw.$$.fragment,pl),pl.forEach(t),rDo=i(ul),We=s(ul,"DIV",{class:!0});var Ut=n(We);f(Uw.$$.fragment,Ut),tDo=i(Ut),Zle=s(Ut,"P",{});var JKr=n(Zle);aDo=r(JKr,"Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model."),JKr.forEach(t),sDo=i(Ut),Za=s(Ut,"P",{});var j3=n(Za);nDo=r(j3,"The model class to instantiate is selected based on the "),eie=s(j3,"CODE",{});var YKr=n(eie);lDo=r(YKr,"model_type"),YKr.forEach(t),iDo=r(j3,` property of the config object (either
passed as an argument or loaded from `),oie=s(j3,"CODE",{});var KKr=n(oie);dDo=r(KKr,"pretrained_model_name_or_path"),KKr.forEach(t),cDo=r(j3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),rie=s(j3,"CODE",{});var ZKr=n(rie);mDo=r(ZKr,"pretrained_model_name_or_path"),ZKr.forEach(t),fDo=r(j3,":"),j3.forEach(t),gDo=i(Ut),es=s(Ut,"UL",{});var N3=n(es);KT=s(N3,"LI",{});var w6e=n(KT);tie=s(w6e,"STRONG",{});var eZr=n(tie);hDo=r(eZr,"data2vec-audio"),eZr.forEach(t),uDo=r(w6e," \u2014 "),Gj=s(w6e,"A",{href:!0});var oZr=n(Gj);pDo=r(oZr,"Data2VecAudioForAudioFrameClassification"),oZr.forEach(t),_Do=r(w6e," (Data2VecAudio model)"),w6e.forEach(t),bDo=i(N3),ZT=s(N3,"LI",{});var A6e=n(ZT);aie=s(A6e,"STRONG",{});var rZr=n(aie);vDo=r(rZr,"unispeech-sat"),rZr.forEach(t),TDo=r(A6e," \u2014 "),Oj=s(A6e,"A",{href:!0});var tZr=n(Oj);FDo=r(tZr,"UniSpeechSatForAudioFrameClassification"),tZr.forEach(t),CDo=r(A6e," (UniSpeechSat model)"),A6e.forEach(t),MDo=i(N3),e1=s(N3,"LI",{});var L6e=n(e1);sie=s(L6e,"STRONG",{});var aZr=n(sie);EDo=r(aZr,"wav2vec2"),aZr.forEach(t),yDo=r(L6e," \u2014 "),Xj=s(L6e,"A",{href:!0});var sZr=n(Xj);wDo=r(sZr,"Wav2Vec2ForAudioFrameClassification"),sZr.forEach(t),ADo=r(L6e," (Wav2Vec2 model)"),L6e.forEach(t),LDo=i(N3),o1=s(N3,"LI",{});var B6e=n(o1);nie=s(B6e,"STRONG",{});var nZr=n(nie);BDo=r(nZr,"wavlm"),nZr.forEach(t),xDo=r(B6e," \u2014 "),Vj=s(B6e,"A",{href:!0});var lZr=n(Vj);kDo=r(lZr,"WavLMForAudioFrameClassification"),lZr.forEach(t),RDo=r(B6e," (WavLM model)"),B6e.forEach(t),N3.forEach(t),SDo=i(Ut),r1=s(Ut,"P",{});var x6e=n(r1);PDo=r(x6e,"The model is set in evaluation mode by default using "),lie=s(x6e,"CODE",{});var iZr=n(lie);$Do=r(iZr,"model.eval()"),iZr.forEach(t),IDo=r(x6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),iie=s(x6e,"CODE",{});var dZr=n(iie);DDo=r(dZr,"model.train()"),dZr.forEach(t),x6e.forEach(t),jDo=i(Ut),die=s(Ut,"P",{});var cZr=n(die);NDo=r(cZr,"Examples:"),cZr.forEach(t),qDo=i(Ut),f(Jw.$$.fragment,Ut),Ut.forEach(t),ul.forEach(t),nBe=i(c),Sd=s(c,"H2",{class:!0});var gke=n(Sd);t1=s(gke,"A",{id:!0,class:!0,href:!0});var mZr=n(t1);cie=s(mZr,"SPAN",{});var fZr=n(cie);f(Yw.$$.fragment,fZr),fZr.forEach(t),mZr.forEach(t),GDo=i(gke),mie=s(gke,"SPAN",{});var gZr=n(mie);ODo=r(gZr,"AutoModelForCTC"),gZr.forEach(t),gke.forEach(t),lBe=i(c),ir=s(c,"DIV",{class:!0});var _l=n(ir);f(Kw.$$.fragment,_l),XDo=i(_l),Pd=s(_l,"P",{});var sz=n(Pd);VDo=r(sz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the `),fie=s(sz,"CODE",{});var hZr=n(fie);zDo=r(hZr,"from_pretrained()"),hZr.forEach(t),WDo=r(sz,"class method or the "),gie=s(sz,"CODE",{});var uZr=n(gie);QDo=r(uZr,"from_config()"),uZr.forEach(t),HDo=r(sz,`class
method.`),sz.forEach(t),UDo=i(_l),Zw=s(_l,"P",{});var hke=n(Zw);JDo=r(hke,"This class cannot be instantiated directly using "),hie=s(hke,"CODE",{});var pZr=n(hie);YDo=r(pZr,"__init__()"),pZr.forEach(t),KDo=r(hke," (throws an error)."),hke.forEach(t),ZDo=i(_l),ot=s(_l,"DIV",{class:!0});var bl=n(ot);f(e6.$$.fragment,bl),ejo=i(bl),uie=s(bl,"P",{});var _Zr=n(uie);ojo=r(_Zr,"Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration."),_Zr.forEach(t),rjo=i(bl),$d=s(bl,"P",{});var nz=n($d);tjo=r(nz,`Note:
Loading a model from its configuration file does `),pie=s(nz,"STRONG",{});var bZr=n(pie);ajo=r(bZr,"not"),bZr.forEach(t),sjo=r(nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),_ie=s(nz,"CODE",{});var vZr=n(_ie);njo=r(vZr,"from_pretrained()"),vZr.forEach(t),ljo=r(nz,"to load the model weights."),nz.forEach(t),ijo=i(bl),bie=s(bl,"P",{});var TZr=n(bie);djo=r(TZr,"Examples:"),TZr.forEach(t),cjo=i(bl),f(o6.$$.fragment,bl),bl.forEach(t),mjo=i(_l),Qe=s(_l,"DIV",{class:!0});var Jt=n(Qe);f(r6.$$.fragment,Jt),fjo=i(Jt),vie=s(Jt,"P",{});var FZr=n(vie);gjo=r(FZr,"Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model."),FZr.forEach(t),hjo=i(Jt),os=s(Jt,"P",{});var q3=n(os);ujo=r(q3,"The model class to instantiate is selected based on the "),Tie=s(q3,"CODE",{});var CZr=n(Tie);pjo=r(CZr,"model_type"),CZr.forEach(t),_jo=r(q3,` property of the config object (either
passed as an argument or loaded from `),Fie=s(q3,"CODE",{});var MZr=n(Fie);bjo=r(MZr,"pretrained_model_name_or_path"),MZr.forEach(t),vjo=r(q3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Cie=s(q3,"CODE",{});var EZr=n(Cie);Tjo=r(EZr,"pretrained_model_name_or_path"),EZr.forEach(t),Fjo=r(q3,":"),q3.forEach(t),Cjo=i(Jt),Le=s(Jt,"UL",{});var qo=n(Le);a1=s(qo,"LI",{});var k6e=n(a1);Mie=s(k6e,"STRONG",{});var yZr=n(Mie);Mjo=r(yZr,"data2vec-audio"),yZr.forEach(t),Ejo=r(k6e," \u2014 "),zj=s(k6e,"A",{href:!0});var wZr=n(zj);yjo=r(wZr,"Data2VecAudioForCTC"),wZr.forEach(t),wjo=r(k6e," (Data2VecAudio model)"),k6e.forEach(t),Ajo=i(qo),s1=s(qo,"LI",{});var R6e=n(s1);Eie=s(R6e,"STRONG",{});var AZr=n(Eie);Ljo=r(AZr,"hubert"),AZr.forEach(t),Bjo=r(R6e," \u2014 "),Wj=s(R6e,"A",{href:!0});var LZr=n(Wj);xjo=r(LZr,"HubertForCTC"),LZr.forEach(t),kjo=r(R6e," (Hubert model)"),R6e.forEach(t),Rjo=i(qo),n1=s(qo,"LI",{});var S6e=n(n1);yie=s(S6e,"STRONG",{});var BZr=n(yie);Sjo=r(BZr,"sew"),BZr.forEach(t),Pjo=r(S6e," \u2014 "),Qj=s(S6e,"A",{href:!0});var xZr=n(Qj);$jo=r(xZr,"SEWForCTC"),xZr.forEach(t),Ijo=r(S6e," (SEW model)"),S6e.forEach(t),Djo=i(qo),l1=s(qo,"LI",{});var P6e=n(l1);wie=s(P6e,"STRONG",{});var kZr=n(wie);jjo=r(kZr,"sew-d"),kZr.forEach(t),Njo=r(P6e," \u2014 "),Hj=s(P6e,"A",{href:!0});var RZr=n(Hj);qjo=r(RZr,"SEWDForCTC"),RZr.forEach(t),Gjo=r(P6e," (SEW-D model)"),P6e.forEach(t),Ojo=i(qo),i1=s(qo,"LI",{});var $6e=n(i1);Aie=s($6e,"STRONG",{});var SZr=n(Aie);Xjo=r(SZr,"unispeech"),SZr.forEach(t),Vjo=r($6e," \u2014 "),Uj=s($6e,"A",{href:!0});var PZr=n(Uj);zjo=r(PZr,"UniSpeechForCTC"),PZr.forEach(t),Wjo=r($6e," (UniSpeech model)"),$6e.forEach(t),Qjo=i(qo),d1=s(qo,"LI",{});var I6e=n(d1);Lie=s(I6e,"STRONG",{});var $Zr=n(Lie);Hjo=r($Zr,"unispeech-sat"),$Zr.forEach(t),Ujo=r(I6e," \u2014 "),Jj=s(I6e,"A",{href:!0});var IZr=n(Jj);Jjo=r(IZr,"UniSpeechSatForCTC"),IZr.forEach(t),Yjo=r(I6e," (UniSpeechSat model)"),I6e.forEach(t),Kjo=i(qo),c1=s(qo,"LI",{});var D6e=n(c1);Bie=s(D6e,"STRONG",{});var DZr=n(Bie);Zjo=r(DZr,"wav2vec2"),DZr.forEach(t),eNo=r(D6e," \u2014 "),Yj=s(D6e,"A",{href:!0});var jZr=n(Yj);oNo=r(jZr,"Wav2Vec2ForCTC"),jZr.forEach(t),rNo=r(D6e," (Wav2Vec2 model)"),D6e.forEach(t),tNo=i(qo),m1=s(qo,"LI",{});var j6e=n(m1);xie=s(j6e,"STRONG",{});var NZr=n(xie);aNo=r(NZr,"wavlm"),NZr.forEach(t),sNo=r(j6e," \u2014 "),Kj=s(j6e,"A",{href:!0});var qZr=n(Kj);nNo=r(qZr,"WavLMForCTC"),qZr.forEach(t),lNo=r(j6e," (WavLM model)"),j6e.forEach(t),qo.forEach(t),iNo=i(Jt),f1=s(Jt,"P",{});var N6e=n(f1);dNo=r(N6e,"The model is set in evaluation mode by default using "),kie=s(N6e,"CODE",{});var GZr=n(kie);cNo=r(GZr,"model.eval()"),GZr.forEach(t),mNo=r(N6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Rie=s(N6e,"CODE",{});var OZr=n(Rie);fNo=r(OZr,"model.train()"),OZr.forEach(t),N6e.forEach(t),gNo=i(Jt),Sie=s(Jt,"P",{});var XZr=n(Sie);hNo=r(XZr,"Examples:"),XZr.forEach(t),uNo=i(Jt),f(t6.$$.fragment,Jt),Jt.forEach(t),_l.forEach(t),iBe=i(c),Id=s(c,"H2",{class:!0});var uke=n(Id);g1=s(uke,"A",{id:!0,class:!0,href:!0});var VZr=n(g1);Pie=s(VZr,"SPAN",{});var zZr=n(Pie);f(a6.$$.fragment,zZr),zZr.forEach(t),VZr.forEach(t),pNo=i(uke),$ie=s(uke,"SPAN",{});var WZr=n($ie);_No=r(WZr,"AutoModelForSpeechSeq2Seq"),WZr.forEach(t),uke.forEach(t),dBe=i(c),dr=s(c,"DIV",{class:!0});var vl=n(dr);f(s6.$$.fragment,vl),bNo=i(vl),Dd=s(vl,"P",{});var lz=n(Dd);vNo=r(lz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),Iie=s(lz,"CODE",{});var QZr=n(Iie);TNo=r(QZr,"from_pretrained()"),QZr.forEach(t),FNo=r(lz,"class method or the "),Die=s(lz,"CODE",{});var HZr=n(Die);CNo=r(HZr,"from_config()"),HZr.forEach(t),MNo=r(lz,`class
method.`),lz.forEach(t),ENo=i(vl),n6=s(vl,"P",{});var pke=n(n6);yNo=r(pke,"This class cannot be instantiated directly using "),jie=s(pke,"CODE",{});var UZr=n(jie);wNo=r(UZr,"__init__()"),UZr.forEach(t),ANo=r(pke," (throws an error)."),pke.forEach(t),LNo=i(vl),rt=s(vl,"DIV",{class:!0});var Tl=n(rt);f(l6.$$.fragment,Tl),BNo=i(Tl),Nie=s(Tl,"P",{});var JZr=n(Nie);xNo=r(JZr,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),JZr.forEach(t),kNo=i(Tl),jd=s(Tl,"P",{});var iz=n(jd);RNo=r(iz,`Note:
Loading a model from its configuration file does `),qie=s(iz,"STRONG",{});var YZr=n(qie);SNo=r(YZr,"not"),YZr.forEach(t),PNo=r(iz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Gie=s(iz,"CODE",{});var KZr=n(Gie);$No=r(KZr,"from_pretrained()"),KZr.forEach(t),INo=r(iz,"to load the model weights."),iz.forEach(t),DNo=i(Tl),Oie=s(Tl,"P",{});var ZZr=n(Oie);jNo=r(ZZr,"Examples:"),ZZr.forEach(t),NNo=i(Tl),f(i6.$$.fragment,Tl),Tl.forEach(t),qNo=i(vl),He=s(vl,"DIV",{class:!0});var Yt=n(He);f(d6.$$.fragment,Yt),GNo=i(Yt),Xie=s(Yt,"P",{});var eet=n(Xie);ONo=r(eet,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),eet.forEach(t),XNo=i(Yt),rs=s(Yt,"P",{});var G3=n(rs);VNo=r(G3,"The model class to instantiate is selected based on the "),Vie=s(G3,"CODE",{});var oet=n(Vie);zNo=r(oet,"model_type"),oet.forEach(t),WNo=r(G3,` property of the config object (either
passed as an argument or loaded from `),zie=s(G3,"CODE",{});var ret=n(zie);QNo=r(ret,"pretrained_model_name_or_path"),ret.forEach(t),HNo=r(G3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Wie=s(G3,"CODE",{});var tet=n(Wie);UNo=r(tet,"pretrained_model_name_or_path"),tet.forEach(t),JNo=r(G3,":"),G3.forEach(t),YNo=i(Yt),c6=s(Yt,"UL",{});var _ke=n(c6);h1=s(_ke,"LI",{});var q6e=n(h1);Qie=s(q6e,"STRONG",{});var aet=n(Qie);KNo=r(aet,"speech-encoder-decoder"),aet.forEach(t),ZNo=r(q6e," \u2014 "),Zj=s(q6e,"A",{href:!0});var set=n(Zj);eqo=r(set,"SpeechEncoderDecoderModel"),set.forEach(t),oqo=r(q6e," (Speech Encoder decoder model)"),q6e.forEach(t),rqo=i(_ke),u1=s(_ke,"LI",{});var G6e=n(u1);Hie=s(G6e,"STRONG",{});var net=n(Hie);tqo=r(net,"speech_to_text"),net.forEach(t),aqo=r(G6e," \u2014 "),eN=s(G6e,"A",{href:!0});var iet=n(eN);sqo=r(iet,"Speech2TextForConditionalGeneration"),iet.forEach(t),nqo=r(G6e," (Speech2Text model)"),G6e.forEach(t),_ke.forEach(t),lqo=i(Yt),p1=s(Yt,"P",{});var O6e=n(p1);iqo=r(O6e,"The model is set in evaluation mode by default using "),Uie=s(O6e,"CODE",{});var det=n(Uie);dqo=r(det,"model.eval()"),det.forEach(t),cqo=r(O6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Jie=s(O6e,"CODE",{});var cet=n(Jie);mqo=r(cet,"model.train()"),cet.forEach(t),O6e.forEach(t),fqo=i(Yt),Yie=s(Yt,"P",{});var met=n(Yie);gqo=r(met,"Examples:"),met.forEach(t),hqo=i(Yt),f(m6.$$.fragment,Yt),Yt.forEach(t),vl.forEach(t),cBe=i(c),Nd=s(c,"H2",{class:!0});var bke=n(Nd);_1=s(bke,"A",{id:!0,class:!0,href:!0});var fet=n(_1);Kie=s(fet,"SPAN",{});var get=n(Kie);f(f6.$$.fragment,get),get.forEach(t),fet.forEach(t),uqo=i(bke),Zie=s(bke,"SPAN",{});var het=n(Zie);pqo=r(het,"AutoModelForAudioXVector"),het.forEach(t),bke.forEach(t),mBe=i(c),cr=s(c,"DIV",{class:!0});var Fl=n(cr);f(g6.$$.fragment,Fl),_qo=i(Fl),qd=s(Fl,"P",{});var dz=n(qd);bqo=r(dz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the `),ede=s(dz,"CODE",{});var uet=n(ede);vqo=r(uet,"from_pretrained()"),uet.forEach(t),Tqo=r(dz,"class method or the "),ode=s(dz,"CODE",{});var pet=n(ode);Fqo=r(pet,"from_config()"),pet.forEach(t),Cqo=r(dz,`class
method.`),dz.forEach(t),Mqo=i(Fl),h6=s(Fl,"P",{});var vke=n(h6);Eqo=r(vke,"This class cannot be instantiated directly using "),rde=s(vke,"CODE",{});var _et=n(rde);yqo=r(_et,"__init__()"),_et.forEach(t),wqo=r(vke," (throws an error)."),vke.forEach(t),Aqo=i(Fl),tt=s(Fl,"DIV",{class:!0});var Cl=n(tt);f(u6.$$.fragment,Cl),Lqo=i(Cl),tde=s(Cl,"P",{});var bet=n(tde);Bqo=r(bet,"Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration."),bet.forEach(t),xqo=i(Cl),Gd=s(Cl,"P",{});var cz=n(Gd);kqo=r(cz,`Note:
Loading a model from its configuration file does `),ade=s(cz,"STRONG",{});var vet=n(ade);Rqo=r(vet,"not"),vet.forEach(t),Sqo=r(cz,` load the model weights. It only affects the
model\u2019s configuration. Use `),sde=s(cz,"CODE",{});var Tet=n(sde);Pqo=r(Tet,"from_pretrained()"),Tet.forEach(t),$qo=r(cz,"to load the model weights."),cz.forEach(t),Iqo=i(Cl),nde=s(Cl,"P",{});var Fet=n(nde);Dqo=r(Fet,"Examples:"),Fet.forEach(t),jqo=i(Cl),f(p6.$$.fragment,Cl),Cl.forEach(t),Nqo=i(Fl),Ue=s(Fl,"DIV",{class:!0});var Kt=n(Ue);f(_6.$$.fragment,Kt),qqo=i(Kt),lde=s(Kt,"P",{});var Cet=n(lde);Gqo=r(Cet,"Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model."),Cet.forEach(t),Oqo=i(Kt),ts=s(Kt,"P",{});var O3=n(ts);Xqo=r(O3,"The model class to instantiate is selected based on the "),ide=s(O3,"CODE",{});var Met=n(ide);Vqo=r(Met,"model_type"),Met.forEach(t),zqo=r(O3,` property of the config object (either
passed as an argument or loaded from `),dde=s(O3,"CODE",{});var Eet=n(dde);Wqo=r(Eet,"pretrained_model_name_or_path"),Eet.forEach(t),Qqo=r(O3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),cde=s(O3,"CODE",{});var yet=n(cde);Hqo=r(yet,"pretrained_model_name_or_path"),yet.forEach(t),Uqo=r(O3,":"),O3.forEach(t),Jqo=i(Kt),as=s(Kt,"UL",{});var X3=n(as);b1=s(X3,"LI",{});var X6e=n(b1);mde=s(X6e,"STRONG",{});var wet=n(mde);Yqo=r(wet,"data2vec-audio"),wet.forEach(t),Kqo=r(X6e," \u2014 "),oN=s(X6e,"A",{href:!0});var Aet=n(oN);Zqo=r(Aet,"Data2VecAudioForXVector"),Aet.forEach(t),eGo=r(X6e," (Data2VecAudio model)"),X6e.forEach(t),oGo=i(X3),v1=s(X3,"LI",{});var V6e=n(v1);fde=s(V6e,"STRONG",{});var Let=n(fde);rGo=r(Let,"unispeech-sat"),Let.forEach(t),tGo=r(V6e," \u2014 "),rN=s(V6e,"A",{href:!0});var Bet=n(rN);aGo=r(Bet,"UniSpeechSatForXVector"),Bet.forEach(t),sGo=r(V6e," (UniSpeechSat model)"),V6e.forEach(t),nGo=i(X3),T1=s(X3,"LI",{});var z6e=n(T1);gde=s(z6e,"STRONG",{});var xet=n(gde);lGo=r(xet,"wav2vec2"),xet.forEach(t),iGo=r(z6e," \u2014 "),tN=s(z6e,"A",{href:!0});var ket=n(tN);dGo=r(ket,"Wav2Vec2ForXVector"),ket.forEach(t),cGo=r(z6e," (Wav2Vec2 model)"),z6e.forEach(t),mGo=i(X3),F1=s(X3,"LI",{});var W6e=n(F1);hde=s(W6e,"STRONG",{});var Ret=n(hde);fGo=r(Ret,"wavlm"),Ret.forEach(t),gGo=r(W6e," \u2014 "),aN=s(W6e,"A",{href:!0});var Set=n(aN);hGo=r(Set,"WavLMForXVector"),Set.forEach(t),uGo=r(W6e," (WavLM model)"),W6e.forEach(t),X3.forEach(t),pGo=i(Kt),C1=s(Kt,"P",{});var Q6e=n(C1);_Go=r(Q6e,"The model is set in evaluation mode by default using "),ude=s(Q6e,"CODE",{});var Pet=n(ude);bGo=r(Pet,"model.eval()"),Pet.forEach(t),vGo=r(Q6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),pde=s(Q6e,"CODE",{});var $et=n(pde);TGo=r($et,"model.train()"),$et.forEach(t),Q6e.forEach(t),FGo=i(Kt),_de=s(Kt,"P",{});var Iet=n(_de);CGo=r(Iet,"Examples:"),Iet.forEach(t),MGo=i(Kt),f(b6.$$.fragment,Kt),Kt.forEach(t),Fl.forEach(t),fBe=i(c),Od=s(c,"H2",{class:!0});var Tke=n(Od);M1=s(Tke,"A",{id:!0,class:!0,href:!0});var Det=n(M1);bde=s(Det,"SPAN",{});var jet=n(bde);f(v6.$$.fragment,jet),jet.forEach(t),Det.forEach(t),EGo=i(Tke),vde=s(Tke,"SPAN",{});var Net=n(vde);yGo=r(Net,"AutoModelForMaskedImageModeling"),Net.forEach(t),Tke.forEach(t),gBe=i(c),mr=s(c,"DIV",{class:!0});var Ml=n(mr);f(T6.$$.fragment,Ml),wGo=i(Ml),Xd=s(Ml,"P",{});var mz=n(Xd);AGo=r(mz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the `),Tde=s(mz,"CODE",{});var qet=n(Tde);LGo=r(qet,"from_pretrained()"),qet.forEach(t),BGo=r(mz,"class method or the "),Fde=s(mz,"CODE",{});var Get=n(Fde);xGo=r(Get,"from_config()"),Get.forEach(t),kGo=r(mz,`class
method.`),mz.forEach(t),RGo=i(Ml),F6=s(Ml,"P",{});var Fke=n(F6);SGo=r(Fke,"This class cannot be instantiated directly using "),Cde=s(Fke,"CODE",{});var Oet=n(Cde);PGo=r(Oet,"__init__()"),Oet.forEach(t),$Go=r(Fke," (throws an error)."),Fke.forEach(t),IGo=i(Ml),at=s(Ml,"DIV",{class:!0});var El=n(at);f(C6.$$.fragment,El),DGo=i(El),Mde=s(El,"P",{});var Xet=n(Mde);jGo=r(Xet,"Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration."),Xet.forEach(t),NGo=i(El),Vd=s(El,"P",{});var fz=n(Vd);qGo=r(fz,`Note:
Loading a model from its configuration file does `),Ede=s(fz,"STRONG",{});var Vet=n(Ede);GGo=r(Vet,"not"),Vet.forEach(t),OGo=r(fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),yde=s(fz,"CODE",{});var zet=n(yde);XGo=r(zet,"from_pretrained()"),zet.forEach(t),VGo=r(fz,"to load the model weights."),fz.forEach(t),zGo=i(El),wde=s(El,"P",{});var Wet=n(wde);WGo=r(Wet,"Examples:"),Wet.forEach(t),QGo=i(El),f(M6.$$.fragment,El),El.forEach(t),HGo=i(Ml),Je=s(Ml,"DIV",{class:!0});var Zt=n(Je);f(E6.$$.fragment,Zt),UGo=i(Zt),Ade=s(Zt,"P",{});var Qet=n(Ade);JGo=r(Qet,"Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model."),Qet.forEach(t),YGo=i(Zt),ss=s(Zt,"P",{});var V3=n(ss);KGo=r(V3,"The model class to instantiate is selected based on the "),Lde=s(V3,"CODE",{});var Het=n(Lde);ZGo=r(Het,"model_type"),Het.forEach(t),eOo=r(V3,` property of the config object (either
passed as an argument or loaded from `),Bde=s(V3,"CODE",{});var Uet=n(Bde);oOo=r(Uet,"pretrained_model_name_or_path"),Uet.forEach(t),rOo=r(V3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xde=s(V3,"CODE",{});var Jet=n(xde);tOo=r(Jet,"pretrained_model_name_or_path"),Jet.forEach(t),aOo=r(V3,":"),V3.forEach(t),sOo=i(Zt),zd=s(Zt,"UL",{});var gz=n(zd);E1=s(gz,"LI",{});var H6e=n(E1);kde=s(H6e,"STRONG",{});var Yet=n(kde);nOo=r(Yet,"deit"),Yet.forEach(t),lOo=r(H6e," \u2014 "),sN=s(H6e,"A",{href:!0});var Ket=n(sN);iOo=r(Ket,"DeiTForMaskedImageModeling"),Ket.forEach(t),dOo=r(H6e," (DeiT model)"),H6e.forEach(t),cOo=i(gz),y1=s(gz,"LI",{});var U6e=n(y1);Rde=s(U6e,"STRONG",{});var Zet=n(Rde);mOo=r(Zet,"swin"),Zet.forEach(t),fOo=r(U6e," \u2014 "),nN=s(U6e,"A",{href:!0});var eot=n(nN);gOo=r(eot,"SwinForMaskedImageModeling"),eot.forEach(t),hOo=r(U6e," (Swin model)"),U6e.forEach(t),uOo=i(gz),w1=s(gz,"LI",{});var J6e=n(w1);Sde=s(J6e,"STRONG",{});var oot=n(Sde);pOo=r(oot,"vit"),oot.forEach(t),_Oo=r(J6e," \u2014 "),lN=s(J6e,"A",{href:!0});var rot=n(lN);bOo=r(rot,"ViTForMaskedImageModeling"),rot.forEach(t),vOo=r(J6e," (ViT model)"),J6e.forEach(t),gz.forEach(t),TOo=i(Zt),A1=s(Zt,"P",{});var Y6e=n(A1);FOo=r(Y6e,"The model is set in evaluation mode by default using "),Pde=s(Y6e,"CODE",{});var tot=n(Pde);COo=r(tot,"model.eval()"),tot.forEach(t),MOo=r(Y6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$de=s(Y6e,"CODE",{});var aot=n($de);EOo=r(aot,"model.train()"),aot.forEach(t),Y6e.forEach(t),yOo=i(Zt),Ide=s(Zt,"P",{});var sot=n(Ide);wOo=r(sot,"Examples:"),sot.forEach(t),AOo=i(Zt),f(y6.$$.fragment,Zt),Zt.forEach(t),Ml.forEach(t),hBe=i(c),Wd=s(c,"H2",{class:!0});var Cke=n(Wd);L1=s(Cke,"A",{id:!0,class:!0,href:!0});var not=n(L1);Dde=s(not,"SPAN",{});var lot=n(Dde);f(w6.$$.fragment,lot),lot.forEach(t),not.forEach(t),LOo=i(Cke),jde=s(Cke,"SPAN",{});var iot=n(jde);BOo=r(iot,"AutoModelForObjectDetection"),iot.forEach(t),Cke.forEach(t),uBe=i(c),fr=s(c,"DIV",{class:!0});var yl=n(fr);f(A6.$$.fragment,yl),xOo=i(yl),Qd=s(yl,"P",{});var hz=n(Qd);kOo=r(hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the `),Nde=s(hz,"CODE",{});var dot=n(Nde);ROo=r(dot,"from_pretrained()"),dot.forEach(t),SOo=r(hz,"class method or the "),qde=s(hz,"CODE",{});var cot=n(qde);POo=r(cot,"from_config()"),cot.forEach(t),$Oo=r(hz,`class
method.`),hz.forEach(t),IOo=i(yl),L6=s(yl,"P",{});var Mke=n(L6);DOo=r(Mke,"This class cannot be instantiated directly using "),Gde=s(Mke,"CODE",{});var mot=n(Gde);jOo=r(mot,"__init__()"),mot.forEach(t),NOo=r(Mke," (throws an error)."),Mke.forEach(t),qOo=i(yl),st=s(yl,"DIV",{class:!0});var wl=n(st);f(B6.$$.fragment,wl),GOo=i(wl),Ode=s(wl,"P",{});var fot=n(Ode);OOo=r(fot,"Instantiates one of the model classes of the library (with a object detection head) from a configuration."),fot.forEach(t),XOo=i(wl),Hd=s(wl,"P",{});var uz=n(Hd);VOo=r(uz,`Note:
Loading a model from its configuration file does `),Xde=s(uz,"STRONG",{});var got=n(Xde);zOo=r(got,"not"),got.forEach(t),WOo=r(uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vde=s(uz,"CODE",{});var hot=n(Vde);QOo=r(hot,"from_pretrained()"),hot.forEach(t),HOo=r(uz,"to load the model weights."),uz.forEach(t),UOo=i(wl),zde=s(wl,"P",{});var uot=n(zde);JOo=r(uot,"Examples:"),uot.forEach(t),YOo=i(wl),f(x6.$$.fragment,wl),wl.forEach(t),KOo=i(yl),Ye=s(yl,"DIV",{class:!0});var ea=n(Ye);f(k6.$$.fragment,ea),ZOo=i(ea),Wde=s(ea,"P",{});var pot=n(Wde);eXo=r(pot,"Instantiate one of the model classes of the library (with a object detection head) from a pretrained model."),pot.forEach(t),oXo=i(ea),ns=s(ea,"P",{});var z3=n(ns);rXo=r(z3,"The model class to instantiate is selected based on the "),Qde=s(z3,"CODE",{});var _ot=n(Qde);tXo=r(_ot,"model_type"),_ot.forEach(t),aXo=r(z3,` property of the config object (either
passed as an argument or loaded from `),Hde=s(z3,"CODE",{});var bot=n(Hde);sXo=r(bot,"pretrained_model_name_or_path"),bot.forEach(t),nXo=r(z3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Ude=s(z3,"CODE",{});var vot=n(Ude);lXo=r(vot,"pretrained_model_name_or_path"),vot.forEach(t),iXo=r(z3,":"),z3.forEach(t),dXo=i(ea),Jde=s(ea,"UL",{});var Tot=n(Jde);B1=s(Tot,"LI",{});var K6e=n(B1);Yde=s(K6e,"STRONG",{});var Fot=n(Yde);cXo=r(Fot,"detr"),Fot.forEach(t),mXo=r(K6e," \u2014 "),iN=s(K6e,"A",{href:!0});var Cot=n(iN);fXo=r(Cot,"DetrForObjectDetection"),Cot.forEach(t),gXo=r(K6e," (DETR model)"),K6e.forEach(t),Tot.forEach(t),hXo=i(ea),x1=s(ea,"P",{});var Z6e=n(x1);uXo=r(Z6e,"The model is set in evaluation mode by default using "),Kde=s(Z6e,"CODE",{});var Mot=n(Kde);pXo=r(Mot,"model.eval()"),Mot.forEach(t),_Xo=r(Z6e,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),Zde=s(Z6e,"CODE",{});var Eot=n(Zde);bXo=r(Eot,"model.train()"),Eot.forEach(t),Z6e.forEach(t),vXo=i(ea),ece=s(ea,"P",{});var yot=n(ece);TXo=r(yot,"Examples:"),yot.forEach(t),FXo=i(ea),f(R6.$$.fragment,ea),ea.forEach(t),yl.forEach(t),pBe=i(c),Ud=s(c,"H2",{class:!0});var Eke=n(Ud);k1=s(Eke,"A",{id:!0,class:!0,href:!0});var wot=n(k1);oce=s(wot,"SPAN",{});var Aot=n(oce);f(S6.$$.fragment,Aot),Aot.forEach(t),wot.forEach(t),CXo=i(Eke),rce=s(Eke,"SPAN",{});var Lot=n(rce);MXo=r(Lot,"AutoModelForImageSegmentation"),Lot.forEach(t),Eke.forEach(t),_Be=i(c),gr=s(c,"DIV",{class:!0});var Al=n(gr);f(P6.$$.fragment,Al),EXo=i(Al),Jd=s(Al,"P",{});var pz=n(Jd);yXo=r(pz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the `),tce=s(pz,"CODE",{});var Bot=n(tce);wXo=r(Bot,"from_pretrained()"),Bot.forEach(t),AXo=r(pz,"class method or the "),ace=s(pz,"CODE",{});var xot=n(ace);LXo=r(xot,"from_config()"),xot.forEach(t),BXo=r(pz,`class
method.`),pz.forEach(t),xXo=i(Al),$6=s(Al,"P",{});var yke=n($6);kXo=r(yke,"This class cannot be instantiated directly using "),sce=s(yke,"CODE",{});var kot=n(sce);RXo=r(kot,"__init__()"),kot.forEach(t),SXo=r(yke," (throws an error)."),yke.forEach(t),PXo=i(Al),nt=s(Al,"DIV",{class:!0});var Ll=n(nt);f(I6.$$.fragment,Ll),$Xo=i(Ll),nce=s(Ll,"P",{});var Rot=n(nce);IXo=r(Rot,"Instantiates one of the model classes of the library (with a image segmentation head) from a configuration."),Rot.forEach(t),DXo=i(Ll),Yd=s(Ll,"P",{});var _z=n(Yd);jXo=r(_z,`Note:
Loading a model from its configuration file does `),lce=s(_z,"STRONG",{});var Sot=n(lce);NXo=r(Sot,"not"),Sot.forEach(t),qXo=r(_z,` load the model weights. It only affects the
model\u2019s configuration. Use `),ice=s(_z,"CODE",{});var Pot=n(ice);GXo=r(Pot,"from_pretrained()"),Pot.forEach(t),OXo=r(_z,"to load the model weights."),_z.forEach(t),XXo=i(Ll),dce=s(Ll,"P",{});var $ot=n(dce);VXo=r($ot,"Examples:"),$ot.forEach(t),zXo=i(Ll),f(D6.$$.fragment,Ll),Ll.forEach(t),WXo=i(Al),Ke=s(Al,"DIV",{class:!0});var oa=n(Ke);f(j6.$$.fragment,oa),QXo=i(oa),cce=s(oa,"P",{});var Iot=n(cce);HXo=r(Iot,"Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model."),Iot.forEach(t),UXo=i(oa),ls=s(oa,"P",{});var W3=n(ls);JXo=r(W3,"The model class to instantiate is selected based on the "),mce=s(W3,"CODE",{});var Dot=n(mce);YXo=r(Dot,"model_type"),Dot.forEach(t),KXo=r(W3,` property of the config object (either
passed as an argument or loaded from `),fce=s(W3,"CODE",{});var jot=n(fce);ZXo=r(jot,"pretrained_model_name_or_path"),jot.forEach(t),eVo=r(W3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),gce=s(W3,"CODE",{});var Not=n(gce);oVo=r(Not,"pretrained_model_name_or_path"),Not.forEach(t),rVo=r(W3,":"),W3.forEach(t),tVo=i(oa),hce=s(oa,"UL",{});var qot=n(hce);R1=s(qot,"LI",{});var eAe=n(R1);uce=s(eAe,"STRONG",{});var Got=n(uce);aVo=r(Got,"detr"),Got.forEach(t),sVo=r(eAe," \u2014 "),dN=s(eAe,"A",{href:!0});var Oot=n(dN);nVo=r(Oot,"DetrForSegmentation"),Oot.forEach(t),lVo=r(eAe," (DETR model)"),eAe.forEach(t),qot.forEach(t),iVo=i(oa),S1=s(oa,"P",{});var oAe=n(S1);dVo=r(oAe,"The model is set in evaluation mode by default using "),pce=s(oAe,"CODE",{});var Xot=n(pce);cVo=r(Xot,"model.eval()"),Xot.forEach(t),mVo=r(oAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),_ce=s(oAe,"CODE",{});var Vot=n(_ce);fVo=r(Vot,"model.train()"),Vot.forEach(t),oAe.forEach(t),gVo=i(oa),bce=s(oa,"P",{});var zot=n(bce);hVo=r(zot,"Examples:"),zot.forEach(t),uVo=i(oa),f(N6.$$.fragment,oa),oa.forEach(t),Al.forEach(t),bBe=i(c),Kd=s(c,"H2",{class:!0});var wke=n(Kd);P1=s(wke,"A",{id:!0,class:!0,href:!0});var Wot=n(P1);vce=s(Wot,"SPAN",{});var Qot=n(vce);f(q6.$$.fragment,Qot),Qot.forEach(t),Wot.forEach(t),pVo=i(wke),Tce=s(wke,"SPAN",{});var Hot=n(Tce);_Vo=r(Hot,"AutoModelForSemanticSegmentation"),Hot.forEach(t),wke.forEach(t),vBe=i(c),hr=s(c,"DIV",{class:!0});var Bl=n(hr);f(G6.$$.fragment,Bl),bVo=i(Bl),Zd=s(Bl,"P",{});var bz=n(Zd);vVo=r(bz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the `),Fce=s(bz,"CODE",{});var Uot=n(Fce);TVo=r(Uot,"from_pretrained()"),Uot.forEach(t),FVo=r(bz,"class method or the "),Cce=s(bz,"CODE",{});var Jot=n(Cce);CVo=r(Jot,"from_config()"),Jot.forEach(t),MVo=r(bz,`class
method.`),bz.forEach(t),EVo=i(Bl),O6=s(Bl,"P",{});var Ake=n(O6);yVo=r(Ake,"This class cannot be instantiated directly using "),Mce=s(Ake,"CODE",{});var Yot=n(Mce);wVo=r(Yot,"__init__()"),Yot.forEach(t),AVo=r(Ake," (throws an error)."),Ake.forEach(t),LVo=i(Bl),lt=s(Bl,"DIV",{class:!0});var xl=n(lt);f(X6.$$.fragment,xl),BVo=i(xl),Ece=s(xl,"P",{});var Kot=n(Ece);xVo=r(Kot,"Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration."),Kot.forEach(t),kVo=i(xl),ec=s(xl,"P",{});var vz=n(ec);RVo=r(vz,`Note:
Loading a model from its configuration file does `),yce=s(vz,"STRONG",{});var Zot=n(yce);SVo=r(Zot,"not"),Zot.forEach(t),PVo=r(vz,` load the model weights. It only affects the
model\u2019s configuration. Use `),wce=s(vz,"CODE",{});var ert=n(wce);$Vo=r(ert,"from_pretrained()"),ert.forEach(t),IVo=r(vz,"to load the model weights."),vz.forEach(t),DVo=i(xl),Ace=s(xl,"P",{});var ort=n(Ace);jVo=r(ort,"Examples:"),ort.forEach(t),NVo=i(xl),f(V6.$$.fragment,xl),xl.forEach(t),qVo=i(Bl),Ze=s(Bl,"DIV",{class:!0});var ra=n(Ze);f(z6.$$.fragment,ra),GVo=i(ra),Lce=s(ra,"P",{});var rrt=n(Lce);OVo=r(rrt,"Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model."),rrt.forEach(t),XVo=i(ra),is=s(ra,"P",{});var Q3=n(is);VVo=r(Q3,"The model class to instantiate is selected based on the "),Bce=s(Q3,"CODE",{});var trt=n(Bce);zVo=r(trt,"model_type"),trt.forEach(t),WVo=r(Q3,` property of the config object (either
passed as an argument or loaded from `),xce=s(Q3,"CODE",{});var art=n(xce);QVo=r(art,"pretrained_model_name_or_path"),art.forEach(t),HVo=r(Q3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),kce=s(Q3,"CODE",{});var srt=n(kce);UVo=r(srt,"pretrained_model_name_or_path"),srt.forEach(t),JVo=r(Q3,":"),Q3.forEach(t),YVo=i(ra),W6=s(ra,"UL",{});var Lke=n(W6);$1=s(Lke,"LI",{});var rAe=n($1);Rce=s(rAe,"STRONG",{});var nrt=n(Rce);KVo=r(nrt,"beit"),nrt.forEach(t),ZVo=r(rAe," \u2014 "),cN=s(rAe,"A",{href:!0});var lrt=n(cN);ezo=r(lrt,"BeitForSemanticSegmentation"),lrt.forEach(t),ozo=r(rAe," (BEiT model)"),rAe.forEach(t),rzo=i(Lke),I1=s(Lke,"LI",{});var tAe=n(I1);Sce=s(tAe,"STRONG",{});var irt=n(Sce);tzo=r(irt,"segformer"),irt.forEach(t),azo=r(tAe," \u2014 "),mN=s(tAe,"A",{href:!0});var drt=n(mN);szo=r(drt,"SegformerForSemanticSegmentation"),drt.forEach(t),nzo=r(tAe," (SegFormer model)"),tAe.forEach(t),Lke.forEach(t),lzo=i(ra),D1=s(ra,"P",{});var aAe=n(D1);izo=r(aAe,"The model is set in evaluation mode by default using "),Pce=s(aAe,"CODE",{});var crt=n(Pce);dzo=r(crt,"model.eval()"),crt.forEach(t),czo=r(aAe,` (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with `),$ce=s(aAe,"CODE",{});var mrt=n($ce);mzo=r(mrt,"model.train()"),mrt.forEach(t),aAe.forEach(t),fzo=i(ra),Ice=s(ra,"P",{});var frt=n(Ice);gzo=r(frt,"Examples:"),frt.forEach(t),hzo=i(ra),f(Q6.$$.fragment,ra),ra.forEach(t),Bl.forEach(t),TBe=i(c),oc=s(c,"H2",{class:!0});var Bke=n(oc);j1=s(Bke,"A",{id:!0,class:!0,href:!0});var grt=n(j1);Dce=s(grt,"SPAN",{});var hrt=n(Dce);f(H6.$$.fragment,hrt),hrt.forEach(t),grt.forEach(t),uzo=i(Bke),jce=s(Bke,"SPAN",{});var urt=n(jce);pzo=r(urt,"TFAutoModel"),urt.forEach(t),Bke.forEach(t),FBe=i(c),ur=s(c,"DIV",{class:!0});var kl=n(ur);f(U6.$$.fragment,kl),_zo=i(kl),rc=s(kl,"P",{});var Tz=n(rc);bzo=r(Tz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),Nce=s(Tz,"CODE",{});var prt=n(Nce);vzo=r(prt,"from_pretrained()"),prt.forEach(t),Tzo=r(Tz,"class method or the "),qce=s(Tz,"CODE",{});var _rt=n(qce);Fzo=r(_rt,"from_config()"),_rt.forEach(t),Czo=r(Tz,`class
method.`),Tz.forEach(t),Mzo=i(kl),J6=s(kl,"P",{});var xke=n(J6);Ezo=r(xke,"This class cannot be instantiated directly using "),Gce=s(xke,"CODE",{});var brt=n(Gce);yzo=r(brt,"__init__()"),brt.forEach(t),wzo=r(xke," (throws an error)."),xke.forEach(t),Azo=i(kl),it=s(kl,"DIV",{class:!0});var Rl=n(it);f(Y6.$$.fragment,Rl),Lzo=i(Rl),Oce=s(Rl,"P",{});var vrt=n(Oce);Bzo=r(vrt,"Instantiates one of the base model classes of the library from a configuration."),vrt.forEach(t),xzo=i(Rl),tc=s(Rl,"P",{});var Fz=n(tc);kzo=r(Fz,`Note:
Loading a model from its configuration file does `),Xce=s(Fz,"STRONG",{});var Trt=n(Xce);Rzo=r(Trt,"not"),Trt.forEach(t),Szo=r(Fz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Vce=s(Fz,"CODE",{});var Frt=n(Vce);Pzo=r(Frt,"from_pretrained()"),Frt.forEach(t),$zo=r(Fz,"to load the model weights."),Fz.forEach(t),Izo=i(Rl),zce=s(Rl,"P",{});var Crt=n(zce);Dzo=r(Crt,"Examples:"),Crt.forEach(t),jzo=i(Rl),f(K6.$$.fragment,Rl),Rl.forEach(t),Nzo=i(kl),go=s(kl,"DIV",{class:!0});var ca=n(go);f(Z6.$$.fragment,ca),qzo=i(ca),Wce=s(ca,"P",{});var Mrt=n(Wce);Gzo=r(Mrt,"Instantiate one of the base model classes of the library from a pretrained model."),Mrt.forEach(t),Ozo=i(ca),ds=s(ca,"P",{});var H3=n(ds);Xzo=r(H3,"The model class to instantiate is selected based on the "),Qce=s(H3,"CODE",{});var Ert=n(Qce);Vzo=r(Ert,"model_type"),Ert.forEach(t),zzo=r(H3,` property of the config object (either
passed as an argument or loaded from `),Hce=s(H3,"CODE",{});var yrt=n(Hce);Wzo=r(yrt,"pretrained_model_name_or_path"),yrt.forEach(t),Qzo=r(H3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Uce=s(H3,"CODE",{});var wrt=n(Uce);Hzo=r(wrt,"pretrained_model_name_or_path"),wrt.forEach(t),Uzo=r(H3,":"),H3.forEach(t),Jzo=i(ca),B=s(ca,"UL",{});var x=n(B);N1=s(x,"LI",{});var sAe=n(N1);Jce=s(sAe,"STRONG",{});var Art=n(Jce);Yzo=r(Art,"albert"),Art.forEach(t),Kzo=r(sAe," \u2014 "),fN=s(sAe,"A",{href:!0});var Lrt=n(fN);Zzo=r(Lrt,"TFAlbertModel"),Lrt.forEach(t),eWo=r(sAe," (ALBERT model)"),sAe.forEach(t),oWo=i(x),q1=s(x,"LI",{});var nAe=n(q1);Yce=s(nAe,"STRONG",{});var Brt=n(Yce);rWo=r(Brt,"bart"),Brt.forEach(t),tWo=r(nAe," \u2014 "),gN=s(nAe,"A",{href:!0});var xrt=n(gN);aWo=r(xrt,"TFBartModel"),xrt.forEach(t),sWo=r(nAe," (BART model)"),nAe.forEach(t),nWo=i(x),G1=s(x,"LI",{});var lAe=n(G1);Kce=s(lAe,"STRONG",{});var krt=n(Kce);lWo=r(krt,"bert"),krt.forEach(t),iWo=r(lAe," \u2014 "),hN=s(lAe,"A",{href:!0});var Rrt=n(hN);dWo=r(Rrt,"TFBertModel"),Rrt.forEach(t),cWo=r(lAe," (BERT model)"),lAe.forEach(t),mWo=i(x),O1=s(x,"LI",{});var iAe=n(O1);Zce=s(iAe,"STRONG",{});var Srt=n(Zce);fWo=r(Srt,"blenderbot"),Srt.forEach(t),gWo=r(iAe," \u2014 "),uN=s(iAe,"A",{href:!0});var Prt=n(uN);hWo=r(Prt,"TFBlenderbotModel"),Prt.forEach(t),uWo=r(iAe," (Blenderbot model)"),iAe.forEach(t),pWo=i(x),X1=s(x,"LI",{});var dAe=n(X1);eme=s(dAe,"STRONG",{});var $rt=n(eme);_Wo=r($rt,"blenderbot-small"),$rt.forEach(t),bWo=r(dAe," \u2014 "),pN=s(dAe,"A",{href:!0});var Irt=n(pN);vWo=r(Irt,"TFBlenderbotSmallModel"),Irt.forEach(t),TWo=r(dAe," (BlenderbotSmall model)"),dAe.forEach(t),FWo=i(x),V1=s(x,"LI",{});var cAe=n(V1);ome=s(cAe,"STRONG",{});var Drt=n(ome);CWo=r(Drt,"camembert"),Drt.forEach(t),MWo=r(cAe," \u2014 "),_N=s(cAe,"A",{href:!0});var jrt=n(_N);EWo=r(jrt,"TFCamembertModel"),jrt.forEach(t),yWo=r(cAe," (CamemBERT model)"),cAe.forEach(t),wWo=i(x),z1=s(x,"LI",{});var mAe=n(z1);rme=s(mAe,"STRONG",{});var Nrt=n(rme);AWo=r(Nrt,"clip"),Nrt.forEach(t),LWo=r(mAe," \u2014 "),bN=s(mAe,"A",{href:!0});var qrt=n(bN);BWo=r(qrt,"TFCLIPModel"),qrt.forEach(t),xWo=r(mAe," (CLIP model)"),mAe.forEach(t),kWo=i(x),W1=s(x,"LI",{});var fAe=n(W1);tme=s(fAe,"STRONG",{});var Grt=n(tme);RWo=r(Grt,"convbert"),Grt.forEach(t),SWo=r(fAe," \u2014 "),vN=s(fAe,"A",{href:!0});var Ort=n(vN);PWo=r(Ort,"TFConvBertModel"),Ort.forEach(t),$Wo=r(fAe," (ConvBERT model)"),fAe.forEach(t),IWo=i(x),Q1=s(x,"LI",{});var gAe=n(Q1);ame=s(gAe,"STRONG",{});var Xrt=n(ame);DWo=r(Xrt,"convnext"),Xrt.forEach(t),jWo=r(gAe," \u2014 "),TN=s(gAe,"A",{href:!0});var Vrt=n(TN);NWo=r(Vrt,"TFConvNextModel"),Vrt.forEach(t),qWo=r(gAe," (ConvNext model)"),gAe.forEach(t),GWo=i(x),H1=s(x,"LI",{});var hAe=n(H1);sme=s(hAe,"STRONG",{});var zrt=n(sme);OWo=r(zrt,"ctrl"),zrt.forEach(t),XWo=r(hAe," \u2014 "),FN=s(hAe,"A",{href:!0});var Wrt=n(FN);VWo=r(Wrt,"TFCTRLModel"),Wrt.forEach(t),zWo=r(hAe," (CTRL model)"),hAe.forEach(t),WWo=i(x),U1=s(x,"LI",{});var uAe=n(U1);nme=s(uAe,"STRONG",{});var Qrt=n(nme);QWo=r(Qrt,"deberta"),Qrt.forEach(t),HWo=r(uAe," \u2014 "),CN=s(uAe,"A",{href:!0});var Hrt=n(CN);UWo=r(Hrt,"TFDebertaModel"),Hrt.forEach(t),JWo=r(uAe," (DeBERTa model)"),uAe.forEach(t),YWo=i(x),J1=s(x,"LI",{});var pAe=n(J1);lme=s(pAe,"STRONG",{});var Urt=n(lme);KWo=r(Urt,"deberta-v2"),Urt.forEach(t),ZWo=r(pAe," \u2014 "),MN=s(pAe,"A",{href:!0});var Jrt=n(MN);eQo=r(Jrt,"TFDebertaV2Model"),Jrt.forEach(t),oQo=r(pAe," (DeBERTa-v2 model)"),pAe.forEach(t),rQo=i(x),Y1=s(x,"LI",{});var _Ae=n(Y1);ime=s(_Ae,"STRONG",{});var Yrt=n(ime);tQo=r(Yrt,"distilbert"),Yrt.forEach(t),aQo=r(_Ae," \u2014 "),EN=s(_Ae,"A",{href:!0});var Krt=n(EN);sQo=r(Krt,"TFDistilBertModel"),Krt.forEach(t),nQo=r(_Ae," (DistilBERT model)"),_Ae.forEach(t),lQo=i(x),K1=s(x,"LI",{});var bAe=n(K1);dme=s(bAe,"STRONG",{});var Zrt=n(dme);iQo=r(Zrt,"dpr"),Zrt.forEach(t),dQo=r(bAe," \u2014 "),yN=s(bAe,"A",{href:!0});var ett=n(yN);cQo=r(ett,"TFDPRQuestionEncoder"),ett.forEach(t),mQo=r(bAe," (DPR model)"),bAe.forEach(t),fQo=i(x),Z1=s(x,"LI",{});var vAe=n(Z1);cme=s(vAe,"STRONG",{});var ott=n(cme);gQo=r(ott,"electra"),ott.forEach(t),hQo=r(vAe," \u2014 "),wN=s(vAe,"A",{href:!0});var rtt=n(wN);uQo=r(rtt,"TFElectraModel"),rtt.forEach(t),pQo=r(vAe," (ELECTRA model)"),vAe.forEach(t),_Qo=i(x),eF=s(x,"LI",{});var TAe=n(eF);mme=s(TAe,"STRONG",{});var ttt=n(mme);bQo=r(ttt,"flaubert"),ttt.forEach(t),vQo=r(TAe," \u2014 "),AN=s(TAe,"A",{href:!0});var att=n(AN);TQo=r(att,"TFFlaubertModel"),att.forEach(t),FQo=r(TAe," (FlauBERT model)"),TAe.forEach(t),CQo=i(x),$n=s(x,"LI",{});var a7=n($n);fme=s(a7,"STRONG",{});var stt=n(fme);MQo=r(stt,"funnel"),stt.forEach(t),EQo=r(a7," \u2014 "),LN=s(a7,"A",{href:!0});var ntt=n(LN);yQo=r(ntt,"TFFunnelModel"),ntt.forEach(t),wQo=r(a7," or "),BN=s(a7,"A",{href:!0});var ltt=n(BN);AQo=r(ltt,"TFFunnelBaseModel"),ltt.forEach(t),LQo=r(a7," (Funnel Transformer model)"),a7.forEach(t),BQo=i(x),oF=s(x,"LI",{});var FAe=n(oF);gme=s(FAe,"STRONG",{});var itt=n(gme);xQo=r(itt,"gpt2"),itt.forEach(t),kQo=r(FAe," \u2014 "),xN=s(FAe,"A",{href:!0});var dtt=n(xN);RQo=r(dtt,"TFGPT2Model"),dtt.forEach(t),SQo=r(FAe," (OpenAI GPT-2 model)"),FAe.forEach(t),PQo=i(x),rF=s(x,"LI",{});var CAe=n(rF);hme=s(CAe,"STRONG",{});var ctt=n(hme);$Qo=r(ctt,"hubert"),ctt.forEach(t),IQo=r(CAe," \u2014 "),kN=s(CAe,"A",{href:!0});var mtt=n(kN);DQo=r(mtt,"TFHubertModel"),mtt.forEach(t),jQo=r(CAe," (Hubert model)"),CAe.forEach(t),NQo=i(x),tF=s(x,"LI",{});var MAe=n(tF);ume=s(MAe,"STRONG",{});var ftt=n(ume);qQo=r(ftt,"layoutlm"),ftt.forEach(t),GQo=r(MAe," \u2014 "),RN=s(MAe,"A",{href:!0});var gtt=n(RN);OQo=r(gtt,"TFLayoutLMModel"),gtt.forEach(t),XQo=r(MAe," (LayoutLM model)"),MAe.forEach(t),VQo=i(x),aF=s(x,"LI",{});var EAe=n(aF);pme=s(EAe,"STRONG",{});var htt=n(pme);zQo=r(htt,"led"),htt.forEach(t),WQo=r(EAe," \u2014 "),SN=s(EAe,"A",{href:!0});var utt=n(SN);QQo=r(utt,"TFLEDModel"),utt.forEach(t),HQo=r(EAe," (LED model)"),EAe.forEach(t),UQo=i(x),sF=s(x,"LI",{});var yAe=n(sF);_me=s(yAe,"STRONG",{});var ptt=n(_me);JQo=r(ptt,"longformer"),ptt.forEach(t),YQo=r(yAe," \u2014 "),PN=s(yAe,"A",{href:!0});var _tt=n(PN);KQo=r(_tt,"TFLongformerModel"),_tt.forEach(t),ZQo=r(yAe," (Longformer model)"),yAe.forEach(t),eHo=i(x),nF=s(x,"LI",{});var wAe=n(nF);bme=s(wAe,"STRONG",{});var btt=n(bme);oHo=r(btt,"lxmert"),btt.forEach(t),rHo=r(wAe," \u2014 "),$N=s(wAe,"A",{href:!0});var vtt=n($N);tHo=r(vtt,"TFLxmertModel"),vtt.forEach(t),aHo=r(wAe," (LXMERT model)"),wAe.forEach(t),sHo=i(x),lF=s(x,"LI",{});var AAe=n(lF);vme=s(AAe,"STRONG",{});var Ttt=n(vme);nHo=r(Ttt,"marian"),Ttt.forEach(t),lHo=r(AAe," \u2014 "),IN=s(AAe,"A",{href:!0});var Ftt=n(IN);iHo=r(Ftt,"TFMarianModel"),Ftt.forEach(t),dHo=r(AAe," (Marian model)"),AAe.forEach(t),cHo=i(x),iF=s(x,"LI",{});var LAe=n(iF);Tme=s(LAe,"STRONG",{});var Ctt=n(Tme);mHo=r(Ctt,"mbart"),Ctt.forEach(t),fHo=r(LAe," \u2014 "),DN=s(LAe,"A",{href:!0});var Mtt=n(DN);gHo=r(Mtt,"TFMBartModel"),Mtt.forEach(t),hHo=r(LAe," (mBART model)"),LAe.forEach(t),uHo=i(x),dF=s(x,"LI",{});var BAe=n(dF);Fme=s(BAe,"STRONG",{});var Ett=n(Fme);pHo=r(Ett,"mobilebert"),Ett.forEach(t),_Ho=r(BAe," \u2014 "),jN=s(BAe,"A",{href:!0});var ytt=n(jN);bHo=r(ytt,"TFMobileBertModel"),ytt.forEach(t),vHo=r(BAe," (MobileBERT model)"),BAe.forEach(t),THo=i(x),cF=s(x,"LI",{});var xAe=n(cF);Cme=s(xAe,"STRONG",{});var wtt=n(Cme);FHo=r(wtt,"mpnet"),wtt.forEach(t),CHo=r(xAe," \u2014 "),NN=s(xAe,"A",{href:!0});var Att=n(NN);MHo=r(Att,"TFMPNetModel"),Att.forEach(t),EHo=r(xAe," (MPNet model)"),xAe.forEach(t),yHo=i(x),mF=s(x,"LI",{});var kAe=n(mF);Mme=s(kAe,"STRONG",{});var Ltt=n(Mme);wHo=r(Ltt,"mt5"),Ltt.forEach(t),AHo=r(kAe," \u2014 "),qN=s(kAe,"A",{href:!0});var Btt=n(qN);LHo=r(Btt,"TFMT5Model"),Btt.forEach(t),BHo=r(kAe," (mT5 model)"),kAe.forEach(t),xHo=i(x),fF=s(x,"LI",{});var RAe=n(fF);Eme=s(RAe,"STRONG",{});var xtt=n(Eme);kHo=r(xtt,"openai-gpt"),xtt.forEach(t),RHo=r(RAe," \u2014 "),GN=s(RAe,"A",{href:!0});var ktt=n(GN);SHo=r(ktt,"TFOpenAIGPTModel"),ktt.forEach(t),PHo=r(RAe," (OpenAI GPT model)"),RAe.forEach(t),$Ho=i(x),gF=s(x,"LI",{});var SAe=n(gF);yme=s(SAe,"STRONG",{});var Rtt=n(yme);IHo=r(Rtt,"pegasus"),Rtt.forEach(t),DHo=r(SAe," \u2014 "),ON=s(SAe,"A",{href:!0});var Stt=n(ON);jHo=r(Stt,"TFPegasusModel"),Stt.forEach(t),NHo=r(SAe," (Pegasus model)"),SAe.forEach(t),qHo=i(x),hF=s(x,"LI",{});var PAe=n(hF);wme=s(PAe,"STRONG",{});var Ptt=n(wme);GHo=r(Ptt,"rembert"),Ptt.forEach(t),OHo=r(PAe," \u2014 "),XN=s(PAe,"A",{href:!0});var $tt=n(XN);XHo=r($tt,"TFRemBertModel"),$tt.forEach(t),VHo=r(PAe," (RemBERT model)"),PAe.forEach(t),zHo=i(x),uF=s(x,"LI",{});var $Ae=n(uF);Ame=s($Ae,"STRONG",{});var Itt=n(Ame);WHo=r(Itt,"roberta"),Itt.forEach(t),QHo=r($Ae," \u2014 "),VN=s($Ae,"A",{href:!0});var Dtt=n(VN);HHo=r(Dtt,"TFRobertaModel"),Dtt.forEach(t),UHo=r($Ae," (RoBERTa model)"),$Ae.forEach(t),JHo=i(x),pF=s(x,"LI",{});var IAe=n(pF);Lme=s(IAe,"STRONG",{});var jtt=n(Lme);YHo=r(jtt,"roformer"),jtt.forEach(t),KHo=r(IAe," \u2014 "),zN=s(IAe,"A",{href:!0});var Ntt=n(zN);ZHo=r(Ntt,"TFRoFormerModel"),Ntt.forEach(t),eUo=r(IAe," (RoFormer model)"),IAe.forEach(t),oUo=i(x),_F=s(x,"LI",{});var DAe=n(_F);Bme=s(DAe,"STRONG",{});var qtt=n(Bme);rUo=r(qtt,"speech_to_text"),qtt.forEach(t),tUo=r(DAe," \u2014 "),WN=s(DAe,"A",{href:!0});var Gtt=n(WN);aUo=r(Gtt,"TFSpeech2TextModel"),Gtt.forEach(t),sUo=r(DAe," (Speech2Text model)"),DAe.forEach(t),nUo=i(x),bF=s(x,"LI",{});var jAe=n(bF);xme=s(jAe,"STRONG",{});var Ott=n(xme);lUo=r(Ott,"t5"),Ott.forEach(t),iUo=r(jAe," \u2014 "),QN=s(jAe,"A",{href:!0});var Xtt=n(QN);dUo=r(Xtt,"TFT5Model"),Xtt.forEach(t),cUo=r(jAe," (T5 model)"),jAe.forEach(t),mUo=i(x),vF=s(x,"LI",{});var NAe=n(vF);kme=s(NAe,"STRONG",{});var Vtt=n(kme);fUo=r(Vtt,"tapas"),Vtt.forEach(t),gUo=r(NAe," \u2014 "),HN=s(NAe,"A",{href:!0});var ztt=n(HN);hUo=r(ztt,"TFTapasModel"),ztt.forEach(t),uUo=r(NAe," (TAPAS model)"),NAe.forEach(t),pUo=i(x),TF=s(x,"LI",{});var qAe=n(TF);Rme=s(qAe,"STRONG",{});var Wtt=n(Rme);_Uo=r(Wtt,"transfo-xl"),Wtt.forEach(t),bUo=r(qAe," \u2014 "),UN=s(qAe,"A",{href:!0});var Qtt=n(UN);vUo=r(Qtt,"TFTransfoXLModel"),Qtt.forEach(t),TUo=r(qAe," (Transformer-XL model)"),qAe.forEach(t),FUo=i(x),FF=s(x,"LI",{});var GAe=n(FF);Sme=s(GAe,"STRONG",{});var Htt=n(Sme);CUo=r(Htt,"vit"),Htt.forEach(t),MUo=r(GAe," \u2014 "),JN=s(GAe,"A",{href:!0});var Utt=n(JN);EUo=r(Utt,"TFViTModel"),Utt.forEach(t),yUo=r(GAe," (ViT model)"),GAe.forEach(t),wUo=i(x),CF=s(x,"LI",{});var OAe=n(CF);Pme=s(OAe,"STRONG",{});var Jtt=n(Pme);AUo=r(Jtt,"wav2vec2"),Jtt.forEach(t),LUo=r(OAe," \u2014 "),YN=s(OAe,"A",{href:!0});var Ytt=n(YN);BUo=r(Ytt,"TFWav2Vec2Model"),Ytt.forEach(t),xUo=r(OAe," (Wav2Vec2 model)"),OAe.forEach(t),kUo=i(x),MF=s(x,"LI",{});var XAe=n(MF);$me=s(XAe,"STRONG",{});var Ktt=n($me);RUo=r(Ktt,"xlm"),Ktt.forEach(t),SUo=r(XAe," \u2014 "),KN=s(XAe,"A",{href:!0});var Ztt=n(KN);PUo=r(Ztt,"TFXLMModel"),Ztt.forEach(t),$Uo=r(XAe," (XLM model)"),XAe.forEach(t),IUo=i(x),EF=s(x,"LI",{});var VAe=n(EF);Ime=s(VAe,"STRONG",{});var eat=n(Ime);DUo=r(eat,"xlm-roberta"),eat.forEach(t),jUo=r(VAe," \u2014 "),ZN=s(VAe,"A",{href:!0});var oat=n(ZN);NUo=r(oat,"TFXLMRobertaModel"),oat.forEach(t),qUo=r(VAe," (XLM-RoBERTa model)"),VAe.forEach(t),GUo=i(x),yF=s(x,"LI",{});var zAe=n(yF);Dme=s(zAe,"STRONG",{});var rat=n(Dme);OUo=r(rat,"xlnet"),rat.forEach(t),XUo=r(zAe," \u2014 "),eq=s(zAe,"A",{href:!0});var tat=n(eq);VUo=r(tat,"TFXLNetModel"),tat.forEach(t),zUo=r(zAe," (XLNet model)"),zAe.forEach(t),x.forEach(t),WUo=i(ca),jme=s(ca,"P",{});var aat=n(jme);QUo=r(aat,"Examples:"),aat.forEach(t),HUo=i(ca),f(eA.$$.fragment,ca),ca.forEach(t),kl.forEach(t),CBe=i(c),ac=s(c,"H2",{class:!0});var kke=n(ac);wF=s(kke,"A",{id:!0,class:!0,href:!0});var sat=n(wF);Nme=s(sat,"SPAN",{});var nat=n(Nme);f(oA.$$.fragment,nat),nat.forEach(t),sat.forEach(t),UUo=i(kke),qme=s(kke,"SPAN",{});var lat=n(qme);JUo=r(lat,"TFAutoModelForPreTraining"),lat.forEach(t),kke.forEach(t),MBe=i(c),pr=s(c,"DIV",{class:!0});var Sl=n(pr);f(rA.$$.fragment,Sl),YUo=i(Sl),sc=s(Sl,"P",{});var Cz=n(sc);KUo=r(Cz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),Gme=s(Cz,"CODE",{});var iat=n(Gme);ZUo=r(iat,"from_pretrained()"),iat.forEach(t),eJo=r(Cz,"class method or the "),Ome=s(Cz,"CODE",{});var dat=n(Ome);oJo=r(dat,"from_config()"),dat.forEach(t),rJo=r(Cz,`class
method.`),Cz.forEach(t),tJo=i(Sl),tA=s(Sl,"P",{});var Rke=n(tA);aJo=r(Rke,"This class cannot be instantiated directly using "),Xme=s(Rke,"CODE",{});var cat=n(Xme);sJo=r(cat,"__init__()"),cat.forEach(t),nJo=r(Rke," (throws an error)."),Rke.forEach(t),lJo=i(Sl),dt=s(Sl,"DIV",{class:!0});var Pl=n(dt);f(aA.$$.fragment,Pl),iJo=i(Pl),Vme=s(Pl,"P",{});var mat=n(Vme);dJo=r(mat,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),mat.forEach(t),cJo=i(Pl),nc=s(Pl,"P",{});var Mz=n(nc);mJo=r(Mz,`Note:
Loading a model from its configuration file does `),zme=s(Mz,"STRONG",{});var fat=n(zme);fJo=r(fat,"not"),fat.forEach(t),gJo=r(Mz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Wme=s(Mz,"CODE",{});var gat=n(Wme);hJo=r(gat,"from_pretrained()"),gat.forEach(t),uJo=r(Mz,"to load the model weights."),Mz.forEach(t),pJo=i(Pl),Qme=s(Pl,"P",{});var hat=n(Qme);_Jo=r(hat,"Examples:"),hat.forEach(t),bJo=i(Pl),f(sA.$$.fragment,Pl),Pl.forEach(t),vJo=i(Sl),ho=s(Sl,"DIV",{class:!0});var ma=n(ho);f(nA.$$.fragment,ma),TJo=i(ma),Hme=s(ma,"P",{});var uat=n(Hme);FJo=r(uat,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),uat.forEach(t),CJo=i(ma),cs=s(ma,"P",{});var U3=n(cs);MJo=r(U3,"The model class to instantiate is selected based on the "),Ume=s(U3,"CODE",{});var pat=n(Ume);EJo=r(pat,"model_type"),pat.forEach(t),yJo=r(U3,` property of the config object (either
passed as an argument or loaded from `),Jme=s(U3,"CODE",{});var _at=n(Jme);wJo=r(_at,"pretrained_model_name_or_path"),_at.forEach(t),AJo=r(U3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Yme=s(U3,"CODE",{});var bat=n(Yme);LJo=r(bat,"pretrained_model_name_or_path"),bat.forEach(t),BJo=r(U3,":"),U3.forEach(t),xJo=i(ma),H=s(ma,"UL",{});var U=n(H);AF=s(U,"LI",{});var WAe=n(AF);Kme=s(WAe,"STRONG",{});var vat=n(Kme);kJo=r(vat,"albert"),vat.forEach(t),RJo=r(WAe," \u2014 "),oq=s(WAe,"A",{href:!0});var Tat=n(oq);SJo=r(Tat,"TFAlbertForPreTraining"),Tat.forEach(t),PJo=r(WAe," (ALBERT model)"),WAe.forEach(t),$Jo=i(U),LF=s(U,"LI",{});var QAe=n(LF);Zme=s(QAe,"STRONG",{});var Fat=n(Zme);IJo=r(Fat,"bart"),Fat.forEach(t),DJo=r(QAe," \u2014 "),rq=s(QAe,"A",{href:!0});var Cat=n(rq);jJo=r(Cat,"TFBartForConditionalGeneration"),Cat.forEach(t),NJo=r(QAe," (BART model)"),QAe.forEach(t),qJo=i(U),BF=s(U,"LI",{});var HAe=n(BF);efe=s(HAe,"STRONG",{});var Mat=n(efe);GJo=r(Mat,"bert"),Mat.forEach(t),OJo=r(HAe," \u2014 "),tq=s(HAe,"A",{href:!0});var Eat=n(tq);XJo=r(Eat,"TFBertForPreTraining"),Eat.forEach(t),VJo=r(HAe," (BERT model)"),HAe.forEach(t),zJo=i(U),xF=s(U,"LI",{});var UAe=n(xF);ofe=s(UAe,"STRONG",{});var yat=n(ofe);WJo=r(yat,"camembert"),yat.forEach(t),QJo=r(UAe," \u2014 "),aq=s(UAe,"A",{href:!0});var wat=n(aq);HJo=r(wat,"TFCamembertForMaskedLM"),wat.forEach(t),UJo=r(UAe," (CamemBERT model)"),UAe.forEach(t),JJo=i(U),kF=s(U,"LI",{});var JAe=n(kF);rfe=s(JAe,"STRONG",{});var Aat=n(rfe);YJo=r(Aat,"ctrl"),Aat.forEach(t),KJo=r(JAe," \u2014 "),sq=s(JAe,"A",{href:!0});var Lat=n(sq);ZJo=r(Lat,"TFCTRLLMHeadModel"),Lat.forEach(t),eYo=r(JAe," (CTRL model)"),JAe.forEach(t),oYo=i(U),RF=s(U,"LI",{});var YAe=n(RF);tfe=s(YAe,"STRONG",{});var Bat=n(tfe);rYo=r(Bat,"distilbert"),Bat.forEach(t),tYo=r(YAe," \u2014 "),nq=s(YAe,"A",{href:!0});var xat=n(nq);aYo=r(xat,"TFDistilBertForMaskedLM"),xat.forEach(t),sYo=r(YAe," (DistilBERT model)"),YAe.forEach(t),nYo=i(U),SF=s(U,"LI",{});var KAe=n(SF);afe=s(KAe,"STRONG",{});var kat=n(afe);lYo=r(kat,"electra"),kat.forEach(t),iYo=r(KAe," \u2014 "),lq=s(KAe,"A",{href:!0});var Rat=n(lq);dYo=r(Rat,"TFElectraForPreTraining"),Rat.forEach(t),cYo=r(KAe," (ELECTRA model)"),KAe.forEach(t),mYo=i(U),PF=s(U,"LI",{});var ZAe=n(PF);sfe=s(ZAe,"STRONG",{});var Sat=n(sfe);fYo=r(Sat,"flaubert"),Sat.forEach(t),gYo=r(ZAe," \u2014 "),iq=s(ZAe,"A",{href:!0});var Pat=n(iq);hYo=r(Pat,"TFFlaubertWithLMHeadModel"),Pat.forEach(t),uYo=r(ZAe," (FlauBERT model)"),ZAe.forEach(t),pYo=i(U),$F=s(U,"LI",{});var e0e=n($F);nfe=s(e0e,"STRONG",{});var $at=n(nfe);_Yo=r($at,"funnel"),$at.forEach(t),bYo=r(e0e," \u2014 "),dq=s(e0e,"A",{href:!0});var Iat=n(dq);vYo=r(Iat,"TFFunnelForPreTraining"),Iat.forEach(t),TYo=r(e0e," (Funnel Transformer model)"),e0e.forEach(t),FYo=i(U),IF=s(U,"LI",{});var o0e=n(IF);lfe=s(o0e,"STRONG",{});var Dat=n(lfe);CYo=r(Dat,"gpt2"),Dat.forEach(t),MYo=r(o0e," \u2014 "),cq=s(o0e,"A",{href:!0});var jat=n(cq);EYo=r(jat,"TFGPT2LMHeadModel"),jat.forEach(t),yYo=r(o0e," (OpenAI GPT-2 model)"),o0e.forEach(t),wYo=i(U),DF=s(U,"LI",{});var r0e=n(DF);ife=s(r0e,"STRONG",{});var Nat=n(ife);AYo=r(Nat,"layoutlm"),Nat.forEach(t),LYo=r(r0e," \u2014 "),mq=s(r0e,"A",{href:!0});var qat=n(mq);BYo=r(qat,"TFLayoutLMForMaskedLM"),qat.forEach(t),xYo=r(r0e," (LayoutLM model)"),r0e.forEach(t),kYo=i(U),jF=s(U,"LI",{});var t0e=n(jF);dfe=s(t0e,"STRONG",{});var Gat=n(dfe);RYo=r(Gat,"lxmert"),Gat.forEach(t),SYo=r(t0e," \u2014 "),fq=s(t0e,"A",{href:!0});var Oat=n(fq);PYo=r(Oat,"TFLxmertForPreTraining"),Oat.forEach(t),$Yo=r(t0e," (LXMERT model)"),t0e.forEach(t),IYo=i(U),NF=s(U,"LI",{});var a0e=n(NF);cfe=s(a0e,"STRONG",{});var Xat=n(cfe);DYo=r(Xat,"mobilebert"),Xat.forEach(t),jYo=r(a0e," \u2014 "),gq=s(a0e,"A",{href:!0});var Vat=n(gq);NYo=r(Vat,"TFMobileBertForPreTraining"),Vat.forEach(t),qYo=r(a0e," (MobileBERT model)"),a0e.forEach(t),GYo=i(U),qF=s(U,"LI",{});var s0e=n(qF);mfe=s(s0e,"STRONG",{});var zat=n(mfe);OYo=r(zat,"mpnet"),zat.forEach(t),XYo=r(s0e," \u2014 "),hq=s(s0e,"A",{href:!0});var Wat=n(hq);VYo=r(Wat,"TFMPNetForMaskedLM"),Wat.forEach(t),zYo=r(s0e," (MPNet model)"),s0e.forEach(t),WYo=i(U),GF=s(U,"LI",{});var n0e=n(GF);ffe=s(n0e,"STRONG",{});var Qat=n(ffe);QYo=r(Qat,"openai-gpt"),Qat.forEach(t),HYo=r(n0e," \u2014 "),uq=s(n0e,"A",{href:!0});var Hat=n(uq);UYo=r(Hat,"TFOpenAIGPTLMHeadModel"),Hat.forEach(t),JYo=r(n0e," (OpenAI GPT model)"),n0e.forEach(t),YYo=i(U),OF=s(U,"LI",{});var l0e=n(OF);gfe=s(l0e,"STRONG",{});var Uat=n(gfe);KYo=r(Uat,"roberta"),Uat.forEach(t),ZYo=r(l0e," \u2014 "),pq=s(l0e,"A",{href:!0});var Jat=n(pq);eKo=r(Jat,"TFRobertaForMaskedLM"),Jat.forEach(t),oKo=r(l0e," (RoBERTa model)"),l0e.forEach(t),rKo=i(U),XF=s(U,"LI",{});var i0e=n(XF);hfe=s(i0e,"STRONG",{});var Yat=n(hfe);tKo=r(Yat,"t5"),Yat.forEach(t),aKo=r(i0e," \u2014 "),_q=s(i0e,"A",{href:!0});var Kat=n(_q);sKo=r(Kat,"TFT5ForConditionalGeneration"),Kat.forEach(t),nKo=r(i0e," (T5 model)"),i0e.forEach(t),lKo=i(U),VF=s(U,"LI",{});var d0e=n(VF);ufe=s(d0e,"STRONG",{});var Zat=n(ufe);iKo=r(Zat,"tapas"),Zat.forEach(t),dKo=r(d0e," \u2014 "),bq=s(d0e,"A",{href:!0});var est=n(bq);cKo=r(est,"TFTapasForMaskedLM"),est.forEach(t),mKo=r(d0e," (TAPAS model)"),d0e.forEach(t),fKo=i(U),zF=s(U,"LI",{});var c0e=n(zF);pfe=s(c0e,"STRONG",{});var ost=n(pfe);gKo=r(ost,"transfo-xl"),ost.forEach(t),hKo=r(c0e," \u2014 "),vq=s(c0e,"A",{href:!0});var rst=n(vq);uKo=r(rst,"TFTransfoXLLMHeadModel"),rst.forEach(t),pKo=r(c0e," (Transformer-XL model)"),c0e.forEach(t),_Ko=i(U),WF=s(U,"LI",{});var m0e=n(WF);_fe=s(m0e,"STRONG",{});var tst=n(_fe);bKo=r(tst,"xlm"),tst.forEach(t),vKo=r(m0e," \u2014 "),Tq=s(m0e,"A",{href:!0});var ast=n(Tq);TKo=r(ast,"TFXLMWithLMHeadModel"),ast.forEach(t),FKo=r(m0e," (XLM model)"),m0e.forEach(t),CKo=i(U),QF=s(U,"LI",{});var f0e=n(QF);bfe=s(f0e,"STRONG",{});var sst=n(bfe);MKo=r(sst,"xlm-roberta"),sst.forEach(t),EKo=r(f0e," \u2014 "),Fq=s(f0e,"A",{href:!0});var nst=n(Fq);yKo=r(nst,"TFXLMRobertaForMaskedLM"),nst.forEach(t),wKo=r(f0e," (XLM-RoBERTa model)"),f0e.forEach(t),AKo=i(U),HF=s(U,"LI",{});var g0e=n(HF);vfe=s(g0e,"STRONG",{});var lst=n(vfe);LKo=r(lst,"xlnet"),lst.forEach(t),BKo=r(g0e," \u2014 "),Cq=s(g0e,"A",{href:!0});var ist=n(Cq);xKo=r(ist,"TFXLNetLMHeadModel"),ist.forEach(t),kKo=r(g0e," (XLNet model)"),g0e.forEach(t),U.forEach(t),RKo=i(ma),Tfe=s(ma,"P",{});var dst=n(Tfe);SKo=r(dst,"Examples:"),dst.forEach(t),PKo=i(ma),f(lA.$$.fragment,ma),ma.forEach(t),Sl.forEach(t),EBe=i(c),lc=s(c,"H2",{class:!0});var Ske=n(lc);UF=s(Ske,"A",{id:!0,class:!0,href:!0});var cst=n(UF);Ffe=s(cst,"SPAN",{});var mst=n(Ffe);f(iA.$$.fragment,mst),mst.forEach(t),cst.forEach(t),$Ko=i(Ske),Cfe=s(Ske,"SPAN",{});var fst=n(Cfe);IKo=r(fst,"TFAutoModelForCausalLM"),fst.forEach(t),Ske.forEach(t),yBe=i(c),_r=s(c,"DIV",{class:!0});var $l=n(_r);f(dA.$$.fragment,$l),DKo=i($l),ic=s($l,"P",{});var Ez=n(ic);jKo=r(Ez,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Mfe=s(Ez,"CODE",{});var gst=n(Mfe);NKo=r(gst,"from_pretrained()"),gst.forEach(t),qKo=r(Ez,"class method or the "),Efe=s(Ez,"CODE",{});var hst=n(Efe);GKo=r(hst,"from_config()"),hst.forEach(t),OKo=r(Ez,`class
method.`),Ez.forEach(t),XKo=i($l),cA=s($l,"P",{});var Pke=n(cA);VKo=r(Pke,"This class cannot be instantiated directly using "),yfe=s(Pke,"CODE",{});var ust=n(yfe);zKo=r(ust,"__init__()"),ust.forEach(t),WKo=r(Pke," (throws an error)."),Pke.forEach(t),QKo=i($l),ct=s($l,"DIV",{class:!0});var Il=n(ct);f(mA.$$.fragment,Il),HKo=i(Il),wfe=s(Il,"P",{});var pst=n(wfe);UKo=r(pst,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),pst.forEach(t),JKo=i(Il),dc=s(Il,"P",{});var yz=n(dc);YKo=r(yz,`Note:
Loading a model from its configuration file does `),Afe=s(yz,"STRONG",{});var _st=n(Afe);KKo=r(_st,"not"),_st.forEach(t),ZKo=r(yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Lfe=s(yz,"CODE",{});var bst=n(Lfe);eZo=r(bst,"from_pretrained()"),bst.forEach(t),oZo=r(yz,"to load the model weights."),yz.forEach(t),rZo=i(Il),Bfe=s(Il,"P",{});var vst=n(Bfe);tZo=r(vst,"Examples:"),vst.forEach(t),aZo=i(Il),f(fA.$$.fragment,Il),Il.forEach(t),sZo=i($l),uo=s($l,"DIV",{class:!0});var fa=n(uo);f(gA.$$.fragment,fa),nZo=i(fa),xfe=s(fa,"P",{});var Tst=n(xfe);lZo=r(Tst,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),Tst.forEach(t),iZo=i(fa),ms=s(fa,"P",{});var J3=n(ms);dZo=r(J3,"The model class to instantiate is selected based on the "),kfe=s(J3,"CODE",{});var Fst=n(kfe);cZo=r(Fst,"model_type"),Fst.forEach(t),mZo=r(J3,` property of the config object (either
passed as an argument or loaded from `),Rfe=s(J3,"CODE",{});var Cst=n(Rfe);fZo=r(Cst,"pretrained_model_name_or_path"),Cst.forEach(t),gZo=r(J3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Sfe=s(J3,"CODE",{});var Mst=n(Sfe);hZo=r(Mst,"pretrained_model_name_or_path"),Mst.forEach(t),uZo=r(J3,":"),J3.forEach(t),pZo=i(fa),he=s(fa,"UL",{});var Me=n(he);JF=s(Me,"LI",{});var h0e=n(JF);Pfe=s(h0e,"STRONG",{});var Est=n(Pfe);_Zo=r(Est,"bert"),Est.forEach(t),bZo=r(h0e," \u2014 "),Mq=s(h0e,"A",{href:!0});var yst=n(Mq);vZo=r(yst,"TFBertLMHeadModel"),yst.forEach(t),TZo=r(h0e," (BERT model)"),h0e.forEach(t),FZo=i(Me),YF=s(Me,"LI",{});var u0e=n(YF);$fe=s(u0e,"STRONG",{});var wst=n($fe);CZo=r(wst,"ctrl"),wst.forEach(t),MZo=r(u0e," \u2014 "),Eq=s(u0e,"A",{href:!0});var Ast=n(Eq);EZo=r(Ast,"TFCTRLLMHeadModel"),Ast.forEach(t),yZo=r(u0e," (CTRL model)"),u0e.forEach(t),wZo=i(Me),KF=s(Me,"LI",{});var p0e=n(KF);Ife=s(p0e,"STRONG",{});var Lst=n(Ife);AZo=r(Lst,"gpt2"),Lst.forEach(t),LZo=r(p0e," \u2014 "),yq=s(p0e,"A",{href:!0});var Bst=n(yq);BZo=r(Bst,"TFGPT2LMHeadModel"),Bst.forEach(t),xZo=r(p0e," (OpenAI GPT-2 model)"),p0e.forEach(t),kZo=i(Me),ZF=s(Me,"LI",{});var _0e=n(ZF);Dfe=s(_0e,"STRONG",{});var xst=n(Dfe);RZo=r(xst,"openai-gpt"),xst.forEach(t),SZo=r(_0e," \u2014 "),wq=s(_0e,"A",{href:!0});var kst=n(wq);PZo=r(kst,"TFOpenAIGPTLMHeadModel"),kst.forEach(t),$Zo=r(_0e," (OpenAI GPT model)"),_0e.forEach(t),IZo=i(Me),eC=s(Me,"LI",{});var b0e=n(eC);jfe=s(b0e,"STRONG",{});var Rst=n(jfe);DZo=r(Rst,"rembert"),Rst.forEach(t),jZo=r(b0e," \u2014 "),Aq=s(b0e,"A",{href:!0});var Sst=n(Aq);NZo=r(Sst,"TFRemBertForCausalLM"),Sst.forEach(t),qZo=r(b0e," (RemBERT model)"),b0e.forEach(t),GZo=i(Me),oC=s(Me,"LI",{});var v0e=n(oC);Nfe=s(v0e,"STRONG",{});var Pst=n(Nfe);OZo=r(Pst,"roberta"),Pst.forEach(t),XZo=r(v0e," \u2014 "),Lq=s(v0e,"A",{href:!0});var $st=n(Lq);VZo=r($st,"TFRobertaForCausalLM"),$st.forEach(t),zZo=r(v0e," (RoBERTa model)"),v0e.forEach(t),WZo=i(Me),rC=s(Me,"LI",{});var T0e=n(rC);qfe=s(T0e,"STRONG",{});var Ist=n(qfe);QZo=r(Ist,"roformer"),Ist.forEach(t),HZo=r(T0e," \u2014 "),Bq=s(T0e,"A",{href:!0});var Dst=n(Bq);UZo=r(Dst,"TFRoFormerForCausalLM"),Dst.forEach(t),JZo=r(T0e," (RoFormer model)"),T0e.forEach(t),YZo=i(Me),tC=s(Me,"LI",{});var F0e=n(tC);Gfe=s(F0e,"STRONG",{});var jst=n(Gfe);KZo=r(jst,"transfo-xl"),jst.forEach(t),ZZo=r(F0e," \u2014 "),xq=s(F0e,"A",{href:!0});var Nst=n(xq);eer=r(Nst,"TFTransfoXLLMHeadModel"),Nst.forEach(t),oer=r(F0e," (Transformer-XL model)"),F0e.forEach(t),rer=i(Me),aC=s(Me,"LI",{});var C0e=n(aC);Ofe=s(C0e,"STRONG",{});var qst=n(Ofe);ter=r(qst,"xlm"),qst.forEach(t),aer=r(C0e," \u2014 "),kq=s(C0e,"A",{href:!0});var Gst=n(kq);ser=r(Gst,"TFXLMWithLMHeadModel"),Gst.forEach(t),ner=r(C0e," (XLM model)"),C0e.forEach(t),ler=i(Me),sC=s(Me,"LI",{});var M0e=n(sC);Xfe=s(M0e,"STRONG",{});var Ost=n(Xfe);ier=r(Ost,"xlnet"),Ost.forEach(t),der=r(M0e," \u2014 "),Rq=s(M0e,"A",{href:!0});var Xst=n(Rq);cer=r(Xst,"TFXLNetLMHeadModel"),Xst.forEach(t),mer=r(M0e," (XLNet model)"),M0e.forEach(t),Me.forEach(t),fer=i(fa),Vfe=s(fa,"P",{});var Vst=n(Vfe);ger=r(Vst,"Examples:"),Vst.forEach(t),her=i(fa),f(hA.$$.fragment,fa),fa.forEach(t),$l.forEach(t),wBe=i(c),cc=s(c,"H2",{class:!0});var $ke=n(cc);nC=s($ke,"A",{id:!0,class:!0,href:!0});var zst=n(nC);zfe=s(zst,"SPAN",{});var Wst=n(zfe);f(uA.$$.fragment,Wst),Wst.forEach(t),zst.forEach(t),uer=i($ke),Wfe=s($ke,"SPAN",{});var Qst=n(Wfe);per=r(Qst,"TFAutoModelForImageClassification"),Qst.forEach(t),$ke.forEach(t),ABe=i(c),br=s(c,"DIV",{class:!0});var Dl=n(br);f(pA.$$.fragment,Dl),_er=i(Dl),mc=s(Dl,"P",{});var wz=n(mc);ber=r(wz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),Qfe=s(wz,"CODE",{});var Hst=n(Qfe);ver=r(Hst,"from_pretrained()"),Hst.forEach(t),Ter=r(wz,"class method or the "),Hfe=s(wz,"CODE",{});var Ust=n(Hfe);Fer=r(Ust,"from_config()"),Ust.forEach(t),Cer=r(wz,`class
method.`),wz.forEach(t),Mer=i(Dl),_A=s(Dl,"P",{});var Ike=n(_A);Eer=r(Ike,"This class cannot be instantiated directly using "),Ufe=s(Ike,"CODE",{});var Jst=n(Ufe);yer=r(Jst,"__init__()"),Jst.forEach(t),wer=r(Ike," (throws an error)."),Ike.forEach(t),Aer=i(Dl),mt=s(Dl,"DIV",{class:!0});var jl=n(mt);f(bA.$$.fragment,jl),Ler=i(jl),Jfe=s(jl,"P",{});var Yst=n(Jfe);Ber=r(Yst,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),Yst.forEach(t),xer=i(jl),fc=s(jl,"P",{});var Az=n(fc);ker=r(Az,`Note:
Loading a model from its configuration file does `),Yfe=s(Az,"STRONG",{});var Kst=n(Yfe);Rer=r(Kst,"not"),Kst.forEach(t),Ser=r(Az,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kfe=s(Az,"CODE",{});var Zst=n(Kfe);Per=r(Zst,"from_pretrained()"),Zst.forEach(t),$er=r(Az,"to load the model weights."),Az.forEach(t),Ier=i(jl),Zfe=s(jl,"P",{});var ent=n(Zfe);Der=r(ent,"Examples:"),ent.forEach(t),jer=i(jl),f(vA.$$.fragment,jl),jl.forEach(t),Ner=i(Dl),po=s(Dl,"DIV",{class:!0});var ga=n(po);f(TA.$$.fragment,ga),qer=i(ga),ege=s(ga,"P",{});var ont=n(ege);Ger=r(ont,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),ont.forEach(t),Oer=i(ga),fs=s(ga,"P",{});var Y3=n(fs);Xer=r(Y3,"The model class to instantiate is selected based on the "),oge=s(Y3,"CODE",{});var rnt=n(oge);Ver=r(rnt,"model_type"),rnt.forEach(t),zer=r(Y3,` property of the config object (either
passed as an argument or loaded from `),rge=s(Y3,"CODE",{});var tnt=n(rge);Wer=r(tnt,"pretrained_model_name_or_path"),tnt.forEach(t),Qer=r(Y3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tge=s(Y3,"CODE",{});var ant=n(tge);Her=r(ant,"pretrained_model_name_or_path"),ant.forEach(t),Uer=r(Y3,":"),Y3.forEach(t),Jer=i(ga),FA=s(ga,"UL",{});var Dke=n(FA);lC=s(Dke,"LI",{});var E0e=n(lC);age=s(E0e,"STRONG",{});var snt=n(age);Yer=r(snt,"convnext"),snt.forEach(t),Ker=r(E0e," \u2014 "),Sq=s(E0e,"A",{href:!0});var nnt=n(Sq);Zer=r(nnt,"TFConvNextForImageClassification"),nnt.forEach(t),eor=r(E0e," (ConvNext model)"),E0e.forEach(t),oor=i(Dke),iC=s(Dke,"LI",{});var y0e=n(iC);sge=s(y0e,"STRONG",{});var lnt=n(sge);ror=r(lnt,"vit"),lnt.forEach(t),tor=r(y0e," \u2014 "),Pq=s(y0e,"A",{href:!0});var int=n(Pq);aor=r(int,"TFViTForImageClassification"),int.forEach(t),sor=r(y0e," (ViT model)"),y0e.forEach(t),Dke.forEach(t),nor=i(ga),nge=s(ga,"P",{});var dnt=n(nge);lor=r(dnt,"Examples:"),dnt.forEach(t),ior=i(ga),f(CA.$$.fragment,ga),ga.forEach(t),Dl.forEach(t),LBe=i(c),gc=s(c,"H2",{class:!0});var jke=n(gc);dC=s(jke,"A",{id:!0,class:!0,href:!0});var cnt=n(dC);lge=s(cnt,"SPAN",{});var mnt=n(lge);f(MA.$$.fragment,mnt),mnt.forEach(t),cnt.forEach(t),dor=i(jke),ige=s(jke,"SPAN",{});var fnt=n(ige);cor=r(fnt,"TFAutoModelForMaskedLM"),fnt.forEach(t),jke.forEach(t),BBe=i(c),vr=s(c,"DIV",{class:!0});var Nl=n(vr);f(EA.$$.fragment,Nl),mor=i(Nl),hc=s(Nl,"P",{});var Lz=n(hc);gor=r(Lz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),dge=s(Lz,"CODE",{});var gnt=n(dge);hor=r(gnt,"from_pretrained()"),gnt.forEach(t),uor=r(Lz,"class method or the "),cge=s(Lz,"CODE",{});var hnt=n(cge);por=r(hnt,"from_config()"),hnt.forEach(t),_or=r(Lz,`class
method.`),Lz.forEach(t),bor=i(Nl),yA=s(Nl,"P",{});var Nke=n(yA);vor=r(Nke,"This class cannot be instantiated directly using "),mge=s(Nke,"CODE",{});var unt=n(mge);Tor=r(unt,"__init__()"),unt.forEach(t),For=r(Nke," (throws an error)."),Nke.forEach(t),Cor=i(Nl),ft=s(Nl,"DIV",{class:!0});var ql=n(ft);f(wA.$$.fragment,ql),Mor=i(ql),fge=s(ql,"P",{});var pnt=n(fge);Eor=r(pnt,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),pnt.forEach(t),yor=i(ql),uc=s(ql,"P",{});var Bz=n(uc);wor=r(Bz,`Note:
Loading a model from its configuration file does `),gge=s(Bz,"STRONG",{});var _nt=n(gge);Aor=r(_nt,"not"),_nt.forEach(t),Lor=r(Bz,` load the model weights. It only affects the
model\u2019s configuration. Use `),hge=s(Bz,"CODE",{});var bnt=n(hge);Bor=r(bnt,"from_pretrained()"),bnt.forEach(t),xor=r(Bz,"to load the model weights."),Bz.forEach(t),kor=i(ql),uge=s(ql,"P",{});var vnt=n(uge);Ror=r(vnt,"Examples:"),vnt.forEach(t),Sor=i(ql),f(AA.$$.fragment,ql),ql.forEach(t),Por=i(Nl),_o=s(Nl,"DIV",{class:!0});var ha=n(_o);f(LA.$$.fragment,ha),$or=i(ha),pge=s(ha,"P",{});var Tnt=n(pge);Ior=r(Tnt,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Tnt.forEach(t),Dor=i(ha),gs=s(ha,"P",{});var K3=n(gs);jor=r(K3,"The model class to instantiate is selected based on the "),_ge=s(K3,"CODE",{});var Fnt=n(_ge);Nor=r(Fnt,"model_type"),Fnt.forEach(t),qor=r(K3,` property of the config object (either
passed as an argument or loaded from `),bge=s(K3,"CODE",{});var Cnt=n(bge);Gor=r(Cnt,"pretrained_model_name_or_path"),Cnt.forEach(t),Oor=r(K3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),vge=s(K3,"CODE",{});var Mnt=n(vge);Xor=r(Mnt,"pretrained_model_name_or_path"),Mnt.forEach(t),Vor=r(K3,":"),K3.forEach(t),zor=i(ha),Y=s(ha,"UL",{});var ee=n(Y);cC=s(ee,"LI",{});var w0e=n(cC);Tge=s(w0e,"STRONG",{});var Ent=n(Tge);Wor=r(Ent,"albert"),Ent.forEach(t),Qor=r(w0e," \u2014 "),$q=s(w0e,"A",{href:!0});var ynt=n($q);Hor=r(ynt,"TFAlbertForMaskedLM"),ynt.forEach(t),Uor=r(w0e," (ALBERT model)"),w0e.forEach(t),Jor=i(ee),mC=s(ee,"LI",{});var A0e=n(mC);Fge=s(A0e,"STRONG",{});var wnt=n(Fge);Yor=r(wnt,"bert"),wnt.forEach(t),Kor=r(A0e," \u2014 "),Iq=s(A0e,"A",{href:!0});var Ant=n(Iq);Zor=r(Ant,"TFBertForMaskedLM"),Ant.forEach(t),err=r(A0e," (BERT model)"),A0e.forEach(t),orr=i(ee),fC=s(ee,"LI",{});var L0e=n(fC);Cge=s(L0e,"STRONG",{});var Lnt=n(Cge);rrr=r(Lnt,"camembert"),Lnt.forEach(t),trr=r(L0e," \u2014 "),Dq=s(L0e,"A",{href:!0});var Bnt=n(Dq);arr=r(Bnt,"TFCamembertForMaskedLM"),Bnt.forEach(t),srr=r(L0e," (CamemBERT model)"),L0e.forEach(t),nrr=i(ee),gC=s(ee,"LI",{});var B0e=n(gC);Mge=s(B0e,"STRONG",{});var xnt=n(Mge);lrr=r(xnt,"convbert"),xnt.forEach(t),irr=r(B0e," \u2014 "),jq=s(B0e,"A",{href:!0});var knt=n(jq);drr=r(knt,"TFConvBertForMaskedLM"),knt.forEach(t),crr=r(B0e," (ConvBERT model)"),B0e.forEach(t),mrr=i(ee),hC=s(ee,"LI",{});var x0e=n(hC);Ege=s(x0e,"STRONG",{});var Rnt=n(Ege);frr=r(Rnt,"deberta"),Rnt.forEach(t),grr=r(x0e," \u2014 "),Nq=s(x0e,"A",{href:!0});var Snt=n(Nq);hrr=r(Snt,"TFDebertaForMaskedLM"),Snt.forEach(t),urr=r(x0e," (DeBERTa model)"),x0e.forEach(t),prr=i(ee),uC=s(ee,"LI",{});var k0e=n(uC);yge=s(k0e,"STRONG",{});var Pnt=n(yge);_rr=r(Pnt,"deberta-v2"),Pnt.forEach(t),brr=r(k0e," \u2014 "),qq=s(k0e,"A",{href:!0});var $nt=n(qq);vrr=r($nt,"TFDebertaV2ForMaskedLM"),$nt.forEach(t),Trr=r(k0e," (DeBERTa-v2 model)"),k0e.forEach(t),Frr=i(ee),pC=s(ee,"LI",{});var R0e=n(pC);wge=s(R0e,"STRONG",{});var Int=n(wge);Crr=r(Int,"distilbert"),Int.forEach(t),Mrr=r(R0e," \u2014 "),Gq=s(R0e,"A",{href:!0});var Dnt=n(Gq);Err=r(Dnt,"TFDistilBertForMaskedLM"),Dnt.forEach(t),yrr=r(R0e," (DistilBERT model)"),R0e.forEach(t),wrr=i(ee),_C=s(ee,"LI",{});var S0e=n(_C);Age=s(S0e,"STRONG",{});var jnt=n(Age);Arr=r(jnt,"electra"),jnt.forEach(t),Lrr=r(S0e," \u2014 "),Oq=s(S0e,"A",{href:!0});var Nnt=n(Oq);Brr=r(Nnt,"TFElectraForMaskedLM"),Nnt.forEach(t),xrr=r(S0e," (ELECTRA model)"),S0e.forEach(t),krr=i(ee),bC=s(ee,"LI",{});var P0e=n(bC);Lge=s(P0e,"STRONG",{});var qnt=n(Lge);Rrr=r(qnt,"flaubert"),qnt.forEach(t),Srr=r(P0e," \u2014 "),Xq=s(P0e,"A",{href:!0});var Gnt=n(Xq);Prr=r(Gnt,"TFFlaubertWithLMHeadModel"),Gnt.forEach(t),$rr=r(P0e," (FlauBERT model)"),P0e.forEach(t),Irr=i(ee),vC=s(ee,"LI",{});var $0e=n(vC);Bge=s($0e,"STRONG",{});var Ont=n(Bge);Drr=r(Ont,"funnel"),Ont.forEach(t),jrr=r($0e," \u2014 "),Vq=s($0e,"A",{href:!0});var Xnt=n(Vq);Nrr=r(Xnt,"TFFunnelForMaskedLM"),Xnt.forEach(t),qrr=r($0e," (Funnel Transformer model)"),$0e.forEach(t),Grr=i(ee),TC=s(ee,"LI",{});var I0e=n(TC);xge=s(I0e,"STRONG",{});var Vnt=n(xge);Orr=r(Vnt,"layoutlm"),Vnt.forEach(t),Xrr=r(I0e," \u2014 "),zq=s(I0e,"A",{href:!0});var znt=n(zq);Vrr=r(znt,"TFLayoutLMForMaskedLM"),znt.forEach(t),zrr=r(I0e," (LayoutLM model)"),I0e.forEach(t),Wrr=i(ee),FC=s(ee,"LI",{});var D0e=n(FC);kge=s(D0e,"STRONG",{});var Wnt=n(kge);Qrr=r(Wnt,"longformer"),Wnt.forEach(t),Hrr=r(D0e," \u2014 "),Wq=s(D0e,"A",{href:!0});var Qnt=n(Wq);Urr=r(Qnt,"TFLongformerForMaskedLM"),Qnt.forEach(t),Jrr=r(D0e," (Longformer model)"),D0e.forEach(t),Yrr=i(ee),CC=s(ee,"LI",{});var j0e=n(CC);Rge=s(j0e,"STRONG",{});var Hnt=n(Rge);Krr=r(Hnt,"mobilebert"),Hnt.forEach(t),Zrr=r(j0e," \u2014 "),Qq=s(j0e,"A",{href:!0});var Unt=n(Qq);etr=r(Unt,"TFMobileBertForMaskedLM"),Unt.forEach(t),otr=r(j0e," (MobileBERT model)"),j0e.forEach(t),rtr=i(ee),MC=s(ee,"LI",{});var N0e=n(MC);Sge=s(N0e,"STRONG",{});var Jnt=n(Sge);ttr=r(Jnt,"mpnet"),Jnt.forEach(t),atr=r(N0e," \u2014 "),Hq=s(N0e,"A",{href:!0});var Ynt=n(Hq);str=r(Ynt,"TFMPNetForMaskedLM"),Ynt.forEach(t),ntr=r(N0e," (MPNet model)"),N0e.forEach(t),ltr=i(ee),EC=s(ee,"LI",{});var q0e=n(EC);Pge=s(q0e,"STRONG",{});var Knt=n(Pge);itr=r(Knt,"rembert"),Knt.forEach(t),dtr=r(q0e," \u2014 "),Uq=s(q0e,"A",{href:!0});var Znt=n(Uq);ctr=r(Znt,"TFRemBertForMaskedLM"),Znt.forEach(t),mtr=r(q0e," (RemBERT model)"),q0e.forEach(t),ftr=i(ee),yC=s(ee,"LI",{});var G0e=n(yC);$ge=s(G0e,"STRONG",{});var elt=n($ge);gtr=r(elt,"roberta"),elt.forEach(t),htr=r(G0e," \u2014 "),Jq=s(G0e,"A",{href:!0});var olt=n(Jq);utr=r(olt,"TFRobertaForMaskedLM"),olt.forEach(t),ptr=r(G0e," (RoBERTa model)"),G0e.forEach(t),_tr=i(ee),wC=s(ee,"LI",{});var O0e=n(wC);Ige=s(O0e,"STRONG",{});var rlt=n(Ige);btr=r(rlt,"roformer"),rlt.forEach(t),vtr=r(O0e," \u2014 "),Yq=s(O0e,"A",{href:!0});var tlt=n(Yq);Ttr=r(tlt,"TFRoFormerForMaskedLM"),tlt.forEach(t),Ftr=r(O0e," (RoFormer model)"),O0e.forEach(t),Ctr=i(ee),AC=s(ee,"LI",{});var X0e=n(AC);Dge=s(X0e,"STRONG",{});var alt=n(Dge);Mtr=r(alt,"tapas"),alt.forEach(t),Etr=r(X0e," \u2014 "),Kq=s(X0e,"A",{href:!0});var slt=n(Kq);ytr=r(slt,"TFTapasForMaskedLM"),slt.forEach(t),wtr=r(X0e," (TAPAS model)"),X0e.forEach(t),Atr=i(ee),LC=s(ee,"LI",{});var V0e=n(LC);jge=s(V0e,"STRONG",{});var nlt=n(jge);Ltr=r(nlt,"xlm"),nlt.forEach(t),Btr=r(V0e," \u2014 "),Zq=s(V0e,"A",{href:!0});var llt=n(Zq);xtr=r(llt,"TFXLMWithLMHeadModel"),llt.forEach(t),ktr=r(V0e," (XLM model)"),V0e.forEach(t),Rtr=i(ee),BC=s(ee,"LI",{});var z0e=n(BC);Nge=s(z0e,"STRONG",{});var ilt=n(Nge);Str=r(ilt,"xlm-roberta"),ilt.forEach(t),Ptr=r(z0e," \u2014 "),eG=s(z0e,"A",{href:!0});var dlt=n(eG);$tr=r(dlt,"TFXLMRobertaForMaskedLM"),dlt.forEach(t),Itr=r(z0e," (XLM-RoBERTa model)"),z0e.forEach(t),ee.forEach(t),Dtr=i(ha),qge=s(ha,"P",{});var clt=n(qge);jtr=r(clt,"Examples:"),clt.forEach(t),Ntr=i(ha),f(BA.$$.fragment,ha),ha.forEach(t),Nl.forEach(t),xBe=i(c),pc=s(c,"H2",{class:!0});var qke=n(pc);xC=s(qke,"A",{id:!0,class:!0,href:!0});var mlt=n(xC);Gge=s(mlt,"SPAN",{});var flt=n(Gge);f(xA.$$.fragment,flt),flt.forEach(t),mlt.forEach(t),qtr=i(qke),Oge=s(qke,"SPAN",{});var glt=n(Oge);Gtr=r(glt,"TFAutoModelForSeq2SeqLM"),glt.forEach(t),qke.forEach(t),kBe=i(c),Tr=s(c,"DIV",{class:!0});var Gl=n(Tr);f(kA.$$.fragment,Gl),Otr=i(Gl),_c=s(Gl,"P",{});var xz=n(_c);Xtr=r(xz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),Xge=s(xz,"CODE",{});var hlt=n(Xge);Vtr=r(hlt,"from_pretrained()"),hlt.forEach(t),ztr=r(xz,"class method or the "),Vge=s(xz,"CODE",{});var ult=n(Vge);Wtr=r(ult,"from_config()"),ult.forEach(t),Qtr=r(xz,`class
method.`),xz.forEach(t),Htr=i(Gl),RA=s(Gl,"P",{});var Gke=n(RA);Utr=r(Gke,"This class cannot be instantiated directly using "),zge=s(Gke,"CODE",{});var plt=n(zge);Jtr=r(plt,"__init__()"),plt.forEach(t),Ytr=r(Gke," (throws an error)."),Gke.forEach(t),Ktr=i(Gl),gt=s(Gl,"DIV",{class:!0});var Ol=n(gt);f(SA.$$.fragment,Ol),Ztr=i(Ol),Wge=s(Ol,"P",{});var _lt=n(Wge);ear=r(_lt,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),_lt.forEach(t),oar=i(Ol),bc=s(Ol,"P",{});var kz=n(bc);rar=r(kz,`Note:
Loading a model from its configuration file does `),Qge=s(kz,"STRONG",{});var blt=n(Qge);tar=r(blt,"not"),blt.forEach(t),aar=r(kz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Hge=s(kz,"CODE",{});var vlt=n(Hge);sar=r(vlt,"from_pretrained()"),vlt.forEach(t),nar=r(kz,"to load the model weights."),kz.forEach(t),lar=i(Ol),Uge=s(Ol,"P",{});var Tlt=n(Uge);iar=r(Tlt,"Examples:"),Tlt.forEach(t),dar=i(Ol),f(PA.$$.fragment,Ol),Ol.forEach(t),car=i(Gl),bo=s(Gl,"DIV",{class:!0});var ua=n(bo);f($A.$$.fragment,ua),mar=i(ua),Jge=s(ua,"P",{});var Flt=n(Jge);far=r(Flt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),Flt.forEach(t),gar=i(ua),hs=s(ua,"P",{});var Z3=n(hs);har=r(Z3,"The model class to instantiate is selected based on the "),Yge=s(Z3,"CODE",{});var Clt=n(Yge);uar=r(Clt,"model_type"),Clt.forEach(t),par=r(Z3,` property of the config object (either
passed as an argument or loaded from `),Kge=s(Z3,"CODE",{});var Mlt=n(Kge);_ar=r(Mlt,"pretrained_model_name_or_path"),Mlt.forEach(t),bar=r(Z3,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Zge=s(Z3,"CODE",{});var Elt=n(Zge);Tar=r(Elt,"pretrained_model_name_or_path"),Elt.forEach(t),Far=r(Z3,":"),Z3.forEach(t),Car=i(ua),ue=s(ua,"UL",{});var Ee=n(ue);kC=s(Ee,"LI",{});var W0e=n(kC);ehe=s(W0e,"STRONG",{});var ylt=n(ehe);Mar=r(ylt,"bart"),ylt.forEach(t),Ear=r(W0e," \u2014 "),oG=s(W0e,"A",{href:!0});var wlt=n(oG);yar=r(wlt,"TFBartForConditionalGeneration"),wlt.forEach(t),war=r(W0e," (BART model)"),W0e.forEach(t),Aar=i(Ee),RC=s(Ee,"LI",{});var Q0e=n(RC);ohe=s(Q0e,"STRONG",{});var Alt=n(ohe);Lar=r(Alt,"blenderbot"),Alt.forEach(t),Bar=r(Q0e," \u2014 "),rG=s(Q0e,"A",{href:!0});var Llt=n(rG);xar=r(Llt,"TFBlenderbotForConditionalGeneration"),Llt.forEach(t),kar=r(Q0e," (Blenderbot model)"),Q0e.forEach(t),Rar=i(Ee),SC=s(Ee,"LI",{});var H0e=n(SC);rhe=s(H0e,"STRONG",{});var Blt=n(rhe);Sar=r(Blt,"blenderbot-small"),Blt.forEach(t),Par=r(H0e," \u2014 "),tG=s(H0e,"A",{href:!0});var xlt=n(tG);$ar=r(xlt,"TFBlenderbotSmallForConditionalGeneration"),xlt.forEach(t),Iar=r(H0e," (BlenderbotSmall model)"),H0e.forEach(t),Dar=i(Ee),PC=s(Ee,"LI",{});var U0e=n(PC);the=s(U0e,"STRONG",{});var klt=n(the);jar=r(klt,"encoder-decoder"),klt.forEach(t),Nar=r(U0e," \u2014 "),aG=s(U0e,"A",{href:!0});var Rlt=n(aG);qar=r(Rlt,"TFEncoderDecoderModel"),Rlt.forEach(t),Gar=r(U0e," (Encoder decoder model)"),U0e.forEach(t),Oar=i(Ee),$C=s(Ee,"LI",{});var J0e=n($C);ahe=s(J0e,"STRONG",{});var Slt=n(ahe);Xar=r(Slt,"led"),Slt.forEach(t),Var=r(J0e," \u2014 "),sG=s(J0e,"A",{href:!0});var Plt=n(sG);zar=r(Plt,"TFLEDForConditionalGeneration"),Plt.forEach(t),War=r(J0e," (LED model)"),J0e.forEach(t),Qar=i(Ee),IC=s(Ee,"LI",{});var Y0e=n(IC);she=s(Y0e,"STRONG",{});var $lt=n(she);Har=r($lt,"marian"),$lt.forEach(t),Uar=r(Y0e," \u2014 "),nG=s(Y0e,"A",{href:!0});var Ilt=n(nG);Jar=r(Ilt,"TFMarianMTModel"),Ilt.forEach(t),Yar=r(Y0e," (Marian model)"),Y0e.forEach(t),Kar=i(Ee),DC=s(Ee,"LI",{});var K0e=n(DC);nhe=s(K0e,"STRONG",{});var Dlt=n(nhe);Zar=r(Dlt,"mbart"),Dlt.forEach(t),esr=r(K0e," \u2014 "),lG=s(K0e,"A",{href:!0});var jlt=n(lG);osr=r(jlt,"TFMBartForConditionalGeneration"),jlt.forEach(t),rsr=r(K0e," (mBART model)"),K0e.forEach(t),tsr=i(Ee),jC=s(Ee,"LI",{});var Z0e=n(jC);lhe=s(Z0e,"STRONG",{});var Nlt=n(lhe);asr=r(Nlt,"mt5"),Nlt.forEach(t),ssr=r(Z0e," \u2014 "),iG=s(Z0e,"A",{href:!0});var qlt=n(iG);nsr=r(qlt,"TFMT5ForConditionalGeneration"),qlt.forEach(t),lsr=r(Z0e," (mT5 model)"),Z0e.forEach(t),isr=i(Ee),NC=s(Ee,"LI",{});var eLe=n(NC);ihe=s(eLe,"STRONG",{});var Glt=n(ihe);dsr=r(Glt,"pegasus"),Glt.forEach(t),csr=r(eLe," \u2014 "),dG=s(eLe,"A",{href:!0});var Olt=n(dG);msr=r(Olt,"TFPegasusForConditionalGeneration"),Olt.forEach(t),fsr=r(eLe," (Pegasus model)"),eLe.forEach(t),gsr=i(Ee),qC=s(Ee,"LI",{});var oLe=n(qC);dhe=s(oLe,"STRONG",{});var Xlt=n(dhe);hsr=r(Xlt,"t5"),Xlt.forEach(t),usr=r(oLe," \u2014 "),cG=s(oLe,"A",{href:!0});var Vlt=n(cG);psr=r(Vlt,"TFT5ForConditionalGeneration"),Vlt.forEach(t),_sr=r(oLe," (T5 model)"),oLe.forEach(t),Ee.forEach(t),bsr=i(ua),che=s(ua,"P",{});var zlt=n(che);vsr=r(zlt,"Examples:"),zlt.forEach(t),Tsr=i(ua),f(IA.$$.fragment,ua),ua.forEach(t),Gl.forEach(t),RBe=i(c),vc=s(c,"H2",{class:!0});var Oke=n(vc);GC=s(Oke,"A",{id:!0,class:!0,href:!0});var Wlt=n(GC);mhe=s(Wlt,"SPAN",{});var Qlt=n(mhe);f(DA.$$.fragment,Qlt),Qlt.forEach(t),Wlt.forEach(t),Fsr=i(Oke),fhe=s(Oke,"SPAN",{});var Hlt=n(fhe);Csr=r(Hlt,"TFAutoModelForSequenceClassification"),Hlt.forEach(t),Oke.forEach(t),SBe=i(c),Fr=s(c,"DIV",{class:!0});var Xl=n(Fr);f(jA.$$.fragment,Xl),Msr=i(Xl),Tc=s(Xl,"P",{});var Rz=n(Tc);Esr=r(Rz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),ghe=s(Rz,"CODE",{});var Ult=n(ghe);ysr=r(Ult,"from_pretrained()"),Ult.forEach(t),wsr=r(Rz,"class method or the "),hhe=s(Rz,"CODE",{});var Jlt=n(hhe);Asr=r(Jlt,"from_config()"),Jlt.forEach(t),Lsr=r(Rz,`class
method.`),Rz.forEach(t),Bsr=i(Xl),NA=s(Xl,"P",{});var Xke=n(NA);xsr=r(Xke,"This class cannot be instantiated directly using "),uhe=s(Xke,"CODE",{});var Ylt=n(uhe);ksr=r(Ylt,"__init__()"),Ylt.forEach(t),Rsr=r(Xke," (throws an error)."),Xke.forEach(t),Ssr=i(Xl),ht=s(Xl,"DIV",{class:!0});var Vl=n(ht);f(qA.$$.fragment,Vl),Psr=i(Vl),phe=s(Vl,"P",{});var Klt=n(phe);$sr=r(Klt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),Klt.forEach(t),Isr=i(Vl),Fc=s(Vl,"P",{});var Sz=n(Fc);Dsr=r(Sz,`Note:
Loading a model from its configuration file does `),_he=s(Sz,"STRONG",{});var Zlt=n(_he);jsr=r(Zlt,"not"),Zlt.forEach(t),Nsr=r(Sz,` load the model weights. It only affects the
model\u2019s configuration. Use `),bhe=s(Sz,"CODE",{});var eit=n(bhe);qsr=r(eit,"from_pretrained()"),eit.forEach(t),Gsr=r(Sz,"to load the model weights."),Sz.forEach(t),Osr=i(Vl),vhe=s(Vl,"P",{});var oit=n(vhe);Xsr=r(oit,"Examples:"),oit.forEach(t),Vsr=i(Vl),f(GA.$$.fragment,Vl),Vl.forEach(t),zsr=i(Xl),vo=s(Xl,"DIV",{class:!0});var pa=n(vo);f(OA.$$.fragment,pa),Wsr=i(pa),The=s(pa,"P",{});var rit=n(The);Qsr=r(rit,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),rit.forEach(t),Hsr=i(pa),us=s(pa,"P",{});var e5=n(us);Usr=r(e5,"The model class to instantiate is selected based on the "),Fhe=s(e5,"CODE",{});var tit=n(Fhe);Jsr=r(tit,"model_type"),tit.forEach(t),Ysr=r(e5,` property of the config object (either
passed as an argument or loaded from `),Che=s(e5,"CODE",{});var ait=n(Che);Ksr=r(ait,"pretrained_model_name_or_path"),ait.forEach(t),Zsr=r(e5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Mhe=s(e5,"CODE",{});var sit=n(Mhe);enr=r(sit,"pretrained_model_name_or_path"),sit.forEach(t),onr=r(e5,":"),e5.forEach(t),rnr=i(pa),X=s(pa,"UL",{});var W=n(X);OC=s(W,"LI",{});var rLe=n(OC);Ehe=s(rLe,"STRONG",{});var nit=n(Ehe);tnr=r(nit,"albert"),nit.forEach(t),anr=r(rLe," \u2014 "),mG=s(rLe,"A",{href:!0});var lit=n(mG);snr=r(lit,"TFAlbertForSequenceClassification"),lit.forEach(t),nnr=r(rLe," (ALBERT model)"),rLe.forEach(t),lnr=i(W),XC=s(W,"LI",{});var tLe=n(XC);yhe=s(tLe,"STRONG",{});var iit=n(yhe);inr=r(iit,"bert"),iit.forEach(t),dnr=r(tLe," \u2014 "),fG=s(tLe,"A",{href:!0});var dit=n(fG);cnr=r(dit,"TFBertForSequenceClassification"),dit.forEach(t),mnr=r(tLe," (BERT model)"),tLe.forEach(t),fnr=i(W),VC=s(W,"LI",{});var aLe=n(VC);whe=s(aLe,"STRONG",{});var cit=n(whe);gnr=r(cit,"camembert"),cit.forEach(t),hnr=r(aLe," \u2014 "),gG=s(aLe,"A",{href:!0});var mit=n(gG);unr=r(mit,"TFCamembertForSequenceClassification"),mit.forEach(t),pnr=r(aLe," (CamemBERT model)"),aLe.forEach(t),_nr=i(W),zC=s(W,"LI",{});var sLe=n(zC);Ahe=s(sLe,"STRONG",{});var fit=n(Ahe);bnr=r(fit,"convbert"),fit.forEach(t),vnr=r(sLe," \u2014 "),hG=s(sLe,"A",{href:!0});var git=n(hG);Tnr=r(git,"TFConvBertForSequenceClassification"),git.forEach(t),Fnr=r(sLe," (ConvBERT model)"),sLe.forEach(t),Cnr=i(W),WC=s(W,"LI",{});var nLe=n(WC);Lhe=s(nLe,"STRONG",{});var hit=n(Lhe);Mnr=r(hit,"ctrl"),hit.forEach(t),Enr=r(nLe," \u2014 "),uG=s(nLe,"A",{href:!0});var uit=n(uG);ynr=r(uit,"TFCTRLForSequenceClassification"),uit.forEach(t),wnr=r(nLe," (CTRL model)"),nLe.forEach(t),Anr=i(W),QC=s(W,"LI",{});var lLe=n(QC);Bhe=s(lLe,"STRONG",{});var pit=n(Bhe);Lnr=r(pit,"deberta"),pit.forEach(t),Bnr=r(lLe," \u2014 "),pG=s(lLe,"A",{href:!0});var _it=n(pG);xnr=r(_it,"TFDebertaForSequenceClassification"),_it.forEach(t),knr=r(lLe," (DeBERTa model)"),lLe.forEach(t),Rnr=i(W),HC=s(W,"LI",{});var iLe=n(HC);xhe=s(iLe,"STRONG",{});var bit=n(xhe);Snr=r(bit,"deberta-v2"),bit.forEach(t),Pnr=r(iLe," \u2014 "),_G=s(iLe,"A",{href:!0});var vit=n(_G);$nr=r(vit,"TFDebertaV2ForSequenceClassification"),vit.forEach(t),Inr=r(iLe," (DeBERTa-v2 model)"),iLe.forEach(t),Dnr=i(W),UC=s(W,"LI",{});var dLe=n(UC);khe=s(dLe,"STRONG",{});var Tit=n(khe);jnr=r(Tit,"distilbert"),Tit.forEach(t),Nnr=r(dLe," \u2014 "),bG=s(dLe,"A",{href:!0});var Fit=n(bG);qnr=r(Fit,"TFDistilBertForSequenceClassification"),Fit.forEach(t),Gnr=r(dLe," (DistilBERT model)"),dLe.forEach(t),Onr=i(W),JC=s(W,"LI",{});var cLe=n(JC);Rhe=s(cLe,"STRONG",{});var Cit=n(Rhe);Xnr=r(Cit,"electra"),Cit.forEach(t),Vnr=r(cLe," \u2014 "),vG=s(cLe,"A",{href:!0});var Mit=n(vG);znr=r(Mit,"TFElectraForSequenceClassification"),Mit.forEach(t),Wnr=r(cLe," (ELECTRA model)"),cLe.forEach(t),Qnr=i(W),YC=s(W,"LI",{});var mLe=n(YC);She=s(mLe,"STRONG",{});var Eit=n(She);Hnr=r(Eit,"flaubert"),Eit.forEach(t),Unr=r(mLe," \u2014 "),TG=s(mLe,"A",{href:!0});var yit=n(TG);Jnr=r(yit,"TFFlaubertForSequenceClassification"),yit.forEach(t),Ynr=r(mLe," (FlauBERT model)"),mLe.forEach(t),Knr=i(W),KC=s(W,"LI",{});var fLe=n(KC);Phe=s(fLe,"STRONG",{});var wit=n(Phe);Znr=r(wit,"funnel"),wit.forEach(t),elr=r(fLe," \u2014 "),FG=s(fLe,"A",{href:!0});var Ait=n(FG);olr=r(Ait,"TFFunnelForSequenceClassification"),Ait.forEach(t),rlr=r(fLe," (Funnel Transformer model)"),fLe.forEach(t),tlr=i(W),ZC=s(W,"LI",{});var gLe=n(ZC);$he=s(gLe,"STRONG",{});var Lit=n($he);alr=r(Lit,"gpt2"),Lit.forEach(t),slr=r(gLe," \u2014 "),CG=s(gLe,"A",{href:!0});var Bit=n(CG);nlr=r(Bit,"TFGPT2ForSequenceClassification"),Bit.forEach(t),llr=r(gLe," (OpenAI GPT-2 model)"),gLe.forEach(t),ilr=i(W),e4=s(W,"LI",{});var hLe=n(e4);Ihe=s(hLe,"STRONG",{});var xit=n(Ihe);dlr=r(xit,"layoutlm"),xit.forEach(t),clr=r(hLe," \u2014 "),MG=s(hLe,"A",{href:!0});var kit=n(MG);mlr=r(kit,"TFLayoutLMForSequenceClassification"),kit.forEach(t),flr=r(hLe," (LayoutLM model)"),hLe.forEach(t),glr=i(W),o4=s(W,"LI",{});var uLe=n(o4);Dhe=s(uLe,"STRONG",{});var Rit=n(Dhe);hlr=r(Rit,"longformer"),Rit.forEach(t),ulr=r(uLe," \u2014 "),EG=s(uLe,"A",{href:!0});var Sit=n(EG);plr=r(Sit,"TFLongformerForSequenceClassification"),Sit.forEach(t),_lr=r(uLe," (Longformer model)"),uLe.forEach(t),blr=i(W),r4=s(W,"LI",{});var pLe=n(r4);jhe=s(pLe,"STRONG",{});var Pit=n(jhe);vlr=r(Pit,"mobilebert"),Pit.forEach(t),Tlr=r(pLe," \u2014 "),yG=s(pLe,"A",{href:!0});var $it=n(yG);Flr=r($it,"TFMobileBertForSequenceClassification"),$it.forEach(t),Clr=r(pLe," (MobileBERT model)"),pLe.forEach(t),Mlr=i(W),t4=s(W,"LI",{});var _Le=n(t4);Nhe=s(_Le,"STRONG",{});var Iit=n(Nhe);Elr=r(Iit,"mpnet"),Iit.forEach(t),ylr=r(_Le," \u2014 "),wG=s(_Le,"A",{href:!0});var Dit=n(wG);wlr=r(Dit,"TFMPNetForSequenceClassification"),Dit.forEach(t),Alr=r(_Le," (MPNet model)"),_Le.forEach(t),Llr=i(W),a4=s(W,"LI",{});var bLe=n(a4);qhe=s(bLe,"STRONG",{});var jit=n(qhe);Blr=r(jit,"openai-gpt"),jit.forEach(t),xlr=r(bLe," \u2014 "),AG=s(bLe,"A",{href:!0});var Nit=n(AG);klr=r(Nit,"TFOpenAIGPTForSequenceClassification"),Nit.forEach(t),Rlr=r(bLe," (OpenAI GPT model)"),bLe.forEach(t),Slr=i(W),s4=s(W,"LI",{});var vLe=n(s4);Ghe=s(vLe,"STRONG",{});var qit=n(Ghe);Plr=r(qit,"rembert"),qit.forEach(t),$lr=r(vLe," \u2014 "),LG=s(vLe,"A",{href:!0});var Git=n(LG);Ilr=r(Git,"TFRemBertForSequenceClassification"),Git.forEach(t),Dlr=r(vLe," (RemBERT model)"),vLe.forEach(t),jlr=i(W),n4=s(W,"LI",{});var TLe=n(n4);Ohe=s(TLe,"STRONG",{});var Oit=n(Ohe);Nlr=r(Oit,"roberta"),Oit.forEach(t),qlr=r(TLe," \u2014 "),BG=s(TLe,"A",{href:!0});var Xit=n(BG);Glr=r(Xit,"TFRobertaForSequenceClassification"),Xit.forEach(t),Olr=r(TLe," (RoBERTa model)"),TLe.forEach(t),Xlr=i(W),l4=s(W,"LI",{});var FLe=n(l4);Xhe=s(FLe,"STRONG",{});var Vit=n(Xhe);Vlr=r(Vit,"roformer"),Vit.forEach(t),zlr=r(FLe," \u2014 "),xG=s(FLe,"A",{href:!0});var zit=n(xG);Wlr=r(zit,"TFRoFormerForSequenceClassification"),zit.forEach(t),Qlr=r(FLe," (RoFormer model)"),FLe.forEach(t),Hlr=i(W),i4=s(W,"LI",{});var CLe=n(i4);Vhe=s(CLe,"STRONG",{});var Wit=n(Vhe);Ulr=r(Wit,"tapas"),Wit.forEach(t),Jlr=r(CLe," \u2014 "),kG=s(CLe,"A",{href:!0});var Qit=n(kG);Ylr=r(Qit,"TFTapasForSequenceClassification"),Qit.forEach(t),Klr=r(CLe," (TAPAS model)"),CLe.forEach(t),Zlr=i(W),d4=s(W,"LI",{});var MLe=n(d4);zhe=s(MLe,"STRONG",{});var Hit=n(zhe);eir=r(Hit,"transfo-xl"),Hit.forEach(t),oir=r(MLe," \u2014 "),RG=s(MLe,"A",{href:!0});var Uit=n(RG);rir=r(Uit,"TFTransfoXLForSequenceClassification"),Uit.forEach(t),tir=r(MLe," (Transformer-XL model)"),MLe.forEach(t),air=i(W),c4=s(W,"LI",{});var ELe=n(c4);Whe=s(ELe,"STRONG",{});var Jit=n(Whe);sir=r(Jit,"xlm"),Jit.forEach(t),nir=r(ELe," \u2014 "),SG=s(ELe,"A",{href:!0});var Yit=n(SG);lir=r(Yit,"TFXLMForSequenceClassification"),Yit.forEach(t),iir=r(ELe," (XLM model)"),ELe.forEach(t),dir=i(W),m4=s(W,"LI",{});var yLe=n(m4);Qhe=s(yLe,"STRONG",{});var Kit=n(Qhe);cir=r(Kit,"xlm-roberta"),Kit.forEach(t),mir=r(yLe," \u2014 "),PG=s(yLe,"A",{href:!0});var Zit=n(PG);fir=r(Zit,"TFXLMRobertaForSequenceClassification"),Zit.forEach(t),gir=r(yLe," (XLM-RoBERTa model)"),yLe.forEach(t),hir=i(W),f4=s(W,"LI",{});var wLe=n(f4);Hhe=s(wLe,"STRONG",{});var edt=n(Hhe);uir=r(edt,"xlnet"),edt.forEach(t),pir=r(wLe," \u2014 "),$G=s(wLe,"A",{href:!0});var odt=n($G);_ir=r(odt,"TFXLNetForSequenceClassification"),odt.forEach(t),bir=r(wLe," (XLNet model)"),wLe.forEach(t),W.forEach(t),vir=i(pa),Uhe=s(pa,"P",{});var rdt=n(Uhe);Tir=r(rdt,"Examples:"),rdt.forEach(t),Fir=i(pa),f(XA.$$.fragment,pa),pa.forEach(t),Xl.forEach(t),PBe=i(c),Cc=s(c,"H2",{class:!0});var Vke=n(Cc);g4=s(Vke,"A",{id:!0,class:!0,href:!0});var tdt=n(g4);Jhe=s(tdt,"SPAN",{});var adt=n(Jhe);f(VA.$$.fragment,adt),adt.forEach(t),tdt.forEach(t),Cir=i(Vke),Yhe=s(Vke,"SPAN",{});var sdt=n(Yhe);Mir=r(sdt,"TFAutoModelForMultipleChoice"),sdt.forEach(t),Vke.forEach(t),$Be=i(c),Cr=s(c,"DIV",{class:!0});var zl=n(Cr);f(zA.$$.fragment,zl),Eir=i(zl),Mc=s(zl,"P",{});var Pz=n(Mc);yir=r(Pz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),Khe=s(Pz,"CODE",{});var ndt=n(Khe);wir=r(ndt,"from_pretrained()"),ndt.forEach(t),Air=r(Pz,"class method or the "),Zhe=s(Pz,"CODE",{});var ldt=n(Zhe);Lir=r(ldt,"from_config()"),ldt.forEach(t),Bir=r(Pz,`class
method.`),Pz.forEach(t),xir=i(zl),WA=s(zl,"P",{});var zke=n(WA);kir=r(zke,"This class cannot be instantiated directly using "),eue=s(zke,"CODE",{});var idt=n(eue);Rir=r(idt,"__init__()"),idt.forEach(t),Sir=r(zke," (throws an error)."),zke.forEach(t),Pir=i(zl),ut=s(zl,"DIV",{class:!0});var Wl=n(ut);f(QA.$$.fragment,Wl),$ir=i(Wl),oue=s(Wl,"P",{});var ddt=n(oue);Iir=r(ddt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),ddt.forEach(t),Dir=i(Wl),Ec=s(Wl,"P",{});var $z=n(Ec);jir=r($z,`Note:
Loading a model from its configuration file does `),rue=s($z,"STRONG",{});var cdt=n(rue);Nir=r(cdt,"not"),cdt.forEach(t),qir=r($z,` load the model weights. It only affects the
model\u2019s configuration. Use `),tue=s($z,"CODE",{});var mdt=n(tue);Gir=r(mdt,"from_pretrained()"),mdt.forEach(t),Oir=r($z,"to load the model weights."),$z.forEach(t),Xir=i(Wl),aue=s(Wl,"P",{});var fdt=n(aue);Vir=r(fdt,"Examples:"),fdt.forEach(t),zir=i(Wl),f(HA.$$.fragment,Wl),Wl.forEach(t),Wir=i(zl),To=s(zl,"DIV",{class:!0});var _a=n(To);f(UA.$$.fragment,_a),Qir=i(_a),sue=s(_a,"P",{});var gdt=n(sue);Hir=r(gdt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),gdt.forEach(t),Uir=i(_a),ps=s(_a,"P",{});var o5=n(ps);Jir=r(o5,"The model class to instantiate is selected based on the "),nue=s(o5,"CODE",{});var hdt=n(nue);Yir=r(hdt,"model_type"),hdt.forEach(t),Kir=r(o5,` property of the config object (either
passed as an argument or loaded from `),lue=s(o5,"CODE",{});var udt=n(lue);Zir=r(udt,"pretrained_model_name_or_path"),udt.forEach(t),edr=r(o5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),iue=s(o5,"CODE",{});var pdt=n(iue);odr=r(pdt,"pretrained_model_name_or_path"),pdt.forEach(t),rdr=r(o5,":"),o5.forEach(t),tdr=i(_a),te=s(_a,"UL",{});var se=n(te);h4=s(se,"LI",{});var ALe=n(h4);due=s(ALe,"STRONG",{});var _dt=n(due);adr=r(_dt,"albert"),_dt.forEach(t),sdr=r(ALe," \u2014 "),IG=s(ALe,"A",{href:!0});var bdt=n(IG);ndr=r(bdt,"TFAlbertForMultipleChoice"),bdt.forEach(t),ldr=r(ALe," (ALBERT model)"),ALe.forEach(t),idr=i(se),u4=s(se,"LI",{});var LLe=n(u4);cue=s(LLe,"STRONG",{});var vdt=n(cue);ddr=r(vdt,"bert"),vdt.forEach(t),cdr=r(LLe," \u2014 "),DG=s(LLe,"A",{href:!0});var Tdt=n(DG);mdr=r(Tdt,"TFBertForMultipleChoice"),Tdt.forEach(t),fdr=r(LLe," (BERT model)"),LLe.forEach(t),gdr=i(se),p4=s(se,"LI",{});var BLe=n(p4);mue=s(BLe,"STRONG",{});var Fdt=n(mue);hdr=r(Fdt,"camembert"),Fdt.forEach(t),udr=r(BLe," \u2014 "),jG=s(BLe,"A",{href:!0});var Cdt=n(jG);pdr=r(Cdt,"TFCamembertForMultipleChoice"),Cdt.forEach(t),_dr=r(BLe," (CamemBERT model)"),BLe.forEach(t),bdr=i(se),_4=s(se,"LI",{});var xLe=n(_4);fue=s(xLe,"STRONG",{});var Mdt=n(fue);vdr=r(Mdt,"convbert"),Mdt.forEach(t),Tdr=r(xLe," \u2014 "),NG=s(xLe,"A",{href:!0});var Edt=n(NG);Fdr=r(Edt,"TFConvBertForMultipleChoice"),Edt.forEach(t),Cdr=r(xLe," (ConvBERT model)"),xLe.forEach(t),Mdr=i(se),b4=s(se,"LI",{});var kLe=n(b4);gue=s(kLe,"STRONG",{});var ydt=n(gue);Edr=r(ydt,"distilbert"),ydt.forEach(t),ydr=r(kLe," \u2014 "),qG=s(kLe,"A",{href:!0});var wdt=n(qG);wdr=r(wdt,"TFDistilBertForMultipleChoice"),wdt.forEach(t),Adr=r(kLe," (DistilBERT model)"),kLe.forEach(t),Ldr=i(se),v4=s(se,"LI",{});var RLe=n(v4);hue=s(RLe,"STRONG",{});var Adt=n(hue);Bdr=r(Adt,"electra"),Adt.forEach(t),xdr=r(RLe," \u2014 "),GG=s(RLe,"A",{href:!0});var Ldt=n(GG);kdr=r(Ldt,"TFElectraForMultipleChoice"),Ldt.forEach(t),Rdr=r(RLe," (ELECTRA model)"),RLe.forEach(t),Sdr=i(se),T4=s(se,"LI",{});var SLe=n(T4);uue=s(SLe,"STRONG",{});var Bdt=n(uue);Pdr=r(Bdt,"flaubert"),Bdt.forEach(t),$dr=r(SLe," \u2014 "),OG=s(SLe,"A",{href:!0});var xdt=n(OG);Idr=r(xdt,"TFFlaubertForMultipleChoice"),xdt.forEach(t),Ddr=r(SLe," (FlauBERT model)"),SLe.forEach(t),jdr=i(se),F4=s(se,"LI",{});var PLe=n(F4);pue=s(PLe,"STRONG",{});var kdt=n(pue);Ndr=r(kdt,"funnel"),kdt.forEach(t),qdr=r(PLe," \u2014 "),XG=s(PLe,"A",{href:!0});var Rdt=n(XG);Gdr=r(Rdt,"TFFunnelForMultipleChoice"),Rdt.forEach(t),Odr=r(PLe," (Funnel Transformer model)"),PLe.forEach(t),Xdr=i(se),C4=s(se,"LI",{});var $Le=n(C4);_ue=s($Le,"STRONG",{});var Sdt=n(_ue);Vdr=r(Sdt,"longformer"),Sdt.forEach(t),zdr=r($Le," \u2014 "),VG=s($Le,"A",{href:!0});var Pdt=n(VG);Wdr=r(Pdt,"TFLongformerForMultipleChoice"),Pdt.forEach(t),Qdr=r($Le," (Longformer model)"),$Le.forEach(t),Hdr=i(se),M4=s(se,"LI",{});var ILe=n(M4);bue=s(ILe,"STRONG",{});var $dt=n(bue);Udr=r($dt,"mobilebert"),$dt.forEach(t),Jdr=r(ILe," \u2014 "),zG=s(ILe,"A",{href:!0});var Idt=n(zG);Ydr=r(Idt,"TFMobileBertForMultipleChoice"),Idt.forEach(t),Kdr=r(ILe," (MobileBERT model)"),ILe.forEach(t),Zdr=i(se),E4=s(se,"LI",{});var DLe=n(E4);vue=s(DLe,"STRONG",{});var Ddt=n(vue);ecr=r(Ddt,"mpnet"),Ddt.forEach(t),ocr=r(DLe," \u2014 "),WG=s(DLe,"A",{href:!0});var jdt=n(WG);rcr=r(jdt,"TFMPNetForMultipleChoice"),jdt.forEach(t),tcr=r(DLe," (MPNet model)"),DLe.forEach(t),acr=i(se),y4=s(se,"LI",{});var jLe=n(y4);Tue=s(jLe,"STRONG",{});var Ndt=n(Tue);scr=r(Ndt,"rembert"),Ndt.forEach(t),ncr=r(jLe," \u2014 "),QG=s(jLe,"A",{href:!0});var qdt=n(QG);lcr=r(qdt,"TFRemBertForMultipleChoice"),qdt.forEach(t),icr=r(jLe," (RemBERT model)"),jLe.forEach(t),dcr=i(se),w4=s(se,"LI",{});var NLe=n(w4);Fue=s(NLe,"STRONG",{});var Gdt=n(Fue);ccr=r(Gdt,"roberta"),Gdt.forEach(t),mcr=r(NLe," \u2014 "),HG=s(NLe,"A",{href:!0});var Odt=n(HG);fcr=r(Odt,"TFRobertaForMultipleChoice"),Odt.forEach(t),gcr=r(NLe," (RoBERTa model)"),NLe.forEach(t),hcr=i(se),A4=s(se,"LI",{});var qLe=n(A4);Cue=s(qLe,"STRONG",{});var Xdt=n(Cue);ucr=r(Xdt,"roformer"),Xdt.forEach(t),pcr=r(qLe," \u2014 "),UG=s(qLe,"A",{href:!0});var Vdt=n(UG);_cr=r(Vdt,"TFRoFormerForMultipleChoice"),Vdt.forEach(t),bcr=r(qLe," (RoFormer model)"),qLe.forEach(t),vcr=i(se),L4=s(se,"LI",{});var GLe=n(L4);Mue=s(GLe,"STRONG",{});var zdt=n(Mue);Tcr=r(zdt,"xlm"),zdt.forEach(t),Fcr=r(GLe," \u2014 "),JG=s(GLe,"A",{href:!0});var Wdt=n(JG);Ccr=r(Wdt,"TFXLMForMultipleChoice"),Wdt.forEach(t),Mcr=r(GLe," (XLM model)"),GLe.forEach(t),Ecr=i(se),B4=s(se,"LI",{});var OLe=n(B4);Eue=s(OLe,"STRONG",{});var Qdt=n(Eue);ycr=r(Qdt,"xlm-roberta"),Qdt.forEach(t),wcr=r(OLe," \u2014 "),YG=s(OLe,"A",{href:!0});var Hdt=n(YG);Acr=r(Hdt,"TFXLMRobertaForMultipleChoice"),Hdt.forEach(t),Lcr=r(OLe," (XLM-RoBERTa model)"),OLe.forEach(t),Bcr=i(se),x4=s(se,"LI",{});var XLe=n(x4);yue=s(XLe,"STRONG",{});var Udt=n(yue);xcr=r(Udt,"xlnet"),Udt.forEach(t),kcr=r(XLe," \u2014 "),KG=s(XLe,"A",{href:!0});var Jdt=n(KG);Rcr=r(Jdt,"TFXLNetForMultipleChoice"),Jdt.forEach(t),Scr=r(XLe," (XLNet model)"),XLe.forEach(t),se.forEach(t),Pcr=i(_a),wue=s(_a,"P",{});var Ydt=n(wue);$cr=r(Ydt,"Examples:"),Ydt.forEach(t),Icr=i(_a),f(JA.$$.fragment,_a),_a.forEach(t),zl.forEach(t),IBe=i(c),yc=s(c,"H2",{class:!0});var Wke=n(yc);k4=s(Wke,"A",{id:!0,class:!0,href:!0});var Kdt=n(k4);Aue=s(Kdt,"SPAN",{});var Zdt=n(Aue);f(YA.$$.fragment,Zdt),Zdt.forEach(t),Kdt.forEach(t),Dcr=i(Wke),Lue=s(Wke,"SPAN",{});var ect=n(Lue);jcr=r(ect,"TFAutoModelForTableQuestionAnswering"),ect.forEach(t),Wke.forEach(t),DBe=i(c),Mr=s(c,"DIV",{class:!0});var Ql=n(Mr);f(KA.$$.fragment,Ql),Ncr=i(Ql),wc=s(Ql,"P",{});var Iz=n(wc);qcr=r(Iz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the `),Bue=s(Iz,"CODE",{});var oct=n(Bue);Gcr=r(oct,"from_pretrained()"),oct.forEach(t),Ocr=r(Iz,"class method or the "),xue=s(Iz,"CODE",{});var rct=n(xue);Xcr=r(rct,"from_config()"),rct.forEach(t),Vcr=r(Iz,`class
method.`),Iz.forEach(t),zcr=i(Ql),ZA=s(Ql,"P",{});var Qke=n(ZA);Wcr=r(Qke,"This class cannot be instantiated directly using "),kue=s(Qke,"CODE",{});var tct=n(kue);Qcr=r(tct,"__init__()"),tct.forEach(t),Hcr=r(Qke," (throws an error)."),Qke.forEach(t),Ucr=i(Ql),pt=s(Ql,"DIV",{class:!0});var Hl=n(pt);f(e0.$$.fragment,Hl),Jcr=i(Hl),Rue=s(Hl,"P",{});var act=n(Rue);Ycr=r(act,"Instantiates one of the model classes of the library (with a table question answering head) from a configuration."),act.forEach(t),Kcr=i(Hl),Ac=s(Hl,"P",{});var Dz=n(Ac);Zcr=r(Dz,`Note:
Loading a model from its configuration file does `),Sue=s(Dz,"STRONG",{});var sct=n(Sue);emr=r(sct,"not"),sct.forEach(t),omr=r(Dz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Pue=s(Dz,"CODE",{});var nct=n(Pue);rmr=r(nct,"from_pretrained()"),nct.forEach(t),tmr=r(Dz,"to load the model weights."),Dz.forEach(t),amr=i(Hl),$ue=s(Hl,"P",{});var lct=n($ue);smr=r(lct,"Examples:"),lct.forEach(t),nmr=i(Hl),f(o0.$$.fragment,Hl),Hl.forEach(t),lmr=i(Ql),Fo=s(Ql,"DIV",{class:!0});var ba=n(Fo);f(r0.$$.fragment,ba),imr=i(ba),Iue=s(ba,"P",{});var ict=n(Iue);dmr=r(ict,"Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model."),ict.forEach(t),cmr=i(ba),_s=s(ba,"P",{});var r5=n(_s);mmr=r(r5,"The model class to instantiate is selected based on the "),Due=s(r5,"CODE",{});var dct=n(Due);fmr=r(dct,"model_type"),dct.forEach(t),gmr=r(r5,` property of the config object (either
passed as an argument or loaded from `),jue=s(r5,"CODE",{});var cct=n(jue);hmr=r(cct,"pretrained_model_name_or_path"),cct.forEach(t),umr=r(r5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Nue=s(r5,"CODE",{});var mct=n(Nue);pmr=r(mct,"pretrained_model_name_or_path"),mct.forEach(t),_mr=r(r5,":"),r5.forEach(t),bmr=i(ba),que=s(ba,"UL",{});var fct=n(que);R4=s(fct,"LI",{});var VLe=n(R4);Gue=s(VLe,"STRONG",{});var gct=n(Gue);vmr=r(gct,"tapas"),gct.forEach(t),Tmr=r(VLe," \u2014 "),ZG=s(VLe,"A",{href:!0});var hct=n(ZG);Fmr=r(hct,"TFTapasForQuestionAnswering"),hct.forEach(t),Cmr=r(VLe," (TAPAS model)"),VLe.forEach(t),fct.forEach(t),Mmr=i(ba),Oue=s(ba,"P",{});var uct=n(Oue);Emr=r(uct,"Examples:"),uct.forEach(t),ymr=i(ba),f(t0.$$.fragment,ba),ba.forEach(t),Ql.forEach(t),jBe=i(c),Lc=s(c,"H2",{class:!0});var Hke=n(Lc);S4=s(Hke,"A",{id:!0,class:!0,href:!0});var pct=n(S4);Xue=s(pct,"SPAN",{});var _ct=n(Xue);f(a0.$$.fragment,_ct),_ct.forEach(t),pct.forEach(t),wmr=i(Hke),Vue=s(Hke,"SPAN",{});var bct=n(Vue);Amr=r(bct,"TFAutoModelForTokenClassification"),bct.forEach(t),Hke.forEach(t),NBe=i(c),Er=s(c,"DIV",{class:!0});var Ul=n(Er);f(s0.$$.fragment,Ul),Lmr=i(Ul),Bc=s(Ul,"P",{});var jz=n(Bc);Bmr=r(jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),zue=s(jz,"CODE",{});var vct=n(zue);xmr=r(vct,"from_pretrained()"),vct.forEach(t),kmr=r(jz,"class method or the "),Wue=s(jz,"CODE",{});var Tct=n(Wue);Rmr=r(Tct,"from_config()"),Tct.forEach(t),Smr=r(jz,`class
method.`),jz.forEach(t),Pmr=i(Ul),n0=s(Ul,"P",{});var Uke=n(n0);$mr=r(Uke,"This class cannot be instantiated directly using "),Que=s(Uke,"CODE",{});var Fct=n(Que);Imr=r(Fct,"__init__()"),Fct.forEach(t),Dmr=r(Uke," (throws an error)."),Uke.forEach(t),jmr=i(Ul),_t=s(Ul,"DIV",{class:!0});var Jl=n(_t);f(l0.$$.fragment,Jl),Nmr=i(Jl),Hue=s(Jl,"P",{});var Cct=n(Hue);qmr=r(Cct,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),Cct.forEach(t),Gmr=i(Jl),xc=s(Jl,"P",{});var Nz=n(xc);Omr=r(Nz,`Note:
Loading a model from its configuration file does `),Uue=s(Nz,"STRONG",{});var Mct=n(Uue);Xmr=r(Mct,"not"),Mct.forEach(t),Vmr=r(Nz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Jue=s(Nz,"CODE",{});var Ect=n(Jue);zmr=r(Ect,"from_pretrained()"),Ect.forEach(t),Wmr=r(Nz,"to load the model weights."),Nz.forEach(t),Qmr=i(Jl),Yue=s(Jl,"P",{});var yct=n(Yue);Hmr=r(yct,"Examples:"),yct.forEach(t),Umr=i(Jl),f(i0.$$.fragment,Jl),Jl.forEach(t),Jmr=i(Ul),Co=s(Ul,"DIV",{class:!0});var va=n(Co);f(d0.$$.fragment,va),Ymr=i(va),Kue=s(va,"P",{});var wct=n(Kue);Kmr=r(wct,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),wct.forEach(t),Zmr=i(va),bs=s(va,"P",{});var t5=n(bs);efr=r(t5,"The model class to instantiate is selected based on the "),Zue=s(t5,"CODE",{});var Act=n(Zue);ofr=r(Act,"model_type"),Act.forEach(t),rfr=r(t5,` property of the config object (either
passed as an argument or loaded from `),epe=s(t5,"CODE",{});var Lct=n(epe);tfr=r(Lct,"pretrained_model_name_or_path"),Lct.forEach(t),afr=r(t5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ope=s(t5,"CODE",{});var Bct=n(ope);sfr=r(Bct,"pretrained_model_name_or_path"),Bct.forEach(t),nfr=r(t5,":"),t5.forEach(t),lfr=i(va),K=s(va,"UL",{});var oe=n(K);P4=s(oe,"LI",{});var zLe=n(P4);rpe=s(zLe,"STRONG",{});var xct=n(rpe);ifr=r(xct,"albert"),xct.forEach(t),dfr=r(zLe," \u2014 "),eO=s(zLe,"A",{href:!0});var kct=n(eO);cfr=r(kct,"TFAlbertForTokenClassification"),kct.forEach(t),mfr=r(zLe," (ALBERT model)"),zLe.forEach(t),ffr=i(oe),$4=s(oe,"LI",{});var WLe=n($4);tpe=s(WLe,"STRONG",{});var Rct=n(tpe);gfr=r(Rct,"bert"),Rct.forEach(t),hfr=r(WLe," \u2014 "),oO=s(WLe,"A",{href:!0});var Sct=n(oO);ufr=r(Sct,"TFBertForTokenClassification"),Sct.forEach(t),pfr=r(WLe," (BERT model)"),WLe.forEach(t),_fr=i(oe),I4=s(oe,"LI",{});var QLe=n(I4);ape=s(QLe,"STRONG",{});var Pct=n(ape);bfr=r(Pct,"camembert"),Pct.forEach(t),vfr=r(QLe," \u2014 "),rO=s(QLe,"A",{href:!0});var $ct=n(rO);Tfr=r($ct,"TFCamembertForTokenClassification"),$ct.forEach(t),Ffr=r(QLe," (CamemBERT model)"),QLe.forEach(t),Cfr=i(oe),D4=s(oe,"LI",{});var HLe=n(D4);spe=s(HLe,"STRONG",{});var Ict=n(spe);Mfr=r(Ict,"convbert"),Ict.forEach(t),Efr=r(HLe," \u2014 "),tO=s(HLe,"A",{href:!0});var Dct=n(tO);yfr=r(Dct,"TFConvBertForTokenClassification"),Dct.forEach(t),wfr=r(HLe," (ConvBERT model)"),HLe.forEach(t),Afr=i(oe),j4=s(oe,"LI",{});var ULe=n(j4);npe=s(ULe,"STRONG",{});var jct=n(npe);Lfr=r(jct,"deberta"),jct.forEach(t),Bfr=r(ULe," \u2014 "),aO=s(ULe,"A",{href:!0});var Nct=n(aO);xfr=r(Nct,"TFDebertaForTokenClassification"),Nct.forEach(t),kfr=r(ULe," (DeBERTa model)"),ULe.forEach(t),Rfr=i(oe),N4=s(oe,"LI",{});var JLe=n(N4);lpe=s(JLe,"STRONG",{});var qct=n(lpe);Sfr=r(qct,"deberta-v2"),qct.forEach(t),Pfr=r(JLe," \u2014 "),sO=s(JLe,"A",{href:!0});var Gct=n(sO);$fr=r(Gct,"TFDebertaV2ForTokenClassification"),Gct.forEach(t),Ifr=r(JLe," (DeBERTa-v2 model)"),JLe.forEach(t),Dfr=i(oe),q4=s(oe,"LI",{});var YLe=n(q4);ipe=s(YLe,"STRONG",{});var Oct=n(ipe);jfr=r(Oct,"distilbert"),Oct.forEach(t),Nfr=r(YLe," \u2014 "),nO=s(YLe,"A",{href:!0});var Xct=n(nO);qfr=r(Xct,"TFDistilBertForTokenClassification"),Xct.forEach(t),Gfr=r(YLe," (DistilBERT model)"),YLe.forEach(t),Ofr=i(oe),G4=s(oe,"LI",{});var KLe=n(G4);dpe=s(KLe,"STRONG",{});var Vct=n(dpe);Xfr=r(Vct,"electra"),Vct.forEach(t),Vfr=r(KLe," \u2014 "),lO=s(KLe,"A",{href:!0});var zct=n(lO);zfr=r(zct,"TFElectraForTokenClassification"),zct.forEach(t),Wfr=r(KLe," (ELECTRA model)"),KLe.forEach(t),Qfr=i(oe),O4=s(oe,"LI",{});var ZLe=n(O4);cpe=s(ZLe,"STRONG",{});var Wct=n(cpe);Hfr=r(Wct,"flaubert"),Wct.forEach(t),Ufr=r(ZLe," \u2014 "),iO=s(ZLe,"A",{href:!0});var Qct=n(iO);Jfr=r(Qct,"TFFlaubertForTokenClassification"),Qct.forEach(t),Yfr=r(ZLe," (FlauBERT model)"),ZLe.forEach(t),Kfr=i(oe),X4=s(oe,"LI",{});var e8e=n(X4);mpe=s(e8e,"STRONG",{});var Hct=n(mpe);Zfr=r(Hct,"funnel"),Hct.forEach(t),egr=r(e8e," \u2014 "),dO=s(e8e,"A",{href:!0});var Uct=n(dO);ogr=r(Uct,"TFFunnelForTokenClassification"),Uct.forEach(t),rgr=r(e8e," (Funnel Transformer model)"),e8e.forEach(t),tgr=i(oe),V4=s(oe,"LI",{});var o8e=n(V4);fpe=s(o8e,"STRONG",{});var Jct=n(fpe);agr=r(Jct,"layoutlm"),Jct.forEach(t),sgr=r(o8e," \u2014 "),cO=s(o8e,"A",{href:!0});var Yct=n(cO);ngr=r(Yct,"TFLayoutLMForTokenClassification"),Yct.forEach(t),lgr=r(o8e," (LayoutLM model)"),o8e.forEach(t),igr=i(oe),z4=s(oe,"LI",{});var r8e=n(z4);gpe=s(r8e,"STRONG",{});var Kct=n(gpe);dgr=r(Kct,"longformer"),Kct.forEach(t),cgr=r(r8e," \u2014 "),mO=s(r8e,"A",{href:!0});var Zct=n(mO);mgr=r(Zct,"TFLongformerForTokenClassification"),Zct.forEach(t),fgr=r(r8e," (Longformer model)"),r8e.forEach(t),ggr=i(oe),W4=s(oe,"LI",{});var t8e=n(W4);hpe=s(t8e,"STRONG",{});var emt=n(hpe);hgr=r(emt,"mobilebert"),emt.forEach(t),ugr=r(t8e," \u2014 "),fO=s(t8e,"A",{href:!0});var omt=n(fO);pgr=r(omt,"TFMobileBertForTokenClassification"),omt.forEach(t),_gr=r(t8e," (MobileBERT model)"),t8e.forEach(t),bgr=i(oe),Q4=s(oe,"LI",{});var a8e=n(Q4);upe=s(a8e,"STRONG",{});var rmt=n(upe);vgr=r(rmt,"mpnet"),rmt.forEach(t),Tgr=r(a8e," \u2014 "),gO=s(a8e,"A",{href:!0});var tmt=n(gO);Fgr=r(tmt,"TFMPNetForTokenClassification"),tmt.forEach(t),Cgr=r(a8e," (MPNet model)"),a8e.forEach(t),Mgr=i(oe),H4=s(oe,"LI",{});var s8e=n(H4);ppe=s(s8e,"STRONG",{});var amt=n(ppe);Egr=r(amt,"rembert"),amt.forEach(t),ygr=r(s8e," \u2014 "),hO=s(s8e,"A",{href:!0});var smt=n(hO);wgr=r(smt,"TFRemBertForTokenClassification"),smt.forEach(t),Agr=r(s8e," (RemBERT model)"),s8e.forEach(t),Lgr=i(oe),U4=s(oe,"LI",{});var n8e=n(U4);_pe=s(n8e,"STRONG",{});var nmt=n(_pe);Bgr=r(nmt,"roberta"),nmt.forEach(t),xgr=r(n8e," \u2014 "),uO=s(n8e,"A",{href:!0});var lmt=n(uO);kgr=r(lmt,"TFRobertaForTokenClassification"),lmt.forEach(t),Rgr=r(n8e," (RoBERTa model)"),n8e.forEach(t),Sgr=i(oe),J4=s(oe,"LI",{});var l8e=n(J4);bpe=s(l8e,"STRONG",{});var imt=n(bpe);Pgr=r(imt,"roformer"),imt.forEach(t),$gr=r(l8e," \u2014 "),pO=s(l8e,"A",{href:!0});var dmt=n(pO);Igr=r(dmt,"TFRoFormerForTokenClassification"),dmt.forEach(t),Dgr=r(l8e," (RoFormer model)"),l8e.forEach(t),jgr=i(oe),Y4=s(oe,"LI",{});var i8e=n(Y4);vpe=s(i8e,"STRONG",{});var cmt=n(vpe);Ngr=r(cmt,"xlm"),cmt.forEach(t),qgr=r(i8e," \u2014 "),_O=s(i8e,"A",{href:!0});var mmt=n(_O);Ggr=r(mmt,"TFXLMForTokenClassification"),mmt.forEach(t),Ogr=r(i8e," (XLM model)"),i8e.forEach(t),Xgr=i(oe),K4=s(oe,"LI",{});var d8e=n(K4);Tpe=s(d8e,"STRONG",{});var fmt=n(Tpe);Vgr=r(fmt,"xlm-roberta"),fmt.forEach(t),zgr=r(d8e," \u2014 "),bO=s(d8e,"A",{href:!0});var gmt=n(bO);Wgr=r(gmt,"TFXLMRobertaForTokenClassification"),gmt.forEach(t),Qgr=r(d8e," (XLM-RoBERTa model)"),d8e.forEach(t),Hgr=i(oe),Z4=s(oe,"LI",{});var c8e=n(Z4);Fpe=s(c8e,"STRONG",{});var hmt=n(Fpe);Ugr=r(hmt,"xlnet"),hmt.forEach(t),Jgr=r(c8e," \u2014 "),vO=s(c8e,"A",{href:!0});var umt=n(vO);Ygr=r(umt,"TFXLNetForTokenClassification"),umt.forEach(t),Kgr=r(c8e," (XLNet model)"),c8e.forEach(t),oe.forEach(t),Zgr=i(va),Cpe=s(va,"P",{});var pmt=n(Cpe);ehr=r(pmt,"Examples:"),pmt.forEach(t),ohr=i(va),f(c0.$$.fragment,va),va.forEach(t),Ul.forEach(t),qBe=i(c),kc=s(c,"H2",{class:!0});var Jke=n(kc);eM=s(Jke,"A",{id:!0,class:!0,href:!0});var _mt=n(eM);Mpe=s(_mt,"SPAN",{});var bmt=n(Mpe);f(m0.$$.fragment,bmt),bmt.forEach(t),_mt.forEach(t),rhr=i(Jke),Epe=s(Jke,"SPAN",{});var vmt=n(Epe);thr=r(vmt,"TFAutoModelForQuestionAnswering"),vmt.forEach(t),Jke.forEach(t),GBe=i(c),yr=s(c,"DIV",{class:!0});var Yl=n(yr);f(f0.$$.fragment,Yl),ahr=i(Yl),Rc=s(Yl,"P",{});var qz=n(Rc);shr=r(qz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),ype=s(qz,"CODE",{});var Tmt=n(ype);nhr=r(Tmt,"from_pretrained()"),Tmt.forEach(t),lhr=r(qz,"class method or the "),wpe=s(qz,"CODE",{});var Fmt=n(wpe);ihr=r(Fmt,"from_config()"),Fmt.forEach(t),dhr=r(qz,`class
method.`),qz.forEach(t),chr=i(Yl),g0=s(Yl,"P",{});var Yke=n(g0);mhr=r(Yke,"This class cannot be instantiated directly using "),Ape=s(Yke,"CODE",{});var Cmt=n(Ape);fhr=r(Cmt,"__init__()"),Cmt.forEach(t),ghr=r(Yke," (throws an error)."),Yke.forEach(t),hhr=i(Yl),bt=s(Yl,"DIV",{class:!0});var Kl=n(bt);f(h0.$$.fragment,Kl),uhr=i(Kl),Lpe=s(Kl,"P",{});var Mmt=n(Lpe);phr=r(Mmt,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),Mmt.forEach(t),_hr=i(Kl),Sc=s(Kl,"P",{});var Gz=n(Sc);bhr=r(Gz,`Note:
Loading a model from its configuration file does `),Bpe=s(Gz,"STRONG",{});var Emt=n(Bpe);vhr=r(Emt,"not"),Emt.forEach(t),Thr=r(Gz,` load the model weights. It only affects the
model\u2019s configuration. Use `),xpe=s(Gz,"CODE",{});var ymt=n(xpe);Fhr=r(ymt,"from_pretrained()"),ymt.forEach(t),Chr=r(Gz,"to load the model weights."),Gz.forEach(t),Mhr=i(Kl),kpe=s(Kl,"P",{});var wmt=n(kpe);Ehr=r(wmt,"Examples:"),wmt.forEach(t),yhr=i(Kl),f(u0.$$.fragment,Kl),Kl.forEach(t),whr=i(Yl),Mo=s(Yl,"DIV",{class:!0});var Ta=n(Mo);f(p0.$$.fragment,Ta),Ahr=i(Ta),Rpe=s(Ta,"P",{});var Amt=n(Rpe);Lhr=r(Amt,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),Amt.forEach(t),Bhr=i(Ta),vs=s(Ta,"P",{});var a5=n(vs);xhr=r(a5,"The model class to instantiate is selected based on the "),Spe=s(a5,"CODE",{});var Lmt=n(Spe);khr=r(Lmt,"model_type"),Lmt.forEach(t),Rhr=r(a5,` property of the config object (either
passed as an argument or loaded from `),Ppe=s(a5,"CODE",{});var Bmt=n(Ppe);Shr=r(Bmt,"pretrained_model_name_or_path"),Bmt.forEach(t),Phr=r(a5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),$pe=s(a5,"CODE",{});var xmt=n($pe);$hr=r(xmt,"pretrained_model_name_or_path"),xmt.forEach(t),Ihr=r(a5,":"),a5.forEach(t),Dhr=i(Ta),Z=s(Ta,"UL",{});var re=n(Z);oM=s(re,"LI",{});var m8e=n(oM);Ipe=s(m8e,"STRONG",{});var kmt=n(Ipe);jhr=r(kmt,"albert"),kmt.forEach(t),Nhr=r(m8e," \u2014 "),TO=s(m8e,"A",{href:!0});var Rmt=n(TO);qhr=r(Rmt,"TFAlbertForQuestionAnswering"),Rmt.forEach(t),Ghr=r(m8e," (ALBERT model)"),m8e.forEach(t),Ohr=i(re),rM=s(re,"LI",{});var f8e=n(rM);Dpe=s(f8e,"STRONG",{});var Smt=n(Dpe);Xhr=r(Smt,"bert"),Smt.forEach(t),Vhr=r(f8e," \u2014 "),FO=s(f8e,"A",{href:!0});var Pmt=n(FO);zhr=r(Pmt,"TFBertForQuestionAnswering"),Pmt.forEach(t),Whr=r(f8e," (BERT model)"),f8e.forEach(t),Qhr=i(re),tM=s(re,"LI",{});var g8e=n(tM);jpe=s(g8e,"STRONG",{});var $mt=n(jpe);Hhr=r($mt,"camembert"),$mt.forEach(t),Uhr=r(g8e," \u2014 "),CO=s(g8e,"A",{href:!0});var Imt=n(CO);Jhr=r(Imt,"TFCamembertForQuestionAnswering"),Imt.forEach(t),Yhr=r(g8e," (CamemBERT model)"),g8e.forEach(t),Khr=i(re),aM=s(re,"LI",{});var h8e=n(aM);Npe=s(h8e,"STRONG",{});var Dmt=n(Npe);Zhr=r(Dmt,"convbert"),Dmt.forEach(t),eur=r(h8e," \u2014 "),MO=s(h8e,"A",{href:!0});var jmt=n(MO);our=r(jmt,"TFConvBertForQuestionAnswering"),jmt.forEach(t),rur=r(h8e," (ConvBERT model)"),h8e.forEach(t),tur=i(re),sM=s(re,"LI",{});var u8e=n(sM);qpe=s(u8e,"STRONG",{});var Nmt=n(qpe);aur=r(Nmt,"deberta"),Nmt.forEach(t),sur=r(u8e," \u2014 "),EO=s(u8e,"A",{href:!0});var qmt=n(EO);nur=r(qmt,"TFDebertaForQuestionAnswering"),qmt.forEach(t),lur=r(u8e," (DeBERTa model)"),u8e.forEach(t),iur=i(re),nM=s(re,"LI",{});var p8e=n(nM);Gpe=s(p8e,"STRONG",{});var Gmt=n(Gpe);dur=r(Gmt,"deberta-v2"),Gmt.forEach(t),cur=r(p8e," \u2014 "),yO=s(p8e,"A",{href:!0});var Omt=n(yO);mur=r(Omt,"TFDebertaV2ForQuestionAnswering"),Omt.forEach(t),fur=r(p8e," (DeBERTa-v2 model)"),p8e.forEach(t),gur=i(re),lM=s(re,"LI",{});var _8e=n(lM);Ope=s(_8e,"STRONG",{});var Xmt=n(Ope);hur=r(Xmt,"distilbert"),Xmt.forEach(t),uur=r(_8e," \u2014 "),wO=s(_8e,"A",{href:!0});var Vmt=n(wO);pur=r(Vmt,"TFDistilBertForQuestionAnswering"),Vmt.forEach(t),_ur=r(_8e," (DistilBERT model)"),_8e.forEach(t),bur=i(re),iM=s(re,"LI",{});var b8e=n(iM);Xpe=s(b8e,"STRONG",{});var zmt=n(Xpe);vur=r(zmt,"electra"),zmt.forEach(t),Tur=r(b8e," \u2014 "),AO=s(b8e,"A",{href:!0});var Wmt=n(AO);Fur=r(Wmt,"TFElectraForQuestionAnswering"),Wmt.forEach(t),Cur=r(b8e," (ELECTRA model)"),b8e.forEach(t),Mur=i(re),dM=s(re,"LI",{});var v8e=n(dM);Vpe=s(v8e,"STRONG",{});var Qmt=n(Vpe);Eur=r(Qmt,"flaubert"),Qmt.forEach(t),yur=r(v8e," \u2014 "),LO=s(v8e,"A",{href:!0});var Hmt=n(LO);wur=r(Hmt,"TFFlaubertForQuestionAnsweringSimple"),Hmt.forEach(t),Aur=r(v8e," (FlauBERT model)"),v8e.forEach(t),Lur=i(re),cM=s(re,"LI",{});var T8e=n(cM);zpe=s(T8e,"STRONG",{});var Umt=n(zpe);Bur=r(Umt,"funnel"),Umt.forEach(t),xur=r(T8e," \u2014 "),BO=s(T8e,"A",{href:!0});var Jmt=n(BO);kur=r(Jmt,"TFFunnelForQuestionAnswering"),Jmt.forEach(t),Rur=r(T8e," (Funnel Transformer model)"),T8e.forEach(t),Sur=i(re),mM=s(re,"LI",{});var F8e=n(mM);Wpe=s(F8e,"STRONG",{});var Ymt=n(Wpe);Pur=r(Ymt,"longformer"),Ymt.forEach(t),$ur=r(F8e," \u2014 "),xO=s(F8e,"A",{href:!0});var Kmt=n(xO);Iur=r(Kmt,"TFLongformerForQuestionAnswering"),Kmt.forEach(t),Dur=r(F8e," (Longformer model)"),F8e.forEach(t),jur=i(re),fM=s(re,"LI",{});var C8e=n(fM);Qpe=s(C8e,"STRONG",{});var Zmt=n(Qpe);Nur=r(Zmt,"mobilebert"),Zmt.forEach(t),qur=r(C8e," \u2014 "),kO=s(C8e,"A",{href:!0});var eft=n(kO);Gur=r(eft,"TFMobileBertForQuestionAnswering"),eft.forEach(t),Our=r(C8e," (MobileBERT model)"),C8e.forEach(t),Xur=i(re),gM=s(re,"LI",{});var M8e=n(gM);Hpe=s(M8e,"STRONG",{});var oft=n(Hpe);Vur=r(oft,"mpnet"),oft.forEach(t),zur=r(M8e," \u2014 "),RO=s(M8e,"A",{href:!0});var rft=n(RO);Wur=r(rft,"TFMPNetForQuestionAnswering"),rft.forEach(t),Qur=r(M8e," (MPNet model)"),M8e.forEach(t),Hur=i(re),hM=s(re,"LI",{});var E8e=n(hM);Upe=s(E8e,"STRONG",{});var tft=n(Upe);Uur=r(tft,"rembert"),tft.forEach(t),Jur=r(E8e," \u2014 "),SO=s(E8e,"A",{href:!0});var aft=n(SO);Yur=r(aft,"TFRemBertForQuestionAnswering"),aft.forEach(t),Kur=r(E8e," (RemBERT model)"),E8e.forEach(t),Zur=i(re),uM=s(re,"LI",{});var y8e=n(uM);Jpe=s(y8e,"STRONG",{});var sft=n(Jpe);epr=r(sft,"roberta"),sft.forEach(t),opr=r(y8e," \u2014 "),PO=s(y8e,"A",{href:!0});var nft=n(PO);rpr=r(nft,"TFRobertaForQuestionAnswering"),nft.forEach(t),tpr=r(y8e," (RoBERTa model)"),y8e.forEach(t),apr=i(re),pM=s(re,"LI",{});var w8e=n(pM);Ype=s(w8e,"STRONG",{});var lft=n(Ype);spr=r(lft,"roformer"),lft.forEach(t),npr=r(w8e," \u2014 "),$O=s(w8e,"A",{href:!0});var ift=n($O);lpr=r(ift,"TFRoFormerForQuestionAnswering"),ift.forEach(t),ipr=r(w8e," (RoFormer model)"),w8e.forEach(t),dpr=i(re),_M=s(re,"LI",{});var A8e=n(_M);Kpe=s(A8e,"STRONG",{});var dft=n(Kpe);cpr=r(dft,"xlm"),dft.forEach(t),mpr=r(A8e," \u2014 "),IO=s(A8e,"A",{href:!0});var cft=n(IO);fpr=r(cft,"TFXLMForQuestionAnsweringSimple"),cft.forEach(t),gpr=r(A8e," (XLM model)"),A8e.forEach(t),hpr=i(re),bM=s(re,"LI",{});var L8e=n(bM);Zpe=s(L8e,"STRONG",{});var mft=n(Zpe);upr=r(mft,"xlm-roberta"),mft.forEach(t),ppr=r(L8e," \u2014 "),DO=s(L8e,"A",{href:!0});var fft=n(DO);_pr=r(fft,"TFXLMRobertaForQuestionAnswering"),fft.forEach(t),bpr=r(L8e," (XLM-RoBERTa model)"),L8e.forEach(t),vpr=i(re),vM=s(re,"LI",{});var B8e=n(vM);e_e=s(B8e,"STRONG",{});var gft=n(e_e);Tpr=r(gft,"xlnet"),gft.forEach(t),Fpr=r(B8e," \u2014 "),jO=s(B8e,"A",{href:!0});var hft=n(jO);Cpr=r(hft,"TFXLNetForQuestionAnsweringSimple"),hft.forEach(t),Mpr=r(B8e," (XLNet model)"),B8e.forEach(t),re.forEach(t),Epr=i(Ta),o_e=s(Ta,"P",{});var uft=n(o_e);ypr=r(uft,"Examples:"),uft.forEach(t),wpr=i(Ta),f(_0.$$.fragment,Ta),Ta.forEach(t),Yl.forEach(t),OBe=i(c),Pc=s(c,"H2",{class:!0});var Kke=n(Pc);TM=s(Kke,"A",{id:!0,class:!0,href:!0});var pft=n(TM);r_e=s(pft,"SPAN",{});var _ft=n(r_e);f(b0.$$.fragment,_ft),_ft.forEach(t),pft.forEach(t),Apr=i(Kke),t_e=s(Kke,"SPAN",{});var bft=n(t_e);Lpr=r(bft,"TFAutoModelForVision2Seq"),bft.forEach(t),Kke.forEach(t),XBe=i(c),wr=s(c,"DIV",{class:!0});var Zl=n(wr);f(v0.$$.fragment,Zl),Bpr=i(Zl),$c=s(Zl,"P",{});var Oz=n($c);xpr=r(Oz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),a_e=s(Oz,"CODE",{});var vft=n(a_e);kpr=r(vft,"from_pretrained()"),vft.forEach(t),Rpr=r(Oz,"class method or the "),s_e=s(Oz,"CODE",{});var Tft=n(s_e);Spr=r(Tft,"from_config()"),Tft.forEach(t),Ppr=r(Oz,`class
method.`),Oz.forEach(t),$pr=i(Zl),T0=s(Zl,"P",{});var Zke=n(T0);Ipr=r(Zke,"This class cannot be instantiated directly using "),n_e=s(Zke,"CODE",{});var Fft=n(n_e);Dpr=r(Fft,"__init__()"),Fft.forEach(t),jpr=r(Zke," (throws an error)."),Zke.forEach(t),Npr=i(Zl),vt=s(Zl,"DIV",{class:!0});var ei=n(vt);f(F0.$$.fragment,ei),qpr=i(ei),l_e=s(ei,"P",{});var Cft=n(l_e);Gpr=r(Cft,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),Cft.forEach(t),Opr=i(ei),Ic=s(ei,"P",{});var Xz=n(Ic);Xpr=r(Xz,`Note:
Loading a model from its configuration file does `),i_e=s(Xz,"STRONG",{});var Mft=n(i_e);Vpr=r(Mft,"not"),Mft.forEach(t),zpr=r(Xz,` load the model weights. It only affects the
model\u2019s configuration. Use `),d_e=s(Xz,"CODE",{});var Eft=n(d_e);Wpr=r(Eft,"from_pretrained()"),Eft.forEach(t),Qpr=r(Xz,"to load the model weights."),Xz.forEach(t),Hpr=i(ei),c_e=s(ei,"P",{});var yft=n(c_e);Upr=r(yft,"Examples:"),yft.forEach(t),Jpr=i(ei),f(C0.$$.fragment,ei),ei.forEach(t),Ypr=i(Zl),Eo=s(Zl,"DIV",{class:!0});var Fa=n(Eo);f(M0.$$.fragment,Fa),Kpr=i(Fa),m_e=s(Fa,"P",{});var wft=n(m_e);Zpr=r(wft,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),wft.forEach(t),e_r=i(Fa),Ts=s(Fa,"P",{});var s5=n(Ts);o_r=r(s5,"The model class to instantiate is selected based on the "),f_e=s(s5,"CODE",{});var Aft=n(f_e);r_r=r(Aft,"model_type"),Aft.forEach(t),t_r=r(s5,` property of the config object (either
passed as an argument or loaded from `),g_e=s(s5,"CODE",{});var Lft=n(g_e);a_r=r(Lft,"pretrained_model_name_or_path"),Lft.forEach(t),s_r=r(s5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),h_e=s(s5,"CODE",{});var Bft=n(h_e);n_r=r(Bft,"pretrained_model_name_or_path"),Bft.forEach(t),l_r=r(s5,":"),s5.forEach(t),i_r=i(Fa),u_e=s(Fa,"UL",{});var xft=n(u_e);FM=s(xft,"LI",{});var x8e=n(FM);p_e=s(x8e,"STRONG",{});var kft=n(p_e);d_r=r(kft,"vision-encoder-decoder"),kft.forEach(t),c_r=r(x8e," \u2014 "),NO=s(x8e,"A",{href:!0});var Rft=n(NO);m_r=r(Rft,"TFVisionEncoderDecoderModel"),Rft.forEach(t),f_r=r(x8e," (Vision Encoder decoder model)"),x8e.forEach(t),xft.forEach(t),g_r=i(Fa),__e=s(Fa,"P",{});var Sft=n(__e);h_r=r(Sft,"Examples:"),Sft.forEach(t),u_r=i(Fa),f(E0.$$.fragment,Fa),Fa.forEach(t),Zl.forEach(t),VBe=i(c),Dc=s(c,"H2",{class:!0});var eRe=n(Dc);CM=s(eRe,"A",{id:!0,class:!0,href:!0});var Pft=n(CM);b_e=s(Pft,"SPAN",{});var $ft=n(b_e);f(y0.$$.fragment,$ft),$ft.forEach(t),Pft.forEach(t),p_r=i(eRe),v_e=s(eRe,"SPAN",{});var Ift=n(v_e);__r=r(Ift,"TFAutoModelForSpeechSeq2Seq"),Ift.forEach(t),eRe.forEach(t),zBe=i(c),Ar=s(c,"DIV",{class:!0});var oi=n(Ar);f(w0.$$.fragment,oi),b_r=i(oi),jc=s(oi,"P",{});var Vz=n(jc);v_r=r(Vz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the `),T_e=s(Vz,"CODE",{});var Dft=n(T_e);T_r=r(Dft,"from_pretrained()"),Dft.forEach(t),F_r=r(Vz,"class method or the "),F_e=s(Vz,"CODE",{});var jft=n(F_e);C_r=r(jft,"from_config()"),jft.forEach(t),M_r=r(Vz,`class
method.`),Vz.forEach(t),E_r=i(oi),A0=s(oi,"P",{});var oRe=n(A0);y_r=r(oRe,"This class cannot be instantiated directly using "),C_e=s(oRe,"CODE",{});var Nft=n(C_e);w_r=r(Nft,"__init__()"),Nft.forEach(t),A_r=r(oRe," (throws an error)."),oRe.forEach(t),L_r=i(oi),Tt=s(oi,"DIV",{class:!0});var ri=n(Tt);f(L0.$$.fragment,ri),B_r=i(ri),M_e=s(ri,"P",{});var qft=n(M_e);x_r=r(qft,"Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration."),qft.forEach(t),k_r=i(ri),Nc=s(ri,"P",{});var zz=n(Nc);R_r=r(zz,`Note:
Loading a model from its configuration file does `),E_e=s(zz,"STRONG",{});var Gft=n(E_e);S_r=r(Gft,"not"),Gft.forEach(t),P_r=r(zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),y_e=s(zz,"CODE",{});var Oft=n(y_e);$_r=r(Oft,"from_pretrained()"),Oft.forEach(t),I_r=r(zz,"to load the model weights."),zz.forEach(t),D_r=i(ri),w_e=s(ri,"P",{});var Xft=n(w_e);j_r=r(Xft,"Examples:"),Xft.forEach(t),N_r=i(ri),f(B0.$$.fragment,ri),ri.forEach(t),q_r=i(oi),yo=s(oi,"DIV",{class:!0});var Ca=n(yo);f(x0.$$.fragment,Ca),G_r=i(Ca),A_e=s(Ca,"P",{});var Vft=n(A_e);O_r=r(Vft,"Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model."),Vft.forEach(t),X_r=i(Ca),Fs=s(Ca,"P",{});var n5=n(Fs);V_r=r(n5,"The model class to instantiate is selected based on the "),L_e=s(n5,"CODE",{});var zft=n(L_e);z_r=r(zft,"model_type"),zft.forEach(t),W_r=r(n5,` property of the config object (either
passed as an argument or loaded from `),B_e=s(n5,"CODE",{});var Wft=n(B_e);Q_r=r(Wft,"pretrained_model_name_or_path"),Wft.forEach(t),H_r=r(n5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),x_e=s(n5,"CODE",{});var Qft=n(x_e);U_r=r(Qft,"pretrained_model_name_or_path"),Qft.forEach(t),J_r=r(n5,":"),n5.forEach(t),Y_r=i(Ca),k_e=s(Ca,"UL",{});var Hft=n(k_e);MM=s(Hft,"LI",{});var k8e=n(MM);R_e=s(k8e,"STRONG",{});var Uft=n(R_e);K_r=r(Uft,"speech_to_text"),Uft.forEach(t),Z_r=r(k8e," \u2014 "),qO=s(k8e,"A",{href:!0});var Jft=n(qO);ebr=r(Jft,"TFSpeech2TextForConditionalGeneration"),Jft.forEach(t),obr=r(k8e," (Speech2Text model)"),k8e.forEach(t),Hft.forEach(t),rbr=i(Ca),S_e=s(Ca,"P",{});var Yft=n(S_e);tbr=r(Yft,"Examples:"),Yft.forEach(t),abr=i(Ca),f(k0.$$.fragment,Ca),Ca.forEach(t),oi.forEach(t),WBe=i(c),qc=s(c,"H2",{class:!0});var rRe=n(qc);EM=s(rRe,"A",{id:!0,class:!0,href:!0});var Kft=n(EM);P_e=s(Kft,"SPAN",{});var Zft=n(P_e);f(R0.$$.fragment,Zft),Zft.forEach(t),Kft.forEach(t),sbr=i(rRe),$_e=s(rRe,"SPAN",{});var egt=n($_e);nbr=r(egt,"FlaxAutoModel"),egt.forEach(t),rRe.forEach(t),QBe=i(c),Lr=s(c,"DIV",{class:!0});var ti=n(Lr);f(S0.$$.fragment,ti),lbr=i(ti),Gc=s(ti,"P",{});var Wz=n(Gc);ibr=r(Wz,`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the `),I_e=s(Wz,"CODE",{});var ogt=n(I_e);dbr=r(ogt,"from_pretrained()"),ogt.forEach(t),cbr=r(Wz,"class method or the "),D_e=s(Wz,"CODE",{});var rgt=n(D_e);mbr=r(rgt,"from_config()"),rgt.forEach(t),fbr=r(Wz,`class
method.`),Wz.forEach(t),gbr=i(ti),P0=s(ti,"P",{});var tRe=n(P0);hbr=r(tRe,"This class cannot be instantiated directly using "),j_e=s(tRe,"CODE",{});var tgt=n(j_e);ubr=r(tgt,"__init__()"),tgt.forEach(t),pbr=r(tRe," (throws an error)."),tRe.forEach(t),_br=i(ti),Ft=s(ti,"DIV",{class:!0});var ai=n(Ft);f($0.$$.fragment,ai),bbr=i(ai),N_e=s(ai,"P",{});var agt=n(N_e);vbr=r(agt,"Instantiates one of the base model classes of the library from a configuration."),agt.forEach(t),Tbr=i(ai),Oc=s(ai,"P",{});var Qz=n(Oc);Fbr=r(Qz,`Note:
Loading a model from its configuration file does `),q_e=s(Qz,"STRONG",{});var sgt=n(q_e);Cbr=r(sgt,"not"),sgt.forEach(t),Mbr=r(Qz,` load the model weights. It only affects the
model\u2019s configuration. Use `),G_e=s(Qz,"CODE",{});var ngt=n(G_e);Ebr=r(ngt,"from_pretrained()"),ngt.forEach(t),ybr=r(Qz,"to load the model weights."),Qz.forEach(t),wbr=i(ai),O_e=s(ai,"P",{});var lgt=n(O_e);Abr=r(lgt,"Examples:"),lgt.forEach(t),Lbr=i(ai),f(I0.$$.fragment,ai),ai.forEach(t),Bbr=i(ti),wo=s(ti,"DIV",{class:!0});var Ma=n(wo);f(D0.$$.fragment,Ma),xbr=i(Ma),X_e=s(Ma,"P",{});var igt=n(X_e);kbr=r(igt,"Instantiate one of the base model classes of the library from a pretrained model."),igt.forEach(t),Rbr=i(Ma),Cs=s(Ma,"P",{});var l5=n(Cs);Sbr=r(l5,"The model class to instantiate is selected based on the "),V_e=s(l5,"CODE",{});var dgt=n(V_e);Pbr=r(dgt,"model_type"),dgt.forEach(t),$br=r(l5,` property of the config object (either
passed as an argument or loaded from `),z_e=s(l5,"CODE",{});var cgt=n(z_e);Ibr=r(cgt,"pretrained_model_name_or_path"),cgt.forEach(t),Dbr=r(l5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),W_e=s(l5,"CODE",{});var mgt=n(W_e);jbr=r(mgt,"pretrained_model_name_or_path"),mgt.forEach(t),Nbr=r(l5,":"),l5.forEach(t),qbr=i(Ma),z=s(Ma,"UL",{});var Q=n(z);yM=s(Q,"LI",{});var R8e=n(yM);Q_e=s(R8e,"STRONG",{});var fgt=n(Q_e);Gbr=r(fgt,"albert"),fgt.forEach(t),Obr=r(R8e," \u2014 "),GO=s(R8e,"A",{href:!0});var ggt=n(GO);Xbr=r(ggt,"FlaxAlbertModel"),ggt.forEach(t),Vbr=r(R8e," (ALBERT model)"),R8e.forEach(t),zbr=i(Q),wM=s(Q,"LI",{});var S8e=n(wM);H_e=s(S8e,"STRONG",{});var hgt=n(H_e);Wbr=r(hgt,"bart"),hgt.forEach(t),Qbr=r(S8e," \u2014 "),OO=s(S8e,"A",{href:!0});var ugt=n(OO);Hbr=r(ugt,"FlaxBartModel"),ugt.forEach(t),Ubr=r(S8e," (BART model)"),S8e.forEach(t),Jbr=i(Q),AM=s(Q,"LI",{});var P8e=n(AM);U_e=s(P8e,"STRONG",{});var pgt=n(U_e);Ybr=r(pgt,"beit"),pgt.forEach(t),Kbr=r(P8e," \u2014 "),XO=s(P8e,"A",{href:!0});var _gt=n(XO);Zbr=r(_gt,"FlaxBeitModel"),_gt.forEach(t),e2r=r(P8e," (BEiT model)"),P8e.forEach(t),o2r=i(Q),LM=s(Q,"LI",{});var $8e=n(LM);J_e=s($8e,"STRONG",{});var bgt=n(J_e);r2r=r(bgt,"bert"),bgt.forEach(t),t2r=r($8e," \u2014 "),VO=s($8e,"A",{href:!0});var vgt=n(VO);a2r=r(vgt,"FlaxBertModel"),vgt.forEach(t),s2r=r($8e," (BERT model)"),$8e.forEach(t),n2r=i(Q),BM=s(Q,"LI",{});var I8e=n(BM);Y_e=s(I8e,"STRONG",{});var Tgt=n(Y_e);l2r=r(Tgt,"big_bird"),Tgt.forEach(t),i2r=r(I8e," \u2014 "),zO=s(I8e,"A",{href:!0});var Fgt=n(zO);d2r=r(Fgt,"FlaxBigBirdModel"),Fgt.forEach(t),c2r=r(I8e," (BigBird model)"),I8e.forEach(t),m2r=i(Q),xM=s(Q,"LI",{});var D8e=n(xM);K_e=s(D8e,"STRONG",{});var Cgt=n(K_e);f2r=r(Cgt,"blenderbot"),Cgt.forEach(t),g2r=r(D8e," \u2014 "),WO=s(D8e,"A",{href:!0});var Mgt=n(WO);h2r=r(Mgt,"FlaxBlenderbotModel"),Mgt.forEach(t),u2r=r(D8e," (Blenderbot model)"),D8e.forEach(t),p2r=i(Q),kM=s(Q,"LI",{});var j8e=n(kM);Z_e=s(j8e,"STRONG",{});var Egt=n(Z_e);_2r=r(Egt,"blenderbot-small"),Egt.forEach(t),b2r=r(j8e," \u2014 "),QO=s(j8e,"A",{href:!0});var ygt=n(QO);v2r=r(ygt,"FlaxBlenderbotSmallModel"),ygt.forEach(t),T2r=r(j8e," (BlenderbotSmall model)"),j8e.forEach(t),F2r=i(Q),RM=s(Q,"LI",{});var N8e=n(RM);ebe=s(N8e,"STRONG",{});var wgt=n(ebe);C2r=r(wgt,"clip"),wgt.forEach(t),M2r=r(N8e," \u2014 "),HO=s(N8e,"A",{href:!0});var Agt=n(HO);E2r=r(Agt,"FlaxCLIPModel"),Agt.forEach(t),y2r=r(N8e," (CLIP model)"),N8e.forEach(t),w2r=i(Q),SM=s(Q,"LI",{});var q8e=n(SM);obe=s(q8e,"STRONG",{});var Lgt=n(obe);A2r=r(Lgt,"distilbert"),Lgt.forEach(t),L2r=r(q8e," \u2014 "),UO=s(q8e,"A",{href:!0});var Bgt=n(UO);B2r=r(Bgt,"FlaxDistilBertModel"),Bgt.forEach(t),x2r=r(q8e," (DistilBERT model)"),q8e.forEach(t),k2r=i(Q),PM=s(Q,"LI",{});var G8e=n(PM);rbe=s(G8e,"STRONG",{});var xgt=n(rbe);R2r=r(xgt,"electra"),xgt.forEach(t),S2r=r(G8e," \u2014 "),JO=s(G8e,"A",{href:!0});var kgt=n(JO);P2r=r(kgt,"FlaxElectraModel"),kgt.forEach(t),$2r=r(G8e," (ELECTRA model)"),G8e.forEach(t),I2r=i(Q),$M=s(Q,"LI",{});var O8e=n($M);tbe=s(O8e,"STRONG",{});var Rgt=n(tbe);D2r=r(Rgt,"gpt2"),Rgt.forEach(t),j2r=r(O8e," \u2014 "),YO=s(O8e,"A",{href:!0});var Sgt=n(YO);N2r=r(Sgt,"FlaxGPT2Model"),Sgt.forEach(t),q2r=r(O8e," (OpenAI GPT-2 model)"),O8e.forEach(t),G2r=i(Q),IM=s(Q,"LI",{});var X8e=n(IM);abe=s(X8e,"STRONG",{});var Pgt=n(abe);O2r=r(Pgt,"gpt_neo"),Pgt.forEach(t),X2r=r(X8e," \u2014 "),KO=s(X8e,"A",{href:!0});var $gt=n(KO);V2r=r($gt,"FlaxGPTNeoModel"),$gt.forEach(t),z2r=r(X8e," (GPT Neo model)"),X8e.forEach(t),W2r=i(Q),DM=s(Q,"LI",{});var V8e=n(DM);sbe=s(V8e,"STRONG",{});var Igt=n(sbe);Q2r=r(Igt,"gptj"),Igt.forEach(t),H2r=r(V8e," \u2014 "),ZO=s(V8e,"A",{href:!0});var Dgt=n(ZO);U2r=r(Dgt,"FlaxGPTJModel"),Dgt.forEach(t),J2r=r(V8e," (GPT-J model)"),V8e.forEach(t),Y2r=i(Q),jM=s(Q,"LI",{});var z8e=n(jM);nbe=s(z8e,"STRONG",{});var jgt=n(nbe);K2r=r(jgt,"marian"),jgt.forEach(t),Z2r=r(z8e," \u2014 "),eX=s(z8e,"A",{href:!0});var Ngt=n(eX);evr=r(Ngt,"FlaxMarianModel"),Ngt.forEach(t),ovr=r(z8e," (Marian model)"),z8e.forEach(t),rvr=i(Q),NM=s(Q,"LI",{});var W8e=n(NM);lbe=s(W8e,"STRONG",{});var qgt=n(lbe);tvr=r(qgt,"mbart"),qgt.forEach(t),avr=r(W8e," \u2014 "),oX=s(W8e,"A",{href:!0});var Ggt=n(oX);svr=r(Ggt,"FlaxMBartModel"),Ggt.forEach(t),nvr=r(W8e," (mBART model)"),W8e.forEach(t),lvr=i(Q),qM=s(Q,"LI",{});var Q8e=n(qM);ibe=s(Q8e,"STRONG",{});var Ogt=n(ibe);ivr=r(Ogt,"mt5"),Ogt.forEach(t),dvr=r(Q8e," \u2014 "),rX=s(Q8e,"A",{href:!0});var Xgt=n(rX);cvr=r(Xgt,"FlaxMT5Model"),Xgt.forEach(t),mvr=r(Q8e," (mT5 model)"),Q8e.forEach(t),fvr=i(Q),GM=s(Q,"LI",{});var H8e=n(GM);dbe=s(H8e,"STRONG",{});var Vgt=n(dbe);gvr=r(Vgt,"pegasus"),Vgt.forEach(t),hvr=r(H8e," \u2014 "),tX=s(H8e,"A",{href:!0});var zgt=n(tX);uvr=r(zgt,"FlaxPegasusModel"),zgt.forEach(t),pvr=r(H8e," (Pegasus model)"),H8e.forEach(t),_vr=i(Q),OM=s(Q,"LI",{});var U8e=n(OM);cbe=s(U8e,"STRONG",{});var Wgt=n(cbe);bvr=r(Wgt,"roberta"),Wgt.forEach(t),vvr=r(U8e," \u2014 "),aX=s(U8e,"A",{href:!0});var Qgt=n(aX);Tvr=r(Qgt,"FlaxRobertaModel"),Qgt.forEach(t),Fvr=r(U8e," (RoBERTa model)"),U8e.forEach(t),Cvr=i(Q),XM=s(Q,"LI",{});var J8e=n(XM);mbe=s(J8e,"STRONG",{});var Hgt=n(mbe);Mvr=r(Hgt,"roformer"),Hgt.forEach(t),Evr=r(J8e," \u2014 "),sX=s(J8e,"A",{href:!0});var Ugt=n(sX);yvr=r(Ugt,"FlaxRoFormerModel"),Ugt.forEach(t),wvr=r(J8e," (RoFormer model)"),J8e.forEach(t),Avr=i(Q),VM=s(Q,"LI",{});var Y8e=n(VM);fbe=s(Y8e,"STRONG",{});var Jgt=n(fbe);Lvr=r(Jgt,"t5"),Jgt.forEach(t),Bvr=r(Y8e," \u2014 "),nX=s(Y8e,"A",{href:!0});var Ygt=n(nX);xvr=r(Ygt,"FlaxT5Model"),Ygt.forEach(t),kvr=r(Y8e," (T5 model)"),Y8e.forEach(t),Rvr=i(Q),zM=s(Q,"LI",{});var K8e=n(zM);gbe=s(K8e,"STRONG",{});var Kgt=n(gbe);Svr=r(Kgt,"vision-text-dual-encoder"),Kgt.forEach(t),Pvr=r(K8e," \u2014 "),lX=s(K8e,"A",{href:!0});var Zgt=n(lX);$vr=r(Zgt,"FlaxVisionTextDualEncoderModel"),Zgt.forEach(t),Ivr=r(K8e," (VisionTextDualEncoder model)"),K8e.forEach(t),Dvr=i(Q),WM=s(Q,"LI",{});var Z8e=n(WM);hbe=s(Z8e,"STRONG",{});var eht=n(hbe);jvr=r(eht,"vit"),eht.forEach(t),Nvr=r(Z8e," \u2014 "),iX=s(Z8e,"A",{href:!0});var oht=n(iX);qvr=r(oht,"FlaxViTModel"),oht.forEach(t),Gvr=r(Z8e," (ViT model)"),Z8e.forEach(t),Ovr=i(Q),QM=s(Q,"LI",{});var e7e=n(QM);ube=s(e7e,"STRONG",{});var rht=n(ube);Xvr=r(rht,"wav2vec2"),rht.forEach(t),Vvr=r(e7e," \u2014 "),dX=s(e7e,"A",{href:!0});var tht=n(dX);zvr=r(tht,"FlaxWav2Vec2Model"),tht.forEach(t),Wvr=r(e7e," (Wav2Vec2 model)"),e7e.forEach(t),Qvr=i(Q),HM=s(Q,"LI",{});var o7e=n(HM);pbe=s(o7e,"STRONG",{});var aht=n(pbe);Hvr=r(aht,"xglm"),aht.forEach(t),Uvr=r(o7e," \u2014 "),cX=s(o7e,"A",{href:!0});var sht=n(cX);Jvr=r(sht,"FlaxXGLMModel"),sht.forEach(t),Yvr=r(o7e," (XGLM model)"),o7e.forEach(t),Q.forEach(t),Kvr=i(Ma),_be=s(Ma,"P",{});var nht=n(_be);Zvr=r(nht,"Examples:"),nht.forEach(t),eTr=i(Ma),f(j0.$$.fragment,Ma),Ma.forEach(t),ti.forEach(t),HBe=i(c),Xc=s(c,"H2",{class:!0});var aRe=n(Xc);UM=s(aRe,"A",{id:!0,class:!0,href:!0});var lht=n(UM);bbe=s(lht,"SPAN",{});var iht=n(bbe);f(N0.$$.fragment,iht),iht.forEach(t),lht.forEach(t),oTr=i(aRe),vbe=s(aRe,"SPAN",{});var dht=n(vbe);rTr=r(dht,"FlaxAutoModelForCausalLM"),dht.forEach(t),aRe.forEach(t),UBe=i(c),Br=s(c,"DIV",{class:!0});var si=n(Br);f(q0.$$.fragment,si),tTr=i(si),Vc=s(si,"P",{});var Hz=n(Vc);aTr=r(Hz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the `),Tbe=s(Hz,"CODE",{});var cht=n(Tbe);sTr=r(cht,"from_pretrained()"),cht.forEach(t),nTr=r(Hz,"class method or the "),Fbe=s(Hz,"CODE",{});var mht=n(Fbe);lTr=r(mht,"from_config()"),mht.forEach(t),iTr=r(Hz,`class
method.`),Hz.forEach(t),dTr=i(si),G0=s(si,"P",{});var sRe=n(G0);cTr=r(sRe,"This class cannot be instantiated directly using "),Cbe=s(sRe,"CODE",{});var fht=n(Cbe);mTr=r(fht,"__init__()"),fht.forEach(t),fTr=r(sRe," (throws an error)."),sRe.forEach(t),gTr=i(si),Ct=s(si,"DIV",{class:!0});var ni=n(Ct);f(O0.$$.fragment,ni),hTr=i(ni),Mbe=s(ni,"P",{});var ght=n(Mbe);uTr=r(ght,"Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration."),ght.forEach(t),pTr=i(ni),zc=s(ni,"P",{});var Uz=n(zc);_Tr=r(Uz,`Note:
Loading a model from its configuration file does `),Ebe=s(Uz,"STRONG",{});var hht=n(Ebe);bTr=r(hht,"not"),hht.forEach(t),vTr=r(Uz,` load the model weights. It only affects the
model\u2019s configuration. Use `),ybe=s(Uz,"CODE",{});var uht=n(ybe);TTr=r(uht,"from_pretrained()"),uht.forEach(t),FTr=r(Uz,"to load the model weights."),Uz.forEach(t),CTr=i(ni),wbe=s(ni,"P",{});var pht=n(wbe);MTr=r(pht,"Examples:"),pht.forEach(t),ETr=i(ni),f(X0.$$.fragment,ni),ni.forEach(t),yTr=i(si),Ao=s(si,"DIV",{class:!0});var Ea=n(Ao);f(V0.$$.fragment,Ea),wTr=i(Ea),Abe=s(Ea,"P",{});var _ht=n(Abe);ATr=r(_ht,"Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model."),_ht.forEach(t),LTr=i(Ea),Ms=s(Ea,"P",{});var i5=n(Ms);BTr=r(i5,"The model class to instantiate is selected based on the "),Lbe=s(i5,"CODE",{});var bht=n(Lbe);xTr=r(bht,"model_type"),bht.forEach(t),kTr=r(i5,` property of the config object (either
passed as an argument or loaded from `),Bbe=s(i5,"CODE",{});var vht=n(Bbe);RTr=r(vht,"pretrained_model_name_or_path"),vht.forEach(t),STr=r(i5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),xbe=s(i5,"CODE",{});var Tht=n(xbe);PTr=r(Tht,"pretrained_model_name_or_path"),Tht.forEach(t),$Tr=r(i5,":"),i5.forEach(t),ITr=i(Ea),Es=s(Ea,"UL",{});var d5=n(Es);JM=s(d5,"LI",{});var r7e=n(JM);kbe=s(r7e,"STRONG",{});var Fht=n(kbe);DTr=r(Fht,"gpt2"),Fht.forEach(t),jTr=r(r7e," \u2014 "),mX=s(r7e,"A",{href:!0});var Cht=n(mX);NTr=r(Cht,"FlaxGPT2LMHeadModel"),Cht.forEach(t),qTr=r(r7e," (OpenAI GPT-2 model)"),r7e.forEach(t),GTr=i(d5),YM=s(d5,"LI",{});var t7e=n(YM);Rbe=s(t7e,"STRONG",{});var Mht=n(Rbe);OTr=r(Mht,"gpt_neo"),Mht.forEach(t),XTr=r(t7e," \u2014 "),fX=s(t7e,"A",{href:!0});var Eht=n(fX);VTr=r(Eht,"FlaxGPTNeoForCausalLM"),Eht.forEach(t),zTr=r(t7e," (GPT Neo model)"),t7e.forEach(t),WTr=i(d5),KM=s(d5,"LI",{});var a7e=n(KM);Sbe=s(a7e,"STRONG",{});var yht=n(Sbe);QTr=r(yht,"gptj"),yht.forEach(t),HTr=r(a7e," \u2014 "),gX=s(a7e,"A",{href:!0});var wht=n(gX);UTr=r(wht,"FlaxGPTJForCausalLM"),wht.forEach(t),JTr=r(a7e," (GPT-J model)"),a7e.forEach(t),YTr=i(d5),ZM=s(d5,"LI",{});var s7e=n(ZM);Pbe=s(s7e,"STRONG",{});var Aht=n(Pbe);KTr=r(Aht,"xglm"),Aht.forEach(t),ZTr=r(s7e," \u2014 "),hX=s(s7e,"A",{href:!0});var Lht=n(hX);e1r=r(Lht,"FlaxXGLMForCausalLM"),Lht.forEach(t),o1r=r(s7e," (XGLM model)"),s7e.forEach(t),d5.forEach(t),r1r=i(Ea),$be=s(Ea,"P",{});var Bht=n($be);t1r=r(Bht,"Examples:"),Bht.forEach(t),a1r=i(Ea),f(z0.$$.fragment,Ea),Ea.forEach(t),si.forEach(t),JBe=i(c),Wc=s(c,"H2",{class:!0});var nRe=n(Wc);eE=s(nRe,"A",{id:!0,class:!0,href:!0});var xht=n(eE);Ibe=s(xht,"SPAN",{});var kht=n(Ibe);f(W0.$$.fragment,kht),kht.forEach(t),xht.forEach(t),s1r=i(nRe),Dbe=s(nRe,"SPAN",{});var Rht=n(Dbe);n1r=r(Rht,"FlaxAutoModelForPreTraining"),Rht.forEach(t),nRe.forEach(t),YBe=i(c),xr=s(c,"DIV",{class:!0});var li=n(xr);f(Q0.$$.fragment,li),l1r=i(li),Qc=s(li,"P",{});var Jz=n(Qc);i1r=r(Jz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the `),jbe=s(Jz,"CODE",{});var Sht=n(jbe);d1r=r(Sht,"from_pretrained()"),Sht.forEach(t),c1r=r(Jz,"class method or the "),Nbe=s(Jz,"CODE",{});var Pht=n(Nbe);m1r=r(Pht,"from_config()"),Pht.forEach(t),f1r=r(Jz,`class
method.`),Jz.forEach(t),g1r=i(li),H0=s(li,"P",{});var lRe=n(H0);h1r=r(lRe,"This class cannot be instantiated directly using "),qbe=s(lRe,"CODE",{});var $ht=n(qbe);u1r=r($ht,"__init__()"),$ht.forEach(t),p1r=r(lRe," (throws an error)."),lRe.forEach(t),_1r=i(li),Mt=s(li,"DIV",{class:!0});var ii=n(Mt);f(U0.$$.fragment,ii),b1r=i(ii),Gbe=s(ii,"P",{});var Iht=n(Gbe);v1r=r(Iht,"Instantiates one of the model classes of the library (with a pretraining head) from a configuration."),Iht.forEach(t),T1r=i(ii),Hc=s(ii,"P",{});var Yz=n(Hc);F1r=r(Yz,`Note:
Loading a model from its configuration file does `),Obe=s(Yz,"STRONG",{});var Dht=n(Obe);C1r=r(Dht,"not"),Dht.forEach(t),M1r=r(Yz,` load the model weights. It only affects the
model\u2019s configuration. Use `),Xbe=s(Yz,"CODE",{});var jht=n(Xbe);E1r=r(jht,"from_pretrained()"),jht.forEach(t),y1r=r(Yz,"to load the model weights."),Yz.forEach(t),w1r=i(ii),Vbe=s(ii,"P",{});var Nht=n(Vbe);A1r=r(Nht,"Examples:"),Nht.forEach(t),L1r=i(ii),f(J0.$$.fragment,ii),ii.forEach(t),B1r=i(li),Lo=s(li,"DIV",{class:!0});var ya=n(Lo);f(Y0.$$.fragment,ya),x1r=i(ya),zbe=s(ya,"P",{});var qht=n(zbe);k1r=r(qht,"Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model."),qht.forEach(t),R1r=i(ya),ys=s(ya,"P",{});var c5=n(ys);S1r=r(c5,"The model class to instantiate is selected based on the "),Wbe=s(c5,"CODE",{});var Ght=n(Wbe);P1r=r(Ght,"model_type"),Ght.forEach(t),$1r=r(c5,` property of the config object (either
passed as an argument or loaded from `),Qbe=s(c5,"CODE",{});var Oht=n(Qbe);I1r=r(Oht,"pretrained_model_name_or_path"),Oht.forEach(t),D1r=r(c5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Hbe=s(c5,"CODE",{});var Xht=n(Hbe);j1r=r(Xht,"pretrained_model_name_or_path"),Xht.forEach(t),N1r=r(c5,":"),c5.forEach(t),q1r=i(ya),me=s(ya,"UL",{});var pe=n(me);oE=s(pe,"LI",{});var n7e=n(oE);Ube=s(n7e,"STRONG",{});var Vht=n(Ube);G1r=r(Vht,"albert"),Vht.forEach(t),O1r=r(n7e," \u2014 "),uX=s(n7e,"A",{href:!0});var zht=n(uX);X1r=r(zht,"FlaxAlbertForPreTraining"),zht.forEach(t),V1r=r(n7e," (ALBERT model)"),n7e.forEach(t),z1r=i(pe),rE=s(pe,"LI",{});var l7e=n(rE);Jbe=s(l7e,"STRONG",{});var Wht=n(Jbe);W1r=r(Wht,"bart"),Wht.forEach(t),Q1r=r(l7e," \u2014 "),pX=s(l7e,"A",{href:!0});var Qht=n(pX);H1r=r(Qht,"FlaxBartForConditionalGeneration"),Qht.forEach(t),U1r=r(l7e," (BART model)"),l7e.forEach(t),J1r=i(pe),tE=s(pe,"LI",{});var i7e=n(tE);Ybe=s(i7e,"STRONG",{});var Hht=n(Ybe);Y1r=r(Hht,"bert"),Hht.forEach(t),K1r=r(i7e," \u2014 "),_X=s(i7e,"A",{href:!0});var Uht=n(_X);Z1r=r(Uht,"FlaxBertForPreTraining"),Uht.forEach(t),eFr=r(i7e," (BERT model)"),i7e.forEach(t),oFr=i(pe),aE=s(pe,"LI",{});var d7e=n(aE);Kbe=s(d7e,"STRONG",{});var Jht=n(Kbe);rFr=r(Jht,"big_bird"),Jht.forEach(t),tFr=r(d7e," \u2014 "),bX=s(d7e,"A",{href:!0});var Yht=n(bX);aFr=r(Yht,"FlaxBigBirdForPreTraining"),Yht.forEach(t),sFr=r(d7e," (BigBird model)"),d7e.forEach(t),nFr=i(pe),sE=s(pe,"LI",{});var c7e=n(sE);Zbe=s(c7e,"STRONG",{});var Kht=n(Zbe);lFr=r(Kht,"electra"),Kht.forEach(t),iFr=r(c7e," \u2014 "),vX=s(c7e,"A",{href:!0});var Zht=n(vX);dFr=r(Zht,"FlaxElectraForPreTraining"),Zht.forEach(t),cFr=r(c7e," (ELECTRA model)"),c7e.forEach(t),mFr=i(pe),nE=s(pe,"LI",{});var m7e=n(nE);e2e=s(m7e,"STRONG",{});var eut=n(e2e);fFr=r(eut,"mbart"),eut.forEach(t),gFr=r(m7e," \u2014 "),TX=s(m7e,"A",{href:!0});var out=n(TX);hFr=r(out,"FlaxMBartForConditionalGeneration"),out.forEach(t),uFr=r(m7e," (mBART model)"),m7e.forEach(t),pFr=i(pe),lE=s(pe,"LI",{});var f7e=n(lE);o2e=s(f7e,"STRONG",{});var rut=n(o2e);_Fr=r(rut,"mt5"),rut.forEach(t),bFr=r(f7e," \u2014 "),FX=s(f7e,"A",{href:!0});var tut=n(FX);vFr=r(tut,"FlaxMT5ForConditionalGeneration"),tut.forEach(t),TFr=r(f7e," (mT5 model)"),f7e.forEach(t),FFr=i(pe),iE=s(pe,"LI",{});var g7e=n(iE);r2e=s(g7e,"STRONG",{});var aut=n(r2e);CFr=r(aut,"roberta"),aut.forEach(t),MFr=r(g7e," \u2014 "),CX=s(g7e,"A",{href:!0});var sut=n(CX);EFr=r(sut,"FlaxRobertaForMaskedLM"),sut.forEach(t),yFr=r(g7e," (RoBERTa model)"),g7e.forEach(t),wFr=i(pe),dE=s(pe,"LI",{});var h7e=n(dE);t2e=s(h7e,"STRONG",{});var nut=n(t2e);AFr=r(nut,"roformer"),nut.forEach(t),LFr=r(h7e," \u2014 "),MX=s(h7e,"A",{href:!0});var lut=n(MX);BFr=r(lut,"FlaxRoFormerForMaskedLM"),lut.forEach(t),xFr=r(h7e," (RoFormer model)"),h7e.forEach(t),kFr=i(pe),cE=s(pe,"LI",{});var u7e=n(cE);a2e=s(u7e,"STRONG",{});var iut=n(a2e);RFr=r(iut,"t5"),iut.forEach(t),SFr=r(u7e," \u2014 "),EX=s(u7e,"A",{href:!0});var dut=n(EX);PFr=r(dut,"FlaxT5ForConditionalGeneration"),dut.forEach(t),$Fr=r(u7e," (T5 model)"),u7e.forEach(t),IFr=i(pe),mE=s(pe,"LI",{});var p7e=n(mE);s2e=s(p7e,"STRONG",{});var cut=n(s2e);DFr=r(cut,"wav2vec2"),cut.forEach(t),jFr=r(p7e," \u2014 "),yX=s(p7e,"A",{href:!0});var mut=n(yX);NFr=r(mut,"FlaxWav2Vec2ForPreTraining"),mut.forEach(t),qFr=r(p7e," (Wav2Vec2 model)"),p7e.forEach(t),pe.forEach(t),GFr=i(ya),n2e=s(ya,"P",{});var fut=n(n2e);OFr=r(fut,"Examples:"),fut.forEach(t),XFr=i(ya),f(K0.$$.fragment,ya),ya.forEach(t),li.forEach(t),KBe=i(c),Uc=s(c,"H2",{class:!0});var iRe=n(Uc);fE=s(iRe,"A",{id:!0,class:!0,href:!0});var gut=n(fE);l2e=s(gut,"SPAN",{});var hut=n(l2e);f(Z0.$$.fragment,hut),hut.forEach(t),gut.forEach(t),VFr=i(iRe),i2e=s(iRe,"SPAN",{});var uut=n(i2e);zFr=r(uut,"FlaxAutoModelForMaskedLM"),uut.forEach(t),iRe.forEach(t),ZBe=i(c),kr=s(c,"DIV",{class:!0});var di=n(kr);f(eL.$$.fragment,di),WFr=i(di),Jc=s(di,"P",{});var Kz=n(Jc);QFr=r(Kz,`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the `),d2e=s(Kz,"CODE",{});var put=n(d2e);HFr=r(put,"from_pretrained()"),put.forEach(t),UFr=r(Kz,"class method or the "),c2e=s(Kz,"CODE",{});var _ut=n(c2e);JFr=r(_ut,"from_config()"),_ut.forEach(t),YFr=r(Kz,`class
method.`),Kz.forEach(t),KFr=i(di),oL=s(di,"P",{});var dRe=n(oL);ZFr=r(dRe,"This class cannot be instantiated directly using "),m2e=s(dRe,"CODE",{});var but=n(m2e);eCr=r(but,"__init__()"),but.forEach(t),oCr=r(dRe," (throws an error)."),dRe.forEach(t),rCr=i(di),Et=s(di,"DIV",{class:!0});var ci=n(Et);f(rL.$$.fragment,ci),tCr=i(ci),f2e=s(ci,"P",{});var vut=n(f2e);aCr=r(vut,"Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration."),vut.forEach(t),sCr=i(ci),Yc=s(ci,"P",{});var Zz=n(Yc);nCr=r(Zz,`Note:
Loading a model from its configuration file does `),g2e=s(Zz,"STRONG",{});var Tut=n(g2e);lCr=r(Tut,"not"),Tut.forEach(t),iCr=r(Zz,` load the model weights. It only affects the
model\u2019s configuration. Use `),h2e=s(Zz,"CODE",{});var Fut=n(h2e);dCr=r(Fut,"from_pretrained()"),Fut.forEach(t),cCr=r(Zz,"to load the model weights."),Zz.forEach(t),mCr=i(ci),u2e=s(ci,"P",{});var Cut=n(u2e);fCr=r(Cut,"Examples:"),Cut.forEach(t),gCr=i(ci),f(tL.$$.fragment,ci),ci.forEach(t),hCr=i(di),Bo=s(di,"DIV",{class:!0});var wa=n(Bo);f(aL.$$.fragment,wa),uCr=i(wa),p2e=s(wa,"P",{});var Mut=n(p2e);pCr=r(Mut,"Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model."),Mut.forEach(t),_Cr=i(wa),ws=s(wa,"P",{});var m5=n(ws);bCr=r(m5,"The model class to instantiate is selected based on the "),_2e=s(m5,"CODE",{});var Eut=n(_2e);vCr=r(Eut,"model_type"),Eut.forEach(t),TCr=r(m5,` property of the config object (either
passed as an argument or loaded from `),b2e=s(m5,"CODE",{});var yut=n(b2e);FCr=r(yut,"pretrained_model_name_or_path"),yut.forEach(t),CCr=r(m5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),v2e=s(m5,"CODE",{});var wut=n(v2e);MCr=r(wut,"pretrained_model_name_or_path"),wut.forEach(t),ECr=r(m5,":"),m5.forEach(t),yCr=i(wa),ve=s(wa,"UL",{});var oo=n(ve);gE=s(oo,"LI",{});var _7e=n(gE);T2e=s(_7e,"STRONG",{});var Aut=n(T2e);wCr=r(Aut,"albert"),Aut.forEach(t),ACr=r(_7e," \u2014 "),wX=s(_7e,"A",{href:!0});var Lut=n(wX);LCr=r(Lut,"FlaxAlbertForMaskedLM"),Lut.forEach(t),BCr=r(_7e," (ALBERT model)"),_7e.forEach(t),xCr=i(oo),hE=s(oo,"LI",{});var b7e=n(hE);F2e=s(b7e,"STRONG",{});var But=n(F2e);kCr=r(But,"bart"),But.forEach(t),RCr=r(b7e," \u2014 "),AX=s(b7e,"A",{href:!0});var xut=n(AX);SCr=r(xut,"FlaxBartForConditionalGeneration"),xut.forEach(t),PCr=r(b7e," (BART model)"),b7e.forEach(t),$Cr=i(oo),uE=s(oo,"LI",{});var v7e=n(uE);C2e=s(v7e,"STRONG",{});var kut=n(C2e);ICr=r(kut,"bert"),kut.forEach(t),DCr=r(v7e," \u2014 "),LX=s(v7e,"A",{href:!0});var Rut=n(LX);jCr=r(Rut,"FlaxBertForMaskedLM"),Rut.forEach(t),NCr=r(v7e," (BERT model)"),v7e.forEach(t),qCr=i(oo),pE=s(oo,"LI",{});var T7e=n(pE);M2e=s(T7e,"STRONG",{});var Sut=n(M2e);GCr=r(Sut,"big_bird"),Sut.forEach(t),OCr=r(T7e," \u2014 "),BX=s(T7e,"A",{href:!0});var Put=n(BX);XCr=r(Put,"FlaxBigBirdForMaskedLM"),Put.forEach(t),VCr=r(T7e," (BigBird model)"),T7e.forEach(t),zCr=i(oo),_E=s(oo,"LI",{});var F7e=n(_E);E2e=s(F7e,"STRONG",{});var $ut=n(E2e);WCr=r($ut,"distilbert"),$ut.forEach(t),QCr=r(F7e," \u2014 "),xX=s(F7e,"A",{href:!0});var Iut=n(xX);HCr=r(Iut,"FlaxDistilBertForMaskedLM"),Iut.forEach(t),UCr=r(F7e," (DistilBERT model)"),F7e.forEach(t),JCr=i(oo),bE=s(oo,"LI",{});var C7e=n(bE);y2e=s(C7e,"STRONG",{});var Dut=n(y2e);YCr=r(Dut,"electra"),Dut.forEach(t),KCr=r(C7e," \u2014 "),kX=s(C7e,"A",{href:!0});var jut=n(kX);ZCr=r(jut,"FlaxElectraForMaskedLM"),jut.forEach(t),e4r=r(C7e," (ELECTRA model)"),C7e.forEach(t),o4r=i(oo),vE=s(oo,"LI",{});var M7e=n(vE);w2e=s(M7e,"STRONG",{});var Nut=n(w2e);r4r=r(Nut,"mbart"),Nut.forEach(t),t4r=r(M7e," \u2014 "),RX=s(M7e,"A",{href:!0});var qut=n(RX);a4r=r(qut,"FlaxMBartForConditionalGeneration"),qut.forEach(t),s4r=r(M7e," (mBART model)"),M7e.forEach(t),n4r=i(oo),TE=s(oo,"LI",{});var E7e=n(TE);A2e=s(E7e,"STRONG",{});var Gut=n(A2e);l4r=r(Gut,"roberta"),Gut.forEach(t),i4r=r(E7e," \u2014 "),SX=s(E7e,"A",{href:!0});var Out=n(SX);d4r=r(Out,"FlaxRobertaForMaskedLM"),Out.forEach(t),c4r=r(E7e," (RoBERTa model)"),E7e.forEach(t),m4r=i(oo),FE=s(oo,"LI",{});var y7e=n(FE);L2e=s(y7e,"STRONG",{});var Xut=n(L2e);f4r=r(Xut,"roformer"),Xut.forEach(t),g4r=r(y7e," \u2014 "),PX=s(y7e,"A",{href:!0});var Vut=n(PX);h4r=r(Vut,"FlaxRoFormerForMaskedLM"),Vut.forEach(t),u4r=r(y7e," (RoFormer model)"),y7e.forEach(t),oo.forEach(t),p4r=i(wa),B2e=s(wa,"P",{});var zut=n(B2e);_4r=r(zut,"Examples:"),zut.forEach(t),b4r=i(wa),f(sL.$$.fragment,wa),wa.forEach(t),di.forEach(t),exe=i(c),Kc=s(c,"H2",{class:!0});var cRe=n(Kc);CE=s(cRe,"A",{id:!0,class:!0,href:!0});var Wut=n(CE);x2e=s(Wut,"SPAN",{});var Qut=n(x2e);f(nL.$$.fragment,Qut),Qut.forEach(t),Wut.forEach(t),v4r=i(cRe),k2e=s(cRe,"SPAN",{});var Hut=n(k2e);T4r=r(Hut,"FlaxAutoModelForSeq2SeqLM"),Hut.forEach(t),cRe.forEach(t),oxe=i(c),Rr=s(c,"DIV",{class:!0});var mi=n(Rr);f(lL.$$.fragment,mi),F4r=i(mi),Zc=s(mi,"P",{});var eW=n(Zc);C4r=r(eW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the `),R2e=s(eW,"CODE",{});var Uut=n(R2e);M4r=r(Uut,"from_pretrained()"),Uut.forEach(t),E4r=r(eW,"class method or the "),S2e=s(eW,"CODE",{});var Jut=n(S2e);y4r=r(Jut,"from_config()"),Jut.forEach(t),w4r=r(eW,`class
method.`),eW.forEach(t),A4r=i(mi),iL=s(mi,"P",{});var mRe=n(iL);L4r=r(mRe,"This class cannot be instantiated directly using "),P2e=s(mRe,"CODE",{});var Yut=n(P2e);B4r=r(Yut,"__init__()"),Yut.forEach(t),x4r=r(mRe," (throws an error)."),mRe.forEach(t),k4r=i(mi),yt=s(mi,"DIV",{class:!0});var fi=n(yt);f(dL.$$.fragment,fi),R4r=i(fi),$2e=s(fi,"P",{});var Kut=n($2e);S4r=r(Kut,"Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration."),Kut.forEach(t),P4r=i(fi),em=s(fi,"P",{});var oW=n(em);$4r=r(oW,`Note:
Loading a model from its configuration file does `),I2e=s(oW,"STRONG",{});var Zut=n(I2e);I4r=r(Zut,"not"),Zut.forEach(t),D4r=r(oW,` load the model weights. It only affects the
model\u2019s configuration. Use `),D2e=s(oW,"CODE",{});var ept=n(D2e);j4r=r(ept,"from_pretrained()"),ept.forEach(t),N4r=r(oW,"to load the model weights."),oW.forEach(t),q4r=i(fi),j2e=s(fi,"P",{});var opt=n(j2e);G4r=r(opt,"Examples:"),opt.forEach(t),O4r=i(fi),f(cL.$$.fragment,fi),fi.forEach(t),X4r=i(mi),xo=s(mi,"DIV",{class:!0});var Aa=n(xo);f(mL.$$.fragment,Aa),V4r=i(Aa),N2e=s(Aa,"P",{});var rpt=n(N2e);z4r=r(rpt,"Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model."),rpt.forEach(t),W4r=i(Aa),As=s(Aa,"P",{});var f5=n(As);Q4r=r(f5,"The model class to instantiate is selected based on the "),q2e=s(f5,"CODE",{});var tpt=n(q2e);H4r=r(tpt,"model_type"),tpt.forEach(t),U4r=r(f5,` property of the config object (either
passed as an argument or loaded from `),G2e=s(f5,"CODE",{});var apt=n(G2e);J4r=r(apt,"pretrained_model_name_or_path"),apt.forEach(t),Y4r=r(f5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),O2e=s(f5,"CODE",{});var spt=n(O2e);K4r=r(spt,"pretrained_model_name_or_path"),spt.forEach(t),Z4r=r(f5,":"),f5.forEach(t),eMr=i(Aa),Te=s(Aa,"UL",{});var ro=n(Te);ME=s(ro,"LI",{});var w7e=n(ME);X2e=s(w7e,"STRONG",{});var npt=n(X2e);oMr=r(npt,"bart"),npt.forEach(t),rMr=r(w7e," \u2014 "),$X=s(w7e,"A",{href:!0});var lpt=n($X);tMr=r(lpt,"FlaxBartForConditionalGeneration"),lpt.forEach(t),aMr=r(w7e," (BART model)"),w7e.forEach(t),sMr=i(ro),EE=s(ro,"LI",{});var A7e=n(EE);V2e=s(A7e,"STRONG",{});var ipt=n(V2e);nMr=r(ipt,"blenderbot"),ipt.forEach(t),lMr=r(A7e," \u2014 "),IX=s(A7e,"A",{href:!0});var dpt=n(IX);iMr=r(dpt,"FlaxBlenderbotForConditionalGeneration"),dpt.forEach(t),dMr=r(A7e," (Blenderbot model)"),A7e.forEach(t),cMr=i(ro),yE=s(ro,"LI",{});var L7e=n(yE);z2e=s(L7e,"STRONG",{});var cpt=n(z2e);mMr=r(cpt,"blenderbot-small"),cpt.forEach(t),fMr=r(L7e," \u2014 "),DX=s(L7e,"A",{href:!0});var mpt=n(DX);gMr=r(mpt,"FlaxBlenderbotSmallForConditionalGeneration"),mpt.forEach(t),hMr=r(L7e," (BlenderbotSmall model)"),L7e.forEach(t),uMr=i(ro),wE=s(ro,"LI",{});var B7e=n(wE);W2e=s(B7e,"STRONG",{});var fpt=n(W2e);pMr=r(fpt,"encoder-decoder"),fpt.forEach(t),_Mr=r(B7e," \u2014 "),jX=s(B7e,"A",{href:!0});var gpt=n(jX);bMr=r(gpt,"FlaxEncoderDecoderModel"),gpt.forEach(t),vMr=r(B7e," (Encoder decoder model)"),B7e.forEach(t),TMr=i(ro),AE=s(ro,"LI",{});var x7e=n(AE);Q2e=s(x7e,"STRONG",{});var hpt=n(Q2e);FMr=r(hpt,"marian"),hpt.forEach(t),CMr=r(x7e," \u2014 "),NX=s(x7e,"A",{href:!0});var upt=n(NX);MMr=r(upt,"FlaxMarianMTModel"),upt.forEach(t),EMr=r(x7e," (Marian model)"),x7e.forEach(t),yMr=i(ro),LE=s(ro,"LI",{});var k7e=n(LE);H2e=s(k7e,"STRONG",{});var ppt=n(H2e);wMr=r(ppt,"mbart"),ppt.forEach(t),AMr=r(k7e," \u2014 "),qX=s(k7e,"A",{href:!0});var _pt=n(qX);LMr=r(_pt,"FlaxMBartForConditionalGeneration"),_pt.forEach(t),BMr=r(k7e," (mBART model)"),k7e.forEach(t),xMr=i(ro),BE=s(ro,"LI",{});var R7e=n(BE);U2e=s(R7e,"STRONG",{});var bpt=n(U2e);kMr=r(bpt,"mt5"),bpt.forEach(t),RMr=r(R7e," \u2014 "),GX=s(R7e,"A",{href:!0});var vpt=n(GX);SMr=r(vpt,"FlaxMT5ForConditionalGeneration"),vpt.forEach(t),PMr=r(R7e," (mT5 model)"),R7e.forEach(t),$Mr=i(ro),xE=s(ro,"LI",{});var S7e=n(xE);J2e=s(S7e,"STRONG",{});var Tpt=n(J2e);IMr=r(Tpt,"pegasus"),Tpt.forEach(t),DMr=r(S7e," \u2014 "),OX=s(S7e,"A",{href:!0});var Fpt=n(OX);jMr=r(Fpt,"FlaxPegasusForConditionalGeneration"),Fpt.forEach(t),NMr=r(S7e," (Pegasus model)"),S7e.forEach(t),qMr=i(ro),kE=s(ro,"LI",{});var P7e=n(kE);Y2e=s(P7e,"STRONG",{});var Cpt=n(Y2e);GMr=r(Cpt,"t5"),Cpt.forEach(t),OMr=r(P7e," \u2014 "),XX=s(P7e,"A",{href:!0});var Mpt=n(XX);XMr=r(Mpt,"FlaxT5ForConditionalGeneration"),Mpt.forEach(t),VMr=r(P7e," (T5 model)"),P7e.forEach(t),ro.forEach(t),zMr=i(Aa),K2e=s(Aa,"P",{});var Ept=n(K2e);WMr=r(Ept,"Examples:"),Ept.forEach(t),QMr=i(Aa),f(fL.$$.fragment,Aa),Aa.forEach(t),mi.forEach(t),rxe=i(c),om=s(c,"H2",{class:!0});var fRe=n(om);RE=s(fRe,"A",{id:!0,class:!0,href:!0});var ypt=n(RE);Z2e=s(ypt,"SPAN",{});var wpt=n(Z2e);f(gL.$$.fragment,wpt),wpt.forEach(t),ypt.forEach(t),HMr=i(fRe),eve=s(fRe,"SPAN",{});var Apt=n(eve);UMr=r(Apt,"FlaxAutoModelForSequenceClassification"),Apt.forEach(t),fRe.forEach(t),txe=i(c),Sr=s(c,"DIV",{class:!0});var gi=n(Sr);f(hL.$$.fragment,gi),JMr=i(gi),rm=s(gi,"P",{});var rW=n(rm);YMr=r(rW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the `),ove=s(rW,"CODE",{});var Lpt=n(ove);KMr=r(Lpt,"from_pretrained()"),Lpt.forEach(t),ZMr=r(rW,"class method or the "),rve=s(rW,"CODE",{});var Bpt=n(rve);eEr=r(Bpt,"from_config()"),Bpt.forEach(t),oEr=r(rW,`class
method.`),rW.forEach(t),rEr=i(gi),uL=s(gi,"P",{});var gRe=n(uL);tEr=r(gRe,"This class cannot be instantiated directly using "),tve=s(gRe,"CODE",{});var xpt=n(tve);aEr=r(xpt,"__init__()"),xpt.forEach(t),sEr=r(gRe," (throws an error)."),gRe.forEach(t),nEr=i(gi),wt=s(gi,"DIV",{class:!0});var hi=n(wt);f(pL.$$.fragment,hi),lEr=i(hi),ave=s(hi,"P",{});var kpt=n(ave);iEr=r(kpt,"Instantiates one of the model classes of the library (with a sequence classification head) from a configuration."),kpt.forEach(t),dEr=i(hi),tm=s(hi,"P",{});var tW=n(tm);cEr=r(tW,`Note:
Loading a model from its configuration file does `),sve=s(tW,"STRONG",{});var Rpt=n(sve);mEr=r(Rpt,"not"),Rpt.forEach(t),fEr=r(tW,` load the model weights. It only affects the
model\u2019s configuration. Use `),nve=s(tW,"CODE",{});var Spt=n(nve);gEr=r(Spt,"from_pretrained()"),Spt.forEach(t),hEr=r(tW,"to load the model weights."),tW.forEach(t),uEr=i(hi),lve=s(hi,"P",{});var Ppt=n(lve);pEr=r(Ppt,"Examples:"),Ppt.forEach(t),_Er=i(hi),f(_L.$$.fragment,hi),hi.forEach(t),bEr=i(gi),ko=s(gi,"DIV",{class:!0});var La=n(ko);f(bL.$$.fragment,La),vEr=i(La),ive=s(La,"P",{});var $pt=n(ive);TEr=r($pt,"Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model."),$pt.forEach(t),FEr=i(La),Ls=s(La,"P",{});var g5=n(Ls);CEr=r(g5,"The model class to instantiate is selected based on the "),dve=s(g5,"CODE",{});var Ipt=n(dve);MEr=r(Ipt,"model_type"),Ipt.forEach(t),EEr=r(g5,` property of the config object (either
passed as an argument or loaded from `),cve=s(g5,"CODE",{});var Dpt=n(cve);yEr=r(Dpt,"pretrained_model_name_or_path"),Dpt.forEach(t),wEr=r(g5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),mve=s(g5,"CODE",{});var jpt=n(mve);AEr=r(jpt,"pretrained_model_name_or_path"),jpt.forEach(t),LEr=r(g5,":"),g5.forEach(t),BEr=i(La),Fe=s(La,"UL",{});var to=n(Fe);SE=s(to,"LI",{});var $7e=n(SE);fve=s($7e,"STRONG",{});var Npt=n(fve);xEr=r(Npt,"albert"),Npt.forEach(t),kEr=r($7e," \u2014 "),VX=s($7e,"A",{href:!0});var qpt=n(VX);REr=r(qpt,"FlaxAlbertForSequenceClassification"),qpt.forEach(t),SEr=r($7e," (ALBERT model)"),$7e.forEach(t),PEr=i(to),PE=s(to,"LI",{});var I7e=n(PE);gve=s(I7e,"STRONG",{});var Gpt=n(gve);$Er=r(Gpt,"bart"),Gpt.forEach(t),IEr=r(I7e," \u2014 "),zX=s(I7e,"A",{href:!0});var Opt=n(zX);DEr=r(Opt,"FlaxBartForSequenceClassification"),Opt.forEach(t),jEr=r(I7e," (BART model)"),I7e.forEach(t),NEr=i(to),$E=s(to,"LI",{});var D7e=n($E);hve=s(D7e,"STRONG",{});var Xpt=n(hve);qEr=r(Xpt,"bert"),Xpt.forEach(t),GEr=r(D7e," \u2014 "),WX=s(D7e,"A",{href:!0});var Vpt=n(WX);OEr=r(Vpt,"FlaxBertForSequenceClassification"),Vpt.forEach(t),XEr=r(D7e," (BERT model)"),D7e.forEach(t),VEr=i(to),IE=s(to,"LI",{});var j7e=n(IE);uve=s(j7e,"STRONG",{});var zpt=n(uve);zEr=r(zpt,"big_bird"),zpt.forEach(t),WEr=r(j7e," \u2014 "),QX=s(j7e,"A",{href:!0});var Wpt=n(QX);QEr=r(Wpt,"FlaxBigBirdForSequenceClassification"),Wpt.forEach(t),HEr=r(j7e," (BigBird model)"),j7e.forEach(t),UEr=i(to),DE=s(to,"LI",{});var N7e=n(DE);pve=s(N7e,"STRONG",{});var Qpt=n(pve);JEr=r(Qpt,"distilbert"),Qpt.forEach(t),YEr=r(N7e," \u2014 "),HX=s(N7e,"A",{href:!0});var Hpt=n(HX);KEr=r(Hpt,"FlaxDistilBertForSequenceClassification"),Hpt.forEach(t),ZEr=r(N7e," (DistilBERT model)"),N7e.forEach(t),e3r=i(to),jE=s(to,"LI",{});var q7e=n(jE);_ve=s(q7e,"STRONG",{});var Upt=n(_ve);o3r=r(Upt,"electra"),Upt.forEach(t),r3r=r(q7e," \u2014 "),UX=s(q7e,"A",{href:!0});var Jpt=n(UX);t3r=r(Jpt,"FlaxElectraForSequenceClassification"),Jpt.forEach(t),a3r=r(q7e," (ELECTRA model)"),q7e.forEach(t),s3r=i(to),NE=s(to,"LI",{});var G7e=n(NE);bve=s(G7e,"STRONG",{});var Ypt=n(bve);n3r=r(Ypt,"mbart"),Ypt.forEach(t),l3r=r(G7e," \u2014 "),JX=s(G7e,"A",{href:!0});var Kpt=n(JX);i3r=r(Kpt,"FlaxMBartForSequenceClassification"),Kpt.forEach(t),d3r=r(G7e," (mBART model)"),G7e.forEach(t),c3r=i(to),qE=s(to,"LI",{});var O7e=n(qE);vve=s(O7e,"STRONG",{});var Zpt=n(vve);m3r=r(Zpt,"roberta"),Zpt.forEach(t),f3r=r(O7e," \u2014 "),YX=s(O7e,"A",{href:!0});var e_t=n(YX);g3r=r(e_t,"FlaxRobertaForSequenceClassification"),e_t.forEach(t),h3r=r(O7e," (RoBERTa model)"),O7e.forEach(t),u3r=i(to),GE=s(to,"LI",{});var X7e=n(GE);Tve=s(X7e,"STRONG",{});var o_t=n(Tve);p3r=r(o_t,"roformer"),o_t.forEach(t),_3r=r(X7e," \u2014 "),KX=s(X7e,"A",{href:!0});var r_t=n(KX);b3r=r(r_t,"FlaxRoFormerForSequenceClassification"),r_t.forEach(t),v3r=r(X7e," (RoFormer model)"),X7e.forEach(t),to.forEach(t),T3r=i(La),Fve=s(La,"P",{});var t_t=n(Fve);F3r=r(t_t,"Examples:"),t_t.forEach(t),C3r=i(La),f(vL.$$.fragment,La),La.forEach(t),gi.forEach(t),axe=i(c),am=s(c,"H2",{class:!0});var hRe=n(am);OE=s(hRe,"A",{id:!0,class:!0,href:!0});var a_t=n(OE);Cve=s(a_t,"SPAN",{});var s_t=n(Cve);f(TL.$$.fragment,s_t),s_t.forEach(t),a_t.forEach(t),M3r=i(hRe),Mve=s(hRe,"SPAN",{});var n_t=n(Mve);E3r=r(n_t,"FlaxAutoModelForQuestionAnswering"),n_t.forEach(t),hRe.forEach(t),sxe=i(c),Pr=s(c,"DIV",{class:!0});var ui=n(Pr);f(FL.$$.fragment,ui),y3r=i(ui),sm=s(ui,"P",{});var aW=n(sm);w3r=r(aW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the `),Eve=s(aW,"CODE",{});var l_t=n(Eve);A3r=r(l_t,"from_pretrained()"),l_t.forEach(t),L3r=r(aW,"class method or the "),yve=s(aW,"CODE",{});var i_t=n(yve);B3r=r(i_t,"from_config()"),i_t.forEach(t),x3r=r(aW,`class
method.`),aW.forEach(t),k3r=i(ui),CL=s(ui,"P",{});var uRe=n(CL);R3r=r(uRe,"This class cannot be instantiated directly using "),wve=s(uRe,"CODE",{});var d_t=n(wve);S3r=r(d_t,"__init__()"),d_t.forEach(t),P3r=r(uRe," (throws an error)."),uRe.forEach(t),$3r=i(ui),At=s(ui,"DIV",{class:!0});var pi=n(At);f(ML.$$.fragment,pi),I3r=i(pi),Ave=s(pi,"P",{});var c_t=n(Ave);D3r=r(c_t,"Instantiates one of the model classes of the library (with a question answering head) from a configuration."),c_t.forEach(t),j3r=i(pi),nm=s(pi,"P",{});var sW=n(nm);N3r=r(sW,`Note:
Loading a model from its configuration file does `),Lve=s(sW,"STRONG",{});var m_t=n(Lve);q3r=r(m_t,"not"),m_t.forEach(t),G3r=r(sW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Bve=s(sW,"CODE",{});var f_t=n(Bve);O3r=r(f_t,"from_pretrained()"),f_t.forEach(t),X3r=r(sW,"to load the model weights."),sW.forEach(t),V3r=i(pi),xve=s(pi,"P",{});var g_t=n(xve);z3r=r(g_t,"Examples:"),g_t.forEach(t),W3r=i(pi),f(EL.$$.fragment,pi),pi.forEach(t),Q3r=i(ui),Ro=s(ui,"DIV",{class:!0});var Ba=n(Ro);f(yL.$$.fragment,Ba),H3r=i(Ba),kve=s(Ba,"P",{});var h_t=n(kve);U3r=r(h_t,"Instantiate one of the model classes of the library (with a question answering head) from a pretrained model."),h_t.forEach(t),J3r=i(Ba),Bs=s(Ba,"P",{});var h5=n(Bs);Y3r=r(h5,"The model class to instantiate is selected based on the "),Rve=s(h5,"CODE",{});var u_t=n(Rve);K3r=r(u_t,"model_type"),u_t.forEach(t),Z3r=r(h5,` property of the config object (either
passed as an argument or loaded from `),Sve=s(h5,"CODE",{});var p_t=n(Sve);e5r=r(p_t,"pretrained_model_name_or_path"),p_t.forEach(t),o5r=r(h5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),Pve=s(h5,"CODE",{});var __t=n(Pve);r5r=r(__t,"pretrained_model_name_or_path"),__t.forEach(t),t5r=r(h5,":"),h5.forEach(t),a5r=i(Ba),Ce=s(Ba,"UL",{});var ao=n(Ce);XE=s(ao,"LI",{});var V7e=n(XE);$ve=s(V7e,"STRONG",{});var b_t=n($ve);s5r=r(b_t,"albert"),b_t.forEach(t),n5r=r(V7e," \u2014 "),ZX=s(V7e,"A",{href:!0});var v_t=n(ZX);l5r=r(v_t,"FlaxAlbertForQuestionAnswering"),v_t.forEach(t),i5r=r(V7e," (ALBERT model)"),V7e.forEach(t),d5r=i(ao),VE=s(ao,"LI",{});var z7e=n(VE);Ive=s(z7e,"STRONG",{});var T_t=n(Ive);c5r=r(T_t,"bart"),T_t.forEach(t),m5r=r(z7e," \u2014 "),eV=s(z7e,"A",{href:!0});var F_t=n(eV);f5r=r(F_t,"FlaxBartForQuestionAnswering"),F_t.forEach(t),g5r=r(z7e," (BART model)"),z7e.forEach(t),h5r=i(ao),zE=s(ao,"LI",{});var W7e=n(zE);Dve=s(W7e,"STRONG",{});var C_t=n(Dve);u5r=r(C_t,"bert"),C_t.forEach(t),p5r=r(W7e," \u2014 "),oV=s(W7e,"A",{href:!0});var M_t=n(oV);_5r=r(M_t,"FlaxBertForQuestionAnswering"),M_t.forEach(t),b5r=r(W7e," (BERT model)"),W7e.forEach(t),v5r=i(ao),WE=s(ao,"LI",{});var Q7e=n(WE);jve=s(Q7e,"STRONG",{});var E_t=n(jve);T5r=r(E_t,"big_bird"),E_t.forEach(t),F5r=r(Q7e," \u2014 "),rV=s(Q7e,"A",{href:!0});var y_t=n(rV);C5r=r(y_t,"FlaxBigBirdForQuestionAnswering"),y_t.forEach(t),M5r=r(Q7e," (BigBird model)"),Q7e.forEach(t),E5r=i(ao),QE=s(ao,"LI",{});var H7e=n(QE);Nve=s(H7e,"STRONG",{});var w_t=n(Nve);y5r=r(w_t,"distilbert"),w_t.forEach(t),w5r=r(H7e," \u2014 "),tV=s(H7e,"A",{href:!0});var A_t=n(tV);A5r=r(A_t,"FlaxDistilBertForQuestionAnswering"),A_t.forEach(t),L5r=r(H7e," (DistilBERT model)"),H7e.forEach(t),B5r=i(ao),HE=s(ao,"LI",{});var U7e=n(HE);qve=s(U7e,"STRONG",{});var L_t=n(qve);x5r=r(L_t,"electra"),L_t.forEach(t),k5r=r(U7e," \u2014 "),aV=s(U7e,"A",{href:!0});var B_t=n(aV);R5r=r(B_t,"FlaxElectraForQuestionAnswering"),B_t.forEach(t),S5r=r(U7e," (ELECTRA model)"),U7e.forEach(t),P5r=i(ao),UE=s(ao,"LI",{});var J7e=n(UE);Gve=s(J7e,"STRONG",{});var x_t=n(Gve);$5r=r(x_t,"mbart"),x_t.forEach(t),I5r=r(J7e," \u2014 "),sV=s(J7e,"A",{href:!0});var k_t=n(sV);D5r=r(k_t,"FlaxMBartForQuestionAnswering"),k_t.forEach(t),j5r=r(J7e," (mBART model)"),J7e.forEach(t),N5r=i(ao),JE=s(ao,"LI",{});var Y7e=n(JE);Ove=s(Y7e,"STRONG",{});var R_t=n(Ove);q5r=r(R_t,"roberta"),R_t.forEach(t),G5r=r(Y7e," \u2014 "),nV=s(Y7e,"A",{href:!0});var S_t=n(nV);O5r=r(S_t,"FlaxRobertaForQuestionAnswering"),S_t.forEach(t),X5r=r(Y7e," (RoBERTa model)"),Y7e.forEach(t),V5r=i(ao),YE=s(ao,"LI",{});var K7e=n(YE);Xve=s(K7e,"STRONG",{});var P_t=n(Xve);z5r=r(P_t,"roformer"),P_t.forEach(t),W5r=r(K7e," \u2014 "),lV=s(K7e,"A",{href:!0});var $_t=n(lV);Q5r=r($_t,"FlaxRoFormerForQuestionAnswering"),$_t.forEach(t),H5r=r(K7e," (RoFormer model)"),K7e.forEach(t),ao.forEach(t),U5r=i(Ba),Vve=s(Ba,"P",{});var I_t=n(Vve);J5r=r(I_t,"Examples:"),I_t.forEach(t),Y5r=i(Ba),f(wL.$$.fragment,Ba),Ba.forEach(t),ui.forEach(t),nxe=i(c),lm=s(c,"H2",{class:!0});var pRe=n(lm);KE=s(pRe,"A",{id:!0,class:!0,href:!0});var D_t=n(KE);zve=s(D_t,"SPAN",{});var j_t=n(zve);f(AL.$$.fragment,j_t),j_t.forEach(t),D_t.forEach(t),K5r=i(pRe),Wve=s(pRe,"SPAN",{});var N_t=n(Wve);Z5r=r(N_t,"FlaxAutoModelForTokenClassification"),N_t.forEach(t),pRe.forEach(t),lxe=i(c),$r=s(c,"DIV",{class:!0});var _i=n($r);f(LL.$$.fragment,_i),eyr=i(_i),im=s(_i,"P",{});var nW=n(im);oyr=r(nW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the `),Qve=s(nW,"CODE",{});var q_t=n(Qve);ryr=r(q_t,"from_pretrained()"),q_t.forEach(t),tyr=r(nW,"class method or the "),Hve=s(nW,"CODE",{});var G_t=n(Hve);ayr=r(G_t,"from_config()"),G_t.forEach(t),syr=r(nW,`class
method.`),nW.forEach(t),nyr=i(_i),BL=s(_i,"P",{});var _Re=n(BL);lyr=r(_Re,"This class cannot be instantiated directly using "),Uve=s(_Re,"CODE",{});var O_t=n(Uve);iyr=r(O_t,"__init__()"),O_t.forEach(t),dyr=r(_Re," (throws an error)."),_Re.forEach(t),cyr=i(_i),Lt=s(_i,"DIV",{class:!0});var bi=n(Lt);f(xL.$$.fragment,bi),myr=i(bi),Jve=s(bi,"P",{});var X_t=n(Jve);fyr=r(X_t,"Instantiates one of the model classes of the library (with a token classification head) from a configuration."),X_t.forEach(t),gyr=i(bi),dm=s(bi,"P",{});var lW=n(dm);hyr=r(lW,`Note:
Loading a model from its configuration file does `),Yve=s(lW,"STRONG",{});var V_t=n(Yve);uyr=r(V_t,"not"),V_t.forEach(t),pyr=r(lW,` load the model weights. It only affects the
model\u2019s configuration. Use `),Kve=s(lW,"CODE",{});var z_t=n(Kve);_yr=r(z_t,"from_pretrained()"),z_t.forEach(t),byr=r(lW,"to load the model weights."),lW.forEach(t),vyr=i(bi),Zve=s(bi,"P",{});var W_t=n(Zve);Tyr=r(W_t,"Examples:"),W_t.forEach(t),Fyr=i(bi),f(kL.$$.fragment,bi),bi.forEach(t),Cyr=i(_i),So=s(_i,"DIV",{class:!0});var xa=n(So);f(RL.$$.fragment,xa),Myr=i(xa),eTe=s(xa,"P",{});var Q_t=n(eTe);Eyr=r(Q_t,"Instantiate one of the model classes of the library (with a token classification head) from a pretrained model."),Q_t.forEach(t),yyr=i(xa),xs=s(xa,"P",{});var u5=n(xs);wyr=r(u5,"The model class to instantiate is selected based on the "),oTe=s(u5,"CODE",{});var H_t=n(oTe);Ayr=r(H_t,"model_type"),H_t.forEach(t),Lyr=r(u5,` property of the config object (either
passed as an argument or loaded from `),rTe=s(u5,"CODE",{});var U_t=n(rTe);Byr=r(U_t,"pretrained_model_name_or_path"),U_t.forEach(t),xyr=r(u5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),tTe=s(u5,"CODE",{});var J_t=n(tTe);kyr=r(J_t,"pretrained_model_name_or_path"),J_t.forEach(t),Ryr=r(u5,":"),u5.forEach(t),Syr=i(xa),no=s(xa,"UL",{});var ta=n(no);ZE=s(ta,"LI",{});var Z7e=n(ZE);aTe=s(Z7e,"STRONG",{});var Y_t=n(aTe);Pyr=r(Y_t,"albert"),Y_t.forEach(t),$yr=r(Z7e," \u2014 "),iV=s(Z7e,"A",{href:!0});var K_t=n(iV);Iyr=r(K_t,"FlaxAlbertForTokenClassification"),K_t.forEach(t),Dyr=r(Z7e," (ALBERT model)"),Z7e.forEach(t),jyr=i(ta),e3=s(ta,"LI",{});var e9e=n(e3);sTe=s(e9e,"STRONG",{});var Z_t=n(sTe);Nyr=r(Z_t,"bert"),Z_t.forEach(t),qyr=r(e9e," \u2014 "),dV=s(e9e,"A",{href:!0});var ebt=n(dV);Gyr=r(ebt,"FlaxBertForTokenClassification"),ebt.forEach(t),Oyr=r(e9e," (BERT model)"),e9e.forEach(t),Xyr=i(ta),o3=s(ta,"LI",{});var o9e=n(o3);nTe=s(o9e,"STRONG",{});var obt=n(nTe);Vyr=r(obt,"big_bird"),obt.forEach(t),zyr=r(o9e," \u2014 "),cV=s(o9e,"A",{href:!0});var rbt=n(cV);Wyr=r(rbt,"FlaxBigBirdForTokenClassification"),rbt.forEach(t),Qyr=r(o9e," (BigBird model)"),o9e.forEach(t),Hyr=i(ta),r3=s(ta,"LI",{});var r9e=n(r3);lTe=s(r9e,"STRONG",{});var tbt=n(lTe);Uyr=r(tbt,"distilbert"),tbt.forEach(t),Jyr=r(r9e," \u2014 "),mV=s(r9e,"A",{href:!0});var abt=n(mV);Yyr=r(abt,"FlaxDistilBertForTokenClassification"),abt.forEach(t),Kyr=r(r9e," (DistilBERT model)"),r9e.forEach(t),Zyr=i(ta),t3=s(ta,"LI",{});var t9e=n(t3);iTe=s(t9e,"STRONG",{});var sbt=n(iTe);ewr=r(sbt,"electra"),sbt.forEach(t),owr=r(t9e," \u2014 "),fV=s(t9e,"A",{href:!0});var nbt=n(fV);rwr=r(nbt,"FlaxElectraForTokenClassification"),nbt.forEach(t),twr=r(t9e," (ELECTRA model)"),t9e.forEach(t),awr=i(ta),a3=s(ta,"LI",{});var a9e=n(a3);dTe=s(a9e,"STRONG",{});var lbt=n(dTe);swr=r(lbt,"roberta"),lbt.forEach(t),nwr=r(a9e," \u2014 "),gV=s(a9e,"A",{href:!0});var ibt=n(gV);lwr=r(ibt,"FlaxRobertaForTokenClassification"),ibt.forEach(t),iwr=r(a9e," (RoBERTa model)"),a9e.forEach(t),dwr=i(ta),s3=s(ta,"LI",{});var s9e=n(s3);cTe=s(s9e,"STRONG",{});var dbt=n(cTe);cwr=r(dbt,"roformer"),dbt.forEach(t),mwr=r(s9e," \u2014 "),hV=s(s9e,"A",{href:!0});var cbt=n(hV);fwr=r(cbt,"FlaxRoFormerForTokenClassification"),cbt.forEach(t),gwr=r(s9e," (RoFormer model)"),s9e.forEach(t),ta.forEach(t),hwr=i(xa),mTe=s(xa,"P",{});var mbt=n(mTe);uwr=r(mbt,"Examples:"),mbt.forEach(t),pwr=i(xa),f(SL.$$.fragment,xa),xa.forEach(t),_i.forEach(t),ixe=i(c),cm=s(c,"H2",{class:!0});var bRe=n(cm);n3=s(bRe,"A",{id:!0,class:!0,href:!0});var fbt=n(n3);fTe=s(fbt,"SPAN",{});var gbt=n(fTe);f(PL.$$.fragment,gbt),gbt.forEach(t),fbt.forEach(t),_wr=i(bRe),gTe=s(bRe,"SPAN",{});var hbt=n(gTe);bwr=r(hbt,"FlaxAutoModelForMultipleChoice"),hbt.forEach(t),bRe.forEach(t),dxe=i(c),Ir=s(c,"DIV",{class:!0});var vi=n(Ir);f($L.$$.fragment,vi),vwr=i(vi),mm=s(vi,"P",{});var iW=n(mm);Twr=r(iW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the `),hTe=s(iW,"CODE",{});var ubt=n(hTe);Fwr=r(ubt,"from_pretrained()"),ubt.forEach(t),Cwr=r(iW,"class method or the "),uTe=s(iW,"CODE",{});var pbt=n(uTe);Mwr=r(pbt,"from_config()"),pbt.forEach(t),Ewr=r(iW,`class
method.`),iW.forEach(t),ywr=i(vi),IL=s(vi,"P",{});var vRe=n(IL);wwr=r(vRe,"This class cannot be instantiated directly using "),pTe=s(vRe,"CODE",{});var _bt=n(pTe);Awr=r(_bt,"__init__()"),_bt.forEach(t),Lwr=r(vRe," (throws an error)."),vRe.forEach(t),Bwr=i(vi),Bt=s(vi,"DIV",{class:!0});var Ti=n(Bt);f(DL.$$.fragment,Ti),xwr=i(Ti),_Te=s(Ti,"P",{});var bbt=n(_Te);kwr=r(bbt,"Instantiates one of the model classes of the library (with a multiple choice head) from a configuration."),bbt.forEach(t),Rwr=i(Ti),fm=s(Ti,"P",{});var dW=n(fm);Swr=r(dW,`Note:
Loading a model from its configuration file does `),bTe=s(dW,"STRONG",{});var vbt=n(bTe);Pwr=r(vbt,"not"),vbt.forEach(t),$wr=r(dW,` load the model weights. It only affects the
model\u2019s configuration. Use `),vTe=s(dW,"CODE",{});var Tbt=n(vTe);Iwr=r(Tbt,"from_pretrained()"),Tbt.forEach(t),Dwr=r(dW,"to load the model weights."),dW.forEach(t),jwr=i(Ti),TTe=s(Ti,"P",{});var Fbt=n(TTe);Nwr=r(Fbt,"Examples:"),Fbt.forEach(t),qwr=i(Ti),f(jL.$$.fragment,Ti),Ti.forEach(t),Gwr=i(vi),Po=s(vi,"DIV",{class:!0});var ka=n(Po);f(NL.$$.fragment,ka),Owr=i(ka),FTe=s(ka,"P",{});var Cbt=n(FTe);Xwr=r(Cbt,"Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model."),Cbt.forEach(t),Vwr=i(ka),ks=s(ka,"P",{});var p5=n(ks);zwr=r(p5,"The model class to instantiate is selected based on the "),CTe=s(p5,"CODE",{});var Mbt=n(CTe);Wwr=r(Mbt,"model_type"),Mbt.forEach(t),Qwr=r(p5,` property of the config object (either
passed as an argument or loaded from `),MTe=s(p5,"CODE",{});var Ebt=n(MTe);Hwr=r(Ebt,"pretrained_model_name_or_path"),Ebt.forEach(t),Uwr=r(p5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),ETe=s(p5,"CODE",{});var ybt=n(ETe);Jwr=r(ybt,"pretrained_model_name_or_path"),ybt.forEach(t),Ywr=r(p5,":"),p5.forEach(t),Kwr=i(ka),lo=s(ka,"UL",{});var aa=n(lo);l3=s(aa,"LI",{});var n9e=n(l3);yTe=s(n9e,"STRONG",{});var wbt=n(yTe);Zwr=r(wbt,"albert"),wbt.forEach(t),e6r=r(n9e," \u2014 "),uV=s(n9e,"A",{href:!0});var Abt=n(uV);o6r=r(Abt,"FlaxAlbertForMultipleChoice"),Abt.forEach(t),r6r=r(n9e," (ALBERT model)"),n9e.forEach(t),t6r=i(aa),i3=s(aa,"LI",{});var l9e=n(i3);wTe=s(l9e,"STRONG",{});var Lbt=n(wTe);a6r=r(Lbt,"bert"),Lbt.forEach(t),s6r=r(l9e," \u2014 "),pV=s(l9e,"A",{href:!0});var Bbt=n(pV);n6r=r(Bbt,"FlaxBertForMultipleChoice"),Bbt.forEach(t),l6r=r(l9e," (BERT model)"),l9e.forEach(t),i6r=i(aa),d3=s(aa,"LI",{});var i9e=n(d3);ATe=s(i9e,"STRONG",{});var xbt=n(ATe);d6r=r(xbt,"big_bird"),xbt.forEach(t),c6r=r(i9e," \u2014 "),_V=s(i9e,"A",{href:!0});var kbt=n(_V);m6r=r(kbt,"FlaxBigBirdForMultipleChoice"),kbt.forEach(t),f6r=r(i9e," (BigBird model)"),i9e.forEach(t),g6r=i(aa),c3=s(aa,"LI",{});var d9e=n(c3);LTe=s(d9e,"STRONG",{});var Rbt=n(LTe);h6r=r(Rbt,"distilbert"),Rbt.forEach(t),u6r=r(d9e," \u2014 "),bV=s(d9e,"A",{href:!0});var Sbt=n(bV);p6r=r(Sbt,"FlaxDistilBertForMultipleChoice"),Sbt.forEach(t),_6r=r(d9e," (DistilBERT model)"),d9e.forEach(t),b6r=i(aa),m3=s(aa,"LI",{});var c9e=n(m3);BTe=s(c9e,"STRONG",{});var Pbt=n(BTe);v6r=r(Pbt,"electra"),Pbt.forEach(t),T6r=r(c9e," \u2014 "),vV=s(c9e,"A",{href:!0});var $bt=n(vV);F6r=r($bt,"FlaxElectraForMultipleChoice"),$bt.forEach(t),C6r=r(c9e," (ELECTRA model)"),c9e.forEach(t),M6r=i(aa),f3=s(aa,"LI",{});var m9e=n(f3);xTe=s(m9e,"STRONG",{});var Ibt=n(xTe);E6r=r(Ibt,"roberta"),Ibt.forEach(t),y6r=r(m9e," \u2014 "),TV=s(m9e,"A",{href:!0});var Dbt=n(TV);w6r=r(Dbt,"FlaxRobertaForMultipleChoice"),Dbt.forEach(t),A6r=r(m9e," (RoBERTa model)"),m9e.forEach(t),L6r=i(aa),g3=s(aa,"LI",{});var f9e=n(g3);kTe=s(f9e,"STRONG",{});var jbt=n(kTe);B6r=r(jbt,"roformer"),jbt.forEach(t),x6r=r(f9e," \u2014 "),FV=s(f9e,"A",{href:!0});var Nbt=n(FV);k6r=r(Nbt,"FlaxRoFormerForMultipleChoice"),Nbt.forEach(t),R6r=r(f9e," (RoFormer model)"),f9e.forEach(t),aa.forEach(t),S6r=i(ka),RTe=s(ka,"P",{});var qbt=n(RTe);P6r=r(qbt,"Examples:"),qbt.forEach(t),$6r=i(ka),f(qL.$$.fragment,ka),ka.forEach(t),vi.forEach(t),cxe=i(c),gm=s(c,"H2",{class:!0});var TRe=n(gm);h3=s(TRe,"A",{id:!0,class:!0,href:!0});var Gbt=n(h3);STe=s(Gbt,"SPAN",{});var Obt=n(STe);f(GL.$$.fragment,Obt),Obt.forEach(t),Gbt.forEach(t),I6r=i(TRe),PTe=s(TRe,"SPAN",{});var Xbt=n(PTe);D6r=r(Xbt,"FlaxAutoModelForNextSentencePrediction"),Xbt.forEach(t),TRe.forEach(t),mxe=i(c),Dr=s(c,"DIV",{class:!0});var Fi=n(Dr);f(OL.$$.fragment,Fi),j6r=i(Fi),hm=s(Fi,"P",{});var cW=n(hm);N6r=r(cW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the `),$Te=s(cW,"CODE",{});var Vbt=n($Te);q6r=r(Vbt,"from_pretrained()"),Vbt.forEach(t),G6r=r(cW,"class method or the "),ITe=s(cW,"CODE",{});var zbt=n(ITe);O6r=r(zbt,"from_config()"),zbt.forEach(t),X6r=r(cW,`class
method.`),cW.forEach(t),V6r=i(Fi),XL=s(Fi,"P",{});var FRe=n(XL);z6r=r(FRe,"This class cannot be instantiated directly using "),DTe=s(FRe,"CODE",{});var Wbt=n(DTe);W6r=r(Wbt,"__init__()"),Wbt.forEach(t),Q6r=r(FRe," (throws an error)."),FRe.forEach(t),H6r=i(Fi),xt=s(Fi,"DIV",{class:!0});var Ci=n(xt);f(VL.$$.fragment,Ci),U6r=i(Ci),jTe=s(Ci,"P",{});var Qbt=n(jTe);J6r=r(Qbt,"Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration."),Qbt.forEach(t),Y6r=i(Ci),um=s(Ci,"P",{});var mW=n(um);K6r=r(mW,`Note:
Loading a model from its configuration file does `),NTe=s(mW,"STRONG",{});var Hbt=n(NTe);Z6r=r(Hbt,"not"),Hbt.forEach(t),eAr=r(mW,` load the model weights. It only affects the
model\u2019s configuration. Use `),qTe=s(mW,"CODE",{});var Ubt=n(qTe);oAr=r(Ubt,"from_pretrained()"),Ubt.forEach(t),rAr=r(mW,"to load the model weights."),mW.forEach(t),tAr=i(Ci),GTe=s(Ci,"P",{});var Jbt=n(GTe);aAr=r(Jbt,"Examples:"),Jbt.forEach(t),sAr=i(Ci),f(zL.$$.fragment,Ci),Ci.forEach(t),nAr=i(Fi),$o=s(Fi,"DIV",{class:!0});var Ra=n($o);f(WL.$$.fragment,Ra),lAr=i(Ra),OTe=s(Ra,"P",{});var Ybt=n(OTe);iAr=r(Ybt,"Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model."),Ybt.forEach(t),dAr=i(Ra),Rs=s(Ra,"P",{});var _5=n(Rs);cAr=r(_5,"The model class to instantiate is selected based on the "),XTe=s(_5,"CODE",{});var Kbt=n(XTe);mAr=r(Kbt,"model_type"),Kbt.forEach(t),fAr=r(_5,` property of the config object (either
passed as an argument or loaded from `),VTe=s(_5,"CODE",{});var Zbt=n(VTe);gAr=r(Zbt,"pretrained_model_name_or_path"),Zbt.forEach(t),hAr=r(_5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),zTe=s(_5,"CODE",{});var e2t=n(zTe);uAr=r(e2t,"pretrained_model_name_or_path"),e2t.forEach(t),pAr=r(_5,":"),_5.forEach(t),_Ar=i(Ra),WTe=s(Ra,"UL",{});var o2t=n(WTe);u3=s(o2t,"LI",{});var g9e=n(u3);QTe=s(g9e,"STRONG",{});var r2t=n(QTe);bAr=r(r2t,"bert"),r2t.forEach(t),vAr=r(g9e," \u2014 "),CV=s(g9e,"A",{href:!0});var t2t=n(CV);TAr=r(t2t,"FlaxBertForNextSentencePrediction"),t2t.forEach(t),FAr=r(g9e," (BERT model)"),g9e.forEach(t),o2t.forEach(t),CAr=i(Ra),HTe=s(Ra,"P",{});var a2t=n(HTe);MAr=r(a2t,"Examples:"),a2t.forEach(t),EAr=i(Ra),f(QL.$$.fragment,Ra),Ra.forEach(t),Fi.forEach(t),fxe=i(c),pm=s(c,"H2",{class:!0});var CRe=n(pm);p3=s(CRe,"A",{id:!0,class:!0,href:!0});var s2t=n(p3);UTe=s(s2t,"SPAN",{});var n2t=n(UTe);f(HL.$$.fragment,n2t),n2t.forEach(t),s2t.forEach(t),yAr=i(CRe),JTe=s(CRe,"SPAN",{});var l2t=n(JTe);wAr=r(l2t,"FlaxAutoModelForImageClassification"),l2t.forEach(t),CRe.forEach(t),gxe=i(c),jr=s(c,"DIV",{class:!0});var Mi=n(jr);f(UL.$$.fragment,Mi),AAr=i(Mi),_m=s(Mi,"P",{});var fW=n(_m);LAr=r(fW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the `),YTe=s(fW,"CODE",{});var i2t=n(YTe);BAr=r(i2t,"from_pretrained()"),i2t.forEach(t),xAr=r(fW,"class method or the "),KTe=s(fW,"CODE",{});var d2t=n(KTe);kAr=r(d2t,"from_config()"),d2t.forEach(t),RAr=r(fW,`class
method.`),fW.forEach(t),SAr=i(Mi),JL=s(Mi,"P",{});var MRe=n(JL);PAr=r(MRe,"This class cannot be instantiated directly using "),ZTe=s(MRe,"CODE",{});var c2t=n(ZTe);$Ar=r(c2t,"__init__()"),c2t.forEach(t),IAr=r(MRe," (throws an error)."),MRe.forEach(t),DAr=i(Mi),kt=s(Mi,"DIV",{class:!0});var Ei=n(kt);f(YL.$$.fragment,Ei),jAr=i(Ei),e1e=s(Ei,"P",{});var m2t=n(e1e);NAr=r(m2t,"Instantiates one of the model classes of the library (with a image classification head) from a configuration."),m2t.forEach(t),qAr=i(Ei),bm=s(Ei,"P",{});var gW=n(bm);GAr=r(gW,`Note:
Loading a model from its configuration file does `),o1e=s(gW,"STRONG",{});var f2t=n(o1e);OAr=r(f2t,"not"),f2t.forEach(t),XAr=r(gW,` load the model weights. It only affects the
model\u2019s configuration. Use `),r1e=s(gW,"CODE",{});var g2t=n(r1e);VAr=r(g2t,"from_pretrained()"),g2t.forEach(t),zAr=r(gW,"to load the model weights."),gW.forEach(t),WAr=i(Ei),t1e=s(Ei,"P",{});var h2t=n(t1e);QAr=r(h2t,"Examples:"),h2t.forEach(t),HAr=i(Ei),f(KL.$$.fragment,Ei),Ei.forEach(t),UAr=i(Mi),Io=s(Mi,"DIV",{class:!0});var Sa=n(Io);f(ZL.$$.fragment,Sa),JAr=i(Sa),a1e=s(Sa,"P",{});var u2t=n(a1e);YAr=r(u2t,"Instantiate one of the model classes of the library (with a image classification head) from a pretrained model."),u2t.forEach(t),KAr=i(Sa),Ss=s(Sa,"P",{});var b5=n(Ss);ZAr=r(b5,"The model class to instantiate is selected based on the "),s1e=s(b5,"CODE",{});var p2t=n(s1e);e0r=r(p2t,"model_type"),p2t.forEach(t),o0r=r(b5,` property of the config object (either
passed as an argument or loaded from `),n1e=s(b5,"CODE",{});var _2t=n(n1e);r0r=r(_2t,"pretrained_model_name_or_path"),_2t.forEach(t),t0r=r(b5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),l1e=s(b5,"CODE",{});var b2t=n(l1e);a0r=r(b2t,"pretrained_model_name_or_path"),b2t.forEach(t),s0r=r(b5,":"),b5.forEach(t),n0r=i(Sa),e8=s(Sa,"UL",{});var ERe=n(e8);_3=s(ERe,"LI",{});var h9e=n(_3);i1e=s(h9e,"STRONG",{});var v2t=n(i1e);l0r=r(v2t,"beit"),v2t.forEach(t),i0r=r(h9e," \u2014 "),MV=s(h9e,"A",{href:!0});var T2t=n(MV);d0r=r(T2t,"FlaxBeitForImageClassification"),T2t.forEach(t),c0r=r(h9e," (BEiT model)"),h9e.forEach(t),m0r=i(ERe),b3=s(ERe,"LI",{});var u9e=n(b3);d1e=s(u9e,"STRONG",{});var F2t=n(d1e);f0r=r(F2t,"vit"),F2t.forEach(t),g0r=r(u9e," \u2014 "),EV=s(u9e,"A",{href:!0});var C2t=n(EV);h0r=r(C2t,"FlaxViTForImageClassification"),C2t.forEach(t),u0r=r(u9e," (ViT model)"),u9e.forEach(t),ERe.forEach(t),p0r=i(Sa),c1e=s(Sa,"P",{});var M2t=n(c1e);_0r=r(M2t,"Examples:"),M2t.forEach(t),b0r=i(Sa),f(o8.$$.fragment,Sa),Sa.forEach(t),Mi.forEach(t),hxe=i(c),vm=s(c,"H2",{class:!0});var yRe=n(vm);v3=s(yRe,"A",{id:!0,class:!0,href:!0});var E2t=n(v3);m1e=s(E2t,"SPAN",{});var y2t=n(m1e);f(r8.$$.fragment,y2t),y2t.forEach(t),E2t.forEach(t),v0r=i(yRe),f1e=s(yRe,"SPAN",{});var w2t=n(f1e);T0r=r(w2t,"FlaxAutoModelForVision2Seq"),w2t.forEach(t),yRe.forEach(t),uxe=i(c),Nr=s(c,"DIV",{class:!0});var yi=n(Nr);f(t8.$$.fragment,yi),F0r=i(yi),Tm=s(yi,"P",{});var hW=n(Tm);C0r=r(hW,`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the `),g1e=s(hW,"CODE",{});var A2t=n(g1e);M0r=r(A2t,"from_pretrained()"),A2t.forEach(t),E0r=r(hW,"class method or the "),h1e=s(hW,"CODE",{});var L2t=n(h1e);y0r=r(L2t,"from_config()"),L2t.forEach(t),w0r=r(hW,`class
method.`),hW.forEach(t),A0r=i(yi),a8=s(yi,"P",{});var wRe=n(a8);L0r=r(wRe,"This class cannot be instantiated directly using "),u1e=s(wRe,"CODE",{});var B2t=n(u1e);B0r=r(B2t,"__init__()"),B2t.forEach(t),x0r=r(wRe," (throws an error)."),wRe.forEach(t),k0r=i(yi),Rt=s(yi,"DIV",{class:!0});var wi=n(Rt);f(s8.$$.fragment,wi),R0r=i(wi),p1e=s(wi,"P",{});var x2t=n(p1e);S0r=r(x2t,"Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration."),x2t.forEach(t),P0r=i(wi),Fm=s(wi,"P",{});var uW=n(Fm);$0r=r(uW,`Note:
Loading a model from its configuration file does `),_1e=s(uW,"STRONG",{});var k2t=n(_1e);I0r=r(k2t,"not"),k2t.forEach(t),D0r=r(uW,` load the model weights. It only affects the
model\u2019s configuration. Use `),b1e=s(uW,"CODE",{});var R2t=n(b1e);j0r=r(R2t,"from_pretrained()"),R2t.forEach(t),N0r=r(uW,"to load the model weights."),uW.forEach(t),q0r=i(wi),v1e=s(wi,"P",{});var S2t=n(v1e);G0r=r(S2t,"Examples:"),S2t.forEach(t),O0r=i(wi),f(n8.$$.fragment,wi),wi.forEach(t),X0r=i(yi),Do=s(yi,"DIV",{class:!0});var Pa=n(Do);f(l8.$$.fragment,Pa),V0r=i(Pa),T1e=s(Pa,"P",{});var P2t=n(T1e);z0r=r(P2t,"Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model."),P2t.forEach(t),W0r=i(Pa),Ps=s(Pa,"P",{});var v5=n(Ps);Q0r=r(v5,"The model class to instantiate is selected based on the "),F1e=s(v5,"CODE",{});var $2t=n(F1e);H0r=r($2t,"model_type"),$2t.forEach(t),U0r=r(v5,` property of the config object (either
passed as an argument or loaded from `),C1e=s(v5,"CODE",{});var I2t=n(C1e);J0r=r(I2t,"pretrained_model_name_or_path"),I2t.forEach(t),Y0r=r(v5,` if possible), or when it\u2019s missing, by
falling back to using pattern matching on `),M1e=s(v5,"CODE",{});var D2t=n(M1e);K0r=r(D2t,"pretrained_model_name_or_path"),D2t.forEach(t),Z0r=r(v5,":"),v5.forEach(t),eLr=i(Pa),E1e=s(Pa,"UL",{});var j2t=n(E1e);T3=s(j2t,"LI",{});var p9e=n(T3);y1e=s(p9e,"STRONG",{});var N2t=n(y1e);oLr=r(N2t,"vision-encoder-decoder"),N2t.forEach(t),rLr=r(p9e," \u2014 "),yV=s(p9e,"A",{href:!0});var q2t=n(yV);tLr=r(q2t,"FlaxVisionEncoderDecoderModel"),q2t.forEach(t),aLr=r(p9e," (Vision Encoder decoder model)"),p9e.forEach(t),j2t.forEach(t),sLr=i(Pa),w1e=s(Pa,"P",{});var G2t=n(w1e);nLr=r(G2t,"Examples:"),G2t.forEach(t),lLr=i(Pa),f(i8.$$.fragment,Pa),Pa.forEach(t),yi.forEach(t),this.h()},h(){d(J,"name","hf:doc:metadata"),d(J,"content",JSON.stringify(J2t)),d(fe,"id","auto-classes"),d(fe,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fe,"href","#auto-classes"),d(ie,"class","relative group"),d($s,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig"),d(Ds,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoModel"),d(js,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer"),d(Pi,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),d(Am,"id","extending-the-auto-classes"),d(Am,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Am,"href","#extending-the-auto-classes"),d($i,"class","relative group"),d(Bm,"id","transformers.AutoConfig"),d(Bm,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Bm,"href","#transformers.AutoConfig"),d(Ii,"class","relative group"),d(m7,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoConfig.from_pretrained"),d(f7,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertConfig"),d(g7,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartConfig"),d(h7,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitConfig"),d(u7,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertConfig"),d(p7,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationConfig"),d(_7,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdConfig"),d(b7,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig"),d(v7,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotConfig"),d(T7,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig"),d(F7,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertConfig"),d(C7,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineConfig"),d(M7,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPConfig"),d(E7,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertConfig"),d(y7,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextConfig"),d(w7,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLConfig"),d(A7,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioConfig"),d(L7,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextConfig"),d(B7,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaConfig"),d(x7,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Config"),d(k7,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTConfig"),d(R7,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrConfig"),d(S7,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertConfig"),d(P7,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRConfig"),d($7,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraConfig"),d(I7,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig"),d(D7,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertConfig"),d(j7,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetConfig"),d(N7,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTConfig"),d(q7,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelConfig"),d(G7,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Config"),d(O7,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoConfig"),d(X7,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJConfig"),d(V7,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertConfig"),d(z7,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertConfig"),d(W7,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTConfig"),d(Q7,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMConfig"),d(H7,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config"),d(U7,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDConfig"),d(J7,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerConfig"),d(Y7,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeConfig"),d(K7,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertConfig"),d(Z7,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Config"),d(e9,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianConfig"),d(o9,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartConfig"),d(r9,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertConfig"),d(t9,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertConfig"),d(a9,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetConfig"),d(s9,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Config"),d(n9,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerConfig"),d(l9,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig"),d(i9,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusConfig"),d(d9,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverConfig"),d(c9,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartConfig"),d(m9,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerConfig"),d(f9,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig"),d(g9,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertConfig"),d(h9,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagConfig"),d(u9,"href","/docs/transformers/master/en/model_doc/realm#transformers.RealmConfig"),d(p9,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerConfig"),d(_9,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertConfig"),d(b9,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertConfig"),d(v9,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaConfig"),d(T9,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerConfig"),d(F9,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerConfig"),d(C9,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWConfig"),d(M9,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDConfig"),d(E9,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig"),d(y9,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextConfig"),d(w9,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config"),d(A9,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterConfig"),d(L9,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertConfig"),d(B9,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinConfig"),d(x9,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Config"),d(k9,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasConfig"),d(R9,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLConfig"),d(S9,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRConfig"),d(P9,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechConfig"),d($9,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig"),d(I9,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltConfig"),d(D9,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig"),d(j9,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig"),d(N9,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertConfig"),d(q9,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTConfig"),d(G9,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEConfig"),d(O9,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Config"),d(X9,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMConfig"),d(V9,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMConfig"),d(z9,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMConfig"),d(W9,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig"),d(Q9,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig"),d(H9,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig"),d(U9,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetConfig"),d(J9,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoConfig"),d(mo,"class","docstring"),d(pg,"class","docstring"),d(Xo,"class","docstring"),d(_g,"id","transformers.AutoTokenizer"),d(_g,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_g,"href","#transformers.AutoTokenizer"),d(ji,"class","relative group"),d(Y9,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained"),d(K9,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizer"),d(Z9,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertTokenizerFast"),d(eB,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizer"),d(oB,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartTokenizerFast"),d(rB,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizer"),d(tB,"href","/docs/transformers/master/en/model_doc/barthez#transformers.BarthezTokenizerFast"),d(aB,"href","/docs/transformers/master/en/model_doc/bartpho#transformers.BartphoTokenizer"),d(sB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),d(nB,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),d(lB,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationTokenizer"),d(iB,"href","/docs/transformers/master/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer"),d(dB,"href","/docs/transformers/master/en/model_doc/bertweet#transformers.BertweetTokenizer"),d(cB,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizer"),d(mB,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdTokenizerFast"),d(fB,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(gB,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(hB,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizer"),d(uB,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast"),d(pB,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer"),d(_B,"href","/docs/transformers/master/en/model_doc/byt5#transformers.ByT5Tokenizer"),d(bB,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizer"),d(vB,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertTokenizerFast"),d(TB,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineTokenizer"),d(FB,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizer"),d(CB,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPTokenizerFast"),d(MB,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizer"),d(EB,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertTokenizerFast"),d(yB,"href","/docs/transformers/master/en/model_doc/cpm#transformers.CpmTokenizer"),d(wB,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLTokenizer"),d(AB,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizer"),d(LB,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaTokenizerFast"),d(BB,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer"),d(xB,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizer"),d(kB,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertTokenizerFast"),d(RB,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer"),d(SB,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast"),d(PB,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizer"),d($B,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraTokenizerFast"),d(IB,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertTokenizer"),d(DB,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizer"),d(jB,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetTokenizerFast"),d(NB,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTTokenizer"),d(qB,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizer"),d(GB,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelTokenizerFast"),d(OB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(XB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(VB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Tokenizer"),d(zB,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2TokenizerFast"),d(WB,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizer"),d(QB,"href","/docs/transformers/master/en/model_doc/herbert#transformers.HerbertTokenizerFast"),d(HB,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(UB,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),d(JB,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(YB,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizer"),d(KB,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast"),d(ZB,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer"),d(ex,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast"),d(ox,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer"),d(rx,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast"),d(tx,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizer"),d(ax,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDTokenizerFast"),d(sx,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizer"),d(nx,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerTokenizerFast"),d(lx,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeTokenizer"),d(ix,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizer"),d(dx,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertTokenizerFast"),d(cx,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Tokenizer"),d(mx,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianTokenizer"),d(fx,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizer"),d(gx,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartTokenizerFast"),d(hx,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50Tokenizer"),d(ux,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBart50TokenizerFast"),d(px,"href","/docs/transformers/master/en/model_doc/mluke#transformers.MLukeTokenizer"),d(_x,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizer"),d(bx,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast"),d(vx,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizer"),d(Tx,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetTokenizerFast"),d(Fx,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),d(Cx,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),d(Mx,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer"),d(Ex,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast"),d(yx,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizer"),d(wx,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusTokenizerFast"),d(Ax,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverTokenizer"),d(Lx,"href","/docs/transformers/master/en/model_doc/phobert#transformers.PhobertTokenizer"),d(Bx,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartTokenizer"),d(xx,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer"),d(kx,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizer"),d(Rx,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertTokenizerFast"),d(Sx,"href","/docs/transformers/master/en/model_doc/rag#transformers.RagTokenizer"),d(Px,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizer"),d($x,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerTokenizerFast"),d(Ix,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizer"),d(Dx,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertTokenizerFast"),d(jx,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizer"),d(Nx,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertTokenizerFast"),d(qx,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizer"),d(Gx,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaTokenizerFast"),d(Ox,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizer"),d(Xx,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerTokenizerFast"),d(Vx,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer"),d(zx,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer"),d(Wx,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizer"),d(Qx,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterTokenizerFast"),d(Hx,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer"),d(Ux,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast"),d(Jx,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5Tokenizer"),d(Yx,"href","/docs/transformers/master/en/model_doc/mt5#transformers.T5TokenizerFast"),d(Kx,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasTokenizer"),d(Zx,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer"),d(ek,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer"),d(ok,"href","/docs/transformers/master/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer"),d(rk,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizer"),d(tk,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMTokenizerFast"),d(ak,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMTokenizer"),d(sk,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer"),d(nk,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer"),d(lk,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast"),d(ik,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizer"),d(dk,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetTokenizerFast"),d(fo,"class","docstring"),d(Qg,"class","docstring"),d(Vo,"class","docstring"),d(Hg,"id","transformers.AutoFeatureExtractor"),d(Hg,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Hg,"href","#transformers.AutoFeatureExtractor"),d(Ni,"class","relative group"),d(ck,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained"),d(mk,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitFeatureExtractor"),d(fk,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPFeatureExtractor"),d(gk,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextFeatureExtractor"),d(hk,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTFeatureExtractor"),d(uk,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrFeatureExtractor"),d(pk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d(_k,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor"),d(bk,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor"),d(vk,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor"),d(Tk,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerFeatureExtractor"),d(Fk,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor"),d(Ck,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(Mk,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(Ek,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTFeatureExtractor"),d(yk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor"),d(xe,"class","docstring"),d(mh,"class","docstring"),d(zo,"class","docstring"),d(fh,"id","transformers.AutoProcessor"),d(fh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fh,"href","#transformers.AutoProcessor"),d(qi,"class","relative group"),d(wk,"href","/docs/transformers/master/en/model_doc/auto#transformers.AutoProcessor.from_pretrained"),d(Ak,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPProcessor"),d(Lk,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor"),d(Bk,"href","/docs/transformers/master/en/model_doc/layoutxlm#transformers.LayoutXLMProcessor"),d(xk,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextProcessor"),d(kk,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor"),d(Rk,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRProcessor"),d(Sk,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor"),d(Pk,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor"),d(ke,"class","docstring"),d(Ch,"class","docstring"),d(Wo,"class","docstring"),d(Mh,"id","transformers.AutoModel"),d(Mh,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Mh,"href","#transformers.AutoModel"),d(Oi,"class","relative group"),d(qr,"class","docstring"),d($k,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertModel"),d(Ik,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartModel"),d(Dk,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitModel"),d(jk,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertModel"),d(Nk,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationEncoder"),d(qk,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdModel"),d(Gk,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel"),d(Ok,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotModel"),d(Xk,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel"),d(Vk,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertModel"),d(zk,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineModel"),d(Wk,"href","/docs/transformers/master/en/model_doc/clip#transformers.CLIPModel"),d(Qk,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertModel"),d(Hk,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextModel"),d(Uk,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLModel"),d(Jk,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioModel"),d(Yk,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextModel"),d(Kk,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaModel"),d(Zk,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2Model"),d(eR,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTModel"),d(oR,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrModel"),d(rR,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertModel"),d(tR,"href","/docs/transformers/master/en/model_doc/dpr#transformers.DPRQuestionEncoder"),d(aR,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraModel"),d(sR,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertModel"),d(nR,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetModel"),d(lR,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTModel"),d(iR,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelModel"),d(dR,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelBaseModel"),d(cR,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2Model"),d(mR,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoModel"),d(fR,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJModel"),d(gR,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertModel"),d(hR,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertModel"),d(uR,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTModel"),d(pR,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMModel"),d(_R,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model"),d(bR,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDModel"),d(vR,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerModel"),d(TR,"href","/docs/transformers/master/en/model_doc/luke#transformers.LukeModel"),d(FR,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertModel"),d(CR,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100Model"),d(MR,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianModel"),d(ER,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartModel"),d(yR,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertModel"),d(wR,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertModel"),d(AR,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetModel"),d(LR,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5Model"),d(BR,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerModel"),d(xR,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTModel"),d(kR,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusModel"),d(RR,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverModel"),d(SR,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartModel"),d(PR,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerModel"),d($R,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),d(IR,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertModel"),d(DR,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModel"),d(jR,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertModel"),d(NR,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),d(qR,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaModel"),d(GR,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerModel"),d(OR,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerModel"),d(XR,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWModel"),d(VR,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDModel"),d(zR,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextModel"),d(WR,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterModel"),d(QR,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertModel"),d(HR,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinModel"),d(UR,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5Model"),d(JR,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasModel"),d(YR,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLModel"),d(KR,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechModel"),d(ZR,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel"),d(eS,"href","/docs/transformers/master/en/model_doc/vilt#transformers.ViltModel"),d(oS,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel"),d(rS,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertModel"),d(tS,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTModel"),d(aS,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEModel"),d(sS,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2Model"),d(nS,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMModel"),d(lS,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMModel"),d(iS,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMModel"),d(dS,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel"),d(cS,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaModel"),d(mS,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel"),d(fS,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetModel"),d(gS,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoModel"),d(Re,"class","docstring"),d(Qo,"class","docstring"),d(rp,"id","transformers.AutoModelForPreTraining"),d(rp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(rp,"href","#transformers.AutoModelForPreTraining"),d(zi,"class","relative group"),d(Gr,"class","docstring"),d(hS,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForPreTraining"),d(uS,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(pS,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForPreTraining"),d(_S,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForPreTraining"),d(bS,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(vS,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(TS,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),d(FS,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(CS,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),d(MS,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(ES,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForPreTraining"),d(yS,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(wS,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForPreTraining"),d(AS,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(LS,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForPreTraining"),d(BS,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(xS,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(kS,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(RS,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(SS,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForPreTraining"),d(PS,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining"),d($S,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForPreTraining"),d(IS,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(DS,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),d(jS,"href","/docs/transformers/master/en/model_doc/retribert#transformers.RetriBertModel"),d(NS,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(qS,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(GS,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(OS,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(XS,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),d(VS,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForPreTraining"),d(zS,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining"),d(WS,"href","/docs/transformers/master/en/model_doc/visual_bert#transformers.VisualBertForPreTraining"),d(QS,"href","/docs/transformers/master/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining"),d(HS,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining"),d(US,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(JS,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),d(YS,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),d(KS,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(Se,"class","docstring"),d(Ho,"class","docstring"),d(Xp,"id","transformers.AutoModelForCausalLM"),d(Xp,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Xp,"href","#transformers.AutoModelForCausalLM"),d(Hi,"class","relative group"),d(Or,"class","docstring"),d(ZS,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForCausalLM"),d(eP,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertLMHeadModel"),d(oP,"href","/docs/transformers/master/en/model_doc/bert-generation#transformers.BertGenerationDecoder"),d(rP,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForCausalLM"),d(tP,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM"),d(aP,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM"),d(sP,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM"),d(nP,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForCausalLM"),d(lP,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLLMHeadModel"),d(iP,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM"),d(dP,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForCausalLM"),d(cP,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2LMHeadModel"),d(mP,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM"),d(fP,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForCausalLM"),d(gP,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianForCausalLM"),d(hP,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForCausalLM"),d(uP,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM"),d(pP,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel"),d(_P,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForCausalLM"),d(bP,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForCausalLM"),d(vP,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),d(TP,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel"),d(FP,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerModelWithLMHead"),d(CP,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForCausalLM"),d(MP,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForCausalLM"),d(EP,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForCausalLM"),d(yP,"href","/docs/transformers/master/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM"),d(wP,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel"),d(AP,"href","/docs/transformers/master/en/model_doc/trocr#transformers.TrOCRForCausalLM"),d(LP,"href","/docs/transformers/master/en/model_doc/xglm#transformers.XGLMForCausalLM"),d(BP,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(xP,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM"),d(kP,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM"),d(RP,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM"),d(SP,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetLMHeadModel"),d(Pe,"class","docstring"),d(Uo,"class","docstring"),d(w_,"id","transformers.AutoModelForMaskedLM"),d(w_,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(w_,"href","#transformers.AutoModelForMaskedLM"),d(Yi,"class","relative group"),d(Xr,"class","docstring"),d(PP,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMaskedLM"),d($P,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(IP,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMaskedLM"),d(DP,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMaskedLM"),d(jP,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMaskedLM"),d(NP,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMaskedLM"),d(qP,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM"),d(GP,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForMaskedLM"),d(OP,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM"),d(XP,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMaskedLM"),d(VP,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMaskedLM"),d(zP,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel"),d(WP,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMaskedLM"),d(QP,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMaskedLM"),d(HP,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMaskedLM"),d(UP,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM"),d(JP,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMaskedLM"),d(YP,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(KP,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM"),d(ZP,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM"),d(e$,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMaskedLM"),d(o$,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM"),d(r$,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForMaskedLM"),d(t$,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM"),d(a$,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForMaskedLM"),d(s$,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMaskedLM"),d(n$,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMaskedLM"),d(l$,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMaskedLM"),d(i$,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM"),d(d$,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForMaskedLM"),d(c$,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMWithLMHeadModel"),d(m$,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM"),d(f$,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM"),d(g$,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMaskedLM"),d($e,"class","docstring"),d(Jo,"class","docstring"),d(db,"id","transformers.AutoModelForSeq2SeqLM"),d(db,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(db,"href","#transformers.AutoModelForSeq2SeqLM"),d(ed,"class","relative group"),d(Vr,"class","docstring"),d(h$,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForConditionalGeneration"),d(u$,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration"),d(p$,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration"),d(_$,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration"),d(b$,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel"),d(v$,"href","/docs/transformers/master/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration"),d(T$,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForConditionalGeneration"),d(F$,"href","/docs/transformers/master/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration"),d(C$,"href","/docs/transformers/master/en/model_doc/marian#transformers.MarianMTModel"),d(M$,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForConditionalGeneration"),d(E$,"href","/docs/transformers/master/en/model_doc/mt5#transformers.MT5ForConditionalGeneration"),d(y$,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration"),d(w$,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForConditionalGeneration"),d(A$,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),d(L$,"href","/docs/transformers/master/en/model_doc/t5#transformers.T5ForConditionalGeneration"),d(B$,"href","/docs/transformers/master/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration"),d(Ie,"class","docstring"),d(Yo,"class","docstring"),d(Ab,"id","transformers.AutoModelForSequenceClassification"),d(Ab,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Ab,"href","#transformers.AutoModelForSequenceClassification"),d(td,"class","relative group"),d(zr,"class","docstring"),d(x$,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForSequenceClassification"),d(k$,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForSequenceClassification"),d(R$,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForSequenceClassification"),d(S$,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification"),d(P$,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification"),d($$,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForSequenceClassification"),d(I$,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForSequenceClassification"),d(D$,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForSequenceClassification"),d(j$,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.CTRLForSequenceClassification"),d(N$,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification"),d(q$,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForSequenceClassification"),d(G$,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification"),d(O$,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification"),d(X$,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForSequenceClassification"),d(V$,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification"),d(z$,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForSequenceClassification"),d(W$,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForSequenceClassification"),d(Q$,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification"),d(H$,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification"),d(U$,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForSequenceClassification"),d(J$,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForSequenceClassification"),d(Y$,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification"),d(K$,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification"),d(Z$,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForSequenceClassification"),d(eI,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForSequenceClassification"),d(oI,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForSequenceClassification"),d(rI,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification"),d(tI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification"),d(aI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForSequenceClassification"),d(sI,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification"),d(nI,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification"),d(lI,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification"),d(iI,"href","/docs/transformers/master/en/model_doc/plbart#transformers.PLBartForSequenceClassification"),d(dI,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification"),d(cI,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForSequenceClassification"),d(mI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForSequenceClassification"),d(fI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForSequenceClassification"),d(gI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForSequenceClassification"),d(hI,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification"),d(uI,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForSequenceClassification"),d(pI,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification"),d(_I,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForSequenceClassification"),d(bI,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification"),d(vI,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification"),d(TI,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForSequenceClassification"),d(FI,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForSequenceClassification"),d(De,"class","docstring"),d(Ko,"class","docstring"),d(F2,"id","transformers.AutoModelForMultipleChoice"),d(F2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(F2,"href","#transformers.AutoModelForMultipleChoice"),d(nd,"class","relative group"),d(Wr,"class","docstring"),d(CI,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForMultipleChoice"),d(MI,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForMultipleChoice"),d(EI,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice"),d(yI,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForMultipleChoice"),d(wI,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForMultipleChoice"),d(AI,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForMultipleChoice"),d(LI,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice"),d(BI,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice"),d(xI,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForMultipleChoice"),d(kI,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice"),d(RI,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForMultipleChoice"),d(SI,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForMultipleChoice"),d(PI,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForMultipleChoice"),d($I,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForMultipleChoice"),d(II,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice"),d(DI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice"),d(jI,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForMultipleChoice"),d(NI,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice"),d(qI,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice"),d(GI,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForMultipleChoice"),d(OI,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForMultipleChoice"),d(XI,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForMultipleChoice"),d(VI,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice"),d(zI,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForMultipleChoice"),d(WI,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice"),d(QI,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice"),d(HI,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForMultipleChoice"),d(UI,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForMultipleChoice"),d(je,"class","docstring"),d(Zo,"class","docstring"),d(Y2,"id","transformers.AutoModelForNextSentencePrediction"),d(Y2,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Y2,"href","#transformers.AutoModelForNextSentencePrediction"),d(dd,"class","relative group"),d(Qr,"class","docstring"),d(JI,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForNextSentencePrediction"),d(YI,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForNextSentencePrediction"),d(KI,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction"),d(ZI,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction"),d(eD,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction"),d(Ne,"class","docstring"),d(er,"class","docstring"),d(av,"id","transformers.AutoModelForTokenClassification"),d(av,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(av,"href","#transformers.AutoModelForTokenClassification"),d(fd,"class","relative group"),d(Hr,"class","docstring"),d(oD,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForTokenClassification"),d(rD,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForTokenClassification"),d(tD,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForTokenClassification"),d(aD,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForTokenClassification"),d(sD,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForTokenClassification"),d(nD,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForTokenClassification"),d(lD,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification"),d(iD,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForTokenClassification"),d(dD,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification"),d(cD,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForTokenClassification"),d(mD,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForTokenClassification"),d(fD,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForTokenClassification"),d(gD,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForTokenClassification"),d(hD,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForTokenClassification"),d(uD,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.GPT2ForTokenClassification"),d(pD,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForTokenClassification"),d(_D,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification"),d(bD,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification"),d(vD,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForTokenClassification"),d(TD,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification"),d(FD,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification"),d(CD,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForTokenClassification"),d(MD,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification"),d(ED,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification"),d(yD,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForTokenClassification"),d(wD,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForTokenClassification"),d(AD,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForTokenClassification"),d(LD,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification"),d(BD,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForTokenClassification"),d(xD,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification"),d(kD,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification"),d(RD,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForTokenClassification"),d(SD,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForTokenClassification"),d(qe,"class","docstring"),d(or,"class","docstring"),d(Nv,"id","transformers.AutoModelForQuestionAnswering"),d(Nv,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(Nv,"href","#transformers.AutoModelForQuestionAnswering"),d(ud,"class","relative group"),d(Ur,"class","docstring"),d(PD,"href","/docs/transformers/master/en/model_doc/albert#transformers.AlbertForQuestionAnswering"),d($D,"href","/docs/transformers/master/en/model_doc/bart#transformers.BartForQuestionAnswering"),d(ID,"href","/docs/transformers/master/en/model_doc/bert#transformers.BertForQuestionAnswering"),d(DD,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering"),d(jD,"href","/docs/transformers/master/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering"),d(ND,"href","/docs/transformers/master/en/model_doc/camembert#transformers.CamembertForQuestionAnswering"),d(qD,"href","/docs/transformers/master/en/model_doc/canine#transformers.CanineForQuestionAnswering"),d(GD,"href","/docs/transformers/master/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering"),d(OD,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering"),d(XD,"href","/docs/transformers/master/en/model_doc/deberta#transformers.DebertaForQuestionAnswering"),d(VD,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering"),d(zD,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering"),d(WD,"href","/docs/transformers/master/en/model_doc/electra#transformers.ElectraForQuestionAnswering"),d(QD,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple"),d(HD,"href","/docs/transformers/master/en/model_doc/fnet#transformers.FNetForQuestionAnswering"),d(UD,"href","/docs/transformers/master/en/model_doc/funnel#transformers.FunnelForQuestionAnswering"),d(JD,"href","/docs/transformers/master/en/model_doc/gptj#transformers.GPTJForQuestionAnswering"),d(YD,"href","/docs/transformers/master/en/model_doc/ibert#transformers.IBertForQuestionAnswering"),d(KD,"href","/docs/transformers/master/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering"),d(ZD,"href","/docs/transformers/master/en/model_doc/led#transformers.LEDForQuestionAnswering"),d(ej,"href","/docs/transformers/master/en/model_doc/longformer#transformers.LongformerForQuestionAnswering"),d(oj,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering"),d(rj,"href","/docs/transformers/master/en/model_doc/mbart#transformers.MBartForQuestionAnswering"),d(tj,"href","/docs/transformers/master/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering"),d(aj,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering"),d(sj,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering"),d(nj,"href","/docs/transformers/master/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering"),d(lj,"href","/docs/transformers/master/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering"),d(ij,"href","/docs/transformers/master/en/model_doc/reformer#transformers.ReformerForQuestionAnswering"),d(dj,"href","/docs/transformers/master/en/model_doc/rembert#transformers.RemBertForQuestionAnswering"),d(cj,"href","/docs/transformers/master/en/model_doc/roberta#transformers.RobertaForQuestionAnswering"),d(mj,"href","/docs/transformers/master/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering"),d(fj,"href","/docs/transformers/master/en/model_doc/splinter#transformers.SplinterForQuestionAnswering"),d(gj,"href","/docs/transformers/master/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering"),d(hj,"href","/docs/transformers/master/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple"),d(uj,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering"),d(pj,"href","/docs/transformers/master/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering"),d(_j,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple"),d(bj,"href","/docs/transformers/master/en/model_doc/yoso#transformers.YosoForQuestionAnswering"),d(Ge,"class","docstring"),d(rr,"class","docstring"),d(wT,"id","transformers.AutoModelForTableQuestionAnswering"),d(wT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(wT,"href","#transformers.AutoModelForTableQuestionAnswering"),d(bd,"class","relative group"),d(Jr,"class","docstring"),d(vj,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TapasForQuestionAnswering"),d(Oe,"class","docstring"),d(tr,"class","docstring"),d(BT,"id","transformers.AutoModelForImageClassification"),d(BT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(BT,"href","#transformers.AutoModelForImageClassification"),d(Fd,"class","relative group"),d(Yr,"class","docstring"),d(Tj,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForImageClassification"),d(Fj,"href","/docs/transformers/master/en/model_doc/convnext#transformers.ConvNextForImageClassification"),d(Cj,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassification"),d(Mj,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher"),d(Ej,"href","/docs/transformers/master/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification"),d(yj,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned"),d(wj,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier"),d(Aj,"href","/docs/transformers/master/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing"),d(Lj,"href","/docs/transformers/master/en/model_doc/poolformer#transformers.PoolFormerForImageClassification"),d(Bj,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForImageClassification"),d(xj,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForImageClassification"),d(kj,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForImageClassification"),d(Xe,"class","docstring"),d(ar,"class","docstring"),d(jT,"id","transformers.AutoModelForVision2Seq"),d(jT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(jT,"href","#transformers.AutoModelForVision2Seq"),d(Ed,"class","relative group"),d(Kr,"class","docstring"),d(Rj,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel"),d(Ve,"class","docstring"),d(sr,"class","docstring"),d(GT,"id","transformers.AutoModelForAudioClassification"),d(GT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(GT,"href","#transformers.AutoModelForAudioClassification"),d(Ad,"class","relative group"),d(Zr,"class","docstring"),d(Sj,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification"),d(Pj,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForSequenceClassification"),d($j,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForSequenceClassification"),d(Ij,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForSequenceClassification"),d(Dj,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification"),d(jj,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification"),d(Nj,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification"),d(qj,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForSequenceClassification"),d(ze,"class","docstring"),d(nr,"class","docstring"),d(YT,"id","transformers.AutoModelForAudioFrameClassification"),d(YT,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(YT,"href","#transformers.AutoModelForAudioFrameClassification"),d(xd,"class","relative group"),d(et,"class","docstring"),d(Gj,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification"),d(Oj,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification"),d(Xj,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification"),d(Vj,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification"),d(We,"class","docstring"),d(lr,"class","docstring"),d(t1,"id","transformers.AutoModelForCTC"),d(t1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(t1,"href","#transformers.AutoModelForCTC"),d(Sd,"class","relative group"),d(ot,"class","docstring"),d(zj,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForCTC"),d(Wj,"href","/docs/transformers/master/en/model_doc/hubert#transformers.HubertForCTC"),d(Qj,"href","/docs/transformers/master/en/model_doc/sew#transformers.SEWForCTC"),d(Hj,"href","/docs/transformers/master/en/model_doc/sew-d#transformers.SEWDForCTC"),d(Uj,"href","/docs/transformers/master/en/model_doc/unispeech#transformers.UniSpeechForCTC"),d(Jj,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC"),d(Yj,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC"),d(Kj,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForCTC"),d(Qe,"class","docstring"),d(ir,"class","docstring"),d(g1,"id","transformers.AutoModelForSpeechSeq2Seq"),d(g1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(g1,"href","#transformers.AutoModelForSpeechSeq2Seq"),d(Id,"class","relative group"),d(rt,"class","docstring"),d(Zj,"href","/docs/transformers/master/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel"),d(eN,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration"),d(He,"class","docstring"),d(dr,"class","docstring"),d(_1,"id","transformers.AutoModelForAudioXVector"),d(_1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(_1,"href","#transformers.AutoModelForAudioXVector"),d(Nd,"class","relative group"),d(tt,"class","docstring"),d(oN,"href","/docs/transformers/master/en/model_doc/data2vec#transformers.Data2VecAudioForXVector"),d(rN,"href","/docs/transformers/master/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector"),d(tN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector"),d(aN,"href","/docs/transformers/master/en/model_doc/wavlm#transformers.WavLMForXVector"),d(Ue,"class","docstring"),d(cr,"class","docstring"),d(M1,"id","transformers.AutoModelForMaskedImageModeling"),d(M1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(M1,"href","#transformers.AutoModelForMaskedImageModeling"),d(Od,"class","relative group"),d(at,"class","docstring"),d(sN,"href","/docs/transformers/master/en/model_doc/deit#transformers.DeiTForMaskedImageModeling"),d(nN,"href","/docs/transformers/master/en/model_doc/swin#transformers.SwinForMaskedImageModeling"),d(lN,"href","/docs/transformers/master/en/model_doc/vit#transformers.ViTForMaskedImageModeling"),d(Je,"class","docstring"),d(mr,"class","docstring"),d(L1,"id","transformers.AutoModelForObjectDetection"),d(L1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(L1,"href","#transformers.AutoModelForObjectDetection"),d(Wd,"class","relative group"),d(st,"class","docstring"),d(iN,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForObjectDetection"),d(Ye,"class","docstring"),d(fr,"class","docstring"),d(k1,"id","transformers.AutoModelForImageSegmentation"),d(k1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(k1,"href","#transformers.AutoModelForImageSegmentation"),d(Ud,"class","relative group"),d(nt,"class","docstring"),d(dN,"href","/docs/transformers/master/en/model_doc/detr#transformers.DetrForSegmentation"),d(Ke,"class","docstring"),d(gr,"class","docstring"),d(P1,"id","transformers.AutoModelForSemanticSegmentation"),d(P1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(P1,"href","#transformers.AutoModelForSemanticSegmentation"),d(Kd,"class","relative group"),d(lt,"class","docstring"),d(cN,"href","/docs/transformers/master/en/model_doc/beit#transformers.BeitForSemanticSegmentation"),d(mN,"href","/docs/transformers/master/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation"),d(Ze,"class","docstring"),d(hr,"class","docstring"),d(j1,"id","transformers.TFAutoModel"),d(j1,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(j1,"href","#transformers.TFAutoModel"),d(oc,"class","relative group"),d(it,"class","docstring"),d(fN,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertModel"),d(gN,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartModel"),d(hN,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertModel"),d(uN,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotModel"),d(pN,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel"),d(_N,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertModel"),d(bN,"href","/docs/transformers/master/en/model_doc/clip#transformers.TFCLIPModel"),d(vN,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertModel"),d(TN,"href","/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextModel"),d(FN,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLModel"),d(CN,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaModel"),d(MN,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2Model"),d(EN,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertModel"),d(yN,"href","/docs/transformers/master/en/model_doc/dpr#transformers.TFDPRQuestionEncoder"),d(wN,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraModel"),d(AN,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertModel"),d(LN,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelModel"),d(BN,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelBaseModel"),d(xN,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2Model"),d(kN,"href","/docs/transformers/master/en/model_doc/hubert#transformers.TFHubertModel"),d(RN,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMModel"),d(SN,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDModel"),d(PN,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerModel"),d($N,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertModel"),d(IN,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianModel"),d(DN,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartModel"),d(jN,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertModel"),d(NN,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetModel"),d(qN,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5Model"),d(GN,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel"),d(ON,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusModel"),d(XN,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertModel"),d(VN,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaModel"),d(zN,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerModel"),d(WN,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel"),d(QN,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5Model"),d(HN,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasModel"),d(UN,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLModel"),d(JN,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTModel"),d(YN,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model"),d(KN,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMModel"),d(ZN,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel"),d(eq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetModel"),d(go,"class","docstring"),d(ur,"class","docstring"),d(wF,"id","transformers.TFAutoModelForPreTraining"),d(wF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(wF,"href","#transformers.TFAutoModelForPreTraining"),d(ac,"class","relative group"),d(dt,"class","docstring"),d(oq,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForPreTraining"),d(rq,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(tq,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForPreTraining"),d(aq,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(sq,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(nq,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(lq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForPreTraining"),d(iq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(dq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForPreTraining"),d(cq,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(mq,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(fq,"href","/docs/transformers/master/en/model_doc/lxmert#transformers.TFLxmertForPreTraining"),d(gq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining"),d(hq,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(uq,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),d(pq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(_q,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(bq,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(vq,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),d(Tq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(Fq,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),d(Cq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(ho,"class","docstring"),d(pr,"class","docstring"),d(UF,"id","transformers.TFAutoModelForCausalLM"),d(UF,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(UF,"href","#transformers.TFAutoModelForCausalLM"),d(lc,"class","relative group"),d(ct,"class","docstring"),d(Mq,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertLMHeadModel"),d(Eq,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel"),d(yq,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel"),d(wq,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel"),d(Aq,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForCausalLM"),d(Lq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForCausalLM"),d(Bq,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForCausalLM"),d(xq,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel"),d(kq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(Rq,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel"),d(uo,"class","docstring"),d(_r,"class","docstring"),d(nC,"id","transformers.TFAutoModelForImageClassification"),d(nC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(nC,"href","#transformers.TFAutoModelForImageClassification"),d(cc,"class","relative group"),d(mt,"class","docstring"),d(Sq,"href","/docs/transformers/master/en/model_doc/convnext#transformers.TFConvNextForImageClassification"),d(Pq,"href","/docs/transformers/master/en/model_doc/vit#transformers.TFViTForImageClassification"),d(po,"class","docstring"),d(br,"class","docstring"),d(dC,"id","transformers.TFAutoModelForMaskedLM"),d(dC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(dC,"href","#transformers.TFAutoModelForMaskedLM"),d(gc,"class","relative group"),d(ft,"class","docstring"),d($q,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMaskedLM"),d(Iq,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMaskedLM"),d(Dq,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMaskedLM"),d(jq,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMaskedLM"),d(Nq,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForMaskedLM"),d(qq,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM"),d(Gq,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM"),d(Oq,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMaskedLM"),d(Xq,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel"),d(Vq,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMaskedLM"),d(zq,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM"),d(Wq,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMaskedLM"),d(Qq,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM"),d(Hq,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM"),d(Uq,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMaskedLM"),d(Jq,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMaskedLM"),d(Yq,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM"),d(Kq,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForMaskedLM"),d(Zq,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel"),d(eG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM"),d(_o,"class","docstring"),d(vr,"class","docstring"),d(xC,"id","transformers.TFAutoModelForSeq2SeqLM"),d(xC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(xC,"href","#transformers.TFAutoModelForSeq2SeqLM"),d(pc,"class","relative group"),d(gt,"class","docstring"),d(oG,"href","/docs/transformers/master/en/model_doc/bart#transformers.TFBartForConditionalGeneration"),d(rG,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration"),d(tG,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration"),d(aG,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel"),d(sG,"href","/docs/transformers/master/en/model_doc/led#transformers.TFLEDForConditionalGeneration"),d(nG,"href","/docs/transformers/master/en/model_doc/marian#transformers.TFMarianMTModel"),d(lG,"href","/docs/transformers/master/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration"),d(iG,"href","/docs/transformers/master/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration"),d(dG,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration"),d(cG,"href","/docs/transformers/master/en/model_doc/t5#transformers.TFT5ForConditionalGeneration"),d(bo,"class","docstring"),d(Tr,"class","docstring"),d(GC,"id","transformers.TFAutoModelForSequenceClassification"),d(GC,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(GC,"href","#transformers.TFAutoModelForSequenceClassification"),d(vc,"class","relative group"),d(ht,"class","docstring"),d(mG,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForSequenceClassification"),d(fG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForSequenceClassification"),d(gG,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification"),d(hG,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification"),d(uG,"href","/docs/transformers/master/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification"),d(pG,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification"),d(_G,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification"),d(bG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification"),d(vG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForSequenceClassification"),d(TG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification"),d(FG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification"),d(CG,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification"),d(MG,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification"),d(EG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification"),d(yG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification"),d(wG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification"),d(AG,"href","/docs/transformers/master/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification"),d(LG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification"),d(BG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification"),d(xG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification"),d(kG,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForSequenceClassification"),d(RG,"href","/docs/transformers/master/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification"),d(SG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForSequenceClassification"),d(PG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification"),d($G,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification"),d(vo,"class","docstring"),d(Fr,"class","docstring"),d(g4,"id","transformers.TFAutoModelForMultipleChoice"),d(g4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(g4,"href","#transformers.TFAutoModelForMultipleChoice"),d(Cc,"class","relative group"),d(ut,"class","docstring"),d(IG,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForMultipleChoice"),d(DG,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForMultipleChoice"),d(jG,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice"),d(NG,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice"),d(qG,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice"),d(GG,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForMultipleChoice"),d(OG,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice"),d(XG,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice"),d(VG,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice"),d(zG,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice"),d(WG,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice"),d(QG,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice"),d(HG,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice"),d(UG,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice"),d(JG,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForMultipleChoice"),d(YG,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice"),d(KG,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice"),d(To,"class","docstring"),d(Cr,"class","docstring"),d(k4,"id","transformers.TFAutoModelForTableQuestionAnswering"),d(k4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(k4,"href","#transformers.TFAutoModelForTableQuestionAnswering"),d(yc,"class","relative group"),d(pt,"class","docstring"),d(ZG,"href","/docs/transformers/master/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering"),d(Fo,"class","docstring"),d(Mr,"class","docstring"),d(S4,"id","transformers.TFAutoModelForTokenClassification"),d(S4,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(S4,"href","#transformers.TFAutoModelForTokenClassification"),d(Lc,"class","relative group"),d(_t,"class","docstring"),d(eO,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForTokenClassification"),d(oO,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForTokenClassification"),d(rO,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForTokenClassification"),d(tO,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForTokenClassification"),d(aO,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForTokenClassification"),d(sO,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification"),d(nO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification"),d(lO,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForTokenClassification"),d(iO,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification"),d(dO,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForTokenClassification"),d(cO,"href","/docs/transformers/master/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification"),d(mO,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForTokenClassification"),d(fO,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification"),d(gO,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification"),d(hO,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForTokenClassification"),d(uO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForTokenClassification"),d(pO,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification"),d(_O,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForTokenClassification"),d(bO,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification"),d(vO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification"),d(Co,"class","docstring"),d(Er,"class","docstring"),d(eM,"id","transformers.TFAutoModelForQuestionAnswering"),d(eM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(eM,"href","#transformers.TFAutoModelForQuestionAnswering"),d(kc,"class","relative group"),d(bt,"class","docstring"),d(TO,"href","/docs/transformers/master/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering"),d(FO,"href","/docs/transformers/master/en/model_doc/bert#transformers.TFBertForQuestionAnswering"),d(CO,"href","/docs/transformers/master/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering"),d(MO,"href","/docs/transformers/master/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering"),d(EO,"href","/docs/transformers/master/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering"),d(yO,"href","/docs/transformers/master/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering"),d(wO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering"),d(AO,"href","/docs/transformers/master/en/model_doc/electra#transformers.TFElectraForQuestionAnswering"),d(LO,"href","/docs/transformers/master/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple"),d(BO,"href","/docs/transformers/master/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering"),d(xO,"href","/docs/transformers/master/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering"),d(kO,"href","/docs/transformers/master/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering"),d(RO,"href","/docs/transformers/master/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering"),d(SO,"href","/docs/transformers/master/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering"),d(PO,"href","/docs/transformers/master/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering"),d($O,"href","/docs/transformers/master/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering"),d(IO,"href","/docs/transformers/master/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple"),d(DO,"href","/docs/transformers/master/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering"),d(jO,"href","/docs/transformers/master/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple"),d(Mo,"class","docstring"),d(yr,"class","docstring"),d(TM,"id","transformers.TFAutoModelForVision2Seq"),d(TM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(TM,"href","#transformers.TFAutoModelForVision2Seq"),d(Pc,"class","relative group"),d(vt,"class","docstring"),d(NO,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel"),d(Eo,"class","docstring"),d(wr,"class","docstring"),d(CM,"id","transformers.TFAutoModelForSpeechSeq2Seq"),d(CM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(CM,"href","#transformers.TFAutoModelForSpeechSeq2Seq"),d(Dc,"class","relative group"),d(Tt,"class","docstring"),d(qO,"href","/docs/transformers/master/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration"),d(yo,"class","docstring"),d(Ar,"class","docstring"),d(EM,"id","transformers.FlaxAutoModel"),d(EM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(EM,"href","#transformers.FlaxAutoModel"),d(qc,"class","relative group"),d(Ft,"class","docstring"),d(GO,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertModel"),d(OO,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartModel"),d(XO,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitModel"),d(VO,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertModel"),d(zO,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdModel"),d(WO,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel"),d(QO,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel"),d(HO,"href","/docs/transformers/master/en/model_doc/clip#transformers.FlaxCLIPModel"),d(UO,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertModel"),d(JO,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraModel"),d(YO,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2Model"),d(KO,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel"),d(ZO,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJModel"),d(eX,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianModel"),d(oX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartModel"),d(rX,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5Model"),d(tX,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusModel"),d(aX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaModel"),d(sX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerModel"),d(nX,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5Model"),d(lX,"href","/docs/transformers/master/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel"),d(iX,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTModel"),d(dX,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model"),d(cX,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMModel"),d(wo,"class","docstring"),d(Lr,"class","docstring"),d(UM,"id","transformers.FlaxAutoModelForCausalLM"),d(UM,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(UM,"href","#transformers.FlaxAutoModelForCausalLM"),d(Xc,"class","relative group"),d(Ct,"class","docstring"),d(mX,"href","/docs/transformers/master/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel"),d(fX,"href","/docs/transformers/master/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM"),d(gX,"href","/docs/transformers/master/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM"),d(hX,"href","/docs/transformers/master/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM"),d(Ao,"class","docstring"),d(Br,"class","docstring"),d(eE,"id","transformers.FlaxAutoModelForPreTraining"),d(eE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(eE,"href","#transformers.FlaxAutoModelForPreTraining"),d(Wc,"class","relative group"),d(Mt,"class","docstring"),d(uX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForPreTraining"),d(pX,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(_X,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForPreTraining"),d(bX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining"),d(vX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForPreTraining"),d(TX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(FX,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(CX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(MX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),d(EX,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(yX,"href","/docs/transformers/master/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining"),d(Lo,"class","docstring"),d(xr,"class","docstring"),d(fE,"id","transformers.FlaxAutoModelForMaskedLM"),d(fE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(fE,"href","#transformers.FlaxAutoModelForMaskedLM"),d(Uc,"class","relative group"),d(Et,"class","docstring"),d(wX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM"),d(AX,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(LX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMaskedLM"),d(BX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM"),d(xX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM"),d(kX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMaskedLM"),d(RX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(SX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM"),d(PX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM"),d(Bo,"class","docstring"),d(kr,"class","docstring"),d(CE,"id","transformers.FlaxAutoModelForSeq2SeqLM"),d(CE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(CE,"href","#transformers.FlaxAutoModelForSeq2SeqLM"),d(Kc,"class","relative group"),d(yt,"class","docstring"),d($X,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration"),d(IX,"href","/docs/transformers/master/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration"),d(DX,"href","/docs/transformers/master/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration"),d(jX,"href","/docs/transformers/master/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel"),d(NX,"href","/docs/transformers/master/en/model_doc/marian#transformers.FlaxMarianMTModel"),d(qX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration"),d(GX,"href","/docs/transformers/master/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration"),d(OX,"href","/docs/transformers/master/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration"),d(XX,"href","/docs/transformers/master/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration"),d(xo,"class","docstring"),d(Rr,"class","docstring"),d(RE,"id","transformers.FlaxAutoModelForSequenceClassification"),d(RE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(RE,"href","#transformers.FlaxAutoModelForSequenceClassification"),d(om,"class","relative group"),d(wt,"class","docstring"),d(VX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification"),d(zX,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForSequenceClassification"),d(WX,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForSequenceClassification"),d(QX,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification"),d(HX,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification"),d(UX,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification"),d(JX,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification"),d(YX,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification"),d(KX,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification"),d(ko,"class","docstring"),d(Sr,"class","docstring"),d(OE,"id","transformers.FlaxAutoModelForQuestionAnswering"),d(OE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(OE,"href","#transformers.FlaxAutoModelForQuestionAnswering"),d(am,"class","relative group"),d(At,"class","docstring"),d(ZX,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering"),d(eV,"href","/docs/transformers/master/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering"),d(oV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering"),d(rV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering"),d(tV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering"),d(aV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering"),d(sV,"href","/docs/transformers/master/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering"),d(nV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering"),d(lV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering"),d(Ro,"class","docstring"),d(Pr,"class","docstring"),d(KE,"id","transformers.FlaxAutoModelForTokenClassification"),d(KE,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(KE,"href","#transformers.FlaxAutoModelForTokenClassification"),d(lm,"class","relative group"),d(Lt,"class","docstring"),d(iV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification"),d(dV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForTokenClassification"),d(cV,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification"),d(mV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification"),d(fV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForTokenClassification"),d(gV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification"),d(hV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification"),d(So,"class","docstring"),d($r,"class","docstring"),d(n3,"id","transformers.FlaxAutoModelForMultipleChoice"),d(n3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(n3,"href","#transformers.FlaxAutoModelForMultipleChoice"),d(cm,"class","relative group"),d(Bt,"class","docstring"),d(uV,"href","/docs/transformers/master/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice"),d(pV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForMultipleChoice"),d(_V,"href","/docs/transformers/master/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice"),d(bV,"href","/docs/transformers/master/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice"),d(vV,"href","/docs/transformers/master/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice"),d(TV,"href","/docs/transformers/master/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice"),d(FV,"href","/docs/transformers/master/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice"),d(Po,"class","docstring"),d(Ir,"class","docstring"),d(h3,"id","transformers.FlaxAutoModelForNextSentencePrediction"),d(h3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(h3,"href","#transformers.FlaxAutoModelForNextSentencePrediction"),d(gm,"class","relative group"),d(xt,"class","docstring"),d(CV,"href","/docs/transformers/master/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction"),d($o,"class","docstring"),d(Dr,"class","docstring"),d(p3,"id","transformers.FlaxAutoModelForImageClassification"),d(p3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(p3,"href","#transformers.FlaxAutoModelForImageClassification"),d(pm,"class","relative group"),d(kt,"class","docstring"),d(MV,"href","/docs/transformers/master/en/model_doc/beit#transformers.FlaxBeitForImageClassification"),d(EV,"href","/docs/transformers/master/en/model_doc/vit#transformers.FlaxViTForImageClassification"),d(Io,"class","docstring"),d(jr,"class","docstring"),d(v3,"id","transformers.FlaxAutoModelForVision2Seq"),d(v3,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),d(v3,"href","#transformers.FlaxAutoModelForVision2Seq"),d(vm,"class","relative group"),d(Rt,"class","docstring"),d(yV,"href","/docs/transformers/master/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel"),d(Do,"class","docstring"),d(Nr,"class","docstring")},m(c,_){e(document.head,J),b(c,Be,_),b(c,ie,_),e(ie,fe),e(fe,so),g(ce,so,null),e(ie,_e),e(ie,Go),e(Go,Li),b(c,Mm,_),b(c,na,_),e(na,Bi),e(na,xi),e(xi,T5),e(na,Em),b(c,ye,_),b(c,io,_),e(io,ki),e(io,$s),e($s,F5),e(io,Is),e(io,Ds),e(Ds,C5),e(io,Ri),e(io,js),e(js,M5),e(io,Si),b(c,ym,_),g($a,c,_),b(c,co,_),b(c,ge,_),e(ge,s7),e(ge,Pi),e(Pi,n7),e(ge,l7),b(c,Oo,_),b(c,Ia,_),e(Ia,i7),e(Ia,wm),e(wm,d7),e(Ia,ARe),b(c,_9e,_),b(c,$i,_),e($i,Am),e(Am,pW),g(E5,pW,null),e($i,LRe),e($i,_W),e(_W,BRe),b(c,b9e,_),b(c,Ns,_),e(Ns,xRe),e(Ns,bW),e(bW,kRe),e(Ns,RRe),e(Ns,vW),e(vW,SRe),e(Ns,PRe),b(c,v9e,_),g(y5,c,_),b(c,T9e,_),b(c,c7,_),e(c7,$Re),b(c,F9e,_),g(Lm,c,_),b(c,C9e,_),b(c,Ii,_),e(Ii,Bm),e(Bm,TW),g(w5,TW,null),e(Ii,IRe),e(Ii,FW),e(FW,DRe),b(c,M9e,_),b(c,Xo,_),g(A5,Xo,null),e(Xo,jRe),e(Xo,L5),e(L5,NRe),e(L5,m7),e(m7,qRe),e(L5,GRe),e(Xo,ORe),e(Xo,B5),e(B5,XRe),e(B5,CW),e(CW,VRe),e(B5,zRe),e(Xo,WRe),e(Xo,mo),g(x5,mo,null),e(mo,QRe),e(mo,MW),e(MW,HRe),e(mo,URe),e(mo,Di),e(Di,JRe),e(Di,EW),e(EW,YRe),e(Di,KRe),e(Di,yW),e(yW,ZRe),e(Di,eSe),e(mo,oSe),e(mo,v),e(v,xm),e(xm,wW),e(wW,rSe),e(xm,tSe),e(xm,f7),e(f7,aSe),e(xm,sSe),e(v,nSe),e(v,km),e(km,AW),e(AW,lSe),e(km,iSe),e(km,g7),e(g7,dSe),e(km,cSe),e(v,mSe),e(v,Rm),e(Rm,LW),e(LW,fSe),e(Rm,gSe),e(Rm,h7),e(h7,hSe),e(Rm,uSe),e(v,pSe),e(v,Sm),e(Sm,BW),e(BW,_Se),e(Sm,bSe),e(Sm,u7),e(u7,vSe),e(Sm,TSe),e(v,FSe),e(v,Pm),e(Pm,xW),e(xW,CSe),e(Pm,MSe),e(Pm,p7),e(p7,ESe),e(Pm,ySe),e(v,wSe),e(v,$m),e($m,kW),e(kW,ASe),e($m,LSe),e($m,_7),e(_7,BSe),e($m,xSe),e(v,kSe),e(v,Im),e(Im,RW),e(RW,RSe),e(Im,SSe),e(Im,b7),e(b7,PSe),e(Im,$Se),e(v,ISe),e(v,Dm),e(Dm,SW),e(SW,DSe),e(Dm,jSe),e(Dm,v7),e(v7,NSe),e(Dm,qSe),e(v,GSe),e(v,jm),e(jm,PW),e(PW,OSe),e(jm,XSe),e(jm,T7),e(T7,VSe),e(jm,zSe),e(v,WSe),e(v,Nm),e(Nm,$W),e($W,QSe),e(Nm,HSe),e(Nm,F7),e(F7,USe),e(Nm,JSe),e(v,YSe),e(v,qm),e(qm,IW),e(IW,KSe),e(qm,ZSe),e(qm,C7),e(C7,ePe),e(qm,oPe),e(v,rPe),e(v,Gm),e(Gm,DW),e(DW,tPe),e(Gm,aPe),e(Gm,M7),e(M7,sPe),e(Gm,nPe),e(v,lPe),e(v,Om),e(Om,jW),e(jW,iPe),e(Om,dPe),e(Om,E7),e(E7,cPe),e(Om,mPe),e(v,fPe),e(v,Xm),e(Xm,NW),e(NW,gPe),e(Xm,hPe),e(Xm,y7),e(y7,uPe),e(Xm,pPe),e(v,_Pe),e(v,Vm),e(Vm,qW),e(qW,bPe),e(Vm,vPe),e(Vm,w7),e(w7,TPe),e(Vm,FPe),e(v,CPe),e(v,zm),e(zm,GW),e(GW,MPe),e(zm,EPe),e(zm,A7),e(A7,yPe),e(zm,wPe),e(v,APe),e(v,Wm),e(Wm,OW),e(OW,LPe),e(Wm,BPe),e(Wm,L7),e(L7,xPe),e(Wm,kPe),e(v,RPe),e(v,Qm),e(Qm,XW),e(XW,SPe),e(Qm,PPe),e(Qm,B7),e(B7,$Pe),e(Qm,IPe),e(v,DPe),e(v,Hm),e(Hm,VW),e(VW,jPe),e(Hm,NPe),e(Hm,x7),e(x7,qPe),e(Hm,GPe),e(v,OPe),e(v,Um),e(Um,zW),e(zW,XPe),e(Um,VPe),e(Um,k7),e(k7,zPe),e(Um,WPe),e(v,QPe),e(v,Jm),e(Jm,WW),e(WW,HPe),e(Jm,UPe),e(Jm,R7),e(R7,JPe),e(Jm,YPe),e(v,KPe),e(v,Ym),e(Ym,QW),e(QW,ZPe),e(Ym,e$e),e(Ym,S7),e(S7,o$e),e(Ym,r$e),e(v,t$e),e(v,Km),e(Km,HW),e(HW,a$e),e(Km,s$e),e(Km,P7),e(P7,n$e),e(Km,l$e),e(v,i$e),e(v,Zm),e(Zm,UW),e(UW,d$e),e(Zm,c$e),e(Zm,$7),e($7,m$e),e(Zm,f$e),e(v,g$e),e(v,ef),e(ef,JW),e(JW,h$e),e(ef,u$e),e(ef,I7),e(I7,p$e),e(ef,_$e),e(v,b$e),e(v,of),e(of,YW),e(YW,v$e),e(of,T$e),e(of,D7),e(D7,F$e),e(of,C$e),e(v,M$e),e(v,rf),e(rf,KW),e(KW,E$e),e(rf,y$e),e(rf,j7),e(j7,w$e),e(rf,A$e),e(v,L$e),e(v,tf),e(tf,ZW),e(ZW,B$e),e(tf,x$e),e(tf,N7),e(N7,k$e),e(tf,R$e),e(v,S$e),e(v,af),e(af,eQ),e(eQ,P$e),e(af,$$e),e(af,q7),e(q7,I$e),e(af,D$e),e(v,j$e),e(v,sf),e(sf,oQ),e(oQ,N$e),e(sf,q$e),e(sf,G7),e(G7,G$e),e(sf,O$e),e(v,X$e),e(v,nf),e(nf,rQ),e(rQ,V$e),e(nf,z$e),e(nf,O7),e(O7,W$e),e(nf,Q$e),e(v,H$e),e(v,lf),e(lf,tQ),e(tQ,U$e),e(lf,J$e),e(lf,X7),e(X7,Y$e),e(lf,K$e),e(v,Z$e),e(v,df),e(df,aQ),e(aQ,eIe),e(df,oIe),e(df,V7),e(V7,rIe),e(df,tIe),e(v,aIe),e(v,cf),e(cf,sQ),e(sQ,sIe),e(cf,nIe),e(cf,z7),e(z7,lIe),e(cf,iIe),e(v,dIe),e(v,mf),e(mf,nQ),e(nQ,cIe),e(mf,mIe),e(mf,W7),e(W7,fIe),e(mf,gIe),e(v,hIe),e(v,ff),e(ff,lQ),e(lQ,uIe),e(ff,pIe),e(ff,Q7),e(Q7,_Ie),e(ff,bIe),e(v,vIe),e(v,gf),e(gf,iQ),e(iQ,TIe),e(gf,FIe),e(gf,H7),e(H7,CIe),e(gf,MIe),e(v,EIe),e(v,hf),e(hf,dQ),e(dQ,yIe),e(hf,wIe),e(hf,U7),e(U7,AIe),e(hf,LIe),e(v,BIe),e(v,uf),e(uf,cQ),e(cQ,xIe),e(uf,kIe),e(uf,J7),e(J7,RIe),e(uf,SIe),e(v,PIe),e(v,pf),e(pf,mQ),e(mQ,$Ie),e(pf,IIe),e(pf,Y7),e(Y7,DIe),e(pf,jIe),e(v,NIe),e(v,_f),e(_f,fQ),e(fQ,qIe),e(_f,GIe),e(_f,K7),e(K7,OIe),e(_f,XIe),e(v,VIe),e(v,bf),e(bf,gQ),e(gQ,zIe),e(bf,WIe),e(bf,Z7),e(Z7,QIe),e(bf,HIe),e(v,UIe),e(v,vf),e(vf,hQ),e(hQ,JIe),e(vf,YIe),e(vf,e9),e(e9,KIe),e(vf,ZIe),e(v,eDe),e(v,Tf),e(Tf,uQ),e(uQ,oDe),e(Tf,rDe),e(Tf,o9),e(o9,tDe),e(Tf,aDe),e(v,sDe),e(v,Ff),e(Ff,pQ),e(pQ,nDe),e(Ff,lDe),e(Ff,r9),e(r9,iDe),e(Ff,dDe),e(v,cDe),e(v,Cf),e(Cf,_Q),e(_Q,mDe),e(Cf,fDe),e(Cf,t9),e(t9,gDe),e(Cf,hDe),e(v,uDe),e(v,Mf),e(Mf,bQ),e(bQ,pDe),e(Mf,_De),e(Mf,a9),e(a9,bDe),e(Mf,vDe),e(v,TDe),e(v,Ef),e(Ef,vQ),e(vQ,FDe),e(Ef,CDe),e(Ef,s9),e(s9,MDe),e(Ef,EDe),e(v,yDe),e(v,yf),e(yf,TQ),e(TQ,wDe),e(yf,ADe),e(yf,n9),e(n9,LDe),e(yf,BDe),e(v,xDe),e(v,wf),e(wf,FQ),e(FQ,kDe),e(wf,RDe),e(wf,l9),e(l9,SDe),e(wf,PDe),e(v,$De),e(v,Af),e(Af,CQ),e(CQ,IDe),e(Af,DDe),e(Af,i9),e(i9,jDe),e(Af,NDe),e(v,qDe),e(v,Lf),e(Lf,MQ),e(MQ,GDe),e(Lf,ODe),e(Lf,d9),e(d9,XDe),e(Lf,VDe),e(v,zDe),e(v,Bf),e(Bf,EQ),e(EQ,WDe),e(Bf,QDe),e(Bf,c9),e(c9,HDe),e(Bf,UDe),e(v,JDe),e(v,xf),e(xf,yQ),e(yQ,YDe),e(xf,KDe),e(xf,m9),e(m9,ZDe),e(xf,eje),e(v,oje),e(v,kf),e(kf,wQ),e(wQ,rje),e(kf,tje),e(kf,f9),e(f9,aje),e(kf,sje),e(v,nje),e(v,Rf),e(Rf,AQ),e(AQ,lje),e(Rf,ije),e(Rf,g9),e(g9,dje),e(Rf,cje),e(v,mje),e(v,Sf),e(Sf,LQ),e(LQ,fje),e(Sf,gje),e(Sf,h9),e(h9,hje),e(Sf,uje),e(v,pje),e(v,Pf),e(Pf,BQ),e(BQ,_je),e(Pf,bje),e(Pf,u9),e(u9,vje),e(Pf,Tje),e(v,Fje),e(v,$f),e($f,xQ),e(xQ,Cje),e($f,Mje),e($f,p9),e(p9,Eje),e($f,yje),e(v,wje),e(v,If),e(If,kQ),e(kQ,Aje),e(If,Lje),e(If,_9),e(_9,Bje),e(If,xje),e(v,kje),e(v,Df),e(Df,RQ),e(RQ,Rje),e(Df,Sje),e(Df,b9),e(b9,Pje),e(Df,$je),e(v,Ije),e(v,jf),e(jf,SQ),e(SQ,Dje),e(jf,jje),e(jf,v9),e(v9,Nje),e(jf,qje),e(v,Gje),e(v,Nf),e(Nf,PQ),e(PQ,Oje),e(Nf,Xje),e(Nf,T9),e(T9,Vje),e(Nf,zje),e(v,Wje),e(v,qf),e(qf,$Q),e($Q,Qje),e(qf,Hje),e(qf,F9),e(F9,Uje),e(qf,Jje),e(v,Yje),e(v,Gf),e(Gf,IQ),e(IQ,Kje),e(Gf,Zje),e(Gf,C9),e(C9,eNe),e(Gf,oNe),e(v,rNe),e(v,Of),e(Of,DQ),e(DQ,tNe),e(Of,aNe),e(Of,M9),e(M9,sNe),e(Of,nNe),e(v,lNe),e(v,Xf),e(Xf,jQ),e(jQ,iNe),e(Xf,dNe),e(Xf,E9),e(E9,cNe),e(Xf,mNe),e(v,fNe),e(v,Vf),e(Vf,NQ),e(NQ,gNe),e(Vf,hNe),e(Vf,y9),e(y9,uNe),e(Vf,pNe),e(v,_Ne),e(v,zf),e(zf,qQ),e(qQ,bNe),e(zf,vNe),e(zf,w9),e(w9,TNe),e(zf,FNe),e(v,CNe),e(v,Wf),e(Wf,GQ),e(GQ,MNe),e(Wf,ENe),e(Wf,A9),e(A9,yNe),e(Wf,wNe),e(v,ANe),e(v,Qf),e(Qf,OQ),e(OQ,LNe),e(Qf,BNe),e(Qf,L9),e(L9,xNe),e(Qf,kNe),e(v,RNe),e(v,Hf),e(Hf,XQ),e(XQ,SNe),e(Hf,PNe),e(Hf,B9),e(B9,$Ne),e(Hf,INe),e(v,DNe),e(v,Uf),e(Uf,VQ),e(VQ,jNe),e(Uf,NNe),e(Uf,x9),e(x9,qNe),e(Uf,GNe),e(v,ONe),e(v,Jf),e(Jf,zQ),e(zQ,XNe),e(Jf,VNe),e(Jf,k9),e(k9,zNe),e(Jf,WNe),e(v,QNe),e(v,Yf),e(Yf,WQ),e(WQ,HNe),e(Yf,UNe),e(Yf,R9),e(R9,JNe),e(Yf,YNe),e(v,KNe),e(v,Kf),e(Kf,QQ),e(QQ,ZNe),e(Kf,eqe),e(Kf,S9),e(S9,oqe),e(Kf,rqe),e(v,tqe),e(v,Zf),e(Zf,HQ),e(HQ,aqe),e(Zf,sqe),e(Zf,P9),e(P9,nqe),e(Zf,lqe),e(v,iqe),e(v,eg),e(eg,UQ),e(UQ,dqe),e(eg,cqe),e(eg,$9),e($9,mqe),e(eg,fqe),e(v,gqe),e(v,og),e(og,JQ),e(JQ,hqe),e(og,uqe),e(og,I9),e(I9,pqe),e(og,_qe),e(v,bqe),e(v,rg),e(rg,YQ),e(YQ,vqe),e(rg,Tqe),e(rg,D9),e(D9,Fqe),e(rg,Cqe),e(v,Mqe),e(v,tg),e(tg,KQ),e(KQ,Eqe),e(tg,yqe),e(tg,j9),e(j9,wqe),e(tg,Aqe),e(v,Lqe),e(v,ag),e(ag,ZQ),e(ZQ,Bqe),e(ag,xqe),e(ag,N9),e(N9,kqe),e(ag,Rqe),e(v,Sqe),e(v,sg),e(sg,eH),e(eH,Pqe),e(sg,$qe),e(sg,q9),e(q9,Iqe),e(sg,Dqe),e(v,jqe),e(v,ng),e(ng,oH),e(oH,Nqe),e(ng,qqe),e(ng,G9),e(G9,Gqe),e(ng,Oqe),e(v,Xqe),e(v,lg),e(lg,rH),e(rH,Vqe),e(lg,zqe),e(lg,O9),e(O9,Wqe),e(lg,Qqe),e(v,Hqe),e(v,ig),e(ig,tH),e(tH,Uqe),e(ig,Jqe),e(ig,X9),e(X9,Yqe),e(ig,Kqe),e(v,Zqe),e(v,dg),e(dg,aH),e(aH,eGe),e(dg,oGe),e(dg,V9),e(V9,rGe),e(dg,tGe),e(v,aGe),e(v,cg),e(cg,sH),e(sH,sGe),e(cg,nGe),e(cg,z9),e(z9,lGe),e(cg,iGe),e(v,dGe),e(v,mg),e(mg,nH),e(nH,cGe),e(mg,mGe),e(mg,W9),e(W9,fGe),e(mg,gGe),e(v,hGe),e(v,fg),e(fg,lH),e(lH,uGe),e(fg,pGe),e(fg,Q9),e(Q9,_Ge),e(fg,bGe),e(v,vGe),e(v,gg),e(gg,iH),e(iH,TGe),e(gg,FGe),e(gg,H9),e(H9,CGe),e(gg,MGe),e(v,EGe),e(v,hg),e(hg,dH),e(dH,yGe),e(hg,wGe),e(hg,U9),e(U9,AGe),e(hg,LGe),e(v,BGe),e(v,ug),e(ug,cH),e(cH,xGe),e(ug,kGe),e(ug,J9),e(J9,RGe),e(ug,SGe),e(mo,PGe),e(mo,mH),e(mH,$Ge),e(mo,IGe),g(k5,mo,null),e(Xo,DGe),e(Xo,pg),g(R5,pg,null),e(pg,jGe),e(pg,fH),e(fH,NGe),b(c,E9e,_),b(c,ji,_),e(ji,_g),e(_g,gH),g(S5,gH,null),e(ji,qGe),e(ji,hH),e(hH,GGe),b(c,y9e,_),b(c,Vo,_),g(P5,Vo,null),e(Vo,OGe),e(Vo,$5),e($5,XGe),e($5,Y9),e(Y9,VGe),e($5,zGe),e(Vo,WGe),e(Vo,I5),e(I5,QGe),e(I5,uH),e(uH,HGe),e(I5,UGe),e(Vo,JGe),e(Vo,fo),g(D5,fo,null),e(fo,YGe),e(fo,pH),e(pH,KGe),e(fo,ZGe),e(fo,Da),e(Da,eOe),e(Da,_H),e(_H,oOe),e(Da,rOe),e(Da,bH),e(bH,tOe),e(Da,aOe),e(Da,vH),e(vH,sOe),e(Da,nOe),e(fo,lOe),e(fo,M),e(M,qs),e(qs,TH),e(TH,iOe),e(qs,dOe),e(qs,K9),e(K9,cOe),e(qs,mOe),e(qs,Z9),e(Z9,fOe),e(qs,gOe),e(M,hOe),e(M,Gs),e(Gs,FH),e(FH,uOe),e(Gs,pOe),e(Gs,eB),e(eB,_Oe),e(Gs,bOe),e(Gs,oB),e(oB,vOe),e(Gs,TOe),e(M,FOe),e(M,Os),e(Os,CH),e(CH,COe),e(Os,MOe),e(Os,rB),e(rB,EOe),e(Os,yOe),e(Os,tB),e(tB,wOe),e(Os,AOe),e(M,LOe),e(M,bg),e(bg,MH),e(MH,BOe),e(bg,xOe),e(bg,aB),e(aB,kOe),e(bg,ROe),e(M,SOe),e(M,Xs),e(Xs,EH),e(EH,POe),e(Xs,$Oe),e(Xs,sB),e(sB,IOe),e(Xs,DOe),e(Xs,nB),e(nB,jOe),e(Xs,NOe),e(M,qOe),e(M,vg),e(vg,yH),e(yH,GOe),e(vg,OOe),e(vg,lB),e(lB,XOe),e(vg,VOe),e(M,zOe),e(M,Tg),e(Tg,wH),e(wH,WOe),e(Tg,QOe),e(Tg,iB),e(iB,HOe),e(Tg,UOe),e(M,JOe),e(M,Fg),e(Fg,AH),e(AH,YOe),e(Fg,KOe),e(Fg,dB),e(dB,ZOe),e(Fg,eXe),e(M,oXe),e(M,Vs),e(Vs,LH),e(LH,rXe),e(Vs,tXe),e(Vs,cB),e(cB,aXe),e(Vs,sXe),e(Vs,mB),e(mB,nXe),e(Vs,lXe),e(M,iXe),e(M,zs),e(zs,BH),e(BH,dXe),e(zs,cXe),e(zs,fB),e(fB,mXe),e(zs,fXe),e(zs,gB),e(gB,gXe),e(zs,hXe),e(M,uXe),e(M,Ws),e(Ws,xH),e(xH,pXe),e(Ws,_Xe),e(Ws,hB),e(hB,bXe),e(Ws,vXe),e(Ws,uB),e(uB,TXe),e(Ws,FXe),e(M,CXe),e(M,Cg),e(Cg,kH),e(kH,MXe),e(Cg,EXe),e(Cg,pB),e(pB,yXe),e(Cg,wXe),e(M,AXe),e(M,Mg),e(Mg,RH),e(RH,LXe),e(Mg,BXe),e(Mg,_B),e(_B,xXe),e(Mg,kXe),e(M,RXe),e(M,Qs),e(Qs,SH),e(SH,SXe),e(Qs,PXe),e(Qs,bB),e(bB,$Xe),e(Qs,IXe),e(Qs,vB),e(vB,DXe),e(Qs,jXe),e(M,NXe),e(M,Eg),e(Eg,PH),e(PH,qXe),e(Eg,GXe),e(Eg,TB),e(TB,OXe),e(Eg,XXe),e(M,VXe),e(M,Hs),e(Hs,$H),e($H,zXe),e(Hs,WXe),e(Hs,FB),e(FB,QXe),e(Hs,HXe),e(Hs,CB),e(CB,UXe),e(Hs,JXe),e(M,YXe),e(M,Us),e(Us,IH),e(IH,KXe),e(Us,ZXe),e(Us,MB),e(MB,eVe),e(Us,oVe),e(Us,EB),e(EB,rVe),e(Us,tVe),e(M,aVe),e(M,Js),e(Js,DH),e(DH,sVe),e(Js,nVe),e(Js,yB),e(yB,lVe),e(Js,iVe),e(Js,jH),e(jH,dVe),e(Js,cVe),e(M,mVe),e(M,yg),e(yg,NH),e(NH,fVe),e(yg,gVe),e(yg,wB),e(wB,hVe),e(yg,uVe),e(M,pVe),e(M,Ys),e(Ys,qH),e(qH,_Ve),e(Ys,bVe),e(Ys,AB),e(AB,vVe),e(Ys,TVe),e(Ys,LB),e(LB,FVe),e(Ys,CVe),e(M,MVe),e(M,wg),e(wg,GH),e(GH,EVe),e(wg,yVe),e(wg,BB),e(BB,wVe),e(wg,AVe),e(M,LVe),e(M,Ks),e(Ks,OH),e(OH,BVe),e(Ks,xVe),e(Ks,xB),e(xB,kVe),e(Ks,RVe),e(Ks,kB),e(kB,SVe),e(Ks,PVe),e(M,$Ve),e(M,Zs),e(Zs,XH),e(XH,IVe),e(Zs,DVe),e(Zs,RB),e(RB,jVe),e(Zs,NVe),e(Zs,SB),e(SB,qVe),e(Zs,GVe),e(M,OVe),e(M,en),e(en,VH),e(VH,XVe),e(en,VVe),e(en,PB),e(PB,zVe),e(en,WVe),e(en,$B),e($B,QVe),e(en,HVe),e(M,UVe),e(M,Ag),e(Ag,zH),e(zH,JVe),e(Ag,YVe),e(Ag,IB),e(IB,KVe),e(Ag,ZVe),e(M,eze),e(M,on),e(on,WH),e(WH,oze),e(on,rze),e(on,DB),e(DB,tze),e(on,aze),e(on,jB),e(jB,sze),e(on,nze),e(M,lze),e(M,Lg),e(Lg,QH),e(QH,ize),e(Lg,dze),e(Lg,NB),e(NB,cze),e(Lg,mze),e(M,fze),e(M,rn),e(rn,HH),e(HH,gze),e(rn,hze),e(rn,qB),e(qB,uze),e(rn,pze),e(rn,GB),e(GB,_ze),e(rn,bze),e(M,vze),e(M,tn),e(tn,UH),e(UH,Tze),e(tn,Fze),e(tn,OB),e(OB,Cze),e(tn,Mze),e(tn,XB),e(XB,Eze),e(tn,yze),e(M,wze),e(M,an),e(an,JH),e(JH,Aze),e(an,Lze),e(an,VB),e(VB,Bze),e(an,xze),e(an,zB),e(zB,kze),e(an,Rze),e(M,Sze),e(M,sn),e(sn,YH),e(YH,Pze),e(sn,$ze),e(sn,WB),e(WB,Ize),e(sn,Dze),e(sn,QB),e(QB,jze),e(sn,Nze),e(M,qze),e(M,Bg),e(Bg,KH),e(KH,Gze),e(Bg,Oze),e(Bg,HB),e(HB,Xze),e(Bg,Vze),e(M,zze),e(M,nn),e(nn,ZH),e(ZH,Wze),e(nn,Qze),e(nn,UB),e(UB,Hze),e(nn,Uze),e(nn,JB),e(JB,Jze),e(nn,Yze),e(M,Kze),e(M,ln),e(ln,eU),e(eU,Zze),e(ln,eWe),e(ln,YB),e(YB,oWe),e(ln,rWe),e(ln,KB),e(KB,tWe),e(ln,aWe),e(M,sWe),e(M,dn),e(dn,oU),e(oU,nWe),e(dn,lWe),e(dn,ZB),e(ZB,iWe),e(dn,dWe),e(dn,ex),e(ex,cWe),e(dn,mWe),e(M,fWe),e(M,cn),e(cn,rU),e(rU,gWe),e(cn,hWe),e(cn,ox),e(ox,uWe),e(cn,pWe),e(cn,rx),e(rx,_We),e(cn,bWe),e(M,vWe),e(M,mn),e(mn,tU),e(tU,TWe),e(mn,FWe),e(mn,tx),e(tx,CWe),e(mn,MWe),e(mn,ax),e(ax,EWe),e(mn,yWe),e(M,wWe),e(M,fn),e(fn,aU),e(aU,AWe),e(fn,LWe),e(fn,sx),e(sx,BWe),e(fn,xWe),e(fn,nx),e(nx,kWe),e(fn,RWe),e(M,SWe),e(M,xg),e(xg,sU),e(sU,PWe),e(xg,$We),e(xg,lx),e(lx,IWe),e(xg,DWe),e(M,jWe),e(M,gn),e(gn,nU),e(nU,NWe),e(gn,qWe),e(gn,ix),e(ix,GWe),e(gn,OWe),e(gn,dx),e(dx,XWe),e(gn,VWe),e(M,zWe),e(M,kg),e(kg,lU),e(lU,WWe),e(kg,QWe),e(kg,cx),e(cx,HWe),e(kg,UWe),e(M,JWe),e(M,Rg),e(Rg,iU),e(iU,YWe),e(Rg,KWe),e(Rg,mx),e(mx,ZWe),e(Rg,eQe),e(M,oQe),e(M,hn),e(hn,dU),e(dU,rQe),e(hn,tQe),e(hn,fx),e(fx,aQe),e(hn,sQe),e(hn,gx),e(gx,nQe),e(hn,lQe),e(M,iQe),e(M,un),e(un,cU),e(cU,dQe),e(un,cQe),e(un,hx),e(hx,mQe),e(un,fQe),e(un,ux),e(ux,gQe),e(un,hQe),e(M,uQe),e(M,Sg),e(Sg,mU),e(mU,pQe),e(Sg,_Qe),e(Sg,px),e(px,bQe),e(Sg,vQe),e(M,TQe),e(M,pn),e(pn,fU),e(fU,FQe),e(pn,CQe),e(pn,_x),e(_x,MQe),e(pn,EQe),e(pn,bx),e(bx,yQe),e(pn,wQe),e(M,AQe),e(M,_n),e(_n,gU),e(gU,LQe),e(_n,BQe),e(_n,vx),e(vx,xQe),e(_n,kQe),e(_n,Tx),e(Tx,RQe),e(_n,SQe),e(M,PQe),e(M,bn),e(bn,hU),e(hU,$Qe),e(bn,IQe),e(bn,Fx),e(Fx,DQe),e(bn,jQe),e(bn,Cx),e(Cx,NQe),e(bn,qQe),e(M,GQe),e(M,vn),e(vn,uU),e(uU,OQe),e(vn,XQe),e(vn,Mx),e(Mx,VQe),e(vn,zQe),e(vn,Ex),e(Ex,WQe),e(vn,QQe),e(M,HQe),e(M,Tn),e(Tn,pU),e(pU,UQe),e(Tn,JQe),e(Tn,yx),e(yx,YQe),e(Tn,KQe),e(Tn,wx),e(wx,ZQe),e(Tn,eHe),e(M,oHe),e(M,Pg),e(Pg,_U),e(_U,rHe),e(Pg,tHe),e(Pg,Ax),e(Ax,aHe),e(Pg,sHe),e(M,nHe),e(M,$g),e($g,bU),e(bU,lHe),e($g,iHe),e($g,Lx),e(Lx,dHe),e($g,cHe),e(M,mHe),e(M,Ig),e(Ig,vU),e(vU,fHe),e(Ig,gHe),e(Ig,Bx),e(Bx,hHe),e(Ig,uHe),e(M,pHe),e(M,Dg),e(Dg,TU),e(TU,_He),e(Dg,bHe),e(Dg,xx),e(xx,vHe),e(Dg,THe),e(M,FHe),e(M,Fn),e(Fn,FU),e(FU,CHe),e(Fn,MHe),e(Fn,kx),e(kx,EHe),e(Fn,yHe),e(Fn,Rx),e(Rx,wHe),e(Fn,AHe),e(M,LHe),e(M,jg),e(jg,CU),e(CU,BHe),e(jg,xHe),e(jg,Sx),e(Sx,kHe),e(jg,RHe),e(M,SHe),e(M,Cn),e(Cn,MU),e(MU,PHe),e(Cn,$He),e(Cn,Px),e(Px,IHe),e(Cn,DHe),e(Cn,$x),e($x,jHe),e(Cn,NHe),e(M,qHe),e(M,Mn),e(Mn,EU),e(EU,GHe),e(Mn,OHe),e(Mn,Ix),e(Ix,XHe),e(Mn,VHe),e(Mn,Dx),e(Dx,zHe),e(Mn,WHe),e(M,QHe),e(M,En),e(En,yU),e(yU,HHe),e(En,UHe),e(En,jx),e(jx,JHe),e(En,YHe),e(En,Nx),e(Nx,KHe),e(En,ZHe),e(M,eUe),e(M,yn),e(yn,wU),e(wU,oUe),e(yn,rUe),e(yn,qx),e(qx,tUe),e(yn,aUe),e(yn,Gx),e(Gx,sUe),e(yn,nUe),e(M,lUe),e(M,wn),e(wn,AU),e(AU,iUe),e(wn,dUe),e(wn,Ox),e(Ox,cUe),e(wn,mUe),e(wn,Xx),e(Xx,fUe),e(wn,gUe),e(M,hUe),e(M,Ng),e(Ng,LU),e(LU,uUe),e(Ng,pUe),e(Ng,Vx),e(Vx,_Ue),e(Ng,bUe),e(M,vUe),e(M,qg),e(qg,BU),e(BU,TUe),e(qg,FUe),e(qg,zx),e(zx,CUe),e(qg,MUe),e(M,EUe),e(M,An),e(An,xU),e(xU,yUe),e(An,wUe),e(An,Wx),e(Wx,AUe),e(An,LUe),e(An,Qx),e(Qx,BUe),e(An,xUe),e(M,kUe),e(M,Ln),e(Ln,kU),e(kU,RUe),e(Ln,SUe),e(Ln,Hx),e(Hx,PUe),e(Ln,$Ue),e(Ln,Ux),e(Ux,IUe),e(Ln,DUe),e(M,jUe),e(M,Bn),e(Bn,RU),e(RU,NUe),e(Bn,qUe),e(Bn,Jx),e(Jx,GUe),e(Bn,OUe),e(Bn,Yx),e(Yx,XUe),e(Bn,VUe),e(M,zUe),e(M,Gg),e(Gg,SU),e(SU,WUe),e(Gg,QUe),e(Gg,Kx),e(Kx,HUe),e(Gg,UUe),e(M,JUe),e(M,Og),e(Og,PU),e(PU,YUe),e(Og,KUe),e(Og,Zx),e(Zx,ZUe),e(Og,eJe),e(M,oJe),e(M,Xg),e(Xg,$U),e($U,rJe),e(Xg,tJe),e(Xg,ek),e(ek,aJe),e(Xg,sJe),e(M,nJe),e(M,Vg),e(Vg,IU),e(IU,lJe),e(Vg,iJe),e(Vg,ok),e(ok,dJe),e(Vg,cJe),e(M,mJe),e(M,xn),e(xn,DU),e(DU,fJe),e(xn,gJe),e(xn,rk),e(rk,hJe),e(xn,uJe),e(xn,tk),e(tk,pJe),e(xn,_Je),e(M,bJe),e(M,zg),e(zg,jU),e(jU,vJe),e(zg,TJe),e(zg,ak),e(ak,FJe),e(zg,CJe),e(M,MJe),e(M,Wg),e(Wg,NU),e(NU,EJe),e(Wg,yJe),e(Wg,sk),e(sk,wJe),e(Wg,AJe),e(M,LJe),e(M,kn),e(kn,qU),e(qU,BJe),e(kn,xJe),e(kn,nk),e(nk,kJe),e(kn,RJe),e(kn,lk),e(lk,SJe),e(kn,PJe),e(M,$Je),e(M,Rn),e(Rn,GU),e(GU,IJe),e(Rn,DJe),e(Rn,ik),e(ik,jJe),e(Rn,NJe),e(Rn,dk),e(dk,qJe),e(Rn,GJe),e(fo,OJe),e(fo,OU),e(OU,XJe),e(fo,VJe),g(j5,fo,null),e(Vo,zJe),e(Vo,Qg),g(N5,Qg,null),e(Qg,WJe),e(Qg,XU),e(XU,QJe),b(c,w9e,_),b(c,Ni,_),e(Ni,Hg),e(Hg,VU),g(q5,VU,null),e(Ni,HJe),e(Ni,zU),e(zU,UJe),b(c,A9e,_),b(c,zo,_),g(G5,zo,null),e(zo,JJe),e(zo,O5),e(O5,YJe),e(O5,ck),e(ck,KJe),e(O5,ZJe),e(zo,eYe),e(zo,X5),e(X5,oYe),e(X5,WU),e(WU,rYe),e(X5,tYe),e(zo,aYe),e(zo,xe),g(V5,xe,null),e(xe,sYe),e(xe,QU),e(QU,nYe),e(xe,lYe),e(xe,ja),e(ja,iYe),e(ja,HU),e(HU,dYe),e(ja,cYe),e(ja,UU),e(UU,mYe),e(ja,fYe),e(ja,JU),e(JU,gYe),e(ja,hYe),e(xe,uYe),e(xe,ne),e(ne,Ug),e(Ug,YU),e(YU,pYe),e(Ug,_Ye),e(Ug,mk),e(mk,bYe),e(Ug,vYe),e(ne,TYe),e(ne,Jg),e(Jg,KU),e(KU,FYe),e(Jg,CYe),e(Jg,fk),e(fk,MYe),e(Jg,EYe),e(ne,yYe),e(ne,Yg),e(Yg,ZU),e(ZU,wYe),e(Yg,AYe),e(Yg,gk),e(gk,LYe),e(Yg,BYe),e(ne,xYe),e(ne,Kg),e(Kg,eJ),e(eJ,kYe),e(Kg,RYe),e(Kg,hk),e(hk,SYe),e(Kg,PYe),e(ne,$Ye),e(ne,Zg),e(Zg,oJ),e(oJ,IYe),e(Zg,DYe),e(Zg,uk),e(uk,jYe),e(Zg,NYe),e(ne,qYe),e(ne,eh),e(eh,rJ),e(rJ,GYe),e(eh,OYe),e(eh,pk),e(pk,XYe),e(eh,VYe),e(ne,zYe),e(ne,oh),e(oh,tJ),e(tJ,WYe),e(oh,QYe),e(oh,_k),e(_k,HYe),e(oh,UYe),e(ne,JYe),e(ne,rh),e(rh,aJ),e(aJ,YYe),e(rh,KYe),e(rh,bk),e(bk,ZYe),e(rh,eKe),e(ne,oKe),e(ne,th),e(th,sJ),e(sJ,rKe),e(th,tKe),e(th,vk),e(vk,aKe),e(th,sKe),e(ne,nKe),e(ne,ah),e(ah,nJ),e(nJ,lKe),e(ah,iKe),e(ah,Tk),e(Tk,dKe),e(ah,cKe),e(ne,mKe),e(ne,sh),e(sh,lJ),e(lJ,fKe),e(sh,gKe),e(sh,Fk),e(Fk,hKe),e(sh,uKe),e(ne,pKe),e(ne,nh),e(nh,iJ),e(iJ,_Ke),e(nh,bKe),e(nh,Ck),e(Ck,vKe),e(nh,TKe),e(ne,FKe),e(ne,lh),e(lh,dJ),e(dJ,CKe),e(lh,MKe),e(lh,Mk),e(Mk,EKe),e(lh,yKe),e(ne,wKe),e(ne,ih),e(ih,cJ),e(cJ,AKe),e(ih,LKe),e(ih,Ek),e(Ek,BKe),e(ih,xKe),e(ne,kKe),e(ne,dh),e(dh,mJ),e(mJ,RKe),e(dh,SKe),e(dh,yk),e(yk,PKe),e(dh,$Ke),e(xe,IKe),g(ch,xe,null),e(xe,DKe),e(xe,fJ),e(fJ,jKe),e(xe,NKe),g(z5,xe,null),e(zo,qKe),e(zo,mh),g(W5,mh,null),e(mh,GKe),e(mh,gJ),e(gJ,OKe),b(c,L9e,_),b(c,qi,_),e(qi,fh),e(fh,hJ),g(Q5,hJ,null),e(qi,XKe),e(qi,uJ),e(uJ,VKe),b(c,B9e,_),b(c,Wo,_),g(H5,Wo,null),e(Wo,zKe),e(Wo,U5),e(U5,WKe),e(U5,wk),e(wk,QKe),e(U5,HKe),e(Wo,UKe),e(Wo,J5),e(J5,JKe),e(J5,pJ),e(pJ,YKe),e(J5,KKe),e(Wo,ZKe),e(Wo,ke),g(Y5,ke,null),e(ke,eZe),e(ke,_J),e(_J,oZe),e(ke,rZe),e(ke,Gi),e(Gi,tZe),e(Gi,bJ),e(bJ,aZe),e(Gi,sZe),e(Gi,vJ),e(vJ,nZe),e(Gi,lZe),e(ke,iZe),e(ke,we),e(we,gh),e(gh,TJ),e(TJ,dZe),e(gh,cZe),e(gh,Ak),e(Ak,mZe),e(gh,fZe),e(we,gZe),e(we,hh),e(hh,FJ),e(FJ,hZe),e(hh,uZe),e(hh,Lk),e(Lk,pZe),e(hh,_Ze),e(we,bZe),e(we,uh),e(uh,CJ),e(CJ,vZe),e(uh,TZe),e(uh,Bk),e(Bk,FZe),e(uh,CZe),e(we,MZe),e(we,ph),e(ph,MJ),e(MJ,EZe),e(ph,yZe),e(ph,xk),e(xk,wZe),e(ph,AZe),e(we,LZe),e(we,_h),e(_h,EJ),e(EJ,BZe),e(_h,xZe),e(_h,kk),e(kk,kZe),e(_h,RZe),e(we,SZe),e(we,bh),e(bh,yJ),e(yJ,PZe),e(bh,$Ze),e(bh,Rk),e(Rk,IZe),e(bh,DZe),e(we,jZe),e(we,vh),e(vh,wJ),e(wJ,NZe),e(vh,qZe),e(vh,Sk),e(Sk,GZe),e(vh,OZe),e(we,XZe),e(we,Th),e(Th,AJ),e(AJ,VZe),e(Th,zZe),e(Th,Pk),e(Pk,WZe),e(Th,QZe),e(ke,HZe),g(Fh,ke,null),e(ke,UZe),e(ke,LJ),e(LJ,JZe),e(ke,YZe),g(K5,ke,null),e(Wo,KZe),e(Wo,Ch),g(Z5,Ch,null),e(Ch,ZZe),e(Ch,BJ),e(BJ,eeo),b(c,x9e,_),b(c,Oi,_),e(Oi,Mh),e(Mh,xJ),g(ey,xJ,null),e(Oi,oeo),e(Oi,kJ),e(kJ,reo),b(c,k9e,_),b(c,Qo,_),g(oy,Qo,null),e(Qo,teo),e(Qo,Xi),e(Xi,aeo),e(Xi,RJ),e(RJ,seo),e(Xi,neo),e(Xi,SJ),e(SJ,leo),e(Xi,ieo),e(Qo,deo),e(Qo,ry),e(ry,ceo),e(ry,PJ),e(PJ,meo),e(ry,feo),e(Qo,geo),e(Qo,qr),g(ty,qr,null),e(qr,heo),e(qr,$J),e($J,ueo),e(qr,peo),e(qr,Vi),e(Vi,_eo),e(Vi,IJ),e(IJ,beo),e(Vi,veo),e(Vi,DJ),e(DJ,Teo),e(Vi,Feo),e(qr,Ceo),e(qr,jJ),e(jJ,Meo),e(qr,Eeo),g(ay,qr,null),e(Qo,yeo),e(Qo,Re),g(sy,Re,null),e(Re,weo),e(Re,NJ),e(NJ,Aeo),e(Re,Leo),e(Re,Na),e(Na,Beo),e(Na,qJ),e(qJ,xeo),e(Na,keo),e(Na,GJ),e(GJ,Reo),e(Na,Seo),e(Na,OJ),e(OJ,Peo),e(Na,$eo),e(Re,Ieo),e(Re,F),e(F,Eh),e(Eh,XJ),e(XJ,Deo),e(Eh,jeo),e(Eh,$k),e($k,Neo),e(Eh,qeo),e(F,Geo),e(F,yh),e(yh,VJ),e(VJ,Oeo),e(yh,Xeo),e(yh,Ik),e(Ik,Veo),e(yh,zeo),e(F,Weo),e(F,wh),e(wh,zJ),e(zJ,Qeo),e(wh,Heo),e(wh,Dk),e(Dk,Ueo),e(wh,Jeo),e(F,Yeo),e(F,Ah),e(Ah,WJ),e(WJ,Keo),e(Ah,Zeo),e(Ah,jk),e(jk,eoo),e(Ah,ooo),e(F,roo),e(F,Lh),e(Lh,QJ),e(QJ,too),e(Lh,aoo),e(Lh,Nk),e(Nk,soo),e(Lh,noo),e(F,loo),e(F,Bh),e(Bh,HJ),e(HJ,ioo),e(Bh,doo),e(Bh,qk),e(qk,coo),e(Bh,moo),e(F,foo),e(F,xh),e(xh,UJ),e(UJ,goo),e(xh,hoo),e(xh,Gk),e(Gk,uoo),e(xh,poo),e(F,_oo),e(F,kh),e(kh,JJ),e(JJ,boo),e(kh,voo),e(kh,Ok),e(Ok,Too),e(kh,Foo),e(F,Coo),e(F,Rh),e(Rh,YJ),e(YJ,Moo),e(Rh,Eoo),e(Rh,Xk),e(Xk,yoo),e(Rh,woo),e(F,Aoo),e(F,Sh),e(Sh,KJ),e(KJ,Loo),e(Sh,Boo),e(Sh,Vk),e(Vk,xoo),e(Sh,koo),e(F,Roo),e(F,Ph),e(Ph,ZJ),e(ZJ,Soo),e(Ph,Poo),e(Ph,zk),e(zk,$oo),e(Ph,Ioo),e(F,Doo),e(F,$h),e($h,eY),e(eY,joo),e($h,Noo),e($h,Wk),e(Wk,qoo),e($h,Goo),e(F,Ooo),e(F,Ih),e(Ih,oY),e(oY,Xoo),e(Ih,Voo),e(Ih,Qk),e(Qk,zoo),e(Ih,Woo),e(F,Qoo),e(F,Dh),e(Dh,rY),e(rY,Hoo),e(Dh,Uoo),e(Dh,Hk),e(Hk,Joo),e(Dh,Yoo),e(F,Koo),e(F,jh),e(jh,tY),e(tY,Zoo),e(jh,ero),e(jh,Uk),e(Uk,oro),e(jh,rro),e(F,tro),e(F,Nh),e(Nh,aY),e(aY,aro),e(Nh,sro),e(Nh,Jk),e(Jk,nro),e(Nh,lro),e(F,iro),e(F,qh),e(qh,sY),e(sY,dro),e(qh,cro),e(qh,Yk),e(Yk,mro),e(qh,fro),e(F,gro),e(F,Gh),e(Gh,nY),e(nY,hro),e(Gh,uro),e(Gh,Kk),e(Kk,pro),e(Gh,_ro),e(F,bro),e(F,Oh),e(Oh,lY),e(lY,vro),e(Oh,Tro),e(Oh,Zk),e(Zk,Fro),e(Oh,Cro),e(F,Mro),e(F,Xh),e(Xh,iY),e(iY,Ero),e(Xh,yro),e(Xh,eR),e(eR,wro),e(Xh,Aro),e(F,Lro),e(F,Vh),e(Vh,dY),e(dY,Bro),e(Vh,xro),e(Vh,oR),e(oR,kro),e(Vh,Rro),e(F,Sro),e(F,zh),e(zh,cY),e(cY,Pro),e(zh,$ro),e(zh,rR),e(rR,Iro),e(zh,Dro),e(F,jro),e(F,Wh),e(Wh,mY),e(mY,Nro),e(Wh,qro),e(Wh,tR),e(tR,Gro),e(Wh,Oro),e(F,Xro),e(F,Qh),e(Qh,fY),e(fY,Vro),e(Qh,zro),e(Qh,aR),e(aR,Wro),e(Qh,Qro),e(F,Hro),e(F,Hh),e(Hh,gY),e(gY,Uro),e(Hh,Jro),e(Hh,sR),e(sR,Yro),e(Hh,Kro),e(F,Zro),e(F,Uh),e(Uh,hY),e(hY,eto),e(Uh,oto),e(Uh,nR),e(nR,rto),e(Uh,tto),e(F,ato),e(F,Jh),e(Jh,uY),e(uY,sto),e(Jh,nto),e(Jh,lR),e(lR,lto),e(Jh,ito),e(F,dto),e(F,Sn),e(Sn,pY),e(pY,cto),e(Sn,mto),e(Sn,iR),e(iR,fto),e(Sn,gto),e(Sn,dR),e(dR,hto),e(Sn,uto),e(F,pto),e(F,Yh),e(Yh,_Y),e(_Y,_to),e(Yh,bto),e(Yh,cR),e(cR,vto),e(Yh,Tto),e(F,Fto),e(F,Kh),e(Kh,bY),e(bY,Cto),e(Kh,Mto),e(Kh,mR),e(mR,Eto),e(Kh,yto),e(F,wto),e(F,Zh),e(Zh,vY),e(vY,Ato),e(Zh,Lto),e(Zh,fR),e(fR,Bto),e(Zh,xto),e(F,kto),e(F,eu),e(eu,TY),e(TY,Rto),e(eu,Sto),e(eu,gR),e(gR,Pto),e(eu,$to),e(F,Ito),e(F,ou),e(ou,FY),e(FY,Dto),e(ou,jto),e(ou,hR),e(hR,Nto),e(ou,qto),e(F,Gto),e(F,ru),e(ru,CY),e(CY,Oto),e(ru,Xto),e(ru,uR),e(uR,Vto),e(ru,zto),e(F,Wto),e(F,tu),e(tu,MY),e(MY,Qto),e(tu,Hto),e(tu,pR),e(pR,Uto),e(tu,Jto),e(F,Yto),e(F,au),e(au,EY),e(EY,Kto),e(au,Zto),e(au,_R),e(_R,eao),e(au,oao),e(F,rao),e(F,su),e(su,yY),e(yY,tao),e(su,aao),e(su,bR),e(bR,sao),e(su,nao),e(F,lao),e(F,nu),e(nu,wY),e(wY,iao),e(nu,dao),e(nu,vR),e(vR,cao),e(nu,mao),e(F,fao),e(F,lu),e(lu,AY),e(AY,gao),e(lu,hao),e(lu,TR),e(TR,uao),e(lu,pao),e(F,_ao),e(F,iu),e(iu,LY),e(LY,bao),e(iu,vao),e(iu,FR),e(FR,Tao),e(iu,Fao),e(F,Cao),e(F,du),e(du,BY),e(BY,Mao),e(du,Eao),e(du,CR),e(CR,yao),e(du,wao),e(F,Aao),e(F,cu),e(cu,xY),e(xY,Lao),e(cu,Bao),e(cu,MR),e(MR,xao),e(cu,kao),e(F,Rao),e(F,mu),e(mu,kY),e(kY,Sao),e(mu,Pao),e(mu,ER),e(ER,$ao),e(mu,Iao),e(F,Dao),e(F,fu),e(fu,RY),e(RY,jao),e(fu,Nao),e(fu,yR),e(yR,qao),e(fu,Gao),e(F,Oao),e(F,gu),e(gu,SY),e(SY,Xao),e(gu,Vao),e(gu,wR),e(wR,zao),e(gu,Wao),e(F,Qao),e(F,hu),e(hu,PY),e(PY,Hao),e(hu,Uao),e(hu,AR),e(AR,Jao),e(hu,Yao),e(F,Kao),e(F,uu),e(uu,$Y),e($Y,Zao),e(uu,eso),e(uu,LR),e(LR,oso),e(uu,rso),e(F,tso),e(F,pu),e(pu,IY),e(IY,aso),e(pu,sso),e(pu,BR),e(BR,nso),e(pu,lso),e(F,iso),e(F,_u),e(_u,DY),e(DY,dso),e(_u,cso),e(_u,xR),e(xR,mso),e(_u,fso),e(F,gso),e(F,bu),e(bu,jY),e(jY,hso),e(bu,uso),e(bu,kR),e(kR,pso),e(bu,_so),e(F,bso),e(F,vu),e(vu,NY),e(NY,vso),e(vu,Tso),e(vu,RR),e(RR,Fso),e(vu,Cso),e(F,Mso),e(F,Tu),e(Tu,qY),e(qY,Eso),e(Tu,yso),e(Tu,SR),e(SR,wso),e(Tu,Aso),e(F,Lso),e(F,Fu),e(Fu,GY),e(GY,Bso),e(Fu,xso),e(Fu,PR),e(PR,kso),e(Fu,Rso),e(F,Sso),e(F,Cu),e(Cu,OY),e(OY,Pso),e(Cu,$so),e(Cu,$R),e($R,Iso),e(Cu,Dso),e(F,jso),e(F,Mu),e(Mu,XY),e(XY,Nso),e(Mu,qso),e(Mu,IR),e(IR,Gso),e(Mu,Oso),e(F,Xso),e(F,Eu),e(Eu,VY),e(VY,Vso),e(Eu,zso),e(Eu,DR),e(DR,Wso),e(Eu,Qso),e(F,Hso),e(F,yu),e(yu,zY),e(zY,Uso),e(yu,Jso),e(yu,jR),e(jR,Yso),e(yu,Kso),e(F,Zso),e(F,wu),e(wu,WY),e(WY,eno),e(wu,ono),e(wu,NR),e(NR,rno),e(wu,tno),e(F,ano),e(F,Au),e(Au,QY),e(QY,sno),e(Au,nno),e(Au,qR),e(qR,lno),e(Au,ino),e(F,dno),e(F,Lu),e(Lu,HY),e(HY,cno),e(Lu,mno),e(Lu,GR),e(GR,fno),e(Lu,gno),e(F,hno),e(F,Bu),e(Bu,UY),e(UY,uno),e(Bu,pno),e(Bu,OR),e(OR,_no),e(Bu,bno),e(F,vno),e(F,xu),e(xu,JY),e(JY,Tno),e(xu,Fno),e(xu,XR),e(XR,Cno),e(xu,Mno),e(F,Eno),e(F,ku),e(ku,YY),e(YY,yno),e(ku,wno),e(ku,VR),e(VR,Ano),e(ku,Lno),e(F,Bno),e(F,Ru),e(Ru,KY),e(KY,xno),e(Ru,kno),e(Ru,zR),e(zR,Rno),e(Ru,Sno),e(F,Pno),e(F,Su),e(Su,ZY),e(ZY,$no),e(Su,Ino),e(Su,WR),e(WR,Dno),e(Su,jno),e(F,Nno),e(F,Pu),e(Pu,eK),e(eK,qno),e(Pu,Gno),e(Pu,QR),e(QR,Ono),e(Pu,Xno),e(F,Vno),e(F,$u),e($u,oK),e(oK,zno),e($u,Wno),e($u,HR),e(HR,Qno),e($u,Hno),e(F,Uno),e(F,Iu),e(Iu,rK),e(rK,Jno),e(Iu,Yno),e(Iu,UR),e(UR,Kno),e(Iu,Zno),e(F,elo),e(F,Du),e(Du,tK),e(tK,olo),e(Du,rlo),e(Du,JR),e(JR,tlo),e(Du,alo),e(F,slo),e(F,ju),e(ju,aK),e(aK,nlo),e(ju,llo),e(ju,YR),e(YR,ilo),e(ju,dlo),e(F,clo),e(F,Nu),e(Nu,sK),e(sK,mlo),e(Nu,flo),e(Nu,KR),e(KR,glo),e(Nu,hlo),e(F,ulo),e(F,qu),e(qu,nK),e(nK,plo),e(qu,_lo),e(qu,ZR),e(ZR,blo),e(qu,vlo),e(F,Tlo),e(F,Gu),e(Gu,lK),e(lK,Flo),e(Gu,Clo),e(Gu,eS),e(eS,Mlo),e(Gu,Elo),e(F,ylo),e(F,Ou),e(Ou,iK),e(iK,wlo),e(Ou,Alo),e(Ou,oS),e(oS,Llo),e(Ou,Blo),e(F,xlo),e(F,Xu),e(Xu,dK),e(dK,klo),e(Xu,Rlo),e(Xu,rS),e(rS,Slo),e(Xu,Plo),e(F,$lo),e(F,Vu),e(Vu,cK),e(cK,Ilo),e(Vu,Dlo),e(Vu,tS),e(tS,jlo),e(Vu,Nlo),e(F,qlo),e(F,zu),e(zu,mK),e(mK,Glo),e(zu,Olo),e(zu,aS),e(aS,Xlo),e(zu,Vlo),e(F,zlo),e(F,Wu),e(Wu,fK),e(fK,Wlo),e(Wu,Qlo),e(Wu,sS),e(sS,Hlo),e(Wu,Ulo),e(F,Jlo),e(F,Qu),e(Qu,gK),e(gK,Ylo),e(Qu,Klo),e(Qu,nS),e(nS,Zlo),e(Qu,eio),e(F,oio),e(F,Hu),e(Hu,hK),e(hK,rio),e(Hu,tio),e(Hu,lS),e(lS,aio),e(Hu,sio),e(F,nio),e(F,Uu),e(Uu,uK),e(uK,lio),e(Uu,iio),e(Uu,iS),e(iS,dio),e(Uu,cio),e(F,mio),e(F,Ju),e(Ju,pK),e(pK,fio),e(Ju,gio),e(Ju,dS),e(dS,hio),e(Ju,uio),e(F,pio),e(F,Yu),e(Yu,_K),e(_K,_io),e(Yu,bio),e(Yu,cS),e(cS,vio),e(Yu,Tio),e(F,Fio),e(F,Ku),e(Ku,bK),e(bK,Cio),e(Ku,Mio),e(Ku,mS),e(mS,Eio),e(Ku,yio),e(F,wio),e(F,Zu),e(Zu,vK),e(vK,Aio),e(Zu,Lio),e(Zu,fS),e(fS,Bio),e(Zu,xio),e(F,kio),e(F,ep),e(ep,TK),e(TK,Rio),e(ep,Sio),e(ep,gS),e(gS,Pio),e(ep,$io),e(Re,Iio),e(Re,op),e(op,Dio),e(op,FK),e(FK,jio),e(op,Nio),e(op,CK),e(CK,qio),e(Re,Gio),e(Re,MK),e(MK,Oio),e(Re,Xio),g(ny,Re,null),b(c,R9e,_),b(c,zi,_),e(zi,rp),e(rp,EK),g(ly,EK,null),e(zi,Vio),e(zi,yK),e(yK,zio),b(c,S9e,_),b(c,Ho,_),g(iy,Ho,null),e(Ho,Wio),e(Ho,Wi),e(Wi,Qio),e(Wi,wK),e(wK,Hio),e(Wi,Uio),e(Wi,AK),e(AK,Jio),e(Wi,Yio),e(Ho,Kio),e(Ho,dy),e(dy,Zio),e(dy,LK),e(LK,edo),e(dy,odo),e(Ho,rdo),e(Ho,Gr),g(cy,Gr,null),e(Gr,tdo),e(Gr,BK),e(BK,ado),e(Gr,sdo),e(Gr,Qi),e(Qi,ndo),e(Qi,xK),e(xK,ldo),e(Qi,ido),e(Qi,kK),e(kK,ddo),e(Qi,cdo),e(Gr,mdo),e(Gr,RK),e(RK,fdo),e(Gr,gdo),g(my,Gr,null),e(Ho,hdo),e(Ho,Se),g(fy,Se,null),e(Se,udo),e(Se,SK),e(SK,pdo),e(Se,_do),e(Se,qa),e(qa,bdo),e(qa,PK),e(PK,vdo),e(qa,Tdo),e(qa,$K),e($K,Fdo),e(qa,Cdo),e(qa,IK),e(IK,Mdo),e(qa,Edo),e(Se,ydo),e(Se,k),e(k,tp),e(tp,DK),e(DK,wdo),e(tp,Ado),e(tp,hS),e(hS,Ldo),e(tp,Bdo),e(k,xdo),e(k,ap),e(ap,jK),e(jK,kdo),e(ap,Rdo),e(ap,uS),e(uS,Sdo),e(ap,Pdo),e(k,$do),e(k,sp),e(sp,NK),e(NK,Ido),e(sp,Ddo),e(sp,pS),e(pS,jdo),e(sp,Ndo),e(k,qdo),e(k,np),e(np,qK),e(qK,Gdo),e(np,Odo),e(np,_S),e(_S,Xdo),e(np,Vdo),e(k,zdo),e(k,lp),e(lp,GK),e(GK,Wdo),e(lp,Qdo),e(lp,bS),e(bS,Hdo),e(lp,Udo),e(k,Jdo),e(k,ip),e(ip,OK),e(OK,Ydo),e(ip,Kdo),e(ip,vS),e(vS,Zdo),e(ip,eco),e(k,oco),e(k,dp),e(dp,XK),e(XK,rco),e(dp,tco),e(dp,TS),e(TS,aco),e(dp,sco),e(k,nco),e(k,cp),e(cp,VK),e(VK,lco),e(cp,ico),e(cp,FS),e(FS,dco),e(cp,cco),e(k,mco),e(k,mp),e(mp,zK),e(zK,fco),e(mp,gco),e(mp,CS),e(CS,hco),e(mp,uco),e(k,pco),e(k,fp),e(fp,WK),e(WK,_co),e(fp,bco),e(fp,MS),e(MS,vco),e(fp,Tco),e(k,Fco),e(k,gp),e(gp,QK),e(QK,Cco),e(gp,Mco),e(gp,ES),e(ES,Eco),e(gp,yco),e(k,wco),e(k,hp),e(hp,HK),e(HK,Aco),e(hp,Lco),e(hp,yS),e(yS,Bco),e(hp,xco),e(k,kco),e(k,up),e(up,UK),e(UK,Rco),e(up,Sco),e(up,wS),e(wS,Pco),e(up,$co),e(k,Ico),e(k,pp),e(pp,JK),e(JK,Dco),e(pp,jco),e(pp,AS),e(AS,Nco),e(pp,qco),e(k,Gco),e(k,_p),e(_p,YK),e(YK,Oco),e(_p,Xco),e(_p,LS),e(LS,Vco),e(_p,zco),e(k,Wco),e(k,bp),e(bp,KK),e(KK,Qco),e(bp,Hco),e(bp,BS),e(BS,Uco),e(bp,Jco),e(k,Yco),e(k,vp),e(vp,ZK),e(ZK,Kco),e(vp,Zco),e(vp,xS),e(xS,emo),e(vp,omo),e(k,rmo),e(k,Tp),e(Tp,eZ),e(eZ,tmo),e(Tp,amo),e(Tp,kS),e(kS,smo),e(Tp,nmo),e(k,lmo),e(k,Fp),e(Fp,oZ),e(oZ,imo),e(Fp,dmo),e(Fp,RS),e(RS,cmo),e(Fp,mmo),e(k,fmo),e(k,Cp),e(Cp,rZ),e(rZ,gmo),e(Cp,hmo),e(Cp,SS),e(SS,umo),e(Cp,pmo),e(k,_mo),e(k,Mp),e(Mp,tZ),e(tZ,bmo),e(Mp,vmo),e(Mp,PS),e(PS,Tmo),e(Mp,Fmo),e(k,Cmo),e(k,Ep),e(Ep,aZ),e(aZ,Mmo),e(Ep,Emo),e(Ep,$S),e($S,ymo),e(Ep,wmo),e(k,Amo),e(k,yp),e(yp,sZ),e(sZ,Lmo),e(yp,Bmo),e(yp,IS),e(IS,xmo),e(yp,kmo),e(k,Rmo),e(k,wp),e(wp,nZ),e(nZ,Smo),e(wp,Pmo),e(wp,DS),e(DS,$mo),e(wp,Imo),e(k,Dmo),e(k,Ap),e(Ap,lZ),e(lZ,jmo),e(Ap,Nmo),e(Ap,jS),e(jS,qmo),e(Ap,Gmo),e(k,Omo),e(k,Lp),e(Lp,iZ),e(iZ,Xmo),e(Lp,Vmo),e(Lp,NS),e(NS,zmo),e(Lp,Wmo),e(k,Qmo),e(k,Bp),e(Bp,dZ),e(dZ,Hmo),e(Bp,Umo),e(Bp,qS),e(qS,Jmo),e(Bp,Ymo),e(k,Kmo),e(k,xp),e(xp,cZ),e(cZ,Zmo),e(xp,efo),e(xp,GS),e(GS,ofo),e(xp,rfo),e(k,tfo),e(k,kp),e(kp,mZ),e(mZ,afo),e(kp,sfo),e(kp,OS),e(OS,nfo),e(kp,lfo),e(k,ifo),e(k,Rp),e(Rp,fZ),e(fZ,dfo),e(Rp,cfo),e(Rp,XS),e(XS,mfo),e(Rp,ffo),e(k,gfo),e(k,Sp),e(Sp,gZ),e(gZ,hfo),e(Sp,ufo),e(Sp,VS),e(VS,pfo),e(Sp,_fo),e(k,bfo),e(k,Pp),e(Pp,hZ),e(hZ,vfo),e(Pp,Tfo),e(Pp,zS),e(zS,Ffo),e(Pp,Cfo),e(k,Mfo),e(k,$p),e($p,uZ),e(uZ,Efo),e($p,yfo),e($p,WS),e(WS,wfo),e($p,Afo),e(k,Lfo),e(k,Ip),e(Ip,pZ),e(pZ,Bfo),e(Ip,xfo),e(Ip,QS),e(QS,kfo),e(Ip,Rfo),e(k,Sfo),e(k,Dp),e(Dp,_Z),e(_Z,Pfo),e(Dp,$fo),e(Dp,HS),e(HS,Ifo),e(Dp,Dfo),e(k,jfo),e(k,jp),e(jp,bZ),e(bZ,Nfo),e(jp,qfo),e(jp,US),e(US,Gfo),e(jp,Ofo),e(k,Xfo),e(k,Np),e(Np,vZ),e(vZ,Vfo),e(Np,zfo),e(Np,JS),e(JS,Wfo),e(Np,Qfo),e(k,Hfo),e(k,qp),e(qp,TZ),e(TZ,Ufo),e(qp,Jfo),e(qp,YS),e(YS,Yfo),e(qp,Kfo),e(k,Zfo),e(k,Gp),e(Gp,FZ),e(FZ,ego),e(Gp,ogo),e(Gp,KS),e(KS,rgo),e(Gp,tgo),e(Se,ago),e(Se,Op),e(Op,sgo),e(Op,CZ),e(CZ,ngo),e(Op,lgo),e(Op,MZ),e(MZ,igo),e(Se,dgo),e(Se,EZ),e(EZ,cgo),e(Se,mgo),g(gy,Se,null),b(c,P9e,_),b(c,Hi,_),e(Hi,Xp),e(Xp,yZ),g(hy,yZ,null),e(Hi,fgo),e(Hi,wZ),e(wZ,ggo),b(c,$9e,_),b(c,Uo,_),g(uy,Uo,null),e(Uo,hgo),e(Uo,Ui),e(Ui,ugo),e(Ui,AZ),e(AZ,pgo),e(Ui,_go),e(Ui,LZ),e(LZ,bgo),e(Ui,vgo),e(Uo,Tgo),e(Uo,py),e(py,Fgo),e(py,BZ),e(BZ,Cgo),e(py,Mgo),e(Uo,Ego),e(Uo,Or),g(_y,Or,null),e(Or,ygo),e(Or,xZ),e(xZ,wgo),e(Or,Ago),e(Or,Ji),e(Ji,Lgo),e(Ji,kZ),e(kZ,Bgo),e(Ji,xgo),e(Ji,RZ),e(RZ,kgo),e(Ji,Rgo),e(Or,Sgo),e(Or,SZ),e(SZ,Pgo),e(Or,$go),g(by,Or,null),e(Uo,Igo),e(Uo,Pe),g(vy,Pe,null),e(Pe,Dgo),e(Pe,PZ),e(PZ,jgo),e(Pe,Ngo),e(Pe,Ga),e(Ga,qgo),e(Ga,$Z),e($Z,Ggo),e(Ga,Ogo),e(Ga,IZ),e(IZ,Xgo),e(Ga,Vgo),e(Ga,DZ),e(DZ,zgo),e(Ga,Wgo),e(Pe,Qgo),e(Pe,$),e($,Vp),e(Vp,jZ),e(jZ,Hgo),e(Vp,Ugo),e(Vp,ZS),e(ZS,Jgo),e(Vp,Ygo),e($,Kgo),e($,zp),e(zp,NZ),e(NZ,Zgo),e(zp,eho),e(zp,eP),e(eP,oho),e(zp,rho),e($,tho),e($,Wp),e(Wp,qZ),e(qZ,aho),e(Wp,sho),e(Wp,oP),e(oP,nho),e(Wp,lho),e($,iho),e($,Qp),e(Qp,GZ),e(GZ,dho),e(Qp,cho),e(Qp,rP),e(rP,mho),e(Qp,fho),e($,gho),e($,Hp),e(Hp,OZ),e(OZ,hho),e(Hp,uho),e(Hp,tP),e(tP,pho),e(Hp,_ho),e($,bho),e($,Up),e(Up,XZ),e(XZ,vho),e(Up,Tho),e(Up,aP),e(aP,Fho),e(Up,Cho),e($,Mho),e($,Jp),e(Jp,VZ),e(VZ,Eho),e(Jp,yho),e(Jp,sP),e(sP,who),e(Jp,Aho),e($,Lho),e($,Yp),e(Yp,zZ),e(zZ,Bho),e(Yp,xho),e(Yp,nP),e(nP,kho),e(Yp,Rho),e($,Sho),e($,Kp),e(Kp,WZ),e(WZ,Pho),e(Kp,$ho),e(Kp,lP),e(lP,Iho),e(Kp,Dho),e($,jho),e($,Zp),e(Zp,QZ),e(QZ,Nho),e(Zp,qho),e(Zp,iP),e(iP,Gho),e(Zp,Oho),e($,Xho),e($,e_),e(e_,HZ),e(HZ,Vho),e(e_,zho),e(e_,dP),e(dP,Who),e(e_,Qho),e($,Hho),e($,o_),e(o_,UZ),e(UZ,Uho),e(o_,Jho),e(o_,cP),e(cP,Yho),e(o_,Kho),e($,Zho),e($,r_),e(r_,JZ),e(JZ,euo),e(r_,ouo),e(r_,mP),e(mP,ruo),e(r_,tuo),e($,auo),e($,t_),e(t_,YZ),e(YZ,suo),e(t_,nuo),e(t_,fP),e(fP,luo),e(t_,iuo),e($,duo),e($,a_),e(a_,KZ),e(KZ,cuo),e(a_,muo),e(a_,gP),e(gP,fuo),e(a_,guo),e($,huo),e($,s_),e(s_,ZZ),e(ZZ,uuo),e(s_,puo),e(s_,hP),e(hP,_uo),e(s_,buo),e($,vuo),e($,n_),e(n_,eee),e(eee,Tuo),e(n_,Fuo),e(n_,uP),e(uP,Cuo),e(n_,Muo),e($,Euo),e($,l_),e(l_,oee),e(oee,yuo),e(l_,wuo),e(l_,pP),e(pP,Auo),e(l_,Luo),e($,Buo),e($,i_),e(i_,ree),e(ree,xuo),e(i_,kuo),e(i_,_P),e(_P,Ruo),e(i_,Suo),e($,Puo),e($,d_),e(d_,tee),e(tee,$uo),e(d_,Iuo),e(d_,bP),e(bP,Duo),e(d_,juo),e($,Nuo),e($,c_),e(c_,aee),e(aee,quo),e(c_,Guo),e(c_,vP),e(vP,Ouo),e(c_,Xuo),e($,Vuo),e($,m_),e(m_,see),e(see,zuo),e(m_,Wuo),e(m_,TP),e(TP,Quo),e(m_,Huo),e($,Uuo),e($,f_),e(f_,nee),e(nee,Juo),e(f_,Yuo),e(f_,FP),e(FP,Kuo),e(f_,Zuo),e($,epo),e($,g_),e(g_,lee),e(lee,opo),e(g_,rpo),e(g_,CP),e(CP,tpo),e(g_,apo),e($,spo),e($,h_),e(h_,iee),e(iee,npo),e(h_,lpo),e(h_,MP),e(MP,ipo),e(h_,dpo),e($,cpo),e($,u_),e(u_,dee),e(dee,mpo),e(u_,fpo),e(u_,EP),e(EP,gpo),e(u_,hpo),e($,upo),e($,p_),e(p_,cee),e(cee,ppo),e(p_,_po),e(p_,yP),e(yP,bpo),e(p_,vpo),e($,Tpo),e($,__),e(__,mee),e(mee,Fpo),e(__,Cpo),e(__,wP),e(wP,Mpo),e(__,Epo),e($,ypo),e($,b_),e(b_,fee),e(fee,wpo),e(b_,Apo),e(b_,AP),e(AP,Lpo),e(b_,Bpo),e($,xpo),e($,v_),e(v_,gee),e(gee,kpo),e(v_,Rpo),e(v_,LP),e(LP,Spo),e(v_,Ppo),e($,$po),e($,T_),e(T_,hee),e(hee,Ipo),e(T_,Dpo),e(T_,BP),e(BP,jpo),e(T_,Npo),e($,qpo),e($,F_),e(F_,uee),e(uee,Gpo),e(F_,Opo),e(F_,xP),e(xP,Xpo),e(F_,Vpo),e($,zpo),e($,C_),e(C_,pee),e(pee,Wpo),e(C_,Qpo),e(C_,kP),e(kP,Hpo),e(C_,Upo),e($,Jpo),e($,M_),e(M_,_ee),e(_ee,Ypo),e(M_,Kpo),e(M_,RP),e(RP,Zpo),e(M_,e_o),e($,o_o),e($,E_),e(E_,bee),e(bee,r_o),e(E_,t_o),e(E_,SP),e(SP,a_o),e(E_,s_o),e(Pe,n_o),e(Pe,y_),e(y_,l_o),e(y_,vee),e(vee,i_o),e(y_,d_o),e(y_,Tee),e(Tee,c_o),e(Pe,m_o),e(Pe,Fee),e(Fee,f_o),e(Pe,g_o),g(Ty,Pe,null),b(c,I9e,_),b(c,Yi,_),e(Yi,w_),e(w_,Cee),g(Fy,Cee,null),e(Yi,h_o),e(Yi,Mee),e(Mee,u_o),b(c,D9e,_),b(c,Jo,_),g(Cy,Jo,null),e(Jo,p_o),e(Jo,Ki),e(Ki,__o),e(Ki,Eee),e(Eee,b_o),e(Ki,v_o),e(Ki,yee),e(yee,T_o),e(Ki,F_o),e(Jo,C_o),e(Jo,My),e(My,M_o),e(My,wee),e(wee,E_o),e(My,y_o),e(Jo,w_o),e(Jo,Xr),g(Ey,Xr,null),e(Xr,A_o),e(Xr,Aee),e(Aee,L_o),e(Xr,B_o),e(Xr,Zi),e(Zi,x_o),e(Zi,Lee),e(Lee,k_o),e(Zi,R_o),e(Zi,Bee),e(Bee,S_o),e(Zi,P_o),e(Xr,$_o),e(Xr,xee),e(xee,I_o),e(Xr,D_o),g(yy,Xr,null),e(Jo,j_o),e(Jo,$e),g(wy,$e,null),e($e,N_o),e($e,kee),e(kee,q_o),e($e,G_o),e($e,Oa),e(Oa,O_o),e(Oa,Ree),e(Ree,X_o),e(Oa,V_o),e(Oa,See),e(See,z_o),e(Oa,W_o),e(Oa,Pee),e(Pee,Q_o),e(Oa,H_o),e($e,U_o),e($e,I),e(I,A_),e(A_,$ee),e($ee,J_o),e(A_,Y_o),e(A_,PP),e(PP,K_o),e(A_,Z_o),e(I,ebo),e(I,L_),e(L_,Iee),e(Iee,obo),e(L_,rbo),e(L_,$P),e($P,tbo),e(L_,abo),e(I,sbo),e(I,B_),e(B_,Dee),e(Dee,nbo),e(B_,lbo),e(B_,IP),e(IP,ibo),e(B_,dbo),e(I,cbo),e(I,x_),e(x_,jee),e(jee,mbo),e(x_,fbo),e(x_,DP),e(DP,gbo),e(x_,hbo),e(I,ubo),e(I,k_),e(k_,Nee),e(Nee,pbo),e(k_,_bo),e(k_,jP),e(jP,bbo),e(k_,vbo),e(I,Tbo),e(I,R_),e(R_,qee),e(qee,Fbo),e(R_,Cbo),e(R_,NP),e(NP,Mbo),e(R_,Ebo),e(I,ybo),e(I,S_),e(S_,Gee),e(Gee,wbo),e(S_,Abo),e(S_,qP),e(qP,Lbo),e(S_,Bbo),e(I,xbo),e(I,P_),e(P_,Oee),e(Oee,kbo),e(P_,Rbo),e(P_,GP),e(GP,Sbo),e(P_,Pbo),e(I,$bo),e(I,$_),e($_,Xee),e(Xee,Ibo),e($_,Dbo),e($_,OP),e(OP,jbo),e($_,Nbo),e(I,qbo),e(I,I_),e(I_,Vee),e(Vee,Gbo),e(I_,Obo),e(I_,XP),e(XP,Xbo),e(I_,Vbo),e(I,zbo),e(I,D_),e(D_,zee),e(zee,Wbo),e(D_,Qbo),e(D_,VP),e(VP,Hbo),e(D_,Ubo),e(I,Jbo),e(I,j_),e(j_,Wee),e(Wee,Ybo),e(j_,Kbo),e(j_,zP),e(zP,Zbo),e(j_,e2o),e(I,o2o),e(I,N_),e(N_,Qee),e(Qee,r2o),e(N_,t2o),e(N_,WP),e(WP,a2o),e(N_,s2o),e(I,n2o),e(I,q_),e(q_,Hee),e(Hee,l2o),e(q_,i2o),e(q_,QP),e(QP,d2o),e(q_,c2o),e(I,m2o),e(I,G_),e(G_,Uee),e(Uee,f2o),e(G_,g2o),e(G_,HP),e(HP,h2o),e(G_,u2o),e(I,p2o),e(I,O_),e(O_,Jee),e(Jee,_2o),e(O_,b2o),e(O_,UP),e(UP,v2o),e(O_,T2o),e(I,F2o),e(I,X_),e(X_,Yee),e(Yee,C2o),e(X_,M2o),e(X_,JP),e(JP,E2o),e(X_,y2o),e(I,w2o),e(I,V_),e(V_,Kee),e(Kee,A2o),e(V_,L2o),e(V_,YP),e(YP,B2o),e(V_,x2o),e(I,k2o),e(I,z_),e(z_,Zee),e(Zee,R2o),e(z_,S2o),e(z_,KP),e(KP,P2o),e(z_,$2o),e(I,I2o),e(I,W_),e(W_,eoe),e(eoe,D2o),e(W_,j2o),e(W_,ZP),e(ZP,N2o),e(W_,q2o),e(I,G2o),e(I,Q_),e(Q_,ooe),e(ooe,O2o),e(Q_,X2o),e(Q_,e$),e(e$,V2o),e(Q_,z2o),e(I,W2o),e(I,H_),e(H_,roe),e(roe,Q2o),e(H_,H2o),e(H_,o$),e(o$,U2o),e(H_,J2o),e(I,Y2o),e(I,U_),e(U_,toe),e(toe,K2o),e(U_,Z2o),e(U_,r$),e(r$,evo),e(U_,ovo),e(I,rvo),e(I,J_),e(J_,aoe),e(aoe,tvo),e(J_,avo),e(J_,t$),e(t$,svo),e(J_,nvo),e(I,lvo),e(I,Y_),e(Y_,soe),e(soe,ivo),e(Y_,dvo),e(Y_,a$),e(a$,cvo),e(Y_,mvo),e(I,fvo),e(I,K_),e(K_,noe),e(noe,gvo),e(K_,hvo),e(K_,s$),e(s$,uvo),e(K_,pvo),e(I,_vo),e(I,Z_),e(Z_,loe),e(loe,bvo),e(Z_,vvo),e(Z_,n$),e(n$,Tvo),e(Z_,Fvo),e(I,Cvo),e(I,eb),e(eb,ioe),e(ioe,Mvo),e(eb,Evo),e(eb,l$),e(l$,yvo),e(eb,wvo),e(I,Avo),e(I,ob),e(ob,doe),e(doe,Lvo),e(ob,Bvo),e(ob,i$),e(i$,xvo),e(ob,kvo),e(I,Rvo),e(I,rb),e(rb,coe),e(coe,Svo),e(rb,Pvo),e(rb,d$),e(d$,$vo),e(rb,Ivo),e(I,Dvo),e(I,tb),e(tb,moe),e(moe,jvo),e(tb,Nvo),e(tb,foe),e(foe,qvo),e(tb,Gvo),e(I,Ovo),e(I,ab),e(ab,goe),e(goe,Xvo),e(ab,Vvo),e(ab,c$),e(c$,zvo),e(ab,Wvo),e(I,Qvo),e(I,sb),e(sb,hoe),e(hoe,Hvo),e(sb,Uvo),e(sb,m$),e(m$,Jvo),e(sb,Yvo),e(I,Kvo),e(I,nb),e(nb,uoe),e(uoe,Zvo),e(nb,eTo),e(nb,f$),e(f$,oTo),e(nb,rTo),e(I,tTo),e(I,lb),e(lb,poe),e(poe,aTo),e(lb,sTo),e(lb,g$),e(g$,nTo),e(lb,lTo),e($e,iTo),e($e,ib),e(ib,dTo),e(ib,_oe),e(_oe,cTo),e(ib,mTo),e(ib,boe),e(boe,fTo),e($e,gTo),e($e,voe),e(voe,hTo),e($e,uTo),g(Ay,$e,null),b(c,j9e,_),b(c,ed,_),e(ed,db),e(db,Toe),g(Ly,Toe,null),e(ed,pTo),e(ed,Foe),e(Foe,_To),b(c,N9e,_),b(c,Yo,_),g(By,Yo,null),e(Yo,bTo),e(Yo,od),e(od,vTo),e(od,Coe),e(Coe,TTo),e(od,FTo),e(od,Moe),e(Moe,CTo),e(od,MTo),e(Yo,ETo),e(Yo,xy),e(xy,yTo),e(xy,Eoe),e(Eoe,wTo),e(xy,ATo),e(Yo,LTo),e(Yo,Vr),g(ky,Vr,null),e(Vr,BTo),e(Vr,yoe),e(yoe,xTo),e(Vr,kTo),e(Vr,rd),e(rd,RTo),e(rd,woe),e(woe,STo),e(rd,PTo),e(rd,Aoe),e(Aoe,$To),e(rd,ITo),e(Vr,DTo),e(Vr,Loe),e(Loe,jTo),e(Vr,NTo),g(Ry,Vr,null),e(Yo,qTo),e(Yo,Ie),g(Sy,Ie,null),e(Ie,GTo),e(Ie,Boe),e(Boe,OTo),e(Ie,XTo),e(Ie,Xa),e(Xa,VTo),e(Xa,xoe),e(xoe,zTo),e(Xa,WTo),e(Xa,koe),e(koe,QTo),e(Xa,HTo),e(Xa,Roe),e(Roe,UTo),e(Xa,JTo),e(Ie,YTo),e(Ie,ae),e(ae,cb),e(cb,Soe),e(Soe,KTo),e(cb,ZTo),e(cb,h$),e(h$,e1o),e(cb,o1o),e(ae,r1o),e(ae,mb),e(mb,Poe),e(Poe,t1o),e(mb,a1o),e(mb,u$),e(u$,s1o),e(mb,n1o),e(ae,l1o),e(ae,fb),e(fb,$oe),e($oe,i1o),e(fb,d1o),e(fb,p$),e(p$,c1o),e(fb,m1o),e(ae,f1o),e(ae,gb),e(gb,Ioe),e(Ioe,g1o),e(gb,h1o),e(gb,_$),e(_$,u1o),e(gb,p1o),e(ae,_1o),e(ae,hb),e(hb,Doe),e(Doe,b1o),e(hb,v1o),e(hb,b$),e(b$,T1o),e(hb,F1o),e(ae,C1o),e(ae,ub),e(ub,joe),e(joe,M1o),e(ub,E1o),e(ub,v$),e(v$,y1o),e(ub,w1o),e(ae,A1o),e(ae,pb),e(pb,Noe),e(Noe,L1o),e(pb,B1o),e(pb,T$),e(T$,x1o),e(pb,k1o),e(ae,R1o),e(ae,_b),e(_b,qoe),e(qoe,S1o),e(_b,P1o),e(_b,F$),e(F$,$1o),e(_b,I1o),e(ae,D1o),e(ae,bb),e(bb,Goe),e(Goe,j1o),e(bb,N1o),e(bb,C$),e(C$,q1o),e(bb,G1o),e(ae,O1o),e(ae,vb),e(vb,Ooe),e(Ooe,X1o),e(vb,V1o),e(vb,M$),e(M$,z1o),e(vb,W1o),e(ae,Q1o),e(ae,Tb),e(Tb,Xoe),e(Xoe,H1o),e(Tb,U1o),e(Tb,E$),e(E$,J1o),e(Tb,Y1o),e(ae,K1o),e(ae,Fb),e(Fb,Voe),e(Voe,Z1o),e(Fb,eFo),e(Fb,y$),e(y$,oFo),e(Fb,rFo),e(ae,tFo),e(ae,Cb),e(Cb,zoe),e(zoe,aFo),e(Cb,sFo),e(Cb,w$),e(w$,nFo),e(Cb,lFo),e(ae,iFo),e(ae,Mb),e(Mb,Woe),e(Woe,dFo),e(Mb,cFo),e(Mb,A$),e(A$,mFo),e(Mb,fFo),e(ae,gFo),e(ae,Eb),e(Eb,Qoe),e(Qoe,hFo),e(Eb,uFo),e(Eb,L$),e(L$,pFo),e(Eb,_Fo),e(ae,bFo),e(ae,yb),e(yb,Hoe),e(Hoe,vFo),e(yb,TFo),e(yb,B$),e(B$,FFo),e(yb,CFo),e(Ie,MFo),e(Ie,wb),e(wb,EFo),e(wb,Uoe),e(Uoe,yFo),e(wb,wFo),e(wb,Joe),e(Joe,AFo),e(Ie,LFo),e(Ie,Yoe),e(Yoe,BFo),e(Ie,xFo),g(Py,Ie,null),b(c,q9e,_),b(c,td,_),e(td,Ab),e(Ab,Koe),g($y,Koe,null),e(td,kFo),e(td,Zoe),e(Zoe,RFo),b(c,G9e,_),b(c,Ko,_),g(Iy,Ko,null),e(Ko,SFo),e(Ko,ad),e(ad,PFo),e(ad,ere),e(ere,$Fo),e(ad,IFo),e(ad,ore),e(ore,DFo),e(ad,jFo),e(Ko,NFo),e(Ko,Dy),e(Dy,qFo),e(Dy,rre),e(rre,GFo),e(Dy,OFo),e(Ko,XFo),e(Ko,zr),g(jy,zr,null),e(zr,VFo),e(zr,tre),e(tre,zFo),e(zr,WFo),e(zr,sd),e(sd,QFo),e(sd,are),e(are,HFo),e(sd,UFo),e(sd,sre),e(sre,JFo),e(sd,YFo),e(zr,KFo),e(zr,nre),e(nre,ZFo),e(zr,eCo),g(Ny,zr,null),e(Ko,oCo),e(Ko,De),g(qy,De,null),e(De,rCo),e(De,lre),e(lre,tCo),e(De,aCo),e(De,Va),e(Va,sCo),e(Va,ire),e(ire,nCo),e(Va,lCo),e(Va,dre),e(dre,iCo),e(Va,dCo),e(Va,cre),e(cre,cCo),e(Va,mCo),e(De,fCo),e(De,A),e(A,Lb),e(Lb,mre),e(mre,gCo),e(Lb,hCo),e(Lb,x$),e(x$,uCo),e(Lb,pCo),e(A,_Co),e(A,Bb),e(Bb,fre),e(fre,bCo),e(Bb,vCo),e(Bb,k$),e(k$,TCo),e(Bb,FCo),e(A,CCo),e(A,xb),e(xb,gre),e(gre,MCo),e(xb,ECo),e(xb,R$),e(R$,yCo),e(xb,wCo),e(A,ACo),e(A,kb),e(kb,hre),e(hre,LCo),e(kb,BCo),e(kb,S$),e(S$,xCo),e(kb,kCo),e(A,RCo),e(A,Rb),e(Rb,ure),e(ure,SCo),e(Rb,PCo),e(Rb,P$),e(P$,$Co),e(Rb,ICo),e(A,DCo),e(A,Sb),e(Sb,pre),e(pre,jCo),e(Sb,NCo),e(Sb,$$),e($$,qCo),e(Sb,GCo),e(A,OCo),e(A,Pb),e(Pb,_re),e(_re,XCo),e(Pb,VCo),e(Pb,I$),e(I$,zCo),e(Pb,WCo),e(A,QCo),e(A,$b),e($b,bre),e(bre,HCo),e($b,UCo),e($b,D$),e(D$,JCo),e($b,YCo),e(A,KCo),e(A,Ib),e(Ib,vre),e(vre,ZCo),e(Ib,e4o),e(Ib,j$),e(j$,o4o),e(Ib,r4o),e(A,t4o),e(A,Db),e(Db,Tre),e(Tre,a4o),e(Db,s4o),e(Db,N$),e(N$,n4o),e(Db,l4o),e(A,i4o),e(A,jb),e(jb,Fre),e(Fre,d4o),e(jb,c4o),e(jb,q$),e(q$,m4o),e(jb,f4o),e(A,g4o),e(A,Nb),e(Nb,Cre),e(Cre,h4o),e(Nb,u4o),e(Nb,G$),e(G$,p4o),e(Nb,_4o),e(A,b4o),e(A,qb),e(qb,Mre),e(Mre,v4o),e(qb,T4o),e(qb,O$),e(O$,F4o),e(qb,C4o),e(A,M4o),e(A,Gb),e(Gb,Ere),e(Ere,E4o),e(Gb,y4o),e(Gb,X$),e(X$,w4o),e(Gb,A4o),e(A,L4o),e(A,Ob),e(Ob,yre),e(yre,B4o),e(Ob,x4o),e(Ob,V$),e(V$,k4o),e(Ob,R4o),e(A,S4o),e(A,Xb),e(Xb,wre),e(wre,P4o),e(Xb,$4o),e(Xb,z$),e(z$,I4o),e(Xb,D4o),e(A,j4o),e(A,Vb),e(Vb,Are),e(Are,N4o),e(Vb,q4o),e(Vb,W$),e(W$,G4o),e(Vb,O4o),e(A,X4o),e(A,zb),e(zb,Lre),e(Lre,V4o),e(zb,z4o),e(zb,Q$),e(Q$,W4o),e(zb,Q4o),e(A,H4o),e(A,Wb),e(Wb,Bre),e(Bre,U4o),e(Wb,J4o),e(Wb,H$),e(H$,Y4o),e(Wb,K4o),e(A,Z4o),e(A,Qb),e(Qb,xre),e(xre,eMo),e(Qb,oMo),e(Qb,U$),e(U$,rMo),e(Qb,tMo),e(A,aMo),e(A,Hb),e(Hb,kre),e(kre,sMo),e(Hb,nMo),e(Hb,J$),e(J$,lMo),e(Hb,iMo),e(A,dMo),e(A,Ub),e(Ub,Rre),e(Rre,cMo),e(Ub,mMo),e(Ub,Y$),e(Y$,fMo),e(Ub,gMo),e(A,hMo),e(A,Jb),e(Jb,Sre),e(Sre,uMo),e(Jb,pMo),e(Jb,K$),e(K$,_Mo),e(Jb,bMo),e(A,vMo),e(A,Yb),e(Yb,Pre),e(Pre,TMo),e(Yb,FMo),e(Yb,Z$),e(Z$,CMo),e(Yb,MMo),e(A,EMo),e(A,Kb),e(Kb,$re),e($re,yMo),e(Kb,wMo),e(Kb,eI),e(eI,AMo),e(Kb,LMo),e(A,BMo),e(A,Zb),e(Zb,Ire),e(Ire,xMo),e(Zb,kMo),e(Zb,oI),e(oI,RMo),e(Zb,SMo),e(A,PMo),e(A,e2),e(e2,Dre),e(Dre,$Mo),e(e2,IMo),e(e2,rI),e(rI,DMo),e(e2,jMo),e(A,NMo),e(A,o2),e(o2,jre),e(jre,qMo),e(o2,GMo),e(o2,tI),e(tI,OMo),e(o2,XMo),e(A,VMo),e(A,r2),e(r2,Nre),e(Nre,zMo),e(r2,WMo),e(r2,aI),e(aI,QMo),e(r2,HMo),e(A,UMo),e(A,t2),e(t2,qre),e(qre,JMo),e(t2,YMo),e(t2,sI),e(sI,KMo),e(t2,ZMo),e(A,eEo),e(A,a2),e(a2,Gre),e(Gre,oEo),e(a2,rEo),e(a2,nI),e(nI,tEo),e(a2,aEo),e(A,sEo),e(A,s2),e(s2,Ore),e(Ore,nEo),e(s2,lEo),e(s2,lI),e(lI,iEo),e(s2,dEo),e(A,cEo),e(A,n2),e(n2,Xre),e(Xre,mEo),e(n2,fEo),e(n2,iI),e(iI,gEo),e(n2,hEo),e(A,uEo),e(A,l2),e(l2,Vre),e(Vre,pEo),e(l2,_Eo),e(l2,dI),e(dI,bEo),e(l2,vEo),e(A,TEo),e(A,i2),e(i2,zre),e(zre,FEo),e(i2,CEo),e(i2,cI),e(cI,MEo),e(i2,EEo),e(A,yEo),e(A,d2),e(d2,Wre),e(Wre,wEo),e(d2,AEo),e(d2,mI),e(mI,LEo),e(d2,BEo),e(A,xEo),e(A,c2),e(c2,Qre),e(Qre,kEo),e(c2,REo),e(c2,fI),e(fI,SEo),e(c2,PEo),e(A,$Eo),e(A,m2),e(m2,Hre),e(Hre,IEo),e(m2,DEo),e(m2,gI),e(gI,jEo),e(m2,NEo),e(A,qEo),e(A,f2),e(f2,Ure),e(Ure,GEo),e(f2,OEo),e(f2,hI),e(hI,XEo),e(f2,VEo),e(A,zEo),e(A,g2),e(g2,Jre),e(Jre,WEo),e(g2,QEo),e(g2,uI),e(uI,HEo),e(g2,UEo),e(A,JEo),e(A,h2),e(h2,Yre),e(Yre,YEo),e(h2,KEo),e(h2,pI),e(pI,ZEo),e(h2,e3o),e(A,o3o),e(A,u2),e(u2,Kre),e(Kre,r3o),e(u2,t3o),e(u2,_I),e(_I,a3o),e(u2,s3o),e(A,n3o),e(A,p2),e(p2,Zre),e(Zre,l3o),e(p2,i3o),e(p2,bI),e(bI,d3o),e(p2,c3o),e(A,m3o),e(A,_2),e(_2,ete),e(ete,f3o),e(_2,g3o),e(_2,vI),e(vI,h3o),e(_2,u3o),e(A,p3o),e(A,b2),e(b2,ote),e(ote,_3o),e(b2,b3o),e(b2,TI),e(TI,v3o),e(b2,T3o),e(A,F3o),e(A,v2),e(v2,rte),e(rte,C3o),e(v2,M3o),e(v2,FI),e(FI,E3o),e(v2,y3o),e(De,w3o),e(De,T2),e(T2,A3o),e(T2,tte),e(tte,L3o),e(T2,B3o),e(T2,ate),e(ate,x3o),e(De,k3o),e(De,ste),e(ste,R3o),e(De,S3o),g(Gy,De,null),b(c,O9e,_),b(c,nd,_),e(nd,F2),e(F2,nte),g(Oy,nte,null),e(nd,P3o),e(nd,lte),e(lte,$3o),b(c,X9e,_),b(c,Zo,_),g(Xy,Zo,null),e(Zo,I3o),e(Zo,ld),e(ld,D3o),e(ld,ite),e(ite,j3o),e(ld,N3o),e(ld,dte),e(dte,q3o),e(ld,G3o),e(Zo,O3o),e(Zo,Vy),e(Vy,X3o),e(Vy,cte),e(cte,V3o),e(Vy,z3o),e(Zo,W3o),e(Zo,Wr),g(zy,Wr,null),e(Wr,Q3o),e(Wr,mte),e(mte,H3o),e(Wr,U3o),e(Wr,id),e(id,J3o),e(id,fte),e(fte,Y3o),e(id,K3o),e(id,gte),e(gte,Z3o),e(id,e5o),e(Wr,o5o),e(Wr,hte),e(hte,r5o),e(Wr,t5o),g(Wy,Wr,null),e(Zo,a5o),e(Zo,je),g(Qy,je,null),e(je,s5o),e(je,ute),e(ute,n5o),e(je,l5o),e(je,za),e(za,i5o),e(za,pte),e(pte,d5o),e(za,c5o),e(za,_te),e(_te,m5o),e(za,f5o),e(za,bte),e(bte,g5o),e(za,h5o),e(je,u5o),e(je,G),e(G,C2),e(C2,vte),e(vte,p5o),e(C2,_5o),e(C2,CI),e(CI,b5o),e(C2,v5o),e(G,T5o),e(G,M2),e(M2,Tte),e(Tte,F5o),e(M2,C5o),e(M2,MI),e(MI,M5o),e(M2,E5o),e(G,y5o),e(G,E2),e(E2,Fte),e(Fte,w5o),e(E2,A5o),e(E2,EI),e(EI,L5o),e(E2,B5o),e(G,x5o),e(G,y2),e(y2,Cte),e(Cte,k5o),e(y2,R5o),e(y2,yI),e(yI,S5o),e(y2,P5o),e(G,$5o),e(G,w2),e(w2,Mte),e(Mte,I5o),e(w2,D5o),e(w2,wI),e(wI,j5o),e(w2,N5o),e(G,q5o),e(G,A2),e(A2,Ete),e(Ete,G5o),e(A2,O5o),e(A2,AI),e(AI,X5o),e(A2,V5o),e(G,z5o),e(G,L2),e(L2,yte),e(yte,W5o),e(L2,Q5o),e(L2,LI),e(LI,H5o),e(L2,U5o),e(G,J5o),e(G,B2),e(B2,wte),e(wte,Y5o),e(B2,K5o),e(B2,BI),e(BI,Z5o),e(B2,eyo),e(G,oyo),e(G,x2),e(x2,Ate),e(Ate,ryo),e(x2,tyo),e(x2,xI),e(xI,ayo),e(x2,syo),e(G,nyo),e(G,k2),e(k2,Lte),e(Lte,lyo),e(k2,iyo),e(k2,kI),e(kI,dyo),e(k2,cyo),e(G,myo),e(G,R2),e(R2,Bte),e(Bte,fyo),e(R2,gyo),e(R2,RI),e(RI,hyo),e(R2,uyo),e(G,pyo),e(G,S2),e(S2,xte),e(xte,_yo),e(S2,byo),e(S2,SI),e(SI,vyo),e(S2,Tyo),e(G,Fyo),e(G,P2),e(P2,kte),e(kte,Cyo),e(P2,Myo),e(P2,PI),e(PI,Eyo),e(P2,yyo),e(G,wyo),e(G,$2),e($2,Rte),e(Rte,Ayo),e($2,Lyo),e($2,$I),e($I,Byo),e($2,xyo),e(G,kyo),e(G,I2),e(I2,Ste),e(Ste,Ryo),e(I2,Syo),e(I2,II),e(II,Pyo),e(I2,$yo),e(G,Iyo),e(G,D2),e(D2,Pte),e(Pte,Dyo),e(D2,jyo),e(D2,DI),e(DI,Nyo),e(D2,qyo),e(G,Gyo),e(G,j2),e(j2,$te),e($te,Oyo),e(j2,Xyo),e(j2,jI),e(jI,Vyo),e(j2,zyo),e(G,Wyo),e(G,N2),e(N2,Ite),e(Ite,Qyo),e(N2,Hyo),e(N2,NI),e(NI,Uyo),e(N2,Jyo),e(G,Yyo),e(G,q2),e(q2,Dte),e(Dte,Kyo),e(q2,Zyo),e(q2,qI),e(qI,ewo),e(q2,owo),e(G,rwo),e(G,G2),e(G2,jte),e(jte,two),e(G2,awo),e(G2,GI),e(GI,swo),e(G2,nwo),e(G,lwo),e(G,O2),e(O2,Nte),e(Nte,iwo),e(O2,dwo),e(O2,OI),e(OI,cwo),e(O2,mwo),e(G,fwo),e(G,X2),e(X2,qte),e(qte,gwo),e(X2,hwo),e(X2,XI),e(XI,uwo),e(X2,pwo),e(G,_wo),e(G,V2),e(V2,Gte),e(Gte,bwo),e(V2,vwo),e(V2,VI),e(VI,Two),e(V2,Fwo),e(G,Cwo),e(G,z2),e(z2,Ote),e(Ote,Mwo),e(z2,Ewo),e(z2,zI),e(zI,ywo),e(z2,wwo),e(G,Awo),e(G,W2),e(W2,Xte),e(Xte,Lwo),e(W2,Bwo),e(W2,WI),e(WI,xwo),e(W2,kwo),e(G,Rwo),e(G,Q2),e(Q2,Vte),e(Vte,Swo),e(Q2,Pwo),e(Q2,QI),e(QI,$wo),e(Q2,Iwo),e(G,Dwo),e(G,H2),e(H2,zte),e(zte,jwo),e(H2,Nwo),e(H2,HI),e(HI,qwo),e(H2,Gwo),e(G,Owo),e(G,U2),e(U2,Wte),e(Wte,Xwo),e(U2,Vwo),e(U2,UI),e(UI,zwo),e(U2,Wwo),e(je,Qwo),e(je,J2),e(J2,Hwo),e(J2,Qte),e(Qte,Uwo),e(J2,Jwo),e(J2,Hte),e(Hte,Ywo),e(je,Kwo),e(je,Ute),e(Ute,Zwo),e(je,e6o),g(Hy,je,null),b(c,V9e,_),b(c,dd,_),e(dd,Y2),e(Y2,Jte),g(Uy,Jte,null),e(dd,o6o),e(dd,Yte),e(Yte,r6o),b(c,z9e,_),b(c,er,_),g(Jy,er,null),e(er,t6o),e(er,cd),e(cd,a6o),e(cd,Kte),e(Kte,s6o),e(cd,n6o),e(cd,Zte),e(Zte,l6o),e(cd,i6o),e(er,d6o),e(er,Yy),e(Yy,c6o),e(Yy,eae),e(eae,m6o),e(Yy,f6o),e(er,g6o),e(er,Qr),g(Ky,Qr,null),e(Qr,h6o),e(Qr,oae),e(oae,u6o),e(Qr,p6o),e(Qr,md),e(md,_6o),e(md,rae),e(rae,b6o),e(md,v6o),e(md,tae),e(tae,T6o),e(md,F6o),e(Qr,C6o),e(Qr,aae),e(aae,M6o),e(Qr,E6o),g(Zy,Qr,null),e(er,y6o),e(er,Ne),g(ew,Ne,null),e(Ne,w6o),e(Ne,sae),e(sae,A6o),e(Ne,L6o),e(Ne,Wa),e(Wa,B6o),e(Wa,nae),e(nae,x6o),e(Wa,k6o),e(Wa,lae),e(lae,R6o),e(Wa,S6o),e(Wa,iae),e(iae,P6o),e(Wa,$6o),e(Ne,I6o),e(Ne,sa),e(sa,K2),e(K2,dae),e(dae,D6o),e(K2,j6o),e(K2,JI),e(JI,N6o),e(K2,q6o),e(sa,G6o),e(sa,Z2),e(Z2,cae),e(cae,O6o),e(Z2,X6o),e(Z2,YI),e(YI,V6o),e(Z2,z6o),e(sa,W6o),e(sa,ev),e(ev,mae),e(mae,Q6o),e(ev,H6o),e(ev,KI),e(KI,U6o),e(ev,J6o),e(sa,Y6o),e(sa,ov),e(ov,fae),e(fae,K6o),e(ov,Z6o),e(ov,ZI),e(ZI,eAo),e(ov,oAo),e(sa,rAo),e(sa,rv),e(rv,gae),e(gae,tAo),e(rv,aAo),e(rv,eD),e(eD,sAo),e(rv,nAo),e(Ne,lAo),e(Ne,tv),e(tv,iAo),e(tv,hae),e(hae,dAo),e(tv,cAo),e(tv,uae),e(uae,mAo),e(Ne,fAo),e(Ne,pae),e(pae,gAo),e(Ne,hAo),g(ow,Ne,null),b(c,W9e,_),b(c,fd,_),e(fd,av),e(av,_ae),g(rw,_ae,null),e(fd,uAo),e(fd,bae),e(bae,pAo),b(c,Q9e,_),b(c,or,_),g(tw,or,null),e(or,_Ao),e(or,gd),e(gd,bAo),e(gd,vae),e(vae,vAo),e(gd,TAo),e(gd,Tae),e(Tae,FAo),e(gd,CAo),e(or,MAo),e(or,aw),e(aw,EAo),e(aw,Fae),e(Fae,yAo),e(aw,wAo),e(or,AAo),e(or,Hr),g(sw,Hr,null),e(Hr,LAo),e(Hr,Cae),e(Cae,BAo),e(Hr,xAo),e(Hr,hd),e(hd,kAo),e(hd,Mae),e(Mae,RAo),e(hd,SAo),e(hd,Eae),e(Eae,PAo),e(hd,$Ao),e(Hr,IAo),e(Hr,yae),e(yae,DAo),e(Hr,jAo),g(nw,Hr,null),e(or,NAo),e(or,qe),g(lw,qe,null),e(qe,qAo),e(qe,wae),e(wae,GAo),e(qe,OAo),e(qe,Qa),e(Qa,XAo),e(Qa,Aae),e(Aae,VAo),e(Qa,zAo),e(Qa,Lae),e(Lae,WAo),e(Qa,QAo),e(Qa,Bae),e(Bae,HAo),e(Qa,UAo),e(qe,JAo),e(qe,N),e(N,sv),e(sv,xae),e(xae,YAo),e(sv,KAo),e(sv,oD),e(oD,ZAo),e(sv,e0o),e(N,o0o),e(N,nv),e(nv,kae),e(kae,r0o),e(nv,t0o),e(nv,rD),e(rD,a0o),e(nv,s0o),e(N,n0o),e(N,lv),e(lv,Rae),e(Rae,l0o),e(lv,i0o),e(lv,tD),e(tD,d0o),e(lv,c0o),e(N,m0o),e(N,iv),e(iv,Sae),e(Sae,f0o),e(iv,g0o),e(iv,aD),e(aD,h0o),e(iv,u0o),e(N,p0o),e(N,dv),e(dv,Pae),e(Pae,_0o),e(dv,b0o),e(dv,sD),e(sD,v0o),e(dv,T0o),e(N,F0o),e(N,cv),e(cv,$ae),e($ae,C0o),e(cv,M0o),e(cv,nD),e(nD,E0o),e(cv,y0o),e(N,w0o),e(N,mv),e(mv,Iae),e(Iae,A0o),e(mv,L0o),e(mv,lD),e(lD,B0o),e(mv,x0o),e(N,k0o),e(N,fv),e(fv,Dae),e(Dae,R0o),e(fv,S0o),e(fv,iD),e(iD,P0o),e(fv,$0o),e(N,I0o),e(N,gv),e(gv,jae),e(jae,D0o),e(gv,j0o),e(gv,dD),e(dD,N0o),e(gv,q0o),e(N,G0o),e(N,hv),e(hv,Nae),e(Nae,O0o),e(hv,X0o),e(hv,cD),e(cD,V0o),e(hv,z0o),e(N,W0o),e(N,uv),e(uv,qae),e(qae,Q0o),e(uv,H0o),e(uv,mD),e(mD,U0o),e(uv,J0o),e(N,Y0o),e(N,pv),e(pv,Gae),e(Gae,K0o),e(pv,Z0o),e(pv,fD),e(fD,eLo),e(pv,oLo),e(N,rLo),e(N,_v),e(_v,Oae),e(Oae,tLo),e(_v,aLo),e(_v,gD),e(gD,sLo),e(_v,nLo),e(N,lLo),e(N,bv),e(bv,Xae),e(Xae,iLo),e(bv,dLo),e(bv,hD),e(hD,cLo),e(bv,mLo),e(N,fLo),e(N,vv),e(vv,Vae),e(Vae,gLo),e(vv,hLo),e(vv,uD),e(uD,uLo),e(vv,pLo),e(N,_Lo),e(N,Tv),e(Tv,zae),e(zae,bLo),e(Tv,vLo),e(Tv,pD),e(pD,TLo),e(Tv,FLo),e(N,CLo),e(N,Fv),e(Fv,Wae),e(Wae,MLo),e(Fv,ELo),e(Fv,_D),e(_D,yLo),e(Fv,wLo),e(N,ALo),e(N,Cv),e(Cv,Qae),e(Qae,LLo),e(Cv,BLo),e(Cv,bD),e(bD,xLo),e(Cv,kLo),e(N,RLo),e(N,Mv),e(Mv,Hae),e(Hae,SLo),e(Mv,PLo),e(Mv,vD),e(vD,$Lo),e(Mv,ILo),e(N,DLo),e(N,Ev),e(Ev,Uae),e(Uae,jLo),e(Ev,NLo),e(Ev,TD),e(TD,qLo),e(Ev,GLo),e(N,OLo),e(N,yv),e(yv,Jae),e(Jae,XLo),e(yv,VLo),e(yv,FD),e(FD,zLo),e(yv,WLo),e(N,QLo),e(N,wv),e(wv,Yae),e(Yae,HLo),e(wv,ULo),e(wv,CD),e(CD,JLo),e(wv,YLo),e(N,KLo),e(N,Av),e(Av,Kae),e(Kae,ZLo),e(Av,e8o),e(Av,MD),e(MD,o8o),e(Av,r8o),e(N,t8o),e(N,Lv),e(Lv,Zae),e(Zae,a8o),e(Lv,s8o),e(Lv,ED),e(ED,n8o),e(Lv,l8o),e(N,i8o),e(N,Bv),e(Bv,ese),e(ese,d8o),e(Bv,c8o),e(Bv,yD),e(yD,m8o),e(Bv,f8o),e(N,g8o),e(N,xv),e(xv,ose),e(ose,h8o),e(xv,u8o),e(xv,wD),e(wD,p8o),e(xv,_8o),e(N,b8o),e(N,kv),e(kv,rse),e(rse,v8o),e(kv,T8o),e(kv,AD),e(AD,F8o),e(kv,C8o),e(N,M8o),e(N,Rv),e(Rv,tse),e(tse,E8o),e(Rv,y8o),e(Rv,LD),e(LD,w8o),e(Rv,A8o),e(N,L8o),e(N,Sv),e(Sv,ase),e(ase,B8o),e(Sv,x8o),e(Sv,BD),e(BD,k8o),e(Sv,R8o),e(N,S8o),e(N,Pv),e(Pv,sse),e(sse,P8o),e(Pv,$8o),e(Pv,xD),e(xD,I8o),e(Pv,D8o),e(N,j8o),e(N,$v),e($v,nse),e(nse,N8o),e($v,q8o),e($v,kD),e(kD,G8o),e($v,O8o),e(N,X8o),e(N,Iv),e(Iv,lse),e(lse,V8o),e(Iv,z8o),e(Iv,RD),e(RD,W8o),e(Iv,Q8o),e(N,H8o),e(N,Dv),e(Dv,ise),e(ise,U8o),e(Dv,J8o),e(Dv,SD),e(SD,Y8o),e(Dv,K8o),e(qe,Z8o),e(qe,jv),e(jv,e7o),e(jv,dse),e(dse,o7o),e(jv,r7o),e(jv,cse),e(cse,t7o),e(qe,a7o),e(qe,mse),e(mse,s7o),e(qe,n7o),g(iw,qe,null),b(c,H9e,_),b(c,ud,_),e(ud,Nv),e(Nv,fse),g(dw,fse,null),e(ud,l7o),e(ud,gse),e(gse,i7o),b(c,U9e,_),b(c,rr,_),g(cw,rr,null),e(rr,d7o),e(rr,pd),e(pd,c7o),e(pd,hse),e(hse,m7o),e(pd,f7o),e(pd,use),e(use,g7o),e(pd,h7o),e(rr,u7o),e(rr,mw),e(mw,p7o),e(mw,pse),e(pse,_7o),e(mw,b7o),e(rr,v7o),e(rr,Ur),g(fw,Ur,null),e(Ur,T7o),e(Ur,_se),e(_se,F7o),e(Ur,C7o),e(Ur,_d),e(_d,M7o),e(_d,bse),e(bse,E7o),e(_d,y7o),e(_d,vse),e(vse,w7o),e(_d,A7o),e(Ur,L7o),e(Ur,Tse),e(Tse,B7o),e(Ur,x7o),g(gw,Ur,null),e(rr,k7o),e(rr,Ge),g(hw,Ge,null),e(Ge,R7o),e(Ge,Fse),e(Fse,S7o),e(Ge,P7o),e(Ge,Ha),e(Ha,$7o),e(Ha,Cse),e(Cse,I7o),e(Ha,D7o),e(Ha,Mse),e(Mse,j7o),e(Ha,N7o),e(Ha,Ese),e(Ese,q7o),e(Ha,G7o),e(Ge,O7o),e(Ge,R),e(R,qv),e(qv,yse),e(yse,X7o),e(qv,V7o),e(qv,PD),e(PD,z7o),e(qv,W7o),e(R,Q7o),e(R,Gv),e(Gv,wse),e(wse,H7o),e(Gv,U7o),e(Gv,$D),e($D,J7o),e(Gv,Y7o),e(R,K7o),e(R,Ov),e(Ov,Ase),e(Ase,Z7o),e(Ov,e9o),e(Ov,ID),e(ID,o9o),e(Ov,r9o),e(R,t9o),e(R,Xv),e(Xv,Lse),e(Lse,a9o),e(Xv,s9o),e(Xv,DD),e(DD,n9o),e(Xv,l9o),e(R,i9o),e(R,Vv),e(Vv,Bse),e(Bse,d9o),e(Vv,c9o),e(Vv,jD),e(jD,m9o),e(Vv,f9o),e(R,g9o),e(R,zv),e(zv,xse),e(xse,h9o),e(zv,u9o),e(zv,ND),e(ND,p9o),e(zv,_9o),e(R,b9o),e(R,Wv),e(Wv,kse),e(kse,v9o),e(Wv,T9o),e(Wv,qD),e(qD,F9o),e(Wv,C9o),e(R,M9o),e(R,Qv),e(Qv,Rse),e(Rse,E9o),e(Qv,y9o),e(Qv,GD),e(GD,w9o),e(Qv,A9o),e(R,L9o),e(R,Hv),e(Hv,Sse),e(Sse,B9o),e(Hv,x9o),e(Hv,OD),e(OD,k9o),e(Hv,R9o),e(R,S9o),e(R,Uv),e(Uv,Pse),e(Pse,P9o),e(Uv,$9o),e(Uv,XD),e(XD,I9o),e(Uv,D9o),e(R,j9o),e(R,Jv),e(Jv,$se),e($se,N9o),e(Jv,q9o),e(Jv,VD),e(VD,G9o),e(Jv,O9o),e(R,X9o),e(R,Yv),e(Yv,Ise),e(Ise,V9o),e(Yv,z9o),e(Yv,zD),e(zD,W9o),e(Yv,Q9o),e(R,H9o),e(R,Kv),e(Kv,Dse),e(Dse,U9o),e(Kv,J9o),e(Kv,WD),e(WD,Y9o),e(Kv,K9o),e(R,Z9o),e(R,Zv),e(Zv,jse),e(jse,eBo),e(Zv,oBo),e(Zv,QD),e(QD,rBo),e(Zv,tBo),e(R,aBo),e(R,eT),e(eT,Nse),e(Nse,sBo),e(eT,nBo),e(eT,HD),e(HD,lBo),e(eT,iBo),e(R,dBo),e(R,oT),e(oT,qse),e(qse,cBo),e(oT,mBo),e(oT,UD),e(UD,fBo),e(oT,gBo),e(R,hBo),e(R,rT),e(rT,Gse),e(Gse,uBo),e(rT,pBo),e(rT,JD),e(JD,_Bo),e(rT,bBo),e(R,vBo),e(R,tT),e(tT,Ose),e(Ose,TBo),e(tT,FBo),e(tT,YD),e(YD,CBo),e(tT,MBo),e(R,EBo),e(R,aT),e(aT,Xse),e(Xse,yBo),e(aT,wBo),e(aT,KD),e(KD,ABo),e(aT,LBo),e(R,BBo),e(R,sT),e(sT,Vse),e(Vse,xBo),e(sT,kBo),e(sT,ZD),e(ZD,RBo),e(sT,SBo),e(R,PBo),e(R,nT),e(nT,zse),e(zse,$Bo),e(nT,IBo),e(nT,ej),e(ej,DBo),e(nT,jBo),e(R,NBo),e(R,lT),e(lT,Wse),e(Wse,qBo),e(lT,GBo),e(lT,oj),e(oj,OBo),e(lT,XBo),e(R,VBo),e(R,iT),e(iT,Qse),e(Qse,zBo),e(iT,WBo),e(iT,rj),e(rj,QBo),e(iT,HBo),e(R,UBo),e(R,dT),e(dT,Hse),e(Hse,JBo),e(dT,YBo),e(dT,tj),e(tj,KBo),e(dT,ZBo),e(R,exo),e(R,cT),e(cT,Use),e(Use,oxo),e(cT,rxo),e(cT,aj),e(aj,txo),e(cT,axo),e(R,sxo),e(R,mT),e(mT,Jse),e(Jse,nxo),e(mT,lxo),e(mT,sj),e(sj,ixo),e(mT,dxo),e(R,cxo),e(R,fT),e(fT,Yse),e(Yse,mxo),e(fT,fxo),e(fT,nj),e(nj,gxo),e(fT,hxo),e(R,uxo),e(R,gT),e(gT,Kse),e(Kse,pxo),e(gT,_xo),e(gT,lj),e(lj,bxo),e(gT,vxo),e(R,Txo),e(R,hT),e(hT,Zse),e(Zse,Fxo),e(hT,Cxo),e(hT,ij),e(ij,Mxo),e(hT,Exo),e(R,yxo),e(R,uT),e(uT,ene),e(ene,wxo),e(uT,Axo),e(uT,dj),e(dj,Lxo),e(uT,Bxo),e(R,xxo),e(R,pT),e(pT,one),e(one,kxo),e(pT,Rxo),e(pT,cj),e(cj,Sxo),e(pT,Pxo),e(R,$xo),e(R,_T),e(_T,rne),e(rne,Ixo),e(_T,Dxo),e(_T,mj),e(mj,jxo),e(_T,Nxo),e(R,qxo),e(R,bT),e(bT,tne),e(tne,Gxo),e(bT,Oxo),e(bT,fj),e(fj,Xxo),e(bT,Vxo),e(R,zxo),e(R,vT),e(vT,ane),e(ane,Wxo),e(vT,Qxo),e(vT,gj),e(gj,Hxo),e(vT,Uxo),e(R,Jxo),e(R,TT),e(TT,sne),e(sne,Yxo),e(TT,Kxo),e(TT,hj),e(hj,Zxo),e(TT,eko),e(R,oko),e(R,FT),e(FT,nne),e(nne,rko),e(FT,tko),e(FT,uj),e(uj,ako),e(FT,sko),e(R,nko),e(R,CT),e(CT,lne),e(lne,lko),e(CT,iko),e(CT,pj),e(pj,dko),e(CT,cko),e(R,mko),e(R,MT),e(MT,ine),e(ine,fko),e(MT,gko),e(MT,_j),e(_j,hko),e(MT,uko),e(R,pko),e(R,ET),e(ET,dne),e(dne,_ko),e(ET,bko),e(ET,bj),e(bj,vko),e(ET,Tko),e(Ge,Fko),e(Ge,yT),e(yT,Cko),e(yT,cne),e(cne,Mko),e(yT,Eko),e(yT,mne),e(mne,yko),e(Ge,wko),e(Ge,fne),e(fne,Ako),e(Ge,Lko),g(uw,Ge,null),b(c,J9e,_),b(c,bd,_),e(bd,wT),e(wT,gne),g(pw,gne,null),e(bd,Bko),e(bd,hne),e(hne,xko),b(c,Y9e,_),b(c,tr,_),g(_w,tr,null),e(tr,kko),e(tr,vd),e(vd,Rko),e(vd,une),e(une,Sko),e(vd,Pko),e(vd,pne),e(pne,$ko),e(vd,Iko),e(tr,Dko),e(tr,bw),e(bw,jko),e(bw,_ne),e(_ne,Nko),e(bw,qko),e(tr,Gko),e(tr,Jr),g(vw,Jr,null),e(Jr,Oko),e(Jr,bne),e(bne,Xko),e(Jr,Vko),e(Jr,Td),e(Td,zko),e(Td,vne),e(vne,Wko),e(Td,Qko),e(Td,Tne),e(Tne,Hko),e(Td,Uko),e(Jr,Jko),e(Jr,Fne),e(Fne,Yko),e(Jr,Kko),g(Tw,Jr,null),e(tr,Zko),e(tr,Oe),g(Fw,Oe,null),e(Oe,eRo),e(Oe,Cne),e(Cne,oRo),e(Oe,rRo),e(Oe,Ua),e(Ua,tRo),e(Ua,Mne),e(Mne,aRo),e(Ua,sRo),e(Ua,Ene),e(Ene,nRo),e(Ua,lRo),e(Ua,yne),e(yne,iRo),e(Ua,dRo),e(Oe,cRo),e(Oe,wne),e(wne,AT),e(AT,Ane),e(Ane,mRo),e(AT,fRo),e(AT,vj),e(vj,gRo),e(AT,hRo),e(Oe,uRo),e(Oe,LT),e(LT,pRo),e(LT,Lne),e(Lne,_Ro),e(LT,bRo),e(LT,Bne),e(Bne,vRo),e(Oe,TRo),e(Oe,xne),e(xne,FRo),e(Oe,CRo),g(Cw,Oe,null),b(c,K9e,_),b(c,Fd,_),e(Fd,BT),e(BT,kne),g(Mw,kne,null),e(Fd,MRo),e(Fd,Rne),e(Rne,ERo),b(c,Z9e,_),b(c,ar,_),g(Ew,ar,null),e(ar,yRo),e(ar,Cd),e(Cd,wRo),e(Cd,Sne),e(Sne,ARo),e(Cd,LRo),e(Cd,Pne),e(Pne,BRo),e(Cd,xRo),e(ar,kRo),e(ar,yw),e(yw,RRo),e(yw,$ne),e($ne,SRo),e(yw,PRo),e(ar,$Ro),e(ar,Yr),g(ww,Yr,null),e(Yr,IRo),e(Yr,Ine),e(Ine,DRo),e(Yr,jRo),e(Yr,Md),e(Md,NRo),e(Md,Dne),e(Dne,qRo),e(Md,GRo),e(Md,jne),e(jne,ORo),e(Md,XRo),e(Yr,VRo),e(Yr,Nne),e(Nne,zRo),e(Yr,WRo),g(Aw,Yr,null),e(ar,QRo),e(ar,Xe),g(Lw,Xe,null),e(Xe,HRo),e(Xe,qne),e(qne,URo),e(Xe,JRo),e(Xe,Ja),e(Ja,YRo),e(Ja,Gne),e(Gne,KRo),e(Ja,ZRo),e(Ja,One),e(One,eSo),e(Ja,oSo),e(Ja,Xne),e(Xne,rSo),e(Ja,tSo),e(Xe,aSo),e(Xe,be),e(be,xT),e(xT,Vne),e(Vne,sSo),e(xT,nSo),e(xT,Tj),e(Tj,lSo),e(xT,iSo),e(be,dSo),e(be,kT),e(kT,zne),e(zne,cSo),e(kT,mSo),e(kT,Fj),e(Fj,fSo),e(kT,gSo),e(be,hSo),e(be,Pn),e(Pn,Wne),e(Wne,uSo),e(Pn,pSo),e(Pn,Cj),e(Cj,_So),e(Pn,bSo),e(Pn,Mj),e(Mj,vSo),e(Pn,TSo),e(be,FSo),e(be,RT),e(RT,Qne),e(Qne,CSo),e(RT,MSo),e(RT,Ej),e(Ej,ESo),e(RT,ySo),e(be,wSo),e(be,la),e(la,Hne),e(Hne,ASo),e(la,LSo),e(la,yj),e(yj,BSo),e(la,xSo),e(la,wj),e(wj,kSo),e(la,RSo),e(la,Aj),e(Aj,SSo),e(la,PSo),e(be,$So),e(be,ST),e(ST,Une),e(Une,ISo),e(ST,DSo),e(ST,Lj),e(Lj,jSo),e(ST,NSo),e(be,qSo),e(be,PT),e(PT,Jne),e(Jne,GSo),e(PT,OSo),e(PT,Bj),e(Bj,XSo),e(PT,VSo),e(be,zSo),e(be,$T),e($T,Yne),e(Yne,WSo),e($T,QSo),e($T,xj),e(xj,HSo),e($T,USo),e(be,JSo),e(be,IT),e(IT,Kne),e(Kne,YSo),e(IT,KSo),e(IT,kj),e(kj,ZSo),e(IT,ePo),e(Xe,oPo),e(Xe,DT),e(DT,rPo),e(DT,Zne),e(Zne,tPo),e(DT,aPo),e(DT,ele),e(ele,sPo),e(Xe,nPo),e(Xe,ole),e(ole,lPo),e(Xe,iPo),g(Bw,Xe,null),b(c,eBe,_),b(c,Ed,_),e(Ed,jT),e(jT,rle),g(xw,rle,null),e(Ed,dPo),e(Ed,tle),e(tle,cPo),b(c,oBe,_),b(c,sr,_),g(kw,sr,null),e(sr,mPo),e(sr,yd),e(yd,fPo),e(yd,ale),e(ale,gPo),e(yd,hPo),e(yd,sle),e(sle,uPo),e(yd,pPo),e(sr,_Po),e(sr,Rw),e(Rw,bPo),e(Rw,nle),e(nle,vPo),e(Rw,TPo),e(sr,FPo),e(sr,Kr),g(Sw,Kr,null),e(Kr,CPo),e(Kr,lle),e(lle,MPo),e(Kr,EPo),e(Kr,wd),e(wd,yPo),e(wd,ile),e(ile,wPo),e(wd,APo),e(wd,dle),e(dle,LPo),e(wd,BPo),e(Kr,xPo),e(Kr,cle),e(cle,kPo),e(Kr,RPo),g(Pw,Kr,null),e(sr,SPo),e(sr,Ve),g($w,Ve,null),e(Ve,PPo),e(Ve,mle),e(mle,$Po),e(Ve,IPo),e(Ve,Ya),e(Ya,DPo),e(Ya,fle),e(fle,jPo),e(Ya,NPo),e(Ya,gle),e(gle,qPo),e(Ya,GPo),e(Ya,hle),e(hle,OPo),e(Ya,XPo),e(Ve,VPo),e(Ve,ule),e(ule,NT),e(NT,ple),e(ple,zPo),e(NT,WPo),e(NT,Rj),e(Rj,QPo),e(NT,HPo),e(Ve,UPo),e(Ve,qT),e(qT,JPo),e(qT,_le),e(_le,YPo),e(qT,KPo),e(qT,ble),e(ble,ZPo),e(Ve,e$o),e(Ve,vle),e(vle,o$o),e(Ve,r$o),g(Iw,Ve,null),b(c,rBe,_),b(c,Ad,_),e(Ad,GT),e(GT,Tle),g(Dw,Tle,null),e(Ad,t$o),e(Ad,Fle),e(Fle,a$o),b(c,tBe,_),b(c,nr,_),g(jw,nr,null),e(nr,s$o),e(nr,Ld),e(Ld,n$o),e(Ld,Cle),e(Cle,l$o),e(Ld,i$o),e(Ld,Mle),e(Mle,d$o),e(Ld,c$o),e(nr,m$o),e(nr,Nw),e(Nw,f$o),e(Nw,Ele),e(Ele,g$o),e(Nw,h$o),e(nr,u$o),e(nr,Zr),g(qw,Zr,null),e(Zr,p$o),e(Zr,yle),e(yle,_$o),e(Zr,b$o),e(Zr,Bd),e(Bd,v$o),e(Bd,wle),e(wle,T$o),e(Bd,F$o),e(Bd,Ale),e(Ale,C$o),e(Bd,M$o),e(Zr,E$o),e(Zr,Lle),e(Lle,y$o),e(Zr,w$o),g(Gw,Zr,null),e(nr,A$o),e(nr,ze),g(Ow,ze,null),e(ze,L$o),e(ze,Ble),e(Ble,B$o),e(ze,x$o),e(ze,Ka),e(Ka,k$o),e(Ka,xle),e(xle,R$o),e(Ka,S$o),e(Ka,kle),e(kle,P$o),e(Ka,$$o),e(Ka,Rle),e(Rle,I$o),e(Ka,D$o),e(ze,j$o),e(ze,Ae),e(Ae,OT),e(OT,Sle),e(Sle,N$o),e(OT,q$o),e(OT,Sj),e(Sj,G$o),e(OT,O$o),e(Ae,X$o),e(Ae,XT),e(XT,Ple),e(Ple,V$o),e(XT,z$o),e(XT,Pj),e(Pj,W$o),e(XT,Q$o),e(Ae,H$o),e(Ae,VT),e(VT,$le),e($le,U$o),e(VT,J$o),e(VT,$j),e($j,Y$o),e(VT,K$o),e(Ae,Z$o),e(Ae,zT),e(zT,Ile),e(Ile,eIo),e(zT,oIo),e(zT,Ij),e(Ij,rIo),e(zT,tIo),e(Ae,aIo),e(Ae,WT),e(WT,Dle),e(Dle,sIo),e(WT,nIo),e(WT,Dj),e(Dj,lIo),e(WT,iIo),e(Ae,dIo),e(Ae,QT),e(QT,jle),e(jle,cIo),e(QT,mIo),e(QT,jj),e(jj,fIo),e(QT,gIo),e(Ae,hIo),e(Ae,HT),e(HT,Nle),e(Nle,uIo),e(HT,pIo),e(HT,Nj),e(Nj,_Io),e(HT,bIo),e(Ae,vIo),e(Ae,UT),e(UT,qle),e(qle,TIo),e(UT,FIo),e(UT,qj),e(qj,CIo),e(UT,MIo),e(ze,EIo),e(ze,JT),e(JT,yIo),e(JT,Gle),e(Gle,wIo),e(JT,AIo),e(JT,Ole),e(Ole,LIo),e(ze,BIo),e(ze,Xle),e(Xle,xIo),e(ze,kIo),g(Xw,ze,null),b(c,aBe,_),b(c,xd,_),e(xd,YT),e(YT,Vle),g(Vw,Vle,null),e(xd,RIo),e(xd,zle),e(zle,SIo),b(c,sBe,_),b(c,lr,_),g(zw,lr,null),e(lr,PIo),e(lr,kd),e(kd,$Io),e(kd,Wle),e(Wle,IIo),e(kd,DIo),e(kd,Qle),e(Qle,jIo),e(kd,NIo),e(lr,qIo),e(lr,Ww),e(Ww,GIo),e(Ww,Hle),e(Hle,OIo),e(Ww,XIo),e(lr,VIo),e(lr,et),g(Qw,et,null),e(et,zIo),e(et,Ule),e(Ule,WIo),e(et,QIo),e(et,Rd),e(Rd,HIo),e(Rd,Jle),e(Jle,UIo),e(Rd,JIo),e(Rd,Yle),e(Yle,YIo),e(Rd,KIo),e(et,ZIo),e(et,Kle),e(Kle,eDo),e(et,oDo),g(Hw,et,null),e(lr,rDo),e(lr,We),g(Uw,We,null),e(We,tDo),e(We,Zle),e(Zle,aDo),e(We,sDo),e(We,Za),e(Za,nDo),e(Za,eie),e(eie,lDo),e(Za,iDo),e(Za,oie),e(oie,dDo),e(Za,cDo),e(Za,rie),e(rie,mDo),e(Za,fDo),e(We,gDo),e(We,es),e(es,KT),e(KT,tie),e(tie,hDo),e(KT,uDo),e(KT,Gj),e(Gj,pDo),e(KT,_Do),e(es,bDo),e(es,ZT),e(ZT,aie),e(aie,vDo),e(ZT,TDo),e(ZT,Oj),e(Oj,FDo),e(ZT,CDo),e(es,MDo),e(es,e1),e(e1,sie),e(sie,EDo),e(e1,yDo),e(e1,Xj),e(Xj,wDo),e(e1,ADo),e(es,LDo),e(es,o1),e(o1,nie),e(nie,BDo),e(o1,xDo),e(o1,Vj),e(Vj,kDo),e(o1,RDo),e(We,SDo),e(We,r1),e(r1,PDo),e(r1,lie),e(lie,$Do),e(r1,IDo),e(r1,iie),e(iie,DDo),e(We,jDo),e(We,die),e(die,NDo),e(We,qDo),g(Jw,We,null),b(c,nBe,_),b(c,Sd,_),e(Sd,t1),e(t1,cie),g(Yw,cie,null),e(Sd,GDo),e(Sd,mie),e(mie,ODo),b(c,lBe,_),b(c,ir,_),g(Kw,ir,null),e(ir,XDo),e(ir,Pd),e(Pd,VDo),e(Pd,fie),e(fie,zDo),e(Pd,WDo),e(Pd,gie),e(gie,QDo),e(Pd,HDo),e(ir,UDo),e(ir,Zw),e(Zw,JDo),e(Zw,hie),e(hie,YDo),e(Zw,KDo),e(ir,ZDo),e(ir,ot),g(e6,ot,null),e(ot,ejo),e(ot,uie),e(uie,ojo),e(ot,rjo),e(ot,$d),e($d,tjo),e($d,pie),e(pie,ajo),e($d,sjo),e($d,_ie),e(_ie,njo),e($d,ljo),e(ot,ijo),e(ot,bie),e(bie,djo),e(ot,cjo),g(o6,ot,null),e(ir,mjo),e(ir,Qe),g(r6,Qe,null),e(Qe,fjo),e(Qe,vie),e(vie,gjo),e(Qe,hjo),e(Qe,os),e(os,ujo),e(os,Tie),e(Tie,pjo),e(os,_jo),e(os,Fie),e(Fie,bjo),e(os,vjo),e(os,Cie),e(Cie,Tjo),e(os,Fjo),e(Qe,Cjo),e(Qe,Le),e(Le,a1),e(a1,Mie),e(Mie,Mjo),e(a1,Ejo),e(a1,zj),e(zj,yjo),e(a1,wjo),e(Le,Ajo),e(Le,s1),e(s1,Eie),e(Eie,Ljo),e(s1,Bjo),e(s1,Wj),e(Wj,xjo),e(s1,kjo),e(Le,Rjo),e(Le,n1),e(n1,yie),e(yie,Sjo),e(n1,Pjo),e(n1,Qj),e(Qj,$jo),e(n1,Ijo),e(Le,Djo),e(Le,l1),e(l1,wie),e(wie,jjo),e(l1,Njo),e(l1,Hj),e(Hj,qjo),e(l1,Gjo),e(Le,Ojo),e(Le,i1),e(i1,Aie),e(Aie,Xjo),e(i1,Vjo),e(i1,Uj),e(Uj,zjo),e(i1,Wjo),e(Le,Qjo),e(Le,d1),e(d1,Lie),e(Lie,Hjo),e(d1,Ujo),e(d1,Jj),e(Jj,Jjo),e(d1,Yjo),e(Le,Kjo),e(Le,c1),e(c1,Bie),e(Bie,Zjo),e(c1,eNo),e(c1,Yj),e(Yj,oNo),e(c1,rNo),e(Le,tNo),e(Le,m1),e(m1,xie),e(xie,aNo),e(m1,sNo),e(m1,Kj),e(Kj,nNo),e(m1,lNo),e(Qe,iNo),e(Qe,f1),e(f1,dNo),e(f1,kie),e(kie,cNo),e(f1,mNo),e(f1,Rie),e(Rie,fNo),e(Qe,gNo),e(Qe,Sie),e(Sie,hNo),e(Qe,uNo),g(t6,Qe,null),b(c,iBe,_),b(c,Id,_),e(Id,g1),e(g1,Pie),g(a6,Pie,null),e(Id,pNo),e(Id,$ie),e($ie,_No),b(c,dBe,_),b(c,dr,_),g(s6,dr,null),e(dr,bNo),e(dr,Dd),e(Dd,vNo),e(Dd,Iie),e(Iie,TNo),e(Dd,FNo),e(Dd,Die),e(Die,CNo),e(Dd,MNo),e(dr,ENo),e(dr,n6),e(n6,yNo),e(n6,jie),e(jie,wNo),e(n6,ANo),e(dr,LNo),e(dr,rt),g(l6,rt,null),e(rt,BNo),e(rt,Nie),e(Nie,xNo),e(rt,kNo),e(rt,jd),e(jd,RNo),e(jd,qie),e(qie,SNo),e(jd,PNo),e(jd,Gie),e(Gie,$No),e(jd,INo),e(rt,DNo),e(rt,Oie),e(Oie,jNo),e(rt,NNo),g(i6,rt,null),e(dr,qNo),e(dr,He),g(d6,He,null),e(He,GNo),e(He,Xie),e(Xie,ONo),e(He,XNo),e(He,rs),e(rs,VNo),e(rs,Vie),e(Vie,zNo),e(rs,WNo),e(rs,zie),e(zie,QNo),e(rs,HNo),e(rs,Wie),e(Wie,UNo),e(rs,JNo),e(He,YNo),e(He,c6),e(c6,h1),e(h1,Qie),e(Qie,KNo),e(h1,ZNo),e(h1,Zj),e(Zj,eqo),e(h1,oqo),e(c6,rqo),e(c6,u1),e(u1,Hie),e(Hie,tqo),e(u1,aqo),e(u1,eN),e(eN,sqo),e(u1,nqo),e(He,lqo),e(He,p1),e(p1,iqo),e(p1,Uie),e(Uie,dqo),e(p1,cqo),e(p1,Jie),e(Jie,mqo),e(He,fqo),e(He,Yie),e(Yie,gqo),e(He,hqo),g(m6,He,null),b(c,cBe,_),b(c,Nd,_),e(Nd,_1),e(_1,Kie),g(f6,Kie,null),e(Nd,uqo),e(Nd,Zie),e(Zie,pqo),b(c,mBe,_),b(c,cr,_),g(g6,cr,null),e(cr,_qo),e(cr,qd),e(qd,bqo),e(qd,ede),e(ede,vqo),e(qd,Tqo),e(qd,ode),e(ode,Fqo),e(qd,Cqo),e(cr,Mqo),e(cr,h6),e(h6,Eqo),e(h6,rde),e(rde,yqo),e(h6,wqo),e(cr,Aqo),e(cr,tt),g(u6,tt,null),e(tt,Lqo),e(tt,tde),e(tde,Bqo),e(tt,xqo),e(tt,Gd),e(Gd,kqo),e(Gd,ade),e(ade,Rqo),e(Gd,Sqo),e(Gd,sde),e(sde,Pqo),e(Gd,$qo),e(tt,Iqo),e(tt,nde),e(nde,Dqo),e(tt,jqo),g(p6,tt,null),e(cr,Nqo),e(cr,Ue),g(_6,Ue,null),e(Ue,qqo),e(Ue,lde),e(lde,Gqo),e(Ue,Oqo),e(Ue,ts),e(ts,Xqo),e(ts,ide),e(ide,Vqo),e(ts,zqo),e(ts,dde),e(dde,Wqo),e(ts,Qqo),e(ts,cde),e(cde,Hqo),e(ts,Uqo),e(Ue,Jqo),e(Ue,as),e(as,b1),e(b1,mde),e(mde,Yqo),e(b1,Kqo),e(b1,oN),e(oN,Zqo),e(b1,eGo),e(as,oGo),e(as,v1),e(v1,fde),e(fde,rGo),e(v1,tGo),e(v1,rN),e(rN,aGo),e(v1,sGo),e(as,nGo),e(as,T1),e(T1,gde),e(gde,lGo),e(T1,iGo),e(T1,tN),e(tN,dGo),e(T1,cGo),e(as,mGo),e(as,F1),e(F1,hde),e(hde,fGo),e(F1,gGo),e(F1,aN),e(aN,hGo),e(F1,uGo),e(Ue,pGo),e(Ue,C1),e(C1,_Go),e(C1,ude),e(ude,bGo),e(C1,vGo),e(C1,pde),e(pde,TGo),e(Ue,FGo),e(Ue,_de),e(_de,CGo),e(Ue,MGo),g(b6,Ue,null),b(c,fBe,_),b(c,Od,_),e(Od,M1),e(M1,bde),g(v6,bde,null),e(Od,EGo),e(Od,vde),e(vde,yGo),b(c,gBe,_),b(c,mr,_),g(T6,mr,null),e(mr,wGo),e(mr,Xd),e(Xd,AGo),e(Xd,Tde),e(Tde,LGo),e(Xd,BGo),e(Xd,Fde),e(Fde,xGo),e(Xd,kGo),e(mr,RGo),e(mr,F6),e(F6,SGo),e(F6,Cde),e(Cde,PGo),e(F6,$Go),e(mr,IGo),e(mr,at),g(C6,at,null),e(at,DGo),e(at,Mde),e(Mde,jGo),e(at,NGo),e(at,Vd),e(Vd,qGo),e(Vd,Ede),e(Ede,GGo),e(Vd,OGo),e(Vd,yde),e(yde,XGo),e(Vd,VGo),e(at,zGo),e(at,wde),e(wde,WGo),e(at,QGo),g(M6,at,null),e(mr,HGo),e(mr,Je),g(E6,Je,null),e(Je,UGo),e(Je,Ade),e(Ade,JGo),e(Je,YGo),e(Je,ss),e(ss,KGo),e(ss,Lde),e(Lde,ZGo),e(ss,eOo),e(ss,Bde),e(Bde,oOo),e(ss,rOo),e(ss,xde),e(xde,tOo),e(ss,aOo),e(Je,sOo),e(Je,zd),e(zd,E1),e(E1,kde),e(kde,nOo),e(E1,lOo),e(E1,sN),e(sN,iOo),e(E1,dOo),e(zd,cOo),e(zd,y1),e(y1,Rde),e(Rde,mOo),e(y1,fOo),e(y1,nN),e(nN,gOo),e(y1,hOo),e(zd,uOo),e(zd,w1),e(w1,Sde),e(Sde,pOo),e(w1,_Oo),e(w1,lN),e(lN,bOo),e(w1,vOo),e(Je,TOo),e(Je,A1),e(A1,FOo),e(A1,Pde),e(Pde,COo),e(A1,MOo),e(A1,$de),e($de,EOo),e(Je,yOo),e(Je,Ide),e(Ide,wOo),e(Je,AOo),g(y6,Je,null),b(c,hBe,_),b(c,Wd,_),e(Wd,L1),e(L1,Dde),g(w6,Dde,null),e(Wd,LOo),e(Wd,jde),e(jde,BOo),b(c,uBe,_),b(c,fr,_),g(A6,fr,null),e(fr,xOo),e(fr,Qd),e(Qd,kOo),e(Qd,Nde),e(Nde,ROo),e(Qd,SOo),e(Qd,qde),e(qde,POo),e(Qd,$Oo),e(fr,IOo),e(fr,L6),e(L6,DOo),e(L6,Gde),e(Gde,jOo),e(L6,NOo),e(fr,qOo),e(fr,st),g(B6,st,null),e(st,GOo),e(st,Ode),e(Ode,OOo),e(st,XOo),e(st,Hd),e(Hd,VOo),e(Hd,Xde),e(Xde,zOo),e(Hd,WOo),e(Hd,Vde),e(Vde,QOo),e(Hd,HOo),e(st,UOo),e(st,zde),e(zde,JOo),e(st,YOo),g(x6,st,null),e(fr,KOo),e(fr,Ye),g(k6,Ye,null),e(Ye,ZOo),e(Ye,Wde),e(Wde,eXo),e(Ye,oXo),e(Ye,ns),e(ns,rXo),e(ns,Qde),e(Qde,tXo),e(ns,aXo),e(ns,Hde),e(Hde,sXo),e(ns,nXo),e(ns,Ude),e(Ude,lXo),e(ns,iXo),e(Ye,dXo),e(Ye,Jde),e(Jde,B1),e(B1,Yde),e(Yde,cXo),e(B1,mXo),e(B1,iN),e(iN,fXo),e(B1,gXo),e(Ye,hXo),e(Ye,x1),e(x1,uXo),e(x1,Kde),e(Kde,pXo),e(x1,_Xo),e(x1,Zde),e(Zde,bXo),e(Ye,vXo),e(Ye,ece),e(ece,TXo),e(Ye,FXo),g(R6,Ye,null),b(c,pBe,_),b(c,Ud,_),e(Ud,k1),e(k1,oce),g(S6,oce,null),e(Ud,CXo),e(Ud,rce),e(rce,MXo),b(c,_Be,_),b(c,gr,_),g(P6,gr,null),e(gr,EXo),e(gr,Jd),e(Jd,yXo),e(Jd,tce),e(tce,wXo),e(Jd,AXo),e(Jd,ace),e(ace,LXo),e(Jd,BXo),e(gr,xXo),e(gr,$6),e($6,kXo),e($6,sce),e(sce,RXo),e($6,SXo),e(gr,PXo),e(gr,nt),g(I6,nt,null),e(nt,$Xo),e(nt,nce),e(nce,IXo),e(nt,DXo),e(nt,Yd),e(Yd,jXo),e(Yd,lce),e(lce,NXo),e(Yd,qXo),e(Yd,ice),e(ice,GXo),e(Yd,OXo),e(nt,XXo),e(nt,dce),e(dce,VXo),e(nt,zXo),g(D6,nt,null),e(gr,WXo),e(gr,Ke),g(j6,Ke,null),e(Ke,QXo),e(Ke,cce),e(cce,HXo),e(Ke,UXo),e(Ke,ls),e(ls,JXo),e(ls,mce),e(mce,YXo),e(ls,KXo),e(ls,fce),e(fce,ZXo),e(ls,eVo),e(ls,gce),e(gce,oVo),e(ls,rVo),e(Ke,tVo),e(Ke,hce),e(hce,R1),e(R1,uce),e(uce,aVo),e(R1,sVo),e(R1,dN),e(dN,nVo),e(R1,lVo),e(Ke,iVo),e(Ke,S1),e(S1,dVo),e(S1,pce),e(pce,cVo),e(S1,mVo),e(S1,_ce),e(_ce,fVo),e(Ke,gVo),e(Ke,bce),e(bce,hVo),e(Ke,uVo),g(N6,Ke,null),b(c,bBe,_),b(c,Kd,_),e(Kd,P1),e(P1,vce),g(q6,vce,null),e(Kd,pVo),e(Kd,Tce),e(Tce,_Vo),b(c,vBe,_),b(c,hr,_),g(G6,hr,null),e(hr,bVo),e(hr,Zd),e(Zd,vVo),e(Zd,Fce),e(Fce,TVo),e(Zd,FVo),e(Zd,Cce),e(Cce,CVo),e(Zd,MVo),e(hr,EVo),e(hr,O6),e(O6,yVo),e(O6,Mce),e(Mce,wVo),e(O6,AVo),e(hr,LVo),e(hr,lt),g(X6,lt,null),e(lt,BVo),e(lt,Ece),e(Ece,xVo),e(lt,kVo),e(lt,ec),e(ec,RVo),e(ec,yce),e(yce,SVo),e(ec,PVo),e(ec,wce),e(wce,$Vo),e(ec,IVo),e(lt,DVo),e(lt,Ace),e(Ace,jVo),e(lt,NVo),g(V6,lt,null),e(hr,qVo),e(hr,Ze),g(z6,Ze,null),e(Ze,GVo),e(Ze,Lce),e(Lce,OVo),e(Ze,XVo),e(Ze,is),e(is,VVo),e(is,Bce),e(Bce,zVo),e(is,WVo),e(is,xce),e(xce,QVo),e(is,HVo),e(is,kce),e(kce,UVo),e(is,JVo),e(Ze,YVo),e(Ze,W6),e(W6,$1),e($1,Rce),e(Rce,KVo),e($1,ZVo),e($1,cN),e(cN,ezo),e($1,ozo),e(W6,rzo),e(W6,I1),e(I1,Sce),e(Sce,tzo),e(I1,azo),e(I1,mN),e(mN,szo),e(I1,nzo),e(Ze,lzo),e(Ze,D1),e(D1,izo),e(D1,Pce),e(Pce,dzo),e(D1,czo),e(D1,$ce),e($ce,mzo),e(Ze,fzo),e(Ze,Ice),e(Ice,gzo),e(Ze,hzo),g(Q6,Ze,null),b(c,TBe,_),b(c,oc,_),e(oc,j1),e(j1,Dce),g(H6,Dce,null),e(oc,uzo),e(oc,jce),e(jce,pzo),b(c,FBe,_),b(c,ur,_),g(U6,ur,null),e(ur,_zo),e(ur,rc),e(rc,bzo),e(rc,Nce),e(Nce,vzo),e(rc,Tzo),e(rc,qce),e(qce,Fzo),e(rc,Czo),e(ur,Mzo),e(ur,J6),e(J6,Ezo),e(J6,Gce),e(Gce,yzo),e(J6,wzo),e(ur,Azo),e(ur,it),g(Y6,it,null),e(it,Lzo),e(it,Oce),e(Oce,Bzo),e(it,xzo),e(it,tc),e(tc,kzo),e(tc,Xce),e(Xce,Rzo),e(tc,Szo),e(tc,Vce),e(Vce,Pzo),e(tc,$zo),e(it,Izo),e(it,zce),e(zce,Dzo),e(it,jzo),g(K6,it,null),e(ur,Nzo),e(ur,go),g(Z6,go,null),e(go,qzo),e(go,Wce),e(Wce,Gzo),e(go,Ozo),e(go,ds),e(ds,Xzo),e(ds,Qce),e(Qce,Vzo),e(ds,zzo),e(ds,Hce),e(Hce,Wzo),e(ds,Qzo),e(ds,Uce),e(Uce,Hzo),e(ds,Uzo),e(go,Jzo),e(go,B),e(B,N1),e(N1,Jce),e(Jce,Yzo),e(N1,Kzo),e(N1,fN),e(fN,Zzo),e(N1,eWo),e(B,oWo),e(B,q1),e(q1,Yce),e(Yce,rWo),e(q1,tWo),e(q1,gN),e(gN,aWo),e(q1,sWo),e(B,nWo),e(B,G1),e(G1,Kce),e(Kce,lWo),e(G1,iWo),e(G1,hN),e(hN,dWo),e(G1,cWo),e(B,mWo),e(B,O1),e(O1,Zce),e(Zce,fWo),e(O1,gWo),e(O1,uN),e(uN,hWo),e(O1,uWo),e(B,pWo),e(B,X1),e(X1,eme),e(eme,_Wo),e(X1,bWo),e(X1,pN),e(pN,vWo),e(X1,TWo),e(B,FWo),e(B,V1),e(V1,ome),e(ome,CWo),e(V1,MWo),e(V1,_N),e(_N,EWo),e(V1,yWo),e(B,wWo),e(B,z1),e(z1,rme),e(rme,AWo),e(z1,LWo),e(z1,bN),e(bN,BWo),e(z1,xWo),e(B,kWo),e(B,W1),e(W1,tme),e(tme,RWo),e(W1,SWo),e(W1,vN),e(vN,PWo),e(W1,$Wo),e(B,IWo),e(B,Q1),e(Q1,ame),e(ame,DWo),e(Q1,jWo),e(Q1,TN),e(TN,NWo),e(Q1,qWo),e(B,GWo),e(B,H1),e(H1,sme),e(sme,OWo),e(H1,XWo),e(H1,FN),e(FN,VWo),e(H1,zWo),e(B,WWo),e(B,U1),e(U1,nme),e(nme,QWo),e(U1,HWo),e(U1,CN),e(CN,UWo),e(U1,JWo),e(B,YWo),e(B,J1),e(J1,lme),e(lme,KWo),e(J1,ZWo),e(J1,MN),e(MN,eQo),e(J1,oQo),e(B,rQo),e(B,Y1),e(Y1,ime),e(ime,tQo),e(Y1,aQo),e(Y1,EN),e(EN,sQo),e(Y1,nQo),e(B,lQo),e(B,K1),e(K1,dme),e(dme,iQo),e(K1,dQo),e(K1,yN),e(yN,cQo),e(K1,mQo),e(B,fQo),e(B,Z1),e(Z1,cme),e(cme,gQo),e(Z1,hQo),e(Z1,wN),e(wN,uQo),e(Z1,pQo),e(B,_Qo),e(B,eF),e(eF,mme),e(mme,bQo),e(eF,vQo),e(eF,AN),e(AN,TQo),e(eF,FQo),e(B,CQo),e(B,$n),e($n,fme),e(fme,MQo),e($n,EQo),e($n,LN),e(LN,yQo),e($n,wQo),e($n,BN),e(BN,AQo),e($n,LQo),e(B,BQo),e(B,oF),e(oF,gme),e(gme,xQo),e(oF,kQo),e(oF,xN),e(xN,RQo),e(oF,SQo),e(B,PQo),e(B,rF),e(rF,hme),e(hme,$Qo),e(rF,IQo),e(rF,kN),e(kN,DQo),e(rF,jQo),e(B,NQo),e(B,tF),e(tF,ume),e(ume,qQo),e(tF,GQo),e(tF,RN),e(RN,OQo),e(tF,XQo),e(B,VQo),e(B,aF),e(aF,pme),e(pme,zQo),e(aF,WQo),e(aF,SN),e(SN,QQo),e(aF,HQo),e(B,UQo),e(B,sF),e(sF,_me),e(_me,JQo),e(sF,YQo),e(sF,PN),e(PN,KQo),e(sF,ZQo),e(B,eHo),e(B,nF),e(nF,bme),e(bme,oHo),e(nF,rHo),e(nF,$N),e($N,tHo),e(nF,aHo),e(B,sHo),e(B,lF),e(lF,vme),e(vme,nHo),e(lF,lHo),e(lF,IN),e(IN,iHo),e(lF,dHo),e(B,cHo),e(B,iF),e(iF,Tme),e(Tme,mHo),e(iF,fHo),e(iF,DN),e(DN,gHo),e(iF,hHo),e(B,uHo),e(B,dF),e(dF,Fme),e(Fme,pHo),e(dF,_Ho),e(dF,jN),e(jN,bHo),e(dF,vHo),e(B,THo),e(B,cF),e(cF,Cme),e(Cme,FHo),e(cF,CHo),e(cF,NN),e(NN,MHo),e(cF,EHo),e(B,yHo),e(B,mF),e(mF,Mme),e(Mme,wHo),e(mF,AHo),e(mF,qN),e(qN,LHo),e(mF,BHo),e(B,xHo),e(B,fF),e(fF,Eme),e(Eme,kHo),e(fF,RHo),e(fF,GN),e(GN,SHo),e(fF,PHo),e(B,$Ho),e(B,gF),e(gF,yme),e(yme,IHo),e(gF,DHo),e(gF,ON),e(ON,jHo),e(gF,NHo),e(B,qHo),e(B,hF),e(hF,wme),e(wme,GHo),e(hF,OHo),e(hF,XN),e(XN,XHo),e(hF,VHo),e(B,zHo),e(B,uF),e(uF,Ame),e(Ame,WHo),e(uF,QHo),e(uF,VN),e(VN,HHo),e(uF,UHo),e(B,JHo),e(B,pF),e(pF,Lme),e(Lme,YHo),e(pF,KHo),e(pF,zN),e(zN,ZHo),e(pF,eUo),e(B,oUo),e(B,_F),e(_F,Bme),e(Bme,rUo),e(_F,tUo),e(_F,WN),e(WN,aUo),e(_F,sUo),e(B,nUo),e(B,bF),e(bF,xme),e(xme,lUo),e(bF,iUo),e(bF,QN),e(QN,dUo),e(bF,cUo),e(B,mUo),e(B,vF),e(vF,kme),e(kme,fUo),e(vF,gUo),e(vF,HN),e(HN,hUo),e(vF,uUo),e(B,pUo),e(B,TF),e(TF,Rme),e(Rme,_Uo),e(TF,bUo),e(TF,UN),e(UN,vUo),e(TF,TUo),e(B,FUo),e(B,FF),e(FF,Sme),e(Sme,CUo),e(FF,MUo),e(FF,JN),e(JN,EUo),e(FF,yUo),e(B,wUo),e(B,CF),e(CF,Pme),e(Pme,AUo),e(CF,LUo),e(CF,YN),e(YN,BUo),e(CF,xUo),e(B,kUo),e(B,MF),e(MF,$me),e($me,RUo),e(MF,SUo),e(MF,KN),e(KN,PUo),e(MF,$Uo),e(B,IUo),e(B,EF),e(EF,Ime),e(Ime,DUo),e(EF,jUo),e(EF,ZN),e(ZN,NUo),e(EF,qUo),e(B,GUo),e(B,yF),e(yF,Dme),e(Dme,OUo),e(yF,XUo),e(yF,eq),e(eq,VUo),e(yF,zUo),e(go,WUo),e(go,jme),e(jme,QUo),e(go,HUo),g(eA,go,null),b(c,CBe,_),b(c,ac,_),e(ac,wF),e(wF,Nme),g(oA,Nme,null),e(ac,UUo),e(ac,qme),e(qme,JUo),b(c,MBe,_),b(c,pr,_),g(rA,pr,null),e(pr,YUo),e(pr,sc),e(sc,KUo),e(sc,Gme),e(Gme,ZUo),e(sc,eJo),e(sc,Ome),e(Ome,oJo),e(sc,rJo),e(pr,tJo),e(pr,tA),e(tA,aJo),e(tA,Xme),e(Xme,sJo),e(tA,nJo),e(pr,lJo),e(pr,dt),g(aA,dt,null),e(dt,iJo),e(dt,Vme),e(Vme,dJo),e(dt,cJo),e(dt,nc),e(nc,mJo),e(nc,zme),e(zme,fJo),e(nc,gJo),e(nc,Wme),e(Wme,hJo),e(nc,uJo),e(dt,pJo),e(dt,Qme),e(Qme,_Jo),e(dt,bJo),g(sA,dt,null),e(pr,vJo),e(pr,ho),g(nA,ho,null),e(ho,TJo),e(ho,Hme),e(Hme,FJo),e(ho,CJo),e(ho,cs),e(cs,MJo),e(cs,Ume),e(Ume,EJo),e(cs,yJo),e(cs,Jme),e(Jme,wJo),e(cs,AJo),e(cs,Yme),e(Yme,LJo),e(cs,BJo),e(ho,xJo),e(ho,H),e(H,AF),e(AF,Kme),e(Kme,kJo),e(AF,RJo),e(AF,oq),e(oq,SJo),e(AF,PJo),e(H,$Jo),e(H,LF),e(LF,Zme),e(Zme,IJo),e(LF,DJo),e(LF,rq),e(rq,jJo),e(LF,NJo),e(H,qJo),e(H,BF),e(BF,efe),e(efe,GJo),e(BF,OJo),e(BF,tq),e(tq,XJo),e(BF,VJo),e(H,zJo),e(H,xF),e(xF,ofe),e(ofe,WJo),e(xF,QJo),e(xF,aq),e(aq,HJo),e(xF,UJo),e(H,JJo),e(H,kF),e(kF,rfe),e(rfe,YJo),e(kF,KJo),e(kF,sq),e(sq,ZJo),e(kF,eYo),e(H,oYo),e(H,RF),e(RF,tfe),e(tfe,rYo),e(RF,tYo),e(RF,nq),e(nq,aYo),e(RF,sYo),e(H,nYo),e(H,SF),e(SF,afe),e(afe,lYo),e(SF,iYo),e(SF,lq),e(lq,dYo),e(SF,cYo),e(H,mYo),e(H,PF),e(PF,sfe),e(sfe,fYo),e(PF,gYo),e(PF,iq),e(iq,hYo),e(PF,uYo),e(H,pYo),e(H,$F),e($F,nfe),e(nfe,_Yo),e($F,bYo),e($F,dq),e(dq,vYo),e($F,TYo),e(H,FYo),e(H,IF),e(IF,lfe),e(lfe,CYo),e(IF,MYo),e(IF,cq),e(cq,EYo),e(IF,yYo),e(H,wYo),e(H,DF),e(DF,ife),e(ife,AYo),e(DF,LYo),e(DF,mq),e(mq,BYo),e(DF,xYo),e(H,kYo),e(H,jF),e(jF,dfe),e(dfe,RYo),e(jF,SYo),e(jF,fq),e(fq,PYo),e(jF,$Yo),e(H,IYo),e(H,NF),e(NF,cfe),e(cfe,DYo),e(NF,jYo),e(NF,gq),e(gq,NYo),e(NF,qYo),e(H,GYo),e(H,qF),e(qF,mfe),e(mfe,OYo),e(qF,XYo),e(qF,hq),e(hq,VYo),e(qF,zYo),e(H,WYo),e(H,GF),e(GF,ffe),e(ffe,QYo),e(GF,HYo),e(GF,uq),e(uq,UYo),e(GF,JYo),e(H,YYo),e(H,OF),e(OF,gfe),e(gfe,KYo),e(OF,ZYo),e(OF,pq),e(pq,eKo),e(OF,oKo),e(H,rKo),e(H,XF),e(XF,hfe),e(hfe,tKo),e(XF,aKo),e(XF,_q),e(_q,sKo),e(XF,nKo),e(H,lKo),e(H,VF),e(VF,ufe),e(ufe,iKo),e(VF,dKo),e(VF,bq),e(bq,cKo),e(VF,mKo),e(H,fKo),e(H,zF),e(zF,pfe),e(pfe,gKo),e(zF,hKo),e(zF,vq),e(vq,uKo),e(zF,pKo),e(H,_Ko),e(H,WF),e(WF,_fe),e(_fe,bKo),e(WF,vKo),e(WF,Tq),e(Tq,TKo),e(WF,FKo),e(H,CKo),e(H,QF),e(QF,bfe),e(bfe,MKo),e(QF,EKo),e(QF,Fq),e(Fq,yKo),e(QF,wKo),e(H,AKo),e(H,HF),e(HF,vfe),e(vfe,LKo),e(HF,BKo),e(HF,Cq),e(Cq,xKo),e(HF,kKo),e(ho,RKo),e(ho,Tfe),e(Tfe,SKo),e(ho,PKo),g(lA,ho,null),b(c,EBe,_),b(c,lc,_),e(lc,UF),e(UF,Ffe),g(iA,Ffe,null),e(lc,$Ko),e(lc,Cfe),e(Cfe,IKo),b(c,yBe,_),b(c,_r,_),g(dA,_r,null),e(_r,DKo),e(_r,ic),e(ic,jKo),e(ic,Mfe),e(Mfe,NKo),e(ic,qKo),e(ic,Efe),e(Efe,GKo),e(ic,OKo),e(_r,XKo),e(_r,cA),e(cA,VKo),e(cA,yfe),e(yfe,zKo),e(cA,WKo),e(_r,QKo),e(_r,ct),g(mA,ct,null),e(ct,HKo),e(ct,wfe),e(wfe,UKo),e(ct,JKo),e(ct,dc),e(dc,YKo),e(dc,Afe),e(Afe,KKo),e(dc,ZKo),e(dc,Lfe),e(Lfe,eZo),e(dc,oZo),e(ct,rZo),e(ct,Bfe),e(Bfe,tZo),e(ct,aZo),g(fA,ct,null),e(_r,sZo),e(_r,uo),g(gA,uo,null),e(uo,nZo),e(uo,xfe),e(xfe,lZo),e(uo,iZo),e(uo,ms),e(ms,dZo),e(ms,kfe),e(kfe,cZo),e(ms,mZo),e(ms,Rfe),e(Rfe,fZo),e(ms,gZo),e(ms,Sfe),e(Sfe,hZo),e(ms,uZo),e(uo,pZo),e(uo,he),e(he,JF),e(JF,Pfe),e(Pfe,_Zo),e(JF,bZo),e(JF,Mq),e(Mq,vZo),e(JF,TZo),e(he,FZo),e(he,YF),e(YF,$fe),e($fe,CZo),e(YF,MZo),e(YF,Eq),e(Eq,EZo),e(YF,yZo),e(he,wZo),e(he,KF),e(KF,Ife),e(Ife,AZo),e(KF,LZo),e(KF,yq),e(yq,BZo),e(KF,xZo),e(he,kZo),e(he,ZF),e(ZF,Dfe),e(Dfe,RZo),e(ZF,SZo),e(ZF,wq),e(wq,PZo),e(ZF,$Zo),e(he,IZo),e(he,eC),e(eC,jfe),e(jfe,DZo),e(eC,jZo),e(eC,Aq),e(Aq,NZo),e(eC,qZo),e(he,GZo),e(he,oC),e(oC,Nfe),e(Nfe,OZo),e(oC,XZo),e(oC,Lq),e(Lq,VZo),e(oC,zZo),e(he,WZo),e(he,rC),e(rC,qfe),e(qfe,QZo),e(rC,HZo),e(rC,Bq),e(Bq,UZo),e(rC,JZo),e(he,YZo),e(he,tC),e(tC,Gfe),e(Gfe,KZo),e(tC,ZZo),e(tC,xq),e(xq,eer),e(tC,oer),e(he,rer),e(he,aC),e(aC,Ofe),e(Ofe,ter),e(aC,aer),e(aC,kq),e(kq,ser),e(aC,ner),e(he,ler),e(he,sC),e(sC,Xfe),e(Xfe,ier),e(sC,der),e(sC,Rq),e(Rq,cer),e(sC,mer),e(uo,fer),e(uo,Vfe),e(Vfe,ger),e(uo,her),g(hA,uo,null),b(c,wBe,_),b(c,cc,_),e(cc,nC),e(nC,zfe),g(uA,zfe,null),e(cc,uer),e(cc,Wfe),e(Wfe,per),b(c,ABe,_),b(c,br,_),g(pA,br,null),e(br,_er),e(br,mc),e(mc,ber),e(mc,Qfe),e(Qfe,ver),e(mc,Ter),e(mc,Hfe),e(Hfe,Fer),e(mc,Cer),e(br,Mer),e(br,_A),e(_A,Eer),e(_A,Ufe),e(Ufe,yer),e(_A,wer),e(br,Aer),e(br,mt),g(bA,mt,null),e(mt,Ler),e(mt,Jfe),e(Jfe,Ber),e(mt,xer),e(mt,fc),e(fc,ker),e(fc,Yfe),e(Yfe,Rer),e(fc,Ser),e(fc,Kfe),e(Kfe,Per),e(fc,$er),e(mt,Ier),e(mt,Zfe),e(Zfe,Der),e(mt,jer),g(vA,mt,null),e(br,Ner),e(br,po),g(TA,po,null),e(po,qer),e(po,ege),e(ege,Ger),e(po,Oer),e(po,fs),e(fs,Xer),e(fs,oge),e(oge,Ver),e(fs,zer),e(fs,rge),e(rge,Wer),e(fs,Qer),e(fs,tge),e(tge,Her),e(fs,Uer),e(po,Jer),e(po,FA),e(FA,lC),e(lC,age),e(age,Yer),e(lC,Ker),e(lC,Sq),e(Sq,Zer),e(lC,eor),e(FA,oor),e(FA,iC),e(iC,sge),e(sge,ror),e(iC,tor),e(iC,Pq),e(Pq,aor),e(iC,sor),e(po,nor),e(po,nge),e(nge,lor),e(po,ior),g(CA,po,null),b(c,LBe,_),b(c,gc,_),e(gc,dC),e(dC,lge),g(MA,lge,null),e(gc,dor),e(gc,ige),e(ige,cor),b(c,BBe,_),b(c,vr,_),g(EA,vr,null),e(vr,mor),e(vr,hc),e(hc,gor),e(hc,dge),e(dge,hor),e(hc,uor),e(hc,cge),e(cge,por),e(hc,_or),e(vr,bor),e(vr,yA),e(yA,vor),e(yA,mge),e(mge,Tor),e(yA,For),e(vr,Cor),e(vr,ft),g(wA,ft,null),e(ft,Mor),e(ft,fge),e(fge,Eor),e(ft,yor),e(ft,uc),e(uc,wor),e(uc,gge),e(gge,Aor),e(uc,Lor),e(uc,hge),e(hge,Bor),e(uc,xor),e(ft,kor),e(ft,uge),e(uge,Ror),e(ft,Sor),g(AA,ft,null),e(vr,Por),e(vr,_o),g(LA,_o,null),e(_o,$or),e(_o,pge),e(pge,Ior),e(_o,Dor),e(_o,gs),e(gs,jor),e(gs,_ge),e(_ge,Nor),e(gs,qor),e(gs,bge),e(bge,Gor),e(gs,Oor),e(gs,vge),e(vge,Xor),e(gs,Vor),e(_o,zor),e(_o,Y),e(Y,cC),e(cC,Tge),e(Tge,Wor),e(cC,Qor),e(cC,$q),e($q,Hor),e(cC,Uor),e(Y,Jor),e(Y,mC),e(mC,Fge),e(Fge,Yor),e(mC,Kor),e(mC,Iq),e(Iq,Zor),e(mC,err),e(Y,orr),e(Y,fC),e(fC,Cge),e(Cge,rrr),e(fC,trr),e(fC,Dq),e(Dq,arr),e(fC,srr),e(Y,nrr),e(Y,gC),e(gC,Mge),e(Mge,lrr),e(gC,irr),e(gC,jq),e(jq,drr),e(gC,crr),e(Y,mrr),e(Y,hC),e(hC,Ege),e(Ege,frr),e(hC,grr),e(hC,Nq),e(Nq,hrr),e(hC,urr),e(Y,prr),e(Y,uC),e(uC,yge),e(yge,_rr),e(uC,brr),e(uC,qq),e(qq,vrr),e(uC,Trr),e(Y,Frr),e(Y,pC),e(pC,wge),e(wge,Crr),e(pC,Mrr),e(pC,Gq),e(Gq,Err),e(pC,yrr),e(Y,wrr),e(Y,_C),e(_C,Age),e(Age,Arr),e(_C,Lrr),e(_C,Oq),e(Oq,Brr),e(_C,xrr),e(Y,krr),e(Y,bC),e(bC,Lge),e(Lge,Rrr),e(bC,Srr),e(bC,Xq),e(Xq,Prr),e(bC,$rr),e(Y,Irr),e(Y,vC),e(vC,Bge),e(Bge,Drr),e(vC,jrr),e(vC,Vq),e(Vq,Nrr),e(vC,qrr),e(Y,Grr),e(Y,TC),e(TC,xge),e(xge,Orr),e(TC,Xrr),e(TC,zq),e(zq,Vrr),e(TC,zrr),e(Y,Wrr),e(Y,FC),e(FC,kge),e(kge,Qrr),e(FC,Hrr),e(FC,Wq),e(Wq,Urr),e(FC,Jrr),e(Y,Yrr),e(Y,CC),e(CC,Rge),e(Rge,Krr),e(CC,Zrr),e(CC,Qq),e(Qq,etr),e(CC,otr),e(Y,rtr),e(Y,MC),e(MC,Sge),e(Sge,ttr),e(MC,atr),e(MC,Hq),e(Hq,str),e(MC,ntr),e(Y,ltr),e(Y,EC),e(EC,Pge),e(Pge,itr),e(EC,dtr),e(EC,Uq),e(Uq,ctr),e(EC,mtr),e(Y,ftr),e(Y,yC),e(yC,$ge),e($ge,gtr),e(yC,htr),e(yC,Jq),e(Jq,utr),e(yC,ptr),e(Y,_tr),e(Y,wC),e(wC,Ige),e(Ige,btr),e(wC,vtr),e(wC,Yq),e(Yq,Ttr),e(wC,Ftr),e(Y,Ctr),e(Y,AC),e(AC,Dge),e(Dge,Mtr),e(AC,Etr),e(AC,Kq),e(Kq,ytr),e(AC,wtr),e(Y,Atr),e(Y,LC),e(LC,jge),e(jge,Ltr),e(LC,Btr),e(LC,Zq),e(Zq,xtr),e(LC,ktr),e(Y,Rtr),e(Y,BC),e(BC,Nge),e(Nge,Str),e(BC,Ptr),e(BC,eG),e(eG,$tr),e(BC,Itr),e(_o,Dtr),e(_o,qge),e(qge,jtr),e(_o,Ntr),g(BA,_o,null),b(c,xBe,_),b(c,pc,_),e(pc,xC),e(xC,Gge),g(xA,Gge,null),e(pc,qtr),e(pc,Oge),e(Oge,Gtr),b(c,kBe,_),b(c,Tr,_),g(kA,Tr,null),e(Tr,Otr),e(Tr,_c),e(_c,Xtr),e(_c,Xge),e(Xge,Vtr),e(_c,ztr),e(_c,Vge),e(Vge,Wtr),e(_c,Qtr),e(Tr,Htr),e(Tr,RA),e(RA,Utr),e(RA,zge),e(zge,Jtr),e(RA,Ytr),e(Tr,Ktr),e(Tr,gt),g(SA,gt,null),e(gt,Ztr),e(gt,Wge),e(Wge,ear),e(gt,oar),e(gt,bc),e(bc,rar),e(bc,Qge),e(Qge,tar),e(bc,aar),e(bc,Hge),e(Hge,sar),e(bc,nar),e(gt,lar),e(gt,Uge),e(Uge,iar),e(gt,dar),g(PA,gt,null),e(Tr,car),e(Tr,bo),g($A,bo,null),e(bo,mar),e(bo,Jge),e(Jge,far),e(bo,gar),e(bo,hs),e(hs,har),e(hs,Yge),e(Yge,uar),e(hs,par),e(hs,Kge),e(Kge,_ar),e(hs,bar),e(hs,Zge),e(Zge,Tar),e(hs,Far),e(bo,Car),e(bo,ue),e(ue,kC),e(kC,ehe),e(ehe,Mar),e(kC,Ear),e(kC,oG),e(oG,yar),e(kC,war),e(ue,Aar),e(ue,RC),e(RC,ohe),e(ohe,Lar),e(RC,Bar),e(RC,rG),e(rG,xar),e(RC,kar),e(ue,Rar),e(ue,SC),e(SC,rhe),e(rhe,Sar),e(SC,Par),e(SC,tG),e(tG,$ar),e(SC,Iar),e(ue,Dar),e(ue,PC),e(PC,the),e(the,jar),e(PC,Nar),e(PC,aG),e(aG,qar),e(PC,Gar),e(ue,Oar),e(ue,$C),e($C,ahe),e(ahe,Xar),e($C,Var),e($C,sG),e(sG,zar),e($C,War),e(ue,Qar),e(ue,IC),e(IC,she),e(she,Har),e(IC,Uar),e(IC,nG),e(nG,Jar),e(IC,Yar),e(ue,Kar),e(ue,DC),e(DC,nhe),e(nhe,Zar),e(DC,esr),e(DC,lG),e(lG,osr),e(DC,rsr),e(ue,tsr),e(ue,jC),e(jC,lhe),e(lhe,asr),e(jC,ssr),e(jC,iG),e(iG,nsr),e(jC,lsr),e(ue,isr),e(ue,NC),e(NC,ihe),e(ihe,dsr),e(NC,csr),e(NC,dG),e(dG,msr),e(NC,fsr),e(ue,gsr),e(ue,qC),e(qC,dhe),e(dhe,hsr),e(qC,usr),e(qC,cG),e(cG,psr),e(qC,_sr),e(bo,bsr),e(bo,che),e(che,vsr),e(bo,Tsr),g(IA,bo,null),b(c,RBe,_),b(c,vc,_),e(vc,GC),e(GC,mhe),g(DA,mhe,null),e(vc,Fsr),e(vc,fhe),e(fhe,Csr),b(c,SBe,_),b(c,Fr,_),g(jA,Fr,null),e(Fr,Msr),e(Fr,Tc),e(Tc,Esr),e(Tc,ghe),e(ghe,ysr),e(Tc,wsr),e(Tc,hhe),e(hhe,Asr),e(Tc,Lsr),e(Fr,Bsr),e(Fr,NA),e(NA,xsr),e(NA,uhe),e(uhe,ksr),e(NA,Rsr),e(Fr,Ssr),e(Fr,ht),g(qA,ht,null),e(ht,Psr),e(ht,phe),e(phe,$sr),e(ht,Isr),e(ht,Fc),e(Fc,Dsr),e(Fc,_he),e(_he,jsr),e(Fc,Nsr),e(Fc,bhe),e(bhe,qsr),e(Fc,Gsr),e(ht,Osr),e(ht,vhe),e(vhe,Xsr),e(ht,Vsr),g(GA,ht,null),e(Fr,zsr),e(Fr,vo),g(OA,vo,null),e(vo,Wsr),e(vo,The),e(The,Qsr),e(vo,Hsr),e(vo,us),e(us,Usr),e(us,Fhe),e(Fhe,Jsr),e(us,Ysr),e(us,Che),e(Che,Ksr),e(us,Zsr),e(us,Mhe),e(Mhe,enr),e(us,onr),e(vo,rnr),e(vo,X),e(X,OC),e(OC,Ehe),e(Ehe,tnr),e(OC,anr),e(OC,mG),e(mG,snr),e(OC,nnr),e(X,lnr),e(X,XC),e(XC,yhe),e(yhe,inr),e(XC,dnr),e(XC,fG),e(fG,cnr),e(XC,mnr),e(X,fnr),e(X,VC),e(VC,whe),e(whe,gnr),e(VC,hnr),e(VC,gG),e(gG,unr),e(VC,pnr),e(X,_nr),e(X,zC),e(zC,Ahe),e(Ahe,bnr),e(zC,vnr),e(zC,hG),e(hG,Tnr),e(zC,Fnr),e(X,Cnr),e(X,WC),e(WC,Lhe),e(Lhe,Mnr),e(WC,Enr),e(WC,uG),e(uG,ynr),e(WC,wnr),e(X,Anr),e(X,QC),e(QC,Bhe),e(Bhe,Lnr),e(QC,Bnr),e(QC,pG),e(pG,xnr),e(QC,knr),e(X,Rnr),e(X,HC),e(HC,xhe),e(xhe,Snr),e(HC,Pnr),e(HC,_G),e(_G,$nr),e(HC,Inr),e(X,Dnr),e(X,UC),e(UC,khe),e(khe,jnr),e(UC,Nnr),e(UC,bG),e(bG,qnr),e(UC,Gnr),e(X,Onr),e(X,JC),e(JC,Rhe),e(Rhe,Xnr),e(JC,Vnr),e(JC,vG),e(vG,znr),e(JC,Wnr),e(X,Qnr),e(X,YC),e(YC,She),e(She,Hnr),e(YC,Unr),e(YC,TG),e(TG,Jnr),e(YC,Ynr),e(X,Knr),e(X,KC),e(KC,Phe),e(Phe,Znr),e(KC,elr),e(KC,FG),e(FG,olr),e(KC,rlr),e(X,tlr),e(X,ZC),e(ZC,$he),e($he,alr),e(ZC,slr),e(ZC,CG),e(CG,nlr),e(ZC,llr),e(X,ilr),e(X,e4),e(e4,Ihe),e(Ihe,dlr),e(e4,clr),e(e4,MG),e(MG,mlr),e(e4,flr),e(X,glr),e(X,o4),e(o4,Dhe),e(Dhe,hlr),e(o4,ulr),e(o4,EG),e(EG,plr),e(o4,_lr),e(X,blr),e(X,r4),e(r4,jhe),e(jhe,vlr),e(r4,Tlr),e(r4,yG),e(yG,Flr),e(r4,Clr),e(X,Mlr),e(X,t4),e(t4,Nhe),e(Nhe,Elr),e(t4,ylr),e(t4,wG),e(wG,wlr),e(t4,Alr),e(X,Llr),e(X,a4),e(a4,qhe),e(qhe,Blr),e(a4,xlr),e(a4,AG),e(AG,klr),e(a4,Rlr),e(X,Slr),e(X,s4),e(s4,Ghe),e(Ghe,Plr),e(s4,$lr),e(s4,LG),e(LG,Ilr),e(s4,Dlr),e(X,jlr),e(X,n4),e(n4,Ohe),e(Ohe,Nlr),e(n4,qlr),e(n4,BG),e(BG,Glr),e(n4,Olr),e(X,Xlr),e(X,l4),e(l4,Xhe),e(Xhe,Vlr),e(l4,zlr),e(l4,xG),e(xG,Wlr),e(l4,Qlr),e(X,Hlr),e(X,i4),e(i4,Vhe),e(Vhe,Ulr),e(i4,Jlr),e(i4,kG),e(kG,Ylr),e(i4,Klr),e(X,Zlr),e(X,d4),e(d4,zhe),e(zhe,eir),e(d4,oir),e(d4,RG),e(RG,rir),e(d4,tir),e(X,air),e(X,c4),e(c4,Whe),e(Whe,sir),e(c4,nir),e(c4,SG),e(SG,lir),e(c4,iir),e(X,dir),e(X,m4),e(m4,Qhe),e(Qhe,cir),e(m4,mir),e(m4,PG),e(PG,fir),e(m4,gir),e(X,hir),e(X,f4),e(f4,Hhe),e(Hhe,uir),e(f4,pir),e(f4,$G),e($G,_ir),e(f4,bir),e(vo,vir),e(vo,Uhe),e(Uhe,Tir),e(vo,Fir),g(XA,vo,null),b(c,PBe,_),b(c,Cc,_),e(Cc,g4),e(g4,Jhe),g(VA,Jhe,null),e(Cc,Cir),e(Cc,Yhe),e(Yhe,Mir),b(c,$Be,_),b(c,Cr,_),g(zA,Cr,null),e(Cr,Eir),e(Cr,Mc),e(Mc,yir),e(Mc,Khe),e(Khe,wir),e(Mc,Air),e(Mc,Zhe),e(Zhe,Lir),e(Mc,Bir),e(Cr,xir),e(Cr,WA),e(WA,kir),e(WA,eue),e(eue,Rir),e(WA,Sir),e(Cr,Pir),e(Cr,ut),g(QA,ut,null),e(ut,$ir),e(ut,oue),e(oue,Iir),e(ut,Dir),e(ut,Ec),e(Ec,jir),e(Ec,rue),e(rue,Nir),e(Ec,qir),e(Ec,tue),e(tue,Gir),e(Ec,Oir),e(ut,Xir),e(ut,aue),e(aue,Vir),e(ut,zir),g(HA,ut,null),e(Cr,Wir),e(Cr,To),g(UA,To,null),e(To,Qir),e(To,sue),e(sue,Hir),e(To,Uir),e(To,ps),e(ps,Jir),e(ps,nue),e(nue,Yir),e(ps,Kir),e(ps,lue),e(lue,Zir),e(ps,edr),e(ps,iue),e(iue,odr),e(ps,rdr),e(To,tdr),e(To,te),e(te,h4),e(h4,due),e(due,adr),e(h4,sdr),e(h4,IG),e(IG,ndr),e(h4,ldr),e(te,idr),e(te,u4),e(u4,cue),e(cue,ddr),e(u4,cdr),e(u4,DG),e(DG,mdr),e(u4,fdr),e(te,gdr),e(te,p4),e(p4,mue),e(mue,hdr),e(p4,udr),e(p4,jG),e(jG,pdr),e(p4,_dr),e(te,bdr),e(te,_4),e(_4,fue),e(fue,vdr),e(_4,Tdr),e(_4,NG),e(NG,Fdr),e(_4,Cdr),e(te,Mdr),e(te,b4),e(b4,gue),e(gue,Edr),e(b4,ydr),e(b4,qG),e(qG,wdr),e(b4,Adr),e(te,Ldr),e(te,v4),e(v4,hue),e(hue,Bdr),e(v4,xdr),e(v4,GG),e(GG,kdr),e(v4,Rdr),e(te,Sdr),e(te,T4),e(T4,uue),e(uue,Pdr),e(T4,$dr),e(T4,OG),e(OG,Idr),e(T4,Ddr),e(te,jdr),e(te,F4),e(F4,pue),e(pue,Ndr),e(F4,qdr),e(F4,XG),e(XG,Gdr),e(F4,Odr),e(te,Xdr),e(te,C4),e(C4,_ue),e(_ue,Vdr),e(C4,zdr),e(C4,VG),e(VG,Wdr),e(C4,Qdr),e(te,Hdr),e(te,M4),e(M4,bue),e(bue,Udr),e(M4,Jdr),e(M4,zG),e(zG,Ydr),e(M4,Kdr),e(te,Zdr),e(te,E4),e(E4,vue),e(vue,ecr),e(E4,ocr),e(E4,WG),e(WG,rcr),e(E4,tcr),e(te,acr),e(te,y4),e(y4,Tue),e(Tue,scr),e(y4,ncr),e(y4,QG),e(QG,lcr),e(y4,icr),e(te,dcr),e(te,w4),e(w4,Fue),e(Fue,ccr),e(w4,mcr),e(w4,HG),e(HG,fcr),e(w4,gcr),e(te,hcr),e(te,A4),e(A4,Cue),e(Cue,ucr),e(A4,pcr),e(A4,UG),e(UG,_cr),e(A4,bcr),e(te,vcr),e(te,L4),e(L4,Mue),e(Mue,Tcr),e(L4,Fcr),e(L4,JG),e(JG,Ccr),e(L4,Mcr),e(te,Ecr),e(te,B4),e(B4,Eue),e(Eue,ycr),e(B4,wcr),e(B4,YG),e(YG,Acr),e(B4,Lcr),e(te,Bcr),e(te,x4),e(x4,yue),e(yue,xcr),e(x4,kcr),e(x4,KG),e(KG,Rcr),e(x4,Scr),e(To,Pcr),e(To,wue),e(wue,$cr),e(To,Icr),g(JA,To,null),b(c,IBe,_),b(c,yc,_),e(yc,k4),e(k4,Aue),g(YA,Aue,null),e(yc,Dcr),e(yc,Lue),e(Lue,jcr),b(c,DBe,_),b(c,Mr,_),g(KA,Mr,null),e(Mr,Ncr),e(Mr,wc),e(wc,qcr),e(wc,Bue),e(Bue,Gcr),e(wc,Ocr),e(wc,xue),e(xue,Xcr),e(wc,Vcr),e(Mr,zcr),e(Mr,ZA),e(ZA,Wcr),e(ZA,kue),e(kue,Qcr),e(ZA,Hcr),e(Mr,Ucr),e(Mr,pt),g(e0,pt,null),e(pt,Jcr),e(pt,Rue),e(Rue,Ycr),e(pt,Kcr),e(pt,Ac),e(Ac,Zcr),e(Ac,Sue),e(Sue,emr),e(Ac,omr),e(Ac,Pue),e(Pue,rmr),e(Ac,tmr),e(pt,amr),e(pt,$ue),e($ue,smr),e(pt,nmr),g(o0,pt,null),e(Mr,lmr),e(Mr,Fo),g(r0,Fo,null),e(Fo,imr),e(Fo,Iue),e(Iue,dmr),e(Fo,cmr),e(Fo,_s),e(_s,mmr),e(_s,Due),e(Due,fmr),e(_s,gmr),e(_s,jue),e(jue,hmr),e(_s,umr),e(_s,Nue),e(Nue,pmr),e(_s,_mr),e(Fo,bmr),e(Fo,que),e(que,R4),e(R4,Gue),e(Gue,vmr),e(R4,Tmr),e(R4,ZG),e(ZG,Fmr),e(R4,Cmr),e(Fo,Mmr),e(Fo,Oue),e(Oue,Emr),e(Fo,ymr),g(t0,Fo,null),b(c,jBe,_),b(c,Lc,_),e(Lc,S4),e(S4,Xue),g(a0,Xue,null),e(Lc,wmr),e(Lc,Vue),e(Vue,Amr),b(c,NBe,_),b(c,Er,_),g(s0,Er,null),e(Er,Lmr),e(Er,Bc),e(Bc,Bmr),e(Bc,zue),e(zue,xmr),e(Bc,kmr),e(Bc,Wue),e(Wue,Rmr),e(Bc,Smr),e(Er,Pmr),e(Er,n0),e(n0,$mr),e(n0,Que),e(Que,Imr),e(n0,Dmr),e(Er,jmr),e(Er,_t),g(l0,_t,null),e(_t,Nmr),e(_t,Hue),e(Hue,qmr),e(_t,Gmr),e(_t,xc),e(xc,Omr),e(xc,Uue),e(Uue,Xmr),e(xc,Vmr),e(xc,Jue),e(Jue,zmr),e(xc,Wmr),e(_t,Qmr),e(_t,Yue),e(Yue,Hmr),e(_t,Umr),g(i0,_t,null),e(Er,Jmr),e(Er,Co),g(d0,Co,null),e(Co,Ymr),e(Co,Kue),e(Kue,Kmr),e(Co,Zmr),e(Co,bs),e(bs,efr),e(bs,Zue),e(Zue,ofr),e(bs,rfr),e(bs,epe),e(epe,tfr),e(bs,afr),e(bs,ope),e(ope,sfr),e(bs,nfr),e(Co,lfr),e(Co,K),e(K,P4),e(P4,rpe),e(rpe,ifr),e(P4,dfr),e(P4,eO),e(eO,cfr),e(P4,mfr),e(K,ffr),e(K,$4),e($4,tpe),e(tpe,gfr),e($4,hfr),e($4,oO),e(oO,ufr),e($4,pfr),e(K,_fr),e(K,I4),e(I4,ape),e(ape,bfr),e(I4,vfr),e(I4,rO),e(rO,Tfr),e(I4,Ffr),e(K,Cfr),e(K,D4),e(D4,spe),e(spe,Mfr),e(D4,Efr),e(D4,tO),e(tO,yfr),e(D4,wfr),e(K,Afr),e(K,j4),e(j4,npe),e(npe,Lfr),e(j4,Bfr),e(j4,aO),e(aO,xfr),e(j4,kfr),e(K,Rfr),e(K,N4),e(N4,lpe),e(lpe,Sfr),e(N4,Pfr),e(N4,sO),e(sO,$fr),e(N4,Ifr),e(K,Dfr),e(K,q4),e(q4,ipe),e(ipe,jfr),e(q4,Nfr),e(q4,nO),e(nO,qfr),e(q4,Gfr),e(K,Ofr),e(K,G4),e(G4,dpe),e(dpe,Xfr),e(G4,Vfr),e(G4,lO),e(lO,zfr),e(G4,Wfr),e(K,Qfr),e(K,O4),e(O4,cpe),e(cpe,Hfr),e(O4,Ufr),e(O4,iO),e(iO,Jfr),e(O4,Yfr),e(K,Kfr),e(K,X4),e(X4,mpe),e(mpe,Zfr),e(X4,egr),e(X4,dO),e(dO,ogr),e(X4,rgr),e(K,tgr),e(K,V4),e(V4,fpe),e(fpe,agr),e(V4,sgr),e(V4,cO),e(cO,ngr),e(V4,lgr),e(K,igr),e(K,z4),e(z4,gpe),e(gpe,dgr),e(z4,cgr),e(z4,mO),e(mO,mgr),e(z4,fgr),e(K,ggr),e(K,W4),e(W4,hpe),e(hpe,hgr),e(W4,ugr),e(W4,fO),e(fO,pgr),e(W4,_gr),e(K,bgr),e(K,Q4),e(Q4,upe),e(upe,vgr),e(Q4,Tgr),e(Q4,gO),e(gO,Fgr),e(Q4,Cgr),e(K,Mgr),e(K,H4),e(H4,ppe),e(ppe,Egr),e(H4,ygr),e(H4,hO),e(hO,wgr),e(H4,Agr),e(K,Lgr),e(K,U4),e(U4,_pe),e(_pe,Bgr),e(U4,xgr),e(U4,uO),e(uO,kgr),e(U4,Rgr),e(K,Sgr),e(K,J4),e(J4,bpe),e(bpe,Pgr),e(J4,$gr),e(J4,pO),e(pO,Igr),e(J4,Dgr),e(K,jgr),e(K,Y4),e(Y4,vpe),e(vpe,Ngr),e(Y4,qgr),e(Y4,_O),e(_O,Ggr),e(Y4,Ogr),e(K,Xgr),e(K,K4),e(K4,Tpe),e(Tpe,Vgr),e(K4,zgr),e(K4,bO),e(bO,Wgr),e(K4,Qgr),e(K,Hgr),e(K,Z4),e(Z4,Fpe),e(Fpe,Ugr),e(Z4,Jgr),e(Z4,vO),e(vO,Ygr),e(Z4,Kgr),e(Co,Zgr),e(Co,Cpe),e(Cpe,ehr),e(Co,ohr),g(c0,Co,null),b(c,qBe,_),b(c,kc,_),e(kc,eM),e(eM,Mpe),g(m0,Mpe,null),e(kc,rhr),e(kc,Epe),e(Epe,thr),b(c,GBe,_),b(c,yr,_),g(f0,yr,null),e(yr,ahr),e(yr,Rc),e(Rc,shr),e(Rc,ype),e(ype,nhr),e(Rc,lhr),e(Rc,wpe),e(wpe,ihr),e(Rc,dhr),e(yr,chr),e(yr,g0),e(g0,mhr),e(g0,Ape),e(Ape,fhr),e(g0,ghr),e(yr,hhr),e(yr,bt),g(h0,bt,null),e(bt,uhr),e(bt,Lpe),e(Lpe,phr),e(bt,_hr),e(bt,Sc),e(Sc,bhr),e(Sc,Bpe),e(Bpe,vhr),e(Sc,Thr),e(Sc,xpe),e(xpe,Fhr),e(Sc,Chr),e(bt,Mhr),e(bt,kpe),e(kpe,Ehr),e(bt,yhr),g(u0,bt,null),e(yr,whr),e(yr,Mo),g(p0,Mo,null),e(Mo,Ahr),e(Mo,Rpe),e(Rpe,Lhr),e(Mo,Bhr),e(Mo,vs),e(vs,xhr),e(vs,Spe),e(Spe,khr),e(vs,Rhr),e(vs,Ppe),e(Ppe,Shr),e(vs,Phr),e(vs,$pe),e($pe,$hr),e(vs,Ihr),e(Mo,Dhr),e(Mo,Z),e(Z,oM),e(oM,Ipe),e(Ipe,jhr),e(oM,Nhr),e(oM,TO),e(TO,qhr),e(oM,Ghr),e(Z,Ohr),e(Z,rM),e(rM,Dpe),e(Dpe,Xhr),e(rM,Vhr),e(rM,FO),e(FO,zhr),e(rM,Whr),e(Z,Qhr),e(Z,tM),e(tM,jpe),e(jpe,Hhr),e(tM,Uhr),e(tM,CO),e(CO,Jhr),e(tM,Yhr),e(Z,Khr),e(Z,aM),e(aM,Npe),e(Npe,Zhr),e(aM,eur),e(aM,MO),e(MO,our),e(aM,rur),e(Z,tur),e(Z,sM),e(sM,qpe),e(qpe,aur),e(sM,sur),e(sM,EO),e(EO,nur),e(sM,lur),e(Z,iur),e(Z,nM),e(nM,Gpe),e(Gpe,dur),e(nM,cur),e(nM,yO),e(yO,mur),e(nM,fur),e(Z,gur),e(Z,lM),e(lM,Ope),e(Ope,hur),e(lM,uur),e(lM,wO),e(wO,pur),e(lM,_ur),e(Z,bur),e(Z,iM),e(iM,Xpe),e(Xpe,vur),e(iM,Tur),e(iM,AO),e(AO,Fur),e(iM,Cur),e(Z,Mur),e(Z,dM),e(dM,Vpe),e(Vpe,Eur),e(dM,yur),e(dM,LO),e(LO,wur),e(dM,Aur),e(Z,Lur),e(Z,cM),e(cM,zpe),e(zpe,Bur),e(cM,xur),e(cM,BO),e(BO,kur),e(cM,Rur),e(Z,Sur),e(Z,mM),e(mM,Wpe),e(Wpe,Pur),e(mM,$ur),e(mM,xO),e(xO,Iur),e(mM,Dur),e(Z,jur),e(Z,fM),e(fM,Qpe),e(Qpe,Nur),e(fM,qur),e(fM,kO),e(kO,Gur),e(fM,Our),e(Z,Xur),e(Z,gM),e(gM,Hpe),e(Hpe,Vur),e(gM,zur),e(gM,RO),e(RO,Wur),e(gM,Qur),e(Z,Hur),e(Z,hM),e(hM,Upe),e(Upe,Uur),e(hM,Jur),e(hM,SO),e(SO,Yur),e(hM,Kur),e(Z,Zur),e(Z,uM),e(uM,Jpe),e(Jpe,epr),e(uM,opr),e(uM,PO),e(PO,rpr),e(uM,tpr),e(Z,apr),e(Z,pM),e(pM,Ype),e(Ype,spr),e(pM,npr),e(pM,$O),e($O,lpr),e(pM,ipr),e(Z,dpr),e(Z,_M),e(_M,Kpe),e(Kpe,cpr),e(_M,mpr),e(_M,IO),e(IO,fpr),e(_M,gpr),e(Z,hpr),e(Z,bM),e(bM,Zpe),e(Zpe,upr),e(bM,ppr),e(bM,DO),e(DO,_pr),e(bM,bpr),e(Z,vpr),e(Z,vM),e(vM,e_e),e(e_e,Tpr),e(vM,Fpr),e(vM,jO),e(jO,Cpr),e(vM,Mpr),e(Mo,Epr),e(Mo,o_e),e(o_e,ypr),e(Mo,wpr),g(_0,Mo,null),b(c,OBe,_),b(c,Pc,_),e(Pc,TM),e(TM,r_e),g(b0,r_e,null),e(Pc,Apr),e(Pc,t_e),e(t_e,Lpr),b(c,XBe,_),b(c,wr,_),g(v0,wr,null),e(wr,Bpr),e(wr,$c),e($c,xpr),e($c,a_e),e(a_e,kpr),e($c,Rpr),e($c,s_e),e(s_e,Spr),e($c,Ppr),e(wr,$pr),e(wr,T0),e(T0,Ipr),e(T0,n_e),e(n_e,Dpr),e(T0,jpr),e(wr,Npr),e(wr,vt),g(F0,vt,null),e(vt,qpr),e(vt,l_e),e(l_e,Gpr),e(vt,Opr),e(vt,Ic),e(Ic,Xpr),e(Ic,i_e),e(i_e,Vpr),e(Ic,zpr),e(Ic,d_e),e(d_e,Wpr),e(Ic,Qpr),e(vt,Hpr),e(vt,c_e),e(c_e,Upr),e(vt,Jpr),g(C0,vt,null),e(wr,Ypr),e(wr,Eo),g(M0,Eo,null),e(Eo,Kpr),e(Eo,m_e),e(m_e,Zpr),e(Eo,e_r),e(Eo,Ts),e(Ts,o_r),e(Ts,f_e),e(f_e,r_r),e(Ts,t_r),e(Ts,g_e),e(g_e,a_r),e(Ts,s_r),e(Ts,h_e),e(h_e,n_r),e(Ts,l_r),e(Eo,i_r),e(Eo,u_e),e(u_e,FM),e(FM,p_e),e(p_e,d_r),e(FM,c_r),e(FM,NO),e(NO,m_r),e(FM,f_r),e(Eo,g_r),e(Eo,__e),e(__e,h_r),e(Eo,u_r),g(E0,Eo,null),b(c,VBe,_),b(c,Dc,_),e(Dc,CM),e(CM,b_e),g(y0,b_e,null),e(Dc,p_r),e(Dc,v_e),e(v_e,__r),b(c,zBe,_),b(c,Ar,_),g(w0,Ar,null),e(Ar,b_r),e(Ar,jc),e(jc,v_r),e(jc,T_e),e(T_e,T_r),e(jc,F_r),e(jc,F_e),e(F_e,C_r),e(jc,M_r),e(Ar,E_r),e(Ar,A0),e(A0,y_r),e(A0,C_e),e(C_e,w_r),e(A0,A_r),e(Ar,L_r),e(Ar,Tt),g(L0,Tt,null),e(Tt,B_r),e(Tt,M_e),e(M_e,x_r),e(Tt,k_r),e(Tt,Nc),e(Nc,R_r),e(Nc,E_e),e(E_e,S_r),e(Nc,P_r),e(Nc,y_e),e(y_e,$_r),e(Nc,I_r),e(Tt,D_r),e(Tt,w_e),e(w_e,j_r),e(Tt,N_r),g(B0,Tt,null),e(Ar,q_r),e(Ar,yo),g(x0,yo,null),e(yo,G_r),e(yo,A_e),e(A_e,O_r),e(yo,X_r),e(yo,Fs),e(Fs,V_r),e(Fs,L_e),e(L_e,z_r),e(Fs,W_r),e(Fs,B_e),e(B_e,Q_r),e(Fs,H_r),e(Fs,x_e),e(x_e,U_r),e(Fs,J_r),e(yo,Y_r),e(yo,k_e),e(k_e,MM),e(MM,R_e),e(R_e,K_r),e(MM,Z_r),e(MM,qO),e(qO,ebr),e(MM,obr),e(yo,rbr),e(yo,S_e),e(S_e,tbr),e(yo,abr),g(k0,yo,null),b(c,WBe,_),b(c,qc,_),e(qc,EM),e(EM,P_e),g(R0,P_e,null),e(qc,sbr),e(qc,$_e),e($_e,nbr),b(c,QBe,_),b(c,Lr,_),g(S0,Lr,null),e(Lr,lbr),e(Lr,Gc),e(Gc,ibr),e(Gc,I_e),e(I_e,dbr),e(Gc,cbr),e(Gc,D_e),e(D_e,mbr),e(Gc,fbr),e(Lr,gbr),e(Lr,P0),e(P0,hbr),e(P0,j_e),e(j_e,ubr),e(P0,pbr),e(Lr,_br),e(Lr,Ft),g($0,Ft,null),e(Ft,bbr),e(Ft,N_e),e(N_e,vbr),e(Ft,Tbr),e(Ft,Oc),e(Oc,Fbr),e(Oc,q_e),e(q_e,Cbr),e(Oc,Mbr),e(Oc,G_e),e(G_e,Ebr),e(Oc,ybr),e(Ft,wbr),e(Ft,O_e),e(O_e,Abr),e(Ft,Lbr),g(I0,Ft,null),e(Lr,Bbr),e(Lr,wo),g(D0,wo,null),e(wo,xbr),e(wo,X_e),e(X_e,kbr),e(wo,Rbr),e(wo,Cs),e(Cs,Sbr),e(Cs,V_e),e(V_e,Pbr),e(Cs,$br),e(Cs,z_e),e(z_e,Ibr),e(Cs,Dbr),e(Cs,W_e),e(W_e,jbr),e(Cs,Nbr),e(wo,qbr),e(wo,z),e(z,yM),e(yM,Q_e),e(Q_e,Gbr),e(yM,Obr),e(yM,GO),e(GO,Xbr),e(yM,Vbr),e(z,zbr),e(z,wM),e(wM,H_e),e(H_e,Wbr),e(wM,Qbr),e(wM,OO),e(OO,Hbr),e(wM,Ubr),e(z,Jbr),e(z,AM),e(AM,U_e),e(U_e,Ybr),e(AM,Kbr),e(AM,XO),e(XO,Zbr),e(AM,e2r),e(z,o2r),e(z,LM),e(LM,J_e),e(J_e,r2r),e(LM,t2r),e(LM,VO),e(VO,a2r),e(LM,s2r),e(z,n2r),e(z,BM),e(BM,Y_e),e(Y_e,l2r),e(BM,i2r),e(BM,zO),e(zO,d2r),e(BM,c2r),e(z,m2r),e(z,xM),e(xM,K_e),e(K_e,f2r),e(xM,g2r),e(xM,WO),e(WO,h2r),e(xM,u2r),e(z,p2r),e(z,kM),e(kM,Z_e),e(Z_e,_2r),e(kM,b2r),e(kM,QO),e(QO,v2r),e(kM,T2r),e(z,F2r),e(z,RM),e(RM,ebe),e(ebe,C2r),e(RM,M2r),e(RM,HO),e(HO,E2r),e(RM,y2r),e(z,w2r),e(z,SM),e(SM,obe),e(obe,A2r),e(SM,L2r),e(SM,UO),e(UO,B2r),e(SM,x2r),e(z,k2r),e(z,PM),e(PM,rbe),e(rbe,R2r),e(PM,S2r),e(PM,JO),e(JO,P2r),e(PM,$2r),e(z,I2r),e(z,$M),e($M,tbe),e(tbe,D2r),e($M,j2r),e($M,YO),e(YO,N2r),e($M,q2r),e(z,G2r),e(z,IM),e(IM,abe),e(abe,O2r),e(IM,X2r),e(IM,KO),e(KO,V2r),e(IM,z2r),e(z,W2r),e(z,DM),e(DM,sbe),e(sbe,Q2r),e(DM,H2r),e(DM,ZO),e(ZO,U2r),e(DM,J2r),e(z,Y2r),e(z,jM),e(jM,nbe),e(nbe,K2r),e(jM,Z2r),e(jM,eX),e(eX,evr),e(jM,ovr),e(z,rvr),e(z,NM),e(NM,lbe),e(lbe,tvr),e(NM,avr),e(NM,oX),e(oX,svr),e(NM,nvr),e(z,lvr),e(z,qM),e(qM,ibe),e(ibe,ivr),e(qM,dvr),e(qM,rX),e(rX,cvr),e(qM,mvr),e(z,fvr),e(z,GM),e(GM,dbe),e(dbe,gvr),e(GM,hvr),e(GM,tX),e(tX,uvr),e(GM,pvr),e(z,_vr),e(z,OM),e(OM,cbe),e(cbe,bvr),e(OM,vvr),e(OM,aX),e(aX,Tvr),e(OM,Fvr),e(z,Cvr),e(z,XM),e(XM,mbe),e(mbe,Mvr),e(XM,Evr),e(XM,sX),e(sX,yvr),e(XM,wvr),e(z,Avr),e(z,VM),e(VM,fbe),e(fbe,Lvr),e(VM,Bvr),e(VM,nX),e(nX,xvr),e(VM,kvr),e(z,Rvr),e(z,zM),e(zM,gbe),e(gbe,Svr),e(zM,Pvr),e(zM,lX),e(lX,$vr),e(zM,Ivr),e(z,Dvr),e(z,WM),e(WM,hbe),e(hbe,jvr),e(WM,Nvr),e(WM,iX),e(iX,qvr),e(WM,Gvr),e(z,Ovr),e(z,QM),e(QM,ube),e(ube,Xvr),e(QM,Vvr),e(QM,dX),e(dX,zvr),e(QM,Wvr),e(z,Qvr),e(z,HM),e(HM,pbe),e(pbe,Hvr),e(HM,Uvr),e(HM,cX),e(cX,Jvr),e(HM,Yvr),e(wo,Kvr),e(wo,_be),e(_be,Zvr),e(wo,eTr),g(j0,wo,null),b(c,HBe,_),b(c,Xc,_),e(Xc,UM),e(UM,bbe),g(N0,bbe,null),e(Xc,oTr),e(Xc,vbe),e(vbe,rTr),b(c,UBe,_),b(c,Br,_),g(q0,Br,null),e(Br,tTr),e(Br,Vc),e(Vc,aTr),e(Vc,Tbe),e(Tbe,sTr),e(Vc,nTr),e(Vc,Fbe),e(Fbe,lTr),e(Vc,iTr),e(Br,dTr),e(Br,G0),e(G0,cTr),e(G0,Cbe),e(Cbe,mTr),e(G0,fTr),e(Br,gTr),e(Br,Ct),g(O0,Ct,null),e(Ct,hTr),e(Ct,Mbe),e(Mbe,uTr),e(Ct,pTr),e(Ct,zc),e(zc,_Tr),e(zc,Ebe),e(Ebe,bTr),e(zc,vTr),e(zc,ybe),e(ybe,TTr),e(zc,FTr),e(Ct,CTr),e(Ct,wbe),e(wbe,MTr),e(Ct,ETr),g(X0,Ct,null),e(Br,yTr),e(Br,Ao),g(V0,Ao,null),e(Ao,wTr),e(Ao,Abe),e(Abe,ATr),e(Ao,LTr),e(Ao,Ms),e(Ms,BTr),e(Ms,Lbe),e(Lbe,xTr),e(Ms,kTr),e(Ms,Bbe),e(Bbe,RTr),e(Ms,STr),e(Ms,xbe),e(xbe,PTr),e(Ms,$Tr),e(Ao,ITr),e(Ao,Es),e(Es,JM),e(JM,kbe),e(kbe,DTr),e(JM,jTr),e(JM,mX),e(mX,NTr),e(JM,qTr),e(Es,GTr),e(Es,YM),e(YM,Rbe),e(Rbe,OTr),e(YM,XTr),e(YM,fX),e(fX,VTr),e(YM,zTr),e(Es,WTr),e(Es,KM),e(KM,Sbe),e(Sbe,QTr),e(KM,HTr),e(KM,gX),e(gX,UTr),e(KM,JTr),e(Es,YTr),e(Es,ZM),e(ZM,Pbe),e(Pbe,KTr),e(ZM,ZTr),e(ZM,hX),e(hX,e1r),e(ZM,o1r),e(Ao,r1r),e(Ao,$be),e($be,t1r),e(Ao,a1r),g(z0,Ao,null),b(c,JBe,_),b(c,Wc,_),e(Wc,eE),e(eE,Ibe),g(W0,Ibe,null),e(Wc,s1r),e(Wc,Dbe),e(Dbe,n1r),b(c,YBe,_),b(c,xr,_),g(Q0,xr,null),e(xr,l1r),e(xr,Qc),e(Qc,i1r),e(Qc,jbe),e(jbe,d1r),e(Qc,c1r),e(Qc,Nbe),e(Nbe,m1r),e(Qc,f1r),e(xr,g1r),e(xr,H0),e(H0,h1r),e(H0,qbe),e(qbe,u1r),e(H0,p1r),e(xr,_1r),e(xr,Mt),g(U0,Mt,null),e(Mt,b1r),e(Mt,Gbe),e(Gbe,v1r),e(Mt,T1r),e(Mt,Hc),e(Hc,F1r),e(Hc,Obe),e(Obe,C1r),e(Hc,M1r),e(Hc,Xbe),e(Xbe,E1r),e(Hc,y1r),e(Mt,w1r),e(Mt,Vbe),e(Vbe,A1r),e(Mt,L1r),g(J0,Mt,null),e(xr,B1r),e(xr,Lo),g(Y0,Lo,null),e(Lo,x1r),e(Lo,zbe),e(zbe,k1r),e(Lo,R1r),e(Lo,ys),e(ys,S1r),e(ys,Wbe),e(Wbe,P1r),e(ys,$1r),e(ys,Qbe),e(Qbe,I1r),e(ys,D1r),e(ys,Hbe),e(Hbe,j1r),e(ys,N1r),e(Lo,q1r),e(Lo,me),e(me,oE),e(oE,Ube),e(Ube,G1r),e(oE,O1r),e(oE,uX),e(uX,X1r),e(oE,V1r),e(me,z1r),e(me,rE),e(rE,Jbe),e(Jbe,W1r),e(rE,Q1r),e(rE,pX),e(pX,H1r),e(rE,U1r),e(me,J1r),e(me,tE),e(tE,Ybe),e(Ybe,Y1r),e(tE,K1r),e(tE,_X),e(_X,Z1r),e(tE,eFr),e(me,oFr),e(me,aE),e(aE,Kbe),e(Kbe,rFr),e(aE,tFr),e(aE,bX),e(bX,aFr),e(aE,sFr),e(me,nFr),e(me,sE),e(sE,Zbe),e(Zbe,lFr),e(sE,iFr),e(sE,vX),e(vX,dFr),e(sE,cFr),e(me,mFr),e(me,nE),e(nE,e2e),e(e2e,fFr),e(nE,gFr),e(nE,TX),e(TX,hFr),e(nE,uFr),e(me,pFr),e(me,lE),e(lE,o2e),e(o2e,_Fr),e(lE,bFr),e(lE,FX),e(FX,vFr),e(lE,TFr),e(me,FFr),e(me,iE),e(iE,r2e),e(r2e,CFr),e(iE,MFr),e(iE,CX),e(CX,EFr),e(iE,yFr),e(me,wFr),e(me,dE),e(dE,t2e),e(t2e,AFr),e(dE,LFr),e(dE,MX),e(MX,BFr),e(dE,xFr),e(me,kFr),e(me,cE),e(cE,a2e),e(a2e,RFr),e(cE,SFr),e(cE,EX),e(EX,PFr),e(cE,$Fr),e(me,IFr),e(me,mE),e(mE,s2e),e(s2e,DFr),e(mE,jFr),e(mE,yX),e(yX,NFr),e(mE,qFr),e(Lo,GFr),e(Lo,n2e),e(n2e,OFr),e(Lo,XFr),g(K0,Lo,null),b(c,KBe,_),b(c,Uc,_),e(Uc,fE),e(fE,l2e),g(Z0,l2e,null),e(Uc,VFr),e(Uc,i2e),e(i2e,zFr),b(c,ZBe,_),b(c,kr,_),g(eL,kr,null),e(kr,WFr),e(kr,Jc),e(Jc,QFr),e(Jc,d2e),e(d2e,HFr),e(Jc,UFr),e(Jc,c2e),e(c2e,JFr),e(Jc,YFr),e(kr,KFr),e(kr,oL),e(oL,ZFr),e(oL,m2e),e(m2e,eCr),e(oL,oCr),e(kr,rCr),e(kr,Et),g(rL,Et,null),e(Et,tCr),e(Et,f2e),e(f2e,aCr),e(Et,sCr),e(Et,Yc),e(Yc,nCr),e(Yc,g2e),e(g2e,lCr),e(Yc,iCr),e(Yc,h2e),e(h2e,dCr),e(Yc,cCr),e(Et,mCr),e(Et,u2e),e(u2e,fCr),e(Et,gCr),g(tL,Et,null),e(kr,hCr),e(kr,Bo),g(aL,Bo,null),e(Bo,uCr),e(Bo,p2e),e(p2e,pCr),e(Bo,_Cr),e(Bo,ws),e(ws,bCr),e(ws,_2e),e(_2e,vCr),e(ws,TCr),e(ws,b2e),e(b2e,FCr),e(ws,CCr),e(ws,v2e),e(v2e,MCr),e(ws,ECr),e(Bo,yCr),e(Bo,ve),e(ve,gE),e(gE,T2e),e(T2e,wCr),e(gE,ACr),e(gE,wX),e(wX,LCr),e(gE,BCr),e(ve,xCr),e(ve,hE),e(hE,F2e),e(F2e,kCr),e(hE,RCr),e(hE,AX),e(AX,SCr),e(hE,PCr),e(ve,$Cr),e(ve,uE),e(uE,C2e),e(C2e,ICr),e(uE,DCr),e(uE,LX),e(LX,jCr),e(uE,NCr),e(ve,qCr),e(ve,pE),e(pE,M2e),e(M2e,GCr),e(pE,OCr),e(pE,BX),e(BX,XCr),e(pE,VCr),e(ve,zCr),e(ve,_E),e(_E,E2e),e(E2e,WCr),e(_E,QCr),e(_E,xX),e(xX,HCr),e(_E,UCr),e(ve,JCr),e(ve,bE),e(bE,y2e),e(y2e,YCr),e(bE,KCr),e(bE,kX),e(kX,ZCr),e(bE,e4r),e(ve,o4r),e(ve,vE),e(vE,w2e),e(w2e,r4r),e(vE,t4r),e(vE,RX),e(RX,a4r),e(vE,s4r),e(ve,n4r),e(ve,TE),e(TE,A2e),e(A2e,l4r),e(TE,i4r),e(TE,SX),e(SX,d4r),e(TE,c4r),e(ve,m4r),e(ve,FE),e(FE,L2e),e(L2e,f4r),e(FE,g4r),e(FE,PX),e(PX,h4r),e(FE,u4r),e(Bo,p4r),e(Bo,B2e),e(B2e,_4r),e(Bo,b4r),g(sL,Bo,null),b(c,exe,_),b(c,Kc,_),e(Kc,CE),e(CE,x2e),g(nL,x2e,null),e(Kc,v4r),e(Kc,k2e),e(k2e,T4r),b(c,oxe,_),b(c,Rr,_),g(lL,Rr,null),e(Rr,F4r),e(Rr,Zc),e(Zc,C4r),e(Zc,R2e),e(R2e,M4r),e(Zc,E4r),e(Zc,S2e),e(S2e,y4r),e(Zc,w4r),e(Rr,A4r),e(Rr,iL),e(iL,L4r),e(iL,P2e),e(P2e,B4r),e(iL,x4r),e(Rr,k4r),e(Rr,yt),g(dL,yt,null),e(yt,R4r),e(yt,$2e),e($2e,S4r),e(yt,P4r),e(yt,em),e(em,$4r),e(em,I2e),e(I2e,I4r),e(em,D4r),e(em,D2e),e(D2e,j4r),e(em,N4r),e(yt,q4r),e(yt,j2e),e(j2e,G4r),e(yt,O4r),g(cL,yt,null),e(Rr,X4r),e(Rr,xo),g(mL,xo,null),e(xo,V4r),e(xo,N2e),e(N2e,z4r),e(xo,W4r),e(xo,As),e(As,Q4r),e(As,q2e),e(q2e,H4r),e(As,U4r),e(As,G2e),e(G2e,J4r),e(As,Y4r),e(As,O2e),e(O2e,K4r),e(As,Z4r),e(xo,eMr),e(xo,Te),e(Te,ME),e(ME,X2e),e(X2e,oMr),e(ME,rMr),e(ME,$X),e($X,tMr),e(ME,aMr),e(Te,sMr),e(Te,EE),e(EE,V2e),e(V2e,nMr),e(EE,lMr),e(EE,IX),e(IX,iMr),e(EE,dMr),e(Te,cMr),e(Te,yE),e(yE,z2e),e(z2e,mMr),e(yE,fMr),e(yE,DX),e(DX,gMr),e(yE,hMr),e(Te,uMr),e(Te,wE),e(wE,W2e),e(W2e,pMr),e(wE,_Mr),e(wE,jX),e(jX,bMr),e(wE,vMr),e(Te,TMr),e(Te,AE),e(AE,Q2e),e(Q2e,FMr),e(AE,CMr),e(AE,NX),e(NX,MMr),e(AE,EMr),e(Te,yMr),e(Te,LE),e(LE,H2e),e(H2e,wMr),e(LE,AMr),e(LE,qX),e(qX,LMr),e(LE,BMr),e(Te,xMr),e(Te,BE),e(BE,U2e),e(U2e,kMr),e(BE,RMr),e(BE,GX),e(GX,SMr),e(BE,PMr),e(Te,$Mr),e(Te,xE),e(xE,J2e),e(J2e,IMr),e(xE,DMr),e(xE,OX),e(OX,jMr),e(xE,NMr),e(Te,qMr),e(Te,kE),e(kE,Y2e),e(Y2e,GMr),e(kE,OMr),e(kE,XX),e(XX,XMr),e(kE,VMr),e(xo,zMr),e(xo,K2e),e(K2e,WMr),e(xo,QMr),g(fL,xo,null),b(c,rxe,_),b(c,om,_),e(om,RE),e(RE,Z2e),g(gL,Z2e,null),e(om,HMr),e(om,eve),e(eve,UMr),b(c,txe,_),b(c,Sr,_),g(hL,Sr,null),e(Sr,JMr),e(Sr,rm),e(rm,YMr),e(rm,ove),e(ove,KMr),e(rm,ZMr),e(rm,rve),e(rve,eEr),e(rm,oEr),e(Sr,rEr),e(Sr,uL),e(uL,tEr),e(uL,tve),e(tve,aEr),e(uL,sEr),e(Sr,nEr),e(Sr,wt),g(pL,wt,null),e(wt,lEr),e(wt,ave),e(ave,iEr),e(wt,dEr),e(wt,tm),e(tm,cEr),e(tm,sve),e(sve,mEr),e(tm,fEr),e(tm,nve),e(nve,gEr),e(tm,hEr),e(wt,uEr),e(wt,lve),e(lve,pEr),e(wt,_Er),g(_L,wt,null),e(Sr,bEr),e(Sr,ko),g(bL,ko,null),e(ko,vEr),e(ko,ive),e(ive,TEr),e(ko,FEr),e(ko,Ls),e(Ls,CEr),e(Ls,dve),e(dve,MEr),e(Ls,EEr),e(Ls,cve),e(cve,yEr),e(Ls,wEr),e(Ls,mve),e(mve,AEr),e(Ls,LEr),e(ko,BEr),e(ko,Fe),e(Fe,SE),e(SE,fve),e(fve,xEr),e(SE,kEr),e(SE,VX),e(VX,REr),e(SE,SEr),e(Fe,PEr),e(Fe,PE),e(PE,gve),e(gve,$Er),e(PE,IEr),e(PE,zX),e(zX,DEr),e(PE,jEr),e(Fe,NEr),e(Fe,$E),e($E,hve),e(hve,qEr),e($E,GEr),e($E,WX),e(WX,OEr),e($E,XEr),e(Fe,VEr),e(Fe,IE),e(IE,uve),e(uve,zEr),e(IE,WEr),e(IE,QX),e(QX,QEr),e(IE,HEr),e(Fe,UEr),e(Fe,DE),e(DE,pve),e(pve,JEr),e(DE,YEr),e(DE,HX),e(HX,KEr),e(DE,ZEr),e(Fe,e3r),e(Fe,jE),e(jE,_ve),e(_ve,o3r),e(jE,r3r),e(jE,UX),e(UX,t3r),e(jE,a3r),e(Fe,s3r),e(Fe,NE),e(NE,bve),e(bve,n3r),e(NE,l3r),e(NE,JX),e(JX,i3r),e(NE,d3r),e(Fe,c3r),e(Fe,qE),e(qE,vve),e(vve,m3r),e(qE,f3r),e(qE,YX),e(YX,g3r),e(qE,h3r),e(Fe,u3r),e(Fe,GE),e(GE,Tve),e(Tve,p3r),e(GE,_3r),e(GE,KX),e(KX,b3r),e(GE,v3r),e(ko,T3r),e(ko,Fve),e(Fve,F3r),e(ko,C3r),g(vL,ko,null),b(c,axe,_),b(c,am,_),e(am,OE),e(OE,Cve),g(TL,Cve,null),e(am,M3r),e(am,Mve),e(Mve,E3r),b(c,sxe,_),b(c,Pr,_),g(FL,Pr,null),e(Pr,y3r),e(Pr,sm),e(sm,w3r),e(sm,Eve),e(Eve,A3r),e(sm,L3r),e(sm,yve),e(yve,B3r),e(sm,x3r),e(Pr,k3r),e(Pr,CL),e(CL,R3r),e(CL,wve),e(wve,S3r),e(CL,P3r),e(Pr,$3r),e(Pr,At),g(ML,At,null),e(At,I3r),e(At,Ave),e(Ave,D3r),e(At,j3r),e(At,nm),e(nm,N3r),e(nm,Lve),e(Lve,q3r),e(nm,G3r),e(nm,Bve),e(Bve,O3r),e(nm,X3r),e(At,V3r),e(At,xve),e(xve,z3r),e(At,W3r),g(EL,At,null),e(Pr,Q3r),e(Pr,Ro),g(yL,Ro,null),e(Ro,H3r),e(Ro,kve),e(kve,U3r),e(Ro,J3r),e(Ro,Bs),e(Bs,Y3r),e(Bs,Rve),e(Rve,K3r),e(Bs,Z3r),e(Bs,Sve),e(Sve,e5r),e(Bs,o5r),e(Bs,Pve),e(Pve,r5r),e(Bs,t5r),e(Ro,a5r),e(Ro,Ce),e(Ce,XE),e(XE,$ve),e($ve,s5r),e(XE,n5r),e(XE,ZX),e(ZX,l5r),e(XE,i5r),e(Ce,d5r),e(Ce,VE),e(VE,Ive),e(Ive,c5r),e(VE,m5r),e(VE,eV),e(eV,f5r),e(VE,g5r),e(Ce,h5r),e(Ce,zE),e(zE,Dve),e(Dve,u5r),e(zE,p5r),e(zE,oV),e(oV,_5r),e(zE,b5r),e(Ce,v5r),e(Ce,WE),e(WE,jve),e(jve,T5r),e(WE,F5r),e(WE,rV),e(rV,C5r),e(WE,M5r),e(Ce,E5r),e(Ce,QE),e(QE,Nve),e(Nve,y5r),e(QE,w5r),e(QE,tV),e(tV,A5r),e(QE,L5r),e(Ce,B5r),e(Ce,HE),e(HE,qve),e(qve,x5r),e(HE,k5r),e(HE,aV),e(aV,R5r),e(HE,S5r),e(Ce,P5r),e(Ce,UE),e(UE,Gve),e(Gve,$5r),e(UE,I5r),e(UE,sV),e(sV,D5r),e(UE,j5r),e(Ce,N5r),e(Ce,JE),e(JE,Ove),e(Ove,q5r),e(JE,G5r),e(JE,nV),e(nV,O5r),e(JE,X5r),e(Ce,V5r),e(Ce,YE),e(YE,Xve),e(Xve,z5r),e(YE,W5r),e(YE,lV),e(lV,Q5r),e(YE,H5r),e(Ro,U5r),e(Ro,Vve),e(Vve,J5r),e(Ro,Y5r),g(wL,Ro,null),b(c,nxe,_),b(c,lm,_),e(lm,KE),e(KE,zve),g(AL,zve,null),e(lm,K5r),e(lm,Wve),e(Wve,Z5r),b(c,lxe,_),b(c,$r,_),g(LL,$r,null),e($r,eyr),e($r,im),e(im,oyr),e(im,Qve),e(Qve,ryr),e(im,tyr),e(im,Hve),e(Hve,ayr),e(im,syr),e($r,nyr),e($r,BL),e(BL,lyr),e(BL,Uve),e(Uve,iyr),e(BL,dyr),e($r,cyr),e($r,Lt),g(xL,Lt,null),e(Lt,myr),e(Lt,Jve),e(Jve,fyr),e(Lt,gyr),e(Lt,dm),e(dm,hyr),e(dm,Yve),e(Yve,uyr),e(dm,pyr),e(dm,Kve),e(Kve,_yr),e(dm,byr),e(Lt,vyr),e(Lt,Zve),e(Zve,Tyr),e(Lt,Fyr),g(kL,Lt,null),e($r,Cyr),e($r,So),g(RL,So,null),e(So,Myr),e(So,eTe),e(eTe,Eyr),e(So,yyr),e(So,xs),e(xs,wyr),e(xs,oTe),e(oTe,Ayr),e(xs,Lyr),e(xs,rTe),e(rTe,Byr),e(xs,xyr),e(xs,tTe),e(tTe,kyr),e(xs,Ryr),e(So,Syr),e(So,no),e(no,ZE),e(ZE,aTe),e(aTe,Pyr),e(ZE,$yr),e(ZE,iV),e(iV,Iyr),e(ZE,Dyr),e(no,jyr),e(no,e3),e(e3,sTe),e(sTe,Nyr),e(e3,qyr),e(e3,dV),e(dV,Gyr),e(e3,Oyr),e(no,Xyr),e(no,o3),e(o3,nTe),e(nTe,Vyr),e(o3,zyr),e(o3,cV),e(cV,Wyr),e(o3,Qyr),e(no,Hyr),e(no,r3),e(r3,lTe),e(lTe,Uyr),e(r3,Jyr),e(r3,mV),e(mV,Yyr),e(r3,Kyr),e(no,Zyr),e(no,t3),e(t3,iTe),e(iTe,ewr),e(t3,owr),e(t3,fV),e(fV,rwr),e(t3,twr),e(no,awr),e(no,a3),e(a3,dTe),e(dTe,swr),e(a3,nwr),e(a3,gV),e(gV,lwr),e(a3,iwr),e(no,dwr),e(no,s3),e(s3,cTe),e(cTe,cwr),e(s3,mwr),e(s3,hV),e(hV,fwr),e(s3,gwr),e(So,hwr),e(So,mTe),e(mTe,uwr),e(So,pwr),g(SL,So,null),b(c,ixe,_),b(c,cm,_),e(cm,n3),e(n3,fTe),g(PL,fTe,null),e(cm,_wr),e(cm,gTe),e(gTe,bwr),b(c,dxe,_),b(c,Ir,_),g($L,Ir,null),e(Ir,vwr),e(Ir,mm),e(mm,Twr),e(mm,hTe),e(hTe,Fwr),e(mm,Cwr),e(mm,uTe),e(uTe,Mwr),e(mm,Ewr),e(Ir,ywr),e(Ir,IL),e(IL,wwr),e(IL,pTe),e(pTe,Awr),e(IL,Lwr),e(Ir,Bwr),e(Ir,Bt),g(DL,Bt,null),e(Bt,xwr),e(Bt,_Te),e(_Te,kwr),e(Bt,Rwr),e(Bt,fm),e(fm,Swr),e(fm,bTe),e(bTe,Pwr),e(fm,$wr),e(fm,vTe),e(vTe,Iwr),e(fm,Dwr),e(Bt,jwr),e(Bt,TTe),e(TTe,Nwr),e(Bt,qwr),g(jL,Bt,null),e(Ir,Gwr),e(Ir,Po),g(NL,Po,null),e(Po,Owr),e(Po,FTe),e(FTe,Xwr),e(Po,Vwr),e(Po,ks),e(ks,zwr),e(ks,CTe),e(CTe,Wwr),e(ks,Qwr),e(ks,MTe),e(MTe,Hwr),e(ks,Uwr),e(ks,ETe),e(ETe,Jwr),e(ks,Ywr),e(Po,Kwr),e(Po,lo),e(lo,l3),e(l3,yTe),e(yTe,Zwr),e(l3,e6r),e(l3,uV),e(uV,o6r),e(l3,r6r),e(lo,t6r),e(lo,i3),e(i3,wTe),e(wTe,a6r),e(i3,s6r),e(i3,pV),e(pV,n6r),e(i3,l6r),e(lo,i6r),e(lo,d3),e(d3,ATe),e(ATe,d6r),e(d3,c6r),e(d3,_V),e(_V,m6r),e(d3,f6r),e(lo,g6r),e(lo,c3),e(c3,LTe),e(LTe,h6r),e(c3,u6r),e(c3,bV),e(bV,p6r),e(c3,_6r),e(lo,b6r),e(lo,m3),e(m3,BTe),e(BTe,v6r),e(m3,T6r),e(m3,vV),e(vV,F6r),e(m3,C6r),e(lo,M6r),e(lo,f3),e(f3,xTe),e(xTe,E6r),e(f3,y6r),e(f3,TV),e(TV,w6r),e(f3,A6r),e(lo,L6r),e(lo,g3),e(g3,kTe),e(kTe,B6r),e(g3,x6r),e(g3,FV),e(FV,k6r),e(g3,R6r),e(Po,S6r),e(Po,RTe),e(RTe,P6r),e(Po,$6r),g(qL,Po,null),b(c,cxe,_),b(c,gm,_),e(gm,h3),e(h3,STe),g(GL,STe,null),e(gm,I6r),e(gm,PTe),e(PTe,D6r),b(c,mxe,_),b(c,Dr,_),g(OL,Dr,null),e(Dr,j6r),e(Dr,hm),e(hm,N6r),e(hm,$Te),e($Te,q6r),e(hm,G6r),e(hm,ITe),e(ITe,O6r),e(hm,X6r),e(Dr,V6r),e(Dr,XL),e(XL,z6r),e(XL,DTe),e(DTe,W6r),e(XL,Q6r),e(Dr,H6r),e(Dr,xt),g(VL,xt,null),e(xt,U6r),e(xt,jTe),e(jTe,J6r),e(xt,Y6r),e(xt,um),e(um,K6r),e(um,NTe),e(NTe,Z6r),e(um,eAr),e(um,qTe),e(qTe,oAr),e(um,rAr),e(xt,tAr),e(xt,GTe),e(GTe,aAr),e(xt,sAr),g(zL,xt,null),e(Dr,nAr),e(Dr,$o),g(WL,$o,null),e($o,lAr),e($o,OTe),e(OTe,iAr),e($o,dAr),e($o,Rs),e(Rs,cAr),e(Rs,XTe),e(XTe,mAr),e(Rs,fAr),e(Rs,VTe),e(VTe,gAr),e(Rs,hAr),e(Rs,zTe),e(zTe,uAr),e(Rs,pAr),e($o,_Ar),e($o,WTe),e(WTe,u3),e(u3,QTe),e(QTe,bAr),e(u3,vAr),e(u3,CV),e(CV,TAr),e(u3,FAr),e($o,CAr),e($o,HTe),e(HTe,MAr),e($o,EAr),g(QL,$o,null),b(c,fxe,_),b(c,pm,_),e(pm,p3),e(p3,UTe),g(HL,UTe,null),e(pm,yAr),e(pm,JTe),e(JTe,wAr),b(c,gxe,_),b(c,jr,_),g(UL,jr,null),e(jr,AAr),e(jr,_m),e(_m,LAr),e(_m,YTe),e(YTe,BAr),e(_m,xAr),e(_m,KTe),e(KTe,kAr),e(_m,RAr),e(jr,SAr),e(jr,JL),e(JL,PAr),e(JL,ZTe),e(ZTe,$Ar),e(JL,IAr),e(jr,DAr),e(jr,kt),g(YL,kt,null),e(kt,jAr),e(kt,e1e),e(e1e,NAr),e(kt,qAr),e(kt,bm),e(bm,GAr),e(bm,o1e),e(o1e,OAr),e(bm,XAr),e(bm,r1e),e(r1e,VAr),e(bm,zAr),e(kt,WAr),e(kt,t1e),e(t1e,QAr),e(kt,HAr),g(KL,kt,null),e(jr,UAr),e(jr,Io),g(ZL,Io,null),e(Io,JAr),e(Io,a1e),e(a1e,YAr),e(Io,KAr),e(Io,Ss),e(Ss,ZAr),e(Ss,s1e),e(s1e,e0r),e(Ss,o0r),e(Ss,n1e),e(n1e,r0r),e(Ss,t0r),e(Ss,l1e),e(l1e,a0r),e(Ss,s0r),e(Io,n0r),e(Io,e8),e(e8,_3),e(_3,i1e),e(i1e,l0r),e(_3,i0r),e(_3,MV),e(MV,d0r),e(_3,c0r),e(e8,m0r),e(e8,b3),e(b3,d1e),e(d1e,f0r),e(b3,g0r),e(b3,EV),e(EV,h0r),e(b3,u0r),e(Io,p0r),e(Io,c1e),e(c1e,_0r),e(Io,b0r),g(o8,Io,null),b(c,hxe,_),b(c,vm,_),e(vm,v3),e(v3,m1e),g(r8,m1e,null),e(vm,v0r),e(vm,f1e),e(f1e,T0r),b(c,uxe,_),b(c,Nr,_),g(t8,Nr,null),e(Nr,F0r),e(Nr,Tm),e(Tm,C0r),e(Tm,g1e),e(g1e,M0r),e(Tm,E0r),e(Tm,h1e),e(h1e,y0r),e(Tm,w0r),e(Nr,A0r),e(Nr,a8),e(a8,L0r),e(a8,u1e),e(u1e,B0r),e(a8,x0r),e(Nr,k0r),e(Nr,Rt),g(s8,Rt,null),e(Rt,R0r),e(Rt,p1e),e(p1e,S0r),e(Rt,P0r),e(Rt,Fm),e(Fm,$0r),e(Fm,_1e),e(_1e,I0r),e(Fm,D0r),e(Fm,b1e),e(b1e,j0r),e(Fm,N0r),e(Rt,q0r),e(Rt,v1e),e(v1e,G0r),e(Rt,O0r),g(n8,Rt,null),e(Nr,X0r),e(Nr,Do),g(l8,Do,null),e(Do,V0r),e(Do,T1e),e(T1e,z0r),e(Do,W0r),e(Do,Ps),e(Ps,Q0r),e(Ps,F1e),e(F1e,H0r),e(Ps,U0r),e(Ps,C1e),e(C1e,J0r),e(Ps,Y0r),e(Ps,M1e),e(M1e,K0r),e(Ps,Z0r),e(Do,eLr),e(Do,E1e),e(E1e,T3),e(T3,y1e),e(y1e,oLr),e(T3,rLr),e(T3,yV),e(yV,tLr),e(T3,aLr),e(Do,sLr),e(Do,w1e),e(w1e,nLr),e(Do,lLr),g(i8,Do,null),pxe=!0},p(c,[_]){const d8={};_&2&&(d8.$$scope={dirty:_,ctx:c}),Lm.$set(d8);const A1e={};_&2&&(A1e.$$scope={dirty:_,ctx:c}),ch.$set(A1e);const L1e={};_&2&&(L1e.$$scope={dirty:_,ctx:c}),Fh.$set(L1e)},i(c){pxe||(h(ce.$$.fragment,c),h($a.$$.fragment,c),h(E5.$$.fragment,c),h(y5.$$.fragment,c),h(Lm.$$.fragment,c),h(w5.$$.fragment,c),h(A5.$$.fragment,c),h(x5.$$.fragment,c),h(k5.$$.fragment,c),h(R5.$$.fragment,c),h(S5.$$.fragment,c),h(P5.$$.fragment,c),h(D5.$$.fragment,c),h(j5.$$.fragment,c),h(N5.$$.fragment,c),h(q5.$$.fragment,c),h(G5.$$.fragment,c),h(V5.$$.fragment,c),h(ch.$$.fragment,c),h(z5.$$.fragment,c),h(W5.$$.fragment,c),h(Q5.$$.fragment,c),h(H5.$$.fragment,c),h(Y5.$$.fragment,c),h(Fh.$$.fragment,c),h(K5.$$.fragment,c),h(Z5.$$.fragment,c),h(ey.$$.fragment,c),h(oy.$$.fragment,c),h(ty.$$.fragment,c),h(ay.$$.fragment,c),h(sy.$$.fragment,c),h(ny.$$.fragment,c),h(ly.$$.fragment,c),h(iy.$$.fragment,c),h(cy.$$.fragment,c),h(my.$$.fragment,c),h(fy.$$.fragment,c),h(gy.$$.fragment,c),h(hy.$$.fragment,c),h(uy.$$.fragment,c),h(_y.$$.fragment,c),h(by.$$.fragment,c),h(vy.$$.fragment,c),h(Ty.$$.fragment,c),h(Fy.$$.fragment,c),h(Cy.$$.fragment,c),h(Ey.$$.fragment,c),h(yy.$$.fragment,c),h(wy.$$.fragment,c),h(Ay.$$.fragment,c),h(Ly.$$.fragment,c),h(By.$$.fragment,c),h(ky.$$.fragment,c),h(Ry.$$.fragment,c),h(Sy.$$.fragment,c),h(Py.$$.fragment,c),h($y.$$.fragment,c),h(Iy.$$.fragment,c),h(jy.$$.fragment,c),h(Ny.$$.fragment,c),h(qy.$$.fragment,c),h(Gy.$$.fragment,c),h(Oy.$$.fragment,c),h(Xy.$$.fragment,c),h(zy.$$.fragment,c),h(Wy.$$.fragment,c),h(Qy.$$.fragment,c),h(Hy.$$.fragment,c),h(Uy.$$.fragment,c),h(Jy.$$.fragment,c),h(Ky.$$.fragment,c),h(Zy.$$.fragment,c),h(ew.$$.fragment,c),h(ow.$$.fragment,c),h(rw.$$.fragment,c),h(tw.$$.fragment,c),h(sw.$$.fragment,c),h(nw.$$.fragment,c),h(lw.$$.fragment,c),h(iw.$$.fragment,c),h(dw.$$.fragment,c),h(cw.$$.fragment,c),h(fw.$$.fragment,c),h(gw.$$.fragment,c),h(hw.$$.fragment,c),h(uw.$$.fragment,c),h(pw.$$.fragment,c),h(_w.$$.fragment,c),h(vw.$$.fragment,c),h(Tw.$$.fragment,c),h(Fw.$$.fragment,c),h(Cw.$$.fragment,c),h(Mw.$$.fragment,c),h(Ew.$$.fragment,c),h(ww.$$.fragment,c),h(Aw.$$.fragment,c),h(Lw.$$.fragment,c),h(Bw.$$.fragment,c),h(xw.$$.fragment,c),h(kw.$$.fragment,c),h(Sw.$$.fragment,c),h(Pw.$$.fragment,c),h($w.$$.fragment,c),h(Iw.$$.fragment,c),h(Dw.$$.fragment,c),h(jw.$$.fragment,c),h(qw.$$.fragment,c),h(Gw.$$.fragment,c),h(Ow.$$.fragment,c),h(Xw.$$.fragment,c),h(Vw.$$.fragment,c),h(zw.$$.fragment,c),h(Qw.$$.fragment,c),h(Hw.$$.fragment,c),h(Uw.$$.fragment,c),h(Jw.$$.fragment,c),h(Yw.$$.fragment,c),h(Kw.$$.fragment,c),h(e6.$$.fragment,c),h(o6.$$.fragment,c),h(r6.$$.fragment,c),h(t6.$$.fragment,c),h(a6.$$.fragment,c),h(s6.$$.fragment,c),h(l6.$$.fragment,c),h(i6.$$.fragment,c),h(d6.$$.fragment,c),h(m6.$$.fragment,c),h(f6.$$.fragment,c),h(g6.$$.fragment,c),h(u6.$$.fragment,c),h(p6.$$.fragment,c),h(_6.$$.fragment,c),h(b6.$$.fragment,c),h(v6.$$.fragment,c),h(T6.$$.fragment,c),h(C6.$$.fragment,c),h(M6.$$.fragment,c),h(E6.$$.fragment,c),h(y6.$$.fragment,c),h(w6.$$.fragment,c),h(A6.$$.fragment,c),h(B6.$$.fragment,c),h(x6.$$.fragment,c),h(k6.$$.fragment,c),h(R6.$$.fragment,c),h(S6.$$.fragment,c),h(P6.$$.fragment,c),h(I6.$$.fragment,c),h(D6.$$.fragment,c),h(j6.$$.fragment,c),h(N6.$$.fragment,c),h(q6.$$.fragment,c),h(G6.$$.fragment,c),h(X6.$$.fragment,c),h(V6.$$.fragment,c),h(z6.$$.fragment,c),h(Q6.$$.fragment,c),h(H6.$$.fragment,c),h(U6.$$.fragment,c),h(Y6.$$.fragment,c),h(K6.$$.fragment,c),h(Z6.$$.fragment,c),h(eA.$$.fragment,c),h(oA.$$.fragment,c),h(rA.$$.fragment,c),h(aA.$$.fragment,c),h(sA.$$.fragment,c),h(nA.$$.fragment,c),h(lA.$$.fragment,c),h(iA.$$.fragment,c),h(dA.$$.fragment,c),h(mA.$$.fragment,c),h(fA.$$.fragment,c),h(gA.$$.fragment,c),h(hA.$$.fragment,c),h(uA.$$.fragment,c),h(pA.$$.fragment,c),h(bA.$$.fragment,c),h(vA.$$.fragment,c),h(TA.$$.fragment,c),h(CA.$$.fragment,c),h(MA.$$.fragment,c),h(EA.$$.fragment,c),h(wA.$$.fragment,c),h(AA.$$.fragment,c),h(LA.$$.fragment,c),h(BA.$$.fragment,c),h(xA.$$.fragment,c),h(kA.$$.fragment,c),h(SA.$$.fragment,c),h(PA.$$.fragment,c),h($A.$$.fragment,c),h(IA.$$.fragment,c),h(DA.$$.fragment,c),h(jA.$$.fragment,c),h(qA.$$.fragment,c),h(GA.$$.fragment,c),h(OA.$$.fragment,c),h(XA.$$.fragment,c),h(VA.$$.fragment,c),h(zA.$$.fragment,c),h(QA.$$.fragment,c),h(HA.$$.fragment,c),h(UA.$$.fragment,c),h(JA.$$.fragment,c),h(YA.$$.fragment,c),h(KA.$$.fragment,c),h(e0.$$.fragment,c),h(o0.$$.fragment,c),h(r0.$$.fragment,c),h(t0.$$.fragment,c),h(a0.$$.fragment,c),h(s0.$$.fragment,c),h(l0.$$.fragment,c),h(i0.$$.fragment,c),h(d0.$$.fragment,c),h(c0.$$.fragment,c),h(m0.$$.fragment,c),h(f0.$$.fragment,c),h(h0.$$.fragment,c),h(u0.$$.fragment,c),h(p0.$$.fragment,c),h(_0.$$.fragment,c),h(b0.$$.fragment,c),h(v0.$$.fragment,c),h(F0.$$.fragment,c),h(C0.$$.fragment,c),h(M0.$$.fragment,c),h(E0.$$.fragment,c),h(y0.$$.fragment,c),h(w0.$$.fragment,c),h(L0.$$.fragment,c),h(B0.$$.fragment,c),h(x0.$$.fragment,c),h(k0.$$.fragment,c),h(R0.$$.fragment,c),h(S0.$$.fragment,c),h($0.$$.fragment,c),h(I0.$$.fragment,c),h(D0.$$.fragment,c),h(j0.$$.fragment,c),h(N0.$$.fragment,c),h(q0.$$.fragment,c),h(O0.$$.fragment,c),h(X0.$$.fragment,c),h(V0.$$.fragment,c),h(z0.$$.fragment,c),h(W0.$$.fragment,c),h(Q0.$$.fragment,c),h(U0.$$.fragment,c),h(J0.$$.fragment,c),h(Y0.$$.fragment,c),h(K0.$$.fragment,c),h(Z0.$$.fragment,c),h(eL.$$.fragment,c),h(rL.$$.fragment,c),h(tL.$$.fragment,c),h(aL.$$.fragment,c),h(sL.$$.fragment,c),h(nL.$$.fragment,c),h(lL.$$.fragment,c),h(dL.$$.fragment,c),h(cL.$$.fragment,c),h(mL.$$.fragment,c),h(fL.$$.fragment,c),h(gL.$$.fragment,c),h(hL.$$.fragment,c),h(pL.$$.fragment,c),h(_L.$$.fragment,c),h(bL.$$.fragment,c),h(vL.$$.fragment,c),h(TL.$$.fragment,c),h(FL.$$.fragment,c),h(ML.$$.fragment,c),h(EL.$$.fragment,c),h(yL.$$.fragment,c),h(wL.$$.fragment,c),h(AL.$$.fragment,c),h(LL.$$.fragment,c),h(xL.$$.fragment,c),h(kL.$$.fragment,c),h(RL.$$.fragment,c),h(SL.$$.fragment,c),h(PL.$$.fragment,c),h($L.$$.fragment,c),h(DL.$$.fragment,c),h(jL.$$.fragment,c),h(NL.$$.fragment,c),h(qL.$$.fragment,c),h(GL.$$.fragment,c),h(OL.$$.fragment,c),h(VL.$$.fragment,c),h(zL.$$.fragment,c),h(WL.$$.fragment,c),h(QL.$$.fragment,c),h(HL.$$.fragment,c),h(UL.$$.fragment,c),h(YL.$$.fragment,c),h(KL.$$.fragment,c),h(ZL.$$.fragment,c),h(o8.$$.fragment,c),h(r8.$$.fragment,c),h(t8.$$.fragment,c),h(s8.$$.fragment,c),h(n8.$$.fragment,c),h(l8.$$.fragment,c),h(i8.$$.fragment,c),pxe=!0)},o(c){u(ce.$$.fragment,c),u($a.$$.fragment,c),u(E5.$$.fragment,c),u(y5.$$.fragment,c),u(Lm.$$.fragment,c),u(w5.$$.fragment,c),u(A5.$$.fragment,c),u(x5.$$.fragment,c),u(k5.$$.fragment,c),u(R5.$$.fragment,c),u(S5.$$.fragment,c),u(P5.$$.fragment,c),u(D5.$$.fragment,c),u(j5.$$.fragment,c),u(N5.$$.fragment,c),u(q5.$$.fragment,c),u(G5.$$.fragment,c),u(V5.$$.fragment,c),u(ch.$$.fragment,c),u(z5.$$.fragment,c),u(W5.$$.fragment,c),u(Q5.$$.fragment,c),u(H5.$$.fragment,c),u(Y5.$$.fragment,c),u(Fh.$$.fragment,c),u(K5.$$.fragment,c),u(Z5.$$.fragment,c),u(ey.$$.fragment,c),u(oy.$$.fragment,c),u(ty.$$.fragment,c),u(ay.$$.fragment,c),u(sy.$$.fragment,c),u(ny.$$.fragment,c),u(ly.$$.fragment,c),u(iy.$$.fragment,c),u(cy.$$.fragment,c),u(my.$$.fragment,c),u(fy.$$.fragment,c),u(gy.$$.fragment,c),u(hy.$$.fragment,c),u(uy.$$.fragment,c),u(_y.$$.fragment,c),u(by.$$.fragment,c),u(vy.$$.fragment,c),u(Ty.$$.fragment,c),u(Fy.$$.fragment,c),u(Cy.$$.fragment,c),u(Ey.$$.fragment,c),u(yy.$$.fragment,c),u(wy.$$.fragment,c),u(Ay.$$.fragment,c),u(Ly.$$.fragment,c),u(By.$$.fragment,c),u(ky.$$.fragment,c),u(Ry.$$.fragment,c),u(Sy.$$.fragment,c),u(Py.$$.fragment,c),u($y.$$.fragment,c),u(Iy.$$.fragment,c),u(jy.$$.fragment,c),u(Ny.$$.fragment,c),u(qy.$$.fragment,c),u(Gy.$$.fragment,c),u(Oy.$$.fragment,c),u(Xy.$$.fragment,c),u(zy.$$.fragment,c),u(Wy.$$.fragment,c),u(Qy.$$.fragment,c),u(Hy.$$.fragment,c),u(Uy.$$.fragment,c),u(Jy.$$.fragment,c),u(Ky.$$.fragment,c),u(Zy.$$.fragment,c),u(ew.$$.fragment,c),u(ow.$$.fragment,c),u(rw.$$.fragment,c),u(tw.$$.fragment,c),u(sw.$$.fragment,c),u(nw.$$.fragment,c),u(lw.$$.fragment,c),u(iw.$$.fragment,c),u(dw.$$.fragment,c),u(cw.$$.fragment,c),u(fw.$$.fragment,c),u(gw.$$.fragment,c),u(hw.$$.fragment,c),u(uw.$$.fragment,c),u(pw.$$.fragment,c),u(_w.$$.fragment,c),u(vw.$$.fragment,c),u(Tw.$$.fragment,c),u(Fw.$$.fragment,c),u(Cw.$$.fragment,c),u(Mw.$$.fragment,c),u(Ew.$$.fragment,c),u(ww.$$.fragment,c),u(Aw.$$.fragment,c),u(Lw.$$.fragment,c),u(Bw.$$.fragment,c),u(xw.$$.fragment,c),u(kw.$$.fragment,c),u(Sw.$$.fragment,c),u(Pw.$$.fragment,c),u($w.$$.fragment,c),u(Iw.$$.fragment,c),u(Dw.$$.fragment,c),u(jw.$$.fragment,c),u(qw.$$.fragment,c),u(Gw.$$.fragment,c),u(Ow.$$.fragment,c),u(Xw.$$.fragment,c),u(Vw.$$.fragment,c),u(zw.$$.fragment,c),u(Qw.$$.fragment,c),u(Hw.$$.fragment,c),u(Uw.$$.fragment,c),u(Jw.$$.fragment,c),u(Yw.$$.fragment,c),u(Kw.$$.fragment,c),u(e6.$$.fragment,c),u(o6.$$.fragment,c),u(r6.$$.fragment,c),u(t6.$$.fragment,c),u(a6.$$.fragment,c),u(s6.$$.fragment,c),u(l6.$$.fragment,c),u(i6.$$.fragment,c),u(d6.$$.fragment,c),u(m6.$$.fragment,c),u(f6.$$.fragment,c),u(g6.$$.fragment,c),u(u6.$$.fragment,c),u(p6.$$.fragment,c),u(_6.$$.fragment,c),u(b6.$$.fragment,c),u(v6.$$.fragment,c),u(T6.$$.fragment,c),u(C6.$$.fragment,c),u(M6.$$.fragment,c),u(E6.$$.fragment,c),u(y6.$$.fragment,c),u(w6.$$.fragment,c),u(A6.$$.fragment,c),u(B6.$$.fragment,c),u(x6.$$.fragment,c),u(k6.$$.fragment,c),u(R6.$$.fragment,c),u(S6.$$.fragment,c),u(P6.$$.fragment,c),u(I6.$$.fragment,c),u(D6.$$.fragment,c),u(j6.$$.fragment,c),u(N6.$$.fragment,c),u(q6.$$.fragment,c),u(G6.$$.fragment,c),u(X6.$$.fragment,c),u(V6.$$.fragment,c),u(z6.$$.fragment,c),u(Q6.$$.fragment,c),u(H6.$$.fragment,c),u(U6.$$.fragment,c),u(Y6.$$.fragment,c),u(K6.$$.fragment,c),u(Z6.$$.fragment,c),u(eA.$$.fragment,c),u(oA.$$.fragment,c),u(rA.$$.fragment,c),u(aA.$$.fragment,c),u(sA.$$.fragment,c),u(nA.$$.fragment,c),u(lA.$$.fragment,c),u(iA.$$.fragment,c),u(dA.$$.fragment,c),u(mA.$$.fragment,c),u(fA.$$.fragment,c),u(gA.$$.fragment,c),u(hA.$$.fragment,c),u(uA.$$.fragment,c),u(pA.$$.fragment,c),u(bA.$$.fragment,c),u(vA.$$.fragment,c),u(TA.$$.fragment,c),u(CA.$$.fragment,c),u(MA.$$.fragment,c),u(EA.$$.fragment,c),u(wA.$$.fragment,c),u(AA.$$.fragment,c),u(LA.$$.fragment,c),u(BA.$$.fragment,c),u(xA.$$.fragment,c),u(kA.$$.fragment,c),u(SA.$$.fragment,c),u(PA.$$.fragment,c),u($A.$$.fragment,c),u(IA.$$.fragment,c),u(DA.$$.fragment,c),u(jA.$$.fragment,c),u(qA.$$.fragment,c),u(GA.$$.fragment,c),u(OA.$$.fragment,c),u(XA.$$.fragment,c),u(VA.$$.fragment,c),u(zA.$$.fragment,c),u(QA.$$.fragment,c),u(HA.$$.fragment,c),u(UA.$$.fragment,c),u(JA.$$.fragment,c),u(YA.$$.fragment,c),u(KA.$$.fragment,c),u(e0.$$.fragment,c),u(o0.$$.fragment,c),u(r0.$$.fragment,c),u(t0.$$.fragment,c),u(a0.$$.fragment,c),u(s0.$$.fragment,c),u(l0.$$.fragment,c),u(i0.$$.fragment,c),u(d0.$$.fragment,c),u(c0.$$.fragment,c),u(m0.$$.fragment,c),u(f0.$$.fragment,c),u(h0.$$.fragment,c),u(u0.$$.fragment,c),u(p0.$$.fragment,c),u(_0.$$.fragment,c),u(b0.$$.fragment,c),u(v0.$$.fragment,c),u(F0.$$.fragment,c),u(C0.$$.fragment,c),u(M0.$$.fragment,c),u(E0.$$.fragment,c),u(y0.$$.fragment,c),u(w0.$$.fragment,c),u(L0.$$.fragment,c),u(B0.$$.fragment,c),u(x0.$$.fragment,c),u(k0.$$.fragment,c),u(R0.$$.fragment,c),u(S0.$$.fragment,c),u($0.$$.fragment,c),u(I0.$$.fragment,c),u(D0.$$.fragment,c),u(j0.$$.fragment,c),u(N0.$$.fragment,c),u(q0.$$.fragment,c),u(O0.$$.fragment,c),u(X0.$$.fragment,c),u(V0.$$.fragment,c),u(z0.$$.fragment,c),u(W0.$$.fragment,c),u(Q0.$$.fragment,c),u(U0.$$.fragment,c),u(J0.$$.fragment,c),u(Y0.$$.fragment,c),u(K0.$$.fragment,c),u(Z0.$$.fragment,c),u(eL.$$.fragment,c),u(rL.$$.fragment,c),u(tL.$$.fragment,c),u(aL.$$.fragment,c),u(sL.$$.fragment,c),u(nL.$$.fragment,c),u(lL.$$.fragment,c),u(dL.$$.fragment,c),u(cL.$$.fragment,c),u(mL.$$.fragment,c),u(fL.$$.fragment,c),u(gL.$$.fragment,c),u(hL.$$.fragment,c),u(pL.$$.fragment,c),u(_L.$$.fragment,c),u(bL.$$.fragment,c),u(vL.$$.fragment,c),u(TL.$$.fragment,c),u(FL.$$.fragment,c),u(ML.$$.fragment,c),u(EL.$$.fragment,c),u(yL.$$.fragment,c),u(wL.$$.fragment,c),u(AL.$$.fragment,c),u(LL.$$.fragment,c),u(xL.$$.fragment,c),u(kL.$$.fragment,c),u(RL.$$.fragment,c),u(SL.$$.fragment,c),u(PL.$$.fragment,c),u($L.$$.fragment,c),u(DL.$$.fragment,c),u(jL.$$.fragment,c),u(NL.$$.fragment,c),u(qL.$$.fragment,c),u(GL.$$.fragment,c),u(OL.$$.fragment,c),u(VL.$$.fragment,c),u(zL.$$.fragment,c),u(WL.$$.fragment,c),u(QL.$$.fragment,c),u(HL.$$.fragment,c),u(UL.$$.fragment,c),u(YL.$$.fragment,c),u(KL.$$.fragment,c),u(ZL.$$.fragment,c),u(o8.$$.fragment,c),u(r8.$$.fragment,c),u(t8.$$.fragment,c),u(s8.$$.fragment,c),u(n8.$$.fragment,c),u(l8.$$.fragment,c),u(i8.$$.fragment,c),pxe=!1},d(c){t(J),c&&t(Be),c&&t(ie),p(ce),c&&t(Mm),c&&t(na),c&&t(ye),c&&t(io),c&&t(ym),p($a,c),c&&t(co),c&&t(ge),c&&t(Oo),c&&t(Ia),c&&t(_9e),c&&t($i),p(E5),c&&t(b9e),c&&t(Ns),c&&t(v9e),p(y5,c),c&&t(T9e),c&&t(c7),c&&t(F9e),p(Lm,c),c&&t(C9e),c&&t(Ii),p(w5),c&&t(M9e),c&&t(Xo),p(A5),p(x5),p(k5),p(R5),c&&t(E9e),c&&t(ji),p(S5),c&&t(y9e),c&&t(Vo),p(P5),p(D5),p(j5),p(N5),c&&t(w9e),c&&t(Ni),p(q5),c&&t(A9e),c&&t(zo),p(G5),p(V5),p(ch),p(z5),p(W5),c&&t(L9e),c&&t(qi),p(Q5),c&&t(B9e),c&&t(Wo),p(H5),p(Y5),p(Fh),p(K5),p(Z5),c&&t(x9e),c&&t(Oi),p(ey),c&&t(k9e),c&&t(Qo),p(oy),p(ty),p(ay),p(sy),p(ny),c&&t(R9e),c&&t(zi),p(ly),c&&t(S9e),c&&t(Ho),p(iy),p(cy),p(my),p(fy),p(gy),c&&t(P9e),c&&t(Hi),p(hy),c&&t($9e),c&&t(Uo),p(uy),p(_y),p(by),p(vy),p(Ty),c&&t(I9e),c&&t(Yi),p(Fy),c&&t(D9e),c&&t(Jo),p(Cy),p(Ey),p(yy),p(wy),p(Ay),c&&t(j9e),c&&t(ed),p(Ly),c&&t(N9e),c&&t(Yo),p(By),p(ky),p(Ry),p(Sy),p(Py),c&&t(q9e),c&&t(td),p($y),c&&t(G9e),c&&t(Ko),p(Iy),p(jy),p(Ny),p(qy),p(Gy),c&&t(O9e),c&&t(nd),p(Oy),c&&t(X9e),c&&t(Zo),p(Xy),p(zy),p(Wy),p(Qy),p(Hy),c&&t(V9e),c&&t(dd),p(Uy),c&&t(z9e),c&&t(er),p(Jy),p(Ky),p(Zy),p(ew),p(ow),c&&t(W9e),c&&t(fd),p(rw),c&&t(Q9e),c&&t(or),p(tw),p(sw),p(nw),p(lw),p(iw),c&&t(H9e),c&&t(ud),p(dw),c&&t(U9e),c&&t(rr),p(cw),p(fw),p(gw),p(hw),p(uw),c&&t(J9e),c&&t(bd),p(pw),c&&t(Y9e),c&&t(tr),p(_w),p(vw),p(Tw),p(Fw),p(Cw),c&&t(K9e),c&&t(Fd),p(Mw),c&&t(Z9e),c&&t(ar),p(Ew),p(ww),p(Aw),p(Lw),p(Bw),c&&t(eBe),c&&t(Ed),p(xw),c&&t(oBe),c&&t(sr),p(kw),p(Sw),p(Pw),p($w),p(Iw),c&&t(rBe),c&&t(Ad),p(Dw),c&&t(tBe),c&&t(nr),p(jw),p(qw),p(Gw),p(Ow),p(Xw),c&&t(aBe),c&&t(xd),p(Vw),c&&t(sBe),c&&t(lr),p(zw),p(Qw),p(Hw),p(Uw),p(Jw),c&&t(nBe),c&&t(Sd),p(Yw),c&&t(lBe),c&&t(ir),p(Kw),p(e6),p(o6),p(r6),p(t6),c&&t(iBe),c&&t(Id),p(a6),c&&t(dBe),c&&t(dr),p(s6),p(l6),p(i6),p(d6),p(m6),c&&t(cBe),c&&t(Nd),p(f6),c&&t(mBe),c&&t(cr),p(g6),p(u6),p(p6),p(_6),p(b6),c&&t(fBe),c&&t(Od),p(v6),c&&t(gBe),c&&t(mr),p(T6),p(C6),p(M6),p(E6),p(y6),c&&t(hBe),c&&t(Wd),p(w6),c&&t(uBe),c&&t(fr),p(A6),p(B6),p(x6),p(k6),p(R6),c&&t(pBe),c&&t(Ud),p(S6),c&&t(_Be),c&&t(gr),p(P6),p(I6),p(D6),p(j6),p(N6),c&&t(bBe),c&&t(Kd),p(q6),c&&t(vBe),c&&t(hr),p(G6),p(X6),p(V6),p(z6),p(Q6),c&&t(TBe),c&&t(oc),p(H6),c&&t(FBe),c&&t(ur),p(U6),p(Y6),p(K6),p(Z6),p(eA),c&&t(CBe),c&&t(ac),p(oA),c&&t(MBe),c&&t(pr),p(rA),p(aA),p(sA),p(nA),p(lA),c&&t(EBe),c&&t(lc),p(iA),c&&t(yBe),c&&t(_r),p(dA),p(mA),p(fA),p(gA),p(hA),c&&t(wBe),c&&t(cc),p(uA),c&&t(ABe),c&&t(br),p(pA),p(bA),p(vA),p(TA),p(CA),c&&t(LBe),c&&t(gc),p(MA),c&&t(BBe),c&&t(vr),p(EA),p(wA),p(AA),p(LA),p(BA),c&&t(xBe),c&&t(pc),p(xA),c&&t(kBe),c&&t(Tr),p(kA),p(SA),p(PA),p($A),p(IA),c&&t(RBe),c&&t(vc),p(DA),c&&t(SBe),c&&t(Fr),p(jA),p(qA),p(GA),p(OA),p(XA),c&&t(PBe),c&&t(Cc),p(VA),c&&t($Be),c&&t(Cr),p(zA),p(QA),p(HA),p(UA),p(JA),c&&t(IBe),c&&t(yc),p(YA),c&&t(DBe),c&&t(Mr),p(KA),p(e0),p(o0),p(r0),p(t0),c&&t(jBe),c&&t(Lc),p(a0),c&&t(NBe),c&&t(Er),p(s0),p(l0),p(i0),p(d0),p(c0),c&&t(qBe),c&&t(kc),p(m0),c&&t(GBe),c&&t(yr),p(f0),p(h0),p(u0),p(p0),p(_0),c&&t(OBe),c&&t(Pc),p(b0),c&&t(XBe),c&&t(wr),p(v0),p(F0),p(C0),p(M0),p(E0),c&&t(VBe),c&&t(Dc),p(y0),c&&t(zBe),c&&t(Ar),p(w0),p(L0),p(B0),p(x0),p(k0),c&&t(WBe),c&&t(qc),p(R0),c&&t(QBe),c&&t(Lr),p(S0),p($0),p(I0),p(D0),p(j0),c&&t(HBe),c&&t(Xc),p(N0),c&&t(UBe),c&&t(Br),p(q0),p(O0),p(X0),p(V0),p(z0),c&&t(JBe),c&&t(Wc),p(W0),c&&t(YBe),c&&t(xr),p(Q0),p(U0),p(J0),p(Y0),p(K0),c&&t(KBe),c&&t(Uc),p(Z0),c&&t(ZBe),c&&t(kr),p(eL),p(rL),p(tL),p(aL),p(sL),c&&t(exe),c&&t(Kc),p(nL),c&&t(oxe),c&&t(Rr),p(lL),p(dL),p(cL),p(mL),p(fL),c&&t(rxe),c&&t(om),p(gL),c&&t(txe),c&&t(Sr),p(hL),p(pL),p(_L),p(bL),p(vL),c&&t(axe),c&&t(am),p(TL),c&&t(sxe),c&&t(Pr),p(FL),p(ML),p(EL),p(yL),p(wL),c&&t(nxe),c&&t(lm),p(AL),c&&t(lxe),c&&t($r),p(LL),p(xL),p(kL),p(RL),p(SL),c&&t(ixe),c&&t(cm),p(PL),c&&t(dxe),c&&t(Ir),p($L),p(DL),p(jL),p(NL),p(qL),c&&t(cxe),c&&t(gm),p(GL),c&&t(mxe),c&&t(Dr),p(OL),p(VL),p(zL),p(WL),p(QL),c&&t(fxe),c&&t(pm),p(HL),c&&t(gxe),c&&t(jr),p(UL),p(YL),p(KL),p(ZL),p(o8),c&&t(hxe),c&&t(vm),p(r8),c&&t(uxe),c&&t(Nr),p(t8),p(s8),p(n8),p(l8),p(i8)}}}const J2t={local:"auto-classes",sections:[{local:"extending-the-auto-classes",title:"Extending the Auto Classes"},{local:"transformers.AutoConfig",title:"AutoConfig"},{local:"transformers.AutoTokenizer",title:"AutoTokenizer"},{local:"transformers.AutoFeatureExtractor",title:"AutoFeatureExtractor"},{local:"transformers.AutoProcessor",title:"AutoProcessor"},{local:"transformers.AutoModel",title:"AutoModel"},{local:"transformers.AutoModelForPreTraining",title:"AutoModelForPreTraining"},{local:"transformers.AutoModelForCausalLM",title:"AutoModelForCausalLM"},{local:"transformers.AutoModelForMaskedLM",title:"AutoModelForMaskedLM"},{local:"transformers.AutoModelForSeq2SeqLM",title:"AutoModelForSeq2SeqLM"},{local:"transformers.AutoModelForSequenceClassification",title:"AutoModelForSequenceClassification"},{local:"transformers.AutoModelForMultipleChoice",title:"AutoModelForMultipleChoice"},{local:"transformers.AutoModelForNextSentencePrediction",title:"AutoModelForNextSentencePrediction"},{local:"transformers.AutoModelForTokenClassification",title:"AutoModelForTokenClassification"},{local:"transformers.AutoModelForQuestionAnswering",title:"AutoModelForQuestionAnswering"},{local:"transformers.AutoModelForTableQuestionAnswering",title:"AutoModelForTableQuestionAnswering"},{local:"transformers.AutoModelForImageClassification",title:"AutoModelForImageClassification"},{local:"transformers.AutoModelForVision2Seq",title:"AutoModelForVision2Seq"},{local:"transformers.AutoModelForAudioClassification",title:"AutoModelForAudioClassification"},{local:"transformers.AutoModelForAudioFrameClassification",title:"AutoModelForAudioFrameClassification"},{local:"transformers.AutoModelForCTC",title:"AutoModelForCTC"},{local:"transformers.AutoModelForSpeechSeq2Seq",title:"AutoModelForSpeechSeq2Seq"},{local:"transformers.AutoModelForAudioXVector",title:"AutoModelForAudioXVector"},{local:"transformers.AutoModelForMaskedImageModeling",title:"AutoModelForMaskedImageModeling"},{local:"transformers.AutoModelForObjectDetection",title:"AutoModelForObjectDetection"},{local:"transformers.AutoModelForImageSegmentation",title:"AutoModelForImageSegmentation"},{local:"transformers.AutoModelForSemanticSegmentation",title:"AutoModelForSemanticSegmentation"},{local:"transformers.TFAutoModel",title:"TFAutoModel"},{local:"transformers.TFAutoModelForPreTraining",title:"TFAutoModelForPreTraining"},{local:"transformers.TFAutoModelForCausalLM",title:"TFAutoModelForCausalLM"},{local:"transformers.TFAutoModelForImageClassification",title:"TFAutoModelForImageClassification"},{local:"transformers.TFAutoModelForMaskedLM",title:"TFAutoModelForMaskedLM"},{local:"transformers.TFAutoModelForSeq2SeqLM",title:"TFAutoModelForSeq2SeqLM"},{local:"transformers.TFAutoModelForSequenceClassification",title:"TFAutoModelForSequenceClassification"},{local:"transformers.TFAutoModelForMultipleChoice",title:"TFAutoModelForMultipleChoice"},{local:"transformers.TFAutoModelForTableQuestionAnswering",title:"TFAutoModelForTableQuestionAnswering"},{local:"transformers.TFAutoModelForTokenClassification",title:"TFAutoModelForTokenClassification"},{local:"transformers.TFAutoModelForQuestionAnswering",title:"TFAutoModelForQuestionAnswering"},{local:"transformers.TFAutoModelForVision2Seq",title:"TFAutoModelForVision2Seq"},{local:"transformers.TFAutoModelForSpeechSeq2Seq",title:"TFAutoModelForSpeechSeq2Seq"},{local:"transformers.FlaxAutoModel",title:"FlaxAutoModel"},{local:"transformers.FlaxAutoModelForCausalLM",title:"FlaxAutoModelForCausalLM"},{local:"transformers.FlaxAutoModelForPreTraining",title:"FlaxAutoModelForPreTraining"},{local:"transformers.FlaxAutoModelForMaskedLM",title:"FlaxAutoModelForMaskedLM"},{local:"transformers.FlaxAutoModelForSeq2SeqLM",title:"FlaxAutoModelForSeq2SeqLM"},{local:"transformers.FlaxAutoModelForSequenceClassification",title:"FlaxAutoModelForSequenceClassification"},{local:"transformers.FlaxAutoModelForQuestionAnswering",title:"FlaxAutoModelForQuestionAnswering"},{local:"transformers.FlaxAutoModelForTokenClassification",title:"FlaxAutoModelForTokenClassification"},{local:"transformers.FlaxAutoModelForMultipleChoice",title:"FlaxAutoModelForMultipleChoice"},{local:"transformers.FlaxAutoModelForNextSentencePrediction",title:"FlaxAutoModelForNextSentencePrediction"},{local:"transformers.FlaxAutoModelForImageClassification",title:"FlaxAutoModelForImageClassification"},{local:"transformers.FlaxAutoModelForVision2Seq",title:"FlaxAutoModelForVision2Seq"}],title:"Auto Classes"};function Y2t(Ai,J,Be){let{fw:ie}=J;return Ai.$$set=fe=>{"fw"in fe&&Be(0,ie=fe.fw)},[ie]}class avt extends O2t{constructor(J){super();X2t(this,J,Y2t,U2t,V2t,{fw:0})}}export{avt as default,J2t as metadata};
