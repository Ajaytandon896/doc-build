import{S as Zh,i as el,s as tl,e as n,k as c,w as f,t as r,M as ol,c as s,d as o,m as h,a as d,x as _,h as a,b as i,F as e,g as p,y as g,q as v,o as k,B as T}from"../../chunks/vendor-4833417e.js";import{T as Rn}from"../../chunks/Tip-fffd6df1.js";import{D as z}from"../../chunks/Docstring-7b52c3d4.js";import{C as Go}from"../../chunks/CodeBlock-6a3d1b46.js";import{I as se}from"../../chunks/IconCopyLink-4b81c553.js";import"../../chunks/CopyButton-dacfbfaf.js";function nl(I){let u,P,m,y,N;return{c(){u=n("p"),P=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),y=r("Module"),N=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){u=s(w,"P",{});var b=d(u);P=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(b,"CODE",{});var q=d(m);y=a(q,"Module"),q.forEach(o),N=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function sl(I){let u,P,m,y,N;return{c(){u=n("p"),P=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),y=r("Module"),N=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){u=s(w,"P",{});var b=d(u);P=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(b,"CODE",{});var q=d(m);y=a(q,"Module"),q.forEach(o),N=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function rl(I){let u,P,m,y,N;return{c(){u=n("p"),P=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),y=r("Module"),N=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){u=s(w,"P",{});var b=d(u);P=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(b,"CODE",{});var q=d(m);y=a(q,"Module"),q.forEach(o),N=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function al(I){let u,P,m,y,N;return{c(){u=n("p"),P=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),y=r("Module"),N=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){u=s(w,"P",{});var b=d(u);P=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(b,"CODE",{});var q=d(m);y=a(q,"Module"),q.forEach(o),N=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function dl(I){let u,P,m,y,N;return{c(){u=n("p"),P=r("Although the recipe for forward pass needs to be defined within this function, one should call the "),m=n("code"),y=r("Module"),N=r(`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`)},l(w){u=s(w,"P",{});var b=d(u);P=a(b,"Although the recipe for forward pass needs to be defined within this function, one should call the "),m=s(b,"CODE",{});var q=d(m);y=a(q,"Module"),q.forEach(o),N=a(b,`
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`),b.forEach(o)},m(w,b){p(w,u,b),e(u,P),e(u,m),e(m,y),e(u,N)},d(w){w&&o(u)}}}function il(I){let u,P,m,y,N,w,b,q,Xs,Jn,re,Wo,Zs,er,Qe,tr,or,Yn,ae,Me,Ho,Xe,nr,Vo,sr,Kn,xe,rr,Ze,ar,dr,Qn,ao,ir,Xn,io,cr,Zn,co,Uo,hr,es,Ce,lr,et,pr,ur,ts,de,Se,Ro,tt,mr,Jo,fr,os,U,ot,_r,nt,gr,ho,vr,kr,Tr,ie,br,lo,wr,yr,po,Pr,Nr,ns,ce,De,Yo,st,qr,Ko,zr,ss,$,rt,$r,Qo,Fr,Er,at,Mr,uo,xr,Cr,Sr,R,dt,Dr,Xo,Or,Lr,it,mo,jr,Zo,Ar,Ir,fo,Br,en,Gr,Wr,Oe,ct,Hr,tn,Vr,Ur,W,ht,Rr,on,Jr,Yr,lt,Kr,he,Qr,nn,Xr,Zr,sn,ea,ta,oa,Le,pt,na,ut,sa,rn,ra,aa,rs,le,je,an,mt,da,dn,ia,as,pe,ft,ca,cn,ha,ds,ue,_t,la,hn,pa,is,me,gt,ua,ln,ma,cs,fe,vt,fa,pn,_a,hs,_e,Ae,un,kt,ga,mn,va,ls,M,Tt,ka,bt,Ta,_o,ba,wa,ya,ge,Pa,wt,Na,qa,fn,za,$a,Fa,yt,Ea,Pt,Ma,xa,Ca,S,Nt,Sa,ve,Da,go,Oa,La,_n,ja,Aa,Ia,Ie,Ba,gn,Ga,Wa,qt,ps,ke,Be,vn,zt,Ha,kn,Va,us,F,$t,Ua,Ft,Ra,vo,Ja,Ya,Ka,Te,Qa,Et,Xa,Za,Tn,ed,td,od,Mt,nd,xt,sd,rd,ad,B,dd,bn,id,cd,wn,hd,ld,yn,pd,ud,ko,md,fd,_d,D,Ct,gd,be,vd,To,kd,Td,Pn,bd,wd,yd,Ge,Pd,Nn,Nd,qd,St,ms,we,We,qn,Dt,zd,zn,$d,fs,E,Ot,Fd,Lt,Ed,bo,Md,xd,Cd,ye,Sd,jt,Dd,Od,$n,Ld,jd,Ad,At,Id,It,Bd,Gd,Wd,G,Hd,Fn,Vd,Ud,En,Rd,Jd,Mn,Yd,Kd,wo,Qd,Xd,Zd,O,Bt,ei,Pe,ti,yo,oi,ni,xn,si,ri,ai,He,di,Cn,ii,ci,Gt,_s,Ne,Ve,Sn,Wt,hi,Dn,li,gs,x,Ht,pi,Vt,ui,Po,mi,fi,_i,qe,gi,Ut,vi,ki,On,Ti,bi,wi,Rt,yi,Jt,Pi,Ni,qi,L,Yt,zi,ze,$i,No,Fi,Ei,Ln,Mi,xi,Ci,Ue,Si,jn,Di,Oi,Kt,vs,$e,Re,An,Qt,Li,In,ji,ks,C,Xt,Ai,Zt,Ii,qo,Bi,Gi,Wi,Fe,Hi,eo,Vi,Ui,Bn,Ri,Ji,Yi,to,Ki,oo,Qi,Xi,Zi,j,no,ec,Ee,tc,zo,oc,nc,Gn,sc,rc,ac,Je,dc,Wn,ic,cc,so,Ts;return w=new se({}),Xe=new se({}),tt=new se({}),ot=new z({props:{name:"class transformers.ProphetNetConfig",anchor:"transformers.ProphetNetConfig",parameters:[{name:"activation_dropout",val:" = 0.1"},{name:"activation_function",val:" = 'gelu'"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 1024"},{name:"encoder_ffn_dim",val:" = 4096"},{name:"num_encoder_layers",val:" = 12"},{name:"num_encoder_attention_heads",val:" = 16"},{name:"decoder_ffn_dim",val:" = 4096"},{name:"num_decoder_layers",val:" = 12"},{name:"num_decoder_attention_heads",val:" = 16"},{name:"attention_dropout",val:" = 0.1"},{name:"dropout",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"init_std",val:" = 0.02"},{name:"is_encoder_decoder",val:" = True"},{name:"add_cross_attention",val:" = True"},{name:"decoder_start_token_id",val:" = 0"},{name:"ngram",val:" = 2"},{name:"num_buckets",val:" = 32"},{name:"relative_max_distance",val:" = 128"},{name:"disable_ngram_loss",val:" = False"},{name:"eps",val:" = 0.0"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/configuration_prophetnet.py#L29",parametersDescription:[{anchor:"transformers.ProphetNetConfig.activation_dropout",description:`<strong>activation_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for activations inside the fully connected layer.`,name:"activation_dropout"},{anchor:"transformers.ProphetNetConfig.activation_function",description:`<strong>activation_function</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"activation_function"},{anchor:"transformers.ProphetNetConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the ProphetNET model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a>.`,name:"vocab_size"},{anchor:"transformers.ProphetNetConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.ProphetNetConfig.encoder_ffn_dim",description:`<strong>encoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in decoder.`,name:"encoder_ffn_dim"},{anchor:"transformers.ProphetNetConfig.num_encoder_layers",description:`<strong>num_encoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of encoder layers.`,name:"num_encoder_layers"},{anchor:"transformers.ProphetNetConfig.num_encoder_attention_heads",description:`<strong>num_encoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_encoder_attention_heads"},{anchor:"transformers.ProphetNetConfig.decoder_ffn_dim",description:`<strong>decoder_ffn_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the <code>intermediate</code> (often named feed-forward) layer in decoder.`,name:"decoder_ffn_dim"},{anchor:"transformers.ProphetNetConfig.num_decoder_layers",description:`<strong>num_decoder_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of decoder layers.`,name:"num_decoder_layers"},{anchor:"transformers.ProphetNetConfig.num_decoder_attention_heads",description:`<strong>num_decoder_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"num_decoder_attention_heads"},{anchor:"transformers.ProphetNetConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.ProphetNetConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.ProphetNetConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.ProphetNetConfig.init_std",description:`<strong>init_std</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"init_std"},{anchor:"transformers.ProphetNetConfig.add_cross_attention",description:`<strong>add_cross_attention</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether cross-attention layers should be added to the model.`,name:"add_cross_attention"},{anchor:"transformers.ProphetNetConfig.is_encoder_decoder",description:`<strong>is_encoder_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether this is an encoder/decoder model.`,name:"is_encoder_decoder"},{anchor:"transformers.ProphetNetConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.ProphetNetConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.ProphetNetConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.ProphetNetConfig.ngram",description:`<strong>ngram</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
Number of future tokens to predict. Set to 1 to be same as traditional Language model to predict next first
token.`,name:"ngram"},{anchor:"transformers.ProphetNetConfig.num_buckets",description:`<strong>num_buckets</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of buckets to use for each attention layer. This is for relative position calculation. See the
[T5 paper](see <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">https://arxiv.org/abs/1910.10683</a>) for more details.`,name:"num_buckets"},{anchor:"transformers.ProphetNetConfig.relative_max_distance",description:`<strong>relative_max_distance</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Relative distances greater than this number will be put into the last same bucket. This is for relative
position calculation. See the [T5 paper](see <a href="https://arxiv.org/abs/1910.10683" rel="nofollow">https://arxiv.org/abs/1910.10683</a>) for more details.`,name:"relative_max_distance"},{anchor:"transformers.ProphetNetConfig.disable_ngram_loss",description:`<strong>disable_ngram_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether be trained predicting only the next first token.`,name:"disable_ngram_loss"},{anchor:"transformers.ProphetNetConfig.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Controls the <code>epsilon</code> parameter value for label smoothing in the loss calculation. If set to 0, no label
smoothing is performed.`,name:"eps"},{anchor:"transformers.ProphetNetConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}]}}),st=new se({}),rt=new z({props:{name:"class transformers.ProphetNetTokenizer",anchor:"transformers.ProphetNetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"x_sep_token",val:" = '[X_SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/tokenization_prophetnet.py#L55",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.ProphetNetTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.ProphetNetTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.ProphetNetTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.ProphetNetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.ProphetNetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.ProphetNetTokenizer.x_sep_token",description:`<strong>x_sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[X_SEP]&quot;</code>) &#x2014;
Special second separator token, which can be generated by <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a>. It is
used to separate bullet-point like sentences in summarization, <em>e.g.</em>.`,name:"x_sep_token"},{anchor:"transformers.ProphetNetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.ProphetNetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.ProphetNetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.ProphetNetTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters.</p>
<p>This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).
strip_accents &#x2014; (<code>bool</code>, <em>optional</em>):
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"tokenize_chinese_chars"}]}}),dt=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/tokenization_prophetnet.py#L261",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),ct=new z({props:{name:"convert_tokens_to_string",anchor:"transformers.ProphetNetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/tokenization_prophetnet.py#L181"}}),ht=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/tokenization_prophetnet.py#L213",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],returnDescription:`
<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),lt=new Go({props:{codee:`0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
| first sequence    | second sequence |,`,highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`}}),pt=new z({props:{name:"get_special_tokens_mask",anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": typing.List[int]"},{name:"token_ids_1",val:": typing.Optional[typing.List[int]] = None"},{name:"already_has_special_tokens",val:": bool = False"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/tokenization_prophetnet.py#L186",parametersDescription:[{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.ProphetNetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],returnDescription:`
<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`
<p><code>List[int]</code></p>
`}}),mt=new se({}),ft=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"logits_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L252",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.logits_ngram",description:`<strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_ngram_hidden_states",description:`<strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"decoder_ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.decoder_ngram_attentions",description:`<strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the self-attention heads.`,name:"decoder_ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>. Attentions weights of the encoder, after the attention
softmax, used to compute the weighted average in the self-attention heads.`,name:"encoder_attentions"}]}}),_t=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor"},{name:"last_hidden_state_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"decoder_ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_last_hidden_state",val:": typing.Optional[torch.FloatTensor] = None"},{name:"encoder_hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"encoder_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L336",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) &#x2014;
Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.last_hidden_state_ngram",description:`<strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.`,name:"last_hidden_state_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_hidden_states",description:`<strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"decoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_ngram_hidden_states",description:`<strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"decoder_ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_attentions",description:`<strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"decoder_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.decoder_ngram_attentions",description:`<strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"decoder_ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_last_hidden_state",description:`<strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder of the model.`,name:"encoder_last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.`,name:"encoder_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput.encoder_attentions",description:`<strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"encoder_attentions"}]}}),gt=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput",parameters:[{name:"last_hidden_state",val:": FloatTensor"},{name:"last_hidden_state_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states_ngram",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L421",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) &#x2014;
Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.`,name:"last_hidden_state"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.last_hidden_state_ngram",description:`<strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.`,name:"last_hidden_state_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.ngram_hidden_states",description:`<strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.ngram_attentions",description:`<strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"}]}}),vt=new z({props:{name:"class transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput",anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput",parameters:[{name:"loss",val:": typing.Optional[torch.FloatTensor] = None"},{name:"logits",val:": FloatTensor = None"},{name:"logits_ngram",val:": typing.Optional[torch.FloatTensor] = None"},{name:"past_key_values",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"hidden_states_ngram",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"ngram_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"},{name:"cross_attentions",val:": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L481",parametersDescription:[{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.loss",description:`<strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) &#x2014;
Language modeling loss.`,name:"loss"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.logits",description:`<strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.logits_ngram",description:`<strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) &#x2014;
Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).`,name:"logits_ngram"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.past_key_values",description:`<strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.`,name:"past_key_values"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.ngram_hidden_states",description:`<strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.`,name:"ngram_hidden_states"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.`,name:"attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.ngram_attentions",description:`<strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the`,name:"ngram_attentions"},{anchor:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput.cross_attentions",description:`<strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the`,name:"cross_attentions"}]}}),kt=new se({}),Tt=new z({props:{name:"class transformers.ProphetNetModel",anchor:"transformers.ProphetNetModel",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1751",parametersDescription:[{anchor:"transformers.ProphetNetModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Nt=new z({props:{name:"forward",anchor:"transformers.ProphetNetModel.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:": typing.Optional[typing.Tuple] = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1783",parametersDescription:[{anchor:"transformers.ProphetNetModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetModel.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>ProphetNet uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.ProphetNetModel.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.ProphetNetModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetModel.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.ProphetNetModel.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetModel.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.ProphetNetModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ProphetNetModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) \u2014 Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size,ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>.</p>
<p>Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
</ul>
`}}),Ie=new Rn({props:{$$slots:{default:[nl]},$$scope:{ctx:I}}}),qt=new Go({props:{codee:`from transformers import ProphetNetTokenizer, ProphetNetModel

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetModel.from_pretrained("microsoft/prophetnet-large-uncased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

last_hidden_states = outputs.last_hidden_state  # main stream hidden states
last_hidden_states_ngram = outputs.last_hidden_state_ngram  # predict hidden states,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetModel.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state  <span class="hljs-comment"># main stream hidden states</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states_ngram = outputs.last_hidden_state_ngram  <span class="hljs-comment"># predict hidden states</span>`}}),zt=new se({}),$t=new z({props:{name:"class transformers.ProphetNetEncoder",anchor:"transformers.ProphetNetEncoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1244",parametersDescription:[{anchor:"transformers.ProphetNetEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Ct=new z({props:{name:"forward",anchor:"transformers.ProphetNetEncoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1274",parametersDescription:[{anchor:"transformers.ProphetNetEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetEncoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetEncoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetEncoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetEncoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetEncoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) \u2014 Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`}}),Ge=new Rn({props:{$$slots:{default:[sl]},$$scope:{ctx:I}}}),St=new Go({props:{codee:`from transformers import ProphetNetTokenizer, ProphetNetEncoder
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetEncoder.from_pretrained("patrickvonplaten/prophetnet-large-uncased-standalone")
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetEncoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetEncoder.from_pretrained(<span class="hljs-string">&quot;patrickvonplaten/prophetnet-large-uncased-standalone&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Dt=new se({}),Ot=new z({props:{name:"class transformers.ProphetNetDecoder",anchor:"transformers.ProphetNetDecoder",parameters:[{name:"config",val:": ProphetNetConfig"},{name:"word_embeddings",val:": Embedding = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1384",parametersDescription:[{anchor:"transformers.ProphetNetDecoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Bt=new z({props:{name:"forward",anchor:"transformers.ProphetNetDecoder.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1421",parametersDescription:[{anchor:"transformers.ProphetNetDecoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetDecoder.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetDecoder.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetDecoder.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetDecoder.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetDecoder.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetDecoder.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong>  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.ProphetNetDecoder.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.ProphetNetDecoder.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetDecoder.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetDecoder.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>) \u2014 Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>last_hidden_state_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
</ul>
`}}),He=new Rn({props:{$$slots:{default:[rl]},$$scope:{ctx:I}}}),Gt=new Go({props:{codee:`from transformers import ProphetNetTokenizer, ProphetNetDecoder
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetDecoder.from_pretrained("microsoft/prophetnet-large-uncased", add_cross_attention=False)
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

last_hidden_states = outputs.last_hidden_state,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetDecoder
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetDecoder.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>, add_cross_attention=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`}}),Wt=new se({}),Ht=new z({props:{name:"class transformers.ProphetNetForConditionalGeneration",anchor:"transformers.ProphetNetForConditionalGeneration",parameters:[{name:"config",val:": ProphetNetConfig"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1878",parametersDescription:[{anchor:"transformers.ProphetNetForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),Yt=new z({props:{name:"forward",anchor:"transformers.ProphetNetForConditionalGeneration.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"decoder_input_ids",val:" = None"},{name:"decoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"decoder_head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"encoder_outputs",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"decoder_inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L1899",parametersDescription:[{anchor:"transformers.ProphetNetForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>ProphetNet uses the <code>eos_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If
<code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).`,name:"decoder_input_ids"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.decoder_head_mask",description:`<strong>decoder_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"decoder_head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.encoder_outputs",description:`<strong>encoder_outputs</strong> (<code>tuple(tuple(torch.FloatTensor)</code>, <em>optional</em>) &#x2014;
Tuple consists of (<code>last_hidden_state</code>, <em>optional</em>: <code>hidden_states</code>, <em>optional</em>: <code>attentions</code>)
<code>last_hidden_state</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) is a sequence of
hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.`,name:"encoder_outputs"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[-100, 0, ..., config.vocab_size - 1]</code>. All labels set to <code>-100</code> are ignored (masked), the loss is only computed for
labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>decoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>decoder_ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>decoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>decoder_ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the self-attention heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
<li>
<p><strong>encoder_last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>, <em>optional</em>) \u2014 Sequence of hidden-states at the output of the last layer of the encoder of the model.</p>
</li>
<li>
<p><strong>encoder_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, encoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>encoder_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, encoder_sequence_length)</code>. Attentions weights of the encoder, after the attention
softmax, used to compute the weighted average in the self-attention heads.</p>
</li>
</ul>
`}}),Ue=new Rn({props:{$$slots:{default:[al]},$$scope:{ctx:I}}}),Kt=new Go({props:{codee:`from transformers import ProphetNetTokenizer, ProphetNetForConditionalGeneration

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetForConditionalGeneration.from_pretrained("microsoft/prophetnet-large-uncased")

input_ids = tokenizer(
    "Studies have been shown that owning a dog is good for you", return_tensors="pt"
).input_ids  # Batch size 1
decoder_input_ids = tokenizer("Studies show that", return_tensors="pt").input_ids  # Batch size 1
outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

logits_next_token = outputs.logits  # logits to predict next token as usual
logits_ngram_next_tokens = outputs.logits_ngram  # logits to predict 2nd, 3rd, ... next tokens,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Studies have been shown that owning a dog is good for you&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>decoder_input_ids = tokenizer(<span class="hljs-string">&quot;Studies show that&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids  <span class="hljs-comment"># Batch size 1</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits_next_token = outputs.logits  <span class="hljs-comment"># logits to predict next token as usual</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_ngram_next_tokens = outputs.logits_ngram  <span class="hljs-comment"># logits to predict 2nd, 3rd, ... next tokens</span>`}}),Qt=new se({}),Xt=new z({props:{name:"class transformers.ProphetNetForCausalLM",anchor:"transformers.ProphetNetForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L2087",parametersDescription:[{anchor:"transformers.ProphetNetForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}]}}),no=new z({props:{name:"forward",anchor:"transformers.ProphetNetForCausalLM.forward",parameters:[{name:"input_ids",val:" = None"},{name:"attention_mask",val:" = None"},{name:"encoder_hidden_states",val:" = None"},{name:"encoder_attention_mask",val:" = None"},{name:"head_mask",val:" = None"},{name:"cross_attn_head_mask",val:" = None"},{name:"past_key_values",val:" = None"},{name:"inputs_embeds",val:" = None"},{name:"labels",val:" = None"},{name:"use_cache",val:" = None"},{name:"output_attentions",val:" = None"},{name:"output_hidden_states",val:" = None"},{name:"return_dict",val:" = None"}],source:"https://github.com/huggingface/transformers/blob/master/src/transformers/models/prophetnet/modeling_prophetnet.py#L2122",parametersDescription:[{anchor:"transformers.ProphetNetForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a>. See <a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/master/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.ProphetNetForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ProphetNetForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ProphetNetForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ProphetNetForCausalLM.forward.encoder_hidden_states",description:`<strong>encoder_hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.`,name:"encoder_hidden_states"},{anchor:"transformers.ProphetNetForCausalLM.forward.encoder_attention_mask",description:`<strong>encoder_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,name:"encoder_attention_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.cross_attn_head_mask",description:`<strong>cross_attn_head_mask</strong> (<code>torch.Tensor</code> of shape <code>(decoder_layers, decoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the cross-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"cross_attn_head_mask"},{anchor:"transformers.ProphetNetForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.ProphetNetForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"use_cache"},{anchor:"transformers.ProphetNetForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels n <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],returnDescription:`
<p>A <a
  href="/docs/transformers/master/en/model_doc/prophetnet#transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput"
>transformers.models.prophetnet.modeling_prophetnet.ProphetNetDecoderLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>ProphenetConfig</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) \u2014 Language modeling loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the main stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>logits_ngram</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, ngram * decoder_sequence_length, config.vocab_size)</code>) \u2014 Prediction scores of the predict stream language modeling head (scores for each vocabulary token before
SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>List[torch.FloatTensor]</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) \u2014 List of <code>torch.FloatTensor</code> of length <code>config.n_layers</code>, with each tensor of shape <code>(2, batch_size, num_attn_heads, decoder_sequence_length, embed_size_per_head)</code>).</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>ngram_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, ngram * decoder_sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding
outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
self-attention heads.</p>
</li>
<li>
<p><strong>ngram_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, decoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the
weighted average in the</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) \u2014 Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_attn_heads, encoder_sequence_length, decoder_sequence_length)</code>.</p>
<p>Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to
compute the weighted average in the</p>
</li>
</ul>
`}}),Je=new Rn({props:{$$slots:{default:[dl]},$$scope:{ctx:I}}}),so=new Go({props:{codee:`from transformers import ProphetNetTokenizer, ProphetNetForCausalLM
import torch

tokenizer = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = ProphetNetForCausalLM.from_pretrained("microsoft/prophetnet-large-uncased")
assert model.config.is_decoder, f"{model.__class__} has to be configured as a decoder."
inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
outputs = model(**inputs)

logits = outputs.logits

# Model can also be used with EncoderDecoder framework
from transformers import BertTokenizer, EncoderDecoderModel, ProphetNetTokenizer
import torch

tokenizer_enc = BertTokenizer.from_pretrained("bert-large-uncased")
tokenizer_dec = ProphetNetTokenizer.from_pretrained("microsoft/prophetnet-large-uncased")
model = EncoderDecoderModel.from_encoder_decoder_pretrained(
    "bert-large-uncased", "microsoft/prophetnet-large-uncased"
)

ARTICLE = (
    "the us state department said wednesday it had received no "
    "formal word from bolivia that it was expelling the us ambassador there "
    "but said the charges made against him are \`\` baseless ."
)
input_ids = tokenizer_enc(ARTICLE, return_tensors="pt").input_ids
labels = tokenizer_dec(
    "us rejects charges against its ambassador in bolivia", return_tensors="pt"
).input_ids
outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-1], labels=labels[:, 1:])

loss = outputs.loss,`,highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ProphetNetTokenizer, ProphetNetForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ProphetNetForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">assert</span> model.config.is_decoder, <span class="hljs-string">f&quot;<span class="hljs-subst">{model.__class__}</span> has to be configured as a decoder.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Model can also be used with EncoderDecoder framework</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, EncoderDecoderModel, ProphetNetTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_enc = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;bert-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer_dec = ProphetNetTokenizer.from_pretrained(<span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = EncoderDecoderModel.from_encoder_decoder_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;bert-large-uncased&quot;</span>, <span class="hljs-string">&quot;microsoft/prophetnet-large-uncased&quot;</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>ARTICLE = (
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;the us state department said wednesday it had received no &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;formal word from bolivia that it was expelling the us ambassador there &quot;</span>
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;but said the charges made against him are \`\` baseless .&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer_enc(ARTICLE, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>labels = tokenizer_dec(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;us rejects charges against its ambassador in bolivia&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(input_ids=input_ids, decoder_input_ids=labels[:, :-<span class="hljs-number">1</span>], labels=labels[:, <span class="hljs-number">1</span>:])

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = outputs.loss`}}),{c(){u=n("meta"),P=c(),m=n("h1"),y=n("a"),N=n("span"),f(w.$$.fragment),b=c(),q=n("span"),Xs=r("ProphetNet"),Jn=c(),re=n("p"),Wo=n("strong"),Zs=r("DISCLAIMER:"),er=r(" If you see something strange, file a "),Qe=n("a"),tr=r("Github Issue"),or=r(` and assign
@patrickvonplaten`),Yn=c(),ae=n("h2"),Me=n("a"),Ho=n("span"),f(Xe.$$.fragment),nr=c(),Vo=n("span"),sr=r("Overview"),Kn=c(),xe=n("p"),rr=r("The ProphetNet model was proposed in "),Ze=n("a"),ar=r("ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),dr=r(` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),Qn=c(),ao=n("p"),ir=r(`ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of just
the next token.`),Xn=c(),io=n("p"),cr=r("The abstract from the paper is the following:"),Zn=c(),co=n("p"),Uo=n("em"),hr=r(`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),es=c(),Ce=n("p"),lr=r("The Authors\u2019 code can be found "),et=n("a"),pr=r("here"),ur=r("."),ts=c(),de=n("h2"),Se=n("a"),Ro=n("span"),f(tt.$$.fragment),mr=c(),Jo=n("span"),fr=r("ProphetNetConfig"),os=c(),U=n("div"),f(ot.$$.fragment),_r=c(),nt=n("p"),gr=r("This is the configuration class to store the configuration of a "),ho=n("a"),vr=r("ProphetNetModel"),kr=r(`. It is used to instantiate a
ProphetNet model according to the specified arguments, defining the model architecture.`),Tr=c(),ie=n("p"),br=r("Configuration objects inherit from "),lo=n("a"),wr=r("PretrainedConfig"),yr=r(` and can be used to control the model outputs. Read the
documentation from `),po=n("a"),Pr=r("PretrainedConfig"),Nr=r(" for more information."),ns=c(),ce=n("h2"),De=n("a"),Yo=n("span"),f(st.$$.fragment),qr=c(),Ko=n("span"),zr=r("ProphetNetTokenizer"),ss=c(),$=n("div"),f(rt.$$.fragment),$r=c(),Qo=n("p"),Fr=r("Construct a ProphetNetTokenizer. Based on WordPiece."),Er=c(),at=n("p"),Mr=r("This tokenizer inherits from "),uo=n("a"),xr=r("PreTrainedTokenizer"),Cr=r(` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),Sr=c(),R=n("div"),f(dt.$$.fragment),Dr=c(),Xo=n("p"),Or=r(`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Lr=c(),it=n("ul"),mo=n("li"),jr=r("single sequence: "),Zo=n("code"),Ar=r("[CLS] X [SEP]"),Ir=c(),fo=n("li"),Br=r("pair of sequences: "),en=n("code"),Gr=r("[CLS] A [SEP] B [SEP]"),Wr=c(),Oe=n("div"),f(ct.$$.fragment),Hr=c(),tn=n("p"),Vr=r("Converts a sequence of tokens (string) in a single string."),Ur=c(),W=n("div"),f(ht.$$.fragment),Rr=c(),on=n("p"),Jr=r(`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet
sequence pair mask has the following format:`),Yr=c(),f(lt.$$.fragment),Kr=c(),he=n("p"),Qr=r("If "),nn=n("code"),Xr=r("token_ids_1"),Zr=r(" is "),sn=n("code"),ea=r("None"),ta=r(", this method only returns the first portion of the mask (0s)."),oa=c(),Le=n("div"),f(pt.$$.fragment),na=c(),ut=n("p"),sa=r(`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),rn=n("code"),ra=r("prepare_for_model"),aa=r(" method."),rs=c(),le=n("h2"),je=n("a"),an=n("span"),f(mt.$$.fragment),da=c(),dn=n("span"),ia=r("ProphetNet specific outputs"),as=c(),pe=n("div"),f(ft.$$.fragment),ca=c(),cn=n("p"),ha=r("Base class for sequence-to-sequence language models outputs."),ds=c(),ue=n("div"),f(_t.$$.fragment),la=c(),hn=n("p"),pa=r(`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),is=c(),me=n("div"),f(gt.$$.fragment),ua=c(),ln=n("p"),ma=r("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),cs=c(),fe=n("div"),f(vt.$$.fragment),fa=c(),pn=n("p"),_a=r("Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),hs=c(),_e=n("h2"),Ae=n("a"),un=n("span"),f(kt.$$.fragment),ga=c(),mn=n("span"),va=r("ProphetNetModel"),ls=c(),M=n("div"),f(Tt.$$.fragment),ka=c(),bt=n("p"),Ta=r(`The bare ProphetNet Model outputting raw hidden-states without any specific head on top.
This model inherits from `),_o=n("a"),ba=r("PreTrainedModel"),wa=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),ya=c(),ge=n("p"),Pa=r("Original ProphetNet code can be found "),wt=n("a"),Na=r("here"),qa=r(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),fn=n("code"),za=r("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),$a=r("."),Fa=c(),yt=n("p"),Ea=r("This model is a PyTorch "),Pt=n("a"),Ma=r("torch.nn.Module"),xa=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Ca=c(),S=n("div"),f(Nt.$$.fragment),Sa=c(),ve=n("p"),Da=r("The "),go=n("a"),Oa=r("ProphetNetModel"),La=r(" forward method, overrides the "),_n=n("code"),ja=r("__call__"),Aa=r(" special method."),Ia=c(),f(Ie.$$.fragment),Ba=c(),gn=n("p"),Ga=r("Example:"),Wa=c(),f(qt.$$.fragment),ps=c(),ke=n("h2"),Be=n("a"),vn=n("span"),f(zt.$$.fragment),Ha=c(),kn=n("span"),Va=r("ProphetNetEncoder"),us=c(),F=n("div"),f($t.$$.fragment),Ua=c(),Ft=n("p"),Ra=r(`The standalone encoder part of the ProphetNetModel.
This model inherits from `),vo=n("a"),Ja=r("PreTrainedModel"),Ya=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ka=c(),Te=n("p"),Qa=r("Original ProphetNet code can be found "),Et=n("a"),Xa=r("here"),Za=r(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Tn=n("code"),ed=r("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),td=r("."),od=c(),Mt=n("p"),nd=r("This model is a PyTorch "),xt=n("a"),sd=r("torch.nn.Module"),rd=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),ad=c(),B=n("p"),dd=r("word_embeddings  ("),bn=n("code"),id=r("torch.nn.Embeddings"),cd=r(" of shape "),wn=n("code"),hd=r("(config.vocab_size, config.hidden_size)"),ld=r(", "),yn=n("em"),pd=r("optional"),ud=r(`):
The word embedding parameters. This can be used to initialize `),ko=n("a"),md=r("ProphetNetEncoder"),fd=r(` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),_d=c(),D=n("div"),f(Ct.$$.fragment),gd=c(),be=n("p"),vd=r("The "),To=n("a"),kd=r("ProphetNetEncoder"),Td=r(" forward method, overrides the "),Pn=n("code"),bd=r("__call__"),wd=r(" special method."),yd=c(),f(Ge.$$.fragment),Pd=c(),Nn=n("p"),Nd=r("Example:"),qd=c(),f(St.$$.fragment),ms=c(),we=n("h2"),We=n("a"),qn=n("span"),f(Dt.$$.fragment),zd=c(),zn=n("span"),$d=r("ProphetNetDecoder"),fs=c(),E=n("div"),f(Ot.$$.fragment),Fd=c(),Lt=n("p"),Ed=r(`The standalone decoder part of the ProphetNetModel.
This model inherits from `),bo=n("a"),Md=r("PreTrainedModel"),xd=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Cd=c(),ye=n("p"),Sd=r("Original ProphetNet code can be found "),jt=n("a"),Dd=r("here"),Od=r(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),$n=n("code"),Ld=r("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),jd=r("."),Ad=c(),At=n("p"),Id=r("This model is a PyTorch "),It=n("a"),Bd=r("torch.nn.Module"),Gd=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Wd=c(),G=n("p"),Hd=r("word_embeddings  ("),Fn=n("code"),Vd=r("torch.nn.Embeddings"),Ud=r(" of shape "),En=n("code"),Rd=r("(config.vocab_size, config.hidden_size)"),Jd=r(", "),Mn=n("em"),Yd=r("optional"),Kd=r(`):
The word embedding parameters. This can be used to initialize `),wo=n("a"),Qd=r("ProphetNetEncoder"),Xd=r(` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),Zd=c(),O=n("div"),f(Bt.$$.fragment),ei=c(),Pe=n("p"),ti=r("The "),yo=n("a"),oi=r("ProphetNetDecoder"),ni=r(" forward method, overrides the "),xn=n("code"),si=r("__call__"),ri=r(" special method."),ai=c(),f(He.$$.fragment),di=c(),Cn=n("p"),ii=r("Example:"),ci=c(),f(Gt.$$.fragment),_s=c(),Ne=n("h2"),Ve=n("a"),Sn=n("span"),f(Wt.$$.fragment),hi=c(),Dn=n("span"),li=r("ProphetNetForConditionalGeneration"),gs=c(),x=n("div"),f(Ht.$$.fragment),pi=c(),Vt=n("p"),ui=r(`The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.
This model inherits from `),Po=n("a"),mi=r("PreTrainedModel"),fi=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),_i=c(),qe=n("p"),gi=r("Original ProphetNet code can be found "),Ut=n("a"),vi=r("here"),ki=r(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),On=n("code"),Ti=r("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),bi=r("."),wi=c(),Rt=n("p"),yi=r("This model is a PyTorch "),Jt=n("a"),Pi=r("torch.nn.Module"),Ni=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),qi=c(),L=n("div"),f(Yt.$$.fragment),zi=c(),ze=n("p"),$i=r("The "),No=n("a"),Fi=r("ProphetNetForConditionalGeneration"),Ei=r(" forward method, overrides the "),Ln=n("code"),Mi=r("__call__"),xi=r(" special method."),Ci=c(),f(Ue.$$.fragment),Si=c(),jn=n("p"),Di=r("Example:"),Oi=c(),f(Kt.$$.fragment),vs=c(),$e=n("h2"),Re=n("a"),An=n("span"),f(Qt.$$.fragment),Li=c(),In=n("span"),ji=r("ProphetNetForCausalLM"),ks=c(),C=n("div"),f(Xt.$$.fragment),Ai=c(),Zt=n("p"),Ii=r(`The standalone decoder part of the ProphetNetModel with a lm head on top. The model can be used for causal language modeling.
This model inherits from `),qo=n("a"),Bi=r("PreTrainedModel"),Gi=r(`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Wi=c(),Fe=n("p"),Hi=r("Original ProphetNet code can be found "),eo=n("a"),Vi=r("here"),Ui=r(`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Bn=n("code"),Ri=r("convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Ji=r("."),Yi=c(),to=n("p"),Ki=r("This model is a PyTorch "),oo=n("a"),Qi=r("torch.nn.Module"),Xi=r(` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Zi=c(),j=n("div"),f(no.$$.fragment),ec=c(),Ee=n("p"),tc=r("The "),zo=n("a"),oc=r("ProphetNetForCausalLM"),nc=r(" forward method, overrides the "),Gn=n("code"),sc=r("__call__"),rc=r(" special method."),ac=c(),f(Je.$$.fragment),dc=c(),Wn=n("p"),ic=r("Example:"),cc=c(),f(so.$$.fragment),this.h()},l(t){const l=ol('[data-svelte="svelte-1phssyn"]',document.head);u=s(l,"META",{name:!0,content:!0}),l.forEach(o),P=h(t),m=s(t,"H1",{class:!0});var ro=d(m);y=s(ro,"A",{id:!0,class:!0,href:!0});var Hn=d(y);N=s(Hn,"SPAN",{});var Vn=d(N);_(w.$$.fragment,Vn),Vn.forEach(o),Hn.forEach(o),b=h(ro),q=s(ro,"SPAN",{});var Un=d(q);Xs=a(Un,"ProphetNet"),Un.forEach(o),ro.forEach(o),Jn=h(t),re=s(t,"P",{});var Ye=d(re);Wo=s(Ye,"STRONG",{});var pc=d(Wo);Zs=a(pc,"DISCLAIMER:"),pc.forEach(o),er=a(Ye," If you see something strange, file a "),Qe=s(Ye,"A",{href:!0,rel:!0});var uc=d(Qe);tr=a(uc,"Github Issue"),uc.forEach(o),or=a(Ye,` and assign
@patrickvonplaten`),Ye.forEach(o),Yn=h(t),ae=s(t,"H2",{class:!0});var bs=d(ae);Me=s(bs,"A",{id:!0,class:!0,href:!0});var mc=d(Me);Ho=s(mc,"SPAN",{});var fc=d(Ho);_(Xe.$$.fragment,fc),fc.forEach(o),mc.forEach(o),nr=h(bs),Vo=s(bs,"SPAN",{});var _c=d(Vo);sr=a(_c,"Overview"),_c.forEach(o),bs.forEach(o),Kn=h(t),xe=s(t,"P",{});var ws=d(xe);rr=a(ws,"The ProphetNet model was proposed in "),Ze=s(ws,"A",{href:!0,rel:!0});var gc=d(Ze);ar=a(gc,"ProphetNet: Predicting Future N-gram for Sequence-to-Sequence Pre-training,"),gc.forEach(o),dr=a(ws,` by Yu Yan, Weizhen Qi, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei
Zhang, Ming Zhou on 13 Jan, 2020.`),ws.forEach(o),Qn=h(t),ao=s(t,"P",{});var vc=d(ao);ir=a(vc,`ProphetNet is an encoder-decoder model and can predict n-future tokens for \u201Cngram\u201D language modeling instead of just
the next token.`),vc.forEach(o),Xn=h(t),io=s(t,"P",{});var kc=d(io);cr=a(kc,"The abstract from the paper is the following:"),kc.forEach(o),Zn=h(t),co=s(t,"P",{});var Tc=d(co);Uo=s(Tc,"EM",{});var bc=d(Uo);hr=a(bc,`In this paper, we present a new sequence-to-sequence pretraining model called ProphetNet, which introduces a novel
self-supervised objective named future n-gram prediction and the proposed n-stream self-attention mechanism. Instead of
the optimization of one-step ahead prediction in traditional sequence-to-sequence model, the ProphetNet is optimized by
n-step ahead prediction which predicts the next n tokens simultaneously based on previous context tokens at each time
step. The future n-gram prediction explicitly encourages the model to plan for the future tokens and prevent
overfitting on strong local correlations. We pre-train ProphetNet using a base scale dataset (16GB) and a large scale
dataset (160GB) respectively. Then we conduct experiments on CNN/DailyMail, Gigaword, and SQuAD 1.1 benchmarks for
abstractive summarization and question generation tasks. Experimental results show that ProphetNet achieves new
state-of-the-art results on all these datasets compared to the models using the same scale pretraining corpus.`),bc.forEach(o),Tc.forEach(o),es=h(t),Ce=s(t,"P",{});var ys=d(Ce);lr=a(ys,"The Authors\u2019 code can be found "),et=s(ys,"A",{href:!0,rel:!0});var wc=d(et);pr=a(wc,"here"),wc.forEach(o),ur=a(ys,"."),ys.forEach(o),ts=h(t),de=s(t,"H2",{class:!0});var Ps=d(de);Se=s(Ps,"A",{id:!0,class:!0,href:!0});var yc=d(Se);Ro=s(yc,"SPAN",{});var Pc=d(Ro);_(tt.$$.fragment,Pc),Pc.forEach(o),yc.forEach(o),mr=h(Ps),Jo=s(Ps,"SPAN",{});var Nc=d(Jo);fr=a(Nc,"ProphetNetConfig"),Nc.forEach(o),Ps.forEach(o),os=h(t),U=s(t,"DIV",{class:!0});var $o=d(U);_(ot.$$.fragment,$o),_r=h($o),nt=s($o,"P",{});var Ns=d(nt);gr=a(Ns,"This is the configuration class to store the configuration of a "),ho=s(Ns,"A",{href:!0});var qc=d(ho);vr=a(qc,"ProphetNetModel"),qc.forEach(o),kr=a(Ns,`. It is used to instantiate a
ProphetNet model according to the specified arguments, defining the model architecture.`),Ns.forEach(o),Tr=h($o),ie=s($o,"P",{});var Fo=d(ie);br=a(Fo,"Configuration objects inherit from "),lo=s(Fo,"A",{href:!0});var zc=d(lo);wr=a(zc,"PretrainedConfig"),zc.forEach(o),yr=a(Fo,` and can be used to control the model outputs. Read the
documentation from `),po=s(Fo,"A",{href:!0});var $c=d(po);Pr=a($c,"PretrainedConfig"),$c.forEach(o),Nr=a(Fo," for more information."),Fo.forEach(o),$o.forEach(o),ns=h(t),ce=s(t,"H2",{class:!0});var qs=d(ce);De=s(qs,"A",{id:!0,class:!0,href:!0});var Fc=d(De);Yo=s(Fc,"SPAN",{});var Ec=d(Yo);_(st.$$.fragment,Ec),Ec.forEach(o),Fc.forEach(o),qr=h(qs),Ko=s(qs,"SPAN",{});var Mc=d(Ko);zr=a(Mc,"ProphetNetTokenizer"),Mc.forEach(o),qs.forEach(o),ss=h(t),$=s(t,"DIV",{class:!0});var A=d($);_(rt.$$.fragment,A),$r=h(A),Qo=s(A,"P",{});var xc=d(Qo);Fr=a(xc,"Construct a ProphetNetTokenizer. Based on WordPiece."),xc.forEach(o),Er=h(A),at=s(A,"P",{});var zs=d(at);Mr=a(zs,"This tokenizer inherits from "),uo=s(zs,"A",{href:!0});var Cc=d(uo);xr=a(Cc,"PreTrainedTokenizer"),Cc.forEach(o),Cr=a(zs,` which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`),zs.forEach(o),Sr=h(A),R=s(A,"DIV",{class:!0});var Eo=d(R);_(dt.$$.fragment,Eo),Dr=h(Eo),Xo=s(Eo,"P",{});var Sc=d(Xo);Or=a(Sc,`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`),Sc.forEach(o),Lr=h(Eo),it=s(Eo,"UL",{});var $s=d(it);mo=s($s,"LI",{});var hc=d(mo);jr=a(hc,"single sequence: "),Zo=s(hc,"CODE",{});var Dc=d(Zo);Ar=a(Dc,"[CLS] X [SEP]"),Dc.forEach(o),hc.forEach(o),Ir=h($s),fo=s($s,"LI",{});var lc=d(fo);Br=a(lc,"pair of sequences: "),en=s(lc,"CODE",{});var Oc=d(en);Gr=a(Oc,"[CLS] A [SEP] B [SEP]"),Oc.forEach(o),lc.forEach(o),$s.forEach(o),Eo.forEach(o),Wr=h(A),Oe=s(A,"DIV",{class:!0});var Fs=d(Oe);_(ct.$$.fragment,Fs),Hr=h(Fs),tn=s(Fs,"P",{});var Lc=d(tn);Vr=a(Lc,"Converts a sequence of tokens (string) in a single string."),Lc.forEach(o),Fs.forEach(o),Ur=h(A),W=s(A,"DIV",{class:!0});var Ke=d(W);_(ht.$$.fragment,Ke),Rr=h(Ke),on=s(Ke,"P",{});var jc=d(on);Jr=a(jc,`Create a mask from the two sequences passed to be used in a sequence-pair classification task. A ProphetNet
sequence pair mask has the following format:`),jc.forEach(o),Yr=h(Ke),_(lt.$$.fragment,Ke),Kr=h(Ke),he=s(Ke,"P",{});var Mo=d(he);Qr=a(Mo,"If "),nn=s(Mo,"CODE",{});var Ac=d(nn);Xr=a(Ac,"token_ids_1"),Ac.forEach(o),Zr=a(Mo," is "),sn=s(Mo,"CODE",{});var Ic=d(sn);ea=a(Ic,"None"),Ic.forEach(o),ta=a(Mo,", this method only returns the first portion of the mask (0s)."),Mo.forEach(o),Ke.forEach(o),oa=h(A),Le=s(A,"DIV",{class:!0});var Es=d(Le);_(pt.$$.fragment,Es),na=h(Es),ut=s(Es,"P",{});var Ms=d(ut);sa=a(Ms,`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `),rn=s(Ms,"CODE",{});var Bc=d(rn);ra=a(Bc,"prepare_for_model"),Bc.forEach(o),aa=a(Ms," method."),Ms.forEach(o),Es.forEach(o),A.forEach(o),rs=h(t),le=s(t,"H2",{class:!0});var xs=d(le);je=s(xs,"A",{id:!0,class:!0,href:!0});var Gc=d(je);an=s(Gc,"SPAN",{});var Wc=d(an);_(mt.$$.fragment,Wc),Wc.forEach(o),Gc.forEach(o),da=h(xs),dn=s(xs,"SPAN",{});var Hc=d(dn);ia=a(Hc,"ProphetNet specific outputs"),Hc.forEach(o),xs.forEach(o),as=h(t),pe=s(t,"DIV",{class:!0});var Cs=d(pe);_(ft.$$.fragment,Cs),ca=h(Cs),cn=s(Cs,"P",{});var Vc=d(cn);ha=a(Vc,"Base class for sequence-to-sequence language models outputs."),Vc.forEach(o),Cs.forEach(o),ds=h(t),ue=s(t,"DIV",{class:!0});var Ss=d(ue);_(_t.$$.fragment,Ss),la=h(Ss),hn=s(Ss,"P",{});var Uc=d(hn);pa=a(Uc,`Base class for model encoder\u2019s outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.`),Uc.forEach(o),Ss.forEach(o),is=h(t),me=s(t,"DIV",{class:!0});var Ds=d(me);_(gt.$$.fragment,Ds),ua=h(Ds),ln=s(Ds,"P",{});var Rc=d(ln);ma=a(Rc,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Rc.forEach(o),Ds.forEach(o),cs=h(t),fe=s(t,"DIV",{class:!0});var Os=d(fe);_(vt.$$.fragment,Os),fa=h(Os),pn=s(Os,"P",{});var Jc=d(pn);_a=a(Jc,"Base class for model\u2019s outputs that may also contain a past key/values (to speed up sequential decoding)."),Jc.forEach(o),Os.forEach(o),hs=h(t),_e=s(t,"H2",{class:!0});var Ls=d(_e);Ae=s(Ls,"A",{id:!0,class:!0,href:!0});var Yc=d(Ae);un=s(Yc,"SPAN",{});var Kc=d(un);_(kt.$$.fragment,Kc),Kc.forEach(o),Yc.forEach(o),ga=h(Ls),mn=s(Ls,"SPAN",{});var Qc=d(mn);va=a(Qc,"ProphetNetModel"),Qc.forEach(o),Ls.forEach(o),ls=h(t),M=s(t,"DIV",{class:!0});var J=d(M);_(Tt.$$.fragment,J),ka=h(J),bt=s(J,"P",{});var js=d(bt);Ta=a(js,`The bare ProphetNet Model outputting raw hidden-states without any specific head on top.
This model inherits from `),_o=s(js,"A",{href:!0});var Xc=d(_o);ba=a(Xc,"PreTrainedModel"),Xc.forEach(o),wa=a(js,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),js.forEach(o),ya=h(J),ge=s(J,"P",{});var xo=d(ge);Pa=a(xo,"Original ProphetNet code can be found "),wt=s(xo,"A",{href:!0,rel:!0});var Zc=d(wt);Na=a(Zc,"here"),Zc.forEach(o),qa=a(xo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),fn=s(xo,"CODE",{});var eh=d(fn);za=a(eh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),eh.forEach(o),$a=a(xo,"."),xo.forEach(o),Fa=h(J),yt=s(J,"P",{});var As=d(yt);Ea=a(As,"This model is a PyTorch "),Pt=s(As,"A",{href:!0,rel:!0});var th=d(Pt);Ma=a(th,"torch.nn.Module"),th.forEach(o),xa=a(As,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),As.forEach(o),Ca=h(J),S=s(J,"DIV",{class:!0});var Y=d(S);_(Nt.$$.fragment,Y),Sa=h(Y),ve=s(Y,"P",{});var Co=d(ve);Da=a(Co,"The "),go=s(Co,"A",{href:!0});var oh=d(go);Oa=a(oh,"ProphetNetModel"),oh.forEach(o),La=a(Co," forward method, overrides the "),_n=s(Co,"CODE",{});var nh=d(_n);ja=a(nh,"__call__"),nh.forEach(o),Aa=a(Co," special method."),Co.forEach(o),Ia=h(Y),_(Ie.$$.fragment,Y),Ba=h(Y),gn=s(Y,"P",{});var sh=d(gn);Ga=a(sh,"Example:"),sh.forEach(o),Wa=h(Y),_(qt.$$.fragment,Y),Y.forEach(o),J.forEach(o),ps=h(t),ke=s(t,"H2",{class:!0});var Is=d(ke);Be=s(Is,"A",{id:!0,class:!0,href:!0});var rh=d(Be);vn=s(rh,"SPAN",{});var ah=d(vn);_(zt.$$.fragment,ah),ah.forEach(o),rh.forEach(o),Ha=h(Is),kn=s(Is,"SPAN",{});var dh=d(kn);Va=a(dh,"ProphetNetEncoder"),dh.forEach(o),Is.forEach(o),us=h(t),F=s(t,"DIV",{class:!0});var H=d(F);_($t.$$.fragment,H),Ua=h(H),Ft=s(H,"P",{});var Bs=d(Ft);Ra=a(Bs,`The standalone encoder part of the ProphetNetModel.
This model inherits from `),vo=s(Bs,"A",{href:!0});var ih=d(vo);Ja=a(ih,"PreTrainedModel"),ih.forEach(o),Ya=a(Bs,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Bs.forEach(o),Ka=h(H),Te=s(H,"P",{});var So=d(Te);Qa=a(So,"Original ProphetNet code can be found "),Et=s(So,"A",{href:!0,rel:!0});var ch=d(Et);Xa=a(ch,"here"),ch.forEach(o),Za=a(So,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Tn=s(So,"CODE",{});var hh=d(Tn);ed=a(hh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),hh.forEach(o),td=a(So,"."),So.forEach(o),od=h(H),Mt=s(H,"P",{});var Gs=d(Mt);nd=a(Gs,"This model is a PyTorch "),xt=s(Gs,"A",{href:!0,rel:!0});var lh=d(xt);sd=a(lh,"torch.nn.Module"),lh.forEach(o),rd=a(Gs,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Gs.forEach(o),ad=h(H),B=s(H,"P",{});var K=d(B);dd=a(K,"word_embeddings  ("),bn=s(K,"CODE",{});var ph=d(bn);id=a(ph,"torch.nn.Embeddings"),ph.forEach(o),cd=a(K," of shape "),wn=s(K,"CODE",{});var uh=d(wn);hd=a(uh,"(config.vocab_size, config.hidden_size)"),uh.forEach(o),ld=a(K,", "),yn=s(K,"EM",{});var mh=d(yn);pd=a(mh,"optional"),mh.forEach(o),ud=a(K,`):
The word embedding parameters. This can be used to initialize `),ko=s(K,"A",{href:!0});var fh=d(ko);md=a(fh,"ProphetNetEncoder"),fh.forEach(o),fd=a(K,` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),K.forEach(o),_d=h(H),D=s(H,"DIV",{class:!0});var Q=d(D);_(Ct.$$.fragment,Q),gd=h(Q),be=s(Q,"P",{});var Do=d(be);vd=a(Do,"The "),To=s(Do,"A",{href:!0});var _h=d(To);kd=a(_h,"ProphetNetEncoder"),_h.forEach(o),Td=a(Do," forward method, overrides the "),Pn=s(Do,"CODE",{});var gh=d(Pn);bd=a(gh,"__call__"),gh.forEach(o),wd=a(Do," special method."),Do.forEach(o),yd=h(Q),_(Ge.$$.fragment,Q),Pd=h(Q),Nn=s(Q,"P",{});var vh=d(Nn);Nd=a(vh,"Example:"),vh.forEach(o),qd=h(Q),_(St.$$.fragment,Q),Q.forEach(o),H.forEach(o),ms=h(t),we=s(t,"H2",{class:!0});var Ws=d(we);We=s(Ws,"A",{id:!0,class:!0,href:!0});var kh=d(We);qn=s(kh,"SPAN",{});var Th=d(qn);_(Dt.$$.fragment,Th),Th.forEach(o),kh.forEach(o),zd=h(Ws),zn=s(Ws,"SPAN",{});var bh=d(zn);$d=a(bh,"ProphetNetDecoder"),bh.forEach(o),Ws.forEach(o),fs=h(t),E=s(t,"DIV",{class:!0});var V=d(E);_(Ot.$$.fragment,V),Fd=h(V),Lt=s(V,"P",{});var Hs=d(Lt);Ed=a(Hs,`The standalone decoder part of the ProphetNetModel.
This model inherits from `),bo=s(Hs,"A",{href:!0});var wh=d(bo);Md=a(wh,"PreTrainedModel"),wh.forEach(o),xd=a(Hs,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Hs.forEach(o),Cd=h(V),ye=s(V,"P",{});var Oo=d(ye);Sd=a(Oo,"Original ProphetNet code can be found "),jt=s(Oo,"A",{href:!0,rel:!0});var yh=d(jt);Dd=a(yh,"here"),yh.forEach(o),Od=a(Oo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),$n=s(Oo,"CODE",{});var Ph=d($n);Ld=a(Ph,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Ph.forEach(o),jd=a(Oo,"."),Oo.forEach(o),Ad=h(V),At=s(V,"P",{});var Vs=d(At);Id=a(Vs,"This model is a PyTorch "),It=s(Vs,"A",{href:!0,rel:!0});var Nh=d(It);Bd=a(Nh,"torch.nn.Module"),Nh.forEach(o),Gd=a(Vs,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Vs.forEach(o),Wd=h(V),G=s(V,"P",{});var X=d(G);Hd=a(X,"word_embeddings  ("),Fn=s(X,"CODE",{});var qh=d(Fn);Vd=a(qh,"torch.nn.Embeddings"),qh.forEach(o),Ud=a(X," of shape "),En=s(X,"CODE",{});var zh=d(En);Rd=a(zh,"(config.vocab_size, config.hidden_size)"),zh.forEach(o),Jd=a(X,", "),Mn=s(X,"EM",{});var $h=d(Mn);Yd=a($h,"optional"),$h.forEach(o),Kd=a(X,`):
The word embedding parameters. This can be used to initialize `),wo=s(X,"A",{href:!0});var Fh=d(wo);Qd=a(Fh,"ProphetNetEncoder"),Fh.forEach(o),Xd=a(X,` with pre-defined word
embeddings instead of randomly initialized word embeddings.`),X.forEach(o),Zd=h(V),O=s(V,"DIV",{class:!0});var Z=d(O);_(Bt.$$.fragment,Z),ei=h(Z),Pe=s(Z,"P",{});var Lo=d(Pe);ti=a(Lo,"The "),yo=s(Lo,"A",{href:!0});var Eh=d(yo);oi=a(Eh,"ProphetNetDecoder"),Eh.forEach(o),ni=a(Lo," forward method, overrides the "),xn=s(Lo,"CODE",{});var Mh=d(xn);si=a(Mh,"__call__"),Mh.forEach(o),ri=a(Lo," special method."),Lo.forEach(o),ai=h(Z),_(He.$$.fragment,Z),di=h(Z),Cn=s(Z,"P",{});var xh=d(Cn);ii=a(xh,"Example:"),xh.forEach(o),ci=h(Z),_(Gt.$$.fragment,Z),Z.forEach(o),V.forEach(o),_s=h(t),Ne=s(t,"H2",{class:!0});var Us=d(Ne);Ve=s(Us,"A",{id:!0,class:!0,href:!0});var Ch=d(Ve);Sn=s(Ch,"SPAN",{});var Sh=d(Sn);_(Wt.$$.fragment,Sh),Sh.forEach(o),Ch.forEach(o),hi=h(Us),Dn=s(Us,"SPAN",{});var Dh=d(Dn);li=a(Dh,"ProphetNetForConditionalGeneration"),Dh.forEach(o),Us.forEach(o),gs=h(t),x=s(t,"DIV",{class:!0});var ee=d(x);_(Ht.$$.fragment,ee),pi=h(ee),Vt=s(ee,"P",{});var Rs=d(Vt);ui=a(Rs,`The ProphetNet Model with a language modeling head. Can be used for sequence generation tasks.
This model inherits from `),Po=s(Rs,"A",{href:!0});var Oh=d(Po);mi=a(Oh,"PreTrainedModel"),Oh.forEach(o),fi=a(Rs,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Rs.forEach(o),_i=h(ee),qe=s(ee,"P",{});var jo=d(qe);gi=a(jo,"Original ProphetNet code can be found "),Ut=s(jo,"A",{href:!0,rel:!0});var Lh=d(Ut);vi=a(Lh,"here"),Lh.forEach(o),ki=a(jo,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),On=s(jo,"CODE",{});var jh=d(On);Ti=a(jh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),jh.forEach(o),bi=a(jo,"."),jo.forEach(o),wi=h(ee),Rt=s(ee,"P",{});var Js=d(Rt);yi=a(Js,"This model is a PyTorch "),Jt=s(Js,"A",{href:!0,rel:!0});var Ah=d(Jt);Pi=a(Ah,"torch.nn.Module"),Ah.forEach(o),Ni=a(Js,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Js.forEach(o),qi=h(ee),L=s(ee,"DIV",{class:!0});var te=d(L);_(Yt.$$.fragment,te),zi=h(te),ze=s(te,"P",{});var Ao=d(ze);$i=a(Ao,"The "),No=s(Ao,"A",{href:!0});var Ih=d(No);Fi=a(Ih,"ProphetNetForConditionalGeneration"),Ih.forEach(o),Ei=a(Ao," forward method, overrides the "),Ln=s(Ao,"CODE",{});var Bh=d(Ln);Mi=a(Bh,"__call__"),Bh.forEach(o),xi=a(Ao," special method."),Ao.forEach(o),Ci=h(te),_(Ue.$$.fragment,te),Si=h(te),jn=s(te,"P",{});var Gh=d(jn);Di=a(Gh,"Example:"),Gh.forEach(o),Oi=h(te),_(Kt.$$.fragment,te),te.forEach(o),ee.forEach(o),vs=h(t),$e=s(t,"H2",{class:!0});var Ys=d($e);Re=s(Ys,"A",{id:!0,class:!0,href:!0});var Wh=d(Re);An=s(Wh,"SPAN",{});var Hh=d(An);_(Qt.$$.fragment,Hh),Hh.forEach(o),Wh.forEach(o),Li=h(Ys),In=s(Ys,"SPAN",{});var Vh=d(In);ji=a(Vh,"ProphetNetForCausalLM"),Vh.forEach(o),Ys.forEach(o),ks=h(t),C=s(t,"DIV",{class:!0});var oe=d(C);_(Xt.$$.fragment,oe),Ai=h(oe),Zt=s(oe,"P",{});var Ks=d(Zt);Ii=a(Ks,`The standalone decoder part of the ProphetNetModel with a lm head on top. The model can be used for causal language modeling.
This model inherits from `),qo=s(Ks,"A",{href:!0});var Uh=d(qo);Bi=a(Uh,"PreTrainedModel"),Uh.forEach(o),Gi=a(Ks,`. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`),Ks.forEach(o),Wi=h(oe),Fe=s(oe,"P",{});var Io=d(Fe);Hi=a(Io,"Original ProphetNet code can be found "),eo=s(Io,"A",{href:!0,rel:!0});var Rh=d(eo);Vi=a(Rh,"here"),Rh.forEach(o),Ui=a(Io,`. Checkpoints were converted
from original Fairseq checkpoints. For more information on the checkpoint conversion, please take a look at the
file `),Bn=s(Io,"CODE",{});var Jh=d(Bn);Ri=a(Jh,"convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py"),Jh.forEach(o),Ji=a(Io,"."),Io.forEach(o),Yi=h(oe),to=s(oe,"P",{});var Qs=d(to);Ki=a(Qs,"This model is a PyTorch "),oo=s(Qs,"A",{href:!0,rel:!0});var Yh=d(oo);Qi=a(Yh,"torch.nn.Module"),Yh.forEach(o),Xi=a(Qs,` sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matters related to general usage and
behavior.`),Qs.forEach(o),Zi=h(oe),j=s(oe,"DIV",{class:!0});var ne=d(j);_(no.$$.fragment,ne),ec=h(ne),Ee=s(ne,"P",{});var Bo=d(Ee);tc=a(Bo,"The "),zo=s(Bo,"A",{href:!0});var Kh=d(zo);oc=a(Kh,"ProphetNetForCausalLM"),Kh.forEach(o),nc=a(Bo," forward method, overrides the "),Gn=s(Bo,"CODE",{});var Qh=d(Gn);sc=a(Qh,"__call__"),Qh.forEach(o),rc=a(Bo," special method."),Bo.forEach(o),ac=h(ne),_(Je.$$.fragment,ne),dc=h(ne),Wn=s(ne,"P",{});var Xh=d(Wn);ic=a(Xh,"Example:"),Xh.forEach(o),cc=h(ne),_(so.$$.fragment,ne),ne.forEach(o),oe.forEach(o),this.h()},h(){i(u,"name","hf:doc:metadata"),i(u,"content",JSON.stringify(cl)),i(y,"id","prophetnet"),i(y,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(y,"href","#prophetnet"),i(m,"class","relative group"),i(Qe,"href","https://github.com/huggingface/transformers/issues/new?assignees=&labels=&template=bug-report.md&title"),i(Qe,"rel","nofollow"),i(Me,"id","overview"),i(Me,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Me,"href","#overview"),i(ae,"class","relative group"),i(Ze,"href","https://arxiv.org/abs/2001.04063"),i(Ze,"rel","nofollow"),i(et,"href","https://github.com/microsoft/ProphetNet"),i(et,"rel","nofollow"),i(Se,"id","transformers.ProphetNetConfig"),i(Se,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Se,"href","#transformers.ProphetNetConfig"),i(de,"class","relative group"),i(ho,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(lo,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),i(po,"href","/docs/transformers/master/en/main_classes/configuration#transformers.PretrainedConfig"),i(U,"class","docstring"),i(De,"id","transformers.ProphetNetTokenizer"),i(De,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(De,"href","#transformers.ProphetNetTokenizer"),i(ce,"class","relative group"),i(uo,"href","/docs/transformers/master/en/main_classes/tokenizer#transformers.PreTrainedTokenizer"),i(R,"class","docstring"),i(Oe,"class","docstring"),i(W,"class","docstring"),i(Le,"class","docstring"),i($,"class","docstring"),i(je,"id","transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"),i(je,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(je,"href","#transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput"),i(le,"class","relative group"),i(pe,"class","docstring"),i(ue,"class","docstring"),i(me,"class","docstring"),i(fe,"class","docstring"),i(Ae,"id","transformers.ProphetNetModel"),i(Ae,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ae,"href","#transformers.ProphetNetModel"),i(_e,"class","relative group"),i(_o,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),i(wt,"href","https://github.com/microsoft/ProphetNet"),i(wt,"rel","nofollow"),i(Pt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Pt,"rel","nofollow"),i(go,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetModel"),i(S,"class","docstring"),i(M,"class","docstring"),i(Be,"id","transformers.ProphetNetEncoder"),i(Be,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Be,"href","#transformers.ProphetNetEncoder"),i(ke,"class","relative group"),i(vo,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),i(Et,"href","https://github.com/microsoft/ProphetNet"),i(Et,"rel","nofollow"),i(xt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(xt,"rel","nofollow"),i(ko,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(To,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(D,"class","docstring"),i(F,"class","docstring"),i(We,"id","transformers.ProphetNetDecoder"),i(We,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(We,"href","#transformers.ProphetNetDecoder"),i(we,"class","relative group"),i(bo,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),i(jt,"href","https://github.com/microsoft/ProphetNet"),i(jt,"rel","nofollow"),i(It,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(It,"rel","nofollow"),i(wo,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetEncoder"),i(yo,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetDecoder"),i(O,"class","docstring"),i(E,"class","docstring"),i(Ve,"id","transformers.ProphetNetForConditionalGeneration"),i(Ve,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Ve,"href","#transformers.ProphetNetForConditionalGeneration"),i(Ne,"class","relative group"),i(Po,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),i(Ut,"href","https://github.com/microsoft/ProphetNet"),i(Ut,"rel","nofollow"),i(Jt,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(Jt,"rel","nofollow"),i(No,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration"),i(L,"class","docstring"),i(x,"class","docstring"),i(Re,"id","transformers.ProphetNetForCausalLM"),i(Re,"class","header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full"),i(Re,"href","#transformers.ProphetNetForCausalLM"),i($e,"class","relative group"),i(qo,"href","/docs/transformers/master/en/main_classes/model#transformers.PreTrainedModel"),i(eo,"href","https://github.com/microsoft/ProphetNet"),i(eo,"rel","nofollow"),i(oo,"href","https://pytorch.org/docs/stable/nn.html#torch.nn.Module"),i(oo,"rel","nofollow"),i(zo,"href","/docs/transformers/master/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM"),i(j,"class","docstring"),i(C,"class","docstring")},m(t,l){e(document.head,u),p(t,P,l),p(t,m,l),e(m,y),e(y,N),g(w,N,null),e(m,b),e(m,q),e(q,Xs),p(t,Jn,l),p(t,re,l),e(re,Wo),e(Wo,Zs),e(re,er),e(re,Qe),e(Qe,tr),e(re,or),p(t,Yn,l),p(t,ae,l),e(ae,Me),e(Me,Ho),g(Xe,Ho,null),e(ae,nr),e(ae,Vo),e(Vo,sr),p(t,Kn,l),p(t,xe,l),e(xe,rr),e(xe,Ze),e(Ze,ar),e(xe,dr),p(t,Qn,l),p(t,ao,l),e(ao,ir),p(t,Xn,l),p(t,io,l),e(io,cr),p(t,Zn,l),p(t,co,l),e(co,Uo),e(Uo,hr),p(t,es,l),p(t,Ce,l),e(Ce,lr),e(Ce,et),e(et,pr),e(Ce,ur),p(t,ts,l),p(t,de,l),e(de,Se),e(Se,Ro),g(tt,Ro,null),e(de,mr),e(de,Jo),e(Jo,fr),p(t,os,l),p(t,U,l),g(ot,U,null),e(U,_r),e(U,nt),e(nt,gr),e(nt,ho),e(ho,vr),e(nt,kr),e(U,Tr),e(U,ie),e(ie,br),e(ie,lo),e(lo,wr),e(ie,yr),e(ie,po),e(po,Pr),e(ie,Nr),p(t,ns,l),p(t,ce,l),e(ce,De),e(De,Yo),g(st,Yo,null),e(ce,qr),e(ce,Ko),e(Ko,zr),p(t,ss,l),p(t,$,l),g(rt,$,null),e($,$r),e($,Qo),e(Qo,Fr),e($,Er),e($,at),e(at,Mr),e(at,uo),e(uo,xr),e(at,Cr),e($,Sr),e($,R),g(dt,R,null),e(R,Dr),e(R,Xo),e(Xo,Or),e(R,Lr),e(R,it),e(it,mo),e(mo,jr),e(mo,Zo),e(Zo,Ar),e(it,Ir),e(it,fo),e(fo,Br),e(fo,en),e(en,Gr),e($,Wr),e($,Oe),g(ct,Oe,null),e(Oe,Hr),e(Oe,tn),e(tn,Vr),e($,Ur),e($,W),g(ht,W,null),e(W,Rr),e(W,on),e(on,Jr),e(W,Yr),g(lt,W,null),e(W,Kr),e(W,he),e(he,Qr),e(he,nn),e(nn,Xr),e(he,Zr),e(he,sn),e(sn,ea),e(he,ta),e($,oa),e($,Le),g(pt,Le,null),e(Le,na),e(Le,ut),e(ut,sa),e(ut,rn),e(rn,ra),e(ut,aa),p(t,rs,l),p(t,le,l),e(le,je),e(je,an),g(mt,an,null),e(le,da),e(le,dn),e(dn,ia),p(t,as,l),p(t,pe,l),g(ft,pe,null),e(pe,ca),e(pe,cn),e(cn,ha),p(t,ds,l),p(t,ue,l),g(_t,ue,null),e(ue,la),e(ue,hn),e(hn,pa),p(t,is,l),p(t,me,l),g(gt,me,null),e(me,ua),e(me,ln),e(ln,ma),p(t,cs,l),p(t,fe,l),g(vt,fe,null),e(fe,fa),e(fe,pn),e(pn,_a),p(t,hs,l),p(t,_e,l),e(_e,Ae),e(Ae,un),g(kt,un,null),e(_e,ga),e(_e,mn),e(mn,va),p(t,ls,l),p(t,M,l),g(Tt,M,null),e(M,ka),e(M,bt),e(bt,Ta),e(bt,_o),e(_o,ba),e(bt,wa),e(M,ya),e(M,ge),e(ge,Pa),e(ge,wt),e(wt,Na),e(ge,qa),e(ge,fn),e(fn,za),e(ge,$a),e(M,Fa),e(M,yt),e(yt,Ea),e(yt,Pt),e(Pt,Ma),e(yt,xa),e(M,Ca),e(M,S),g(Nt,S,null),e(S,Sa),e(S,ve),e(ve,Da),e(ve,go),e(go,Oa),e(ve,La),e(ve,_n),e(_n,ja),e(ve,Aa),e(S,Ia),g(Ie,S,null),e(S,Ba),e(S,gn),e(gn,Ga),e(S,Wa),g(qt,S,null),p(t,ps,l),p(t,ke,l),e(ke,Be),e(Be,vn),g(zt,vn,null),e(ke,Ha),e(ke,kn),e(kn,Va),p(t,us,l),p(t,F,l),g($t,F,null),e(F,Ua),e(F,Ft),e(Ft,Ra),e(Ft,vo),e(vo,Ja),e(Ft,Ya),e(F,Ka),e(F,Te),e(Te,Qa),e(Te,Et),e(Et,Xa),e(Te,Za),e(Te,Tn),e(Tn,ed),e(Te,td),e(F,od),e(F,Mt),e(Mt,nd),e(Mt,xt),e(xt,sd),e(Mt,rd),e(F,ad),e(F,B),e(B,dd),e(B,bn),e(bn,id),e(B,cd),e(B,wn),e(wn,hd),e(B,ld),e(B,yn),e(yn,pd),e(B,ud),e(B,ko),e(ko,md),e(B,fd),e(F,_d),e(F,D),g(Ct,D,null),e(D,gd),e(D,be),e(be,vd),e(be,To),e(To,kd),e(be,Td),e(be,Pn),e(Pn,bd),e(be,wd),e(D,yd),g(Ge,D,null),e(D,Pd),e(D,Nn),e(Nn,Nd),e(D,qd),g(St,D,null),p(t,ms,l),p(t,we,l),e(we,We),e(We,qn),g(Dt,qn,null),e(we,zd),e(we,zn),e(zn,$d),p(t,fs,l),p(t,E,l),g(Ot,E,null),e(E,Fd),e(E,Lt),e(Lt,Ed),e(Lt,bo),e(bo,Md),e(Lt,xd),e(E,Cd),e(E,ye),e(ye,Sd),e(ye,jt),e(jt,Dd),e(ye,Od),e(ye,$n),e($n,Ld),e(ye,jd),e(E,Ad),e(E,At),e(At,Id),e(At,It),e(It,Bd),e(At,Gd),e(E,Wd),e(E,G),e(G,Hd),e(G,Fn),e(Fn,Vd),e(G,Ud),e(G,En),e(En,Rd),e(G,Jd),e(G,Mn),e(Mn,Yd),e(G,Kd),e(G,wo),e(wo,Qd),e(G,Xd),e(E,Zd),e(E,O),g(Bt,O,null),e(O,ei),e(O,Pe),e(Pe,ti),e(Pe,yo),e(yo,oi),e(Pe,ni),e(Pe,xn),e(xn,si),e(Pe,ri),e(O,ai),g(He,O,null),e(O,di),e(O,Cn),e(Cn,ii),e(O,ci),g(Gt,O,null),p(t,_s,l),p(t,Ne,l),e(Ne,Ve),e(Ve,Sn),g(Wt,Sn,null),e(Ne,hi),e(Ne,Dn),e(Dn,li),p(t,gs,l),p(t,x,l),g(Ht,x,null),e(x,pi),e(x,Vt),e(Vt,ui),e(Vt,Po),e(Po,mi),e(Vt,fi),e(x,_i),e(x,qe),e(qe,gi),e(qe,Ut),e(Ut,vi),e(qe,ki),e(qe,On),e(On,Ti),e(qe,bi),e(x,wi),e(x,Rt),e(Rt,yi),e(Rt,Jt),e(Jt,Pi),e(Rt,Ni),e(x,qi),e(x,L),g(Yt,L,null),e(L,zi),e(L,ze),e(ze,$i),e(ze,No),e(No,Fi),e(ze,Ei),e(ze,Ln),e(Ln,Mi),e(ze,xi),e(L,Ci),g(Ue,L,null),e(L,Si),e(L,jn),e(jn,Di),e(L,Oi),g(Kt,L,null),p(t,vs,l),p(t,$e,l),e($e,Re),e(Re,An),g(Qt,An,null),e($e,Li),e($e,In),e(In,ji),p(t,ks,l),p(t,C,l),g(Xt,C,null),e(C,Ai),e(C,Zt),e(Zt,Ii),e(Zt,qo),e(qo,Bi),e(Zt,Gi),e(C,Wi),e(C,Fe),e(Fe,Hi),e(Fe,eo),e(eo,Vi),e(Fe,Ui),e(Fe,Bn),e(Bn,Ri),e(Fe,Ji),e(C,Yi),e(C,to),e(to,Ki),e(to,oo),e(oo,Qi),e(to,Xi),e(C,Zi),e(C,j),g(no,j,null),e(j,ec),e(j,Ee),e(Ee,tc),e(Ee,zo),e(zo,oc),e(Ee,nc),e(Ee,Gn),e(Gn,sc),e(Ee,rc),e(j,ac),g(Je,j,null),e(j,dc),e(j,Wn),e(Wn,ic),e(j,cc),g(so,j,null),Ts=!0},p(t,[l]){const ro={};l&2&&(ro.$$scope={dirty:l,ctx:t}),Ie.$set(ro);const Hn={};l&2&&(Hn.$$scope={dirty:l,ctx:t}),Ge.$set(Hn);const Vn={};l&2&&(Vn.$$scope={dirty:l,ctx:t}),He.$set(Vn);const Un={};l&2&&(Un.$$scope={dirty:l,ctx:t}),Ue.$set(Un);const Ye={};l&2&&(Ye.$$scope={dirty:l,ctx:t}),Je.$set(Ye)},i(t){Ts||(v(w.$$.fragment,t),v(Xe.$$.fragment,t),v(tt.$$.fragment,t),v(ot.$$.fragment,t),v(st.$$.fragment,t),v(rt.$$.fragment,t),v(dt.$$.fragment,t),v(ct.$$.fragment,t),v(ht.$$.fragment,t),v(lt.$$.fragment,t),v(pt.$$.fragment,t),v(mt.$$.fragment,t),v(ft.$$.fragment,t),v(_t.$$.fragment,t),v(gt.$$.fragment,t),v(vt.$$.fragment,t),v(kt.$$.fragment,t),v(Tt.$$.fragment,t),v(Nt.$$.fragment,t),v(Ie.$$.fragment,t),v(qt.$$.fragment,t),v(zt.$$.fragment,t),v($t.$$.fragment,t),v(Ct.$$.fragment,t),v(Ge.$$.fragment,t),v(St.$$.fragment,t),v(Dt.$$.fragment,t),v(Ot.$$.fragment,t),v(Bt.$$.fragment,t),v(He.$$.fragment,t),v(Gt.$$.fragment,t),v(Wt.$$.fragment,t),v(Ht.$$.fragment,t),v(Yt.$$.fragment,t),v(Ue.$$.fragment,t),v(Kt.$$.fragment,t),v(Qt.$$.fragment,t),v(Xt.$$.fragment,t),v(no.$$.fragment,t),v(Je.$$.fragment,t),v(so.$$.fragment,t),Ts=!0)},o(t){k(w.$$.fragment,t),k(Xe.$$.fragment,t),k(tt.$$.fragment,t),k(ot.$$.fragment,t),k(st.$$.fragment,t),k(rt.$$.fragment,t),k(dt.$$.fragment,t),k(ct.$$.fragment,t),k(ht.$$.fragment,t),k(lt.$$.fragment,t),k(pt.$$.fragment,t),k(mt.$$.fragment,t),k(ft.$$.fragment,t),k(_t.$$.fragment,t),k(gt.$$.fragment,t),k(vt.$$.fragment,t),k(kt.$$.fragment,t),k(Tt.$$.fragment,t),k(Nt.$$.fragment,t),k(Ie.$$.fragment,t),k(qt.$$.fragment,t),k(zt.$$.fragment,t),k($t.$$.fragment,t),k(Ct.$$.fragment,t),k(Ge.$$.fragment,t),k(St.$$.fragment,t),k(Dt.$$.fragment,t),k(Ot.$$.fragment,t),k(Bt.$$.fragment,t),k(He.$$.fragment,t),k(Gt.$$.fragment,t),k(Wt.$$.fragment,t),k(Ht.$$.fragment,t),k(Yt.$$.fragment,t),k(Ue.$$.fragment,t),k(Kt.$$.fragment,t),k(Qt.$$.fragment,t),k(Xt.$$.fragment,t),k(no.$$.fragment,t),k(Je.$$.fragment,t),k(so.$$.fragment,t),Ts=!1},d(t){o(u),t&&o(P),t&&o(m),T(w),t&&o(Jn),t&&o(re),t&&o(Yn),t&&o(ae),T(Xe),t&&o(Kn),t&&o(xe),t&&o(Qn),t&&o(ao),t&&o(Xn),t&&o(io),t&&o(Zn),t&&o(co),t&&o(es),t&&o(Ce),t&&o(ts),t&&o(de),T(tt),t&&o(os),t&&o(U),T(ot),t&&o(ns),t&&o(ce),T(st),t&&o(ss),t&&o($),T(rt),T(dt),T(ct),T(ht),T(lt),T(pt),t&&o(rs),t&&o(le),T(mt),t&&o(as),t&&o(pe),T(ft),t&&o(ds),t&&o(ue),T(_t),t&&o(is),t&&o(me),T(gt),t&&o(cs),t&&o(fe),T(vt),t&&o(hs),t&&o(_e),T(kt),t&&o(ls),t&&o(M),T(Tt),T(Nt),T(Ie),T(qt),t&&o(ps),t&&o(ke),T(zt),t&&o(us),t&&o(F),T($t),T(Ct),T(Ge),T(St),t&&o(ms),t&&o(we),T(Dt),t&&o(fs),t&&o(E),T(Ot),T(Bt),T(He),T(Gt),t&&o(_s),t&&o(Ne),T(Wt),t&&o(gs),t&&o(x),T(Ht),T(Yt),T(Ue),T(Kt),t&&o(vs),t&&o($e),T(Qt),t&&o(ks),t&&o(C),T(Xt),T(no),T(Je),T(so)}}}const cl={local:"prophetnet",sections:[{local:"overview",title:"Overview"},{local:"transformers.ProphetNetConfig",title:"ProphetNetConfig"},{local:"transformers.ProphetNetTokenizer",title:"ProphetNetTokenizer"},{local:"transformers.models.prophetnet.modeling_prophetnet.ProphetNetSeq2SeqLMOutput",title:"ProphetNet specific outputs"},{local:"transformers.ProphetNetModel",title:"ProphetNetModel"},{local:"transformers.ProphetNetEncoder",title:"ProphetNetEncoder"},{local:"transformers.ProphetNetDecoder",title:"ProphetNetDecoder"},{local:"transformers.ProphetNetForConditionalGeneration",title:"ProphetNetForConditionalGeneration"},{local:"transformers.ProphetNetForCausalLM",title:"ProphetNetForCausalLM"}],title:"ProphetNet"};function hl(I,u,P){let{fw:m}=u;return I.$$set=y=>{"fw"in y&&P(0,m=y.fw)},[m]}class gl extends Zh{constructor(u){super();el(this,u,hl,il,tl,{fw:0})}}export{gl as default,cl as metadata};
